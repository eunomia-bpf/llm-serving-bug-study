[
  {
    "number": 16645,
    "title": "[Bug]: AttributeError: 'Qwen2_5OmniConfig' object has no attribute 'num_attention_heads'",
    "body": "### Your current environment\n\njust look here:\n[https://github.com/huggingface/transformers/issues/37515#issuecomment-2804126324](url)\n\n### \ud83d\udc1b Describe the bug\n\n`System Info\nroot@445d74596699:/vllm-workspace# transformers-cli env\n\nCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\n\ntransformers version: 4.52.0.dev0\nPlatform: Linux-5.15.0-43-generic-x86_64-with-glibc2.35\nPython version: 3.12.9\nHuggingface_hub version: 0.30.2\nSafetensors version: 0.5.3\nAccelerate version: 1.5.2\nAccelerate config: not found\nDeepSpeed version: not installed\nPyTorch version (GPU?): 2.6.0+cu124 (True)\nTensorflow version (GPU?): not installed (NA)\nFlax version (CPU?/GPU?/TPU?): not installed (NA)\nJax version: not installed\nJaxLib version: not installed\nUsing distributed or parallel set-up in script?:\nUsing GPU in script?:\nGPU type: NVIDIA L20\n`(base) root@node15:/disk2/Qwen2.5-Omni-7B# more docker-compose.yml\n#version: '3.3'\nservices:\n\nvllm\nvllm-openai:\nimage: vllm/vllm-openai:v0.8.2\ncontainer_name: Qwen2.5-Omni-7B\nrestart: unless-stopped\nruntime: nvidia\nports:\n- 8007:8000\nvolumes:\n- /disk2:/models\ncommand: >\n--model /models/Qwen2.5-Omni-7B\n--tokenizer_mode=\"auto\"\n--trust-remote-code\n--dtype=bfloat16\n--max_num_seqs=256\n--tensor_parallel_size=1\n--gpu-memory-utilization=0.9\n--max-model-len=65536\n--served-model-name=Qwen2.5-Omni-7B\ndeploy:\nresources:\nreservations:\ndevices:\n- driver: nvidia\ncapabilities: [gpu]\ndevice_ids: [ \"1\" ]\nipc: host\nnetworks:\nvllm:\n(base) root@node15:/disk2/Qwen2.5-Omni-7B# docker commit 445d74596699 vllm/vllm-openai:v0.8.2\nsha256:fdf1171c4bc4edc473bb3857597124ae73176c1691a27befccb4360c81ff0d60\n(base) root@node15:/disk2/Qwen2.5-Omni-7B# docker compose -f docker-compose.yml up -d\n[+] Running 2/2\n\u2714 Network qwen25-omni-7b_default Created 0.0s\n\u2714 Container Qwen2.5-Omni-7B Started 0.6s\n(base) root@node15:/disk2/Qwen2.5-Omni-7B# docker logs -f Qwen2.5-Omni-7B\nINFO 04-15 00:06:11 [init.py:239] Automatically detected platform cuda.\nINFO 04-15 00:06:13 [api_server.py:981] vLLM API server version 0.8.2\nINFO 04-15 00:06:13 [api_server.py:982] args: Namespace(host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=[''], allowed_methods=[''], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/models/Qwen2.5-Omni-7B', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='bfloat16', kv_cache_dtype='auto', max_model_len=65536, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['Qwen2.5-Omni-7B'], qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)\nUnrecognized keys in rope_scaling for 'rope_type'='default': {'mrope_section'}\nINFO 04-15 00:06:22 [config.py:585] This model supports multiple tasks: {'reward', 'generate', 'classify', 'score', 'embed'}. Defaulting to 'generate'.\nINFO 04-15 00:06:22 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=2048.\nINFO 04-15 00:06:24 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='/models/Qwen2.5-Omni-7B', speculative_config=None, tokenizer='/models/Qwen2.5-Omni-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=65536, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen2.5-Omni-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\nWARNING 04-15 00:06:25 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fabea685df0>\nINFO 04-15 00:06:26 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nERROR 04-15 00:06:26 [core.py:343] EngineCore hit an exception: Traceback (most recent call last):\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 335, in run_engine_core\nERROR 04-15 00:06:26 [core.py:343] engine_core = EngineCoreProc(*args, **kwargs)\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 290, in init\nERROR 04-15 00:06:26 [core.py:343] super().init(vllm_config, executor_class, log_stats)\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 60, in init\nERROR 04-15 00:06:26 [core.py:343] self.model_executor = executor_class(vllm_config)\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 52, in init\nERROR 04-15 00:06:26 [core.py:343] self._init_executor()\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 46, in _init_executor\nERROR 04-15 00:06:26 [core.py:343] self.collective_rpc(\"init_device\")\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\nERROR 04-15 00:06:26 [core.py:343] answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2255, in run_method\nERROR 04-15 00:06:26 [core.py:343] return func(*args, **kwargs)\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 604, in init_device\nERROR 04-15 00:06:26 [core.py:343] self.worker.init_device() # type: ignore\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 120, in init_device\nERROR 04-15 00:06:26 [core.py:343] self.model_runner: GPUModelRunner = GPUModelRunner(\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 106, in init\nERROR 04-15 00:06:26 [core.py:343] self.num_kv_heads = model_config.get_num_kv_heads(parallel_config)\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/config.py\", line 884, in get_num_kv_heads\nERROR 04-15 00:06:26 [core.py:343] total_num_kv_heads = self.get_total_num_kv_heads()\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/config.py\", line 876, in get_total_num_kv_heads\nERROR 04-15 00:06:26 [core.py:343] return self.hf_text_config.num_attention_heads\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\", line 211, in getattribute\nERROR 04-15 00:06:26 [core.py:343] return super().getattribute(key)\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] AttributeError: 'Qwen2_5OmniConfig' object has no attribute 'num_attention_heads'\nERROR 04-15 00:06:26 [core.py:343]\nCRITICAL 04-15 00:06:26 [core_client.py:269] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.`\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-15T07:48:19+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16645/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16645"
  },
  {
    "number": 18887,
    "title": "[Bug]: FlashMLA V1 with FP8 KV cache not yet supported!",
    "body": "### Your current environment\n\nFlashMLA V1 with FP8 KV cache not yet supported!\n\n```\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\nNCCL_SOCKET_IFNAME=bond0 \\\nGLOO_SOCKET_IFNAME=bond0 \\\nVLLM_USE_V1=1 \\\nVLLM_USE_MODELSCOPE=true \\\nvllm serve /data/models/huggingface.co/deepseek-ai/DeepSeek-R1/DeepSeek-R1-Hzz1 \\\n--served-model-name deepseek-r1 \\\n--gpu-memory-utilization 0.8 \\\n--tensor-parallel-size 8  \\\n--trust-remote-code \\\n--enable-chunked-prefill \\\n--port 8000 \\\n--kv-cache-dtype fp8 \\\n--enable-expert-parallel\n```\n\nresult:\n\n![Image](https://github.com/user-attachments/assets/44ca8e77-1ce2-4b4c-b80b-9bda20e1ca35)\n\nGPU DEVICE h800 x 8 x 140GB\ncuda drive version 550.127.08\nvllm version 0.8.5 \n\npip list \n\n```\nPackage                                  Version\n---------------------------------------- --------------------\naccelerate                               0.34.0\naiofiles                                 23.2.1\naiohappyeyeballs                         2.4.0\naiohttp                                  3.10.5\naiohttp-cors                             0.7.0\naioprometheus                            23.12.0\naiosignal                                1.3.1\nairportsdata                             20241001\naliyun-python-sdk-core                   2.16.0\naliyun-python-sdk-kms                    2.16.5\naltair                                   5.5.0\nannotated-types                          0.7.0\nantlr4-python3-runtime                   4.9.3\nanyascii                                 0.3.2\nanyio                                    4.4.0\nargcomplete                              3.5.3\nastor                                    0.8.1\nasync-timeout                            4.0.3\nattrdict                                 2.0.1\nattrs                                    24.2.0\naudioread                                3.0.1\nauto_gptq                                0.7.1\nautoawq                                  0.2.5\nautoawq_kernels                          0.0.9\nav                                       14.0.1\nbabel                                    2.16.0\nbcrypt                                   4.2.1\nbeautifulsoup4                           4.12.3\nbitsandbytes                             0.45.1\nblack                                    24.10.0\nblake3                                   1.0.4\nblinker                                  1.9.0\nboto3                                    1.28.64\nbotocore                                 1.31.85\ncached_path                              1.6.7\ncachetools                               5.5.1\ncdifflib                                 1.2.9\ncertifi                                  2019.11.28\ncffi                                     1.17.1\nchardet                                  3.0.4\ncharset-normalizer                       3.3.2\nchattts                                  0.2.1\nclick                                    8.1.7\ncloudpickle                              3.0.0\ncn2an                                    0.5.23\ncolorama                                 0.4.6\ncoloredlogs                              15.0.1\ncolorful                                 0.5.6\ncompressed-tensors                       0.9.3\nconformer                                0.3.2\ncontourpy                                1.3.1\ncontrolnet-aux                           0.0.7\ncrcmod                                   1.7\ncryptography                             44.0.0\ncuda-python                              12.6.2.post1\ncupy-cuda12x                             13.4.0\ncycler                                   0.12.1\nCython                                   3.0.11\ndataclass-wizard                         0.35.0\ndatamodel-code-generator                 0.26.5\ndatasets                                 2.21.0\ndateparser                               1.1.8\ndbus-python                              1.2.16\ndecorator                                5.1.1\ndecord                                   0.6.0\nDeepCache                                0.1.1\nDeprecated                               1.2.15\ndepyf                                    0.18.0\ndiffusers                                0.32.2\ndill                                     0.3.8\ndiskcache                                5.6.3\nDistance                                 0.1.3\ndistlib                                  0.3.9\ndistro                                   1.9.0\ndistro-info                              0.23+ubuntu1.1\ndnspython                                2.7.0\ndocopt                                   0.6.2\necdsa                                    0.19.0\neditdistance                             0.8.1\neinops                                   0.8.0\neinx                                     0.3.0\nemail_validator                          2.2.0\nencodec                                  0.1.1\neva-decord                               0.6.1\nexceptiongroup                           1.2.2\nfastapi                                  0.115.11\nfastapi-cli                              0.0.7\nfastrlock                                0.8.3\nffmpy                                    0.5.0\nfilelock                                 3.17.0\nFlagEmbedding                            1.2.11\nflashinfer                               0.1.6+cu124torch2.4\nFlask                                    3.1.0\nflatbuffers                              25.1.21\nfonttools                                4.55.5\nfrozendict                               2.4.6\nfrozenlist                               1.4.1\nfsspec                                   2024.6.1\nfugashi                                  1.4.0\nfunasr                                   1.1.16\nfvcore                                   0.1.5.post20221221\ng2p-en                                   2.1.0\ngdown                                    5.2.0\ngekko                                    1.2.1\ngenson                                   1.3.0\ngguf                                     0.16.3\ngoogle-api-core                          2.24.0\ngoogle-auth                              2.38.0\ngoogle-cloud-core                        2.4.1\ngoogle-cloud-storage                     2.19.0\ngoogle-crc32c                            1.6.0\ngoogle-resumable-media                   2.7.2\ngoogleapis-common-protos                 1.66.0\ngradio                                   4.26.0\ngradio_client                            0.15.1\ngrpcio                                   1.70.0\ngruut                                    2.4.0\ngruut-ipa                                0.13.0\ngruut-lang-de                            2.0.1\ngruut-lang-en                            2.0.1\ngruut-lang-es                            2.0.1\ngruut-lang-fr                            2.0.2\nh11                                      0.14.0\nh2                                       4.2.0\nhf_transfer                              0.1.8\nhf-xet                                   1.0.2\nhiredis                                  3.1.0\nhpack                                    4.1.0\nhttpcore                                 1.0.5\nhttptools                                0.6.1\nhttpx                                    0.27.2\nhuggingface-hub                          0.30.1\nhumanfriendly                            10.0\nhydra-core                               1.3.2\nHypercorn                                0.17.3\nhyperframe                               6.1.0\nHyperPyYAML                              1.2.2\nidna                                     2.8\nimageio                                  2.37.0\nimageio-ffmpeg                           0.6.0\nimportlib_metadata                       8.0.0\nimportlib_resources                      6.5.2\ninflect                                  5.6.2\niniconfig                                2.0.0\ninteregular                              0.3.3\niopath                                   0.1.10\nisort                                    5.13.2\nitsdangerous                             2.2.0\njaconv                                   0.4.0\njamo                                     0.4.1\njieba                                    0.42.1\nJinja2                                   3.1.5\njiter                                    0.5.0\njj-pytorchvideo                          0.1.5\njmespath                                 0.10.0\njoblib                                   1.4.2\njsonlines                                1.2.0\njsonschema                               4.23.0\njsonschema-specifications                2023.12.1\nkaldiio                                  2.18.0\nkiwisolver                               1.4.8\nlark                                     1.2.2\nlazy_loader                              0.4\nlibnacl                                  2.1.0\nlibrosa                                  0.10.2.post1\nlightning                                2.5.0.post0\nlightning-utilities                      0.11.9\nlinkify-it-py                            2.0.3\nllama_cpp_python                         0.3.4\nllguidance                               0.7.13\nllvmlite                                 0.44.0\nlm-format-enforcer                       0.10.11\nloguru                                   0.7.3\nloralib                                  0.1.2\nmarkdown-it-py                           3.0.0\nMarkupSafe                               2.1.5\nmatplotlib                               3.10.0\nmdit-py-plugins                          0.4.2\nmdurl                                    0.1.2\nmecab-python3                            1.0.10\nmemray                                   1.15.0\nmistral_common                           1.5.4\nmodelscope                               1.18.1\nmpmath                                   1.3.0\nmsgpack                                  1.0.8\nmsgspec                                  0.18.6\nmultidict                                6.0.5\nmultiprocess                             0.70.16\nmypy-extensions                          1.0.0\nnanobind                                 2.6.1\nnarwhals                                 1.23.0\nnatsort                                  8.4.0\nnemo_text_processing                     1.0.2\nnest-asyncio                             1.6.0\nnetworkx                                 3.3\nninja                                    1.11.1.3\nnltk                                     3.9.1\nnum2words                                0.5.14\nnumba                                    0.61.2\nnumpy                                    1.26.4\nnvidia-cublas-cu12                       12.4.5.8\nnvidia-cuda-cupti-cu12                   12.4.127\nnvidia-cuda-nvrtc-cu12                   12.4.127\nnvidia-cuda-runtime-cu12                 12.4.127\nnvidia-cudnn-cu12                        9.1.0.70\nnvidia-cufft-cu12                        11.2.1.3\nnvidia-curand-cu12                       10.3.5.147\nnvidia-cusolver-cu12                     11.6.1.9\nnvidia-cusparse-cu12                     12.3.1.170\nnvidia-cusparselt-cu12                   0.6.2\nnvidia-ml-py                             12.560.30\nnvidia-nccl-cu12                         2.21.5\nnvidia-nvjitlink-cu12                    12.4.127\nnvidia-nvtx-cu12                         12.4.127\nomegaconf                                2.3.0\nonnxruntime                              1.20.1\nonnxruntime-gpu                          1.16.0\nopenai                                   1.60.0\nopencensus                               0.11.4\nopencensus-context                       0.1.3\nopencv-contrib-python-headless           4.11.0.86\nopencv-python                            4.11.0.86\nopencv-python-headless                   4.11.0.86\nopentelemetry-api                        1.26.0\nopentelemetry-exporter-otlp              1.26.0\nopentelemetry-exporter-otlp-proto-common 1.26.0\nopentelemetry-exporter-otlp-proto-grpc   1.26.0\nopentelemetry-exporter-otlp-proto-http   1.26.0\nopentelemetry-proto                      1.26.0\nopentelemetry-sdk                        1.26.0\nopentelemetry-semantic-conventions       0.47b0\nopentelemetry-semantic-conventions-ai    0.4.6\noptimum                                  1.23.3\norjson                                   3.10.15\normsgpack                                1.7.0\noss2                                     2.19.1\noutlines                                 0.1.11\noutlines_core                            0.1.26\npackaging                                24.1\npandas                                   2.2.2\nparameterized                            0.9.0\npartial-json-parser                      0.2.1.1.post4\npasslib                                  1.7.4\npathspec                                 0.12.1\npeft                                     0.14.0\npillow                                   10.4.0\npip                                      24.3.1\nplatformdirs                             4.3.6\npluggy                                   1.5.0\npooch                                    1.8.2\nportalocker                              3.1.1\nprettytable                              3.14.0\npriority                                 2.0.0\nproces                                   0.1.7\nprometheus_client                        0.20.0\nprometheus-fastapi-instrumentator        7.0.0\nproto-plus                               1.25.0\nprotobuf                                 4.25.7\npsutil                                   6.0.0\npy-cpuinfo                               9.0.0\npy-spy                                   0.4.0\npyairports                               2.1.1\npyarrow                                  17.0.0\npyasn1                                   0.6.1\npyasn1_modules                           0.4.1\npybase16384                              0.3.7\npybind11                                 2.13.6\npycountry                                24.6.1\npycparser                                2.22\npycryptodome                             3.21.0\npydantic                                 2.10.6\npydantic_core                            2.27.2\npydub                                    0.25.1\nPygments                                 2.19.1\nPyGObject                                3.36.0\npykakasi                                 2.3.0\npynini                                   2.1.5\npynndescent                              0.5.13\npyparsing                                3.2.1\npypinyin                                 0.53.0\nPySocks                                  1.7.1\npytest                                   8.3.4\npython-apt                               2.0.1+ubuntu0.20.4.1\npython-crfsuite                          0.9.11\npython-dateutil                          2.9.0.post0\npython-dotenv                            1.0.1\npython-jose                              3.3.0\npython-json-logger                       3.2.1\npython-multipart                         0.0.20\npython-rapidjson                         1.20\npytorch-lightning                        2.5.0.post0\npytorch-wpe                              0.0.1\npytz                                     2024.1\nPyYAML                                   6.0.2\npyzmq                                    26.2.0\nquantile-python                          1.1\nQuart                                    0.20.0\nqwen-vl-utils                            0.0.8\nray                                      2.43.0\nredis                                    5.2.1\nreferencing                              0.35.1\nregex                                    2024.7.24\nrequests                                 2.32.3\nrequests-unixsocket                      0.2.0\nrich                                     13.9.4\nrich-toolkit                             0.13.2\nrouge                                    1.0.1\nrpds-py                                  0.20.0\nrsa                                      4.9\nruamel.yaml                              0.18.10\nruamel.yaml.clib                         0.2.12\nruff                                     0.9.3\ns3transfer                               0.7.0\nsacremoses                               0.1.1\nsafetensors                              0.4.4\nscikit-image                             0.25.0\nscikit-learn                             1.6.1\nscipy                                    1.15.1\nsemantic-version                         2.10.0\nsentence-transformers                    3.4.0\nsentencepiece                            0.2.0\nsetproctitle                             1.3.4\nsetuptools                               75.8.0\nsglang                                   0.4.1.post7\nshellingham                              1.5.4\nsilero-vad                               5.1.2\nsix                                      1.17.0\nsmart-open                               7.1.0\nsniffio                                  1.3.1\nsoundfile                                0.13.0\nsoupsieve                                2.6\nsoxr                                     0.5.0.post1\nsse-starlette                            2.1.3\nstarlette                                0.46.0\nsympy                                    1.13.1\ntabulate                                 0.9.0\ntaskgroup                                0.2.2\ntblib                                    3.0.0\ntensorboardX                             2.6.2.2\ntensorizer                               2.9.1\ntermcolor                                2.5.0\ntext-generation                          0.7.0\ntextual                                  1.0.0\nthreadpoolctl                            3.5.0\ntifffile                                 2025.1.10\ntiktoken                                 0.7.0\ntimm                                     1.0.14\ntokenizers                               0.21.1\ntoml                                     0.10.2\ntomli                                    2.2.1\ntomlkit                                  0.12.0\ntorch                                    2.6.0\ntorch-complex                            0.4.4\ntorchao                                  0.10.0\ntorchaudio                               2.6.0\ntorchdiffeq                              0.2.5\ntorchmetrics                             1.6.1\ntorchvision                              0.21.0\ntqdm                                     4.66.5\ntransformers                             4.51.3\ntransformers-stream-generator            0.0.5\ntriton                                   3.2.0\ntritonclient                             2.54.0\ntyper                                    0.15.2\ntyping_extensions                        4.12.2\ntzdata                                   2024.1\ntzlocal                                  5.2\nuc-micro-py                              1.0.3\numap-learn                               0.5.7\nunattended-upgrades                      0.1\nunidic-lite                              1.0.8\nurllib3                                  2.0.7\nuvicorn                                  0.30.6\nuvloop                                   0.20.0\nvector-quantize-pytorch                  1.17.3\nverovio                                  4.5.1\nvirtualenv                               20.29.2\nvllm                                     0.8.5\nvllm-flash-attn                          2.6.1\nvocos                                    0.1.0\nwatchfiles                               0.24.0\nwcwidth                                  0.2.13\nwebsockets                               11.0.3\nWerkzeug                                 3.1.3\nWeTextProcessing                         1.0.3\nwget                                     3.2\nwheel                                    0.34.2\nwrapt                                    1.17.2\nwsproto                                  1.2.0\nx-transformers                           1.44.6\nxformers                                 0.0.29.post2\nxgrammar                                 0.1.18\nxinference                               1.2.2\nxoscar                                   0.4.6\nxxhash                                   3.5.0\nyacs                                     0.1.8\nyarl                                     1.9.9\nzipp                                     3.20.1\nzstandard                                0.23.0\n```\n\n\n### \ud83d\udc1b Describe the bug\n\nFlashMLA V1 with FP8 KV cache not yet supported!\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-05-29T07:48:59+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18887/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/18887"
  },
  {
    "number": 9225,
    "title": "[Bug]: vllm v0.6.2 is crashed on multiple GPU",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nINFO:     Started server process [11912]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 8080): address already in use\r\nINFO:     Waiting for application shutdown.\r\nINFO:     Application shutdown complete.\r\n\r\nMy command to start vllm:\r\n```\r\npython3 -m vllm.entrypoints.openai.api_server --model hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4 \\\r\n        --host 0.0.0.0 --port 8080  --seed 42 --trust-remote-code --disable-frontend-multiprocessing \\\r\n        --enable-chunked-prefill --tensor-parallel-size 2 --max-model-len 98304 >> \"$LOG_FILE\" 2>&1 &\r\n```\r\n\r\nIf I change tensor-parallel-size from 2 to 1, no such issue.\r\n\r\ndocker image in use is \"vllm/vllm-openai:v0.6.2\".\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-10-10T05:46:02+00:00",
    "closed_at": "2024-10-10T16:08:36+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9225/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9225"
  },
  {
    "number": 13154,
    "title": "[Usage]: How to optimize long text input token\uff1f",
    "body": "### Your current environment\n\nThe content of each group of conversations is stored as historical information, and the historical information + new questions are used as new input to ask questions. However, a problem will arise. As historical information accumulates, the time required for the first token increases. Is there any optimization solution for this?\n\nfor example:\n\n ```\nhistory:  [{'role': 'user', 'content': '\u4ecb\u7ecd\u4e0b\u7231\u56e0\u65af\u5766'}]\nhistory:  [{'role': 'user', 'content': '\u4ecb\u7ecd\u4e0b\u7231\u56e0\u65af\u5766'}, {'role': 'assistant', 'content': '\u7231\u56e0\u65af\u5766\u662f\u8457\u540d\u7684\u7406\u8bba\u7269\u7406\u5b66\u5bb6\uff0c\u6700\u8457\u540d\u7684\u662f\u4ed6\u7684\u76f8\u5bf9\u8bba.'}]\nhistory:  [{'role': 'user', 'content': '\u4ecb\u7ecd\u4e0b\u7231\u56e0\u65af\u5766'}, {'role': 'assistant', 'content': '\u7231\u56e0\u65af\u5766\u662f\u8457\u540d\u7684\u7406\u8bba\u7269\u7406\u5b66\u5bb6\uff0c\u6700\u8457\u540d\u7684\u662f\u4ed6\u7684\u76f8\u5bf9\u8bba.'}, {'role': 'user', 'content': '\u4f60\u662f\u8c01?'}]\n ```\n\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-02-12T11:58:41+00:00",
    "closed_at": "2025-02-17T09:43:14+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13154/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13154"
  },
  {
    "number": 19144,
    "title": "[Bug]: error `is not a multimodal model` when serving `Qwen/Qwen3-8B` connected to `gr.load_chat(...)`",
    "body": "### Your current environment\n\nrecent vllm nightly 0.9.0\n\n### \ud83d\udc1b Describe the bug\n\nIn combination with this problem (related to \"text_encoded\") https://github.com/gradio-app/gradio/issues/11331\n\nI get the error when using `gr.load_chat(...)` to send extremely long prompt and then extremely short prompt into `vllm serve`-d `Qwen/Qwen3-8B` model. Not sure if the problem is in Gradio or in vllm impl of OpenAI server\n \n```\n\nERROR 06-04 11:39:38 [serving_chat.py:199] Error in preprocessing prompt inputs                                                                                                                                                           \nERROR 06-04 11:39:38 [serving_chat.py:199] Traceback (most recent call last):                                                                                                                                                             \nERROR 06-04 11:39:38 [serving_chat.py:199]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/entrypoints/openai/serving_chat.py\", line 182, in create_chat_completion                                                    \nERROR 06-04 11:39:38 [serving_chat.py:199]     ) = await self._preprocess_chat(                                                                                                                                                           \nERROR 06-04 11:39:38 [serving_chat.py:199]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                           \nERROR 06-04 11:39:38 [serving_chat.py:199]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/entrypoints/openai/serving_engine.py\", line 679, in _preprocess_chat                                                        \nERROR 06-04 11:39:38 [serving_chat.py:199]     conversation, mm_data_future = parse_chat_messages_futures(\n\nERROR 06-04 11:39:38 [serving_chat.py:199]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                \nERROR 06-04 11:39:38 [serving_chat.py:199]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/entrypoints/chat_utils.py\", line 1193, in parse_chat_messages_futures                                                       \nERROR 06-04 11:39:38 [serving_chat.py:199]     sub_messages = _parse_chat_message_content(\n\nERROR 06-04 11:39:38 [serving_chat.py:199]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                \nERROR 06-04 11:39:38 [serving_chat.py:199]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/entrypoints/chat_utils.py\", line 1117, in _parse_chat_message_content                                                       \nERROR 06-04 11:39:38 [serving_chat.py:199]     result = _parse_chat_message_content_parts(                                                                                                                                                \nERROR 06-04 11:39:38 [serving_chat.py:199]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                \nERROR 06-04 11:39:38 [serving_chat.py:199]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/entrypoints/chat_utils.py\", line 1017, in _parse_chat_message_content_parts                                                 \nERROR 06-04 11:39:38 [serving_chat.py:199]     parse_res = _parse_chat_message_content_part(                                                                                                                                              \nERROR 06-04 11:39:38 [serving_chat.py:199]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                              \nERROR 06-04 11:39:38 [serving_chat.py:199]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/entrypoints/chat_utils.py\", line 1074, in _parse_chat_message_content_part                                                  \nERROR 06-04 11:39:38 [serving_chat.py:199]     mm_parser.parse_image(str_content)                                                                                                                                                         \nERROR 06-04 11:39:38 [serving_chat.py:199]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/entrypoints/chat_utils.py\", line 774, in parse_image                                                                        \nERROR 06-04 11:39:38 [serving_chat.py:199]     placeholder = self._tracker.add(\"image\", image_coro)                                                                                                                                       \nERROR 06-04 11:39:38 [serving_chat.py:199]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nERROR 06-04 11:39:38 [serving_chat.py:199]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/entrypoints/chat_utils.py\", line 583, in add                                                                                \nERROR 06-04 11:39:38 [serving_chat.py:199]     mm_processor = mm_registry.create_processor(model_config)\n\nERROR 06-04 11:39:38 [serving_chat.py:199]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\nERROR 06-04 11:39:38 [serving_chat.py:199]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/multimodal/registry.py\", line 268, in create_processor\n\nERROR 06-04 11:39:38 [serving_chat.py:199]     raise ValueError(f\"{model_config.model} is not a multimodal model\")\n\nERROR 06-04 11:39:38 [serving_chat.py:199] ValueError: /home/inferencer/demockpt is not a multimodal model\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-06-04T13:41:26+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19144/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19144"
  },
  {
    "number": 9868,
    "title": "[New Model]: NV-Embed-v2",
    "body": "### The model to consider.\n\nhttps://huggingface.co/nvidia/NV-Embed-v2\n\n### The closest model vllm already supports.\n\n_No response_\n\n### What's your difficulty of supporting the model you want?\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-31T05:07:05+00:00",
    "closed_at": "2025-07-13T02:15:56+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9868/reactions",
      "total_count": 9,
      "+1": 9,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9868"
  },
  {
    "number": 18533,
    "title": "[Bug]: v1 engine with full cuda graph option outputs garbage",
    "body": "### Your current environment\n\nx\n\n### \ud83d\udc1b Describe the bug\n\nrefers to https://github.com/vllm-project/vllm/issues/18520\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-05-22T07:19:36+00:00",
    "closed_at": "2025-06-04T08:10:24+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18533/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/18533"
  },
  {
    "number": 5085,
    "title": "[Feature]: Adopt Colossal Inference Features (55% speedup over vLLM)",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nColossalAI has been able to demonstrate an impressive speedup over vLLM in multi-GPU inference. With TP=2, batch size 64, input len 512, output len 256 - a 55% speedup can be observed. I believe vLLM could see a speedup if it was to adopt a more performant batched prefilling.\r\n\r\n![image](https://github.com/vllm-project/vllm/assets/27340033/ad30d755-8683-4ecb-b1b7-52ee6216b6bd)\r\n\r\nFor reference, here is the continuous batching feature:\r\n\r\n![image](https://github.com/vllm-project/vllm/assets/27340033/a5026956-d087-4a5e-b0aa-d7a4c19f73d9)\r\n\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\nBlog post: https://hpc-ai.com/blog/colossal-inference\r\nSource code: https://github.com/hpcaitech/ColossalAI/tree/main/colossalai/inference",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-28T10:12:41+00:00",
    "closed_at": "2024-11-27T02:07:59+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5085/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/5085"
  },
  {
    "number": 18889,
    "title": "[Bug]:",
    "body": "### Your current environment\n\nFlashMLA V1 with FP8 KV cache not yet supported!\n\n```\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\nNCCL_SOCKET_IFNAME=bond0 \\\nGLOO_SOCKET_IFNAME=bond0 \\\nVLLM_USE_V1=1 \\\nVLLM_USE_MODELSCOPE=true \\\nvllm serve /data/models/huggingface.co/deepseek-ai/DeepSeek-R1/DeepSeek-R1-Hzz1 \\\n--served-model-name deepseek-r1 \\\n--gpu-memory-utilization 0.8 \\\n--tensor-parallel-size 8  \\\n--trust-remote-code \\\n--enable-chunked-prefill \\\n--port 8000 \\\n--kv-cache-dtype fp8 \\\n--enable-expert-parallel\n```\n\nresult:\n\n![Image](https://github.com/user-attachments/assets/44ca8e77-1ce2-4b4c-b80b-9bda20e1ca35)\n\nGPU DEVICE h800 x 8 x 140GB\ncuda drive version 550.127.08\nvllm version 0.8.5 \n\npip list \n\n```\nPackage                                  Version\n---------------------------------------- --------------------\naccelerate                               0.34.0\naiofiles                                 23.2.1\naiohappyeyeballs                         2.4.0\naiohttp                                  3.10.5\naiohttp-cors                             0.7.0\naioprometheus                            23.12.0\naiosignal                                1.3.1\nairportsdata                             20241001\naliyun-python-sdk-core                   2.16.0\naliyun-python-sdk-kms                    2.16.5\naltair                                   5.5.0\nannotated-types                          0.7.0\nantlr4-python3-runtime                   4.9.3\nanyascii                                 0.3.2\nanyio                                    4.4.0\nargcomplete                              3.5.3\nastor                                    0.8.1\nasync-timeout                            4.0.3\nattrdict                                 2.0.1\nattrs                                    24.2.0\naudioread                                3.0.1\nauto_gptq                                0.7.1\nautoawq                                  0.2.5\nautoawq_kernels                          0.0.9\nav                                       14.0.1\nbabel                                    2.16.0\nbcrypt                                   4.2.1\nbeautifulsoup4                           4.12.3\nbitsandbytes                             0.45.1\nblack                                    24.10.0\nblake3                                   1.0.4\nblinker                                  1.9.0\nboto3                                    1.28.64\nbotocore                                 1.31.85\ncached_path                              1.6.7\ncachetools                               5.5.1\ncdifflib                                 1.2.9\ncertifi                                  2019.11.28\ncffi                                     1.17.1\nchardet                                  3.0.4\ncharset-normalizer                       3.3.2\nchattts                                  0.2.1\nclick                                    8.1.7\ncloudpickle                              3.0.0\ncn2an                                    0.5.23\ncolorama                                 0.4.6\ncoloredlogs                              15.0.1\ncolorful                                 0.5.6\ncompressed-tensors                       0.9.3\nconformer                                0.3.2\ncontourpy                                1.3.1\ncontrolnet-aux                           0.0.7\ncrcmod                                   1.7\ncryptography                             44.0.0\ncuda-python                              12.6.2.post1\ncupy-cuda12x                             13.4.0\ncycler                                   0.12.1\nCython                                   3.0.11\ndataclass-wizard                         0.35.0\ndatamodel-code-generator                 0.26.5\ndatasets                                 2.21.0\ndateparser                               1.1.8\ndbus-python                              1.2.16\ndecorator                                5.1.1\ndecord                                   0.6.0\nDeepCache                                0.1.1\nDeprecated                               1.2.15\ndepyf                                    0.18.0\ndiffusers                                0.32.2\ndill                                     0.3.8\ndiskcache                                5.6.3\nDistance                                 0.1.3\ndistlib                                  0.3.9\ndistro                                   1.9.0\ndistro-info                              0.23+ubuntu1.1\ndnspython                                2.7.0\ndocopt                                   0.6.2\necdsa                                    0.19.0\neditdistance                             0.8.1\neinops                                   0.8.0\neinx                                     0.3.0\nemail_validator                          2.2.0\nencodec                                  0.1.1\neva-decord                               0.6.1\nexceptiongroup                           1.2.2\nfastapi                                  0.115.11\nfastapi-cli                              0.0.7\nfastrlock                                0.8.3\nffmpy                                    0.5.0\nfilelock                                 3.17.0\nFlagEmbedding                            1.2.11\nflashinfer                               0.1.6+cu124torch2.4\nFlask                                    3.1.0\nflatbuffers                              25.1.21\nfonttools                                4.55.5\nfrozendict                               2.4.6\nfrozenlist                               1.4.1\nfsspec                                   2024.6.1\nfugashi                                  1.4.0\nfunasr                                   1.1.16\nfvcore                                   0.1.5.post20221221\ng2p-en                                   2.1.0\ngdown                                    5.2.0\ngekko                                    1.2.1\ngenson                                   1.3.0\ngguf                                     0.16.3\ngoogle-api-core                          2.24.0\ngoogle-auth                              2.38.0\ngoogle-cloud-core                        2.4.1\ngoogle-cloud-storage                     2.19.0\ngoogle-crc32c                            1.6.0\ngoogle-resumable-media                   2.7.2\ngoogleapis-common-protos                 1.66.0\ngradio                                   4.26.0\ngradio_client                            0.15.1\ngrpcio                                   1.70.0\ngruut                                    2.4.0\ngruut-ipa                                0.13.0\ngruut-lang-de                            2.0.1\ngruut-lang-en                            2.0.1\ngruut-lang-es                            2.0.1\ngruut-lang-fr                            2.0.2\nh11                                      0.14.0\nh2                                       4.2.0\nhf_transfer                              0.1.8\nhf-xet                                   1.0.2\nhiredis                                  3.1.0\nhpack                                    4.1.0\nhttpcore                                 1.0.5\nhttptools                                0.6.1\nhttpx                                    0.27.2\nhuggingface-hub                          0.30.1\nhumanfriendly                            10.0\nhydra-core                               1.3.2\nHypercorn                                0.17.3\nhyperframe                               6.1.0\nHyperPyYAML                              1.2.2\nidna                                     2.8\nimageio                                  2.37.0\nimageio-ffmpeg                           0.6.0\nimportlib_metadata                       8.0.0\nimportlib_resources                      6.5.2\ninflect                                  5.6.2\niniconfig                                2.0.0\ninteregular                              0.3.3\niopath                                   0.1.10\nisort                                    5.13.2\nitsdangerous                             2.2.0\njaconv                                   0.4.0\njamo                                     0.4.1\njieba                                    0.42.1\nJinja2                                   3.1.5\njiter                                    0.5.0\njj-pytorchvideo                          0.1.5\njmespath                                 0.10.0\njoblib                                   1.4.2\njsonlines                                1.2.0\njsonschema                               4.23.0\njsonschema-specifications                2023.12.1\nkaldiio                                  2.18.0\nkiwisolver                               1.4.8\nlark                                     1.2.2\nlazy_loader                              0.4\nlibnacl                                  2.1.0\nlibrosa                                  0.10.2.post1\nlightning                                2.5.0.post0\nlightning-utilities                      0.11.9\nlinkify-it-py                            2.0.3\nllama_cpp_python                         0.3.4\nllguidance                               0.7.13\nllvmlite                                 0.44.0\nlm-format-enforcer                       0.10.11\nloguru                                   0.7.3\nloralib                                  0.1.2\nmarkdown-it-py                           3.0.0\nMarkupSafe                               2.1.5\nmatplotlib                               3.10.0\nmdit-py-plugins                          0.4.2\nmdurl                                    0.1.2\nmecab-python3                            1.0.10\nmemray                                   1.15.0\nmistral_common                           1.5.4\nmodelscope                               1.18.1\nmpmath                                   1.3.0\nmsgpack                                  1.0.8\nmsgspec                                  0.18.6\nmultidict                                6.0.5\nmultiprocess                             0.70.16\nmypy-extensions                          1.0.0\nnanobind                                 2.6.1\nnarwhals                                 1.23.0\nnatsort                                  8.4.0\nnemo_text_processing                     1.0.2\nnest-asyncio                             1.6.0\nnetworkx                                 3.3\nninja                                    1.11.1.3\nnltk                                     3.9.1\nnum2words                                0.5.14\nnumba                                    0.61.2\nnumpy                                    1.26.4\nnvidia-cublas-cu12                       12.4.5.8\nnvidia-cuda-cupti-cu12                   12.4.127\nnvidia-cuda-nvrtc-cu12                   12.4.127\nnvidia-cuda-runtime-cu12                 12.4.127\nnvidia-cudnn-cu12                        9.1.0.70\nnvidia-cufft-cu12                        11.2.1.3\nnvidia-curand-cu12                       10.3.5.147\nnvidia-cusolver-cu12                     11.6.1.9\nnvidia-cusparse-cu12                     12.3.1.170\nnvidia-cusparselt-cu12                   0.6.2\nnvidia-ml-py                             12.560.30\nnvidia-nccl-cu12                         2.21.5\nnvidia-nvjitlink-cu12                    12.4.127\nnvidia-nvtx-cu12                         12.4.127\nomegaconf                                2.3.0\nonnxruntime                              1.20.1\nonnxruntime-gpu                          1.16.0\nopenai                                   1.60.0\nopencensus                               0.11.4\nopencensus-context                       0.1.3\nopencv-contrib-python-headless           4.11.0.86\nopencv-python                            4.11.0.86\nopencv-python-headless                   4.11.0.86\nopentelemetry-api                        1.26.0\nopentelemetry-exporter-otlp              1.26.0\nopentelemetry-exporter-otlp-proto-common 1.26.0\nopentelemetry-exporter-otlp-proto-grpc   1.26.0\nopentelemetry-exporter-otlp-proto-http   1.26.0\nopentelemetry-proto                      1.26.0\nopentelemetry-sdk                        1.26.0\nopentelemetry-semantic-conventions       0.47b0\nopentelemetry-semantic-conventions-ai    0.4.6\noptimum                                  1.23.3\norjson                                   3.10.15\normsgpack                                1.7.0\noss2                                     2.19.1\noutlines                                 0.1.11\noutlines_core                            0.1.26\npackaging                                24.1\npandas                                   2.2.2\nparameterized                            0.9.0\npartial-json-parser                      0.2.1.1.post4\npasslib                                  1.7.4\npathspec                                 0.12.1\npeft                                     0.14.0\npillow                                   10.4.0\npip                                      24.3.1\nplatformdirs                             4.3.6\npluggy                                   1.5.0\npooch                                    1.8.2\nportalocker                              3.1.1\nprettytable                              3.14.0\npriority                                 2.0.0\nproces                                   0.1.7\nprometheus_client                        0.20.0\nprometheus-fastapi-instrumentator        7.0.0\nproto-plus                               1.25.0\nprotobuf                                 4.25.7\npsutil                                   6.0.0\npy-cpuinfo                               9.0.0\npy-spy                                   0.4.0\npyairports                               2.1.1\npyarrow                                  17.0.0\npyasn1                                   0.6.1\npyasn1_modules                           0.4.1\npybase16384                              0.3.7\npybind11                                 2.13.6\npycountry                                24.6.1\npycparser                                2.22\npycryptodome                             3.21.0\npydantic                                 2.10.6\npydantic_core                            2.27.2\npydub                                    0.25.1\nPygments                                 2.19.1\nPyGObject                                3.36.0\npykakasi                                 2.3.0\npynini                                   2.1.5\npynndescent                              0.5.13\npyparsing                                3.2.1\npypinyin                                 0.53.0\nPySocks                                  1.7.1\npytest                                   8.3.4\npython-apt                               2.0.1+ubuntu0.20.4.1\npython-crfsuite                          0.9.11\npython-dateutil                          2.9.0.post0\npython-dotenv                            1.0.1\npython-jose                              3.3.0\npython-json-logger                       3.2.1\npython-multipart                         0.0.20\npython-rapidjson                         1.20\npytorch-lightning                        2.5.0.post0\npytorch-wpe                              0.0.1\npytz                                     2024.1\nPyYAML                                   6.0.2\npyzmq                                    26.2.0\nquantile-python                          1.1\nQuart                                    0.20.0\nqwen-vl-utils                            0.0.8\nray                                      2.43.0\nredis                                    5.2.1\nreferencing                              0.35.1\nregex                                    2024.7.24\nrequests                                 2.32.3\nrequests-unixsocket                      0.2.0\nrich                                     13.9.4\nrich-toolkit                             0.13.2\nrouge                                    1.0.1\nrpds-py                                  0.20.0\nrsa                                      4.9\nruamel.yaml                              0.18.10\nruamel.yaml.clib                         0.2.12\nruff                                     0.9.3\ns3transfer                               0.7.0\nsacremoses                               0.1.1\nsafetensors                              0.4.4\nscikit-image                             0.25.0\nscikit-learn                             1.6.1\nscipy                                    1.15.1\nsemantic-version                         2.10.0\nsentence-transformers                    3.4.0\nsentencepiece                            0.2.0\nsetproctitle                             1.3.4\nsetuptools                               75.8.0\nsglang                                   0.4.1.post7\nshellingham                              1.5.4\nsilero-vad                               5.1.2\nsix                                      1.17.0\nsmart-open                               7.1.0\nsniffio                                  1.3.1\nsoundfile                                0.13.0\nsoupsieve                                2.6\nsoxr                                     0.5.0.post1\nsse-starlette                            2.1.3\nstarlette                                0.46.0\nsympy                                    1.13.1\ntabulate                                 0.9.0\ntaskgroup                                0.2.2\ntblib                                    3.0.0\ntensorboardX                             2.6.2.2\ntensorizer                               2.9.1\ntermcolor                                2.5.0\ntext-generation                          0.7.0\ntextual                                  1.0.0\nthreadpoolctl                            3.5.0\ntifffile                                 2025.1.10\ntiktoken                                 0.7.0\ntimm                                     1.0.14\ntokenizers                               0.21.1\ntoml                                     0.10.2\ntomli                                    2.2.1\ntomlkit                                  0.12.0\ntorch                                    2.6.0\ntorch-complex                            0.4.4\ntorchao                                  0.10.0\ntorchaudio                               2.6.0\ntorchdiffeq                              0.2.5\ntorchmetrics                             1.6.1\ntorchvision                              0.21.0\ntqdm                                     4.66.5\ntransformers                             4.51.3\ntransformers-stream-generator            0.0.5\ntriton                                   3.2.0\ntritonclient                             2.54.0\ntyper                                    0.15.2\ntyping_extensions                        4.12.2\ntzdata                                   2024.1\ntzlocal                                  5.2\nuc-micro-py                              1.0.3\numap-learn                               0.5.7\nunattended-upgrades                      0.1\nunidic-lite                              1.0.8\nurllib3                                  2.0.7\nuvicorn                                  0.30.6\nuvloop                                   0.20.0\nvector-quantize-pytorch                  1.17.3\nverovio                                  4.5.1\nvirtualenv                               20.29.2\nvllm                                     0.8.5\nvllm-flash-attn                          2.6.1\nvocos                                    0.1.0\nwatchfiles                               0.24.0\nwcwidth                                  0.2.13\nwebsockets                               11.0.3\nWerkzeug                                 3.1.3\nWeTextProcessing                         1.0.3\nwget                                     3.2\nwheel                                    0.34.2\nwrapt                                    1.17.2\nwsproto                                  1.2.0\nx-transformers                           1.44.6\nxformers                                 0.0.29.post2\nxgrammar                                 0.1.18\nxinference                               1.2.2\nxoscar                                   0.4.6\nxxhash                                   3.5.0\nyacs                                     0.1.8\nyarl                                     1.9.9\nzipp                                     3.20.1\nzstandard                                0.23.0\n```\n\n\n### \ud83d\udc1b Describe the bug\n\nFlashMLA V1 with FP8 KV cache not yet supported!\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-05-29T07:49:03+00:00",
    "closed_at": "2025-05-29T08:31:41+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18889/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/18889"
  },
  {
    "number": 7332,
    "title": "[Bug]: Compiling FSM index high memory && subprocess OOM",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-3.10.0-1160.71.1.el7.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA GeForce RTX 3090\r\nGPU 1: NVIDIA GeForce RTX 3090\r\n\r\nNvidia driver version: 535.129.03\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          16\r\nOn-line CPU(s) list:             0-15\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Gold 6140M CPU @ 2.30GHz\r\nCPU family:                      6\r\nModel:                           85\r\nThread(s) per core:              1\r\nCore(s) per socket:              4\r\nSocket(s):                       4\r\nStepping:                        4\r\nBogoMIPS:                        4599.99\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology eagerfpu pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat umip pku ospke md_clear spec_ctrl intel_stibp arch_capabilities\r\nVirtualization:                  VT-x\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       512 KiB (16 instances)\r\nL1i cache:                       512 KiB (16 instances)\r\nL2 cache:                        64 MiB (16 instances)\r\nL3 cache:                        64 MiB (4 instances)\r\nNUMA node(s):                    1\r\nNUMA node0 CPU(s):               0-15\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Mitigation; PTE Inversion; VMX conditional cache flushes, SMT disabled\r\nVulnerability Mds:               Mitigation; Clear CPU buffers; SMT Host state unknown\r\nVulnerability Meltdown:          Mitigation; PTI\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; Load fences, usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; IBRS (kernel), IBPB\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT Host state unknown\r\n\r\nVersions of relevant libraries:\r\n[pip3] intel-extension-for-transformers==1.4.2\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] optree==0.12.1\r\n[pip3] pyzmq==26.0.3\r\n[pip3] torch==2.4.0\r\n[pip3] torchaudio==2.4.0\r\n[pip3] torchelastic==0.2.2\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.43.3\r\n[pip3] triton==3.0.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\r\n[conda] intel-extension-for-transformers 1.4.2                    pypi_0    pypi\r\n[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\r\n[conda] mkl                       2023.1.0         h213fc3f_46344  \r\n[conda] mkl-service               2.4.0           py311h5eee18b_1  \r\n[conda] mkl_fft                   1.3.8           py311h5eee18b_0  \r\n[conda] mkl_random                1.2.4           py311hdb19cb5_0  \r\n[conda] numpy                     1.26.4          py311h08b1b3b_0  \r\n[conda] numpy-base                1.26.4          py311hf175353_0  \r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] optree                    0.12.1                   pypi_0    pypi\r\n[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch\r\n[conda] pytorch-mutex             1.0                        cuda    pytorch\r\n[conda] pyzmq                     26.0.3                   pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchaudio                2.4.0               py311_cu121    pytorch\r\n[conda] torchelastic              0.2.2                    pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.43.3                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.4\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PHB     0-15    0               N/A\r\nGPU1    PHB      X      0-15    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nserver\r\n```bash\r\npython -m vllm.entrypoints.openai.api_server --model /model/Qwen1.5-14B-Chat-GPTQ-Int4 --quantization gptq --max-model-len 12720\r\n```\r\nclient\r\n```python\r\nclass ClassificationItem(BaseModel):\r\n    name: str = Field(max_length=20, title=\"\u5206\u7c7b\u540d\")\r\n    risk_level: conint(ge=0, lt=8) = Field(title=\"\u98ce\u9669\u7b49\u7ea7\")\r\n\r\n\r\nclass ClassificationSet(BaseModel):\r\n    classification_list: List[ClassificationItem] = Field(min_items=100, title=\"\u5206\u7c7b\u540d\u7684\u5217\u8868\")\r\n\r\n\r\nopenai_client = OpenAI(\r\n        base_url=\"http://192.168.91.25:8000/v1\",\r\n        api_key=\"EMPTY\",\r\n    )\r\nclient = instructor.from_openai(openai_client)\r\n\r\nresp = client.chat.completions.create(\r\n        model=\"/model/Qwen1.5-14B-Chat-GPTQ-Int4\",\r\n        messages=[{\"role\": \"user\",\r\n                   \"content\": \"\u4f60\u662f\u4e00\u540d\u6570\u636e\u5b89\u5168\u8fd0\u8425\u4e13\u5bb6\uff0c\u6211\u662f\u4e00\u4e2a\u6cd5\u5f8b\u884c\u4e1a\u7684\u516c\u53f8\uff0c\u662f\u4e00\u5bb6\u5f8b\u5e08\u4e8b\u52a1\u6240\uff0c\u6211\u4eec\u516c\u53f8\u8d1f\u8d23\u54cd\u5e94\u5ba2\u6237\u7684\u6cd5\u5f8b\u54a8\u8be2\u3001\u5e2e\u5ba2\u6237\u5728\u6cd5\u5ead\u4e0a\u8fa9\u62a4\uff0c\u6211\u4eec\u516c\u53f8\u91cc\u6709\u5f88\u591a\u673a\u5bc6\u7c7b\u578b\u7684\u6587\u4ef6\u6216\u8005\u6587\u6863\uff0c\u8bf7\u4f60\u4e3a\u6211\u5217\u4e3e\u4e00\u4e0b\u8fd9\u4e9b'\u7c7b\u578b'\uff0c\u53ea\u9700\u8981\u7ed9\u51fa\u7c7b\u578b\u540d\u548c\u8be5\u7c7b\u578b\u5bf9\u5e94\u7684\u98ce\u9669\u7b49\u7ea7\uff0c\u4e0d\u9700\u8981\u8f93\u51fajson\u4ee5\u5916\u7684\u5185\u5bb9\"}],\r\n        response_model=ClassificationSet\r\n    )\r\n```\r\n\r\npayload\r\n```bash\r\ncurl http://192.168.91.25:8000/v1/chat/completions \\\r\n-H \"Content-Type: application/json\" \\\r\n-d '{\r\n    \"messages\": [\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": \"\u4f60\u662f\u4e00\u540d\u6570\u636e\u5b89\u5168\u8fd0\u8425\u4e13\u5bb6\uff0c\u6211\u662f\u4e00\u4e2a\u6cd5\u5f8b\u884c\u4e1a\u7684\u516c\u53f8\uff0c\u662f\u4e00\u5bb6\u5f8b\u5e08\u4e8b\u52a1\u6240\uff0c\u6211\u4eec\u516c\u53f8\u8d1f\u8d23\u54cd\u5e94\u5ba2\u6237\u7684\u6cd5\u5f8b\u54a8\u8be2\u3001\u5e2e\u5ba2\u6237\u5728\u6cd5\u5ead\u4e0a\u8fa9\u62a4\uff0c\u6211\u4eec\u516c\u53f8\u91cc\u6709\u5f88\u591a\u673a\u5bc6\u7c7b\u578b\u7684\u6587\u4ef6\u6216\u8005\u6587\u6863\uff0c\u8bf7\u4f60\u4e3a\u6211\u5217\u4e3e\u4e00\u4e0b\u8fd9\u4e9b'\u7c7b\u578b'\uff0c\u53ea\u9700\u8981\u7ed9\u51fa\u7c7b\u578b\u540d\u548c\u8be5\u7c7b\u578b\u5bf9\u5e94\u7684\u98ce\u9669\u7b49\u7ea7\uff0c\u4e0d\u9700\u8981\u8f93\u51fajson\u4ee5\u5916\u7684\u5185\u5bb9\"\r\n        }\r\n    ],\r\n    \"model\": \"/model/Qwen1.5-14B-Chat-GPTQ-Int4\",\r\n    \"tools\": [\r\n        {\r\n            \"type\": \"function\",\r\n            \"function\": {\r\n                \"name\": \"ClassificationSet\",\r\n                \"description\": \"Correctly extracted `ClassificationSet` with all the required parameters with correct types\",\r\n                \"parameters\": {\r\n                    \"$defs\": {\r\n                        \"ClassificationItem\": {\r\n                            \"properties\": {\r\n                                \"name\": {\r\n                                    \"maxLength\": 20,\r\n                                    \"title\": \"\u5206\u7c7b\u540d\",\r\n                                    \"type\": \"string\"\r\n                                },\r\n                                \"risk_level\": {\r\n                                    \"exclusiveMaximum\": 8,\r\n                                    \"minimum\": 0,\r\n                                    \"title\": \"\u98ce\u9669\u7b49\u7ea7\",\r\n                                    \"type\": \"integer\"\r\n                                }\r\n                            },\r\n                            \"required\": [\r\n                                \"name\",\r\n                                \"risk_level\"\r\n                            ],\r\n                            \"title\": \"ClassificationItem\",\r\n                            \"type\": \"object\"\r\n                        }\r\n                    },\r\n                    \"properties\": {\r\n                        \"classification_list\": {\r\n                            \"items\": {\r\n                                \"$ref\": \"#/$defs/ClassificationItem\"\r\n                            },\r\n                            \"minItems\": 100,\r\n                            \"title\": \"\u5206\u7c7b\u540d\u7684\u5217\u8868\",\r\n                            \"type\": \"array\"\r\n                        }\r\n                    },\r\n                    \"required\": [\r\n                        \"classification_list\"\r\n                    ],\r\n                    \"type\": \"object\"\r\n                }\r\n            }\r\n        }\r\n    ],\r\n    \"tool_choice\": {\r\n        \"type\": \"function\",\r\n        \"function\": {\r\n            \"name\": \"ClassificationSet\"\r\n        }\r\n    }\r\n}\r\n```\r\n![20240809-110817](https://github.com/user-attachments/assets/48b8ee7b-a4d8-47f8-b13e-8480551b9cb1)\r\n\r\noutput\r\n```\r\n.....\r\nCompiling FSM index for all state transitions: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 15753/15831 [15:05<00:04, 16.41it/s]INFO 08-09 03:02:26 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nCompiling FSM index for all state transitions: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15831/15831 [15:10<00:00, 17.39it/s]\r\nINFO 08-09 03:02:36 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 08-09 03:02:46 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 08-09 03:02:56 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 08-09 03:03:11 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 08-09 03:03:28 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 08-09 03:03:48 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 08-09 03:03:58 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 08-09 03:04:03 logger.py:36] Received request chat-1048cec6489b46c4902ff692c76094a6: prompt: '<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\n\u4f60\u662f\u4e00\u540d\u6570\u636e\u5b89\u5168\u8fd0\u8425\u4e13\u5bb6\uff0c\u6211\u662f\u4e00\u4e2a\u6cd5\u5f8b\u884c\u4e1a\u7684\u516c\u53f8\uff0c\u662f\u4e00\u5bb6\u5f8b\u5e08\u4e8b\u52a1\u6240\uff0c\u6211\u4eec\u516c\u53f8\u8d1f\u8d23\u54cd\u5e94\u5ba2\u6237\u7684\u6cd5\u5f8b\u54a8\u8be2\u3001\u5e2e\u5ba2\u6237\u5728\u6cd5\u5ead\u4e0a\u8fa9\u62a4\uff0c\u6211\u4eec\u516c\u53f8\u91cc\u6709\u5f88\u591a\u673a\u5bc6\u7c7b\u578b\u7684\u6587\u4ef6\u6216\u8005\u6587\u6863\uff0c\u8bf7\u4f60\u4e3a\u6211\u5217\u4e3e\u4e00\u4e0b\u8fd9\u4e9b\u7c7b\u578b\uff0c\u53ea\u9700\u8981\u7ed9\u51fa\u7c7b\u578b\u540d\u548c\u8be5\u7c7b\u578b\u5bf9\u5e94\u7684\u98ce\u9669\u7b49\u7ea7\uff0c\u4e0d\u9700\u8981\u8f93\u51fajson\u4ee5\u5916\u7684\u5185\u5bb9<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12635, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 151645, 198, 151644, 872, 198, 56568, 110124, 20074, 99464, 101087, 101057, 3837, 35946, 101909, 100376, 104586, 73218, 3837, 105783, 110178, 31838, 3837, 97639, 73218, 100668, 102808, 107069, 100376, 100703, 5373, 99663, 100017, 18493, 108943, 17447, 114051, 3837, 97639, 73218, 69249, 101194, 32648, 27641, 109963, 26898, 100631, 111116, 37945, 56568, 17714, 35946, 118569, 100158, 100001, 31905, 3837, 107525, 107485, 31905, 13072, 33108, 75882, 31905, 103124, 106066, 104408, 3837, 104689, 66017, 2236, 105175, 104597, 151645, 198, 151644, 77091, 198], lora_request: None, prompt_adapter_request: None.\r\nINFO 08-09 03:04:18 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\n```\r\n\r\nhow skip compile FSM index?\r\n\r\n\r\n",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "closed",
    "created_at": "2024-08-09T03:10:36+00:00",
    "closed_at": "2025-06-27T19:59:20+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7332/reactions",
      "total_count": 4,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 4
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7332"
  },
  {
    "number": 10702,
    "title": "[Bug]: load llama 70B more than 10min\uff0c is that right\uff1f",
    "body": "### Your current environment\r\n\r\nimage: vllm/vllm-openai:latest\r\n\r\n0.6.4.post1\r\n\r\nH100 8GPU\r\n\r\n\r\n### Model Input Dumps\r\n```\r\nINFO 11-27 02:01:37 api_server.py:585] vLLM API server version 0.6.4.post1\r\nINFO 11-27 02:01:37 api_server.py:586] args: Namespace(subparser='serve', model_tag='/models/Meta-Llama-3.1-70B-Instruct', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/models/Meta-Llama-3.1-70B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', chat_template_text_format='string', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=8, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['deploy_model'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7fdd04d958a0>)\r\nINFO 11-27 02:01:37 api_server.py:175] Multiprocessing frontend to use ipc:///tmp/48eb9037-6b86-4fd5-9c49-a3fb593c9e3f for IPC Path.\r\nINFO 11-27 02:01:37 api_server.py:194] Started engine process with PID 78\r\nINFO 11-27 02:01:40 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\r\nINFO 11-27 02:01:40 config.py:1020] Defaulting to use mp for distributed inference\r\nWARNING 11-27 02:01:40 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\r\nWARNING 11-27 02:01:40 arg_utils.py:1075] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\r\nINFO 11-27 02:01:40 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.\r\nINFO 11-27 02:02:09 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\r\nINFO 11-27 02:02:09 config.py:1020] Defaulting to use mp for distributed inference\r\nWARNING 11-27 02:02:09 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\r\nWARNING 11-27 02:02:09 arg_utils.py:1075] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\r\nINFO 11-27 02:02:09 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.\r\nINFO 11-27 02:02:09 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='/models/Meta-Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='/models/Meta-Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deploy_model, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\r\nWARNING 11-27 02:02:09 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 96 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\nINFO 11-27 02:02:10 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\nINFO 11-27 02:02:10 selector.py:135] Using Flash Attention backend.\r\n(VllmWorkerProcess pid=353) INFO 11-27 02:02:16 selector.py:135] Using Flash Attention backend.\r\n(VllmWorkerProcess pid=353) INFO 11-27 02:02:16 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=351) INFO 11-27 02:02:16 selector.py:135] Using Flash Attention backend.\r\n(VllmWorkerProcess pid=351) INFO 11-27 02:02:16 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=350) INFO 11-27 02:02:16 selector.py:135] Using Flash Attention backend.\r\n(VllmWorkerProcess pid=350) INFO 11-27 02:02:16 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=355) INFO 11-27 02:02:16 selector.py:135] Using Flash Attention backend.\r\n(VllmWorkerProcess pid=355) INFO 11-27 02:02:16 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=356) INFO 11-27 02:02:16 selector.py:135] Using Flash Attention backend.\r\n(VllmWorkerProcess pid=356) INFO 11-27 02:02:16 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=352) INFO 11-27 02:02:16 selector.py:135] Using Flash Attention backend.\r\n(VllmWorkerProcess pid=352) INFO 11-27 02:02:16 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=354) INFO 11-27 02:02:16 selector.py:135] Using Flash Attention backend.\r\n(VllmWorkerProcess pid=354) INFO 11-27 02:02:16 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=350) INFO 11-27 02:02:20 utils.py:961] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=351) INFO 11-27 02:02:20 utils.py:961] Found nccl from library libnccl.so.2\r\nINFO 11-27 02:02:20 utils.py:961] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=350) INFO 11-27 02:02:20 pynccl.py:69] vLLM is using nccl==2.21.5\r\n(VllmWorkerProcess pid=351) INFO 11-27 02:02:20 pynccl.py:69] vLLM is using nccl==2.21.5\r\nINFO 11-27 02:02:20 pynccl.py:69] vLLM is using nccl==2.21.5\r\n(VllmWorkerProcess pid=355) INFO 11-27 02:02:20 utils.py:961] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=355) INFO 11-27 02:02:20 pynccl.py:69] vLLM is using nccl==2.21.5\r\n(VllmWorkerProcess pid=352) INFO 11-27 02:02:20 utils.py:961] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=352) INFO 11-27 02:02:20 pynccl.py:69] vLLM is using nccl==2.21.5\r\n(VllmWorkerProcess pid=354) INFO 11-27 02:02:20 utils.py:961] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=354) INFO 11-27 02:02:20 pynccl.py:69] vLLM is using nccl==2.21.5\r\n(VllmWorkerProcess pid=356) INFO 11-27 02:02:20 utils.py:961] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=356) INFO 11-27 02:02:20 pynccl.py:69] vLLM is using nccl==2.21.5\r\n(VllmWorkerProcess pid=353) INFO 11-27 02:02:20 utils.py:961] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=353) INFO 11-27 02:02:20 pynccl.py:69] vLLM is using nccl==2.21.5\r\nINFO 11-27 02:02:27 custom_all_reduce_utils.py:204] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\r\nINFO 11-27 02:03:09 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\r\n(VllmWorkerProcess pid=352) INFO 11-27 02:03:09 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\r\n(VllmWorkerProcess pid=353) INFO 11-27 02:03:09 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\r\n(VllmWorkerProcess pid=355) INFO 11-27 02:03:09 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\r\n(VllmWorkerProcess pid=356) INFO 11-27 02:03:09 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\r\n(VllmWorkerProcess pid=354) INFO 11-27 02:03:09 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\r\n(VllmWorkerProcess pid=350) INFO 11-27 02:03:09 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\r\n(VllmWorkerProcess pid=351) INFO 11-27 02:03:09 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\r\nINFO 11-27 02:03:09 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7faa88fd5700>, local_subscribe_port=33587, remote_subscribe_port=None)\r\nINFO 11-27 02:03:09 model_runner.py:1072] Starting to load model /models/Meta-Llama-3.1-70B-Instruct...\r\n(VllmWorkerProcess pid=353) INFO 11-27 02:03:09 model_runner.py:1072] Starting to load model /models/Meta-Llama-3.1-70B-Instruct...\r\n(VllmWorkerProcess pid=354) INFO 11-27 02:03:09 model_runner.py:1072] Starting to load model /models/Meta-Llama-3.1-70B-Instruct...\r\n(VllmWorkerProcess pid=350) INFO 11-27 02:03:09 model_runner.py:1072] Starting to load model /models/Meta-Llama-3.1-70B-Instruct...\r\n(VllmWorkerProcess pid=352) INFO 11-27 02:03:09 model_runner.py:1072] Starting to load model /models/Meta-Llama-3.1-70B-Instruct...\r\n(VllmWorkerProcess pid=351) INFO 11-27 02:03:09 model_runner.py:1072] Starting to load model /models/Meta-Llama-3.1-70B-Instruct...\r\n(VllmWorkerProcess pid=355) INFO 11-27 02:03:09 model_runner.py:1072] Starting to load model /models/Meta-Llama-3.1-70B-Instruct...\r\n(VllmWorkerProcess pid=356) INFO 11-27 02:03:09 model_runner.py:1072] Starting to load model /models/Meta-Llama-3.1-70B-Instruct...\r\nLoading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:   3% Completed | 1/30 [00:24<11:39, 24.11s/it]\r\nLoading safetensors checkpoint shards:   7% Completed | 2/30 [00:29<06:08, 13.17s/it]\r\nLoading safetensors checkpoint shards:  10% Completed | 3/30 [00:49<07:23, 16.44s/it]\r\nLoading safetensors checkpoint shards:  13% Completed | 4/30 [01:15<08:44, 20.17s/it]\r\nLoading safetensors checkpoint shards:  17% Completed | 5/30 [01:20<06:06, 14.66s/it]\r\nLoading safetensors checkpoint shards:  20% Completed | 6/30 [01:42<06:51, 17.16s/it]\r\nLoading safetensors checkpoint shards:  23% Completed | 7/30 [01:48<05:07, 13.36s/it]\r\nLoading safetensors checkpoint shards:  27% Completed | 8/30 [02:09<05:48, 15.85s/it]\r\nLoading safetensors checkpoint shards:  30% Completed | 9/30 [02:15<04:27, 12.73s/it]\r\nLoading safetensors checkpoint shards:  33% Completed | 10/30 [02:36<05:06, 15.31s/it]\r\nLoading safetensors checkpoint shards:  37% Completed | 11/30 [03:04<06:05, 19.22s/it]\r\nLoading safetensors checkpoint shards:  40% Completed | 12/30 [03:09<04:29, 14.98s/it]\r\nLoading safetensors checkpoint shards:  43% Completed | 13/30 [03:47<06:10, 21.79s/it]\r\nLoading safetensors checkpoint shards:  47% Completed | 14/30 [04:13<06:12, 23.27s/it]\r\nLoading safetensors checkpoint shards:  50% Completed | 15/30 [04:36<05:43, 22.91s/it]\r\nLoading safetensors checkpoint shards:  53% Completed | 16/30 [04:41<04:07, 17.65s/it]\r\nLoading safetensors checkpoint shards:  57% Completed | 17/30 [06:34<10:01, 46.30s/it]\r\nLoading safetensors checkpoint shards:  60% Completed | 18/30 [06:57<07:52, 39.37s/it]\r\nLoading safetensors checkpoint shards:  63% Completed | 19/30 [07:43<07:36, 41.47s/it]\r\nLoading safetensors checkpoint shards:  67% Completed | 20/30 [07:48<05:05, 30.52s/it]\r\nLoading safetensors checkpoint shards:  70% Completed | 21/30 [08:09<04:06, 27.38s/it]\r\nLoading safetensors checkpoint shards:  73% Completed | 22/30 [08:29<03:21, 25.22s/it]\r\nLoading safetensors checkpoint shards:  77% Completed | 23/30 [09:03<03:16, 28.01s/it]\r\nLoading safetensors checkpoint shards:  80% Completed | 24/30 [09:09<02:07, 21.27s/it]\r\nLoading safetensors checkpoint shards:  83% Completed | 25/30 [10:24<03:08, 37.60s/it]\r\nLoading safetensors checkpoint shards:  87% Completed | 26/30 [10:30<01:51, 27.97s/it]\r\nLoading safetensors checkpoint shards:  90% Completed | 27/30 [10:59<01:24, 28.22s/it]\r\nLoading safetensors checkpoint shards:  93% Completed | 28/30 [11:04<00:42, 21.34s/it]\r\nLoading safetensors checkpoint shards:  97% Completed | 29/30 [11:30<00:22, 22.67s/it]\r\n(VllmWorkerProcess pid=354) INFO 11-27 02:16:16 model_runner.py:1077] Loading model weights took 16.4605 GB\r\nLoading safetensors checkpoint shards: 100% Completed | 30/30 [12:22<00:00, 31.51s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 30/30 [12:22<00:00, 24.75s/it]\r\n\r\n(VllmWorkerProcess pid=355) INFO 11-27 02:16:17 model_runner.py:1077] Loading model weights took 16.4605 GB\r\n(VllmWorkerProcess pid=356) INFO 11-27 02:16:17 model_runner.py:1077] Loading model weights took 16.4605 GB\r\nINFO 11-27 02:16:17 model_runner.py:1077] Loading model weights took 16.4605 GB\r\n(VllmWorkerProcess pid=352) INFO 11-27 02:16:17 model_runner.py:1077] Loading model weights took 16.4605 GB\r\n(VllmWorkerProcess pid=350) INFO 11-27 02:16:17 model_runner.py:1077] Loading model weights took 16.4605 GB\r\n(VllmWorkerProcess pid=353) INFO 11-27 02:16:17 model_runner.py:1077] Loading model weights took 16.4605 GB\r\n(VllmWorkerProcess pid=351) INFO 11-27 02:16:17 model_runner.py:1077] Loading model weights took 16.4605 GB\r\n(VllmWorkerProcess pid=350) INFO 11-27 02:16:23 worker.py:232] Memory profiling results: total_gpu_memory=139.72GiB initial_memory_usage=19.65GiB peak_torch_memory=16.53GiB memory_usage_post_profile=22.18GiB non_torch_memory=5.68GiB kv_cache_size=110.52GiB gpu_memory_utilization=0.95\r\n(VllmWorkerProcess pid=351) INFO 11-27 02:16:23 worker.py:232] Memory profiling results: total_gpu_memory=139.72GiB initial_memory_usage=19.65GiB peak_torch_memory=16.53GiB memory_usage_post_profile=22.18GiB non_torch_memory=5.68GiB kv_cache_size=110.52GiB gpu_memory_utilization=0.95\r\n(VllmWorkerProcess pid=354) INFO 11-27 02:16:23 worker.py:232] Memory profiling results: total_gpu_memory=139.72GiB initial_memory_usage=19.65GiB peak_torch_memory=16.53GiB memory_usage_post_profile=22.18GiB non_torch_memory=5.68GiB kv_cache_size=110.52GiB gpu_memory_utilization=0.95\r\n(VllmWorkerProcess pid=356) INFO 11-27 02:16:23 worker.py:232] Memory profiling results: total_gpu_memory=139.72GiB initial_memory_usage=19.42GiB peak_torch_memory=16.53GiB memory_usage_post_profile=21.71GiB non_torch_memory=5.21GiB kv_cache_size=110.99GiB gpu_memory_utilization=0.95\r\n(VllmWorkerProcess pid=353) INFO 11-27 02:16:23 worker.py:232] Memory profiling results: total_gpu_memory=139.72GiB initial_memory_usage=19.65GiB peak_torch_memory=16.53GiB memory_usage_post_profile=22.18GiB non_torch_memory=5.68GiB kv_cache_size=110.52GiB gpu_memory_utilization=0.95\r\n(VllmWorkerProcess pid=352) INFO 11-27 02:16:23 worker.py:232] Memory profiling results: total_gpu_memory=139.72GiB initial_memory_usage=19.65GiB peak_torch_memory=16.53GiB memory_usage_post_profile=22.18GiB non_torch_memory=5.68GiB kv_cache_size=110.52GiB gpu_memory_utilization=0.95\r\n(VllmWorkerProcess pid=355) INFO 11-27 02:16:23 worker.py:232] Memory profiling results: total_gpu_memory=139.72GiB initial_memory_usage=19.65GiB peak_torch_memory=16.53GiB memory_usage_post_profile=22.18GiB non_torch_memory=5.68GiB kv_cache_size=110.52GiB gpu_memory_utilization=0.95\r\nINFO 11-27 02:16:23 worker.py:232] Memory profiling results: total_gpu_memory=139.72GiB initial_memory_usage=19.60GiB peak_torch_memory=17.67GiB memory_usage_post_profile=24.21GiB non_torch_memory=7.71GiB kv_cache_size=107.35GiB gpu_memory_utilization=0.95\r\nINFO 11-27 02:16:23 distributed_gpu_executor.py:57] # GPU blocks: 175884, # CPU blocks: 6553\r\nINFO 11-27 02:16:23 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 21.47x\r\n(VllmWorkerProcess pid=350) INFO 11-27 02:16:26 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n(VllmWorkerProcess pid=350) INFO 11-27 02:16:26 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\nINFO 11-27 02:16:26 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 11-27 02:16:26 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(VllmWorkerProcess pid=352) INFO 11-27 02:16:26 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n(VllmWorkerProcess pid=352) INFO 11-27 02:16:26 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(VllmWorkerProcess pid=351) INFO 11-27 02:16:26 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n(VllmWorkerProcess pid=351) INFO 11-27 02:16:26 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(VllmWorkerProcess pid=355) INFO 11-27 02:16:26 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n(VllmWorkerProcess pid=355) INFO 11-27 02:16:26 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(VllmWorkerProcess pid=356) INFO 11-27 02:16:26 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n(VllmWorkerProcess pid=356) INFO 11-27 02:16:26 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(VllmWorkerProcess pid=353) INFO 11-27 02:16:26 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n(VllmWorkerProcess pid=354) INFO 11-27 02:16:26 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n(VllmWorkerProcess pid=353) INFO 11-27 02:16:26 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(VllmWorkerProcess pid=354) INFO 11-27 02:16:26 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(VllmWorkerProcess pid=351) INFO 11-27 02:16:38 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\r\n(VllmWorkerProcess pid=354) INFO 11-27 02:16:38 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\r\n(VllmWorkerProcess pid=353) INFO 11-27 02:16:38 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\r\n(VllmWorkerProcess pid=356) INFO 11-27 02:16:38 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\r\n(VllmWorkerProcess pid=352) INFO 11-27 02:16:38 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\r\n(VllmWorkerProcess pid=355) INFO 11-27 02:16:38 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\r\nINFO 11-27 02:16:38 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\r\n(VllmWorkerProcess pid=350) INFO 11-27 02:16:38 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\r\n(VllmWorkerProcess pid=352) INFO 11-27 02:16:39 model_runner.py:1518] Graph capturing finished in 13 secs, took 0.48 GiB\r\n(VllmWorkerProcess pid=354) INFO 11-27 02:16:39 model_runner.py:1518] Graph capturing finished in 13 secs, took 0.48 GiB\r\n(VllmWorkerProcess pid=355) INFO 11-27 02:16:39 model_runner.py:1518] Graph capturing finished in 13 secs, took 0.48 GiB\r\n(VllmWorkerProcess pid=356) INFO 11-27 02:16:39 model_runner.py:1518] Graph capturing finished in 13 secs, took 0.48 GiB\r\n(VllmWorkerProcess pid=351) INFO 11-27 02:16:39 model_runner.py:1518] Graph capturing finished in 13 secs, took 0.48 GiB\r\n(VllmWorkerProcess pid=350) INFO 11-27 02:16:39 model_runner.py:1518] Graph capturing finished in 13 secs, took 0.48 GiB\r\n(VllmWorkerProcess pid=353) INFO 11-27 02:16:39 model_runner.py:1518] Graph capturing finished in 13 secs, took 0.48 GiB\r\nINFO 11-27 02:16:39 model_runner.py:1518] Graph capturing finished in 13 secs, took 0.48 GiB\r\n```\r\n\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\n- k8s yaml\r\n```\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: vll-model-dep\r\n  namespace: default\r\n  labels:\r\n    app: vll-model-dep\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      app: vll-model-dep\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: vll-model-dep\r\n    spec:\r\n      nodeSelector:\r\n        kubernetes.io/hostname: k8s-node8\r\n      volumes:\r\n      - name: cache-volume\r\n        hostPath:\r\n            path: /mnt/data01/models_down/Meta-Llama-3.1-70B-Instruct\r\n      # vLLM needs to access the host's shared memory for tensor parallel inference.\r\n      - name: shm\r\n        emptyDir:\r\n          medium: Memory\r\n          sizeLimit: \"10Gi\"\r\n      containers:\r\n      - name: vll-model-dep\r\n        image: vllm/vllm-openai:latest\r\n        command: [\"/bin/sh\", \"-c\"]\r\n        args: [\r\n          \"vllm serve /models/Meta-Llama-3.1-70B-Instruct -tp 8 --gpu-memory-utilization 0.95 --served-model-name deploy_model\"\r\n        ]\r\n        ports:\r\n        - containerPort: 8000\r\n        resources:\r\n          limits:\r\n            #cpu: \"16\"\r\n            #memory: 32G\r\n            nvidia.com/gpu: \"8\"\r\n          requests:\r\n            #cpu: \"16\"\r\n            #memory: 32G\r\n            nvidia.com/gpu: \"8\"\r\n        volumeMounts:\r\n        - mountPath: /models/Meta-Llama-3.1-70B-Instruct\r\n          name: cache-volume\r\n        - name: shm\r\n          mountPath: /dev/shm\r\n        readinessProbe:\r\n          httpGet:\r\n            path: /health\r\n            port: 8000\r\n          initialDelaySeconds: 30\r\n          periodSeconds: 5\r\n```\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-11-27T09:57:48+00:00",
    "closed_at": "2024-12-04T10:27:19+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10702/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10702"
  },
  {
    "number": 8500,
    "title": "[Bug]: vllm-cpu docker gguf: AttributeError: '_OpNamespace' '_C' object has no attribute 'ggml_dequantize'",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nINFO 09-16 04:16:36 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\r\nPyTorch version: 2.4.0+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.3\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.8.0-1015-aws-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               48\r\nOn-line CPU(s) list:                  0-47\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD EPYC 7R32\r\nCPU family:                           23\r\nModel:                                49\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   24\r\nSocket(s):                            1\r\nStepping:                             0\r\nBogoMIPS:                             5600.00\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save rdpid\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            768 KiB (24 instances)\r\nL1i cache:                            768 KiB (24 instances)\r\nL2 cache:                             12 MiB (24 instances)\r\nL3 cache:                             96 MiB (6 instances)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-47\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec rstack overflow:   Vulnerable: Safe RET, no microcode\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] intel_extension_for_pytorch==2.4.0+gitfbaa4bc\r\n[pip3] numpy==1.26.4\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0+cpu\r\n[pip3] torchvision==0.19.0+cpu\r\n[pip3] transformers==4.44.2\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.1.post2@fc990f97958636ce25e4471acfd5651b096b0311\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nFirst, I followed [this instruction](https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#cpu-backend-quick-start-dockerfile) and built my docker image.\r\nThen I started my container with the below `docker-compose.yml` file.\r\n\r\n```yaml\r\nservices:\r\n  llm-vllm-dev:\r\n    image: vllm/vllm-openai:cpu\r\n    container_name: llm-vllm-dev\r\n    restart: unless-stopped\r\n    environment:\r\n      HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN}\r\n    ports:\r\n      - \"8007:8007\"\r\n    deploy:\r\n      resources:\r\n        limits:\r\n          cpus: \"24\"\r\n          memory: 32GB\r\n\r\n    ipc: host\r\n    volumes:\r\n      - ~/.cache/huggingface:/root/.cache/huggingface\r\n      - ./models:/models\r\n    networks:\r\n      - ai-assistant\r\n    command: >\r\n      --host 0.0.0.0\r\n      --port 8007\r\n      --api-key <my-api-key>\r\n      --max-model-len 4096\r\n      --tensor-parallel-size 1\r\n      --served-model-name gpt-4o\r\n      --seed 42\r\n      --disable-log-requests\r\n      --quantization gguf\r\n      --model /models/Llama-3.1-Storm-8B.Q4_K_M.gguf\r\n\r\nnetworks:\r\n  ai-assistant:\r\n    external: true\r\n```\r\n\r\nMy running script:\r\n\r\n```python\r\nimport openai\r\n\r\nBASE_URL=\"http://localhost:8007/v1\" # port 8000 or 8005\r\nAPI_KEY=<my-key>\r\n\r\nopenai_client = openai.OpenAI(\r\n    base_url=BASE_URL,\r\n    api_key=API_KEY\r\n)\r\nchat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\r\n\r\ncompletion = openai_client.chat.completions.create(\r\n    model=\"gpt-4o\",\r\n    messages=[\r\n        {\"role\": \"user\", \"content\": \"Hello\"}\r\n    ],\r\n    temperature=0.0,\r\n    n=1,\r\n    seed=42,\r\n    max_tokens=2048,\r\n    extra_body={\r\n        \"chat_template\": chat_template\r\n    },\r\n)\r\n\r\nprint(completion.choices[0].message.content)\r\n```\r\n\r\nThe I got the error:\r\n\r\n```shell\r\nllm-vllm-dev  | INFO 09-16 04:20:27 server.py:228] vLLM ZMQ RPC Server was interrupted.\r\nllm-vllm-dev  | Future exception was never retrieved\r\nllm-vllm-dev  | future: <Future finished exception=AttributeError(\"'_OpNamespace' '_C' object has no attribute 'ggml_dequantize'\")>\r\nllm-vllm-dev  | Traceback (most recent call last):\r\nllm-vllm-dev  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/server.py\", line 115, in generate\r\nllm-vllm-dev  |     async for request_output in results_generator:\r\nllm-vllm-dev  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 859, in generate\r\nllm-vllm-dev  |     async for output in await self.add_request(\r\nllm-vllm-dev  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 106, in generator\r\nllm-vllm-dev  |     raise result\r\nllm-vllm-dev  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 48, in _log_task_completion\r\nllm-vllm-dev  |     return_value = task.result()\r\nllm-vllm-dev  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 733, in run_engine_loop\r\nllm-vllm-dev  |     result = task.result()\r\nllm-vllm-dev  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 673, in engine_step\r\nllm-vllm-dev  |     request_outputs = await self.engine.step_async(virtual_engine)\r\nllm-vllm-dev  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 340, in step_async\r\nllm-vllm-dev  |     outputs = await self.model_executor.execute_model_async(\r\nllm-vllm-dev  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/cpu_executor.py\", line 314, in execute_model_async\r\nllm-vllm-dev  |     output = await make_async(self.execute_model\r\nllm-vllm-dev  |   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\nllm-vllm-dev  |     result = self.fn(*self.args, **self.kwargs)\r\nllm-vllm-dev  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/cpu_executor.py\", line 226, in execute_model\r\nllm-vllm-dev  |     output = self.driver_method_invoker(self.driver_worker,\r\nllm-vllm-dev  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/cpu_executor.py\", line 380, in _async_driver_method_invoker\r\nllm-vllm-dev  |     return driver.execute_method(method, *args, **kwargs).get()\r\nllm-vllm-dev  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 58, in get\r\nllm-vllm-dev  |     raise self.result.exception\r\nllm-vllm-dev  | AttributeError: '_OpNamespace' '_C' object has no attribute 'ggml_dequantize'\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-09-16T04:21:46+00:00",
    "closed_at": "2024-09-17T01:26:22+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8500/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8500"
  },
  {
    "number": 11345,
    "title": "[Performance]: 1P1D Disaggregation performance",
    "body": "### Proposal to improve performance\r\n\r\nI try to reproduce the P&D 1P1D benchmark to compare performance with chunked prefill  https://github.com/vllm-project/vllm/blob/main/benchmarks/disagg_benchmarks/disagg_performance_benchmark.sh. TTFL is higher than what I expected. Because the overhead benchmark only shows ~20-30ms level. What's more, seems ITL is also much higher than chunked prefill. \r\n\r\n- GPU device: 2* L40S.\r\n- Model: Qwen/Qwen2.5-7B-Instruct\r\n- Parameters: gpu-memory-utilization 0.6 + kv_buffer_size 10e9 \r\n- dataset input 1024 output 50.\r\n\r\n/cc @KuntaiDu \r\n\r\n\r\n### Report of performance regression\r\n\r\n![image](https://github.com/user-attachments/assets/2c5ec50f-1e5b-48c6-aca2-ab0be42935ed)\r\n\r\n### Misc discussion on performance\r\n\r\n_No response_\r\n\r\n### Your current environment (if you think it is necessary)\r\n\r\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance"
    ],
    "state": "open",
    "created_at": "2024-12-19T18:37:57+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11345/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11345"
  },
  {
    "number": 11807,
    "title": "[Bug]: AttributeError: 'Int8Params' object has no attribute 'bnb_shard_offsets', It seems that vllm's bnb prequantification support for cls models is not yet complete.",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\n\r\n\r\n It seems that vllm's bnb prequantification support for cls models is not yet complete.\r\n\r\nThis problem occurs only in the score layer of any bnb format cls model.\r\n\r\n\r\n```python\r\noutputs = llm.classify(\r\n    all_prompts,\r\n    use_tqdm=True,\r\n)\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n```python\r\nProcessed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-11-d3f2b71f59e1> in <cell line: 7>()\r\n      5 \r\n      6 all_prompts = [TokensPrompt(prompt_token_ids=ids) for ids in data[\"input_ids\"].values]\r\n----> 7 outputs = llm.classify(\r\n      8     all_prompts,\r\n      9     lora_request=LoRARequest(\"lora_adapter\", 1, WEIGHTS_PATH),\r\n\r\n/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py in classify(self, prompts, use_tqdm, lora_request, prompt_adapter_request)\r\n    946                 \"Classification API is only enabled for `--task classify`\")\r\n    947 \r\n--> 948         items = self.encode(prompts,\r\n    949                             use_tqdm=use_tqdm,\r\n    950                             lora_request=lora_request,\r\n\r\n/usr/local/lib/python3.10/dist-packages/vllm/utils.py in inner(*args, **kwargs)\r\n   1019                     )\r\n   1020 \r\n-> 1021             return fn(*args, **kwargs)\r\n   1022 \r\n   1023         return inner  # type: ignore\r\n\r\n/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py in encode(self, prompts, pooling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request)\r\n    869         )\r\n    870 \r\n--> 871         outputs = self._run_engine(use_tqdm=use_tqdm)\r\n    872         return self.engine_class.validate_outputs(outputs,\r\n    873                                                   PoolingRequestOutput)\r\n\r\n/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py in _run_engine(self, use_tqdm)\r\n   1240         total_out_toks = 0\r\n   1241         while self.llm_engine.has_unfinished_requests():\r\n-> 1242             step_outputs = self.llm_engine.step()\r\n   1243             for output in step_outputs:\r\n   1244                 if output.finished:\r\n\r\n/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py in step(self)\r\n   1388                     virtual_engine]\r\n   1389 \r\n-> 1390             outputs = self.model_executor.execute_model(\r\n   1391                 execute_model_req=execute_model_req)\r\n   1392 \r\n\r\n/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py in execute_model(self, execute_model_req)\r\n     86         self, execute_model_req: ExecuteModelRequest\r\n     87     ) -> Optional[List[Union[SamplerOutput, PoolerOutput]]]:\r\n---> 88         output = self.driver_worker.execute_model(execute_model_req)\r\n     89         return output\r\n     90 \r\n\r\n/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py in execute_model(self, execute_model_req)\r\n    341                     \"model_execute_time\", torch.tensor(0)).item()\r\n    342 \r\n--> 343         output = self.model_runner.execute_model(\r\n    344             model_input=model_input,\r\n    345             kv_caches=self.kv_cache[worker_input.virtual_engine]\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py in decorate_context(*args, **kwargs)\r\n    114     def decorate_context(*args, **kwargs):\r\n    115         with ctx_factory():\r\n--> 116             return func(*args, **kwargs)\r\n    117 \r\n    118     return decorate_context\r\n\r\n/usr/local/lib/python3.10/dist-packages/vllm/worker/pooling_model_runner.py in execute_model(self, model_input, kv_caches, intermediate_tensors, num_steps)\r\n    107 \r\n    108         with set_forward_context(model_input.attn_metadata, self.vllm_config):\r\n--> 109             hidden_or_intermediate_states = model_executable(\r\n    110                 input_ids=model_input.input_tokens,\r\n    111                 positions=model_input.input_positions,\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _wrapped_call_impl(self, *args, **kwargs)\r\n   1734             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n   1735         else:\r\n-> 1736             return self._call_impl(*args, **kwargs)\r\n   1737 \r\n   1738     # torchrec tests the code consistency with the following code\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)\r\n   1745                 or _global_backward_pre_hooks or _global_backward_hooks\r\n   1746                 or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1747             return forward_call(*args, **kwargs)\r\n   1748 \r\n   1749         result = None\r\n\r\n/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/adapters.py in forward(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors, inputs_embeds)\r\n    209                                             intermediate_tensors,\r\n    210                                             inputs_embeds)\r\n--> 211             logits, _ = self.score(hidden_states)\r\n    212             return logits\r\n    213 \r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _wrapped_call_impl(self, *args, **kwargs)\r\n   1734             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n   1735         else:\r\n-> 1736             return self._call_impl(*args, **kwargs)\r\n   1737 \r\n   1738     # torchrec tests the code consistency with the following code\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)\r\n   1745                 or _global_backward_pre_hooks or _global_backward_hooks\r\n   1746                 or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1747             return forward_call(*args, **kwargs)\r\n   1748 \r\n   1749         result = None\r\n\r\n/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py in forward(self, input_)\r\n   1113         # bias will not get added more than once in TP>1 case)\r\n   1114         bias_ = None if (self.tp_rank > 0 or self.skip_bias_add) else self.bias\r\n-> 1115         output_parallel = self.quant_method.apply(self,\r\n   1116                                                   input_parallel,\r\n   1117                                                   bias=bias_)\r\n\r\n/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/bitsandbytes.py in apply(self, layer, x, bias)\r\n    223 \r\n    224         if self.quant_config.load_in_8bit:\r\n--> 225             return self._apply_8bit_weight(layer, x, bias)\r\n    226         else:\r\n    227             return self._apply_4bit_weight(layer, x, bias)\r\n\r\n/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/bitsandbytes.py in _apply_8bit_weight(self, layer, x, bias)\r\n    245 \r\n    246         qweight = layer.weight\r\n--> 247         offsets = qweight.bnb_shard_offsets\r\n    248         quant_states = qweight.bnb_quant_state\r\n    249         matmul_states = qweight.matmul_state\r\n\r\nAttributeError: 'Int8Params' object has no attribute 'bnb_shard_offsets'\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-07T12:59:06+00:00",
    "closed_at": "2025-06-01T02:14:40+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11807/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11807"
  },
  {
    "number": 15538,
    "title": "[Usage]: How to get \"num_gpu_blocks\" in V1\uff1f",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\n```\n\n\n### How would you like to use vllm\n\nIn V0, I can get \"num_gpu_blocks\" through \"llm.llm_engine.cache_config.num_gpu_blocks\". \n\nBut in V1, LLM and EngineCore are in different processes. How can I get \"num_gpu_blocks\"?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "help wanted",
      "good first issue",
      "usage"
    ],
    "state": "open",
    "created_at": "2025-03-26T09:40:16+00:00",
    "closed_at": null,
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15538/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15538"
  },
  {
    "number": 13929,
    "title": "[Bug]: mllama AssertionError during kv cache profiling",
    "body": "### Your current environment\n\nRepro command below.\n\n### \ud83d\udc1b Describe the bug\n\nAttempting to serve `meta-llama/Llama-3.2-11B-Vision-Instruct` with recent vLLM (>=v0.7.3), results in the error below during the execution of `determine_num_available_blocks()` during bootup\n\n```\n$ vllm serve meta-llama/Llama-3.2-11B-Vision-Instruct --max-num-seqs 8\n```\n\n```\nTraceback (most recent call last):\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 400, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 125, in from_engine_args\n    return cls(ipc_path=ipc_path,\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 77, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/engine/llm_engine.py\", line 277, in __init__\n    self._initialize_kv_caches()\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/engine/llm_engine.py\", line 426, in _initialize_kv_caches\n    self.model_executor.determine_num_available_blocks())\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/executor/executor_base.py\", line 102, in determine_num_available_blocks\n    results = self.collective_rpc(\"determine_num_available_blocks\")\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/executor/executor_base.py\", line 316, in collective_rpc\n    return self._run_workers(method, *args, **(kwargs or {}))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/executor/mp_distributed_executor.py\", line 185, in _run_workers\n    driver_worker_output = run_method(self.driver_worker, sent_method,\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/utils.py\", line 2196, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/worker/worker.py\", line 229, in determine_num_available_blocks\n    self.model_runner.profile_run()\n  File \"/opt/vllm/lib64/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/worker/enc_dec_model_runner.py\", line 341, in profile_run\n    self.execute_model(model_input, kv_caches, intermediate_tensors)\n  File \"/opt/vllm/lib64/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/worker/enc_dec_model_runner.py\", line 182, in execute_model\n    hidden_or_intermediate_states = model_executable(\n                                    ^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/model_executor/models/mllama.py\", line 1392, in forward\n    assert actual_len >= last_group_len\n```\n\nI have done some investigations, but do not have a fix yet... Here is what I have found:\n- the error occurs because the dummy encoder sequences constructed for profiling are longer than the actual encoder len computed in mllama; for the single-image requests, this means greater than 6404 tokens\n- serving the model works as long as `max_seq_len / max_num_seqs <= 6404`; with the full seq length `--max-num-seq=21` works\n- I think this bug was introduced in https://github.com/vllm-project/vllm/pull/11427\n    - before this PR there was a `dummy_encoder_data_for_mllama` function responsible for constructing the dummy data\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-02-26T22:19:48+00:00",
    "closed_at": "2025-04-07T04:07:16+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13929/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/13929"
  },
  {
    "number": 10116,
    "title": "[Bug]: vllm0.6.3.post1  7B model can not use cmd vllm.entrypoints.openai.api_server on wsl",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 24.04.1 LTS (x86_64)\r\nGCC version: (Ubuntu 13.2.0-23ubuntu4) 13.2.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.39\r\n\r\nPython version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.39\r\nIs CUDA available: True\r\nCUDA runtime version: 12.6.77\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090\r\nNvidia driver version: 560.94\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.5.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.5.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.5.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.5.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.5.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.5.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.5.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.5.1\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             32\r\nOn-line CPU(s) list:                0-31\r\nVendor ID:                          GenuineIntel\r\nModel name:                         13th Gen Intel(R) Core(TM) i9-13900KS\r\nCPU family:                         6\r\nModel:                              183\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 16\r\nSocket(s):                          1\r\nStepping:                           1\r\nBogoMIPS:                           6374.39\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves avx_vnni umip waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nHypervisor vendor:                  Microsoft\r\nVirtualization type:                full\r\nL1d cache:                          768 KiB (16 instances)\r\nL1i cache:                          512 KiB (16 instances)\r\nL2 cache:                           32 MiB (16 instances)\r\nL3 cache:                           36 MiB (1 instance)\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Mitigation; Enhanced IBRS\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==7.0.0\r\n[pip3] mypy==1.11.2\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] numpydoc==1.7.0\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.77\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==25.1.2\r\n[pip3] torch==2.4.0\r\n[pip3] torchaudio==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.46.1\r\n[pip3] triton==3.0.0\r\n[conda] _anaconda_depends         2024.10             py312_mkl_0  \r\n[conda] blas                      1.0                         mkl  \r\n[conda] cuda-cudart               12.4.127                      0    nvidia\r\n[conda] cuda-cupti                12.4.127                      0    nvidia\r\n[conda] cuda-libraries            12.4.1                        0    nvidia\r\n[conda] cuda-nvrtc                12.4.127                      0    nvidia\r\n[conda] cuda-nvtx                 12.4.127                      0    nvidia\r\n[conda] cuda-opencl               12.6.77                       0    nvidia\r\n[conda] cuda-runtime              12.4.1                        0    nvidia\r\n[conda] cuda-version              12.6                          3    nvidia\r\n[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\r\n[conda] libcublas                 12.4.5.8                      0    nvidia\r\n[conda] libcufft                  11.2.1.3                      0    nvidia\r\n[conda] libcufile                 1.11.1.6                      0    nvidia\r\n[conda] libcurand                 10.3.7.77                     0    nvidia\r\n[conda] libcusolver               11.6.1.9                      0    nvidia\r\n[conda] libcusparse               12.3.1.170                    0    nvidia\r\n[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\r\n[conda] libnpp                    12.2.5.30                     0    nvidia\r\n[conda] libnvfatbin               12.6.77                       0    nvidia\r\n[conda] libnvjitlink              12.4.127                      0    nvidia\r\n[conda] libnvjpeg                 12.3.1.117                    0    nvidia\r\n[conda] mkl                       2023.1.0         h213fc3f_46344  \r\n[conda] mkl-service               2.4.0           py312h5eee18b_1  \r\n[conda] mkl_fft                   1.3.10          py312h5eee18b_0  \r\n[conda] mkl_random                1.2.7           py312h526ad5a_0  \r\n[conda] numpy                     1.26.4          py312hc5e2394_0  \r\n[conda] numpy-base                1.26.4          py312h0da6c21_0  \r\n[conda] numpydoc                  1.7.0           py312h06a4308_0  \r\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.6.77                  pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\r\n[conda] pytorch                   2.4.0           py3.12_cuda12.4_cudnn9.1.0_0    pytorch\r\n[conda] pytorch-cuda              12.4                 hc786d27_7    pytorch\r\n[conda] pytorch-mutex             1.0                        cuda    pytorch\r\n[conda] pyzmq                     25.1.2          py312h6a678d5_0  \r\n[conda] torchaudio                2.4.0               py312_cu124    pytorch\r\n[conda] torchtriton               3.0.0                     py312    pytorch\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.46.1                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.3.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X                              N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\nINFO 11-07 17:49:14 api_server.py:530] vLLM API server version 0.6.3.post1\r\nINFO 11-07 17:49:14 api_server.py:531] args: Namespace(host='0.0.0.0', port=46000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/mnt/d/Users/Admin/.cache/modelscope/hub/Qwen/Qwen2___5-7B-Instruct', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=42, max_logprobs=20, disable_log_stats=True, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['/mnt/d/Users/Admin/.cache/modelscope/hub/Qwen/Qwen2___5-7B-Instruct'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False)\r\nINFO 11-07 17:49:14 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/3302193f-fc34-4158-b198-700ff1abbc02 for IPC Path.\r\nINFO 11-07 17:49:14 api_server.py:179] Started engine process with PID 40888\r\nWARNING 11-07 17:49:16 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\r\nWARNING 11-07 17:49:16 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\nWARNING 11-07 17:49:18 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\r\nWARNING 11-07 17:49:18 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\nINFO 11-07 17:49:18 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/mnt/d/Users/Admin/.cache/modelscope/hub/Qwen/Qwen2___5-7B-Instruct', speculative_config=None, tokenizer='/mnt/d/Users/Admin/.cache/modelscope/hub/Qwen/Qwen2___5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/mnt/d/Users/Admin/.cache/modelscope/hub/Qwen/Qwen2___5-7B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\r\nWARNING 11-07 17:49:19 utils.py:772] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\r\nINFO 11-07 17:49:19 model_runner.py:1056] Starting to load model /mnt/d/Users/Admin/.cache/modelscope/hub/Qwen/Qwen2___5-7B-Instruct...\r\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [01:04<03:12, 64.03s/it]\r\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [02:10<02:10, 65.39s/it]\r\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [03:13<01:04, 64.43s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [04:09<00:00, 60.89s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [04:09<00:00, 62.28s/it]\r\n\r\nINFO 11-07 17:53:29 model_runner.py:1067] Loading model weights took 14.2487 GB\r\nINFO 11-07 17:53:33 gpu_executor.py:122] # GPU blocks: 4025, # CPU blocks: 4681\r\nINFO 11-07 17:53:33 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 1.97x\r\nINFO 11-07 17:53:33 api_server.py:232] vLLM to use /tmp/tmpexa4ks3t as PROMETHEUS_MULTIPROC_DIR\r\nWARNING 11-07 17:53:33 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\r\nINFO 11-07 17:53:33 launcher.py:19] Available routes are:\r\nINFO 11-07 17:53:33 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\r\nINFO 11-07 17:53:33 launcher.py:27] Route: /docs, Methods: GET, HEAD\r\nINFO 11-07 17:53:33 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\r\nINFO 11-07 17:53:33 launcher.py:27] Route: /redoc, Methods: GET, HEAD\r\nINFO 11-07 17:53:33 launcher.py:27] Route: /health, Methods: GET\r\nINFO 11-07 17:53:33 launcher.py:27] Route: /tokenize, Methods: POST\r\nINFO 11-07 17:53:33 launcher.py:27] Route: /detokenize, Methods: POST\r\nINFO 11-07 17:53:33 launcher.py:27] Route: /v1/models, Methods: GET\r\nINFO 11-07 17:53:33 launcher.py:27] Route: /version, Methods: GET\r\nINFO 11-07 17:53:33 launcher.py:27] Route: /, Methods: GET\r\nINFO 11-07 17:53:33 launcher.py:27] Route: /v1/chat/completions, Methods: POST\r\nINFO 11-07 17:53:33 launcher.py:27] Route: /v1/completions, Methods: POST\r\nINFO 11-07 17:53:33 launcher.py:27] Route: /v1/embeddings, Methods: POST\r\nINFO:     Started server process [40874]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on socket ('0.0.0.0', 46000) (Press CTRL+C to quit)\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nuse  Qwen/Qwen2.5-0.5B-Instruct model the api can open success.\r\nif use Qwen/Qwen2.5-7B-Instruct model  the api timeout \r\nin ipynb\r\n```ipynb\r\n!curl \"http://localhost:{port}/version\"\r\n```\r\n```info\r\ncurl: (28) Failed to connect to localhost port 46000 after 125802 ms: Could not connect to server\r\n```\r\npython code\r\n```python\r\nfrom modelscope import snapshot_download\r\n# MODLE_PATH = snapshot_download('Qwen/Qwen2.5-0.5B-Instruct')\r\nMODLE_PATH = snapshot_download('Qwen/Qwen2.5-7B-Instruct')\r\nport = 46000\r\nimport os\r\nimport re\r\ndef close_port(port):\r\n    system = os.name\r\n\r\n    if system == 'nt':  # Windows\r\n        try:\r\n            # \u67e5\u627e\u5360\u7528\u7aef\u53e3\u7684\u8fdb\u7a0b\r\n            result = subprocess.run(['netstat', '-ano'], capture_output=True, text=True)\r\n            lines = result.stdout.split('\\n')\r\n            \r\n            pid = None\r\n            for line in lines:\r\n                if f':{port} ' in line:\r\n                    parts = line.split()\r\n                    pid = parts[-1]\r\n                    break\r\n            \r\n            if pid:\r\n                # \u7ec8\u6b62\u8fdb\u7a0b\r\n                subprocess.run(['taskkill', '/PID', pid, '/F'])\r\n                print(f\"Terminated process with PID {pid} on port {port}\")\r\n            else:\r\n                print(f\"No process found using port {port}\")\r\n        except Exception as e:\r\n            print(f\"Error closing port {port}: {e}\")\r\n    \r\n    elif system == 'posix':  # Linux and macOS\r\n        try:\r\n            # \u67e5\u627e\u5360\u7528\u7aef\u53e3\u7684\u8fdb\u7a0b\r\n            result = subprocess.run(['lsof', '-i', f':{port}'], capture_output=True, text=True)\r\n            lines = result.stdout.split('\\n')\r\n            \r\n            pid = None\r\n            for line in lines:\r\n                if 'TCP' in line or 'UDP' in line:\r\n                    parts = line.split()\r\n                    pid = parts[1]\r\n                    break\r\n            \r\n            if pid:\r\n                # \u7ec8\u6b62\u8fdb\u7a0b\r\n                subprocess.run(['kill', '-9', pid])\r\n                print(f\"Terminated process with PID {pid} on port {port}\")\r\n            else:\r\n                print(f\"No process found using port {port}\")\r\n        except Exception as e:\r\n            print(f\"Error closing port {port}: {e}\")\r\n    else:\r\n        print(f\"Unsupported operating system: {system}\")\r\n\r\nclose_port(port)\r\ndef run_command_with_timeout(cmd, timeout=600,stop_info = \"Press CTRL+C to quit\",close_log = False):\r\n    # \u542f\u52a8\u5b50\u8fdb\u7a0b\r\n    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\r\n\r\n    start_time = time.time()\r\n    found_string = False\r\n    output_line = \"\"\r\n\r\n    while True:\r\n        # \u68c0\u67e5\u662f\u5426\u8d85\u65f6\r\n        if time.time() - start_time > timeout:\r\n            print(\"Error: Command timed out after {} seconds\".format(timeout))\r\n            # \u8fd9\u91cc\u4e0d\u7ec8\u6b62\r\n            # process.terminate()  # \u7ec8\u6b62\u5b50\u8fdb\u7a0b\r\n            return None\r\n\r\n        # \u5c1d\u8bd5\u4ece stdout \u8bfb\u53d6\u4e00\u884c\u8f93\u51fa\r\n        output_line = process.stdout.readline().decode('utf-8').strip()\r\n\r\n        # \u5982\u679c\u6ca1\u6709\u66f4\u591a\u7684\u8f93\u51fa\u4e14\u8fdb\u7a0b\u5df2\u7ecf\u7ed3\u675f\uff0c\u9000\u51fa\u5faa\u73af\r\n        if not output_line and process.poll() is not None:\r\n            break\r\n\r\n        # \u6253\u5370\u8f93\u51fa\u4ee5\u4fbf\u8ddf\u8e2a\r\n        print(output_line)\r\n\r\n        # \u68c0\u67e5\u662f\u5426\u627e\u5230\u76ee\u6807\u5b57\u7b26\u4e32\r\n        if stop_info in output_line:\r\n            found_string = True\r\n            print(\"Target string found. Exiting...\")\r\n            break\r\n\r\n        # \u7b49\u5f85\u4e00\u6bb5\u65f6\u95f4\u518d\u7ee7\u7eed\u8bfb\u53d6\r\n        time.sleep(0.1)  # \u9632\u6b62CPU\u5360\u7528\u8fc7\u9ad8\r\n    # \u7528\u4e0d\u7528close_log \u90fd\u4f1a\u4e22\u5931\u8f93\u51fa\r\n    # if close_log:\r\n    # \u5173\u95ed\u5b50\u8fdb\u7a0b\u7684\u6807\u51c6\u8f93\u51fa\r\n    process.stdout.close()\r\n    # \u7b49\u5f85\u8fdb\u7a0b\u5b8c\u5168\u7ed3\u675f\r\n    # process.wait()\r\n\r\n    return found_string\r\n\r\nclass args:\r\n    # model=\"/kaggle/input/qwen2.5/transformers/1.5b-instruct/1/Qwen2.5-Math-1.5B-Instruct\"\r\n    # model=\"/kaggle/input/qwen-qwen2.5-math-7b-instruct/transformers/8b/1\"\r\n    model=MODLE_PATH\r\n    num_gpus=torch.cuda.device_count()\r\n    port=port\r\n    dtype = 'bfloat'\r\n    gpu_memory_utilization=0.95\r\n    max_num_seqs=256//6\r\n    host = \"0.0.0.0\"\r\n    temperature = 0.7\r\n    top_p = 0.8\r\n    batch_size = 128//6\r\n    tokens_to_generate = 3072\r\n    sleep_time = 250\r\n    \r\n    cmd = (\r\n    f'python -m vllm.entrypoints.openai.api_server '\r\n    f'    --model=\"{args.model}\" '\r\n    f'    --served-model-name=\"{args.model}\"'\r\n    f'    --trust-remote-code '\r\n    f'    --host={args.host} '\r\n    f'    --port={args.port} '\r\n    f'    --tensor-parallel-size={args.num_gpus} '\r\n    f'    --gpu-memory-utilization={args.gpu_memory_utilization} '\r\n    f'    --max-num-seqs={args.max_num_seqs} '\r\n    f'    --enforce-eager '\r\n    f'    --disable-log-requests '\r\n    f'    --disable-log-stats'\r\n    #f'    --dtype={args.dtype}'\r\n    #f'    {extra_arguments} '\r\n)\r\n# subprocess.Popen(cmd, shell=True)\r\nis_foud_end = run_command_with_timeout(cmd, timeout=600,stop_info = \"Press CTRL+C to quit\")\r\n```\r\nthen run !curl http://localhost:{port}/version in ipynb\r\nif 0.5b model will get {\"version\":\"0.6.3.post1\"}\r\nif 7b model will get curl: (28) Failed to connect to localhost port 46000 after 125802 ms: Could not connect to server\r\nand set max_num_seqs batch_size max_model_len are same. and the log seems is true\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.\r\n\r\n@[DarkLight1337](https://github.com/DarkLight1337)\r\nand 'Qwen/Qwen2.5-Math-7B-Instruct' model are the same",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-11-07T10:24:57+00:00",
    "closed_at": "2024-11-16T14:16:00+00:00",
    "comments": 43,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10116/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10116"
  },
  {
    "number": 8006,
    "title": "[Performance]: INT4 quantisation does not lead to any observable throughput increase ",
    "body": "### Proposal to improve performance\n\n_No response_\n\n### Report of performance regression\n\nI have been using vLLM with prefix caching to optimise inference in cases where majority of operations are pre-fills with large shared prefix. Specially, most of the prompts are 130 tokens in size with 90% of it is a shared system prompt. \r\nThe decode is only phase is only one token.\r\nThere benchmark is a 100000 prompts (`formatted_prompts` below) executed via generate:\r\n```python\r\nfrom outlines import models, generate\r\nllm = LLM(\"meta-llama/Meta-Llama-3-8B-Instruct\", enable_prefix_caching=True)\r\nsampling_params = SamplingParams(temperature=0.5, top_p=0.2, max_tokens=1)\r\nmodel = models.VLLM(llm)\r\ngenerator = generate.choice(model, [\"yes\", \"no\"])\r\npredictions = generator(formatted_prompts, sampling_params=sampling_params)\r\n``` \r\n\r\nWhen I use INT4 quantised model `neuralmagic/Meta-Llama-3-8B-Instruct-quantized.w4a16` I observe no speed-up in inference. \r\nSince my use cases highly leverages prefix caching, I would have expected at least 60% speed up driven by corresponding increase in KV-cache given a smaller model.\r\n\r\nMany thanks for your suggestions in advance\r\n\n\n### Misc discussion on performance\n\n_No response_\n\n### Your current environment (if you think it is necessary)\n\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.22.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1067-aws-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A10G\r\nNvidia driver version: 535.161.07\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               16\r\nOn-line CPU(s) list:                  0-15\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD EPYC 7R32\r\nCPU family:                           23\r\nModel:                                49\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   8\r\nSocket(s):                            1\r\nStepping:                             0\r\nBogoMIPS:                             5599.99\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save rdpid\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            256 KiB (8 instances)\r\nL1i cache:                            256 KiB (8 instances)\r\nL2 cache:                             4 MiB (8 instances)\r\nL3 cache:                             32 MiB (2 instances)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-15\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec rstack overflow:   Mitigation; safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==0.4.3\r\n[pip3] numpy==1.23.5\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] optree==0.12.1\r\n[pip3] sentence-transformers==2.7.0\r\n[pip3] torch==2.4.0\r\n[pip3] torcheval==0.0.7\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.2\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.5\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \t0-15\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance"
    ],
    "state": "closed",
    "created_at": "2024-08-29T16:57:41+00:00",
    "closed_at": "2024-08-29T19:48:37+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8006/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8006"
  },
  {
    "number": 10930,
    "title": "[Bug]: Different default value for temperature in SamplingParams and ChatCompletionRequest",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nThe default value for `temperature` in `SamplingParams` is `1.0`. https://github.com/vllm-project/vllm/blob/571da8fc431ec36427ee1034a7779b23229b015e/vllm/sampling_params.py#L176\r\n\r\nThe default value for `temperature` in `ChatCompletionRequest` is `0.7`. https://github.com/vllm-project/vllm/blob/571da8fc431ec36427ee1034a7779b23229b015e/vllm/entrypoints/openai/protocol.py#L173\r\n\r\nThis can lead to inconsistencies between online and offline inference results.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-12-05T14:18:25+00:00",
    "closed_at": "2024-12-16T08:15:41+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10930/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/10930"
  },
  {
    "number": 1853,
    "title": "Speed between gptq w4a16 and awq w4a16?",
    "body": "Hi, I am wondering the implementation of gptq w4a16(exllama) and awq w4a16(llm-awq), which is faster?\r\n\r\nIt seems the mathematical computation is similar between the two, so can these two share the same copy of cuda function?\r\n\r\nHoping for your reply, thank you",
    "labels": [],
    "state": "closed",
    "created_at": "2023-11-30T07:20:55+00:00",
    "closed_at": "2024-04-04T07:42:55+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1853/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1853"
  },
  {
    "number": 5862,
    "title": "[Usage]: Can I get the streaming output when using offline inference?",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### How would you like to use vllm\n\nI want to get the streaming output when using offline inference. But I can't find the 'stream' switch.\r\n",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-06-26T09:08:45+00:00",
    "closed_at": "2024-07-02T01:34:42+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5862/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/5862"
  },
  {
    "number": 11600,
    "title": "[Usage]: Parameters for improving throughput of deepseek v3 ",
    "body": "### Your current environment\r\n\r\n```text\r\nThe output of `python collect_env.py`\r\n```\r\nCollecting environment information...\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-130-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H200\r\nGPU 1: NVIDIA H200\r\nGPU 2: NVIDIA H200\r\nGPU 3: NVIDIA H200\r\nGPU 4: NVIDIA H200\r\nGPU 5: NVIDIA H200\r\nGPU 6: NVIDIA H200\r\nGPU 7: NVIDIA H200\r\n\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        46 bits physical, 57 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               192\r\nOn-line CPU(s) list:                  0-191\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Xeon(R) Platinum 8468\r\nCPU family:                           6\r\nModel:                                143\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   48\r\nSocket(s):                            2\r\nStepping:                             8\r\nFrequency boost:                      enabled\r\nCPU max MHz:                          2101.0000\r\nCPU min MHz:                          800.0000\r\nBogoMIPS:                             4200.00\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                       VT-x\r\nL1d cache:                            4.5 MiB (96 instances)\r\nL1i cache:                            3 MiB (96 instances)\r\nL2 cache:                             192 MiB (96 instances)\r\nL3 cache:                             210 MiB (2 instances)\r\nNUMA node(s):                         2\r\nNUMA node0 CPU(s):                    0-47,96-143\r\nNUMA node1 CPU(s):                    48-95,144-191\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.6+cu121torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.47.1\r\n[pip3] triton==3.1.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.6\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10   NIC11   NIC12   NIC13   NIC14   NIC15   NIC16    NIC17   CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      SYS     SYS     0-47,96-143     0               N/A\r\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS      SYS     SYS     0-47,96-143     0               N/A\r\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS      SYS     SYS     0-47,96-143     0               N/A\r\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS      SYS     SYS     0-47,96-143     0               N/A\r\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS      SYS     SYS     48-95,144-191   1               N/A\r\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      SYS     SYS     48-95,144-191   1               N/A\r\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      PIX     SYS     48-95,144-191   1               N/A\r\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      SYS     PIX     48-95,144-191   1               N/A\r\nNIC0    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      SYS     SYS\r\nNIC1    SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS      SYS     SYS\r\nNIC2    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS      SYS     SYS\r\nNIC3    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS      SYS     SYS\r\nNIC4    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS      SYS     SYS\r\nNIC5    SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      SYS     SYS\r\nNIC6    SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      PIX     SYS\r\nNIC7    SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      SYS     PIX\r\nNIC8    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS      SYS     SYS\r\nNIC9    SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS      SYS     SYS\r\nNIC10   SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     SYS     SYS     SYS     SYS      SYS     SYS\r\nNIC11   SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      SYS     SYS     SYS     SYS      SYS     SYS\r\nNIC12   SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS      SYS     SYS\r\nNIC13   SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS      SYS     SYS\r\nNIC14   SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS      SYS     SYS\r\nNIC15   SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X       SYS     SYS\r\nNIC16   SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       X      SYS\r\nNIC17   SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  NIC8: mlx5_8\r\n  NIC9: mlx5_9\r\n  NIC10: mlx5_10\r\n  NIC11: mlx5_11\r\n  NIC12: mlx5_12\r\n  NIC13: mlx5_13\r\n  NIC14: mlx5_14\r\n  NIC15: mlx5_15\r\n  NIC16: mlx5_16\r\n  NIC17: mlx5_17\r\n\r\nNVIDIA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\r\nNVIDIA_REQUIRE_CUDA=cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\r\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\r\nVLLM_USAGE_SOURCE=production-docker-image\r\nCUDA_VERSION=12.1.0\r\nLD_LIBRARY_PATH=/usr/local/lib/python3.12/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nCUDA_MODULE_LOADING=LAZY\r\n\r\n\r\n\r\n### How would you like to use vllm\r\n\r\nI want to run inference of a [specific model](deepseek v3). \r\nI deploy deepseek v3 with vllm/vllm-openai:v0.6.6 on 8*H200 locally\uff0cthe deploy params bellow\uff1a\r\n```\r\n- '--model'\r\n- /data/model/DeepSeek-V3\r\n- '--max-model-len'\r\n- '4096'\r\n- '--tensor-parallel-size'\r\n- '8'\r\n- '--served-model-name'\r\n- deepseek-v3\r\n- '--trust-remote-code'\r\n ```\r\n\r\nthe generation throughout is round max 10 tokens/s, is there any param that I should set so that could improve the thought? or the current speed is normal in current?\r\nthanks~\r\n\r\n![image](https://github.com/user-attachments/assets/0ba1217c-39f9-4901-bfa8-7af89c549b9b)\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-29T09:02:28+00:00",
    "closed_at": "2025-05-06T02:09:21+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11600/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11600"
  },
  {
    "number": 7439,
    "title": "[Feature]: CI - Split up \"Models Test\" and \"Vision Language Models Test\"",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\r\n\r\nTakes 1 hour+ on CI compared to others, which take <~30 min. Thus, ends up being a bottleneck\r\n\r\nSo, should be split up similar to kernels\r\n\r\nCC: @khluu \r\n",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-08-12T20:01:31+00:00",
    "closed_at": "2024-10-29T17:40:25+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7439/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7439"
  },
  {
    "number": 10707,
    "title": "[Performance]: Unified flashattn kernel not outperforming current one",
    "body": "### Proposal to improve performance\n\nWhile working on https://github.com/vllm-project/vllm/pull/9291/, I experimented with unifying prefills and decodes processing in a single forward call (through the `flash_attn_varlen_func` API), while currently we separate the two by \"splitting\" the flattened 1d tokens tensor (size n_prefills+n_decodes). \r\nThe unification is meaningful when chunked prefill is enabled, as it will allow mixed prefill-decodes batches to be scheduled. \r\n\r\nFollowing the change, @sroy745 found no speedup in his benchmarks with the new version using a single kernel call, which is quite baffling.\r\n\r\nI believe we should give the fused version another try in a separate PR, investigating the causes of the unexpected slowdown, as in theory this should be a low-hanging fruit in terms of performance optimization.\r\n\r\nThe plan would be to rebase the changes introduced prior to this commit https://github.com/vllm-project/vllm/pull/9291/commits/2a9d8f1e48646eb79431c72b608bb2f44532666c#diff-c310ada35beeefacf4f019051ceaffeb471117d5d5b8be51610d80c7632c6bdcL657-L678  and benchmark performance once again to set the baseline, then take it from there. No need to focus on spec decoding from the start, since we *should* observe a boost on regular chunked prefill too (just enable it).  \n\n### Report of performance regression\n\n_No response_\n\n### Misc discussion on performance\n\n_No response_\n\n### Your current environment (if you think it is necessary)\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-27T11:09:32+00:00",
    "closed_at": "2025-03-20T17:16:43+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10707/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/10707"
  },
  {
    "number": 8295,
    "title": "[Feature]: Breaking Down Single Process into Asynchronous Tokenization, Model Inference, and Detokenization for Enhanced GPU Utilization",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nFeature Proposal:\r\nI would like to request an optimization feature where tokenization, model inference, and detokenization are performed asynchronously in separate processes, leading to a significant improvement in GPU utilization. This setup would enable parallel execution of these tasks, minimizing idle GPU time between the phases of the pipeline and increasing overall throughput.\r\n\r\nMotivation:\r\nCurrently, these three stages (tokenization, inference, detokenization) are typically handled sequentially, which results in underutilization of the GPU during the tokenization and detokenization phases. By separating these stages into asynchronous, tri-process collaboration, the GPU could be used more efficiently, especially for large models where tokenization and detokenization overhead becomes non-negligible.\r\n\r\nPitch:\r\nImplementing this feature could greatly enhance the performance of vLLM for high-throughput applications, leading to faster inference times and better resource utilization. I believe this would be beneficial for any workload where latency and throughput are critical.\n\n### Alternatives\n\nOne alternative solution would be to look into other frameworks like sglang and lightllm, which have already implemented tri-process asynchronous collaboration for tokenization, model inference, and detokenization. However, these solutions may not be as optimized or compatible with the specific features and design goals of vLLM. Another option could be manually orchestrating separate tokenization, inference, and detokenization steps outside of vLLM, but this would require additional complexity and could introduce synchronization issues.\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-09-09T12:23:22+00:00",
    "closed_at": "2024-09-09T23:34:45+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8295/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8295"
  },
  {
    "number": 3773,
    "title": "[Bug]: sample_params",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen I start openai server with `openai = = 1.12.0` i run into some problems when I want to pass in some unique sample Param, and it prompts me the error.\r\nBut the old version of `openai=0.27.8` wouldn't have such a problem. Will vLLM consider adapting the server's incoming parameters to the new version of openai in the future?\r\n``````---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[1], line 12\r\n      5 openai_api_base = \"http://localhost:8000/v1\" # \"http://39.98.81.39:6005/v1\"\r\n      7 client = OpenAI(\r\n      8     api_key=openai_api_key,\r\n      9     base_url=openai_api_base,\r\n     10 )\r\n---> 12 chat_response = client.chat.completions.create(\r\n     13     # model=\"/root/autodl-tmp/model/Qwen1___5-72B-Chat-GPTQ-Int4\",\r\n     14     model=\"Qwen1.5-14B-Chat\",\r\n     15     messages=[\r\n     16         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n     17         {\"role\": \"user\", \"content\": \"\u5199\u4e00\u6bb5800\u5b57\u8bba\u6587\"},\r\n     18     ],\r\n     19     stop=[\"<|endoftext|>\",\"<|im_end|>\"],\r\n     20     # stop_token_ids=[151643, 151645],\r\n     21     skip_special_tokens=False,\r\n     22     temperature=0.7,\r\n     23 )\r\n     24 print(chat_response.choices[0].message.content)\r\n\r\nFile ~/miniconda3/lib/python3.10/site-packages/openai/_utils/_utils.py:275, in required_args.<locals>.inner.<locals>.wrapper(*args, **kwargs)\r\n    273             msg = f\"Missing required argument: {quote(missing[0])}\"\r\n    274     raise TypeError(msg)\r\n--> 275 return func(*args, **kwargs)\r\n\r\nTypeError: Completions.create() got an unexpected keyword argument 'skip_special_tokens'\r\n\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-04-01T10:06:02+00:00",
    "closed_at": "2024-04-02T06:25:24+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3773/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3773"
  },
  {
    "number": 3929,
    "title": "[Misc]: page attention v2",
    "body": "### Anything you want to discuss about vllm.\n\nCan VLLM's page attention v2 be understood as incorporating the implementation of flash decoding",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-04-09T07:40:04+00:00",
    "closed_at": "2024-04-10T06:27:48+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3929/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3929"
  },
  {
    "number": 12899,
    "title": "[Usage]: Failure to Init Qwen2.5-VL-7B-Instruct with inflight bnb quantization",
    "body": "### Your current environment\n\n```text\ndocker vllm-openai:v0.7.2 with latest transformers installed\n```\n\n\n### How would you like to use vllm\n\nHi and I'm trying to launch qwen2.5-vl-7b-instruct in bnb inflight quanization but got error\n```\n (AssertionError: param_data.shape == loaded_weight.shape) \n```\n\nI was able to run this model at full precision with docker. Below is how I init the full precision one:\n```\nsudo docker run --runtime nvidia --gpus '\"device=0,1\"' --ipc=host -p 18434:8000 \\\n   -v hf_cache:/root/.cache/huggingface -d \\\n   --name qwen2.5-vl-7b \\\n   --entrypoint \"python3\" qwen-vl-fixed \\ # I installed new transformer and commited into a new image. \n   -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2.5-VL-7B-Instruct \\\n   --tensor-parallel-size 2 --trust-remote-code --max-model-len 18000 --dtype half\n```\n\nWhen I added `--quantization bitsandbytes --load-format bitsandbytes` into the docker command, the launch of the model in bnb 4bit inflight quantization failed.\n\nBelow is the full error log:\n```\nINFO 02-07 05:08:11 __init__.py:190] Automatically detected platform cuda.\n\nINFO 02-07 05:08:13 api_server.py:840] vLLM API server version 0.7.2\n\n\u0001\nuINFO 02-07 05:08:13 api_server.py:841] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2.5-VL-7B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='bitsandbytes', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='float16', kv_cache_dtype='auto', max_model_len=12000, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization='bitsandbytes', rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)\n\nINFO 02-07 05:08:13 api_server.py:206] Started engine process with PID 77\n\nWARNING 02-07 05:08:17 config.py:2386] Casting torch.bfloat16 to torch.float16.\n\nINFO 02-07 05:08:18 __init__.py:190] Automatically detected platform cuda.\n\nWARNING 02-07 05:08:23 config.py:2386] Casting torch.bfloat16 to torch.float16.\n\nINFO 02-07 05:08:24 config.py:542] This model supports multiple tasks: {'generate', 'classify', 'embed', 'score', 'reward'}. Defaulting to 'generate'.\n\nWARNING 02-07 05:08:24 config.py:621] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n\nINFO 02-07 05:08:31 config.py:542] This model supports multiple tasks: {'score', 'classify', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.\n\nWARNING 02-07 05:08:31 config.py:621] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n\nINFO 02-07 05:08:33 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='Qwen/Qwen2.5-VL-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-VL-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=12000, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-VL-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \n\nINFO 02-07 05:08:34 cuda.py:179] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\nINFO 02-07 05:08:34 cuda.py:227] Using XFormers backend.\n\nINFO 02-07 05:08:34 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-VL-7B-Instruct...\n\nINFO 02-07 05:08:35 config.py:2992] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]\n\nINFO 02-07 05:08:35 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n\nINFO 02-07 05:08:36 weight_utils.py:252] Using model weights format ['*.safetensors']\n\n\nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\nERROR 02-07 05:08:37 engine.py:389] \n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 380, in run_mp_engine\n\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 123, in from_engine_args\n\n    return cls(ipc_path=ipc_path,\n\n           ^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 75, in __init__\n\n    self.engine = LLMEngine(*args, **kwargs)\n\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 273, in __init__\n\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 51, in __init__\n\n    self._init_executor()\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 42, in _init_executor\n\n    self.collective_rpc(\"load_model\")\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 51, in collective_rpc\n\n    answer = run_method(self.driver_worker, method, args, kwargs)\n\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2220, in run_method\n\n    return func(*args, **kwargs)\n\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 183, in load_model\n\n    self.model_runner.load_model()\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1112, in load_model\n\n    self.model = get_model(vllm_config=self.vllm_config)\n\nProcess SpawnProcess-1:\n\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n\n    return loader.load_model(vllm_config=vllm_config)\n\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 1225, in load_model\n\n    self._load_weights(model_config, model)\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 1135, in _load_weights\n\n    loaded_weights = model.load_weights(qweight_iterator)\n\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2_5_vl.py\", line 1124, in load_weights\n\n    return loader.load_weights(weights, mapper=self.hf_to_vllm_mapper)\n\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 235, in load_weights\n\n    autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 196, in _load_module\n\n    yield from self._load_module(prefix,\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 173, in _load_module\n\n    loaded_params = module_load_weights(weights)\n\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2.py\", line 515, in load_weights\n\n    return loader.load_weights(weights)\n\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 235, in load_weights\n\n    autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 196, in _load_module\n\n    yield from self._load_module(prefix,\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 173, in _load_module\n\n    loaded_params = module_load_weights(weights)\n\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2.py\", line 400, in load_weights\n\n    weight_loader(param, loaded_weight, shard_id)\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py\", line 589, in weight_loader\n\n    assert param_data.shape == loaded_weight.shape\n\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAssertionError\n\nTraceback (most recent call last):\n\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n\n    self.run()\n\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n\n    self._target(*self._args, **self._kwargs)\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 391, in run_mp_engine\n\n    raise e\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 380, in run_mp_engine\n\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 123, in from_engine_args\n\n    return cls(ipc_path=ipc_path,\n\n           ^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 75, in __init__\n\n    self.engine = LLMEngine(*args, **kwargs)\n\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 273, in __init__\n\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 51, in __init__\n\n    self._init_executor()\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 42, in _init_executor\n\n    self.collective_rpc(\"load_model\")\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 51, in collective_rpc\n\n    answer = run_method(self.driver_worker, method, args, kwargs)\n\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2220, in run_method\n\n    return func(*args, **kwargs)\n\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 183, in load_model\n\n    self.model_runner.load_model()\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1112, in load_model\n\n    self.model = get_model(vllm_config=self.vllm_config)\n\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n\n    return loader.load_model(vllm_config=vllm_config)\n\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 1225, in load_model\n\n    self._load_weights(model_config, model)\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 1135, in _load_weights\n\n    loaded_weights = model.load_weights(qweight_iterator)\n\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2_5_vl.py\", line 1124, in load_weights\n\n    return loader.load_weights(weights, mapper=self.hf_to_vllm_mapper)\n\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 235, in load_weights\n\n    autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 196, in _load_module\n\n    yield from self._load_module(prefix,\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 173, in _load_module\n\n    loaded_params = module_load_weights(weights)\n\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2.py\", line 515, in load_weights\n\n    return loader.load_weights(weights)\n\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 235, in load_weights\n\n    autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 196, in _load_module\n\n    yield from self._load_module(prefix,\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 173, in _load_module\n\n    loaded_params = module_load_weights(weights)\n\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2.py\", line 400, in load_weights\n\n    weight_loader(param, loaded_weight, shard_id)\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py\", line 589, in weight_loader\n\n    assert param_data.shape == loaded_weight.shape\n\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAssertionError\n\n\nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n\nTraceback (most recent call last):\n\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 911, in <module>\n\n    uvloop.run(run_server(args))\n\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\n\n    return __asyncio.run(\n\n           ^^^^^^^^^^^^^^\n\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n\n    return runner.run(main)\n\n           ^^^^^^^^^^^^^^^^\n\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n\n    return self._loop.run_until_complete(task)\n\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n\n    return await main\n\n           ^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 875, in run_server\n\n    async with build_async_engine_client(args) as engine_client:\n\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n\n    return await anext(self.gen)\n\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 136, in build_async_engine_client\n\n    async with build_async_engine_client_from_engine_args(\n\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n\n    return await anext(self.gen)\n\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 230, in build_async_engine_client_from_engine_args\n\n    raise RuntimeError(\n\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-02-07T13:20:47+00:00",
    "closed_at": "2025-02-07T13:27:31+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12899/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12899"
  },
  {
    "number": 12055,
    "title": "[Bug]: Drop use of pickle where possible",
    "body": "\n### \ud83d\udc1b Describe the bug\n\nvLLM uses pickle for serialization and sometimes also sends serialized objects over a local zeromq unix socket. Using pickle and any sort of network communication is a known dangerous combination, as it's an easy way to open a vulnerability to remote code execution on a host when a host deserializes pickled data.\n\nThere have already been some changes to use [msgpack](https://github.com/msgpack/msgpack-python) instead. This issue is open to track the conversion away from using pickle where possible.\n\nThank you to @avilum who responsibly reported this as a security report. We discussed it and concluded we did not see any path to exploit at this time. However, it is still an important weakness that should be addressed to improve vLLM security.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-14T20:57:29+00:00",
    "closed_at": "2025-05-15T02:09:21+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12055/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/12055"
  },
  {
    "number": 3307,
    "title": "[feature on nm-vllm] Sparse Inference with weight only int8 quant",
    "body": "Can sparsity and quantization be used simultaneously to further improve inference speed? Do you have any plans in this regard? Looking forward to your reply @robertgshaw2-neuralmagic ",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-11T03:39:31+00:00",
    "closed_at": "2024-11-29T02:07:54+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3307/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3307"
  }
]
[
  {
    "number": 14002,
    "title": "[Feature]: Implement Priority Scheduling In V1 Engine",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nIn V0, we support request priority. I would like to see this in V1\n\ncc @WoosukKwon \n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "help wanted",
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-02-28T01:33:35+00:00",
    "closed_at": "2025-06-23T03:18:09+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14002/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14002"
  },
  {
    "number": 4847,
    "title": "Benchmark: benchmark_throughput and benchmark_latency should be able to write output to JSON file. ",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nSimilar to `benchmarks/benchmark_serving.py`, the throughput and latency benchmark should be able to write their metrics to JSON file for result aggregated so we don't need to parse the log data. \n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-05-16T02:21:56+00:00",
    "closed_at": "2024-05-16T17:02:57+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4847/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4847"
  },
  {
    "number": 2621,
    "title": "Mixtral AWQ fails to work: asyncio.exceptions.CancelledError: Cancelled by cancel scope 7fd214489990",
    "body": "```\r\nexport CUDA_HOME=/usr/local/cuda-12.3\r\nexport PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cu123\"\r\npip install git+https://github.com/vllm-project/vllm.git --upgrade\r\nexport CUDA_VISIBLE_DEVICES=1\r\n\r\npython -m vllm.entrypoints.openai.api_server --port=5002 --host=0.0.0.0 --model TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ --quantization awq --dtype auto --seed 1234 --tensor-parallel-size=1 --max-num-batched-tokens=66560 --max-log-len=100\r\n```\r\n\r\nAny where, even simple, leads to:\r\n\r\n```\r\nINFO 01-27 01:15:31 api_server.py:209] args: Namespace(host='0.0.0.0', port=5002, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, root_path=None, middleware=[], model='TheBloke/Mixtral-8x7B-Instru>\r\nWARNING 01-27 01:15:31 config.py:176] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\nINFO 01-27 01:15:31 llm_engine.py:72] Initializing an LLM engine with config: model='TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ', tokenizer='TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=>\r\nINFO 01-27 01:15:33 weight_utils.py:164] Using model weights format ['*.safetensors']\r\nINFO 01-27 01:17:50 llm_engine.py:316] # GPU blocks: 12486, # CPU blocks: 2048\r\nINFO 01-27 01:17:51 model_runner.py:625] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 01-27 01:17:51 model_runner.py:629] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\nINFO 01-27 01:18:04 model_runner.py:689] Graph capturing finished in 13 secs.\r\nINFO 01-27 01:18:04 serving_chat.py:260] Using default chat template:\r\nINFO 01-27 01:18:04 serving_chat.py:260] {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'ass>\r\nINFO:     Started server process [276444]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://0.0.0.0:5002 (Press CTRL+C to quit)\r\nINFO 01-27 01:18:14 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\r\nINFO 01-27 01:18:24 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\r\nINFO 01-27 01:18:34 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\r\nINFO 01-27 01:18:41 async_llm_engine.py:433] Received request cmpl-cd9d75c607614e7db704b01164bc0c83-0: prompt: None, prefix_pos: None,sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.14000000000000012, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, use_beam_search=False, length_penalty=1.0, early>\r\nINFO:     52.0.25.199:43684 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO 01-27 01:18:41 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/starlette/responses.py\", line 261, in __call__\r\n    await wrap(partial(self.listen_for_disconnect, receive))\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/starlette/responses.py\", line 257, in wrap\r\n    await func()\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/starlette/responses.py\", line 234, in listen_for_disconnect\r\n    message = await receive()\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 580, in receive\r\n    await self.message_event.wait()\r\n  File \"/ephemeral/vllm/lib/python3.10/asyncio/locks.py\", line 214, in wait\r\n    await fut\r\nasyncio.exceptions.CancelledError: Cancelled by cancel scope 7fd214489990\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 419, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/starlette/applications.py\", line 123, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n    raise exc\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/starlette/middleware/cors.py\", line 83, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/aioprometheus/asgi/middleware.py\", line 184, in __call__\r\n    await self.asgi_callable(scope, receive, wrapped_send)\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/starlette/routing.py\", line 762, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/starlette/routing.py\", line 782, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/starlette/routing.py\", line 297, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/starlette/routing.py\", line 77, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/starlette/routing.py\", line 75, in app\r\n    await response(scope, receive, send)\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/starlette/responses.py\", line 254, in __call__\r\n    async with anyio.create_task_group() as task_group:\r\n  File \"/ephemeral/vllm/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 678, in __aexit__\r\n    raise BaseExceptionGroup(\r\nexceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\r\nINFO 01-27 01:18:46 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\r\nINFO 01-27 01:18:51 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\r\nINFO 01-27 01:18:56 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%\r\nINFO 01-27 01:19:01 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%\r\nINFO 01-27 01:19:06 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%\r\nINFO 01-27 01:19:11 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%\r\nINFO 01-27 01:19:16 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%\r\nINFO 01-27 01:19:21 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%\r\nINFO 01-27 01:19:26 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%\r\nINFO 01-27 01:19:31 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%\r\nINFO 01-27 01:19:33 async_llm_engine.py:112] Finished request cmpl-cd9d75c607614e7db704b01164bc0c83-0.\r\nINFO 01-27 01:19:44 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\r\nINFO 01-27 01:19:54 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\r\nINFO 01-27 01:20:04 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\r\nINFO 01-27 01:20:14 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\r\nINFO 01-27 01:20:24 llm_engine.py:871] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\r\n\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-27T01:26:26+00:00",
    "closed_at": "2024-04-04T15:15:39+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2621/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2621"
  },
  {
    "number": 4663,
    "title": "[RFC]: Inline Golden (Expected) Tests",
    "body": "### Motivation.\n\nSome of tests in vllm is neither sufficient nor easy to read, e.g.\r\n\r\nhttps://github.com/vllm-project/vllm/blob/8344f7742b794ca6ec9bcb891c178cd0551f23d0/tests/core/test_scheduler.py#L293-L297\r\n\r\nThe test `assert out.blocks_to_swap_out != {}` is insufficient, and these lines only test certain properties of the output.\n\n### Proposed Change.\n\nWe can use inline golden tests (a.k.a. expected tests) from https://github.com/ezyang/expecttest, which is used heavily in pytorch:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/8b4d62009ddbc24a69dfcdbebc2cc84e4b2ee8f5/test/test_python_dispatch.py#L645-L654\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n_No response_\n\n### Any Other Things.\n\n_No response_",
    "labels": [
      "RFC",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-07T23:47:12+00:00",
    "closed_at": "2024-11-28T02:05:17+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4663/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/4663"
  },
  {
    "number": 14529,
    "title": "[Bug]: glm4v Is Broken",
    "body": "### Your current environment\n\nPer comment\n\n### \ud83d\udc1b Describe the bug\n\n- GLM4V is broken on V0 and V1\n\n```bash\nVLLM_USE_V1=0 pytest -v -x models/decoder_only/vision_language/test_models.py -k glm4v\n```\n\n```bash\ndef _validate_mm_placeholders(\n        self,\n        mm_placeholders: Mapping[str, list[PlaceholderFeaturesInfo]],\n        mm_item_counts: Mapping[str, int],\n    ) -> None:\n        for modality, item_count in mm_item_counts.items():\n            placeholders = mm_placeholders.get(modality, [])\n    \n            if len(placeholders) != item_count:\n>               raise RuntimeError(\n                    f\"Expected there to be {item_count} prompt updates \"\n                    f\"corresponding to {item_count} {modality} items, but \"\n                    f\"instead found {len(placeholders)} prompt updates! \"\n                    \"Either the prompt text has missing/incorrect tokens for \"\n                    \"multi-modal inputs, or there is a problem with your \"\n                    \"implementation of merged multi-modal processor for this \"\n                    \"model (usually arising from an inconsistency between \"\n                    \"`_call_hf_processor` and `_get_prompt_updates`).\")\nE               RuntimeError: Expected there to be 1 prompt updates corresponding to 1 image items, but instead found 0 prompt updates! Either the prompt text has missing/incorrect tokens for multi-modal inputs, or there is a problem with your implementation of merged multi-modal processor for this model (usually arising from an inconsistency between `_call_hf_processor` and `_get_prompt_updates`).\n\n\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-10T02:14:07+00:00",
    "closed_at": "2025-03-13T11:37:19+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14529/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14529"
  },
  {
    "number": 4263,
    "title": "[Bug]: offline test, Process hangs without exiting when using cuda graph",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.2.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-4.18.0-240.el8.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.3.107\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA L20\r\nGPU 1: NVIDIA L20\r\nGPU 2: NVIDIA L20\r\nGPU 3: NVIDIA L20\r\nGPU 4: NVIDIA L20\r\nGPU 5: NVIDIA L20\r\nGPU 6: NVIDIA L20\r\nGPU 7: NVIDIA L20\r\n\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   52 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nVendor ID:                       GenuineIntel\r\nBIOS Vendor ID:                  Intel(R) Corporation\r\nModel name:                      Intel(R) Xeon(R) Gold 6430\r\nBIOS Model name:                 Intel(R) Xeon(R) Gold 6430\r\nCPU family:                      6\r\nModel:                           143\r\nThread(s) per core:              2\r\nCore(s) per socket:              32\r\nSocket(s):                       2\r\nStepping:                        8\r\nFrequency boost:                 enabled\r\nCPU max MHz:                     2101.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        4200.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid cldemote movdiri movdir64b md_clear pconfig flush_l1d arch_capabilities\r\nVirtualization:                  VT-x\r\nL1d cache:                       3 MiB (64 instances)\r\nL1i cache:                       2 MiB (64 instances)\r\nL2 cache:                        128 MiB (64 instances)\r\nL3 cache:                        120 MiB (2 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-31,64-95\r\nNUMA node1 CPU(s):               32-63,96-127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.19.3\r\n[pip3] torch==2.2.1\r\n[pip3] triton==2.2.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.19.3                   pypi_0    pypi\r\n[conda] torch                     2.2.1                    pypi_0    pypi\r\n[conda] triton                    2.2.0                    pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.1\r\nvLLM Build Flags:\r\nCUDA Archs: 5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\n\ufffd[4mGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\ufffd[0m\r\nGPU0\t X \tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU1\tPIX\t X \tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU2\tPIX\tPIX\t X \tPIX\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU3\tPIX\tPIX\tPIX\t X \tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU4\tSYS\tSYS\tSYS\tSYS\t X \tPIX\tPIX\tPIX\t32-63,96-127\t1\t\tN/A\r\nGPU5\tSYS\tSYS\tSYS\tSYS\tPIX\t X \tPIX\tPIX\t32-63,96-127\t1\t\tN/A\r\nGPU6\tSYS\tSYS\tSYS\tSYS\tPIX\tPIX\t X \tPIX\t32-63,96-127\t1\t\tN/A\r\nGPU7\tSYS\tSYS\tSYS\tSYS\tPIX\tPIX\tPIX\t X \t32-63,96-127\t1\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\n offline test, Process hangs without exiting when using cuda graph\r\n```bash\r\ncost time(s) 2.5229225158691406\r\n(RayWorkerWrapper pid=404354) INFO 04-22 17:42:14 model_runner.py:1057] Graph capturing finished in 9 secs. [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=404354) [W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [::ffff:10.189.108.254]:47111 (errno: 97 - Address family not supported by protocol). [repeated 2x across cluster]\r\n#  offline test, Process hangs without exiting when using cuda graph\r\n```\r\nbut without cuda graph, it will exit normally. It is somethings wrong while using cuda graph in vllm?\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-04-22T09:53:12+00:00",
    "closed_at": "2024-05-09T12:37:47+00:00",
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4263/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4263"
  },
  {
    "number": 812,
    "title": "There are often recomputations in the prefill stage, is that a bug?",
    "body": "Here is my code in this file:  vllm/worker/worker.py\r\n```python\r\n    @torch.inference_mode()\r\n    def execute_model(\r\n        self,\r\n        seq_group_metadata_list: List[SequenceGroupMetadata],\r\n        blocks_to_swap_in: Dict[int, int],\r\n        blocks_to_swap_out: Dict[int, int],\r\n        blocks_to_copy: Dict[int, List[int]],\r\n    ) -> Dict[int, SequenceOutputs]:\r\n\r\n        print(f\"begin to run {len(seq_group_metadata_list)} groups\")\r\n        num = len(seq_group_metadata_list)\r\n        need_prefill = False\r\n        need_decoding = False\r\n        if num > 1:\r\n            for seq in seq_group_metadata_list:\r\n                for k, v in seq.seq_data.items():\r\n                    if len(v.output_token_ids) == 0:\r\n                        need_prefill = True\r\n                    else:\r\n                        need_decoding = True\r\n        if need_prefill == True and need_decoding == True:\r\n            import pdb; pdb.set_trace()\r\n```\r\nI want to capture such situation: a batched input with both sequence that need to run prefill and  sequence that need to run decoding.\r\n \r\nThen I get this:\r\n\r\n![image](https://github.com/vllm-project/vllm/assets/26128514/a68ce773-4327-46cc-b98e-383cd439602b)\r\n\r\nThis group contains two sequence, the first one has generated 37 tokens, and the second one has no generated tokens, am I right?\r\n\r\nafter this line of code:\r\n```python\r\n      # Prepare input tensors.\r\n        input_tokens, input_positions, input_metadata = self._prepare_inputs(\r\n            seq_group_metadata_list)\r\n```\r\n\r\nNotice the num_generation_tokens is 0. \r\n```python\r\nInputMetadata(num_valid_tokens=1400, num_prompt_tokens=1400, num_prompts=2, prompt_lens=[661, 739],\r\n num_generation_tokens=0, \r\ncontext_lens=tensor([], device='cuda:0', dtype=torch.int32), max_context_len=0), max_num_blocks_per_seq=0, block_tables=tensor([], device='cuda:0', dtype=torch.int32)), slot_mapping=tensor([1600, 1601, 1602,  ..., 1888, 1889, 1890], device='cuda:0',\r\n       dtype=torch.int32)\r\n```\r\nThe 1st sequence is flattened together with the 2nd sequence, they both run prefill?? why is that? The 1st one just need to do decoding, is that the fact?\r\n\r\nIs this a feature or it's a bug?\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-08-21T11:04:49+00:00",
    "closed_at": "2024-05-31T04:19:56+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/812/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/812"
  },
  {
    "number": 48,
    "title": "Improve Weight Loading",
    "body": "Just use Huggingface's weights. Don't do another copy!",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-22T04:16:22+00:00",
    "closed_at": "2023-05-03T07:32:05+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/48/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/48"
  },
  {
    "number": 6803,
    "title": "[Misc]: setting environment variables in multi-node serving",
    "body": "### Anything you want to discuss about vllm.\n\nAs we embrace large models like Llama 3.1 405B, lots of users are trying multi-node inference now.\r\n\r\nCompared with single-node inference, multi-node inference is much more difficult to set up, due to the nature of complicated machine configuration.\r\n\r\nThe [documentation](https://docs.vllm.ai/en/stable/serving/distributed_serving.html#multi-node-inference-and-serving) serves as a starting point. And we have a [debugging guide](https://docs.vllm.ai/en/stable/getting_started/debugging.html) with a sanity check script for testing the configuration. Even with this help, users might still find it difficult to set up the cluster, especially w.r.t. network configuration to make the machines talk to each other.\r\n\r\nThis discussion issue tries to clarify one aspect: how to set environment variables in multi-node setting, and how does environment variable inheritance work  in multi-node serving.\r\n\r\n> NOTE: https://github.com/vllm-project/vllm/issues/6775 is a very good example of how to open an issue to ask for help in the right way. The clearer the issue descriptions, the faster you can get help.\r\n\r\nThere are many levels of environment variables:\r\n\r\n1. environment variables set at machine start time (or container start time)\r\n2. environment variables set by shell commands\r\n3. environment variables set during program execution\r\n\r\nThis becomes complicated when we create multiple processes, and even create processes in another node when using `ray`. In distributed serving with multiple GPUs and multiple processes, **we need to make sure environment variables are set correctly for every process. Otherwise it might cause error**.\r\n\r\nWhen vLLM starts a new process, what environment variables will the new process have? The complete answer is:\r\n\r\n- multiprocessing backend: all 1/2/3 are inherited by the new process\r\n- ray backend, with a local Ray instance: inherit 1/2/3 until `ray.init()`\r\n- ray backend, with a pre-launched cluster: inherit environment variables when `ray start` is executed\r\n\r\nHere is a short script to help people understand:\r\n\r\n```python\r\n# test.py\r\n\r\nimport os\r\nimport ray\r\n\r\ndef report_env(prefix=\"ray actor process\"):\r\n    keys = [\r\n        \"CONTAINER_ENV\",\r\n        \"CMD_ENV\",\r\n        \"DYNAMIC_ENV\",\r\n    ]\r\n    for k in keys:\r\n        v = os.environ.get(k, None)\r\n        print(f\"{prefix}: {k}={v}\")\r\n\r\n@ray.remote(num_gpus=1)\r\nclass Actor:\r\n    def __init__(self) -> None:\r\n        report_env()\r\n\r\ndef driver_function(value=\"DRIVER\"):\r\n    os.environ[\"DYNAMIC_ENV\"] = value\r\n\r\nif __name__ == \"__main__\":\r\n    driver_function(value=\"Before ray\")\r\n    ray.init()\r\n    driver_function(value=\"After ray\")\r\n\r\n    report_env(prefix=\"parent process\")\r\n\r\n    actors = [Actor.remote(), Actor.remote()]\r\n\r\n    import multiprocessing\r\n\r\n    context = multiprocessing.get_context(\"spawn\")\r\n    p = context.Process(target=report_env, args=(\"child process:\",))\r\n    p.start()\r\n    p.join()\r\n```\r\n\r\nAnd here is the output of executing with and without ray cluster:\r\n\r\n```shell\r\n$ CMD_ENV=1 python test.py\r\n\r\n2024-07-25 13:05:57,185 INFO worker.py:1788 -- Started a local Ray instance.\r\nparent process: CONTAINER_ENV=None\r\nparent process: CMD_ENV=1\r\nparent process: DYNAMIC_ENV=After ray\r\nchild process:: CONTAINER_ENV=None\r\nchild process:: CMD_ENV=1\r\nchild process:: DYNAMIC_ENV=After ray\r\n(Actor pid=4044013) ray actor process: CONTAINER_ENV=None\r\n(Actor pid=4044013) ray actor process: CMD_ENV=1\r\n(Actor pid=4044013) ray actor process: DYNAMIC_ENV=Before ray\r\n(Actor pid=4044016) ray actor process: CONTAINER_ENV=None\r\n(Actor pid=4044016) ray actor process: CMD_ENV=1\r\n(Actor pid=4044016) ray actor process: DYNAMIC_ENV=Before ray\r\n\r\n$ CMD_ENV=ray_start ray start --head\r\n\r\n... output omitted ...\r\n\r\n$ CMD_ENV=1 python test.py\r\n\r\n2024-07-25 13:07:05,128 INFO worker.py:1603 -- Connecting to existing Ray cluster at address: xxx.xxx.xxx.xxx:6379...\r\n2024-07-25 13:07:05,134 INFO worker.py:1788 -- Connected to Ray cluster.\r\nparent process: CONTAINER_ENV=None\r\nparent process: CMD_ENV=1\r\nparent process: DYNAMIC_ENV=After ray\r\nchild process:: CONTAINER_ENV=None\r\nchild process:: CMD_ENV=1\r\nchild process:: DYNAMIC_ENV=After ray\r\n(Actor pid=4058591) ray actor process: CONTAINER_ENV=None\r\n(Actor pid=4058591) ray actor process: CMD_ENV=ray_start\r\n(Actor pid=4058591) ray actor process: DYNAMIC_ENV=None\r\n(Actor pid=4058593) ray actor process: CONTAINER_ENV=None\r\n(Actor pid=4058593) ray actor process: CMD_ENV=ray_start\r\n(Actor pid=4058593) ray actor process: DYNAMIC_ENV=None\r\n```\r\n\r\nAs we can see, child process always inherits all environment variables from the parent process. The ray actor process, instead, only inherits the state when we create the cluster:\r\n\r\n- when we start a local instance via `ray.init`, the cluster is started from the Python interpreter, and inherit the state at that moment. So we can see `ray actor process: DYNAMIC_ENV=Before ray` and `child process:: DYNAMIC_ENV=After ray`.\r\n- when we start a ray cluster before we execute the python script, the ray actor process later also remember the cluster's state, but is not affected by the python process. So we can see `child process:: CMD_ENV=1` and `ray actor process: CMD_ENV=ray_start`.\r\n\r\nThe common usecase for single-node ray backend is to start a local instance via `ray.init`. In this regard, shell environment variables can be successfully applied to all vLLM processes.\r\n\r\n> TL;DR: normally, in single-node case, setting environment variables before executing vLLM should just work as expected.\r\n\r\nNow let's go to the multi-node case. We start a cluster by (the script and usage are described in the [doc](https://docs.vllm.ai/en/stable/serving/distributed_serving.html#multi-node-inference-and-serving)):\r\n\r\n```shell\r\n# head node\r\n$ bash run_cluster.sh vllm/vllm-openai 10.150.0.34 --head ~/.cache/huggingface -v /home/gcpuser/vllm/examples:/root/cwd -e CONTAINER_ENV=head\r\n\r\n# worker node\r\n$ bash run_cluster.sh vllm/vllm-openai 10.150.0.34 --worker ~/.cache/huggingface -v /home/gcpuser/vllm/examples:/root/cwd -e CONTAINER_ENV=worker\r\n```\r\n\r\nNote that we defined different value for `CONTAINER_ENV` for different nodes. Let's see what would happen:\r\n\r\n```shell\r\n# head node\r\n$ docker exec -it node /bin/bash\r\n\r\n$ CMD_ENV=1 python /root/cwd/test.py\r\n\r\n2024-07-25 21:29:24,521\tINFO worker.py:1603 -- Connecting to existing Ray cluster at address: 10.150.0.34:6379...\r\n2024-07-25 21:29:24,529\tINFO worker.py:1788 -- Connected to Ray cluster.\r\nparent process: CONTAINER_ENV=head\r\nparent process: CMD_ENV=1\r\nparent process: DYNAMIC_ENV=After ray\r\nchild process:: CONTAINER_ENV=head\r\nchild process:: CMD_ENV=1\r\nchild process:: DYNAMIC_ENV=After ray\r\n(Actor pid=100, ip=10.150.0.35) ray actor process: CONTAINER_ENV=worker\r\n(Actor pid=100, ip=10.150.0.35) ray actor process: CMD_ENV=None\r\n(Actor pid=100, ip=10.150.0.35) ray actor process: DYNAMIC_ENV=None\r\n(Actor pid=314) ray actor process: CONTAINER_ENV=head\r\n(Actor pid=314) ray actor process: CMD_ENV=None\r\n(Actor pid=314) ray actor process: DYNAMIC_ENV=None\r\n```\r\n\r\nGiven the result, all processes in the head node get `CONTAINER_ENV=head`, while the process in the worker node gets `CONTAINER_ENV=worker`. Meanwhile, all the Python processes get `CMD_ENV=1`, but all the ray actor processes get `CMD_ENV=None`.\r\n\r\nTherefore, the conclusion is: if we start a multi-node cluster, the only reliable way of setting environment variables , is to set environment variables at the container start time. All the processes in the node, will use the environment variables set by that node.\r\n\r\nThis is important for dealing with complicated network configurations. For example, if the head node uses `eth0` network interface to talk to `eth1` network interface in the worker node, then we should launch the cluster as:\r\n\r\n```shell\r\n# head node\r\n$ bash run_cluster.sh vllm/vllm-openai 10.150.0.34 --head ~/.cache/huggingface -v /home/gcpuser/vllm/examples:/root/cwd -e NCCL_SOCKET_IFNAME=eth0 -e GLOO_SOCKET_IFNAME=eth0\r\n\r\n# worker node\r\n$ bash run_cluster.sh vllm/vllm-openai 10.150.0.34 --worker ~/.cache/huggingface -v /home/gcpuser/vllm/examples:/root/cwd -e NCCL_SOCKET_IFNAME=eth1 -e GLOO_SOCKET_IFNAME=eth1\r\n```\r\n\r\n> TL;DR: in multi-node case, we need to set environment variables when we start the container. Processes in each container will use the value of environment variables defined by that container.\r\n",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-07-25T21:55:21+00:00",
    "closed_at": "2024-07-25T22:38:33+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6803/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/6803"
  },
  {
    "number": 2565,
    "title": "vLLM computes max sequence length for Yi 200k at 4k",
    "body": "https://huggingface.co/01-ai/Yi-34B-Chat/blob/main/config.json\r\n\r\nvLLM not accounting for rope scaling.\r\n\r\nSo can't use full context.\r\n\r\n```\r\n -m vllm.entrypoints.openai.api_server \\\r\n        --port=5000 \\\r\n        --host=0.0.0.0 \\\r\n        --model=01-ai/Yi-34B-Chat \\\r\n        --seed 1234 \\\r\n        --tensor-parallel-size=4 \\\r\n        --trust-remote-code \\\r\n        --max-model-len=204800 \\\r\n        --download-dir=/workspace/.cache/huggingface/hub\r\n```\r\n\r\ngives:\r\n```\r\nINFO 01-23 21:18:03 api_server.py:727] args: Namespace(host='0.0.0.0', port=5000, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], served_model_name=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, model='01-ai/Yi-34B-Chat', tokenizer=None, revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, download_dir='/workspace/.cache/huggingface/hub', load_format='auto', dtype='auto', max_model_len=204800, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, block_size=16, seed=1234, swap_space=4, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_seqs=256, max_paddings=256, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\nTraceback (most recent call last):\r\n  File \"/h2ogpt_conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/h2ogpt_conda/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/h2ogpt_conda/vllm_env/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 737, in <module>\r\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\r\n  File \"/h2ogpt_conda/vllm_env/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 494, in from_engine_args\r\n    engine_configs = engine_args.create_engine_configs()\r\n  File \"/h2ogpt_conda/vllm_env/lib/python3.10/site-packages/vllm/engine/arg_utils.py\", line 218, in create_engine_configs\r\n    model_config = ModelConfig(self.model, self.tokenizer,\r\n  File \"/h2ogpt_conda/vllm_env/lib/python3.10/site-packages/vllm/config.py\", line 103, in __init__\r\n    self.max_model_len = _get_and_verify_max_len(self.hf_config,\r\n  File \"/h2ogpt_conda/vllm_env/lib/python3.10/site-packages/vllm/config.py\", line 511, in _get_and_verify_max_len\r\n    raise ValueError(\r\nValueError: User-specified max_model_len (204800) is greater than the derived max_model_len (None=4096 in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\r\n```\r\n\r\nand otherwise defaults to 4096.",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-23T21:23:40+00:00",
    "closed_at": "2024-01-23T21:32:16+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2565/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2565"
  },
  {
    "number": 9382,
    "title": "[New Model]: Support Zyphra/Zamba2-7B",
    "body": "### The model to consider.\r\n\r\nAnnouncement blog: https://www.zyphra.com/post/zamba2-7b\r\n\r\nBase model: https://huggingface.co/Zyphra/Zamba2-7B\r\nInstruct tuned: https://huggingface.co/Zyphra/Zamba2-7B-Instruct\r\n\r\n![image](https://github.com/user-attachments/assets/bba7f100-f7cf-4284-b8b0-90ed99d9a522)\r\n\r\n\r\n### The closest model vllm already supports.\r\n\r\nJamba, as it is a mixture of state-space and transformers blocks\r\n\r\n> Zamba2-7B-Instruct is a hybrid model composed of state-space ([Mamba2](https://github.com/state-spaces/mamba)) and transformer blocks.\r\n\r\n### What's your difficulty of supporting the model you want?\r\n\r\nShould be easy once Mamba2 support lands in https://github.com/vllm-project/vllm/pull/9292, however this `use_shared_attention_lora` case seems possibly complex\r\n\r\nAll of the HF-compatible modeling code can be found here: https://github.com/Zyphra/transformers_zamba2/tree/main/src/transformers/models/zamba2\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model",
      "unstale"
    ],
    "state": "closed",
    "created_at": "2024-10-15T16:53:55+00:00",
    "closed_at": "2025-04-05T11:11:30+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9382/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/9382"
  },
  {
    "number": 5426,
    "title": "[Feature]: ci test with vGPU",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nit seems aws and gcp supports [vGPU](https://docs.nvidia.com/grid/cloud-service-support.html) . we can run some small tests in vGPU, which should be cost-efficient and also test broader software support to avoid https://github.com/vllm-project/vllm/issues/4587 .\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-06-11T16:39:05+00:00",
    "closed_at": "2024-11-27T02:07:12+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5426/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/5426"
  },
  {
    "number": 7216,
    "title": "[Bug]: Dynamic FP8 Marlin quantization fails on `0.5.4` ",
    "body": "### Your current environment\r\n\r\n<details>\r\n\r\n```\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-107-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.5.82\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nGPU 1: NVIDIA A100-SXM4-80GB\r\nGPU 2: NVIDIA A100-SXM4-80GB\r\nGPU 3: NVIDIA A100-SXM4-80GB\r\nGPU 4: NVIDIA A100-SXM4-80GB\r\nGPU 5: NVIDIA A100-SXM4-80GB\r\nGPU 6: NVIDIA A100-SXM4-80GB\r\nGPU 7: NVIDIA A100-SXM4-80GB\r\n\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             128\r\nOn-line CPU(s) list:                0-127\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8462Y+\r\nCPU family:                         6\r\nModel:                              143\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 32\r\nSocket(s):                          2\r\nStepping:                           8\r\nBogoMIPS:                           5600.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          3 MiB (64 instances)\r\nL1i cache:                          2 MiB (64 instances)\r\nL2 cache:                           128 MiB (64 instances)\r\nL3 cache:                           120 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126\r\nNUMA node1 CPU(s):                  1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.0\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.4\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV12    NV12    NV12    NV12    NV12    NV12    NV12    PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0,2,4,6,8,10    0               N/A\r\nGPU1    NV12     X      NV12    NV12    NV12    NV12    NV12    NV12    SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     0,2,4,6,8,10    0               N/A\r\nGPU2    NV12    NV12     X      NV12    NV12    NV12    NV12    NV12    SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     0,2,4,6,8,10    0               N/A\r\nGPU3    NV12    NV12    NV12     X      NV12    NV12    NV12    NV12    SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     0,2,4,6,8,10    0               N/A\r\nGPU4    NV12    NV12    NV12    NV12     X      NV12    NV12    NV12    SYS     SYS     SYS     SYS     PXB     SYS     SYS     SYS     1,3,5,7,9,11    1               N/A\r\nGPU5    NV12    NV12    NV12    NV12    NV12     X      NV12    NV12    SYS     SYS     SYS     SYS     SYS     PXB     SYS     SYS     1,3,5,7,9,11    1               N/A\r\nGPU6    NV12    NV12    NV12    NV12    NV12    NV12     X      NV12    SYS     SYS     SYS     SYS     SYS     SYS     PXB     SYS     1,3,5,7,9,11    1               N/A\r\nGPU7    NV12    NV12    NV12    NV12    NV12    NV12    NV12     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB     1,3,5,7,9,11    1               N/A\r\nNIC0    PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC1    SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC2    SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS\r\nNIC3    SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS\r\nNIC4    SYS     SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS\r\nNIC5    SYS     SYS     SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS\r\nNIC6    SYS     SYS     SYS     SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS\r\nNIC7    SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n```\r\n\r\n</details>\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nDynamic FP8 works fine on H100 but fails on A100. This is an issue with the dynamic FP8 Marlin backend.\r\n```\r\nvllm serve meta-llama/Meta-Llama-3-8B-Instruct --quantization=\"fp8\" --port 9000 \r\n...\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/vllm/model_executor/layers/quantization/utils/marlin_utils.py\", line 172, in marlin_permute_scales\r\n    s = s.reshape((-1, len(scale_perm_single)))[:, scale_perm_single]\r\nRuntimeError: shape '[-1, 32]' is invalid for input of size 1\r\n```\r\n\r\nIt does work fine with models that are already quantized to FP8 on A100:\r\n```\r\nvllm serve neuralmagic/Meta-Llama-3-8B-Instruct-FP8 --quantization=\"fp8\" --port 9000\r\n...\r\nINFO:     Uvicorn running on http://0.0.0.0:9000 (Press CTRL+C to quit)\r\n```\r\n\r\nFull command and output/stacktrace:\r\n```\r\nvllm serve meta-llama/Meta-Llama-3-8B-Instruct --quantization=\"fp8\" --port 9000     \r\nINFO 08-06 19:27:35 api_server.py:339] vLLM API server version 0.5.4\r\nINFO 08-06 19:27:35 api_server.py:340] args: Namespace(model_tag='meta-llama/Meta-Llama-3-8B-Instruct', host=None, port=9000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization='fp8', rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None, dispatch_function=<function serve at 0x7f5440e617e0>)\r\nWARNING 08-06 19:27:35 config.py:1454] Casting torch.bfloat16 to torch.float16.\r\nINFO 08-06 19:27:35 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\r\nINFO 08-06 19:27:38 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\r\nINFO 08-06 19:27:39 weight_utils.py:225] Using model weights format ['*.safetensors']\r\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  7.55it/s]\r\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.52it/s]\r\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.97it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  1.81it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.02it/s]\r\n\r\nWARNING 08-06 19:27:41 utils.py:578] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.\r\nProcess Process-1:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/vllm/entrypoints/openai/rpc/server.py\", line 217, in run_rpc_server\r\n    server = AsyncEngineRPCServer(async_engine_args, usage_context, port)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/vllm/entrypoints/openai/rpc/server.py\", line 25, in __init__\r\n    self.engine = AsyncLLMEngine.from_engine_args(async_engine_args,\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 471, in from_engine_args\r\n    engine = cls(\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 381, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 552, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 249, in __init__\r\n    self.model_executor = executor_class(\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 47, in __init__\r\n    self._init_executor()\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/vllm/executor/gpu_executor.py\", line 36, in _init_executor\r\n    self.driver_worker.load_model()\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/vllm/worker/worker.py\", line 139, in load_model\r\n    self.model_runner.load_model()\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 722, in load_model\r\n    self.model = get_model(model_config=self.model_config,\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model\r\n    return loader.load_model(model_config=model_config,\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 344, in load_model\r\n    quant_method.process_weights_after_loading(module)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/vllm/model_executor/layers/quantization/fp8.py\", line 212, in process_weights_after_loading\r\n    prepare_fp8_layer_for_marlin(layer)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py\", line 80, in prepare_fp8_layer_for_marlin\r\n    marlin_scales = marlin_permute_scales(s=scales,\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/vllm/model_executor/layers/quantization/utils/marlin_utils.py\", line 172, in marlin_permute_scales\r\n    s = s.reshape((-1, len(scale_perm_single)))[:, scale_perm_single]\r\nRuntimeError: shape '[-1, 32]' is invalid for input of size 1\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-08-06T19:33:19+00:00",
    "closed_at": "2024-08-07T18:23:13+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7216/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/7216"
  },
  {
    "number": 14263,
    "title": "[Bug]: Dose V1  support MLA + PP now? Raise error while using PP+TP+V1.",
    "body": "Device:\n\n3xL20x8, DeepSeek-R1, PP=3, TP=8, VLLM_USE_V1=1\n\nLaunch:\n\n```bash\npython3 -m vllm.entrypoints.openai.api_server  --dtype=auto --tensor-parallel-size=8 --host=0.0.0.0 --port=80 --tokenizer-mode=slow --model=/DeepSeek-R1 --block-size=32 --swap-space=16 --g\npu-memory-utilization=0.9 --pipeline-parallel-size=3 --max-num-seqs=48 --trust-remote-code --no-enable-prefix-caching  --enable-chunked-prefill=True --max-model-len=16384 --max-num-batched-tokens=2048 --served-model-name=/DeepSeek-R1\n```\n\nError:\n```\n[2025-03-05 14:28:30,815] [INFO] [MainThread] [vllm.transformers_utils.config] >>> Replacing legacy 'type' key with 'rope_type'\n[2025-03-05 14:28:34,452] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:37,141] [INFO] [MainThread] [vllm.config] >>> This model supports multiple tasks: {'generate', 'embed', 'classify', 'score', 'reward'}. Defaulting to 'generate'.\n[2025-03-05 14:28:37,343] [INFO] [MainThread] [vllm.config] >>> Defaulting to use ray for distributed inference\n[2025-03-05 14:28:37,343] [INFO] [MainThread] [vllm.config] >>> Chunked prefill is enabled with max_num_batched_tokens=2048.\n[2025-03-05 14:28:37,344] [WARNING] [MainThread] [vllm.model_executor.layers.quantization.fp8] >>> Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-03-05 14:28:37,801] [INFO] [MainThread] [vllm.v1.engine.core] >>> Initializing a V1 LLM engine (v0.7.4a4) with config: model='/apps/svr/model', speculative_config=None, tokenizer='/apps/svr/model', skip_tokenizer_init=False, tokenizer_mode=slow, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=3, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/apps/svr/model, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n[2025-03-05 14:28:39,138] [INFO] [MainThread] [vllm.executor.ray_distributed_executor] >>> use_ray_spmd_worker: True\n[2025-03-05 14:28:42,898] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:42,999] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,038] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,063] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,352] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,358] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,375] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,384] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,545] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,699] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,847] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,851] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,855] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,862] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,864] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,865] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,871] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,878] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,878] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,879] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,880] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,892] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,915] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:43,929] [INFO] [MainThread] [vllm.platforms] >>> Automatically detected platform cuda.\n[2025-03-05 14:28:45,680] [INFO] [MainThread] [vllm.executor.ray_distributed_executor] >>> Copying the following environment variables to workers: ['VLLM_USE_V1']\n[2025-03-05 14:28:45,735] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x147955b8e0b0>\n[2025-03-05 14:28:45,735] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14bb8738a020>\n[2025-03-05 14:28:45,735] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x145ffc965ff0>\n[2025-03-05 14:28:45,736] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14d9fb0b2110>\n[2025-03-05 14:28:45,736] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x146771dadff0>\n[2025-03-05 14:28:45,737] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14fca2336110>\n[2025-03-05 14:28:45,740] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14c29844dc90>\n[2025-03-05 14:28:45,740] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14f18a181c00>\n[2025-03-05 14:28:45,740] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x1474d4349b70>\n[2025-03-05 14:28:45,741] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x148ef31ddc00>\n[2025-03-05 14:28:45,741] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x1484a3599bd0>\n[2025-03-05 14:28:45,741] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14a645d81bd0>\n[2025-03-05 14:28:45,742] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14b7f1571ba0>\n[2025-03-05 14:28:45,742] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x1484079f9b40>\n[2025-03-05 14:28:45,742] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x1505d318db10>\n[2025-03-05 14:28:45,742] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14daa6b85c60>\n[2025-03-05 14:28:45,742] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14ec4c825b70>\n[2025-03-05 14:28:45,743] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x148902d9dba0>\n[2025-03-05 14:28:45,743] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14f230341c30>\n[2025-03-05 14:28:45,743] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x148a0c44dbd0>\n[2025-03-05 14:28:45,744] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x147db5de9bd0>\n[2025-03-05 14:28:45,744] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x144b35df9cc0>\n[2025-03-05 14:28:45,748] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14f50b1aa080>\n[2025-03-05 14:28:45,750] [WARNING] [MainThread] [vllm.utils] >>> Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14765a096110>\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> Error executing method 'init_device'. This might cause deadlock in distributed execution.\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> Traceback (most recent call last):\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 332, in _lazy_init\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     queued_call()\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 200, in _check_capability\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     capability = get_device_capability(d)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 509, in get_device_capability\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     prop = get_device_properties(device)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 527, in get_device_properties\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     return _get_device_properties(device)  # type: ignore[name-defined]\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> RuntimeError: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":49, please report a bug to PyTorch. device=1, num_gpus=\u0001\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> \n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> The above exception was the direct cause of the following exception:\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> \n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> Traceback (most recent call last):\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 585, in execute_method\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     return run_method(self, method, args, kwargs)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/vllm/utils.py\", line 2238, in run_method\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     return func(*args, **kwargs)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     return method(self, *_args, **_kwargs)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 577, in init_device\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     self.worker.init_device()  # type: ignore\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/worker/gpu_worker.py\", line 102, in init_device\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     torch.cuda.set_device(self.device)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 478, in set_device\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     torch._C._cuda_setDevice(device)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 338, in _lazy_init\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     raise DeferredCudaCallError(msg) from e\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> torch.cuda.DeferredCudaCallError: CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":49, please report a bug to PyTorch. device=1, num_gpus=\u0001\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> \n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> CUDA call was originally invoked at:\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> \n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/workers/default_worker.py\", line 297, in <module>\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     worker.main_loop()\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 935, in main_loop\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     self.core_worker.run_task_loop()\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/function_manager.py\", line 544, in load_actor_class\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     actor_class = self._load_actor_class_from_gcs(\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/function_manager.py\", line 647, in _load_actor_class_from_gcs\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     actor_class = pickle.loads(pickled_class)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/vllm/__init__.py\", line 9, in <module>\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     import torch\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 1954, in <module>\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     _C._initExtension(_manager_path())\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 264, in <module>\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     _lazy_call(_check_capability)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 261, in _lazy_call\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     _queued_calls.append((callable, traceback.format_stack()))\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> \n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> Error executing method 'init_device'. This might cause deadlock in distributed execution.\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> Traceback (most recent call last):\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 332, in _lazy_init\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     queued_call()\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 200, in _check_capability\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     capability = get_device_capability(d)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 509, in get_device_capability\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     prop = get_device_properties(device)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 527, in get_device_properties\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     return _get_device_properties(device)  # type: ignore[name-defined]\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> RuntimeError: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":49, please report a bug to PyTorch. device=1, num_gpus=\u0001\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> \n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> The above exception was the direct cause of the following exception:\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> \n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> Traceback (most recent call last):\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 585, in execute_method\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     return run_method(self, method, args, kwargs)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/vllm/utils.py\", line 2238, in run_method\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     return func(*args, **kwargs)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     return method(self, *_args, **_kwargs)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 577, in init_device\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     self.worker.init_device()  # type: ignore\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/worker/gpu_worker.py\", line 102, in init_device\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     torch.cuda.set_device(self.device)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 478, in set_device\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     torch._C._cuda_setDevice(device)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 338, in _lazy_init\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     raise DeferredCudaCallError(msg) from e\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> torch.cuda.DeferredCudaCallError: CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":49, please report a bug to PyTorch. device=1, num_gpus=\u0001\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> \n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> CUDA call was originally invoked at:\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> \n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/workers/default_worker.py\", line 297, in <module>\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     worker.main_loop()\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 935, in main_loop\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     self.core_worker.run_task_loop()\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/function_manager.py\", line 544, in load_actor_class\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     actor_class = self._load_actor_class_from_gcs(\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/function_manager.py\", line 647, in _load_actor_class_from_gcs\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     actor_class = pickle.loads(pickled_class)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/vllm/__init__.py\", line 9, in <module>\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     import torch\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 1954, in <module>\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     _C._initExtension(_manager_path())\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 264, in <module>\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     _lazy_call(_check_capability)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 261, in _lazy_call\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     _queued_calls.append((callable, traceback.format_stack()))\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> \n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> Error executing method 'init_device'. This might cause deadlock in distributed execution.\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> Traceback (most recent call last):\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 332, in _lazy_init\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     queued_call()\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 200, in _check_capability\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     capability = get_device_capability(d)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 509, in get_device_capability\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     prop = get_device_properties(device)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 527, in get_device_properties\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     return _get_device_properties(device)  # type: ignore[name-defined]\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> RuntimeError: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":49, please report a bug to PyTorch. device=1, num_gpus=\u0001\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> \n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> The above exception was the direct cause of the following exception:\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> \n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> Traceback (most recent call last):\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 585, in execute_method\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     return run_method(self, method, args, kwargs)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/vllm/utils.py\", line 2238, in run_method\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     return func(*args, **kwargs)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     return method(self, *_args, **_kwargs)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 577, in init_device\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     self.worker.init_device()  # type: ignore\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/worker/gpu_worker.py\", line 102, in init_device\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     torch.cuda.set_device(self.device)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 478, in set_device\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     torch._C._cuda_setDevice(device)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 338, in _lazy_init\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     raise DeferredCudaCallError(msg) from e\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> torch.cuda.DeferredCudaCallError: CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":49, please report a bug to PyTorch. device=1, num_gpus=\u0001\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> \n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> CUDA call was originally invoked at:\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> \n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/workers/default_worker.py\", line 297, in <module>\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     worker.main_loop()\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 935, in main_loop\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     self.core_worker.run_task_loop()\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/function_manager.py\", line 544, in load_actor_class\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     actor_class = self._load_actor_class_from_gcs(\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/function_manager.py\", line 647, in _load_actor_class_from_gcs\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     actor_class = pickle.loads(pickled_class)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/vllm/__init__.py\", line 9, in <module>\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     import torch\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 1954, in <module>\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     _C._initExtension(_manager_path())\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 264, in <module>\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     _lazy_call(_check_capability)\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 261, in _lazy_call\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     _queued_calls.append((callable, traceback.format_stack()))\n[2025-03-05 14:28:45,820] [ERROR] [MainThread] [vllm.worker.worker_base] >>> \n[2025-03-05 14:28:45,821] [ERROR] [MainThread] [vllm.worker.worker_base] >>> Error executing method 'init_device'. This might cause deadlock in distributed execution.\n[2025-03-05 14:28:45,821] [ERROR] [MainThread] [vllm.worker.worker_base] >>> Traceback (most recent call last):\n[2025-03-05 14:28:45,821] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 332, in _lazy_init\n[2025-03-05 14:28:45,821] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     queued_call()\n[2025-03-05 14:28:45,821] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 200, in _check_capability\n[2025-03-05 14:28:45,821] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     capability = get_device_capability(d)\n[2025-03-05 14:28:45,821] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 509, in get_device_capability\n[2025-03-05 14:28:45,821] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     prop = get_device_properties(device)\n[2025-03-05 14:28:45,821] [ERROR] [MainThread] [vllm.worker.worker_base] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 527, in get_device_properties\n[2025-03-05 14:28:45,821] [ERROR] [MainThread] [vllm.worker.worker_base] >>>     return _get_device_properties(device)  # type: ignore[name-defined]\n[2025-03-05 14:28:45,821] [ERROR] [MainThread] [vllm.worker.worker_base] >>> RuntimeError: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":49, please report a bug to PyTorch. device=1, num_gpus=\u0001\n[2025-03-05 14:28:45,821] [ERROR] [MainThread] [vllm.worker.worker_base] >>> \n[2025-03-05 14:28:45,821] [ERROR] [MainThread] [vllm.worker.worker_base] >>> The above exception was the direct cause of the following exception:\n```",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-03-05T07:03:58+00:00",
    "closed_at": "2025-03-12T12:02:02+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14263/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14263"
  },
  {
    "number": 6056,
    "title": "[Bug]: debugging guide for device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\"",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nThis is a compond and annoying bug, coupled with pytorch bug https://github.com/pytorch/pytorch/pull/122815 .\r\n\r\nBasically, pytorch `torch.cuda.device_count` function will cache the device count when first called. Users might not call it directly, but if you use `import torch._dynamo` , it will be called. The call chain is:\r\n\r\n```text\r\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/ops/roi_align.py\", line 4, in <module>\r\n    import torch._dynamo\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/__init__.py\", line 2, in <module>\r\n    from . import convert_frame, eval_frame, resume_execution\r\n  File \"<frozen importlib._bootstrap>\", line 1078, in _handle_fromlist\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 40, in <module>\r\n    from . import config, exc, trace_rules\r\n  File \"<frozen importlib._bootstrap>\", line 1078, in _handle_fromlist\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/trace_rules.py\", line 50, in <module>\r\n    from .variables import (\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/__init__.py\", line 4, in <module>\r\n    from .builtin import BuiltinVariable\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builtin.py\", line 42, in <module>\r\n    from .ctx_manager import EventVariable, StreamVariable\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/ctx_manager.py\", line 12, in <module>\r\n    from ..device_interface import get_interface_for_device\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/device_interface.py\", line 198, in <module>\r\n    for i in range(torch.cuda.device_count()):\r\n```\r\n\r\nIn our case, some image processing code will import `torchvision`, which implicitly import `torch._dynamo`:\r\n\r\n```text\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 31, in <module>\r\n    from vllm.multimodal.utils import (async_get_and_parse_image,\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/multimodal/__init__.py\", line 2, in <module>\r\n    from .registry import MultiModalRegistry\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/multimodal/registry.py\", line 10, in <module>\r\n    from .image import (ImageFeatureData, ImageFeaturePlugin, ImagePixelData,\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/multimodal/image.py\", line 10, in <module>\r\n    from vllm.transformers_utils.image_processor import get_image_processor\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/transformers_utils/image_processor.py\", line 3, in <module>\r\n    from transformers import AutoImageProcessor\r\n  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1551, in __getattr__\r\n    value = getattr(module, name)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1550, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1560, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/image_processing_auto.py\", line 27, in <module>\r\n    from ...image_processing_utils import BaseImageProcessor, ImageProcessingMixin\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py\", line 21, in <module>\r\n    from .image_transforms import center_crop, normalize, rescale\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\", line 22, in <module>\r\n    from .image_utils import (\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_utils.py\", line 58, in <module>\r\n    from torchvision.transforms import InterpolationMode\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/__init__.py\", line 6, in <module>\r\n    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils\r\n  File \"<frozen importlib._bootstrap>\", line 1078, in _handle_fromlist\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/models/__init__.py\", line 2, in <module>\r\n    from .convnext import *\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/models/convnext.py\", line 8, in <module>\r\n    from ..ops.misc import Conv2dNormActivation, Permute\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/ops/__init__.py\", line 23, in <module>\r\n    from .poolers import MultiScaleRoIAlign\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/ops/poolers.py\", line 10, in <module>\r\n    from .roi_align import roi_align\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/ops/roi_align.py\", line 4, in <module>\r\n    import torch._dynamo\r\n```\r\n\r\nSince `torch._dynamo` remembers the device count, it registers a hook to initialize all devices after cuda is initialized. If we shrink `CUDA_VISIBLE_DEVICES` later, before we initialize cuda, then `torch._dynamo` will hit this error.\r\n\r\nPyTorch fixes this bug in https://github.com/pytorch/pytorch/pull/122795 .\r\n\r\nHowever, before we upgrade to pytorch 2.4 , we cannot do anything.\r\n\r\nInside vLLM, we already use `vllm.utils.cuda_device_count_stateless` as much as possible. (If you see `torch.cuda.device_count()`, it is a bug, and we should fix it by calling `vllm.utils.cuda_device_count_stateless()` ).\r\n\r\nIf some other library (e.g. `transformers` in this case) accidentally called `torch.cuda.device_count()`, we cannot do anything but defer the `import`, as is done in https://github.com/vllm-project/vllm/pull/6055 .\r\n\r\nHow to find the code to blame? My current approach is to manually insert `import traceback; traceback.print_stack()` inside `torch.cuda.device_count` . Yes, modify pytorch's code, that's it. If it prints a stack trace before we initialize the engine, then we need to find the line to blame.\r\n\r\nAfter deferring all possible lines to blame, we should fix this bug.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-07-02T05:53:06+00:00",
    "closed_at": "2024-07-03T06:37:30+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6056/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/6056"
  },
  {
    "number": 18217,
    "title": "[Bug]: vllm suffer extreme latency or even timeout when enabling lora",
    "body": "### Your current environment\n\ncurrent environment:\nvllm version: `0.8.5.post1`\n```\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 24.04.2 LTS (x86_64)\nGCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.39\n\nPython version: 3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-6.11.0-25-generic-x86_64-with-glibc2.39\nIs CUDA available: True\nCUDA runtime version: 12.0.140\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX 5000 Ada Generation\nGPU 1: NVIDIA RTX 5000 Ada Generation\n\nNvidia driver version: 550.144.03\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               40\nOn-line CPU(s) list:                  0-39\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) w7-3445\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   20\nSocket(s):                            1\nStepping:                             8\nCPU(s) scaling MHz:                   26%\nCPU max MHz:                          4800.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             5184.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            960 KiB (20 instances)\nL1i cache:                            640 KiB (20 instances)\nL2 cache:                             40 MiB (20 instances)\nL3 cache:                             52.5 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-39\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] open_clip_torch==2.31.0\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.3\n[pip3] triton==3.2.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-ml-py              12.570.86                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] open-clip-torch           2.31.0                   pypi_0    pypi\n[conda] pyzmq                     26.2.1                   pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] transformers              4.51.3                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.5.post1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     0-39    0               N/A\nGPU1    SYS      X      0-39    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nTORCHINDUCTOR_CACHE_DIR=/tmp/torchinductor_yanan\nVLLM_WORKER_MULTIPROC_METHOD=spawn\nCUDA_MODULE_LOADING=LAZY\n```\n\n### \ud83d\udc1b Describe the bug\n\nLora modules from `trl sft`, located in path `~/agentbank`\n\n![Image](https://github.com/user-attachments/assets/38c64f3f-942d-45ee-9d28-7cad84f98018)\n\nsft command:\n```bash\ntrl sft \\\n    --model_name_or_path meta-llama/Llama-3.1-8B-Instruct  \\\n    --dataset_name yananchen/agentbank_mixture \\\n    --report_to \"none\" \\\n    --learning_rate 2e-4 \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --output_dir ~/agentbank \\\n    --logging_steps 1 \\\n    --num_train_epochs 4 \\\n    --save_strategy \"epoch\" \\\n    --lr_scheduler_type \"constant\" \\\n    --max_steps -1 \\\n    --gradient_checkpointing \\\n    --logging_strategy \"epoch\" \\\n    --packing True \\\n    --do_eval False \\\n    --overwrite_output_dir True \\\n    --bf16 True \\\n    --bf16_full_eval True \\\n    --max_seq_length 4096 \\\n    --max_length 4096 \\\n    --eval_accumulation_steps 4 \\\n    --use_peft \\\n    --lora_r 16 \\\n    --lora_alpha 16 \\\n    --save_only_model True \\\n    --load_in_8bit \\\n    --attn_implementation 'flash_attention_2'\n```\n\n\n\nand here is the vllm serve command:\n```bash\nvllm serve meta-llama/Llama-3.1-8B-Instruct  \\\n         --api-key \"yyy\" --port 1703 --trust-remote-code --gpu_memory_utilization 0.95 --tensor-parallel-size 1 \\\n         --max-model-len 32000 \\\n         --quantization bitsandbytes --load_format bitsandbytes --enforce_eager \\\n         --enable-lora \\\n         --lora-modules sql-lora=~/agentbank/checkpoint-3891\n```\n\n\nhowever, although the vllm is working, it is extremely slow when calling it thru openai compatitable client. often resulting into `openai.APITimeoutError: Request timed out.`.\n\nIf the lora is not enabled, everything is fine.\n\nany solution ?\n\n![Image](https://github.com/user-attachments/assets/f1d1d57b-4e67-41df-ac3f-5538ada04c6f)\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-05-15T17:47:01+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18217/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/18217"
  },
  {
    "number": 4018,
    "title": "[Bug][ROCm]: Performance issue in ROCm Triton FlashAttention",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: 2.1.1+git011de5c\r\nIs debug build: False\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: 6.0.32830-d62f6a171\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: 17.0.0 (https://github.com/RadeonOpenCompute/llvm-project roc-6.0.0 23483 7208e8d15fbf218deb74483ea8c549c67ca4985e)\r\nCMake version: version 3.29.1\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.19.0-45-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 10.1.243\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: AMD Instinct MI250X/MI250NoGCNArchNameOnOldPyTorch\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: 6.0.32830\r\nMIOpen runtime version: 3.0.0\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   48 bits physical, 48 bits virtual\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nThread(s) per core:              1\r\nCore(s) per socket:              64\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       AuthenticAMD\r\nCPU family:                      25\r\nModel:                           1\r\nModel name:                      AMD EPYC 7713 64-Core Processor\r\nStepping:                        1\r\nFrequency boost:                 enabled\r\nCPU MHz:                         1500.000\r\nCPU max MHz:                     3720.7029\r\nCPU min MHz:                     1500.0000\r\nBogoMIPS:                        3992.21\r\nVirtualization:                  AMD-V\r\nL1d cache:                       4 MiB\r\nL1i cache:                       4 MiB\r\nL2 cache:                        64 MiB\r\nL3 cache:                        512 MiB\r\nNUMA node0 CPU(s):               0-63\r\nNUMA node1 CPU(s):               64-127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.4.1\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.22.4\r\n[pip3] torch==2.1.1+git011de5c\r\n[pip3] torchvision==0.16.1+fdea156\r\n[pip3] triton==2.1.0\r\n[conda] No relevant packagesROCM Version: 6.0.32830-d62f6a171\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\n\n### \ud83d\udc1b Describe the bug\n\nI ran `benchmark_throughput.py` and found that it printed the same tokenizer warning repeatedly:\r\n\r\n```\r\nProcessed prompts:  22%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                                                                    | 218/1000 [02:16<07:27,  1.75it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\nTo disable this warning, you can either:\r\n        - Avoid using `tokenizers` before the fork if possible\r\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\nProcessed prompts:  22%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                                                                                   | 223/1000 [02:17<04:09,  3.11it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\nTo disable this warning, you can either:\r\n        - Avoid using `tokenizers` before the fork if possible\r\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\nProcessed prompts:  22%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                                  | 225/1000 [02:19<05:36,  2.30it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\nTo disable this warning, you can either:\r\n        - Avoid using `tokenizers` before the fork if possible\r\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\nProcessed prompts:  23%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                                                                                  | 227/1000 [02:20<06:48,  1.89it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\nTo disable this warning, you can either:\r\n        - Avoid using `tokenizers` before the fork if possible\r\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\nProcessed prompts:  23%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                                 | 229/1000 [02:21<06:46,  1.89it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\nTo disable this warning, you can either:\r\n        - Avoid using `tokenizers` before the fork if possible\r\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\nProcessed prompts:  23%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                                                                                                | 232/1000 [02:23<05:51,  2.19it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\nTo disable this warning, you can either:\r\n        - Avoid using `tokenizers` before the fork if possible\r\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\nProcessed prompts:  24%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f \r\n```\r\n\r\nThe warning didn't appear when using CK FlashAttention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`. Also, the performance when using Triton FA was much lower than when using CK FA. I guess Triton compiles or auto-tunes the kernel repeatedly for some reason.",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2024-04-11T20:47:59+00:00",
    "closed_at": "2024-09-04T14:05:58+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4018/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4018"
  },
  {
    "number": 15661,
    "title": "[Feature]: distribute the package on macos",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nNow that vLLM can be built directly in macos with `pip install -e .` , we can build the wheel and publish it to pypi, so that users can directly install it.\n\nThis is mostly for testing and development, so that people can develop some pure python code without spinning up a gpu server. This will help lower the barrier of developing vllm.\n\nAlong with it, we should also have some nightly ci to have basic smoke test to make sure it is not broken.\n\ncc @simon-mo for release, and @khluu for ci.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-28T01:51:30+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15661/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/15661"
  },
  {
    "number": 14587,
    "title": "[Bug]: [V1] Beam Search Test Fails on V1",
    "body": "### Your current environment\n\nPer note. It would be great if someone could look into this!\n\n### \ud83d\udc1b Describe the bug\n\n```bash\npytest VLLM_USE_V1=1 tests/samplers/test_beam_search.py\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-11T00:30:02+00:00",
    "closed_at": "2025-07-11T02:15:57+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14587/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14587"
  },
  {
    "number": 19736,
    "title": "[CI Failure]: Samplers Test - samplers/test_beam_search.py::test_beam_search_passes_multimodal_data",
    "body": "### Name of failing test\n\n`samplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half]`\n\n### Basic information\n\n- [ ] Flaky test\n- [x] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\nIt seems the issue is because we are now passing empty lists to _flatten_embeddings\n\n```\nFAILED samplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half] - RuntimeError: torch.cat(): expected a non-empty list of Tensors\n```\n\nFull output:\n```\npytest -s -v \"samplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half]\"\nINFO 06-17 09:19:56 [__init__.py:244] Automatically detected platform cuda.\n/home/mgoin/venvs/vllm/lib/python3.12/site-packages/pytest_asyncio/plugin.py:208: PytestDeprecationWarning: The configuration option \"asyncio_default_fixture_loop_scope\" is unset.\nThe event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))\n============================================================================================ test session starts =============================================================================================\nplatform linux -- Python 3.12.4, pytest-8.3.3, pluggy-1.5.0 -- /home/mgoin/venvs/vllm/bin/python3\ncachedir: .pytest_cache\nhypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/home/mgoin/code/vllm/tests/.hypothesis/examples'))\nrootdir: /home/mgoin/code/vllm\nconfigfile: pyproject.toml\nplugins: forked-1.6.0, subtests-0.14.1, asyncio-0.24.0, shard-0.1.2, buildkite-test-collector-0.1.9, timeout-2.3.1, schemathesis-3.39.15, anyio-4.6.2.post1, mock-3.14.0, hypothesis-6.131.0, rerunfailures-14.0\nasyncio: mode=Mode.STRICT, default_loop_scope=None\ncollected 1 item                                                                                                                                                                                             \nRunning 1 items in this shard: tests/samplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half]\n\nsamplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half] WARNING 06-17 09:19:58 [config.py:3273] Casting torch.bfloat16 to torch.float16.\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  4.90it/s]\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nINFO 06-17 09:20:16 [config.py:831] This model supports multiple tasks: {'generate', 'score', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.\nWARNING 06-17 09:20:16 [config.py:3273] Casting torch.bfloat16 to torch.float16.\nINFO 06-17 09:20:16 [config.py:1444] Using max model len 1024\nINFO 06-17 09:20:16 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.1.dev287+g89b1388d8) with config: model='Qwen/Qwen2-Audio-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-Audio-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2-Audio-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \nINFO 06-17 09:20:18 [cuda.py:336] Using Flash Attention backend.\nINFO 06-17 09:20:18 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nINFO 06-17 09:20:18 [model_runner.py:1171] Starting to load model Qwen/Qwen2-Audio-7B-Instruct...\nINFO 06-17 09:20:19 [weight_utils.py:292] Using model weights format ['*.safetensors']\nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:02,  1.59it/s]\nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:02,  1.49it/s]\nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:02<00:01,  1.44it/s]\nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:02<00:00,  1.48it/s]\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:02<00:00,  1.88it/s]\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:02<00:00,  1.68it/s]\n\nINFO 06-17 09:20:22 [default_loader.py:272] Loading weights took 3.00 seconds\nINFO 06-17 09:20:22 [model_runner.py:1203] Model loading took 15.6455 GiB and 3.447517 seconds\nINFO 06-17 09:20:25 [worker.py:294] Memory profiling takes 2.68 seconds\nINFO 06-17 09:20:25 [worker.py:294] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.90) = 71.27GiB\nINFO 06-17 09:20:25 [worker.py:294] model weights take 15.65GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.51GiB; the rest of the memory reserved for KV Cache is 55.11GiB.\nINFO 06-17 09:20:25 [executor_base.py:113] # cuda blocks: 7054, # CPU blocks: 512\nINFO 06-17 09:20:25 [executor_base.py:118] Maximum concurrency for 1024 tokens per request: 110.22x\nINFO 06-17 09:20:27 [model_runner.py:1513] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nCapturing CUDA graph shapes:   0%|                                                                                                                     | 0/35 [00:00<?, ?it/s]\nFAILED\n\n================================================================================== FAILURES ==================================================================================\n__________________________________________________________ test_beam_search_passes_multimodal_data[False-2-64-half] __________________________________________________________\n\nhf_runner = <class 'tests.conftest.HfRunner'>, vllm_runner = <class 'tests.conftest.VllmRunner'>, dtype = 'half', max_tokens = 64, beam_width = 2\n\n    @pytest.mark.parametrize(\"dtype\", [\"half\"])\n    @pytest.mark.parametrize(\"max_tokens\", MAX_TOKENS)\n    @pytest.mark.parametrize(\"beam_width\", MM_BEAM_WIDTHS)\n    def test_beam_search_passes_multimodal_data(\n        hf_runner,\n        vllm_runner,\n        dtype: str,\n        max_tokens: int,\n        beam_width: int,\n    ) -> None:\n        \"\"\"Ensure that beam search passes multimodal data through correctly.\"\"\"\n        # NOTE - this test is primarily to check that mm data is passed to beams\n        # correctly. As such, we just need to check one extra modality to make\n        # sure things pass through properly.\n        audios = [AudioAsset(\"mary_had_lamb\").audio_and_sample_rate]\n        model = \"Qwen/Qwen2-Audio-7B-Instruct\"\n        audio_seq = \"<|audio_bos|><|AUDIO|><|audio_eos|>\"\n        prompts = [\n            f\"<|im_start|>user\\n{audio_seq}Can you transcribe this?<|im_end|>\\n<|im_start|>assistant\\n\"  #noqa: E501\n        ]\n    \n        with hf_runner(model, dtype=dtype,\n                       auto_cls=AutoModelForSeq2SeqLM) as hf_model:\n            audio_token_id = hf_model.config.audio_token_index\n            eos_token_id = hf_model.tokenizer.eos_token_id  # <|im_end|>\n            hf_outputs = hf_model.generate_beam_search(\n                prompts,\n                beam_width=beam_width,\n                max_tokens=max_tokens,\n                audios=audios,\n            )\n    \n>       with vllm_runner(model, dtype=dtype) as vllm_model:\n\nsamplers/test_beam_search.py:102: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nconftest.py:782: in __init__\n    self.model = LLM(\n../vllm/entrypoints/llm.py:262: in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n../vllm/engine/llm_engine.py:501: in from_engine_args\n    return engine_cls.from_vllm_config(\n../vllm/engine/llm_engine.py:477: in from_vllm_config\n    return cls(\n../vllm/engine/llm_engine.py:268: in __init__\n    self._initialize_kv_caches()\n../vllm/engine/llm_engine.py:426: in _initialize_kv_caches\n    self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n../vllm/executor/executor_base.py:124: in initialize_cache\n    self.collective_rpc(\"initialize_cache\",\n../vllm/executor/uniproc_executor.py:57: in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n../vllm/utils.py:2690: in run_method\n    return func(*args, **kwargs)\n../vllm/worker/worker.py:335: in initialize_cache\n    self._warm_up_model()\n../vllm/worker/worker.py:365: in _warm_up_model\n    self.model_runner.capture_model(self.gpu_cache)\n../../../venvs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py:116: in decorate_context\n    return func(*args, **kwargs)\n../vllm/worker/model_runner.py:1658: in capture_model\n    graph_runner.capture(**capture_inputs)\n../vllm/worker/model_runner.py:2059: in capture\n    self.model(\n../../../venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1751: in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n../../../venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: in _call_impl\n    return forward_call(*args, **kwargs)\n../vllm/model_executor/models/qwen2_audio.py:389: in forward\n    inputs_embeds = self.get_input_embeddings(input_ids,\n../vllm/model_executor/models/qwen2_audio.py:368: in get_input_embeddings\n    inputs_embeds = merge_multimodal_embeddings(\n../vllm/model_executor/models/utils.py:498: in merge_multimodal_embeddings\n    return _merge_multimodal_embeddings(\n../vllm/model_executor/models/utils.py:411: in _merge_multimodal_embeddings\n    flattened = _flatten_embeddings(multimodal_embeddings)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nembeddings = []\n\n    def _flatten_embeddings(embeddings: NestedTensors) -> torch.Tensor:\n        \"\"\"\n        Recursively flattens and concatenates NestedTensors on all but the last\n        dimension.\n        \"\"\"\n    \n        if isinstance(embeddings, torch.Tensor):\n            # Flatten all but the last dimension.\n            return embeddings.flatten(0, -2)\n    \n>       return torch.cat(tuple(_flatten_embeddings(t) for t in embeddings))\nE       RuntimeError: torch.cat(): expected a non-empty list of Tensors\n\n../vllm/model_executor/models/utils.py:363: RuntimeError\n============================================================================== warnings summary ==============================================================================\n../../../venvs/vllm/lib/python3.12/site-packages/schemathesis/generation/coverage.py:305\n  /home/mgoin/venvs/vllm/lib/python3.12/site-packages/schemathesis/generation/coverage.py:305: DeprecationWarning: jsonschema.exceptions.RefResolutionError is deprecated as of version 4.18.0. If you wish to catch potential reference resolution errors, directly catch referencing.exceptions.Unresolvable.\n    ref_error: type[Exception] = jsonschema.RefResolutionError,\n\ntests/samplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half]\n  /home/mgoin/venvs/vllm/lib/python3.12/site-packages/librosa/core/intervals.py:15: DeprecationWarning: path is deprecated. Use files() instead. Refer to https://importlib-resources.readthedocs.io/en/latest/using.html#migrating-from-legacy for migration advice.\n    with resources.path(\"librosa.core\", \"intervals.msgpack\") as imsgpack:\n\ntests/samplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half]\n  /home/mgoin/venvs/vllm/lib/python3.12/site-packages/audioread/rawread.py:16: DeprecationWarning: 'aifc' is deprecated and slated for removal in Python 3.13\n    import aifc\n\ntests/samplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half]\n  /home/mgoin/venvs/vllm/lib/python3.12/site-packages/audioread/rawread.py:17: DeprecationWarning: 'audioop' is deprecated and slated for removal in Python 3.13\n    import audioop\n\ntests/samplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half]\n  /home/mgoin/venvs/vllm/lib/python3.12/site-packages/audioread/rawread.py:19: DeprecationWarning: 'sunau' is deprecated and slated for removal in Python 3.13\n    import sunau\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n========================================================================== short test summary info ===========================================================================\nFAILED samplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half] - RuntimeError: torch.cat(): expected a non-empty list of Tensors\n======================================================================= 1 failed, 5 warnings in 29.93s =======================================================================\n```\n\n### \ud83d\udcdd History of failing test\n\nThis was introduced by https://github.com/vllm-project/vllm/pull/19446\n\nhttps://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests/1f2a99b2-fbc9-89fc-a08f-c4d431429893?period=7days&tags=scm.branch%3Amain\n<img width=\"1213\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/843cfb35-53b6-4db2-b326-c65e1226442b\" />\n\nIt was not caught because the test wasn't triggered by the change I believe.\n\n\n### CC List.\n\n@russellb ",
    "labels": [
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-06-17T09:23:35+00:00",
    "closed_at": "2025-06-18T22:48:30+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19736/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/19736"
  },
  {
    "number": 8683,
    "title": "[Feature]: improve distributed backend selection",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nWe have three ways to start a new process:\r\n\r\n- multiprocessing by `fork`\r\n- multiprocessing by `spawn`\r\n- ray\r\n\r\nby default, we use ray for multi-node serving, and multiprocessing by `fork` for single node setting.\r\n\r\nhowever, if users initialize cuda context, multiprocessing by `fork` will not work.\r\n\r\nif we set multiprocessing by `spawn` by default, it will not work when users don't have `if __name__ == \"__main__\"`. \r\n\r\nif we can figure out whether users have `if __name__ == \"__main__\"` automatically, we can improve the default user experience.\r\n\r\nthe proposed solution is:\r\n\r\nif we find that cuda is initialized, we inspect the current function call stack, and trace back the stack until we reach the `__main__` module, check the current line to see if we are under `if __name__ == \"__main__\"`, if yes, switch the multiprocessing method from `fork` to `spawn`.\r\n\r\ncc @russellb \n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-09-20T22:54:29+00:00",
    "closed_at": "2024-09-29T01:17:08+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8683/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/8683"
  },
  {
    "number": 14876,
    "title": "[Tracking Issue]: Multi-modal model requests",
    "body": "Moved to https://github.com/orgs/vllm-project/projects/10",
    "labels": [
      "new-model",
      "multi-modality"
    ],
    "state": "closed",
    "created_at": "2025-03-16T02:14:05+00:00",
    "closed_at": "2025-03-16T13:01:04+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14876/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/14876"
  },
  {
    "number": 5520,
    "title": "[RFC]: Usage Data Enhancement for v0.5.*",
    "body": "### Motivation.\n\nvLLM currently has a usage reporting feature https://docs.vllm.ai/en/stable/serving/usage_stats.html to inform us what features can be safely deprecated or what hardware to improve performance on.\r\n\r\nAfter v0.5.0, vLLM has various features that's being tested (chunked prefill, prefix caching, spec decode, fp8, and VLM), we would like to start gathering statistics on the usage of these features with different hardware and model types so we know what we are tested on. \n\n### Proposed Change.\n\nAdd the following data to `usage_lib`\r\n* `--enable-chunked-prefill`\r\n* `--enable-prefix-cache`\r\n* `speculative_model` (need model architecture/size or [ngram])\r\n\r\nAnother missing value from previous data is the size of the model, so we find it difficult to compare llama3 8b vs 70b. This might require some creative way to find the size of the model without capturing too much information. \r\n\r\nAny other suggestion welcomed.\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n_No response_\n\n### Any Other Things.\n\n_No response_",
    "labels": [
      "RFC",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-06-14T01:26:36+00:00",
    "closed_at": "2024-11-27T02:06:47+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5520/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/5520"
  },
  {
    "number": 15452,
    "title": "[Bug]: Build error, nvcc error   : 'ptxas' died due to signal 11 (Invalid memory reference)",
    "body": "### Your current environment\n\nL20, CUDA 12.6, PyTorch 2.6.0\n\n### \ud83d\udc1b Describe the bug\n\n```bash\nFAILED: vllm-flash-attn/CMakeFiles/_vllm_fa3_C.dir/hopper/instantiations/flash_fwd_hdimall_fp16_paged_split_sm90.cu.o\n/usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DFLASHATTENTION_DISABLE_BACKWARD -DFLASHATTENTION_DISABLE_DROPOUT -DFLASHATTENTION_DISABLE_PYBIND -DFLASHATTENTION_DISABLE_UNEVEN_K -DFLASHATTENTION_VARLEN_ONLY -DPy_LIMITED_API=3 -DTORCH_EXTENSION_NAME=_vllm_fa3_C -DUSE_C10D_GLOO -DUSE_C10D_NCCL -DUSE_DISTRIBUTED -DUSE_RPC -DUSE_TENSORPIPE -D_vllm_fa3_C_EXPORTS -I/workspace/dev/vipshop/vllm/.deps/vllm-flash-attn-src/csrc -I/workspace/dev/vipshop/vllm/.deps/vllm-flash-attn-src/hopper -I/workspace/dev/vipshop/vllm/.deps/vllm-flash-attn-src/csrc/common -I/workspace/dev/vipshop/vllm/.deps/vllm-flash-attn-src/csrc/cutlass/include -isystem /usr/include/python3.12 -isystem /usr/local/lib/python3.12/dist-packages/torch/include -isystem /usr/local/lib/python3.12/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -DONNX_NAMESPACE=onnx_c2 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda -O3 -g -DNDEBUG -std=c++17 -Xcompiler=-fPIC --expt-relaxed-constexpr -DENABLE_FP8 --threads=1 --expt-extended-lambda --use_fast_math -DCUTLASS_ENABLE_DIRECT_CUDA_DRIVER_CALL=1 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_90a,code=sm_90a -gencode arch=compute_80,code=sm_80 -MD -MT vllm-flash-attn/CMakeFiles/_vllm_fa3_C.dir/hopper/instantiations/flash_fwd_hdimall_fp16_paged_split_sm90.cu.o -MF vllm-flash-attn/CMakeFiles/_vllm_fa3_C.dir/hopper/instantiations/flash_fwd_hdimall_fp16_paged_split_sm90.cu.o.d -x cu -c /workspace/dev/vipshop/vllm/.deps/vllm-flash-attn-src/hopper/instantiations/flash_fwd_hdimall_fp16_paged_split_sm90.cu -o vllm-flash-attn/CMakeFiles/_vllm_fa3_C.dir/hopper/instantiations/flash_fwd_hdimall_fp16_paged_split_sm90.cu.o\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-25T08:58:06+00:00",
    "closed_at": "2025-03-25T09:40:51+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15452/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15452"
  },
  {
    "number": 113,
    "title": "Check whether the input request is too long",
    "body": null,
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-05-20T23:02:26+00:00",
    "closed_at": "2024-04-10T08:47:01+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/113/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/113"
  },
  {
    "number": 5693,
    "title": "[Bug]: vision chat completion output with odd Instruction/Output prompting.",
    "body": "### Your current environment\n\n```text\r\ngit clone https://github.com/vllm-project/vllm.git\r\ncd ~/vllm\r\nconda create -n vllm -y\r\nconda activate vllm\r\nconda install python=3.10 -y\r\npip install -e .\r\npip install hf_transfer\r\npip install torchvision\r\n```\r\nlatest main afed90a0344b1b0ce6aae46efc630adb489ec769\r\n\r\nrun:\r\n```\r\nexport NCCL_IGNORE_DISABLED_P2P=1\r\nexport CUDA_VISIBLE_DEVICES=5\r\npython -m vllm.entrypoints.openai.api_server --port=5063 \\\r\n      --host=0.0.0.0 --model microsoft/Phi-3-vision-128k-instruct \\\r\n      --tensor-parallel-size=1 --seed 1234 \\\r\n      --max-num-batched-tokens=8192        \\\r\n      --trust-remote-code \\\r\n      --tensor-parallel-size=1 \\\r\n      --max-num-batched-tokens=131072 --max-log-len=100 \\\r\n      --image-input-type=pixel_values \\\r\n      --image-token-id=32044 \\\r\n      --image-input-shape=\"1,3,1008,1344\" \\\r\n      --image-feature-size=1921 \\\r\n      --download-dir=$HOME/.cache/huggingface/hub &> vllm_phi3_vision.log &\r\n```\n\n### \ud83d\udc1b Describe the bug\n\n```\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(base_url='http://localhost:5063/v1')\r\n\r\nmessages1 = [\r\n    {\r\n        'role': 'user',\r\n        'content': [\r\n            {'type': 'text', 'text': 'What do you see?'},\r\n            {'type': 'image_url',\r\n             'image_url': {\r\n                'url': 'data:image/jpeg;base64,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',\r\n                }\r\n             },\r\n        ],\r\n    }\r\n]\r\n\r\nmessages2 = [\r\n    {\r\n        \"role\": \"user\",\r\n        \"content\": [\r\n            {\"type\": \"text\", \"text\": \"What\u2019s in this image?\"},\r\n            {\r\n                \"type\": \"image_url\",\r\n                \"image_url\": {\r\n                    \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\r\n                },\r\n            },\r\n        ],\r\n    }\r\n]\r\n\r\nmessages = messages1\r\n\r\nresponse = client.chat.completions.create(\r\n    model=\"microsoft/Phi-3-vision-128k-instruct\",\r\n    messages=messages,\r\n    max_tokens=300,\r\n)\r\n\r\nprint(response.choices[0])\r\n```\r\n\r\nWhile the latter \"messages2\" works, the former does not.  It leads to:\r\n\r\n```\r\n .\\n Instruction: How would you express the content of this image succinctly?\\nOutput:  a long exposure shot of the big ben at night \\n'\r\n```\r\n\r\nSo it sees the image, but the response is all messed up in terms of prompting.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-06-19T21:12:05+00:00",
    "closed_at": "2024-06-25T06:57:56+00:00",
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5693/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5693"
  },
  {
    "number": 2571,
    "title": "Slow build speed with punica after #1804",
    "body": "After #1804, the build speed of vLLM becomes significantly slower. Most of the time was spent on `/home/zhuohan/vllm/vllm/csrc/punica/bgmv/bgmv_all.cu`. Should we turn off punica by default?\r\n\r\ncc @Yard1 ",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-24T00:44:29+00:00",
    "closed_at": "2024-01-29T13:29:22+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2571/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/2571"
  },
  {
    "number": 13435,
    "title": "[Bug][V1]: TP is broken when torch compile cache is used",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n\nGot the error message when using tp_size=4:\n```\n(VllmWorker rank=2 pid=2307184) ERROR 02-17 14:48:01 multiproc_executor.py:374] ValueError: Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)\n```\n**Importantly, the bug doesn't happen when the torch.compile cache is not used.**\n\nThe error raises at the first torch.compile-generated op for the embedding layer:\n```python\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((s0, 4096), (4096, 1), torch.bfloat16)\n        # Topologically Sorted Source Nodes: [ge, lt, and_, ge_1, lt_1, and__1, or_, masked_fill_, mul, mul_1, add, sub, mul_2, embedding], Original ATen: [aten.ge, aten.lt, aten.bitwise_and, aten.bitwise_or, aten.masked_fill, aten.mul, aten.add, aten.sub, aten.embedding]\n        triton_poi_fused_add_bitwise_and_bitwise_or_embedding_ge_lt_masked_fill_mul_sub_0_xnumel = 4096*s0\n        stream0 = get_raw_stream(0)\n        triton_poi_fused_add_bitwise_and_bitwise_or_embedding_ge_lt_masked_fill_mul_sub_0.run(arg0_1, arg2_1, buf0, triton_poi_fused_add_bitwise_and_bitwise_or_embedding_ge_lt_masked_fill_mul_sub_0_xnumel, grid=grid(triton_poi_fused_add_bitwise_and_bitwise_or_embedding_ge_lt_masked_fill_mul_sub_0_xnumel), stream=stream0)\n```\n\nHere, the input arguments (`arg0_1` and `arg2_1`, which correspond to input activations and weights) live in `cuda:{rank}`, while the output tensor (`buf0`) lives in `cuda:0` regardless of the actual ranks.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-02-17T22:58:05+00:00",
    "closed_at": "2025-02-18T04:33:47+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13435/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/13435"
  },
  {
    "number": 12274,
    "title": "[Performance]: why vllm-0.6.1.post2 faster than latest vllm=0.6.6.post1?",
    "body": "### Your current environment\n\nold: vllm-0.6.1.post2 \nnew:  vllm=0.6.6.post1\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nI used the previous version of vllm-0.6.1.post2 and I did benchmark to get the max TTFT and then I upgrade to the latest vllm=0.6.6.post1, but when I did the benchmark again I see a huge difference in the performance regarding the TTFT!\n\nbenchmarking llama3.1-70b-awq model, with 20 1k request on 4gpus, the max TTFT was 10 seconds for the previous vllm but with the latest vllm it increased to be 37 seconds!!\n\nAny thoughts why this huge difference in TTFT here? Do I miss any configs or args to be set to make it faster?\n\nThanks.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-21T18:36:39+00:00",
    "closed_at": "2025-05-23T02:10:28+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12274/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12274"
  },
  {
    "number": 18197,
    "title": "[Usage]: Don't placement_group support NPU?",
    "body": "https://github.com/vllm-project/vllm-ascend/issues/870\n",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-05-15T09:33:50+00:00",
    "closed_at": "2025-05-24T02:34:06+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18197/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/18197"
  }
]
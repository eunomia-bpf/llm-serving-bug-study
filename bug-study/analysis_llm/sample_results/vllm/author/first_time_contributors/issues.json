[
  {
    "number": 5576,
    "title": "[Bug]: Installation Issue with torch Version Conflict on vllm v0.5.0.post1",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.1\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.2 LTS (x86_64)\r\nGCC version: (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.12.3 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:46:43) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-165-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A800-SXM4-80GB\r\nNvidia driver version: 535.54.03\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             64\r\nOn-line CPU(s) list:                0-63\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8358P CPU @ 2.60GHz\r\nCPU family:                         6\r\nModel:                              106\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 32\r\nSocket(s):                          2\r\nStepping:                           6\r\nCPU max MHz:                        3400.0000\r\nCPU min MHz:                        800.0000\r\nBogoMIPS:                           5200.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          3 MiB (64 instances)\r\nL1i cache:                          2 MiB (64 instances)\r\nL2 cache:                           80 MiB (64 instances)\r\nL3 cache:                           96 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-31\r\nNUMA node1 CPU(s):                  32-63\r\nVulnerability Gather data sampling: Mitigation; Microcode\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT disabled\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.3.1\r\n[pip3] torchaudio==2.3.1\r\n[pip3] torchvision==0.18.1\r\n[conda] blas                      1.0                         mkl    defaults\r\n[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\r\n[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\r\n[conda] mkl                       2023.1.0         h213fc3f_46344    defaults\r\n[conda] mkl-service               2.4.0           py312h5eee18b_1    defaults\r\n[conda] mkl_fft                   1.3.8           py312h5eee18b_0    defaults\r\n[conda] mkl_random                1.2.4           py312hdb19cb5_0    defaults\r\n[conda] numpy                     1.26.4          py312hc5e2394_0    defaults\r\n[conda] numpy-base                1.26.4          py312h0da6c21_0    defaults\r\n[conda] pytorch                   2.3.1           py3.12_cuda12.1_cudnn8.9.2_0    pytorch\r\n[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch\r\n[conda] pytorch-mutex             1.0                        cuda    pytorch\r\n[conda] torchaudio                2.3.1               py312_cu121    pytorch\r\n[conda] torchvision               0.18.1              py312_cu121    pytorch\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      SYS     SYS     32-63   1               N/A\r\nNIC0    SYS      X      PIX\r\nNIC1    SYS     PIX      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_bond_0\r\n```\n\n### \ud83d\udc1b Describe the bug\n\nHi,\r\n\r\nI encountered an issue while installing the latest release of vllm ([v0.5.0.post1](https://github.com/vllm-project/vllm/releases/tag/v0.5.0.post1)), which now supports `torch 2.3.0`. When executing the command `pip install vllm`, I received the following error:\r\n```\r\npip subprocess to install build dependencies did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [9 lines of output]\r\n      Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\n      Collecting ninja\r\n        Using cached https://pypi.tuna.tsinghua.edu.cn/packages/6d/92/8d7aebd4430ab5ff65df2bfee6d5745f95c004284db2d8ca76dcbfd9de47/ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\r\n      Collecting packaging\r\n        Using cached https://pypi.tuna.tsinghua.edu.cn/packages/08/aa/cc0199a5f0ad350994d660967a8efb233fe0416e4639146c089643407ce6/packaging-24.1-py3-none-any.whl (53 kB)\r\n      Collecting setuptools>=49.4.0\r\n        Using cached https://pypi.tuna.tsinghua.edu.cn/packages/de/88/70c5767a0e43eb4451c2200f07d042a4bcd7639276003a9c54a68cfcc1f8/setuptools-70.0.0-py3-none-any.whl (863 kB)\r\n      ERROR: Could not find a version that satisfies the requirement torch==2.1.2 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1)\r\n      ERROR: No matching distribution found for torch==2.1.2\r\n      [end of output]\r\n```\r\n\r\n\r\nInterestingly, the `requirements-cuda.txt` specifies `torch >= 2.3.0`. It appears that there might be a version conflict with some dependencies, possibly with `ninja`. Could you help me understand the source of this issue?\r\n\r\nThank you!\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-06-16T07:08:06+00:00",
    "closed_at": "2025-02-11T16:37:08+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5576/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5576"
  },
  {
    "number": 16715,
    "title": "[Installation]: Kimi-VL-A3B failed to be deployed using vllm mirroring",
    "body": "### Your current environment\n\nmodel\uff1aKimi-VL-A3B-Thinking\nimage\uff1avllm-openai\uff1alatest    \nvllm version:0.8.4\n\n1.docker pull vllm/vllm-openai\n\u91cc\u9762\u7684vllm version:0.8.4\n2.docker run --gpus all -v /mnt/data1/LargeLanguageModels/qwen:/model --ipc=host --network=host  --name kimi-vl -it --entrypoint vllm/vllm-openai \uff1alatest  bash\n3. \u5728\u5bb9\u5668\u4e2d\u5982\u4e0b\u547d\u4ee4\u542f\u52a8\u5927\u6a21\u578b\n\n> CUDA_VISIBLE_DEVICES=0 python -m vllm.entrypoints.openai.api_server \\\n\n    --port 3000 \\\n    --served-model-name kimi-vl \\\n    --trust-remote-code \\\n    --model /models/Kimi-VL-A3B-Thinking/Kimi-VL-A3B-Thinking \\\n    --tensor-parallel-size 1 \\\n    --max-num-batched-tokens 131072 \\\n    --max-model-len 131072 \\\n    --max-num-seqs 512 \\\n    --limit-mm-per-prompt image=256 \\\n    --disable-mm-preprocessor-cache\n\n\u51fa\u73b0\u62a5\u9519\n\n> Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/gwm-tmp/kimi_vl/vllm/vllm/entrypoints/openai/api_server.py\", line 1121, in <module>\n    uvloop.run(run_server(args))\n  File \"/gwm-tmp/kimi_vl/venv/lib/python3.10/site-packages/uvloop/__init__.py\", line 82, in run\n    return loop.run_until_complete(wrapper())\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/gwm-tmp/kimi_vl/venv/lib/python3.10/site-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n  File \"/gwm-tmp/kimi_vl/vllm/vllm/entrypoints/openai/api_server.py\", line 1069, in run_server\n    async with build_async_engine_client(args) as engine_client:\n  File \"/usr/local/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/gwm-tmp/kimi_vl/vllm/vllm/entrypoints/openai/api_server.py\", line 146, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/usr/local/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/gwm-tmp/kimi_vl/vllm/vllm/entrypoints/openai/api_server.py\", line 166, in build_async_engine_client_from_engine_args\n    vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n  File \"/gwm-tmp/kimi_vl/vllm/vllm/engine/arg_utils.py\", line 1169, in create_engine_config\n    model_config = self.create_model_config()\n  File \"/gwm-tmp/kimi_vl/vllm/vllm/engine/arg_utils.py\", line 1057, in create_model_config\n    return ModelConfig(\n  File \"/gwm-tmp/kimi_vl/vllm/vllm/config.py\", line 413, in __init__\n    self.multimodal_config = self._init_multimodal_config(\n  File \"/gwm-tmp/kimi_vl/vllm/vllm/config.py\", line 486, in _init_multimodal_config\n    raise ValueError(\"`limit_mm_per_prompt` is only supported for \"\nValueError: `limit_mm_per_prompt` is only supported for multimodal models.\n\n\n4.\u6211\u53bb\u6389\u4e86`limit_mm_per_prompt`\u53c2\u6570\u540e\u7528\u5982\u4e0b\u547d\u4ee4\u591a\u51fa\u5c1d\u8bd5\u5168\u662f\u5982\u4e0b\u62a5\u9519\n\u9996\u5148\u662fvllm\u955c\u50cf\u7f3a\u5c11blobfile\uff0c\u624b\u52a8\u5bfc\u5165\u5b89\u88c5\u89e3\u51b3\n\n> CUDA_VISIBLE_DEVICES=3  python3 -m vllm.entrypoints.openai.api_server --port 8888 --served-model-name kimi-vl --trust-remote-code --model moonshotai/Kimi-VL-A3B-Instruct --tensor-parallel-size 1 --max-num-batched-tokens 131072 --max-model-len 131072 --max-num-seqs 512  \n\n> VLLM_USE_V1_0 CUDA_VISIBLE_DEVICES=3  python3 -m vllm.entrypoints.openai.api_server --port 8888 --served-model-name kimi-vl --trust-remote-code --model moonshotai/Kimi-VL-A3B-Instruct --tensor-parallel-size 1 --max-num-batched-tokens 131072 --max-model-len 131072 --max-num-seqs 512\n\n> VLLM_USE_V1_0 CUDA_VISIBLE_DEVICES=3  python3 -m vllm.entrypoints.openai.api_server --port 8888 --served-model-name kimi-vl --trust-remote-code --model moonshotai/Kimi-VL-A3B-Instruct --tensor-parallel-size 1 \n\n\n> CUDA_VISIBLE_DEVICES=3  python3 -m vllm.entrypoints.openai.api_server --port 8888 --served-model-name kimi-vl --trust-remote-code --model moonshotai/Kimi-VL-A3B-Instruct --tensor-parallel-size 1 \n\n\u62a5\u9519\u5982\u4e0b\n> ERROR 04-16 02:31:40[engine.py:448] Traceback (most recent call last):\nERROR 04-16 02:31:40[engine.py:448]File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/mult iprocess ing/engine .py\", line 436, in run_mp_engine\nERROR 04-16 02:31:40[engine.py:448]engine = MQLLMEngine,from_yllm_config(\nERROR 04-16 02:31:40[engine.py:448]^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-16 02:31:40[engine.py:448]File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/mult iprocess ing/engine .py\", line 128, in from_vllm_config\nERROR 04-16 02:31:40[engine.py:448] return cls(^^^ ERROR 04-16 02:31:40[engine.py:448]\nERROR 04-16 02:31:40[engine.py:448]3File\"/usr/local/lib/python3.12/dist-packages/vllm/engine/mult iprocess ing/engine.py\",line 82, in __init_ ERROR 04-16 02:31:40 ERROR 04-16 02:31:40[engine.py:448[engine.py:448] self,engine = LLMEngine(*args, **kwargs)\n2025-04-16 17:33215000-0813815712-0014101E8791\nERROR 04-16 02:31:40[engine.py:448]File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/lln_engine .py\", line 282, in _init_\nERROR 04-16 02:31:40 ERROR 04-16 02:31:40[engine.py:448][engine.py:448] self.model_executor = executor_class(vllm config=viim canfia.)\nERROR 04-16 02:31:40 ERROR 04-16 02:31:40[engine.py:448][engine.py:448] self._init_executor( ) File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 52, in _init_\nERROR 04-16 02:31:40[engine.py:448]File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\nERROR 04-16 02:31:40[engine.py:448]answerself.collective_rpc(\"load_model\" )File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor .py\", line 56, in collective_rpc\nERROR 04-16 02:31:40[engine.py:448]\nERROR 04-16 02:31:40[engine.py:448]run method(self.driver_worker, method, args,kwargs )\nERROR 04-16 02:31:40[engine.py:448]\nERROR 04-16 02:31:40[engine.py:448]File \"/usr/local/lib/python3.12/dist-packages/vllm/utiis.py . line 2378, in run_method\nERROR 04-16 02:31:40[engine.py:448]return func(*args,**kwargs)\nERROR 04-16 02:31:40[engine.py:448\nERROR 04-16 02:31:40[engine.py:448]File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 183, in load_model\nERROR 04-16 02:31:40[engine.py:448self.model_runner.load model()\nERROR 04-16 02:31:40[engine.py:448]File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/mddel_runner.py\", line 1113, in load_model\nERROR 04-16 02:31:40[engine.py:448]self.model = get_model(vllm_config=solf.vllm_config)\nERROR 04-16 02:31:40ERROR 04-16 02:31:40ERROR 04-16 02:31:40[engine,py:448][engine.py:448[engine.py:448return loader.load model(vilm config=vllm config)File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/_init_.py\". line 14, in get_model\n843315712-001410188791\n-04-16 17:33\nERROR 04-16 02:31:40ERROR 04-16 02:31:40[engine.py:448[engine.py:448\u300cengine.py:448File \"/usr/local/lib/python3. 12/dist-packages/vllm/model_executor/model_loader/loader.py\". line 452. in load_model\nERROR 04-16 02:31:40model=initiallize model(yllm config=vlim config)\nERROR 04-16 02:31:40[engine,py:448\nERROR 04-16 02:31:40[engine.py:448File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 123, in _initialize_model\nERROR 04-16 02:31:40[engine.py:448model_class,_ = get_model_architecture(modol_config)\nERROR 04-16 02:31:40ERROR 04-16 02:31:40ERROR 04-16 02:31:40ERROR 04-16 02:31:40[eng ine.py:448[engine.py:448[engine.py:448[engine.py:448n^^^^^^~^^^^^^^^^^^^^^^^^^^^^^^^^architectures = resolve_transformers_arch(model_config, architectures)File */usr/local/lib/python3.12/dist-packages/vllm/model_executor/mode?_loader/utils.py\", line 104, in get_model_architecture\nERROR 04-16 02:31:40[engine .py:448File */usr/local/lib/python3. 12/dist-packages/vllm/model_executor/model_loader/utils.py\", line 72, in resolve_transformers_arch\nERROR 04-16 02:31:40ERROR 04-16 02:31:40[engine,py:448][engine.py:448]raise ValueError(valueError: KimiyLForConditionalGeneration has no vLLM implementation and the Transformers implementation is not compatible with vLLM. Try settin\n9 VLLM USE_V1=0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### How you are installing vllm\n\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2025-04-16T10:11:58+00:00",
    "closed_at": "2025-04-17T12:53:32+00:00",
    "comments": 28,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16715/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16715"
  },
  {
    "number": 705,
    "title": "ImportError: cannot import name 'activation_ops'",
    "body": "I cloned the repository",
    "labels": [],
    "state": "closed",
    "created_at": "2023-08-08T13:39:56+00:00",
    "closed_at": "2023-09-11T15:55:40+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/705/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/705"
  },
  {
    "number": 14500,
    "title": "[Bug]:  ValueError: Expected a torch.device with a specified index or an integer, but got:cuda",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 03-09 02:38:05 [__init__.py:264] Automatically detected platform rocm.\nWARNING 03-09 02:38:05 [rocm.py:25] Failed to import from amdsmi with ModuleNotFoundError(\"No module named 'amdsmi'\")\nCollecting environment information...\nPyTorch version: 2.4.0+rocm6.3.4.git7cecbf6d\nIs debug build: False\nCUDA used to build PyTorch: N/A\nROCM used to build PyTorch: 6.3.42134-a9a80e791\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (conda-forge gcc 12.1.0-17) 12.1.0\nClang version: Could not collect\nCMake version: version 3.31.6\nLibc version: glibc-2.35\n\nPython version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: AMD Radeon RX 7900 XT (gfx1100)\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\nHIP runtime version: 6.3.42134\nMIOpen runtime version: 3.3.0\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        39 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               20\nOn-line CPU(s) list:                  0-19\nVendor ID:                            GenuineIntel\nModel name:                           13th Gen Intel(R) Core(TM) i5-13600KF\nCPU family:                           6\nModel:                                183\nThread(s) per core:                   2\nCore(s) per socket:                   10\nSocket(s):                            1\nStepping:                             1\nBogoMIPS:                             6988.80\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves avx_vnni umip waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize flush_l1d arch_capabilities\nVirtualization:                       VT-x\nHypervisor vendor:                    Microsoft\nVirtualization type:                  full\nL1d cache:                            480 KiB (10 instances)\nL1i cache:                            320 KiB (10 instances)\nL2 cache:                             20 MiB (10 instances)\nL3 cache:                             24 MiB (1 instance)\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Vulnerable: No microcode\nVulnerability Retbleed:               Mitigation; Enhanced IBRS\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] pytorch-triton-rocm==3.0.0+rocm6.3.4.git75cc27c2\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.4.0+rocm6.3.4.git7cecbf6d\n[pip3] torchaudio==2.4.0+rocm6.3.4.git69d40773\n[pip3] torchvision==0.19.0+rocm6.3.4.gitfab84886\n[pip3] transformers==4.49.0\n[pip3] triton==3.2.0+gite5be006a\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] pytorch-triton-rocm       3.0.0+rocm6.3.4.git75cc27c2          pypi_0    pypi\n[conda] pyzmq                     26.2.1                   pypi_0    pypi\n[conda] torch                     2.4.0+rocm6.3.4.git7cecbf6d          pypi_0    pypi\n[conda] torchaudio                2.4.0+rocm6.3.4.git69d40773          pypi_0    pypi\n[conda] torchvision               0.19.0+rocm6.3.4.gitfab84886          pypi_0    pypi\n[conda] transformers              4.49.0                   pypi_0    pypi\n[conda] triton                    3.2.0+gite5be006a          pypi_0    pypi\nROCM Version: 6.3.42134-a9a80e791\nNeuron SDK Version: N/A\nvLLM Version: 0.7.4.dev328+g609ef61f.d20250308\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n\n\nCUDA_VISIBLE_DEVICES=0\nCUDA_VISIBLE_DEVICES=0\nPYTORCH_ROCM_ARCH=gfx1100\nVLLM_USE_TRITON_FLASH_ATTN=0\nLD_LIBRARY_PATH=/home/wangwuwu/anaconda3/envs/vllm/lib/python3.10/site-packages/cv2/../../lib64:\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nhttps://github.com/ROCm/ROCm/issues/3914#issuecomment-2530568375\uff0cI installed vllm on wsl2 according to this method, but I encountered problems when I wanted to use vllm to perform \"microsoft/Phi-4-mini-instruct\" model inference\n\n\n```>>> from vllm import LLM, SamplingParams\nINFO 03-09 01:21:09 [__init__.py:264] Automatically detected platform rocm.\nWARNING 03-09 01:21:09 [rocm.py:25] Failed to import from amdsmi with ModuleNotFoundError(\"No module named 'amdsmi'\")\n>>> prompts = [\"The capital of France is\"]\n>>> sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n>>> llm = LLM(model=\"microsoft/Phi-4-mini-instruct\")\nINFO 03-09 01:21:36 [config.py:208] Replacing legacy 'type' key with 'rope_type'\nINFO 03-09 01:21:42 [config.py:576] This model supports multiple tasks: {'score', 'generate', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.\nINFO 03-09 01:21:42 [config.py:1521] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\nWARNING 03-09 01:21:42 [arg_utils.py:1276] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\nINFO 03-09 01:21:42 [llm_engine.py:235] Initializing a V0 LLM engine (v0.7.4.dev328+g609ef61f.d20250308) with config: model='microsoft/Phi-4-mini-instruct', speculative_config=None, tokenizer='microsoft/Phi-4-mini-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=microsoft/Phi-4-mini-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False,\nWARNING 03-09 01:21:44 [interface.py:305] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\nINFO 03-09 01:21:44 [rocm.py:130] None is not supported in AMD GPUs.\nINFO 03-09 01:21:44 [rocm.py:131] Using ROCmFlashAttention backend.\nINFO 03-09 01:21:48 [parallel_state.py:948] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 03-09 01:21:48 [model_runner.py:1110] Starting to load model microsoft/Phi-4-mini-instruct...\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"<stdin>\", line 1, in <module>\n[rank0]:   File \"/home/wangwuwu/vllm/vllm/utils.py\", line 1053, in inner\n[rank0]:     return fn(*args, **kwargs)\n[rank0]:   File \"/home/wangwuwu/vllm/vllm/entrypoints/llm.py\", line 244, in __init__\n[rank0]:     self.llm_engine = self.engine_class.from_engine_args(\n[rank0]:   File \"/home/wangwuwu/vllm/vllm/engine/llm_engine.py\", line 494, in from_engine_args\n[rank0]:     engine = cls(\n[rank0]:   File \"/home/wangwuwu/vllm/vllm/engine/llm_engine.py\", line 274, in __init__\n[rank0]:     self.model_executor = executor_class(vllm_config=vllm_config, )\n[rank0]:   File \"/home/wangwuwu/vllm/vllm/executor/executor_base.py\", line 52, in __init__\n[rank0]:     self._init_executor()\n[rank0]:   File \"/home/wangwuwu/vllm/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n[rank0]:     self.collective_rpc(\"load_model\")\n[rank0]:   File \"/home/wangwuwu/vllm/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n[rank0]:     answer = run_method(self.driver_worker, method, args, kwargs)\n[rank0]:   File \"/home/wangwuwu/vllm/vllm/utils.py\", line 2238, in run_method\n[rank0]:     return func(*args, **kwargs)\n[rank0]:   File \"/home/wangwuwu/vllm/vllm/worker/worker.py\", line 183, in load_model\n[rank0]:     self.model_runner.load_model()\n[rank0]:   File \"/home/wangwuwu/vllm/vllm/worker/model_runner.py\", line 1111, in load_model\n[rank0]:     with DeviceMemoryProfiler(self.device) as m:\n[rank0]:   File \"/home/wangwuwu/vllm/vllm/utils.py\", line 752, in __enter__\n[rank0]:     self.initial_memory = self.current_memory_usage()\n[rank0]:   File \"/home/wangwuwu/vllm/vllm/utils.py\", line 749, in current_memory_usage\n[rank0]:     return current_platform.get_current_memory_usage(self.device)\n[rank0]:   File \"/home/wangwuwu/vllm/vllm/platforms/rocm.py\", line 228, in get_current_memory_usage\n[rank0]:     return torch.cuda.mem_get_info(device)[1] - torch.cuda.mem_get_info(\n[rank0]:   File \"/home/wangwuwu/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/cuda/memory.py\", line 684, in mem_get_info\n[rank0]:     device = _get_device_index(device)\n[rank0]:   File \"/home/wangwuwu/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/cuda/_utils.py\", line 38, in _get_device_index\n[rank0]:     return _torch_get_device_index(device, optional, allow_cpu)\n[rank0]:   File \"/home/wangwuwu/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/_utils.py\", line 803, in _get_device_index\n[rank0]:     raise ValueError(\n[rank0]: ValueError: Expected a torch.device with a specified index or an integer, but got:cuda\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-08T18:54:49+00:00",
    "closed_at": "2025-07-07T02:14:18+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14500/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14500"
  },
  {
    "number": 9573,
    "title": "[Bug]: The driver_worker gets stuck 100% of the time, when using Medusa with TP > 1",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121 \r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime) \r\n\r\nGPU models and configuration:  \r\nGPU 0: NVIDIA A800-SXM4-80GB \r\nGPU 1: NVIDIA A800-SXM4-80GB \r\n\r\nCPU: \r\nArchitecture:                    x86_64 \r\n\r\nVersions of relevant libraries: \r\n[pip3] numpy==1.26.4 \r\n[pip3] nvidia-cublas-cu12==12.1.3.1 \r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105 \r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105 \r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105 \r\n[pip3] nvidia-cudnn-cu12==9.1.0.70 \r\n[pip3] nvidia-cufft-cu12==11.0.2.54 \r\n[pip3] nvidia-curand-cu12==10.3.2.106 \r\n[pip3] nvidia-cusolver-cu12==11.4.5.107 \r\n[pip3] nvidia-cusparse-cu12==12.1.0.106 \r\n[pip3] nvidia-dali-cuda120==1.33.0 \r\n[pip3] nvidia-ml-py==12.560.30 \r\n[pip3] nvidia-nccl-cu12==2.20.5 \r\n[pip3] nvidia-nvjitlink-cu12==12.6.68 \r\n[pip3] nvidia-nvtx-cu12==12.1.105 \r\n[pip3] nvidia-pyindex==1.0.9 \r\n[pip3] pynvml==11.4.1 \r\n[pip3] pyzmq==25.1.2 \r\n[pip3] torch==2.4.0 \r\n[pip3] transformers==4.45.2 \r\n\r\nvLLM Version: 0.6.3.post1\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\nNone\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nRun using the following command\r\n```bash\r\nexport CUDA_VISIBLE_DEVICES=0,1\r\nexport VLLM_ATTENTION_BACKEND=\"FLASH_ATTN\"\r\n\r\npython3 -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 30000 \\\r\n  --served-model-name base_model --tokenizer-mode auto --max-model-len 2048 \\\r\n  --max-num-batched-tokens 20480 --max-num-seqs 8 \\\r\n  --tensor-parallel-size 2  --trust-remote-code \\\r\n  --gpu-memory-utilization 0.8 --disable-custom-all-reduce --dtype float16 \\\r\n  --speculative-model /home/work/qwen2/medusa \\\r\n  --model /home/work/qwen2 \\\r\n  --use-v2-block-manager --num-speculative-tokens 2 \\\r\n  --enable-prefix-caching\r\n```\r\n\r\nThe driver_worker is **stuck** here because the second worker (GPU) did not compute the logits returned by lm_head.linear_method.apply(). The phenomenon is that the second worker process reports an error \"**No available block found in 60 seconds**\".\r\n\r\n```python\r\n#vllm/model_executor/layers/logits_processor.py\r\ndef _get_logits(\r\n    self,\r\n    hidden_states: torch.Tensor,\r\n    lm_head: VocabParallelEmbedding,\r\n    embedding_bias: Optional[torch.Tensor],\r\n) -> Optional[torch.Tensor]:\r\n    logits = lm_head.linear_method.apply(lm_head,\r\n                                         hidden_states,\r\n                                         bias=embedding_bias)\r\n    if self.use_gather:\r\n        logits = tensor_model_parallel_gather(logits) \r\n        # HERE!!!, The driver_worker is stuck here because the second worker (GPU) did not compute the logits returned by lm_head.linear_method.apply(). \r\n\r\n    else:\r\n        logits = tensor_model_parallel_all_gather(logits)\r\n    if logits is not None:\r\n        logits = logits[..., :self.org_vocab_size]\r\n    return logits\r\n```\r\n\r\nAdditionally, I found that everything works fine when using n-gram.\r\n```bash\r\nexport CUDA_VISIBLE_DEVICES=0,1\r\nexport VLLM_ATTENTION_BACKEND=\"FLASH_ATTN\"\r\n\r\npython3 -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 30000 \\\r\n  --served-model-name base_model --tokenizer-mode auto --max-model-len 2048 \\\r\n  --max-num-batched-tokens 20480 --max-num-seqs 8 \\\r\n  --tensor-parallel-size 2  --trust-remote-code \\\r\n  --gpu-memory-utilization 0.8 --disable-custom-all-reduce --dtype float16 \\\r\n  --speculative-model=\"[ngram]\" --ngram_prompt_lookup_max=4 \\\r\n  --model /home/work/qwen2 \\\r\n  --use-v2-block-manager --num-speculative-tokens 2 \\\r\n  --enable-prefix-caching\r\n```\r\n\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-22T02:43:08+00:00",
    "closed_at": "2025-03-07T02:03:48+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9573/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/9573"
  },
  {
    "number": 20305,
    "title": "[Usage]: vLLM v0.9.1 Free Memory on device on startup is less than desired GPU memory utilization",
    "body": "### Your current environment\n\n```text\n```\n\n### How would you like to use vllm\n\nI created a container with these arguments by using vLLM v0.9.1 by usnig Dockerode.\n\n```text\n  --gpus all \\\n  --ipc=host \\\n  -p 8000:8000 \\\n  -v /home/openzeka/.cache/huggingface:/root/.cache/huggingface:rw \\\n  -e HF_TOKEN=<TOKEN> \\\n  vllm/vllm-openai:v0.9.1 \\\n  --max-model-len 1024 \\\n  --gpu-memory-utilization 0.95 \\\n  --max-num-seqs 1 \\\n  --tensor-parallel-size 2 \\\n  --model deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n```\nBut after that I got an error like. My GPUs are free to use.\n```text\n[multiproc_executor.py:492] ValueError: Free memory on device (31.44/47.38 GiB) on startup is less than desired GPU memory utilization (0.95, 45.01 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\nERROR 06-30 13:53:19 [core.py:515] EngineCore failed to start.\nERROR 06-30 13:53:19 [core.py:515] Traceback (most recent call last):\nERROR 06-30 13:53:19 [core.py:515]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 506, in run_engine_core\nERROR 06-30 13:53:19 [core.py:515]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 06-30 13:53:19 [core.py:515]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-30 13:53:19 [core.py:515]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 390, in __init__\nERROR 06-30 13:53:19 [core.py:515]     super().__init__(vllm_config, executor_class, log_stats,\nERROR 06-30 13:53:19 [core.py:515]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 76, in __init__\nERROR 06-30 13:53:19 [core.py:515]     self.model_executor = executor_class(vllm_config)\nERROR 06-30 13:53:19 [core.py:515]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-30 13:53:19 [core.py:515]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 53, in __init__\nERROR 06-30 13:53:19 [core.py:515]     self._init_executor()\nERROR 06-30 13:53:19 [core.py:515]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 98, in _init_executor\nERROR 06-30 13:53:19 [core.py:515]     self.workers = WorkerProc.wait_for_ready(unready_workers)\nERROR 06-30 13:53:19 [core.py:515]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-30 13:53:19 [core.py:515]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 427, in wait_for_ready\nERROR 06-30 13:53:19 [core.py:515]     raise e from None\nERROR 06-30 13:53:19 [core.py:515] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.\nProcess EngineCore_0:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 519, in run_engine_core\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 506, in run_engine_core\n    engine_core = EngineCoreProc(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 390, in __init__\n    super().__init__(vllm_config, executor_class, log_stats,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 76, in __init__\n    self.model_executor = executor_class(vllm_config)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 53, in __init__\n    self._init_executor()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 98, in _init_executor\n    self.workers = WorkerProc.wait_for_ready(unready_workers)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 427, in wait_for_ready\n    raise e from None\nException: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1387, in <module>\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1323, in run_server\n    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1343, in run_server_worker\n    async with build_async_engine_client(args, client_config) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 155, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 191, in build_async_engine_client_from_engine_args\n    async_llm = AsyncLLM.from_vllm_config(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 162, in from_vllm_config\n    return cls(\n           ^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 124, in __init__\n    self.engine_core = EngineCoreClient.make_async_mp_client(\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 93, in make_async_mp_client\n    return AsyncMPClient(vllm_config, executor_class, log_stats,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 716, in __init__\n    super().__init__(\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 422, in __init__\n    self._init_engines_direct(vllm_config, local_only,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 491, in _init_engines_direct\n    self._wait_for_engine_startup(handshake_socket, input_address,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 511, in _wait_for_engine_startup\n    wait_for_engine_startup(\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/utils.py\", line 494, in wait_for_engine_startup\n    raise RuntimeError(\"Engine core initialization failed. \"\nRuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-07-01T07:21:49+00:00",
    "closed_at": "2025-07-03T11:26:17+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20305/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20305"
  },
  {
    "number": 3795,
    "title": "[Bug]: FastAPI Swagger /docs does not working correctly",
    "body": "### Your current environment\r\n\r\nThe latest vLLM docker: vllm/vllm-openai:v0.4.0\r\n\r\nIssue seems to exist as far back as 0.2.7.\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nVisiting **http://localhost:8080/docs** when running the server offline shows a blank page however loads with the proper title for the page. I haven't yet attempted this with an online / non-firewalled running instance of vLLM.\r\n\r\nInspecting the page with developer tools shows some html, however there are errors in downloading .js files from a cdn so I suspect that is the problem. \r\n\r\nIs there a way to enable support for the Swagger documentation offline? \r\n\r\nSince I'm unable to test, http://localhost:8080/docs properly functioning with online instances?\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-04-02T11:56:41+00:00",
    "closed_at": "2024-04-20T00:16:11+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3795/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3795"
  },
  {
    "number": 8083,
    "title": "[Installation]: building CPU docker image crashes my machine ",
    "body": "### Your current environment\n\ntitle says it all, when running \r\n\r\n```bash\r\ndocker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .\r\n```\r\n\r\nthe build process gets up to [this line](https://github.com/vllm-project/vllm/blob/e2b2aa5a0fdd3e682dd1fbd62e2ba81b8aa054d2/Dockerfile.cpu#L44\r\n) and subsequently freezes my pc indefinitely.\r\n \r\n No error messages to share..\r\n\r\n\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.8.0-40-generic-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        39 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               8\r\nOn-line CPU(s) list:                  0-7\r\nVendor ID:                            GenuineIntel\r\nModel name:                           11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz\r\nCPU family:                           6\r\nModel:                                140\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   4\r\nSocket(s):                            1\r\nStepping:                             1\r\nCPU max MHz:                          4700,0000\r\nCPU min MHz:                          400,0000\r\nBogoMIPS:                             5606.40\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves split_lock_detect user_shstk dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid movdiri movdir64b fsrm avx512_vp2intersect md_clear ibt flush_l1d arch_capabilities\r\nVirtualization:                       VT-x\r\nL1d cache:                            192 KiB (4 instances)\r\nL1i cache:                            128 KiB (4 instances)\r\nL2 cache:                             5 MiB (4 instances)\r\nL3 cache:                             12 MiB (1 instance)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-7\r\nVulnerability Gather data sampling:   Mitigation; Microcode\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] numpy-stl==2.8.0\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.68\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] onnxruntime-genai==0.3.0\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.2\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.5@2148441fd371faf3e90748b310fdb4500939e527\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\n\n### How you are installing vllm\n\n```sh\r\ndocker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .\r\n```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-02T12:19:16+00:00",
    "closed_at": "2025-01-02T01:59:28+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8083/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8083"
  },
  {
    "number": 11431,
    "title": "[Feature]: AssertionError: MolmoForCausalLM does not support LoRA yet.",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.22.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-122-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.5.82\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA RTX A6000\r\nGPU 1: NVIDIA RTX A6000\r\n\r\nNvidia driver version: 555.42.02\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               32\r\nOn-line CPU(s) list:                  0-31\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD Ryzen Threadripper PRO 5955WX 16-Cores\r\nCPU family:                           25\r\nModel:                                8\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   16\r\nSocket(s):                            1\r\nStepping:                             2\r\nFrequency boost:                      enabled\r\nCPU max MHz:                          7031.2500\r\nCPU min MHz:                          1800.0000\r\nBogoMIPS:                             7985.02\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\nVirtualization:                       AMD-V\r\nL1d cache:                            512 KiB (16 instances)\r\nL1i cache:                            512 KiB (16 instances)\r\nL2 cache:                             8 MiB (16 instances)\r\nL3 cache:                             64 MiB (2 instances)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-31\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Mitigation; safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.47.1\r\n[pip3] triton==3.1.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.5\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      SYS     0-31    0               N/A\r\nGPU1    SYS      X      0-31    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nCUDAPATH=/usr/local/cuda-12.5\r\nCUDACXX=/usr/local/cuda-12.5/bin/nvcc\r\nLD_LIBRARY_PATH=/molmo/.venv/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda-12.5/lib64:\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nApparently MolmoForCasualLM does not yet support Lora adapters, yielding an AssertionError on serving:\r\n\r\n`AssertionError: MolmoForCausalLM does not support LoRA yet.`\r\n\r\nI trained a Lora adapter with HF Trainer and would like to use it together with vLLM for fast inference. This seems not implemented yet. I tested this by trying to serve Molmo directly via:\r\n`vllm serve allenai/Molmo-7B-D-0924 --enable-lora --trust-remote-code --max-num-seqs 6 --tensor-parallel-size 1 --lora-modules test=$LORA_DIR/checkpoint-25`\r\n\r\nAre there any plans to get this working or is there a guide somewhere how i can enable lora for Molmo myself?\r\nIf all works I'd be open to submit a PR but i'd need some guidance.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-12-23T10:38:18+00:00",
    "closed_at": "2024-12-31T01:33:08+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11431/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11431"
  },
  {
    "number": 8152,
    "title": "[Feature]: Add support for `GPTNeoXForSequenceClassification`",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently vLLM does not support `GPTNeoXForSequenceClassification` architecture. Many reward models that are used in RLHF training have similar architecture (causalLM + linear projection on top). Supporting this architecture can make training and evaluation of RLHF methods way faster.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-04T10:45:45+00:00",
    "closed_at": "2025-01-04T01:58:20+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8152/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8152"
  },
  {
    "number": 2899,
    "title": "Duplicate Token `<s>` in Tokenizer Encoded Token ids",
    "body": "When working on tokenizer result for `llama-2-7b-chat-hf` model, I noticed that the `prompt_token_ids` generated in [this place](https://github.com/vllm-project/vllm/blob/5f08050d8d0bfcdaced0fe706cdfc9e311e0f263/vllm/engine/llm_engine.py#L385C13-L385C29) would generate an extra token `<s>` in the beginning of the sentence.\r\n\r\nFor example for the follow prompt `<s>[INST] what is the color of the snow? [/INST]` , hf tokenizer can directly tokenize it to\r\n```\r\n['<s>', '\u2581[', 'INST', ']', '\u2581what', '\u2581is', '\u2581the', '\u2581color', '\u2581of', '\u2581the', '\u2581snow', '?', '\u2581[', '/', 'INST', ']']\r\n[1, 518, 25580, 29962, 825, 338, 278, 2927, 310, 278, 15007, 29973, 518, 29914, 25580, 29962]\r\n```\r\nbut for the very same prompt vllm would generate tokenized prompt ids as follows\r\n```\r\n[1, 1, 518, 25580, 29962, 825, 338, 278, 2927, 310, 278, 15007, 29973, 518, 29914, 25580, 29962]\r\n```\r\nwhich has an extra token `1`, aka `<s>` in the beginning.\r\n\r\nLooking forward to have someone help me confirm if this is designated behaviour or caused by some of the model options.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-02-17T06:39:51+00:00",
    "closed_at": "2024-06-04T00:17:02+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2899/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2899"
  },
  {
    "number": 15256,
    "title": "[Bug]: working with openai-agents sdk an use Runner.run_streamed() got fucntion call error",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n2025-03-21 08:04:16 (61.7 KB/s) - \u2018collect_env.py\u2019 saved [26257/26257]\n\nINFO 03-21 08:04:21 [__init__.py:256] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.30.2\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.6.20\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090\nNvidia driver version: 560.94\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.3.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        39 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               12\nOn-line CPU(s) list:                  0-11\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz\nCPU family:                           6\nModel:                                158\nThread(s) per core:                   2\nCore(s) per socket:                   6\nSocket(s):                            1\nStepping:                             10\nBogoMIPS:                             6384.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid pni pclmulqdq ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 xsaves md_clear flush_l1d arch_capabilities\nHypervisor vendor:                    Microsoft\nVirtualization type:                  full\nL1d cache:                            192 KiB (6 instances)\nL1i cache:                            192 KiB (6 instances)\nL2 cache:                             1.5 MiB (6 instances)\nL3 cache:                             12 MiB (1 instance)\nVulnerability Gather data sampling:   Unknown: Dependent on hypervisor status\nVulnerability Itlb multihit:          KVM: Mitigation: VMX unsupported\nVulnerability L1tf:                   Mitigation; PTE Inversion\nVulnerability Mds:                    Mitigation; Clear CPU buffers; SMT Host state unknown\nVulnerability Meltdown:               Mitigation; PTI\nVulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT Host state unknown\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; IBRS\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; IBRS; IBPB conditional; STIBP conditional; RSB filling; PBRSB-eIBRS Not affected; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Unknown: Dependent on hypervisor status\nVulnerability Tsx async abort:        Mitigation; Clear CPU buffers; SMT Host state unknown\n\nVersions of relevant libraries:\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.1.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.49.0\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X                              N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=/home/zx/vLLM/venv/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu::/usr/local/cuda-12.6/lib64:/usr/local/cuda-12.6/lib64\nCUDA_HOME=:/usr/local/cuda-12.6:/usr/local/cuda-12.6\nCUDA_HOME=:/usr/local/cuda-12.6:/usr/local/cuda-12.6\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nworking with openai-agents sdk and use Runner.run_streamed(), i got  error below:\n\n**on the calling side \uff1a**\n=== Run starting ===\nAgent updated: Joker\nTraceback (most recent call last):\n  File \"/home/zhouxiang/file_server/my_agent.py\", line 64, in <module>\n    asyncio.run(main())\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/home/zhouxiang/file_server/my_agent.py\", line 43, in main\n    async for event in result.stream_events():\n  File \"/home/zhouxiang/file_server/venv/lib/python3.12/site-packages/agents/result.py\", line 182, in stream_events\n    raise self._stored_exception\n  File \"/home/zhouxiang/file_server/venv/lib/python3.12/site-packages/agents/run.py\", line 537, in _run_streamed_impl\n    turn_result = await cls._run_single_turn_streamed(\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhouxiang/file_server/venv/lib/python3.12/site-packages/agents/run.py\", line 639, in _run_single_turn_streamed\n    async for event in model.stream_response(\n  File \"/home/zhouxiang/file_server/venv/lib/python3.12/site-packages/agents/models/openai_chatcompletions.py\", line 413, in stream_response\n    ResponseUsage(\n  File \"/home/zhouxiang/file_server/venv/lib/python3.12/site-packages/pydantic/main.py\", line 214, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 1 validation error for ResponseUsage\ninput_tokens_details\n  Field required [type=missing, input_value={'input_tokens': 165, 'ou...ils(reasoning_tokens=0)}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\n\n\non the vllm side\uff1a\n\n2025-03-21 07:44:04 ERROR 03-20 16:44:04 hermes_tool_parser.py:338] Error trying to handle streaming tool call.\n2025-03-21 07:44:04 ERROR 03-20 16:44:04 hermes_tool_parser.py:338] Traceback (most recent call last):\n2025-03-21 07:44:04 ERROR 03-20 16:44:04 hermes_tool_parser.py:338]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py\", line 227, in extract_tool_calls_streaming\n2025-03-21 07:44:04 ERROR 03-20 16:44:04 hermes_tool_parser.py:338]     function_name: Union[str, None] = current_tool_call.get(\"name\")\n2025-03-21 07:44:04 ERROR 03-20 16:44:04 hermes_tool_parser.py:338]                                       ^^^^^^^^^^^^^^^^^^^^^\n2025-03-21 07:44:04 ERROR 03-20 16:44:04 hermes_tool_parser.py:338] AttributeError: 'NoneType' object has no attribute 'get'\n2025-03-21 07:44:05 ERROR 03-20 16:44:05 hermes_tool_parser.py:338] Error trying to handle streaming tool call.\n2025-03-21 07:44:05 ERROR 03-20 16:44:05 hermes_tool_parser.py:338] Traceback (most recent call last):\n2025-03-21 07:44:05 ERROR 03-20 16:44:05 hermes_tool_parser.py:338]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py\", line 218, in extract_tool_calls_streaming\n2025-03-21 07:44:05 ERROR 03-20 16:44:05 hermes_tool_parser.py:338]     flags) if tool_call_portion else None\n2025-03-21 07:44:05 ERROR 03-20 16:44:05 hermes_tool_parser.py:338]               ^^^^^^^^^^^^^^^^^\n2025-03-21 07:44:05 ERROR 03-20 16:44:05 hermes_tool_parser.py:338] UnboundLocalError: cannot access local variable 'tool_call_portion' where it is not associated with a value\n\n**my code:**\n\nasync def main():\n    agent = Agent(\n        name=\"Joker\",\n        instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n        tools=[how_many_jokes],\n        model=\"/home/zx/vLLM/model/Qwen2.5-14B-Instruct-GPTQ-Int4\"\n    )\n\n    result = Runner.run_streamed(\n        agent,\n        input=\"Hello\",\n    )\n    print(\"=== Run starting ===\")\n    async for event in result.stream_events():\n        # We'll ignore the raw responses event deltas\n        if event.type == \"raw_response_event\":\n            continue\n        elif event.type == \"agent_updated_stream_event\":\n            print(f\"Agent updated: {event.new_agent.name}\")\n            continue\n        elif event.type == \"run_item_stream_event\":\n            if event.item.type == \"tool_call_item\":\n                print(\"-- Tool was called\")\n            elif event.item.type == \"tool_call_output_item\":\n                print(f\"-- Tool output: {event.item.output}\")\n            elif event.item.type == \"message_output_item\":\n                print(f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\")\n            else:\n                pass  # Ignore other event types\n\n    print(\"=== Run complete ===\")\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-21T00:10:12+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15256/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15256"
  },
  {
    "number": 1830,
    "title": "awq CUDA error: an illegal memory access was encountered",
    "body": "hi, \r\n\r\nI get an \"an illegal memory access was encountered\" error when inference [deepseek-coder-33B-base-AWQ](https://huggingface.co/TheBloke/deepseek-coder-33B-base-AWQ),  which is a Llama2 (GQA) architecture model, but the smaller model is fine([deepseek-coder-6.7B-base-AWQ](https://huggingface.co/TheBloke/deepseek-coder-6.7B-base-AWQ)), the relevant information as follows:\r\n\r\n## Environment\r\npython==3.8\r\ntorch==2.0.1+cu118\r\ntransformers==4.34.1\r\nvllm==0.2.2\r\n\r\n## Code\r\n````\r\nfrom vllm import LLM, SamplingParams\r\nimport torch\r\n\r\nmodel_path = \"deepseek-coder-33b-base-awq\"\r\n\r\nsampling_params = SamplingParams(temperature=0.0, \r\n                                      n=1,\r\n                                      use_beam_search=False,\r\n                                      top_p=1, top_k=-1, max_tokens=200, \r\n                                      skip_special_tokens=False, \r\n                                      stop_token_ids=stop_token_ids)\r\n\r\nllm = LLM(model=model_path, quantization=\"awq\", dtype=\"auto\", gpu_memory_utilization=0.9, swap_space=32)\r\n\r\ntext = \"def quick_sort(\"\r\noutputs = llm.generate([text], sampling_params)\r\nprint(outputs)\r\n````\r\n\r\n## Error\r\n````\r\nINFO 11-29 16:37:53 llm_engine.py:72] Initializing an LLM engine with config: model='deepseek-coder-33b-base-awq', tokenizer='deepseek-coder-33b-base-awq', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=65536, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=awq, seed=0)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nTraceback (most recent call last):\r\n  File \"infer_vllm.py\", line 26, in <module>\r\n    llm = LLM(model=ckpt, quantization=\"awq\", dtype=\"auto\", gpu_memory_utilization=0.9, swap_space=32)\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/entrypoints/llm.py\", line 93, in __init__\r\n    self.llm_engine = LLMEngine.from_engine_args(engine_args)\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/engine/llm_engine.py\", line 243, in from_engine_args\r\n    engine = cls(*engine_configs,\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/engine/llm_engine.py\", line 113, in __init__\r\n    self._init_cache()\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/engine/llm_engine.py\", line 205, in _init_cache\r\n    num_blocks = self._run_workers(\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/engine/llm_engine.py\", line 737, in _run_workers\r\n    self._run_workers_in_batch(workers, method, *args, **kwargs))\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/engine/llm_engine.py\", line 711, in _run_workers_in_batch\r\n    output = executor(*args, **kwargs)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/worker/worker.py\", line 113, in profile_num_available_blocks\r\n    self.model(\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/model_executor/models/llama.py\", line 288, in forward\r\n    hidden_states = self.model(input_ids, positions, kv_caches,\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/model_executor/models/llama.py\", line 254, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/model_executor/models/llama.py\", line 216, in forward\r\n    hidden_states = self.mlp(hidden_states)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/model_executor/models/llama.py\", line 74, in forward\r\n    gate_up, _ = self.gate_up_proj(x)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/model_executor/layers/linear.py\", line 203, in forward\r\n    output_parallel = self.linear_method.apply_weights(\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/model_executor/layers/quantization/awq.py\", line 154, in apply_weights\r\n    out = ops.awq_gemm(reshaped_x, qweight, scales, qzeros, pack_factor)\r\nRuntimeError: CUDA error: an illegal memory access was encountered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n````",
    "labels": [],
    "state": "closed",
    "created_at": "2023-11-29T08:57:46+00:00",
    "closed_at": "2023-11-30T09:14:47+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1830/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1830"
  },
  {
    "number": 15241,
    "title": "[Bug]: Temperature is ignored in vLLM 0.8.0/0.8.1",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n</details>\n\n### Description\nIn vLLM 0.7 and before, using a high temperature (10) with a random input string **always** returns \"max_tokens\" number of tokens (random output of the correct length)\nWith a temperature of 0, it returns something similar to \"It seems like you've entered a string of characters that doesn't appear to be a meaningful word, phrase, or question.\"\n\nUsing the docker image 0.8.0 or 0.8.1, no matter the temperature, it always answers something like \"It seems like you've entered a string of characters that doesn't appear to be a meaningful word, phrase, or question.\"\n\n### Details\nI tried with multiple models and the temperature seems to be ignored for all of them\n\n### \ud83d\udc1b Describe the bug\n\n### Reproduction\nStarting a Docker container with:\n`docker run --gpus all \\\n    --entrypoint bash \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --ipc=host \\\n    -p 8000:8000 \\\n    -it \\\n    vllm/vllm-openai:v0.7.3`\nand running \n```python3 -m vllm.entrypoints.openai.api_server   --model Qwen/Qwen2.5-VL-7B-Instruct   --trust-remote-code   --max-model-len 32768 --tensor-parallel-size 2 --gpu-memory-utilization 0.95```\non the server-side, and\n\n```import random\nimport string\nimport time\nfrom openai import OpenAI\nmodel_name = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n\nclient = OpenAI(\n        api_key=\"EMPTY\",\n        base_url=\"http://localhost:8000/v1\",\n    )\n    \nclient.chat.completions.create(\n        model = model_name,\n        max_tokens = 1000,\n        temperature = 10,\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are Qwen.\"},\n            {\n                \"role\": \"user\", \n                \"content\": \"\".join(random.choices(string.ascii_letters + string.digits, k=10)),\n            },\n        ],\n    )```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-20T18:30:40+00:00",
    "closed_at": "2025-03-21T11:01:03+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15241/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15241"
  },
  {
    "number": 3797,
    "title": "[Bug]: YI:34B\u5728\u4f7f\u7528\u4e0a\u65e0\u6cd5\u505c\u6b62\u3002",
    "body": "### Your current environment\n\n\u4f7f\u7528VLLM\u63a8\u7406Yi:34b,\u6ce8\u610f\u4e0d\u662fchat\uff0c\u603b\u662f\u4e00\u76f4\u91cd\u590d\u76f4\u5230\u6700\u5927token\u3002\u8bd5\u4e86\u5f88\u591a\u79cd\u529e\u6cd5\uff0c\u90fd\u6ca1\u6709\u5f88\u597d\u7684\u89e3\u51b3\u65b9\u6cd5\u3002\u6709\u5927\u4f6c\u80fd\u534f\u52a9\u4e00\u4e0b\u5417\r\n![image](https://github.com/vllm-project/vllm/assets/50692992/b7894d6e-df35-42dd-9056-2ce14f6f4c45)\r\n![image](https://github.com/vllm-project/vllm/assets/50692992/c3784dce-e614-42b9-aeaf-56b6ec72caf4)\r\n\r\n\n\n### \ud83d\udc1b Describe the bug\n\n\u4f7f\u7528VLLM\u63a8\u7406Yi:34b,\u6ce8\u610f\u4e0d\u662fchat\uff0c\u603b\u662f\u4e00\u76f4\u91cd\u590d\u76f4\u5230\u6700\u5927token\u3002\u8bd5\u4e86\u5f88\u591a\u79cd\u529e\u6cd5\uff0c\u90fd\u6ca1\u6709\u5f88\u597d\u7684\u89e3\u51b3\u65b9\u6cd5\u3002\u6709\u5927\u4f6c\u80fd\u534f\u52a9\u4e00\u4e0b\u5417\r\n![image](https://github.com/vllm-project/vllm/assets/50692992/b7894d6e-df35-42dd-9056-2ce14f6f4c45)\r\n![image](https://github.com/vllm-project/vllm/assets/50692992/c3784dce-e614-42b9-aeaf-56b6ec72caf4)\r\n\r\n",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-02T14:53:17+00:00",
    "closed_at": "2024-11-28T02:07:07+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3797/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3797"
  },
  {
    "number": 20670,
    "title": "[Bug]: Tensor dimension mismatch when loading Qwen3-Reranker-4B with tensor parallel > 1",
    "body": "\n### \ud83d\udc1b Describe the bug\n\nWhen trying to load the Qwen3-Reranker-4B model with tensor parallelism enabled (tensor_parallel_size=2), the model initialization fails due to a tensor dimension mismatch error. \n\n## Environment \n- vLLM version: 0.9.2\n- Model: Qwen/Qwen3-Reranker-4B\n- GPU configuration: 2 GPUs with tensor parallelism \n- CUDA version: 12.9 \n\n## Steps to reproduce \n1. Run vLLM with the following configuration: \n```bash \n--model Qwen/Qwen3-Reranker-4B --task score --enforce_eager True --served_model_name Qwen/Qwen3-Reranker-4B-30k --hf_overrides '{\"architectures\":[\"Qwen3ForSequenceClassification\"],\"classifier_from_token\":[\"no\",\"yes\"],\"is_original_qwen3_reranker\":true}' --tensor_parallel_size 2 --gpu_memory_utilization 0.97 \n``` \n\n## Expected behavior The model should load successfully with tensor parallelism across 2 GPUs. \n## Actual behavior The model fails to load with the following error: \n``` \nRuntimeError: The size of tensor a (1280) must match the size of tensor b (2560) at non-singleton dimension 1 \n``` \nFull stack trace shows the error occurs in the `load_weights_using_from_2_way_softmax` function when attempting to copy weights to the score layer: \n``` \nFile \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/adapters.py\", line 351, in load_weights_using_from_2_way_softmax model.score.weight.data.copy_(weight) RuntimeError: The size of tensor a (1280) must match the size of tensor b (2560) at non-singleton dimension 1 \n``` \n## Possible workaround The model loads successfully when using `tensor_parallel_size=1` (no tensor parallelism). \n\n## Full Log\uff1a\n```\nINFO 07-09 00:56:59 [__init__.py:244] Automatically detected platform cuda.\nINFO 07-09 00:57:02 [api_server.py:1395] vLLM API server version 0.9.2\nINFO 07-09 00:57:02 [cli_args.py:325] non-default args: {'host': '0.0.0.0', 'model': 'Qwen/Qwen3-Reranker-4B', 'task': 'score', 'enforce_eager': True, 'served_model_name': ['Qwen/Qwen3-Reranker-4B-30k'], 'hf_overrides': {'architectures': ['Qwen3ForSequenceClassification'], 'classifier_from_token': ['no', 'yes'], 'is_original_qwen3_reranker': True}, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.97}\nINFO 07-09 00:57:09 [config.py:1472] Using max model len 40960\nINFO 07-09 00:57:09 [arg_utils.py:1596] (Disabling) chunked prefill by default\nINFO 07-09 00:57:09 [arg_utils.py:1599] (Disabling) prefix caching by default\nWARNING 07-09 00:57:09 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\nINFO 07-09 00:57:09 [config.py:4601] Only \"last\" pooling supports chunked prefill and prefix caching; disabling both.\nINFO 07-09 00:57:14 [__init__.py:244] Automatically detected platform cuda.\nINFO 07-09 00:57:17 [core.py:526] Waiting for init message from front-end.\nINFO 07-09 00:57:17 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='Qwen/Qwen3-Reranker-4B', speculative_config=None, tokenizer='Qwen/Qwen3-Reranker-4B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-Reranker-4B-30k, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}\nWARNING 07-09 00:57:17 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 07-09 00:57:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_86991475'), local_subscribe_addr='ipc:///tmp/d4a143f0-8d21-4c46-a4fd-667a62dba5dd', remote_subscribe_addr=None, remote_addr_ipv6=False)\nINFO 07-09 00:57:21 [__init__.py:244] Automatically detected platform cuda.\nINFO 07-09 00:57:21 [__init__.py:244] Automatically detected platform cuda.\n(VllmWorker rank=0 pid=212) INFO 07-09 00:57:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8b4da7d2'), local_subscribe_addr='ipc:///tmp/03a0090e-ee69-42d1-a8d5-d900afccb3b0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n(VllmWorker rank=1 pid=213) INFO 07-09 00:57:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f40c3804'), local_subscribe_addr='ipc:///tmp/1fe57a13-1ef5-480f-a578-f67b4f015124', remote_subscribe_addr=None, remote_addr_ipv6=False)\n(VllmWorker rank=1 pid=213) INFO 07-09 00:57:25 [__init__.py:1152] Found nccl from library libnccl.so.2\n(VllmWorker rank=0 pid=212) INFO 07-09 00:57:25 [__init__.py:1152] Found nccl from library libnccl.so.2\n(VllmWorker rank=0 pid=212) INFO 07-09 00:57:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n(VllmWorker rank=1 pid=213) INFO 07-09 00:57:25 [pynccl.py:70] vLLM is using nccl==2.26.2\n(VllmWorker rank=1 pid=213) INFO 07-09 00:57:25 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n(VllmWorker rank=0 pid=212) INFO 07-09 00:57:25 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n(VllmWorker rank=1 pid=213) WARNING 07-09 00:57:25 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n(VllmWorker rank=0 pid=212) WARNING 07-09 00:57:25 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n(VllmWorker rank=0 pid=212) INFO 07-09 00:57:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_468dc0a1'), local_subscribe_addr='ipc:///tmp/0cdef032-c307-4c3a-b4df-1c75d7eca008', remote_subscribe_addr=None, remote_addr_ipv6=False)\n(VllmWorker rank=0 pid=212) INFO 07-09 00:57:25 [parallel_state.py:1076] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n(VllmWorker rank=1 pid=213) INFO 07-09 00:57:25 [parallel_state.py:1076] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n(VllmWorker rank=0 pid=212) INFO 07-09 00:57:25 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.\n(VllmWorker rank=1 pid=213) INFO 07-09 00:57:25 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.\n(VllmWorker rank=1 pid=213) INFO 07-09 00:57:25 [gpu_model_runner.py:1770] Starting to load model Qwen/Qwen3-Reranker-4B...\n(VllmWorker rank=0 pid=212) INFO 07-09 00:57:25 [gpu_model_runner.py:1770] Starting to load model Qwen/Qwen3-Reranker-4B...\n(VllmWorker rank=1 pid=213) INFO 07-09 00:57:25 [gpu_model_runner.py:1775] Loading model from scratch...\n(VllmWorker rank=0 pid=212) INFO 07-09 00:57:25 [gpu_model_runner.py:1775] Loading model from scratch...\n(VllmWorker rank=0 pid=212) INFO 07-09 00:57:25 [cuda.py:284] Using Flash Attention backend on V1 engine.\n(VllmWorker rank=1 pid=213) INFO 07-09 00:57:25 [cuda.py:284] Using Flash Attention backend on V1 engine.\n(VllmWorker rank=0 pid=212) INFO 07-09 00:57:26 [weight_utils.py:292] Using model weights format ['*.safetensors', '*.bin', '*.pt']\n(VllmWorker rank=0 pid=212) \nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n(VllmWorker rank=1 pid=213) INFO 07-09 00:57:26 [weight_utils.py:292] Using model weights format ['*.safetensors', '*.bin', '*.pt']\n(VllmWorker rank=0 pid=212) \nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.33it/s]\n(VllmWorker rank=0 pid=212) \nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.38it/s]\n(VllmWorker rank=0 pid=212) \nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.37it/s]\n(VllmWorker rank=0 pid=212) \n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487] WorkerProc failed to start.\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487] Traceback (most recent call last):\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 461, in worker_main\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]     worker = WorkerProc(*args, **kwargs)\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 358, in __init__\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]     self.worker.load_model()\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 185, in load_model\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]     self.model_runner.load_model()\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1776, in load_model\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]     self.model = model_loader.load_model(\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]                  ^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/base_loader.py\", line 41, in load_model\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]     self.load_weights(model, model_config)\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/default_loader.py\", line 269, in load_weights\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]     loaded_weights = model.load_weights(\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]                      ^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/adapters.py\", line 256, in load_weights\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]     return seq_cls_model_loader(self, weights)\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/adapters.py\", line 375, in seq_cls_model_loader\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]     return SEQ_CLS_LOAD_METHODS[method](model, weights)\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/adapters.py\", line 351, in load_weights_using_from_2_way_softmax\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487]     model.score.weight.data.copy_(weight)\n(VllmWorker rank=1 pid=213) ERROR 07-09 00:57:28 [multiproc_executor.py:487] RuntimeError: The size of tensor a (1280) must match the size of tensor b (2560) at non-singleton dimension 1\n84487e05a96e:213:213 [1] NCCL INFO cudaDriverVersion 12090\n84487e05a96e:213:213 [1] NCCL INFO Bootstrap: Using eth0:172.21.0.3<0>\n84487e05a96e:213:213 [1] NCCL INFO NCCL version 2.26.2+cuda12.2\n84487e05a96e:213:213 [1] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.\n84487e05a96e:213:213 [1] NCCL INFO NET/IB : No device found.\n84487e05a96e:213:213 [1] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.0.3<0>\n84487e05a96e:213:213 [1] NCCL INFO NET/Socket : Using [0]eth0:172.21.0.3<0>\n84487e05a96e:213:213 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. \n84487e05a96e:213:213 [1] NCCL INFO Using network Socket\n84487e05a96e:213:213 [1] NCCL INFO ncclCommInitRank comm 0xe54d5d0 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 6000 commId 0x1760359f24154150 - Init START\n84487e05a96e:213:213 [1] NCCL INFO RAS client listening socket at ::1<28028>\n84487e05a96e:213:213 [1] NCCL INFO Bootstrap timings total 0.001015 (create 0.000035, send 0.000131, recv 0.000371, ring 0.000024, delay 0.000000)\n84487e05a96e:213:213 [1] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.\n\n[2025-07-09 00:57:25] 84487e05a96e:213:213 [1] graph/search.cc:1127 NCCL WARN Could not find a path for pattern 4, falling back to simple order\n\n[2025-07-09 00:57:25] 84487e05a96e:213:213 [1] graph/search.cc:1127 NCCL WARN Could not find a path for pattern 1, falling back to simple order\n84487e05a96e:213:213 [1] NCCL INFO comm 0xe54d5d0 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0\n84487e05a96e:213:213 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0\n84487e05a96e:213:213 [1] NCCL INFO P2P Chunksize set to 131072\n84487e05a96e:213:262 [1] NCCL INFO [Proxy Service] Device 1 CPU core 7\n84487e05a96e:213:265 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 9\n84487e05a96e:213:213 [1] NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct\n84487e05a96e:213:213 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct\n84487e05a96e:213:213 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1\n84487e05a96e:213:213 [1] NCCL INFO Connected all trees\n84487e05a96e:213:266 [1] NCCL INFO [Proxy Progress] Device 1 CPU core 12\n84487e05a96e:213:213 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n84487e05a96e:213:213 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n84487e05a96e:213:213 [1] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.\n84487e05a96e:213:213 [1] NCCL INFO ncclCommInitRank comm 0xe54d5d0 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 6000 commId 0x1760359f24154150 - Init COMPLETE\n84487e05a96e:213:213 [1] NCCL INFO Init timings - ncclCommInitRank: rank 1 nranks 2 total 0.22 (kernels 0.15, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.05, rest 0.00)\n84487e05a96e:212:212 [0] NCCL INFO Bootstrap: Using eth0:172.21.0.3<0>\n84487e05a96e:212:212 [0] NCCL INFO cudaDriverVersion 12090\n84487e05a96e:212:212 [0] NCCL INFO NCCL version 2.26.2+cuda12.2\n84487e05a96e:212:212 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.\n84487e05a96e:212:212 [0] NCCL INFO NET/IB : No device found.\n84487e05a96e:212:212 [0] NCCL INFO NET/IB : Using [RO]; OOB eth0:172.21.0.3<0>\n84487e05a96e:212:212 [0] NCCL INFO NET/Socket : Using [0]eth0:172.21.0.3<0>\n84487e05a96e:212:212 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. \n84487e05a96e:212:212 [0] NCCL INFO Using network Socket\n84487e05a96e:212:212 [0] NCCL INFO ncclCommInitRank comm 0xe64fff0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 5000 commId 0x1760359f24154150 - Init START\n84487e05a96e:212:212 [0] NCCL INFO RAS client listening socket at ::1<28028>\n84487e05a96e:212:212 [0] NCCL INFO Bootstrap timings total 0.001006 (create 0.000029, send 0.000126, recv 0.000313, ring 0.000026, delay 0.000000)\n84487e05a96e:212:212 [0] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.\n\n[2025-07-09 00:57:25] 84487e05a96e:212:212 [0] graph/search.cc:1127 NCCL WARN Could not find a path for pattern 4, falling back to simple order\n\n[2025-07-09 00:57:25] 84487e05a96e:212:212 [0] graph/search.cc:1127 NCCL WARN Could not find a path for pattern 1, falling back to simple order\n84487e05a96e:212:212 [0] NCCL INFO comm 0xe64fff0 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0\n84487e05a96e:212:212 [0] NCCL INFO Channel 00/02 : 0 1\n84487e05a96e:212:212 [0] NCCL INFO Channel 01/02 : 0 1\n84487e05a96e:212:212 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\n84487e05a96e:212:212 [0] NCCL INFO P2P Chunksize set to 131072\n84487e05a96e:212:212 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0\n84487e05a96e:212:264 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9\n84487e05a96e:212:263 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7\n84487e05a96e:212:212 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct\n84487e05a96e:212:212 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct\n84487e05a96e:212:212 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1\n84487e05a96e:212:212 [0] NCCL INFO Connected all trees\n84487e05a96e:212:267 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 13\n84487e05a96e:212:212 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n84487e05a96e:212:212 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n84487e05a96e:212:212 [0] NCCL INFO CC Off, workFifoBytes 1048576\n84487e05a96e:212:212 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.\n84487e05a96e:212:212 [0] NCCL INFO ncclCommInitRank comm 0xe64fff0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 5000 commId 0x1760359f24154150 - Init COMPLETE\n84487e05a96e:212:212 [0] NCCL INFO Init timings - ncclCommInitRank: rank 0 nranks 2 total 0.22 (kernels 0.15, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.05, rest 0.00)\n[rank0]:[W709 00:57:29.669899852 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nERROR 07-09 00:57:30 [core.py:586] EngineCore failed to start.\nERROR 07-09 00:57:30 [core.py:586] Traceback (most recent call last):\nERROR 07-09 00:57:30 [core.py:586]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 577, in run_engine_core\nERROR 07-09 00:57:30 [core.py:586]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 07-09 00:57:30 [core.py:586]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 00:57:30 [core.py:586]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 404, in __init__\nERROR 07-09 00:57:30 [core.py:586]     super().__init__(vllm_config, executor_class, log_stats,\nERROR 07-09 00:57:30 [core.py:586]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 75, in __init__\nERROR 07-09 00:57:30 [core.py:586]     self.model_executor = executor_class(vllm_config)\nERROR 07-09 00:57:30 [core.py:586]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 00:57:30 [core.py:586]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 53, in __init__\nERROR 07-09 00:57:30 [core.py:586]     self._init_executor()\nERROR 07-09 00:57:30 [core.py:586]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 93, in _init_executor\nERROR 07-09 00:57:30 [core.py:586]     self.workers = WorkerProc.wait_for_ready(unready_workers)\nERROR 07-09 00:57:30 [core.py:586]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 00:57:30 [core.py:586]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 422, in wait_for_ready\nERROR 07-09 00:57:30 [core.py:586]     raise e from None\nERROR 07-09 00:57:30 [core.py:586] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.\nProcess EngineCore_0:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 590, in run_engine_core\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 577, in run_engine_core\n    engine_core = EngineCoreProc(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 404, in __init__\n    super().__init__(vllm_config, executor_class, log_stats,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 75, in __init__\n    self.model_executor = executor_class(vllm_config)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 53, in __init__\n    self._init_executor()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 93, in _init_executor\n    self.workers = WorkerProc.wait_for_ready(unready_workers)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 422, in wait_for_ready\n    raise e from None\nException: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1495, in <module>\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1431, in run_server\n    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1451, in run_server_worker\n    async with build_async_engine_client(args, client_config) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 158, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 194, in build_async_engine_client_from_engine_args\n    async_llm = AsyncLLM.from_vllm_config(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 162, in from_vllm_config\n    return cls(\n           ^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 124, in __init__\n    self.engine_core = EngineCoreClient.make_async_mp_client(\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 96, in make_async_mp_client\n    return AsyncMPClient(*client_args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 666, in __init__\n    super().__init__(\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 403, in __init__\n    with launch_core_engines(vllm_config, executor_class,\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 144, in __exit__\n    next(self.gen)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py\", line 434, in launch_core_engines\n    wait_for_engine_startup(\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py\", line 484, in wait_for_engine_startup\n    raise RuntimeError(\"Engine core initialization failed. \"\nRuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n/usr/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n```\n\n### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : Could not collect\nCMake version                : version 4.0.3\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu128\nIs debug build               : False\nCUDA used to build PyTorch   : 12.8\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0] (64-bit runtime)\nPython platform              : Linux-5.15.0-43-generic-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.8.93\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : \nGPU 0: NVIDIA GeForce RTX 3080\nGPU 1: NVIDIA GeForce RTX 3080\n\nNvidia driver version        : 575.64.03\ncuDNN version                : Could not collect\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   46 bits physical, 48 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          32\nOn-line CPU(s) list:             0-31\nVendor ID:                       GenuineIntel\nModel name:                      Intel(R) Xeon(R) Gold 6248R CPU @ 3.00GHz\nCPU family:                      6\nModel:                           85\nThread(s) per core:              1\nCore(s) per socket:              16\nSocket(s):                       2\nStepping:                        7\nBogoMIPS:                        6000.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat umip pku ospke avx512_vnni md_clear arch_capabilities\nL1d cache:                       1 MiB (32 instances)\nL1i cache:                       1 MiB (32 instances)\nL2 cache:                        128 MiB (32 instances)\nNUMA node(s):                    2\nNUMA node0 CPU(s):               0-15\nNUMA node1 CPU(s):               16-31\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Mitigation; TSX disabled\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] flashinfer-python==0.2.6.post1+cu128torch2.7\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu12==12.8.3.14\n[pip3] nvidia-cuda-cupti-cu12==12.8.57\n[pip3] nvidia-cuda-nvrtc-cu12==12.8.61\n[pip3] nvidia-cuda-runtime-cu12==12.8.57\n[pip3] nvidia-cudnn-cu12==9.7.1.26\n[pip3] nvidia-cufft-cu12==11.3.3.41\n[pip3] nvidia-cufile-cu12==1.13.0.11\n[pip3] nvidia-curand-cu12==10.3.9.55\n[pip3] nvidia-cusolver-cu12==11.7.2.55\n[pip3] nvidia-cusparse-cu12==12.5.7.53\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.8.61\n[pip3] nvidia-nvtx-cu12==12.8.55\n[pip3] pyzmq==27.0.0\n[pip3] torch==2.7.0+cu128\n[pip3] torchaudio==2.7.0+cu128\n[pip3] torchvision==0.22.0+cu128\n[pip3] transformers==4.53.1\n[pip3] triton==3.3.0\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.2\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n    GPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X  PHB 0-31    0-1     N/A\nGPU1    PHB  X  0-31    0-1     N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n==============================\n     Environment Variables\n==============================\nNVIDIA_VISIBLE_DEVICES=all\nNVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\nNCCL_VERSION=2.25.1-1\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNCCL_DEBUG=INFO\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nCUDA_VERSION=12.8.1\nPYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\nLD_LIBRARY_PATH=/usr/local/cuda/lib64\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY```\n\n</details>\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-07-09T09:05:10+00:00",
    "closed_at": "2025-07-12T03:52:44+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20670/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20670"
  },
  {
    "number": 17821,
    "title": "Tool calls not triggered properly with vLLM 0.8.5 and Qwen2.5-Coder-32B-Instruct-GPTQ-Int4",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nINFO 05-07 17:26:12 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 4.0.0\nLibc version: glibc-2.35\n\nPython version: 3.12.10 (main, Apr  9 2025, 08:55:05) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-6.8.0-35-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: NVIDIA H100 80GB HBM3\nGPU 1: NVIDIA H100 80GB HBM3\nGPU 2: NVIDIA H100 80GB HBM3\nGPU 3: NVIDIA H100 80GB HBM3\n\nNvidia driver version: 550.54.15\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               192\nOn-line CPU(s) list:                  0-191\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8468\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   48\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          3800.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4200.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts re\np_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c\n rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt\n_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx\n512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect\ncldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            4.5 MiB (96 instances)\nL1i cache:                            3 MiB (96 instances)\nL2 cache:                             192 MiB (96 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-47,96-143\nNUMA node1 CPU(s):                    48-95,144-191\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.1.post2+cu124torch2.6\n[pip3] numpy==2.2.5\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.3\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.5\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    0-47,96-143     0               N/A\nGPU1    NV18     X      NV18    NV18    0-47,96-143     0               N/A\nGPU2    NV18    NV18     X      NV18    48-95,144-191   1               N/A\nGPU3    NV18    NV18    NV18     X      48-95,144-191   1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNVIDIA_VISIBLE_DEVICES=GPU-6a4a45dc-ecb9-6afd-7352-32af35190639,GPU-61cd127b-e2e7-08f6-2a43-7f247e84d08b,GPU-5459e040-6673-7776-06b4-456c2b7a9dcd,GPU-72602610-166f-a3c2-1bd5-ecd36e81bddd\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforce\nrtx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driv\ner>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,drive\nr>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,d\nriver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driv\ner<536\nNCCL_VERSION=2.20.5-1\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nCUDA_VERSION=12.4.0\nNVIDIA_DISABLE_REQUIRE=true\nLD_LIBRARY_PATH=/usr/lib/nvidia:/usr/lib/x86_64-linux-gnu\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n Hi team,\n\nI am running `vllm` version 0.8.5 with the following launch command:\n\n```\nvllm serve Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4 \\\n  --quantization gptq \\\n  --download-dir /models \\\n  --tensor-parallel-size 4 \\\n  --enable-auto-tool-choice \\\n  --tool-call-parser hermes\n```\n\nI am trying to test function calling (tool calling) with the model (`Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4`), sending the following request body (for example, via Postman):\n\n```json\n{\n    \"model\": \"Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4\",\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"you are helpful ai\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"what is the weather in seoul?\"\n        }\n    ],\n    \"stream\": false,\n    \"tool_choice\": \"auto\",\n    \"tools\": [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_weather\",\n                \"description\": \"Get the current weather in a given location\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"City and state, e.g., 'San Francisco, CA'\"\n                        },\n                        \"unit\": {\n                            \"type\": \"string\",\n                            \"enum\": [\n                                \"celsius\",\n                                \"fahrenheit\"\n                            ]\n                        }\n                    },\n                    \"required\": [\n                        \"location\",\n                        \"unit\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n```\n\nHowever, I'm seeing the following issue:  \nInstead of getting an actual `tool_call` in the response, this is what I get:\n\n```json\n\"choices\": [\n    {\n        \"index\": 0,\n        \"message\": {\n            \"role\": \"assistant\",\n            \"reasoning_content\": null,\n            \"content\": \"{\\\"name\\\": \\\"get_weather\\\", \\\"arguments\\\": {\\\"location\\\": \\\"Seoul, South Korea\\\", \\\"unit\\\": \\\"celsius\\\"}}\",\n            \"tool_calls\": []\n        },\n        \"logprobs\": null,\n        \"finish_reason\": \"stop\",\n        \"stop_reason\": null\n    }\n],\n```\n\nSo, the function is being output as a string in the assistant's message content, and the `tool_calls` field is empty. It seems that the tool calling logic is not being triggered as expected, even though I'm sending `tool_choice: \"auto\"` and passed `--enable-auto-tool-choice` to `vllm serve`.\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-05-08T00:29:00+00:00",
    "closed_at": null,
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17821/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17821"
  },
  {
    "number": 6272,
    "title": "[Doc]: Latency vs Throughput Configurations",
    "body": "### \ud83d\udcda The doc issue\n\n**Context:** During July 9, 2024, vLLM open office hours (FP8), there were several questions regarding how to **optimize** model deployment inference configurations targeting the two major regimes: **latency** and **throughput** (batch processing). Relevant articles around the same discussion, [Efficiently Scaling Transformer Inference](https://arxiv.org/pdf/2211.05102). Whereas there is an exploration of batch size, chip count and context length. Additionally we should explore the whole set of features (e.g optimized kernels, quantization strategies, pipeline/tensor/sequence parallelism)\n\n### Suggest a potential alternative/fix\n\n**Targets:** Create documentation making explicit what configurations are suitable for each regime, and listing some of its constraints and tradeoffs. The creation of this documentation should add new benchmarking and experimental scripts for reproducing such results. Simultaneously this issue will list the set of compatible flags, thus helping understanding invalid deployment configurations.",
    "labels": [
      "documentation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-09T21:36:46+00:00",
    "closed_at": "2024-12-02T02:08:36+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6272/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6272"
  },
  {
    "number": 16266,
    "title": "[Bug]: unsupported operand type(s) for -: 'int' and 'tuple' when compute max_images in qwen2.5vl",
    "body": "### Your current environment\n\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\nClang version: Could not collect\nCMake version: version 3.18.4\nLibc version: glibc-2.31\n\nPython version: 3.11.11 (main, Feb 14 2025, 14:40:43) [GCC 10.2.1 20210110] (64-bit runtime)\nPython platform: Linux-5.4.143.bsk.8-amd64-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA L20\nGPU 1: NVIDIA L20\nGPU 2: NVIDIA L20\nGPU 3: NVIDIA L20\n\nNvidia driver version: 535.161.08\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nByte Order:                      Little Endian\nAddress sizes:                   52 bits physical, 57 bits virtual\nCPU(s):                          180\nOn-line CPU(s) list:             0-179\nThread(s) per core:              2\nCore(s) per socket:              45\nSocket(s):                       2\nNUMA node(s):                    2\nVendor ID:                       GenuineIntel\nCPU family:                      6\nModel:                           143\nModel name:                      Intel(R) Xeon(R) Platinum 8457C\nStepping:                        8\nCPU MHz:                         2600.000\nBogoMIPS:                        5200.00\nHypervisor vendor:               KVM\nVirtualization type:             full\nL1d cache:                       4.2 MiB\nL1i cache:                       2.8 MiB\nL2 cache:                        180 MiB\nL3 cache:                        195 MiB\nNUMA node0 CPU(s):               0-89\nNUMA node1 CPU(s):               90-179\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Mitigation; TSX disabled\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 wbnoinvd arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid cldemote movdiri movdir64b md_clear arch_capabilities\n\nVersions of relevant libraries:\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] sentence-transformers==4.0.2\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.1\n[pip3] transformers-stream-generator==0.0.5\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NODE    NODE    NODE    SYS     90-179  1               N/A\nGPU1    NODE     X      NODE    NODE    SYS     90-179  1               N/A\nGPU2    NODE    NODE     X      NODE    SYS     90-179  1               N/A\nGPU3    NODE    NODE    NODE     X      SYS     90-179  1               N/A\nNIC0    SYS     SYS     SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n\nNVIDIA_VISIBLE_DEVICES=GPU-0efd2a51-179d-4323-108f-bdbe8eda4588,GPU-92aa58be-6941-5c16-3f96-27461f7b6451,GPU-086a01a1-c0d3-5050-7431-cc35addca390,GPU-8f45530a-6669-6c13-516d-8008bb83bd09\nNVIDIA_REQUIRE_CUDA=cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\nNCCL_NET_PLUGIN=none\nNCCL_VERSION=2.17.1-1\nNCCL_SOCKET_IFNAME==eth0\nNCCL_DEBUG_SUBSYS=INIT,ENV,GRAPH\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNCCL_DEBUG=INFO\nNCCL_IB_GID_INDEX=3\nCUDA_VERSION=12.4.0\nNCCL_SOCKET_FAMILY=AF_INET6\nNCCL_IB_TIMEOUT=23\nMAX_JOBS=16\nLD_LIBRARY_PATH=/usr/local/lib/python3.9/site-packages/nvidia/cudnn/lib:/opt/tiger/native_libhdfs/lib/native:/opt/tiger/jdk/jdk8u265-b01/jre/lib/amd64/server:/opt/tiger/yarn_deploy/hadoop_current/lib/native:/opt/tiger/yarn_deploy/hadoop_current/lib/native/ufs:/opt/tiger/yarn_deploy/hadoop/lib/native:/opt/tiger/yarn_deploy/hadoop_current/lib/native:/opt/tiger/yarn_deploy/hadoop_current/lzo/lib:/usr/local/cuda-12.4/lib64:/opt/tiger/yarn_deploy/hadoop/lib/native:/usr/lib/jvm/java-11-openjdk-amd64/jre/lib/amd64/server:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/extras/CUPTI/lib64\nNCCL_IB_DISABLE=0\nOMP_NUM_THREADS=22\nNCCL_IB_RETRY_CNT=7\nNCCL_TOPO_FILE=/opt/tiger/arnold/arnold_entrypoint/nccl_topo_files/l20_8gpu_topo.xml\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n\n### \ud83d\udc1b Describe the bug\n\n    def get_num_frames_with_most_features(\n        self,\n        seq_len: int,\n        mm_counts: Mapping[str, int],\n    ) -> int:\n        max_images = mm_counts.get(\"image\", 0)\n        max_videos = mm_counts.get(\"video\", 0)\n\n        max_image_tokens = self.get_max_image_tokens() * max_images\n        max_total_frames = self._get_max_video_frames(seq_len -\n                                                      max_image_tokens)\n        max_frames_per_video = min(max_total_frames // max(max_videos, 1),\n                                   _MAX_FRAMES_PER_VIDEO)\n\n        return max(max_frames_per_video, 1)\nhere,max_images is a tuple like (4,) not a int\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-04-08T12:54:52+00:00",
    "closed_at": "2025-04-08T16:10:44+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16266/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16266"
  },
  {
    "number": 4795,
    "title": "[Bug]: `logprobs` is not compatible with the OpenAI spec",
    "body": "### Your current environment\n\nI'm using Runpod Serverless vLLM (https://github.com/runpod-workers/worker-vllm) so I can't run this command. However, I confirmed that the issue is in the codebase in `main`:\r\n\r\nhttps://github.com/vllm-project/vllm/blob/0fca3cdcf265cd375bca684d951702b6b7adf65a/vllm/entrypoints/openai/protocol.py\r\n\n\n### \ud83d\udc1b Describe the bug\n\nThe behavior of `logprobs=True` does not match OpenAI's.\r\n\r\nI identified two issues:\r\n\r\n**(1) vLLM throws an error when `logprobs=True` and `top_logprobs` is missing.**\r\n\r\nOpenAI works fine:\r\n\r\n```py\r\ncompletion = openai_client.chat.completions.create(\r\n  model=\"gpt-4-turbo-preview\",\r\n  messages=[\r\n    {\"role\": \"user\", \"content\": \"Hi!\"}\r\n  ],\r\n  logprobs=True,\r\n)\r\n```\r\n\r\n```\r\nChatCompletion(id='chatcmpl-9OY4XFK8suJ7ed0yw5vglbTsOZUt1', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='Hello', bytes=[72, 101, 108, 108, 111], logprob=-0.0008963357, top_logprobs=[]), ChatCompletionTokenLogprob(token='!', bytes=[33], logprob=-9.729906e-06, top_logprobs=[]), ChatCompletionTokenLogprob(token=' How', bytes=[32, 72, 111, 119], logprob=-1.4140442e-05, top_logprobs=[]), ChatCompletionTokenLogprob(token=' can', bytes=[32, 99, 97, 110], logprob=-0.0004804817, top_logprobs=[]), ChatCompletionTokenLogprob(token=' I', bytes=[32, 73], logprob=-1.11603495e-05, top_logprobs=[]), ChatCompletionTokenLogprob(token=' assist', bytes=[32, 97, 115, 115, 105, 115, 116], logprob=-0.30324343, top_logprobs=[]), ChatCompletionTokenLogprob(token=' you', bytes=[32, 121, 111, 117], logprob=-5.5122365e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token=' today', bytes=[32, 116, 111, 100, 97, 121], logprob=-1.700133e-05, top_logprobs=[]), ChatCompletionTokenLogprob(token='?', bytes=[63], logprob=-0.001247851, top_logprobs=[])]), message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1715637873, model='gpt-4-0125-preview', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\r\n```\r\n\r\nvLLM breaks:\r\n\r\n```python\r\ncompletion = vllm_client.chat.completions.create(\r\n  model=\"...my-llama3-8b-finetune...\",\r\n  messages=[\r\n    {\"role\": \"user\", \"content\": \"Hi!\"}\r\n  ],\r\n  logprobs=True,\r\n)\r\n```\r\n\r\n```\r\nChatCompletion(id=None, choices=None, created=None, model=None, object='error', system_fingerprint=None, usage=None, code=400, message='Top logprobs must be set when logprobs is.', param=None, type='BadRequestError')\r\n```\r\n\r\nvia https://github.com/vllm-project/vllm/blob/0fca3cdcf265cd375bca684d951702b6b7adf65a/vllm/entrypoints/openai/protocol.py#L162\r\n\r\n**(2) Even wtih `top_logprobs=1`, the behavior doesn't match.**\r\n\r\nOpenAI:\r\n\r\n```python\r\ncompletion = openai_client.chat.completions.create(\r\n  model=\"gpt-4-turbo-preview\",\r\n  messages=[\r\n    {\"role\": \"user\", \"content\": \"Hi!\"}\r\n  ],\r\n  logprobs=True,\r\n  top_logprobs=1,\r\n)\r\n```\r\n\r\n```\r\nChatCompletion(id='chatcmpl-9OY4PwigZVtoM6vHXELby3NWqCyaX', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='Hello', bytes=[72, 101, 108, 108, 111], logprob=-0.0008963357, top_logprobs=[TopLogprob(token='Hello', bytes=[72, 101, 108, 108, 111], logprob=-0.0008963357)]), ChatCompletionTokenLogprob(token='!', bytes=[33], logprob=-9.729906e-06, top_logprobs=[TopLogprob(token='!', bytes=[33], logprob=-9.729906e-06)]), ChatCompletionTokenLogprob(token=' How', bytes=[32, 72, 111, 119], logprob=-1.4140442e-05, top_logprobs=[TopLogprob(token=' How', bytes=[32, 72, 111, 119], logprob=-1.4140442e-05)]), ChatCompletionTokenLogprob(token=' can', bytes=[32, 99, 97, 110], logprob=-0.0004804817, top_logprobs=[TopLogprob(token=' can', bytes=[32, 99, 97, 110], logprob=-0.0004804817)]), ChatCompletionTokenLogprob(token=' I', bytes=[32, 73], logprob=-1.11603495e-05, top_logprobs=[TopLogprob(token=' I', bytes=[32, 73], logprob=-1.11603495e-05)]), ChatCompletionTokenLogprob(token=' assist', bytes=[32, 97, 115, 115, 105, 115, 116], logprob=-0.3164038, top_logprobs=[TopLogprob(token=' assist', bytes=[32, 97, 115, 115, 105, 115, 116], logprob=-0.3164038)]), ChatCompletionTokenLogprob(token=' you', bytes=[32, 121, 111, 117], logprob=-5.5122365e-07, top_logprobs=[TopLogprob(token=' you', bytes=[32, 121, 111, 117], logprob=-5.5122365e-07)]), ChatCompletionTokenLogprob(token=' today', bytes=[32, 116, 111, 100, 97, 121], logprob=-1.50940705e-05, top_logprobs=[TopLogprob(token=' today', bytes=[32, 116, 111, 100, 97, 121], logprob=-1.50940705e-05)]), ChatCompletionTokenLogprob(token='?', bytes=[63], logprob=-0.0011334282, top_logprobs=[TopLogprob(token='?', bytes=[63], logprob=-0.0011334282)])]), message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1715637865, model='gpt-4-0125-preview', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\r\n```\r\n\r\nvLLM:\r\n\r\n```python\r\ncompletion = vllm_client.chat.completions.create(\r\n  model=\"...my-llama3-8b-finetune...\",\r\n  messages=[\r\n    {\"role\": \"user\", \"content\": \"Hi!\"}\r\n  ],\r\n  logprobs=True,\r\n  top_logprobs=1,\r\n)\r\n```\r\n\r\n```\r\nChatCompletion(id='cmpl-c9459bd09bb24a028fef65190d22248d', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=None, text_offset=[0, 2, 3, 7, 9, 14, 18, 24, 25], token_logprobs=[-1.6077232360839844, -1.1920228004455566, -0.0803707167506218, -0.8764647841453552, -0.2521621584892273, -1.823885577323381e-05, -0.020511768758296967, -0.007883979007601738, -0.0015675650211051106], tokens=['Hi', '!', '\u0120How', \"'s\", '\u0120your', '\u0120day', '\u0120going', '?', '<|im_end|>'], top_logprobs=[{'Hey': -1.2327232360839844, 'Hi': -1.6077232360839844}, {'!': -1.1920228004455566, '\u0120there': -0.44202280044555664}, {'\u0120How': -0.0803707167506218}, {\"'s\": -0.8764647841453552, '\u0120are': -0.6264647841453552}, {'\u0120your': -0.2521621584892273}, {'\u0120day': -1.823885577323381e-05}, {'\u0120going': -0.020511768758296967}, {'?': -0.007883979007601738}, {'<|im_end|>': -0.0015675650211051106}]), message=ChatCompletionMessage(content=\"Hi! How's your day going?\", role='assistant', function_call=None, tool_calls=None))], created=2362885, model='REDACTED', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=12, total_tokens=21))\r\n```\r\n\r\nNotice that, for example, `token_logprobs` comes up with vLLM but not with OpenAI.\r\n\r\n---\r\n\r\nThese issues break libraries expecting OpenAI-compatible responses, e.g. Rust's async_openai we are using.\r\n",
    "labels": [
      "bug",
      "help wanted",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-05-13T22:11:16+00:00",
    "closed_at": "2024-05-29T23:13:24+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4795/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4795"
  },
  {
    "number": 14845,
    "title": "[Bug]: TTFT Performance Regression in vLLM v0.7.0 Compared to v0.6.1.post2",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py` on vLLM 0.7.0</summary>\n\n```text\nINFO 03-14 21:24:53 __init__.py:183] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: CBL-Mariner/Linux (x86_64)\nGCC version: (GCC) 11.2.0\nClang version: Could not collect\nCMake version: version 3.21.4\nLibc version: glibc-2.35\n\nPython version: 3.10.14 (main, Jul 14 2024, 22:24:12) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-1070-azure-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 11.8.89\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: NVIDIA A100 80GB PCIe\nGPU 1: NVIDIA A100 80GB PCIe\nGPU 2: NVIDIA A100 80GB PCIe\nGPU 3: NVIDIA A100 80GB PCIe\n\nNvidia driver version: 550.90.07\ncuDNN version: Probably one of the following:\n/usr/lib/libcudnn.so.8.9.5\n/usr/lib/libcudnn_adv_infer.so.8.9.5\n/usr/lib/libcudnn_adv_train.so.8.9.5\n/usr/lib/libcudnn_cnn_infer.so.8.9.5\n/usr/lib/libcudnn_cnn_train.so.8.9.5\n/usr/lib/libcudnn_ops_infer.so.8.9.5\n/usr/lib/libcudnn_ops_train.so.8.9.5\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               96\nOn-line CPU(s) list:                  0-95\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 7V13 64-Core Processor\nCPU family:                           25\nModel:                                1\nThread(s) per core:                   1\nCore(s) per socket:                   48\nSocket(s):                            2\nStepping:                             1\nBogoMIPS:                             4890.88\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr rdpru arat umip vaes vpclmulqdq rdpid fsrm\nHypervisor vendor:                    Microsoft\nVirtualization type:                  full\nL1d cache:                            3 MiB (96 instances)\nL1i cache:                            3 MiB (96 instances)\nL2 cache:                             48 MiB (96 instances)\nL3 cache:                             384 MiB (12 instances)\nNUMA node(s):                         4\nNUMA node0 CPU(s):                    0-23\nNUMA node1 CPU(s):                    24-47\nNUMA node2 CPU(s):                    48-71\nNUMA node3 CPU(s):                    72-95\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; safe RET, no microcode\nVulnerability Spec store bypass:      Vulnerable\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] flake8==4.0.1.1\n[pip3] mypy==1.11.2\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.0\n[pip3] sentence-transformers==3.0.0\n[pip3] torch==2.5.1+cu124\n[pip3] torchvision==0.20.1+cu124\n[pip3] transformers==4.49.0\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.0\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV12    SYS     SYS     NODE    0-23    0               N/A\nGPU1    NV12     X      SYS     SYS     SYS     24-47   1               N/A\nGPU2    SYS     SYS      X      NV12    SYS     48-71   2               N/A\nGPU3    SYS     SYS     NV12     X      SYS     72-95   3               N/A\nNIC0    NODE    SYS     SYS     SYS      X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n\nNVIDIA_VISIBLE_DEVICES=1\nCUDA_VISIBLE_DEVICES=1\nCUDA_VISIBLE_DEVICES=1\nLD_LIBRARY_PATH=/home/coder/proxima-hosted-language-models/build/proxima-hosted-language-models/environments/development-venv/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:/usr/local/cuda/compat/:/usr/local/cuda/targets/x86_64-linux/lib/:/export/apps/hadoop/latest/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/compat/:/usr/local/cuda/targets/x86_64-linux/lib/:/export/apps/hadoop/latest/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/compat/:/usr/local/cuda/targets/x86_64-linux/lib/:/export/apps/hadoop/latest/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/compat/:/usr/local/cuda/targets/x86_64-linux/lib/:/export/apps/hadoop/latest/lib/native/:\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n</details>\n\n\n<details>\n<summary>Some changes on vLLM 0.6.1.post2</summary>\n\n```text\nvllm==0.6.1.post2\ntorch==2.4.0+cu121\ntorchvision==0.19.0\nvllm-flash-attn==2.6.1\nxformers==0.0.27.post2\ntransformers==4.46.0\npydantic==2.9.0\n```\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n**Summary**\nAfter upgrading from vLLM 0.6.1.post2 to vLLM 0.7.0, we observed a significant increase in TTFT. While e2e latency and throughput improved in v0.7.0, TTFT became 3x slower.\n\nWe use:\n- Our own model. The base model is Llama3 8B\n- QPS: 1.2\n- TTFT (v0.7.0): 0.39s \n- TTFT (v0.6.1.post2): 0.13s\n- Input Prompt: 4K tokens\n- Output Tokens: 100\n\n\n**Reproduction Steps**\n1. Get TTFT on v0.7.0\n- Run vLLM server\n```\nvllm serve /export/content/data/tmp/custom_model_path/proxima/model_resources \\\n    --tokenizer /export/content/data/tmp/custom_model_path/proxima/tokenizer_resources \\\n    --tensor-parallel-size 1 \\\n    --max-num-batched-tokens 2048 \\\n    --gpu-memory-utilization 0.9 \\\n    --enable-chunked-prefill \\\n    --use-v2-block-manager \\\n    --trust-remote-code \\\n    --guided-decoding-backend outlines\n\n```\n\n- Send requests to vLLM backend\n```\npython benchmark_serving.py --backend vllm --model (our model) --request-rate 1.2 --save-result\n```\n2. Get TTFT on v0.6.1.post2\n- Run vLLM server\n```\nvllm serve /export/content/data/tmp/custom_model_path/proxima/model_resources \\\n    --tokenizer /export/content/data/tmp/custom_model_path/proxima/tokenizer_resources \\\n    --tensor-parallel-size 1 \\\n    --gpu-memory-utilization 0.9 \\\n    --enable-chunked-prefill \\\n    --use-v2-block-manager \\\n    --trust-remote-code \\\n    --guided-decoding-backend outlines\n\n```\n- Send requests to vLLM backend\n```\npython benchmark_serving.py --backend vllm --model (our model) --request-rate 1.2 --save-result\n```\n3. Compare TTFT results\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-14T22:12:49+00:00",
    "closed_at": "2025-07-13T02:15:01+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14845/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14845"
  },
  {
    "number": 15286,
    "title": "[Installation]: installation succeeded but No module named 'vllm._C'",
    "body": "### Your current environment\n\nMy operation system is Windows, and i use anaconda3 to manage my virtual environment and site-packages. I create a new virtual environment with python 3.10.\n\nI ran the command  ```pip install vllm``` to install vllm in my system\nThe installation process is smooth, and it seems that i successfully installed vllm.\nHowever, when i tried to run ```vllm serve \"Qwen/Qwen2.5-VL-7B-Instruct\"```,  it occured such bug:\n```\nINFO 03-21 17:22:05 [__init__.py:256] Automatically detected platform cuda.\nTraceback (most recent call last):\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\vllm\\lib\\runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\vllm\\lib\\runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\vllm\\Scripts\\vllm.exe\\__main__.py\", line 4, in <module>\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\vllm\\lib\\site-packages\\vllm\\__init__.py\", line 11, in <module>\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\vllm\\lib\\site-packages\\vllm\\engine\\arg_utils.py\", line 22, in <module>\n    from vllm.executor.executor_base import ExecutorBase\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\vllm\\lib\\site-packages\\vllm\\executor\\executor_base.py\", line 16, in <module>\n    from vllm.model_executor.layers.sampler import SamplerOutput\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\vllm\\lib\\site-packages\\vllm\\model_executor\\layers\\sampler.py\", line 23, in <module>\n    from vllm.spec_decode.metrics import SpecDecodeWorkerMetrics\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\vllm\\lib\\site-packages\\vllm\\spec_decode\\metrics.py\", line 9, in <module>\n    from vllm.model_executor.layers.spec_decode_base_sampler import (\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\vllm\\lib\\site-packages\\vllm\\model_executor\\layers\\spec_decode_base_sampler.py\", line 10, in <module>\n    from vllm.platforms import current_platform\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\vllm\\lib\\site-packages\\vllm\\platforms\\__init__.py\", line 288, in __getattr__\n    _current_platform = resolve_obj_by_qualname(\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\vllm\\lib\\site-packages\\vllm\\utils.py\", line 1899, in resolve_obj_by_qualname\n    module = importlib.import_module(module_name)\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\vllm\\lib\\importlib\\__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\vllm\\lib\\site-packages\\vllm\\platforms\\cuda.py\", line 15, in <module>\n    import vllm._C  # noqa\nModuleNotFoundError: No module named 'vllm._C'\n```\n\n\n### How you are installing vllm\n\n```sh\npip install vllm\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2025-03-21T10:02:51+00:00",
    "closed_at": "2025-03-21T11:31:26+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15286/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15286"
  },
  {
    "number": 2324,
    "title": "Can't get the same results ",
    "body": "I try to adapte CogVLM to VLLM, but I got different result when using the same input and the same network weight\r\n\r\n**VLLM**\r\n\r\n```bash\r\nhiddens_states\r\ntensor([[[ 0.0019, -0.0034,  0.0021,  ..., -0.0099,  0.0027, -0.0037],\r\n         [-0.0073, -0.0082, -0.0618,  ...,  0.0200, -0.0043, -0.0006],\r\n         [-0.0889,  0.0262,  0.0144,  ..., -0.1797,  0.0549,  0.1406],\r\n         ...,\r\n         [-0.0205, -0.0162,  0.0045,  ...,  0.0172,  0.0013,  0.0128],\r\n         [-0.0049, -0.0032, -0.0046,  ...,  0.0201,  0.0166,  0.0149],\r\n         [ 0.0040,  0.0019,  0.0052,  ..., -0.0051,  0.0153,  0.0014]]],\r\n       device='cuda:0', dtype=torch.bfloat16)\r\ninput_layernorm\r\nParameter containing:\r\ntensor([0.0266, 0.0114, 0.0044,  ..., 0.0113, 0.0120, 0.0052], device='cuda:0',\r\n       dtype=torch.bfloat16, requires_grad=True)\r\nhidden_states\r\ntensor([[[-0.0889,  0.0262,  0.0144,  ..., -0.1797,  0.0549,  0.1406],\r\n         [ 0.0332,  0.0322,  0.1157,  ...,  0.1309, -0.0688,  0.0781],\r\n         [-0.1050,  0.2734, -0.1230,  ..., -0.0879,  0.0874,  0.0047],\r\n         ...,\r\n         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\r\n         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\r\n         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\r\n       device='cuda:0', dtype=torch.bfloat16)\r\ntensor([[[ 0.0173,  0.0126, -0.0182,  ..., -0.0106,  0.0649,  0.0432],\r\n         [ 0.0728,  0.0918, -0.1826,  ...,  0.0996, -0.1011,  0.1865],\r\n         [ 0.0806,  0.1357, -0.0337,  ...,  0.1074, -0.1387,  0.1118],\r\n         ...,\r\n         [-0.0264,  0.0019,  0.0317,  ...,  0.0221, -0.0586,  0.0261],\r\n         [-0.0264,  0.0019,  0.0315,  ...,  0.0221, -0.0588,  0.0261],\r\n         [-0.0264,  0.0019,  0.0315,  ...,  0.0221, -0.0586,  0.0260]]],\r\n       device='cuda:0', dtype=torch.bfloat16)\r\n```\r\n\r\n**Original CogVLM**\r\n\r\n\r\n```bash\r\ntensor([[[ 0.0019, -0.0034,  0.0021,  ..., -0.0099,  0.0027, -0.0037],\r\n         [-0.0073, -0.0082, -0.0618,  ...,  0.0200, -0.0043, -0.0006],\r\n         [-0.0889,  0.0262,  0.0144,  ..., -0.1797,  0.0549,  0.1406],\r\n         ...,\r\n         [-0.0205, -0.0162,  0.0045,  ...,  0.0172,  0.0013,  0.0128],\r\n         [-0.0049, -0.0032, -0.0046,  ...,  0.0201,  0.0166,  0.0149],\r\n         [ 0.0040,  0.0019,  0.0052,  ..., -0.0051,  0.0153,  0.0014]]],\r\n       device='cuda:0', dtype=torch.bfloat16)\r\nParameter containing:\r\ntensor([0.0266, 0.0114, 0.0044,  ..., 0.0113, 0.0120, 0.0052], device='cuda:0',\r\n       dtype=torch.bfloat16, requires_grad=True)\r\ntensor([[[ 0.0060, -0.0047,  0.0011,  ..., -0.0135,  0.0040, -0.0023],\r\n         [-0.0079, -0.0038, -0.0111,  ...,  0.0092, -0.0021, -0.0001],\r\n         [-0.0209,  0.0026,  0.0006,  ..., -0.0178,  0.0058,  0.0065],\r\n         ...,\r\n         [-0.0469, -0.0159,  0.0017,  ...,  0.0167,  0.0013,  0.0057],\r\n         [-0.0085, -0.0024, -0.0013,  ...,  0.0148,  0.0130,  0.0051],\r\n         [ 0.0118,  0.0024,  0.0025,  ..., -0.0063,  0.0201,  0.0008]]],\r\n       device='cuda:0', dtype=torch.bfloat16)\r\n```\r\n\r\n```py\r\n        if token_type_ids is not None:\r\n            print(\"hiddens_states\")\r\n            print(hidden_states)\r\n        if residual is None:\r\n            residual = hidden_states\r\n            hidden_states = self.input_layernorm(hidden_states)\r\n        else:\r\n            hidden_states, residual = self.input_layernorm(\r\n                hidden_states, residual)\r\n        if token_type_ids is not None:\r\n            print(\"input_layernorm\")\r\n            print(self.input_layernorm.weight)\r\n            print(\"hidden_states\")\r\n            print(hidden_states)\r\n```\r\n\r\nSo how can I fix the error\uff1f",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-01-03T03:26:52+00:00",
    "closed_at": "2024-11-30T02:03:13+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2324/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2324"
  },
  {
    "number": 16905,
    "title": "[Bug]: architecture of models not correctly recognized",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 4.0.0\nLibc version: glibc-2.35\n\nPython version: 3.12.9 | packaged by conda-forge | (main, Mar  4 2025, 22:48:41) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-5.15.0-134-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H100 PCIe\nNvidia driver version: 535.230.02\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               128\nOn-line CPU(s) list:                  0-127\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 9354 32-Core Processor\nCPU family:                           25\nModel:                                17\nThread(s) per core:                   2\nCore(s) per socket:                   32\nSocket(s):                            2\nStepping:                             1\nFrequency boost:                      enabled\nCPU max MHz:                          3799.0720\nCPU min MHz:                          1500.0000\nBogoMIPS:                             6490.17\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov succor smca fsrm flush_l1d\nVirtualization:                       AMD-V\nL1d cache:                            2 MiB (64 instances)\nL1i cache:                            2 MiB (64 instances)\nL2 cache:                             64 MiB (64 instances)\nL3 cache:                             512 MiB (16 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-31,64-95\nNUMA node1 CPU(s):                    32-63,96-127\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pytorch-lightning==2.5.1\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchmetrics==1.7.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.3\n[pip3] triton==3.2.0\n[conda] nomkl                     1.0                  h5ca1d4c_0    conda-forge\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pytorch-lightning         2.5.1                    pypi_0    pypi\n[conda] pyzmq                     26.3.0          py312hbf22597_0    conda-forge\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchmetrics              1.7.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] transformers              4.51.3                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.4\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-31,64-95      0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNVIDIA_VISIBLE_DEVICES=GPU-8fc5bea3-6d2a-c959-8273-340512c72a9e\nCUDA_VERSION=12.1\nPYTORCH_VERSION=2.5.1\nCUDA_HOME=/usr/local/cuda-12.1\nCUDA_HOME=/usr/local/cuda-12.1\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI tried several models to run in latest vllm, but although the vllm documentation says the model is supported, during runtime it says the architecture is not yet supported. It seems that there is a mismatch between the expected model identifier and the actual one found. The documentation e.g. says OLMo2, but in the error message it says olmo2.\nI use the official GGUF from AllenAI. Therefore, it should not be the problem of the model. \nThe same applies for Nvidia Nemotron. The vllm documentation says DeciLM is supported, but during runtime it says GGUF model with architecture deci is not supported yet.\nQwen QwQ on the other hand is loading without any problems with the same setup. \n\n\nfrom huggingface_hub import hf_hub_download\nfrom vllm import LLM, SamplingParams\nmodel_id = \"allenai/OLMo-2-0325-32B-Instruct\"\nrepo_id = \"allenai/OLMo-2-0325-32B-Instruct-GGUF\"\nfilename = \"OLMo-2-0325-32B-Instruct-Q4_K_S.gguf\"\nmodel = hf_hub_download(repo_id, filename=filename)\nllm = LLM(model=model,\n              tokenizer=model_id, max_model_len = 8192,\n              gpu_memory_utilization=0.80)\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-04-21T06:33:09+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16905/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16905"
  },
  {
    "number": 2338,
    "title": "OOM while loading THUDM/chatglm-6b-int4",
    "body": "I am trying this code to load THUDM/chatglm-6b-int4 on a single GPU:\r\n`llm = LLM(model=model_path, trust_remote_code=True)`\r\n\r\nHowever it raises an OOM exception:\r\n\r\n> Traceback (most recent call last):\r\n  File \"demo_vllm.py\", line 15, in <module>\r\n    llm = LLM(model=\"chatglm-6b-int4\",\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/entrypoints/llm.py\", line 105, in __init__\r\n    self.llm_engine = LLMEngine.from_engine_args(engine_args)\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/llm_engine.py\", line 250, in from_engine_args\r\n    engine = cls(*engine_configs,\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/llm_engine.py\", line 110, in __init__\r\n    self._init_workers(distributed_init_method)\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/llm_engine.py\", line 146, in _init_workers\r\n    self._run_workers(\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/llm_engine.py\", line 755, in _run_workers\r\n    self._run_workers_in_batch(workers, method, *args, **kwargs))\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/llm_engine.py\", line 729, in _run_workers_in_batch\r\n    output = executor(*args, **kwargs)\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/worker/worker.py\", line 79, in load_model\r\n    self.model_runner.load_model()\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/worker/model_runner.py\", line 57, in load_model\r\n    self.model = get_model(self.model_config)\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/model_loader.py\", line 65, in get_model\r\n    model = model_class(model_config.hf_config, linear_method)\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/models/chatglm.py\", line 333, in __init__\r\n    self.transformer = ChatGLMModel(config, linear_method)\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/models/chatglm.py\", line 299, in __init__\r\n    self.encoder = GLMTransformer(config, linear_method)\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/models/chatglm.py\", line 254, in __init__\r\n    [GLMBlock(config, linear_method) for i in range(self.num_layers)])\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/models/chatglm.py\", line 254, in <listcomp>\r\n    [GLMBlock(config, linear_method) for i in range(self.num_layers)])\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/models/chatglm.py\", line 196, in __init__\r\n    self.mlp = GLMMLP(config, linear_method)\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/models/chatglm.py\", line 138, in __init__\r\n    self.dense_h_to_4h = MergedColumnParallelLinear(\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/layers/linear.py\", line 256, in __init__\r\n    super().__init__(input_size, sum(output_sizes), bias, gather_output,\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/layers/linear.py\", line 176, in __init__\r\n    self.linear_weights = self.linear_method.create_weights(\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/layers/linear.py\", line 55, in create_weights\r\n    weight = Parameter(torch.empty(output_size_per_partition,\r\n  File \"miniconda3/envs/vllm/lib/python3.9/site-packages/torch/utils/_device.py\", line 77, in __torch_function__\r\n    return func(*args, **kwargs)\r\n\r\n> torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 214.00 MiB. GPU 0 has a total capacty of 10.75 GiB of which 134.50 MiB is free. Including non-PyTorch memory, this process has 10.61 GiB memory in use. Of the allocated memory 10.29 GiB is allocated by PyTorch, and 1.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\n-------------------------------------------------------\r\nI have tried `https://github.com/vllm-project/vllm/issues/2176` and set `gpu_memory_utilization`, not work.\r\nI also tried `https://github.com/vllm-project/vllm/issues/1949` and set `max_seq_len`, not work.\r\nI saw `https://github.com/vllm-project/vllm/issues/2304`, does it means that this situation is expected?\r\n\r\n-------------------------------------------------------\r\nNote that I downgrade the `transformers` to `4.29.2` to avoid the tokenizer loading exception.",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-04T02:13:23+00:00",
    "closed_at": "2024-03-06T09:58:12+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2338/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2338"
  },
  {
    "number": 10605,
    "title": "[Usage]: Why `use_beam_search` is eliminated in `vllm.SamplingParams` from v0.6.3?",
    "body": "### Your current environment\n\nI am asking a general API question regarding `vllm`, therefore, env info is not needed.\n\n### How would you like to use vllm\n\nI want to ask why `use_beam_search` is eliminated in `vllm.SamplingParams` from v0.6.3 (https://docs.vllm.ai/en/v0.6.3/dev/sampling_params.html)?\r\n\r\nHow can we control the usage of beam search from v0.6.3 onwards?\r\n\r\nTo the best of my knowledge, `use_beam_search` is supported in all versions from [v0.4.0.post1](https://docs.vllm.ai/en/v0.4.0.post1/dev/sampling_params.html) to [v0.6.2](https://docs.vllm.ai/en/v0.6.2/dev/sampling_params.html).\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-11-24T08:40:38+00:00",
    "closed_at": "2024-11-25T12:37:50+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10605/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10605"
  },
  {
    "number": 7772,
    "title": "[Bug]: Connection closed by peer.",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Alibaba Group Enterprise Linux Server 7.2 (Paladin) (x86_64)\r\nGCC version: (GCC) 9.2.1 20200522 (Alibaba 9.2.1-3 2.17)\r\nClang version: Could not collect\r\nCMake version: version 3.29.3\r\nLibc version: glibc-2.30\r\n\r\nPython version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-4.9.151-015.ali3000.alios7.x86_64-x86_64-with-glibc2.17\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA L40S\r\nGPU 1: NVIDIA L40S\r\nGPU 2: NVIDIA L40S\r\nGPU 3: NVIDIA L40S\r\n\r\nNvidia driver version: 535.129.03\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn.so.8.9.3\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.9.3\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.9.3\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.9.3\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.9.3\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.9.3\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.9.3\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                128\r\nOn-line CPU(s) list:   0-11,24-75,88-127\r\nOff-line CPU(s) list:  12-23,76-87\r\nThread(s) per core:    1\r\nCore(s) per socket:    32\r\nSocket(s):             2\r\nNUMA node(s):          2\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 143\r\nModel name:            Intel(R) Xeon(R) Gold 6462C\r\nStepping:              8\r\nCPU MHz:               3899.816\r\nCPU max MHz:           3900.0000\r\nCPU min MHz:           800.0000\r\nBogoMIPS:              6600.00\r\nVirtualization:        VT-x\r\nL1d cache:             48K\r\nL1i cache:             32K\r\nL2 cache:              2048K\r\nL3 cache:              61440K\r\nNUMA node0 CPU(s):     0-31,64-95\r\nNUMA node1 CPU(s):     32-63,96-127\r\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb invpcid_single cat_l2 cdp_l3 ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req avx512vbmi pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq pconfig flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] atorch==1.1.0rc8\r\n[pip3] flake8==6.1.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.24.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==8.9.2.26\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.555.43\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.5.40\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pynvml==11.4.1\r\n[pip3] pyzmq==25.1.2\r\n[pip3] torch==2.3.0\r\n[pip3] torchaudio==2.1.0+cu121\r\n[pip3] torchpippy==0.1.1+cecc4fc\r\n[pip3] torchvision==0.16.0+cu121\r\n[pip3] transformers==4.41.1\r\n[pip3] triton==2.3.0\r\n[pip3] triton-on-ray==0.3.1\r\n[conda] atorch                    1.1.0rc8                 pypi_0    pypi\r\n[conda] numpy                     1.23.1                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         8.9.2.26                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.535.133               pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.5.40                  pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\r\n[conda] pynvml                    11.4.1                   pypi_0    pypi\r\n[conda] pyzmq                     25.1.2                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] torchaudio                2.1.0+cu121              pypi_0    pypi\r\n[conda] torchpippy                0.1.1+cecc4fc            pypi_0    pypi\r\n[conda] torchvision               0.16.0+cu121             pypi_0    pypi\r\n[conda] transformers              4.41.1                   pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PIX     SYS     SYS     32-63,96-127    1               N/A\r\nGPU1    PIX      X      SYS     SYS     32-63,96-127    1               N/A\r\nGPU2    SYS     SYS      X      PIX     32-63,96-127    1               N/A\r\nGPU3    SYS     SYS     PIX      X      32-63,96-127    1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148] Error executing method start_worker_execution_loop. This might cause deadlock in distributed execution.\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148] Traceback (most recent call last):\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148]   File \"/opt/conda/lib/python3.8/site-packages/vllm/worker/worker_base.py\", line 140, in execute_method\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148]     return executor(*args, **kwargs)\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148]   File \"/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148]     return func(*args, **kwargs)\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148]   File \"/opt/conda/lib/python3.8/site-packages/vllm/spec_decode/spec_decode_worker.py\", line 313, in start_worker_execution_loop\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148]     while self._run_non_driver_rank():\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148]   File \"/opt/conda/lib/python3.8/site-packages/vllm/spec_decode/spec_decode_worker.py\", line 371, in _run_non_driver_rank\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148]     data = broadcast_tensor_dict(src=self._driver_rank)\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148]   File \"/opt/conda/lib/python3.8/site-packages/vllm/distributed/communication_op.py\", line 284, in broadcast_tensor_dict\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148]     torch.distributed.broadcast_object_list(recv_metadata_list,\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148]   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148]     return func(*args, **kwargs)\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148]   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 2649, in broadcast_object_list\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148]     broadcast(object_sizes_tensor, src=src, group=group)\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148]   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148]     return func(*args, **kwargs)\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148]   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 2144, in broadcast\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148]     work.wait()\r\n2024-08-22 13:50:25 ERROR 200507 [worker_base.py:148] RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-22T06:08:52+00:00",
    "closed_at": "2024-12-22T02:04:23+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7772/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7772"
  },
  {
    "number": 11965,
    "title": "[Doc]: Invalid JSON examples in Engine Args Document",
    "body": "### \ud83d\udcda The doc issue\n\nOn page https://docs.vllm.ai/en/latest/serving/engine_args.html#engine-args\r\n\r\nRegarding the flag `--override-pooler-config`. The documentation provides the following example:\r\n\r\n> Override or set the pooling method for pooling models. e.g. {\u201cpooling_type\u201d: \u201cmean\u201d, \u201cnormalize\u201d: false}.\u2019\r\n\r\nHowever this example does not work if copy-pasted into a UTF-8 aware text editor as it is not a valid JSON document. (The quotation marks are not ascii quotation marks, they are left-quote and right-quote.) This is an insidious error as it is nearly invisible to the naked eye.\r\n\r\nIn addition to `--override-pooler-config`, this issue affects `--override-neuron-config`, `--rope-scaling`, and `--mm-processor-kwargs`.\n\n### Suggest a potential alternative/fix\n\nChange\r\n\r\n> Override or set the pooling method for pooling models. e.g. {\u201cpooling_type\u201d: \u201cmean\u201d, \u201cnormalize\u201d: false}.\u2019\r\n\r\nto\r\n\r\n> Override or set the pooling method for pooling models. e.g. `{\"pooling_type\": \"mean\", \"normalize\": false}`.\u2019\r\n\r\n(Replace non-ascii quotes with ascii quotes and surround with a code block to ensure verbatim inclusion.)\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation",
      "help wanted",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2025-01-12T05:16:06+00:00",
    "closed_at": "2025-01-14T17:03:06+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11965/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11965"
  },
  {
    "number": 9158,
    "title": "[Misc]: How to set num-scheduler-steps",
    "body": "### Anything you want to discuss about vllm.\n\nRecently **num-scheduler-steps** was introduced to \"set the maximum number of forward steps per scheduler call\". Is there any documentation on what this exactly means?\r\nAlso some guidance would on how to set this value would be much appreciated. For example, if I host a 70B model on 2x A100 with 80GB, does this narrow down the range of values I should consider?\r\n\r\nThanks to all the amazing vllm contributers for making this great peace of software! \ud83c\udfce\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-08T13:23:11+00:00",
    "closed_at": "2025-02-07T01:59:39+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9158/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9158"
  },
  {
    "number": 1333,
    "title": "Question: why could different ray.Workers produce same random sampling result when processing the same prob tensor?",
    "body": "I am referring to the `_random_sample` method from `vllm/model_executor/layers/sampler.py`",
    "labels": [],
    "state": "closed",
    "created_at": "2023-10-12T13:11:18+00:00",
    "closed_at": "2024-03-13T11:30:21+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1333/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1333"
  }
]
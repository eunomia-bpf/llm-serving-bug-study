[
  {
    "number": 12443,
    "title": "[Bug]: nrt_tensor_allocate status=4 message=\"Allocation Failure\" on AWS Neuron",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 01-26 10:23:53 __init__.py:183] Automatically detected platform neuron.\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-6.8.0-1021-aws-x86_64-with-glibc2.35\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               32\nOn-line CPU(s) list:                  0-31\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 7R13 Processor\nCPU family:                           25\nModel:                                1\nThread(s) per core:                   2\nCore(s) per socket:                   16\nSocket(s):                            1\nStepping:                             1\nBogoMIPS:                             5299.96\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            512 KiB (16 instances)\nL1i cache:                            512 KiB (16 instances)\nL2 cache:                             8 MiB (16 instances)\nL3 cache:                             64 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-7,16-23\nNUMA node1 CPU(s):                    8-15,24-31\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Vulnerable: Safe RET, no microcode\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.25.2\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.5.1\n[pip3] torch-neuronx==2.5.1.2.4.0\n[pip3] torch-xla==2.5.1\n[pip3] torchvision==0.16.2\n[pip3] transformers==4.47.1\n[pip3] transformers-neuronx==0.13.380\n[pip3] triton==3.0.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: (0, 'instance-type: inf2.8xlarge\\ninstance-id: i-03ebc5b844ad70efa\\n+--------+--------+--------+---------+\\n| NEURON | NEURON | NEURON |   PCI   |\\n| DEVICE | CORES  | MEMORY |   BDF   |\\n+--------+--------+--------+---------+\\n| 0      | 2      | 32 GB  | 00:1f.0 |\\n+--------+--------+--------+---------+', '')\nvLLM Version: 0.6.6.post2.dev332+g16366ee8\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nLD_LIBRARY_PATH=/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/cv2/../../lib64:\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\n```\n</details>\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nI've followed the setup procedure for Neuron here: https://docs.vllm.ai/en/latest/getting_started/installation/ai_accelerator/index.html\nI have also tried various models and inf2 configurations, the details below are from inf2.8xlarge.\n\n<details>\n<summary>The output of `vllm serve meta-llama/Llama-3.2-1B-Instruct   --dtype auto   --max-model-len 2048   --max-num-batched-tokens 2048  --device neuron --gpu-memory-utilization 0.9`</summary>\n\n```text\nINFO 01-26 10:27:15 __init__.py:183] Automatically detected platform neuron.\nINFO 01-26 10:27:16 api_server.py:768] vLLM API server version 0.6.6.post2.dev332+g16366ee8\nINFO 01-26 10:27:16 api_server.py:769] args: Namespace(subparser='serve', model_tag='meta-llama/Llama-3.2-1B-Instruct', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='meta-llama/Llama-3.2-1B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=2048, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=2048, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='neuron', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, enable_sleep_mode=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x773d23f4a290>)\nINFO 01-26 10:27:16 api_server.py:195] Started engine process with PID 2998\n/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\nINFO 01-26 10:27:21 __init__.py:183] Automatically detected platform neuron.\nINFO 01-26 10:27:22 config.py:528] This model supports multiple tasks: {'generate', 'classify', 'score', 'embed', 'reward'}. Defaulting to 'generate'.\nWARNING 01-26 10:27:22 config.py:664] Async output processing is not supported on the current platform type neuron.\nINFO 01-26 10:27:27 config.py:528] This model supports multiple tasks: {'embed', 'score', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\nWARNING 01-26 10:27:27 config.py:664] Async output processing is not supported on the current platform type neuron.\nINFO 01-26 10:27:27 llm_engine.py:232] Initializing an LLM engine (v0.6.6.post2.dev332+g16366ee8) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \nWARNING 01-26 10:27:28 neuron.py:54] Pin memory is not supported on Neuron.\nWARNING 01-26 10:27:28 neuron_model_runner.py:89] On-device sampling is turned on in Neuron by default, only top_k, top_p, and temperature are current supported sampling parameters. To turn off the on-device sampling, please set the environment variable NEURON_ON_DEVICE_SAMPLING_DISABLED=1.\nWARNING 01-26 10:27:28 _custom_ops.py:19] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: Set seed for `privateuseone` device does not take effect, please add API's `_is_in_bad_fork` and `manual_seed_all` to `privateuseone` device module.\n  return fn(*args, **kwargs)\nWARNING 01-26 10:27:28 config.py:3358] Current VLLM config is not set.\n2025-Jan-26 10:27:42.285788  2998:2998  ERROR  TDRV:dmem_alloc_internal                     Failed to alloc DEVICE memory: 536870912\n2025-Jan-26 10:27:42.289392  2998:2998  ERROR  TDRV:dml_dump                                Wrote nrt memory alloc debug info to /tmp/nrt_mem_log_device_0_67960e1e.csv\n2025-Jan-26 10:27:42.291398  2998:2998  ERROR  TDRV:log_dev_mem                             Failed to allocate 512.000MB (usage: tensors) on ND 0:NC 0, current utilization:\n\t* total: 15.086GB\n\t* model code: 0.000B\n\t* model constants: 0.000B\n\t* tensors: 15.086GB\n\t* shared scratchpad: 0.000B\n\t* scratchpad: 0.000B\n\t* runtime: 1.062KB\n\t* dma rings io: 0.000B\n\t* dma rings spill: 0.000B\n\t* dma rings collectives: 0.000B\n\t* dma rings runtime: 32.000KB\n\t* collectives: 0.000B\n\t* xt cc: 0.000B\n\n2025-Jan-26 10:27:42.299439  2998:2998  ERROR  TDRV:tensor_allocate                         Failed to allocate 536870912 bytes on DEVICE for tensor UNKNOWN.\nERROR 01-26 10:27:42 engine.py:381] nrt_tensor_allocate status=4 message=\"Allocation Failure\"\nERROR 01-26 10:27:42 engine.py:381] Traceback (most recent call last):\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 372, in run_mp_engine\nERROR 01-26 10:27:42 engine.py:381]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 120, in from_engine_args\nERROR 01-26 10:27:42 engine.py:381]     return cls(ipc_path=ipc_path,\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 72, in __init__\nERROR 01-26 10:27:42 engine.py:381]     self.engine = LLMEngine(*args, **kwargs)\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 271, in __init__\nERROR 01-26 10:27:42 engine.py:381]     self.model_executor = executor_class(vllm_config=vllm_config, )\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 49, in __init__\nERROR 01-26 10:27:42 engine.py:381]     self._init_executor()\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 40, in _init_executor\nERROR 01-26 10:27:42 engine.py:381]     self.collective_rpc(\"load_model\")\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 49, in collective_rpc\nERROR 01-26 10:27:42 engine.py:381]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/utils.py\", line 2208, in run_method\nERROR 01-26 10:27:42 engine.py:381]     return func(*args, **kwargs)\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/worker/neuron_worker.py\", line 66, in load_model\nERROR 01-26 10:27:42 engine.py:381]     self.model_runner.load_model()\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/worker/neuron_model_runner.py\", line 108, in load_model\nERROR 01-26 10:27:42 engine.py:381]     self.model = get_neuron_model(\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/model_executor/model_loader/neuron.py\", line 203, in get_neuron_model\nERROR 01-26 10:27:42 engine.py:381]     model.load_weights(model_config.model,\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/model_executor/model_loader/neuron.py\", line 112, in load_weights\nERROR 01-26 10:27:42 engine.py:381]     self.model.to_neuron()\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/transformers_neuronx/base.py\", line 84, in to_neuron\nERROR 01-26 10:27:42 engine.py:381]     self.load_weights()\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/transformers_neuronx/llama/model.py\", line 196, in load_weights\nERROR 01-26 10:27:42 engine.py:381]     new_layer.to_neuron()\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/transformers_neuronx/decoder.py\", line 1695, in to_neuron\nERROR 01-26 10:27:42 engine.py:381]     self.init_caches()\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/transformers_neuronx/decoder.py\", line 1816, in init_caches\nERROR 01-26 10:27:42 engine.py:381]     self.attn_v_cache[batch_size] = (manipulator.shard_along(cpu_cache, dim=2))\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/transformers_neuronx/parallel.py\", line 135, in shard_along\nERROR 01-26 10:27:42 engine.py:381]     return ops.parallel_to_nc(self.shard_along_on_cpu(tensor, dim))\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/transformers_neuronx/ops.py\", line 49, in parallel_to_nc\nERROR 01-26 10:27:42 engine.py:381]     return torch.ops.neuron._parallel_to_neuron(tensors)\nERROR 01-26 10:27:42 engine.py:381]   File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/torch/_ops.py\", line 1116, in __call__\nERROR 01-26 10:27:42 engine.py:381]     return self._op(*args, **(kwargs or {}))\nERROR 01-26 10:27:42 engine.py:381] RuntimeError: nrt_tensor_allocate status=4 message=\"Allocation Failure\"\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 383, in run_mp_engine\n    raise e\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 372, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 120, in from_engine_args\n    return cls(ipc_path=ipc_path,\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 72, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 271, in __init__\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 49, in __init__\n    self._init_executor()\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 40, in _init_executor\n    self.collective_rpc(\"load_model\")\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 49, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/utils.py\", line 2208, in run_method\n    return func(*args, **kwargs)\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/worker/neuron_worker.py\", line 66, in load_model\n    self.model_runner.load_model()\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/worker/neuron_model_runner.py\", line 108, in load_model\n    self.model = get_neuron_model(\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/model_executor/model_loader/neuron.py\", line 203, in get_neuron_model\n    model.load_weights(model_config.model,\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/model_executor/model_loader/neuron.py\", line 112, in load_weights\n    self.model.to_neuron()\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/transformers_neuronx/base.py\", line 84, in to_neuron\n    self.load_weights()\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/transformers_neuronx/llama/model.py\", line 196, in load_weights\n    new_layer.to_neuron()\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/transformers_neuronx/decoder.py\", line 1695, in to_neuron\n    self.init_caches()\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/transformers_neuronx/decoder.py\", line 1816, in init_caches\n    self.attn_v_cache[batch_size] = (manipulator.shard_along(cpu_cache, dim=2))\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/transformers_neuronx/parallel.py\", line 135, in shard_along\n    return ops.parallel_to_nc(self.shard_along_on_cpu(tensor, dim))\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/transformers_neuronx/ops.py\", line 49, in parallel_to_nc\n    return torch.ops.neuron._parallel_to_neuron(tensors)\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/torch/_ops.py\", line 1116, in __call__\n    return self._op(*args, **(kwargs or {}))\nRuntimeError: nrt_tensor_allocate status=4 message=\"Allocation Failure\"\nTraceback (most recent call last):\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/bin/vllm\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/scripts.py\", line 201, in main\n    args.dispatch_function(args)\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/scripts.py\", line 42, in serve\n    uvloop.run(run_server(args))\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/uvloop/__init__.py\", line 82, in run\n    return loop.run_until_complete(wrapper())\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 796, in run_server\n    async with build_async_engine_client(args) as engine_client:\n  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 125, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 219, in build_async_engine_client_from_engine_args\n    raise RuntimeError(\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\n```\n</details>\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-01-26T10:43:56+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12443/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12443"
  },
  {
    "number": 3497,
    "title": "[Bug]: ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`",
    "body": "### Your current environment\n\n```text\r\npython3 benchmarks/benchmark_throughput.py  --backend hf --hf-max-batch-size 20 --model /data/pretrain_models/Baichuan2-7B-Chat --trust-remote-code --input-len 512 --output-len 2048 --num-prompts 20\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nWarning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\r\nWarning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\r\nWarning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\r\nLoading checkpoint shards:   0%|                                                                                                                                                                        | 0/8 [00:00<?, ?it/s]/data/luhairong/anaconda3/envs/llm2/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:36<00:00,  4.59s/it]\r\n  0%|                                                                                                                                                                                                  | 0/20 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"/data/luhairong/work/LLM/vllm/vllm-0.3.3/benchmarks/benchmark_throughput.py\", line 340, in <module>\r\n    main(args)\r\n  File \"/data/luhairong/work/LLM/vllm/vllm-0.3.3/benchmarks/benchmark_throughput.py\", line 219, in main\r\n    elapsed_time = run_hf(requests, args.model, tokenizer, args.n,\r\n  File \"/data/luhairong/work/LLM/vllm/vllm-0.3.3/benchmarks/benchmark_throughput.py\", line 154, in run_hf\r\n    input_ids = tokenizer(batch, return_tensors=\"pt\",\r\n  File \"/data/luhairong/anaconda3/envs/llm2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2829, in __call__\r\n    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\r\n  File \"/data/luhairong/anaconda3/envs/llm2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2915, in _call_one\r\n    return self.batch_encode_plus(\r\n  File \"/data/luhairong/anaconda3/envs/llm2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 3097, in batch_encode_plus\r\n    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\r\n  File \"/data/luhairong/anaconda3/envs/llm2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2734, in _get_padding_truncation_strategies\r\n    raise ValueError(\r\nValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\r\n  0%|   ",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-19T11:29:16+00:00",
    "closed_at": "2024-11-29T02:07:39+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3497/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3497"
  },
  {
    "number": 5809,
    "title": "[Feature]: MLPSpeculator Tensor Parallel support",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\r\n\r\n`MLPSpeculator`-based speculative decoding was recently added in https://github.com/vllm-project/vllm/pull/4947, but the initial integration only covers single GPU usage.\r\n\r\nThere will soon be \"speculator\" models available for larger target models that require multiple GPUs so we would like to ensure that TP can be used.\r\n\r\nThe first part of this issue would be testing it out in conjunction with https://github.com/vllm-project/vllm/pull/5414 and making necessary adjustments so that it will work with TP=1 for the speculator and TP=N for the target model.\r\n\r\nFollowing this we can look at having the speculator itself run with TP>1, but that may be more involved since it will require some distributed coordination of the sampling of each speculated token in the MLPSpeculator loop. It might be possible to avoid additional communication here by the having the sampler used by the speculator model use a dedicated `torch.Generator` for its sampling and doing this sampling in tandem across the ranks.\r\n\r\n@JRosenkranz already used `VocabParallelEmbedding` in the implementation so the model layers themselves should work fine.\r\n\r\ncc @cadedaniel @sirejdua @JRosenkranz @tdoublep \r\n",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-06-25T01:10:59+00:00",
    "closed_at": "2024-07-02T14:20:30+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5809/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/5809"
  },
  {
    "number": 19197,
    "title": "[Bug]: \u6a21\u578b\u8fd0\u884c\u671f\u95f4\uff0c\u62a5\u9519TimeoutError: RPC call to execute_model timed out.\uff0c\u5bfc\u81f4\u6a21\u578b\u9000\u51fa\u3002",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 24.04.2 LTS (x86_64)\nGCC version                  : (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\nClang version                : Could not collect\nCMake version                : Could not collect\nLibc version                 : glibc-2.39\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu126\nIs debug build               : False\nCUDA used to build PyTorch   : 12.6\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0] (64-bit runtime)\nPython platform              : Linux-6.6.87.1-microsoft-standard-WSL2-x86_64-with-glibc2.39\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.0.140\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : \nGPU 0: NVIDIA GeForce RTX 4090\nGPU 1: NVIDIA GeForce RTX 4090\nGPU 2: NVIDIA GeForce RTX 4090\nGPU 3: NVIDIA GeForce RTX 4090\n\nNvidia driver version        : 576.52\ncuDNN version                : Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.10.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.10.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.10.1\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.10.1\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.10.1\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.10.1\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.10.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.10.1\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               128\nOn-line CPU(s) list:                  0-127\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 9354 32-Core Processor\nCPU family:                           25\nModel:                                17\nThread(s) per core:                   2\nCore(s) per socket:                   64\nSocket(s):                            1\nStepping:                             1\nBogoMIPS:                             6490.34\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr arat npt nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm\nVirtualization:                       AMD-V\nHypervisor vendor:                    Microsoft\nVirtualization type:                  full\nL1d cache:                            2 MiB (64 instances)\nL1i cache:                            2 MiB (64 instances)\nL2 cache:                             64 MiB (64 instances)\nL3 cache:                             512 MiB (16 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-127\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Vulnerable: Safe RET, no microcode\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-cufile-cu12==1.11.1.6\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.0\n[pip3] torchaudio==2.7.0\n[pip3] torchvision==0.22.0\n[pip3] transformers==4.52.3\n[pip3] triton==3.3.0\n[conda] numpy                     2.2.6                    pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.6.4.1                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.6.80                  pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.6.77                  pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.6.77                  pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.5.1.17                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.3.0.4                 pypi_0    pypi\n[conda] nvidia-cufile-cu12        1.11.1.6                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.7.77                pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.7.1.2                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.5.4.2                 pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.3                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.26.2                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.6.85                  pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.6.77                  pypi_0    pypi\n[conda] pyzmq                     26.4.0                   pypi_0    pypi\n[conda] torch                     2.7.0                    pypi_0    pypi\n[conda] torchaudio                2.7.0                    pypi_0    pypi\n[conda] torchvision               0.22.0                   pypi_0    pypi\n[conda] transformers              4.52.3                   pypi_0    pypi\n[conda] triton                    3.3.0                    pypi_0    pypi\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.0\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n  Could not collect\n\n==============================\n     Environment Variables\n==============================\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n\u6a21\u578b\u542f\u52a8\u540e\uff0c\u8fde\u7eed\u8c03\u7528\u6a21\u578b\u4e00\u5929\uff0c\u7136\u540e\u62a5\u9519\u8be6\u60c5\u5982\u4e0b\uff1a\nERROR 06-05 16:27:39 [dump_input.py:68] Dumping input data\nERROR 06-05 16:27:39 [dump_input.py:70] V1 LLM engine (v0.9.0) with config: model='ds_32B', speculative_config=None, tokenizer='ds_32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=20000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=deepseek, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={\"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"cudagraph_capture_sizes\": [], \"max_capture_size\": 0}, \nERROR 06-05 16:27:39 [dump_input.py:78] Dumping scheduler output for model execution:\nERROR 06-05 16:27:39 [dump_input.py:79] SchedulerOutput(scheduled_new_reqs=[], scheduled_cached_reqs=[CachedRequestData(req_id='chatcmpl-35245641e2d2465aa6c7abdfc6cbd8a2', resumed_from_preemption=true, new_token_ids=[497, 330, 59879, 75355, 51154, 3837, 59258, 16, 20, 35727, 18493, 65676, 99660, 100008, 101915, 106641, 788, 330, 18, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 100358, 100008, 12, 71356, 101337, 100358, 101915, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 22, 35727, 18493, 100358, 100008, 107884, 101915, 106641, 788, 330, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 18493, 100358, 100008, 101915, 100082, 111619, 35727, 8863, 788, 330, 23, 24, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 101915, 102065, 118906, 102305, 100055, 9370, 100008, 8863, 788, 330, 22, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 100358, 100008, 18830, 101915, 65577, 9754, 6142, 29941, 788, 330, 18, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 30709, 100055, 100008, 101915, 100008, 8863, 788, 330, 18, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 100030, 106107, 101915, 106641, 788, 330, 18, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 107884, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 108801, 9754, 101915, 106641, 788, 330, 20, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 101915, 106523, 9909, 21515, 106523, 7552, 9370, 100008, 8863, 788, 330, 22, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 101915, 102431, 100030, 111521, 9370, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 100358, 100008, 12, 71356, 101337, 100358, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 100030, 106107, 101915, 100008, 8863, 788, 330, 20, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 100358, 100008, 105648, 101915, 100008, 8863, 788, 330, 20, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 101915, 108801, 111619, 35727, 8863, 788, 330, 15, 497, 330, 59879, 75355, 51154, 3837, 99946, 102888, 18493, 100358, 100008, 101915, 9370, 111619, 35727, 8863, 788, 330, 18, 20, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 18493, 100358, 100008, 101915, 100008, 8863, 788, 330, 16, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 100358, 100008, 12, 100169, 100358, 101915, 106641, 788, 330, 16, 23, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 100358, 100008, 105648, 101915, 106641, 788, 330, 22, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 101915, 102065, 118906, 102305, 100055, 9370, 106641, 788, 330, 20, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 100358, 100008, 12, 100169, 100358, 101915, 106641, 788, 330, 16, 23, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 101915, 102065, 102305, 111521, 9370, 106641, 788, 330, 18, 19, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 108801, 9754, 101915, 106641, 788, 330, 23, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 20, 35727, 18493, 100358, 100008, 105648, 101915, 100008, 8863, 788, 330, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 20, 35727, 18493, 100358, 100008, 12, 100169, 100358, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 101915, 102065, 102305, 111521, 9370, 100008, 8863, 788, 330, 16, 22, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 22, 35727, 18493, 21894, 100008, 7, 21894, 100008, 17714, 100358, 8, 9370, 101915, 106641, 788, 330, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 20, 35727, 101915, 107738, 100008, 8863, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 101915, 102065, 100030, 111521, 9370, 106641, 788, 330, 21, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 20, 35727, 18493, 65676, 99660, 100008, 107884, 101915, 100008, 8863, 788, 330, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 20, 35727, 101915, 102065, 102305, 111521, 9370, 100008, 8863, 788, 330, 18, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 20, 35727, 18493, 65676, 99660, 100008, 12, 68878, 99327, 100030, 106107, 101915, 106641, 788, 330, 17, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 99943, 101915, 107738, 100008, 8863, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 100082, 9754, 101915, 106641, 788, 330, 17, 22, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 12, 92894, 101915, 106641, 788, 330, 21, 23, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 22, 35727, 18493, 65676, 99660, 100008, 101915, 106641, 788, 330, 18, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 100030, 106107, 101915, 106641, 788, 330, 23, 497, 330, 59879, 75355, 51154, 3837, 59258, 22, 35727, 18493, 65676, 99660, 100008, 12, 68878, 99327, 100030, 106107, 101915, 106641, 788, 330, 17, 497, 330, 59879, 75355, 51154, 3837, 99946, 102888, 18493, 65676, 99660, 100008, 101915, 9370, 111619, 35727, 8863, 788, 330, 18, 20, 24, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 101915, 106641, 788, 330, 18, 17, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 30709, 100055, 100008, 101915, 106641, 788, 330, 18, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 71356, 30709, 100055, 100008, 101915, 100008, 8863, 788, 330, 19, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 21894, 100008, 7, 21894, 100008, 17714, 65676, 99660, 8, 101915, 106641, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 100358, 100008, 101200, 105676, 101915, 106641, 7, 18830, 101915, 104081, 101200, 84636, 330, 18, 13, 21, 22, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 105648, 101915, 100008, 8863, 788, 330, 16, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 101915, 100082, 111619, 35727, 8863, 788, 330, 24, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 116953, 100008, 101915, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 12, 108987, 100008, 101915, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 99943, 18493, 21894, 100008, 7, 21894, 100008, 17714, 65676, 99660, 8, 101915, 106641, 788, 330, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 30709, 100055, 100008, 101915, 106641, 788, 330, 23, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 20, 35727, 101915, 102065, 102305, 111521, 9370, 106641, 788, 330, 18, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 105648, 101915, 100008, 8863, 788, 330, 23, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 101200, 105676, 101915, 106641, 7, 18830, 101915, 104081, 101200, 84636, 330, 16, 19, 13, 18, 18, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 101200, 105676, 101915, 106641, 7, 18830, 101915, 104081, 101200, 84636, 330, 16, 16, 13, 20, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 101915, 102065, 102305, 111521, 9370, 100008, 8863, 788, 330, 16, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 101915, 102431, 100030, 111521, 9370, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 20, 35727, 18493, 65676, 99660, 100008, 12, 68878, 99327, 100030, 106107, 101915, 100008, 8863, 788, 330, 17, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 22, 35727, 101915, 102065, 102305, 111521, 9370, 106641, 788, 330, 18, 497, 330, 59879, 75355, 51154, 3837, 59258, 22, 35727, 18493, 65676, 99660, 100008, 12, 68878, 99327, 100030, 106107, 101915, 100008, 8863, 788, 330, 17, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 12, 100030, 21515, 111521, 101915, 100008, 8863, 788, 330, 19, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 101915, 102431, 102305, 111521, 9370, 100008, 8863, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 101915, 102065, 102305, 111521, 9370, 100008, 8863, 788, 330, 16, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 99943, 18493, 65676, 99660, 100008, 101915, 100008, 8863, 788, 330, 22, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 116953, 100008, 101915, 106641, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 100358, 100008, 105648, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 100358, 100008, 12, 71356, 101337, 100358, 101915, 106641, 788, 330, 20, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 101915, 100008, 8863, 788, 330, 16, 23, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 12, 102305, 21515, 111521, 100008, 101915, 106641, 788, 330, 17, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 100358, 100008, 101915, 106641, 788, 330, 18, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 21894, 100008, 7, 21894, 100008, 17714, 65676, 99660, 8, 101915, 106641, 788, 330, 17, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 101915, 100082, 111619, 35727, 8863, 788, 330, 16, 20, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 20, 35727, 18493, 65676, 99660, 100008, 101915, 106641, 788, 330, 18, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 101915, 102065, 118906, 102305, 100055, 9370, 106641, 788, 330, 16, 19, 497, 330, 59879, 75355, 51154, 3837, 99946, 104044, 18493, 100358, 100008, 101915, 9370, 111619, 35727, 8863, 788, 330, 18, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 101915, 107738, 106641, 788, 330, 17, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 101915, 102065, 118906, 102305, 100055, 9370, 106641, 788, 330, 19, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 12, 92894, 101915, 106641, 788, 330, 17, 20, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 116953, 100008, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 99943, 18493, 65676, 99660, 100008, 105648, 101915, 100008, 8863, 788, 330, 18, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 18830, 101915, 65577, 9754, 6142, 29941, 788, 330, 21, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 30709, 100055, 100008, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 30709, 100055, 100008, 101915, 106641, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 101915, 102431, 100030, 111521, 9370, 100008, 8863, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 101915, 100008, 8863, 788, 330, 18, 18, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 101915, 102431, 102305, 111521, 9370, 100008, 8863, 788, 330, 19, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 20, 35727, 18493, 65676, 99660, 100008, 12, 92894, 101915, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 71356, 30709, 100055, 100008, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 100358, 100008, 107884, 101915, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 99943, 18493, 65676, 99660, 100008, 105648, 101915, 106641, 788, 330, 18, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 101200, 105676, 101915, 106641, 7, 18830, 101915, 104081, 101200, 84636, 330, 23, 13, 21, 22, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 12, 108987, 100008, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 104044, 55286, 18493, 65676, 100358, 100008, 104005, 101915, 9370, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 22, 35727, 18493, 100358, 100008, 12, 100169, 100358, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 101915, 108801, 111619, 35727, 8863, 788, 330, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 20, 35727, 18493, 65676, 99660, 100008, 12, 92894, 101915, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 21894, 100008, 7, 21894, 100008, 17714, 100358, 8, 9370, 101915, 106641, 788, 330, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 101915, 107738, 106641, 788, 330, 17, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 101915, 107738, 106641, 788, 330, 19, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 100358, 100008, 105648, 101915, 100008, 8863, 788, 330, 19, 497, 330, 59879, 105765, 17992, 51154, 3837, 99946, 102888, 18493, 65676, 100358, 100008, 101915, 9370, 111619, 35727, 8863, 788, 330, 18, 20, 24, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 100358, 100008, 18830, 101915, 65577, 9754, 6142, 29941, 788, 330, 21, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 71356, 30709, 100055, 100008, 101915, 106641, 788, 330, 17, 17, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 12, 100030, 21515, 111521, 100008, 101915, 106641, 788, 330, 16, 19, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 99943, 18493, 100358, 100008, 107884, 101915, 100008, 8863, 788, 330, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 12, 92894, 101915, 100008, 8863, 788, 330, 24, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 20, 35727, 18493, 65676, 99660, 100008, 105648, 101915, 106641, 788, 330, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 101915, 102065, 102305, 111521, 9370, 100008, 8863, 788, 330, 16, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 20, 35727, 18493, 100358, 100008, 12, 100169, 100358, 101915, 106641, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 99943, 101915, 102065, 118906, 102305, 100055, 9370, 100008, 8863, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 101915, 108801, 111619, 35727, 8863, 788, 330, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 22, 35727, 18493, 100358, 100008, 12, 100169, 100358, 101915, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 101915, 102065, 118906, 102305, 100055, 9370, 100008, 8863, 788, 330, 21, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 101915, 102065, 118906, 102305, 100055, 9370, 106641, 788, 330, 16, 19, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 99943, 18493, 100358, 100008, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 101915, 106641, 788, 330, 20, 17, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 99943, 18493, 65676, 99660, 100008, 105648, 101915, 100008, 8863, 788, 330, 18, 497, 330, 59879, 75355, 51154, 3837, 59258, 22, 35727, 18493, 65676, 99660, 100008, 105648, 101915, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 99943, 101915, 107738, 106641, 788, 330, 17, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 101915, 102065, 102305, 111521, 9370, 106641, 788, 330, 17, 19, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 99943, 18493, 100358, 100008, 105648, 101915, 106641, 788, 330, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 100030, 106107, 101915, 106641, 788, 330, 23, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18830, 101915, 65577, 9754, 6142, 29941, 788, 330, 21, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 20, 35727, 18493, 65676, 99660, 100008, 12, 92894, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 22, 35727, 18493, 65676, 99660, 100008, 107884, 101915, 100008, 8863, 788, 330, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 101915, 102431, 102305, 111521, 9370, 106641, 788, 330, 19, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 22, 35727, 18493, 65676, 99660, 100008, 105648, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 108801, 9754, 101915, 106641, 788, 330, 20, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 100082, 9754, 101915, 106641, 788, 330, 16, 20, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 100358, 100008, 12, 100169, 100358, 101915, 100008, 8863, 788, 330, 23, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 22, 35727, 18493, 100358, 100008, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 116953, 100008, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 99943, 101915, 102065, 100030, 111521, 9370, 100008, 8863, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 107884, 101915, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 20, 35727, 18493, 100358, 100008, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 20, 35727, 18493, 100358, 100008, 107884, 101915, 106641, 788, 330, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 22, 35727, 101915, 107738, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 21894, 100008, 7, 21894, 100008, 17714, 100358, 8, 9370, 101915, 106641, 788, 330, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 101915, 100008, 8863, 788, 330, 17, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 100358, 100008, 108801, 9754, 101915, 106641, 788, 330, 17, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 100082, 9754, 101915, 106641, 788, 330, 16, 20, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 100358, 100008, 12, 71356, 101337, 100358, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 100358, 100008, 18830, 101915, 65577, 9754, 6142, 29941, 788, 330, 21, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 100358, 100008, 101915, 100082, 111619, 35727, 8863, 788, 330, 23, 24, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 12, 100030, 21515, 111521, 101915, 100008, 8863, 788, 330, 19, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 101915, 100082, 111619, 35727, 8863, 788, 330, 16, 20, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 21894, 100008, 7, 21894, 100008, 17714, 65676, 99660, 8, 101915, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 101915, 100082, 111619, 35727, 8863, 788, 330, 16, 20, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 108801, 9754, 101915, 106641, 788, 330, 20, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 22, 35727, 18493, 65676, 99660, 100008, 12, 68878, 99327, 100030, 106107, 101915, 100008, 8863, 788, 330, 17, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 100358, 100008, 101915, 100082, 111619, 35727, 8863, 788, 330, 17, 21, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 108801, 9754, 101915, 106641, 788, 330, 20, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 100358, 100008, 101915, 108801, 111619, 35727, 8863, 788, 330, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 18493, 100358, 100008, 12, 100169, 100358, 101915, 100008, 8863, 788, 330, 24, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 100358, 100008, 101915, 106641, 788, 330, 17, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 12, 100030, 21515, 111521, 101915, 100008, 8863, 788, 330, 18, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 108801, 9754, 101915, 106641, 788, 330, 23, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 99943, 101915, 102065, 118906, 102305, 100055, 9370, 106641, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 12, 92894, 101915, 100008, 8863, 788, 330, 24, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 20, 35727, 18493, 65676, 99660, 100008, 107884, 101915, 106641, 788, 330, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 101915, 106641, 788, 330, 16, 18, 23, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 100030, 106107, 101915, 100008, 8863, 788, 330, 18, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 101915, 106641, 788, 330, 18, 17, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 101915, 102065, 100030, 111521, 9370, 106641, 788, 330, 21, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 99943, 18493, 100358, 100008, 107884, 101915, 106641, 788, 330, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 12, 102305, 21515, 111521, 101915, 100008, 8863, 788, 330, 17, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 99943, 18493, 100358, 100008, 101915, 106641, 788, 330, 17, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 99943, 101915, 107738, 106641, 788, 330, 17, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 101915, 106523, 9909, 21515, 106523, 7552, 9370, 106641, 788, 330, 16, 20, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 101915, 107738, 100008, 8863, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 100358, 100008, 101915, 100082, 111619, 35727, 8863, 788, 330, 17, 21, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 100358, 100008, 12, 100169, 100358, 101915, 106641, 788, 330, 16, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 12, 102305, 21515, 111521, 101915, 100008, 8863, 788, 330, 18, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 101915, 106523, 9909, 21515, 106523, 7552, 9370, 106641, 788, 330, 22, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 101200, 105676, 101915, 106641, 7, 18830, 101915, 104081, 101200, 84636, 330, 16, 15, 13, 21, 22, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 105648, 101915, 106641, 788, 330, 16, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 22, 35727, 18493, 100358, 100008, 105648, 101915, 106641, 788, 330, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 22, 35727, 18493, 21894, 100008, 7, 21894, 100008, 17714, 65676, 99660, 8, 101915, 106641, 788, 330, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 100358, 100008, 100082, 9754, 101915, 106641, 788, 330, 21, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 99943, 18493, 65676, 99660, 100008, 101915, 106641, 788, 330, 24, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 99943, 18493, 65676, 99660, 100008, 107884, 101915, 106641, 788, 330, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 105648, 101915, 100008, 8863, 788, 330, 23, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 22, 35727, 18493, 100358, 100008, 105648, 101915, 100008, 8863, 788, 330, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 99946, 104044, 18493, 65676, 100358, 100008, 101915, 9370, 111619, 35727, 8863, 788, 330, 18, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 22, 35727, 101915, 102065, 102305, 111521, 9370, 100008, 8863, 788, 330, 18, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 100030, 106107, 101915, 100008, 8863, 788, 330, 21, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 101915, 102065, 100030, 111521, 9370, 100008, 8863, 788, 330, 20, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 20, 35727, 101915, 107738, 100008, 8863, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 20, 35727, 18493, 100358, 100008, 105648, 101915, 106641, 788, 330, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 100358, 100008, 105648, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 18493, 100358, 100008, 101915, 106641, 788, 330, 18, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 101915, 107738, 100008, 8863, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 101915, 102431, 102305, 111521, 9370, 106641, 788, 330, 18, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 101915, 102431, 100030, 111521, 9370, 100008, 8863, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 18830, 101915, 65577, 9754, 6142, 29941, 788, 330, 21, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 99943, 18493, 65676, 99660, 100008, 12, 92894, 101915, 106641, 788, 330, 20, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 18493, 100358, 100008, 107884, 101915, 106641, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 99943, 18493, 100358, 100008, 101915, 106641, 788, 330, 17, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 99943, 18493, 65676, 99660, 100008, 105648, 101915, 106641, 788, 330, 18, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 100082, 9754, 101915, 106641, 788, 330, 17, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 101915, 108801, 111619, 35727, 8863, 788, 330, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 101915, 108801, 111619, 35727, 8863, 788, 330, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 100358, 100008, 101200, 105676, 101915, 106641, 7, 18830, 101915, 104081, 101200, 84636, 330, 18, 13, 16, 22, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 101200, 105676, 101915, 106641, 7, 18830, 101915, 104081, 101200, 84636, 330, 16, 16, 13, 23, 18, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 100358, 100008, 108801, 9754, 101915, 106641, 788, 330, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 71356, 30709, 100055, 100008, 101915, 100008, 8863, 788, 330, 20, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 12, 92894, 101915, 100008, 8863, 788, 330, 24, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 105648, 101915, 100008, 8863, 788, 330, 16, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 18493, 100358, 100008, 12, 71356, 101337, 100358, 101915, 106641, 788, 330, 20, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 100082, 9754, 101915, 106641, 788, 330, 18, 19, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 18493, 21894, 100008, 7, 21894, 100008, 17714, 65676, 99660, 8, 101915, 106641, 788, 330, 17, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18830, 101915, 65577, 9754, 6142, 29941, 788, 330, 18, 497, 330, 59879, 75355, 51154, 3837, 59258, 22, 35727, 18493, 65676, 99660, 100008, 107884, 101915, 106641, 788, 330, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 101915, 102431, 100030, 111521, 9370, 100008, 8863, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 20, 35727, 18493, 65676, 99660, 100008, 12, 68878, 99327, 100030, 106107, 101915, 100008, 8863, 788, 330, 17, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 99943, 101915, 102065, 102305, 111521, 9370, 100008, 8863, 788, 330, 20, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 101915, 106641, 788, 330, 20, 17, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 116953, 100008, 101915, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 12, 92894, 101915, 106641, 788, 330, 16, 22, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 100358, 100008, 105648, 101915, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 101915, 108801, 111619, 35727, 8863, 788, 330, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 101915, 102431, 100030, 111521, 9370, 106641, 788, 330, 19, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 100358, 100008, 107884, 101915, 106641, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 101915, 102431, 102305, 111521, 9370, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 30709, 100055, 100008, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 101200, 105676, 101915, 106641, 7, 18830, 101915, 104081, 101200, 84636, 330, 16, 19, 13, 15, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 107884, 101915, 100008, 8863, 788, 330, 18, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 20, 35727, 18493, 100358, 100008, 107884, 101915, 106641, 788, 330, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 12, 102305, 21515, 111521, 100008, 101915, 106641, 788, 330, 17, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 18493, 21894, 100008, 7, 21894, 100008, 17714, 100358, 8, 9370, 101915, 106641, 788, 330, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 101915, 106523, 9909, 21515, 106523, 7552, 9370, 106641, 788, 330, 17, 20, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 100358, 100008, 12, 71356, 101337, 100358, 101915, 100008, 8863, 788, 330, 17, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 100358, 100008, 107884, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 22, 35727, 18493, 65676, 99660, 100008, 12, 92894, 101915, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 99943, 101915, 102065, 118906, 102305, 100055, 9370, 100008, 8863, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 21894, 100008, 7, 21894, 100008, 17714, 100358, 8, 9370, 101915, 106641, 788, 330, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 22, 35727, 18493, 100358, 100008, 107884, 101915, 100008, 8863, 788, 330, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 21894, 100008, 7, 21894, 100008, 17714, 100358, 8, 9370, 101915, 106641, 788, 330, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 107884, 101915, 106641, 788, 330, 16, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 116953, 100008, 101915, 106641, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 101915, 106523, 9909, 21515, 106523, 7552, 9370, 100008, 8863, 788, 330, 22, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 20, 35727, 101915, 107738, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 101915, 107738, 100008, 8863, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 105648, 101915, 106641, 788, 330, 18, 19, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 20, 35727, 18493, 100358, 100008, 105648, 101915, 100008, 8863, 788, 330, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 21894, 100008, 7, 21894, 100008, 17714, 100358, 8, 9370, 101915, 106641, 788, 330, 15, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 107884, 101915, 100008, 8863, 788, 330, 18, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 30709, 100055, 100008, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 18493, 100358, 100008, 12, 71356, 101337, 100358, 101915, 100008, 8863, 788, 330, 17, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 101915, 102065, 100030, 111521, 9370, 100008, 8863, 788, 330, 18, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 30709, 100055, 100008, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 71356, 30709, 100055, 100008, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 20, 35727, 18493, 100358, 100008, 12, 100169, 100358, 101915, 106641, 788, 330, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 12, 100030, 21515, 111521, 100008, 101915, 106641, 788, 330, 18, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 100358, 100008, 108801, 9754, 101915, 106641, 788, 330, 17, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 100030, 106107, 101915, 100008, 8863, 788, 330, 18, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 12, 92894, 101915, 100008, 8863, 788, 330, 16, 18, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 18830, 101915, 65577, 9754, 6142, 29941, 788, 330, 18, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 101915, 102431, 100030, 111521, 9370, 100008, 8863, 788, 330, 18, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 99943, 18493, 65676, 99660, 100008, 12, 92894, 101915, 100008, 8863, 788, 330, 18, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 105648, 101915, 100008, 8863, 788, 330, 16, 16, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 101915, 102431, 100030, 111521, 9370, 106641, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 99943, 18493, 100358, 100008, 12, 100169, 100358, 101915, 106641, 788, 330, 17, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 100082, 9754, 101915, 106641, 788, 330, 16, 20, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 100358, 100008, 12, 100169, 100358, 101915, 100008, 8863, 788, 330, 22, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 99943, 101915, 102065, 102305, 111521, 9370, 106641, 788, 330, 22, 497, 330, 59879, 105765, 17992, 51154, 3837, 99946, 102888, 18493, 100358, 100008, 101915, 9370, 111619, 35727, 8863, 788, 330, 18, 20, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 101915, 102065, 100030, 111521, 9370, 106641, 788, 330, 18, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 105648, 101915, 106641, 788, 330, 18, 19, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 100358, 100008, 105648, 101915, 106641, 788, 330, 20, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 100358, 100008, 107884, 101915, 106641, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 20, 35727, 101915, 107738, 106641, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 100358, 100008, 107884, 101915, 100008, 8863, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 100358, 100008, 105648, 101915, 100008, 8863, 788, 330, 19, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 12, 92894, 101915, 106641, 788, 330, 16, 22, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18830, 101915, 65577, 9754, 6142, 29941, 788, 330, 21, 497, 330, 59879, 75355, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 105648, 101915, 106641, 788, 330, 16, 23, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 18, 99943, 18493, 65676, 99660, 100008, 108801, 9754, 101915, 106641, 788, 330, 23, 497, 330, 59879, 75355, 51154, 3837, 104044, 55286, 18493, 65676, 99660, 100008, 104005, 101915, 9370, 106641, 788, 330, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 12, 100030, 21515, 111521, 101915, 100008, 8863, 788, 330, 19, 497, 330, 59879, 75355, 51154, 3837, 59258, 18, 99943, 101915, 106523, 9909, 21515, 106523, 7552, 9370, 106641, 788, 330, 22, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 65676, 99660, 100008, 12, 68878, 99327, 100030, 106107, 101915, 106641, 788, 330, 16, 22, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 17, 99943, 18493, 100358, 100008, 101915, 100008, 8863, 788, 330, 16, 16, 497, 330, 59879, 75355, 51154, 3837, 59258, 16, 17, 99943, 18493, 65676, 99660, 100008, 107884, 101915, 106641, 788, 330, 16, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 21, 99943, 18493, 100358, 100008, 101915, 108801, 111619, 35727, 8863, 788, 330, 15, 497, 330, 59879, 105765, 17992, 51154, 3837, 59258, 16, 20, 35727, 18493, 65676, 99660], new_block_ids=[[1, 2, 3, 4, 5, 1283, 3809, 451, 1087, 3542, 4052, 3438, 3023, 4636, 3356, 433, 2909, 4569, 4548, 4357, 3932, 4538, 4509, 4342, 1906, 4337, 1298, 2527, 189, 3905, 3039, 212, 1207, 2764, 4508, 1920, 1449, 2128, 4597, 1613, 3072, 1342, 3362, 858, 1047, 3062, 1785, 2272, 2049, 387, 2898, 3691, 253, 4991, 1122, 4682, 3079, 251, 3660, 2035, 2066, 2872, 3126, 4483, 1838, 4043, 257, 3919, 2735, 4144, 431, 3105, 3841, 1507, 2655, 4505, 766, 41, 2671, 3522, 190, 3224, 2326, 778, 2752, 3577, 58, 100, 805, 988, 950, 2795, 2206, 82, 2025, 1912, 1573, 880, 3977, 2865, 1673, 4866, 2878, 4766, 186, 155, 1996, 2927, 1393, 2854, 789, 3325, 4039, 4665, 495, 895, 755, 3174, 4833, 3330, 3206, 4329, 527, 3394, 2189, 2703, 4607, 3338, 695, 2760, 624, 4109, 1039, 985, 2091, 4900, 1089, 964, 4612, 1871, 4807, 2349, 1380, 4029, 4263, 3106, 1534, 883, 4491, 1069, 1356, 3114, 3315, 3627, 3997, 929, 1486, 1282, 2551, 2245, 999, 4740, 4036, 3514, 3624, 1526, 635, 4479, 963, 856, 4540, 3159, 2986, 406, 4747, 1826, 4367, 2605, 677, 486, 2474, 3833, 4531, 160, 2464, 3271, 683, 2163, 4958, 2002, 1824, 4844, 3904, 1967, 912, 3813, 4058, 4184, 3843, 1448, 2488, 1695, 3257, 876, 3193, 1701, 4319, 2094, 4680, 248, 1804, 4663, 3220, 1269, 2828, 2197, 1169, 2690, 3769, 91, 154, 3053, 3536, 923, 2142, 4895, 4107, 1333, 3118, 1583, 2546, 3341, 4045, 664, 2663, 60, 4346, 2604, 3065, 1227, 2670, 1704, 1627, 311, 4544, 1214, 2950, 549, 2710, 3903, 2233, 2534, 1493, 2930, 1311, 4602, 2400, 2200, 4566, 4285, 2862, 3082, 947, 1947, 2683, 3112, 2619, 2668, 704, 1769, 3531, 3968, 3802, 2393, 193, 4116, 3793, 4355, 4727, 300, 2407, 1222, 3443, 2733, 1085, 2538, 1519, 5003, 1293, 3437, 1479, 3237, 3529, 863, 4381, 1396, 2636, 903, 2789, 2051, 3711, 4085, 1809, 1806, 2439, 3882, 4799, 3300, 2829, 4873, 4989, 1483, 3674, 2202, 2979, 3808, 4817, 2571, 1715, 1844, 3244, 2173, 1120, 464, 2884, 3746, 1765, 195, 4139, 4453, 647, 1359, 1764, 3318, 1326, 3452, 2063, 2844, 1038, 2520, 3190, 2784, 3881, 4842, 2116, 2436, 4129, 4258, 4499, 2153, 1154, 1475, 3681, 3386, 509, 126, 4888, 1843, 3374, 2511, 4635, 1177, 785, 1505, 3292, 2440, 2507, 4302, 2055, 4780, 3488, 4703, 2766, 2585, 4785, 2768, 4240, 2755, 2964, 3583, 3097, 2398, 3659, 3507, 3226, 3435, 2478, 1936, 3069, 3552, 2877, 2291, 62, 4557, 2069, 4276, 965, 3859, 105, 2143, 1892, 1654, 1357, 13, 4928, 2519, 3770, 4354, 4511, 1494, 3249, 4622, 3527, 2377, 4944, 79, 1186, 3038, 1244, 2541, 4809, 870, 1775, 2918, 598, 4314, 490, 3081, 1471, 4306, 3125, 2434, 1012, 1751, 2005, 185, 3596, 3646, 4000, 631, 854, 4073, 2509, 970, 3205, 303, 697, 571, 1874, 1231, 52, 2375, 4393, 3155, 3964, 4365, 1470, 3350, 1420, 3740, 3076, 905, 2649, 4688, 1547, 691, 1299, 4016, 2984, 2423, 3886, 4564, 679, 520, 3161, 3458, 3462, 1213, 346, 2262, 4617, 4706, 2110, 2991, 4320, 104, 2052, 3664, 1146, 217, 4972, 3838, 2559, 196, 2203, 3957, 2967, 4603, 2905, 2441, 4715, 709, 921, 1092, 1104, 2934, 4166, 656, 2888, 4495, 1365, 2837, 2759, 2154, 906, 1386, 2188, 44, 2693, 3546, 2179, 4015, 4956, 2247, 1314, 2466, 3343, 584, 4233, 4669, 2304, 4701, 1340, 1426, 819, 1354, 2953, 2079, 1270, 1204, 3498, 726, 2906, 4071, 4542, 1639, 469, 536, 534, 4506, 2510, 4226, 4563, 3967, 2268, 891, 3340, 75, 845, 4407, 3826, 2711, 4925, 3319, 2167, 4119, 491, 4878, 969, 1858, 580, 660, 4521, 4887, 3796, 3752, 1979, 2544, 4096, 2748, 4021, 4877, 2315, 2183, 1482, 1339, 367, 2853, 2389, 533, 2857, 34, 4189, 1833, 3907, 1473, 4034, 1103, 4822, 2874, 307, 2136, 1614, 3613, 710, 391, 2122, 4840, 614, 242, 386, 1203, 1951, 3487, 931, 3510, 4704, 3707, 995, 171, 2697, 3476, 1635, 688, 4589, 2421, 2999, 4751, 3128, 1957, 1555, 1153, 116, 2718, 1854, 739, 3876, 120, 3167, 1973, 3574, 1698, 3851, 2032, 4596, 555, 4547, 2042, 589, 2900, 1899, 4994, 1916, 685, 1316, 3102, 2101, 472, 1515, 2852, 1381, 1705, 14, 2224, 551, 1952, 2667, 226, 4412, 1647, 2971, 2676, 1341, 2734, 3895, 3780, 2011, 3972, 2322, 3888, 1148, 1962, 2652, 2923, 4281, 852, 831, 4022, 4629, 1251, 3540, 806, 2064, 3415, 1406, 3679, 1369, 1897, 4528, 4871, 1382, 3138, 3342, 2397, 3865, 4883, 570, 3850, 2015, 3442, 4104, 2282, 3115, 2702, 4930, 338, 1955, 2498, 403, 814, 1107]], num_computed_tokens=5456), CachedRequestData(req_id='chatcmpl-c7a3b673016342caab8d139dd3ac3a45', resumed_from_preemption=false, new_token_ids=[21], new_block_ids=[[]], num_computed_tokens=3400), CachedRequestData(req_id='chatcmpl-f1b660302caa47b0bcd766b588fc8e08', resumed_from_preemption=false, new_token_ids=[102188], new_block_ids=[[]], num_computed_tokens=3928), CachedRequestData(req_id='chatcmpl-92e2e47c5f6f42138f812decee2b1f12', resumed_from_preemption=false, new_token_ids=[99601], new_block_ids=[[]], num_computed_tokens=4307), CachedRequestData(req_id='chatcmpl-71b26b114e8b43cca02d5a9746ffc1e2', resumed_from_preemption=false, new_token_ids=[101915], new_block_ids=[[]], num_computed_tokens=462), CachedRequestData(req_id='chatcmpl-70a560a73472441d933471d3690702e3', resumed_from_preemption=false, new_token_ids=[101353], new_block_ids=[[]], num_computed_tokens=1544), CachedRequestData(req_id='chatcmpl-488165af50754e45b2f9ce4f256b57ca', resumed_from_preemption=false, new_token_ids=[104352], new_block_ids=[[]], num_computed_tokens=2254), CachedRequestData(req_id='chatcmpl-18408eaaa1db439e8bcc8b96b919c5f0', resumed_from_preemption=false, new_token_ids=[101915], new_block_ids=[[4216]], num_computed_tokens=11536), CachedRequestData(req_id='chatcmpl-c8b392cc5abd4778b147daaedab020b9', resumed_from_preemption=false, new_token_ids=[101042], new_block_ids=[[]], num_computed_tokens=4275), CachedRequestData(req_id='chatcmpl-279a0ea6a69a4cb7bd683830f9e187d5', resumed_from_preemption=false, new_token_ids=[42140], new_block_ids=[[]], num_computed_tokens=418), CachedRequestData(req_id='chatcmpl-ca8ecf6456f04f45be31651084a84f65', resumed_from_preemption=false, new_token_ids=[788], new_block_ids=[[]], num_computed_tokens=3256), CachedRequestData(req_id='chatcmpl-a3a38aaac0e64f1aa79fd846a8470931', resumed_from_preemption=false, new_token_ids=[110678], new_block_ids=[[]], num_computed_tokens=3854), CachedRequestData(req_id='chatcmpl-3dfb05ebb3de4f60abed0f5fa523a296', resumed_from_preemption=false, new_token_ids=[9370], new_block_ids=[[]], num_computed_tokens=2102), CachedRequestData(req_id='chatcmpl-e5386fc2e1e74359b1e2b17110152faf', resumed_from_preemption=false, new_token_ids=[106689], new_block_ids=[[]], num_computed_tokens=13138), CachedRequestData(req_id='chatcmpl-05377851bba1427fb8b281f598a1576c', resumed_from_preemption=false, new_token_ids=[59258], new_block_ids=[[]], num_computed_tokens=1385), CachedRequestData(req_id='chatcmpl-9122c51553ec43f9b50bed4d2dbbf43b', resumed_from_preemption=false, new_token_ids=[99936], new_block_ids=[[]], num_computed_tokens=4153), CachedRequestData(req_id='chatcmpl-ae45f24648df43c8800ab67800ed130e', resumed_from_preemption=false, new_token_ids=[20002], new_block_ids=[[]], num_computed_tokens=1897), CachedRequestData(req_id='chatcmpl-61f9a707eae04b13a0a3c90c6a444d89', resumed_from_preemption=false, new_token_ids=[17], new_block_ids=[[]], num_computed_tokens=2142), CachedRequestData(req_id='chatcmpl-ec7eec1062264da5a29f9b1d0fe8415a', resumed_from_preemption=false, new_token_ids=[26381], new_block_ids=[[]], num_computed_tokens=3236), CachedRequestData(req_id='chatcmpl-b97f5146d3184c078a4cefdb0b8cba62', resumed_from_preemption=false, new_token_ids=[104050], new_block_ids=[[]], num_computed_tokens=1359)], num_scheduled_tokens={chatcmpl-488165af50754e45b2f9ce4f256b57ca: 1, chatcmpl-ae45f24648df43c8800ab67800ed130e: 1, chatcmpl-ec7eec1062264da5a29f9b1d0fe8415a: 1, chatcmpl-18408eaaa1db439e8bcc8b96b919c5f0: 1, chatcmpl-b97f5146d3184c078a4cefdb0b8cba62: 1, chatcmpl-ca8ecf6456f04f45be31651084a84f65: 1, chatcmpl-3dfb05ebb3de4f60abed0f5fa523a296: 1, chatcmpl-9122c51553ec43f9b50bed4d2dbbf43b: 1, chatcmpl-f1b660302caa47b0bcd766b588fc8e08: 1, chatcmpl-70a560a73472441d933471d3690702e3: 1, chatcmpl-e5386fc2e1e74359b1e2b17110152faf: 1, chatcmpl-05377851bba1427fb8b281f598a1576c: 1, chatcmpl-92e2e47c5f6f42138f812decee2b1f12: 1, chatcmpl-61f9a707eae04b13a0a3c90c6a444d89: 1, chatcmpl-35245641e2d2465aa6c7abdfc6cbd8a2: 6077, chatcmpl-71b26b114e8b43cca02d5a9746ffc1e2: 1, chatcmpl-c7a3b673016342caab8d139dd3ac3a45: 1, chatcmpl-c8b392cc5abd4778b147daaedab020b9: 1, chatcmpl-a3a38aaac0e64f1aa79fd846a8470931: 1, chatcmpl-279a0ea6a69a4cb7bd683830f9e187d5: 1}, total_num_scheduled_tokens=6096, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[5], finished_req_ids=['chatcmpl-96551814a8994fba8935ff2d4daff665'], free_encoder_input_ids=[], structured_output_request_ids={}, grammar_bitmask=null, kv_connector_metadata=null)\nERROR 06-05 16:27:39 [dump_input.py:81] SchedulerStats(num_running_reqs=20, num_waiting_reqs=193, gpu_cache_usage=0.9776936865166301, prefix_cache_stats=PrefixCacheStats(reset=False, requests=1, queries=12001, hits=5456), spec_decoding_stats=None)\nERROR 06-05 16:27:39 [core.py:502] EngineCore encountered a fatal error.\nERROR 06-05 16:27:39 [core.py:502] Traceback (most recent call last):\nERROR 06-05 16:27:39 [core.py:502]   File \"/home/risk-dev/miniconda3/envs/ds/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 215, in collective_rpc\nERROR 06-05 16:27:39 [core.py:502]     result = get_response(w, dequeue_timeout)\nERROR 06-05 16:27:39 [core.py:502]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-05 16:27:39 [core.py:502]   File \"/home/risk-dev/miniconda3/envs/ds/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 198, in get_response\nERROR 06-05 16:27:39 [core.py:502]     status, result = w.worker_response_mq.dequeue(\nERROR 06-05 16:27:39 [core.py:502]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-05 16:27:39 [core.py:502]   File \"/home/risk-dev/miniconda3/envs/ds/lib/python3.12/site-packages/vllm/distributed/device_communicators/shm_broadcast.py\", line 463, in dequeue\nERROR 06-05 16:27:39 [core.py:502]     with self.acquire_read(timeout, cancel) as buf:\nERROR 06-05 16:27:39 [core.py:502]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-05 16:27:39 [core.py:502]   File \"/home/risk-dev/miniconda3/envs/ds/lib/python3.12/contextlib.py\", line 137, in __enter__\nERROR 06-05 16:27:39 [core.py:502]     return next(self.gen)\nERROR 06-05 16:27:39 [core.py:502]            ^^^^^^^^^^^^^^\nERROR 06-05 16:27:39 [core.py:502]   File \"/home/risk-dev/miniconda3/envs/ds/lib/python3.12/site-packages/vllm/distributed/device_communicators/shm_broadcast.py\", line 427, in acquire_read\nERROR 06-05 16:27:39 [core.py:502]     raise TimeoutError\nERROR 06-05 16:27:39 [core.py:502] TimeoutError\nERROR 06-05 16:27:39 [core.py:502] \nERROR 06-05 16:27:39 [core.py:502] The above exception was the direct cause of the following exception:\nERROR 06-05 16:27:39 [core.py:502] \nERROR 06-05 16:27:39 [core.py:502] Traceback (most recent call last):\nERROR 06-05 16:27:39 [core.py:502]   File \"/home/risk-dev/miniconda3/envs/ds/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 493, in run_engine_core\nERROR 06-05 16:27:39 [core.py:502]     engine_core.run_busy_loop()\nERROR 06-05 16:27:39 [core.py:502]   File \"/home/risk-dev/miniconda3/envs/ds/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 520, in run_busy_loop\nERROR 06-05 16:27:39 [core.py:502]     self._process_engine_step()\nERROR 06-05 16:27:39 [core.py:502]   File \"/home/risk-dev/miniconda3/envs/ds/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 545, in _process_engine_step\nERROR 06-05 16:27:39 [core.py:502]     outputs = self.step_fn()\nERROR 06-05 16:27:39 [core.py:502]               ^^^^^^^^^^^^^^\nERROR 06-05 16:27:39 [core.py:502]   File \"/home/risk-dev/miniconda3/envs/ds/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 226, in step\nERROR 06-05 16:27:39 [core.py:502]     model_output = self.execute_model(scheduler_output)\nERROR 06-05 16:27:39 [core.py:502]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-05 16:27:39 [core.py:502]   File \"/home/risk-dev/miniconda3/envs/ds/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 213, in execute_model\nERROR 06-05 16:27:39 [core.py:502]     raise err\nERROR 06-05 16:27:39 [core.py:502]   File \"/home/risk-dev/miniconda3/envs/ds/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 207, in execute_model\nERROR 06-05 16:27:39 [core.py:502]     return self.model_executor.execute_model(scheduler_output)\nERROR 06-05 16:27:39 [core.py:502]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-05 16:27:39 [core.py:502]   File \"/home/risk-dev/miniconda3/envs/ds/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 158, in execute_model\nERROR 06-05 16:27:39 [core.py:502]     (output, ) = self.collective_rpc(\"execute_model\",\nERROR 06-05 16:27:39 [core.py:502]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-05 16:27:39 [core.py:502]   File \"/home/risk-dev/miniconda3/envs/ds/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 221, in collective_rpc\nERROR 06-05 16:27:39 [core.py:502]     raise TimeoutError(f\"RPC call to {method} timed out.\") from e\nERROR 06-05 16:27:39 [core.py:502] TimeoutError: RPC call to execute_model timed out.\nERROR 06-05 16:27:39 [async_llm.py:408] AsyncLLM output_handler failed.\nERROR 06-05 16:27:39 [async_llm.py:408] Traceback (most recent call last):\nERROR 06-05 16:27:39 [async_llm.py:408]   File \"/home/risk-dev/miniconda3/envs/ds/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py\", line 366, in output_handler\nERROR 06-05 16:27:39 [async_llm.py:408]     outputs = await engine_core.get_output_async()\nERROR 06-05 16:27:39 [async_llm.py:408]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-05 16:27:39 [async_llm.py:408]   File \"/home/risk-dev/miniconda3/envs/ds/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 806, in get_output_async\nERROR 06-05 16:27:39 [async_llm.py:408]     raise self._format_exception(outputs) from None\nERROR 06-05 16:27:39 [async_llm.py:408] vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-06-05T09:19:47+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19197/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19197"
  },
  {
    "number": 17337,
    "title": "[Bug]: sm75 can not serve qwen3 bnb 4bit model",
    "body": "### Your current environment\n\n<details>\n<summary>docker image v0.8.5</summary>\n\n\nvllm-openai-1  | (VllmWorkerProcess pid=149) WARNING 04-28 18:00:58 [utils.py:168] The model class Qwen3MoeForCausalLM has not defined `packed_modules_mapping`, this may lead to incorrect mapping of quantized or ignored modules\nvllm-openai-1  | WARNING 04-28 18:00:58 [utils.py:168] The model class Qwen3MoeForCausalLM has not defined `packed_modules_mapping`, this may lead to incorrect mapping of quantized or ignored modules\nvllm-openai-1  | (VllmWorkerProcess pid=149) ERROR 04-28 18:00:58 [multiproc_worker_utils.py:238] Exception in worker VllmWorkerProcess while processing method load_model.\nvllm-openai-1  | (VllmWorkerProcess pid=149) ERROR 04-28 18:00:58 [multiproc_worker_utils.py:238] Traceback (most recent call last):\nvllm-openai-1  | (VllmWorkerProcess pid=149) ERROR 04-28 18:00:58 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 232, in _run_worker_process\nvllm-openai-1  | (VllmWorkerProcess pid=149) ERROR 04-28 18:00:58 [multiproc_worker_utils.py:238]     output = run_method(worker, method, args, kwargs)\nvllm-openai-1  | (VllmWorkerProcess pid=149) ERROR 04-28 18:00:58 [multiproc_worker_utils.py:238]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvllm-openai-1  | (VllmWorkerProcess pid=149) ERROR 04-28 18:00:58 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2456, in run_method\nvllm-openai-1  | (VllmWorkerProcess pid=149) ERROR 04-28 18:00:58 [multiproc_worker_utils.py:238]     return func(*args, **kwargs)\nvllm-openai-1  | (VllmWorkerProcess pid=149) ERROR 04-28 18:00:58 [multiproc_worker_utils.py:238]            ^^^^^^^^^^^^^^^^^^^^^\nvllm-openai-1  | (VllmWorkerProcess pid=149) ERROR 04-28 18:00:58 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 203, in load_model\nvllm-openai-1  | (VllmWorkerProcess pid=149) ERROR 04-28 18:00:58 [multiproc_worker_utils.py:238]     self.model_runner.load_model()\nvllm-openai-1  | (VllmWorkerProcess pid=149) ERROR 04-28 18:00:58 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1111, in load_model\nvllm-openai-1  | (VllmWorkerProcess pid=149) ERROR 04-28 18:00:58 [multiproc_worker_utils.py:238]     self.model = get_model(vllm_config=self.vllm_config)\nvllm-openai-1  | (VllmWorkerProcess pid=149) ERROR 04-28 18:00:58 [multiproc_worker_utils.py:238]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvllm-openai-1  | (VllmWorkerProcess pid=149) ERROR 04-28 18:00:58 [multiproc_worker_utils.py:238]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\nvllm-openai-1  | (VllmWorkerProcess pid=149) ERROR 04-28 18:00:58 [multiproc_worker_utils.py:238]     return loader.load_model(vllm_config=vllm_config)\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n  vllm-openai:\n      runtime: nvidia\n      restart: always\n      deploy:\n        resources:\n          reservations:\n            devices:\n              - driver: nvidia\n                device_ids: [ '2', '3']\n                capabilities: [ gpu ]\n      volumes:\n        - ~/.cache/huggingface:/root/.cache/huggingface\n        - /home/hucd/models:/models\n      environment:\n          - HUGGING_FACE_HUB_TOKEN=<secret>\n          - CUDA_VISIBLE_DEVICES=0,1\n      ports:\n          - 8001:8000\n      ipc: host\n      image: vllm/vllm-openai:v0.8.5\n      command: --model /models/Qwen3-30B-A3B-bnb-4bit --served-model-name qwen3-a3b --tensor_parallel_size 2 --max_model_len 8192 --dtype half --max_num_seqs 1 --gpu_memory_utilization 0.9 --enable-reasoning --reasoning-parser deepseek_r1\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-04-29T01:03:34+00:00",
    "closed_at": null,
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17337/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17337"
  },
  {
    "number": 14937,
    "title": "[Feature]:Slim Attention (lossless 2x reduction in KV cache size)",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\n**Feature Description**\nSlim attention can reduce KV cache size by a factor of 2 without a loss of accuracy as it's lossless: https://arxiv.org/pdf/2503.05840\n\n**Motivation**\nAllows you to run with larger context sizes at the same (V)RAM usage or allows you to cram the same context into less (V)RAM. Furthermore, it improves performance at long context sizes.\n\n**Possible Implementation**\nNo response\n",
    "labels": [
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-03-17T08:20:19+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14937/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14937"
  },
  {
    "number": 2314,
    "title": "vLLM on OpenShift/Kubernetes Manifests",
    "body": "Does anyone have a sample manifest on how to deploy vLLM on OpenShift or Kubernetes?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-01T11:50:39+00:00",
    "closed_at": "2024-09-20T20:09:30+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2314/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2314"
  },
  {
    "number": 7315,
    "title": "[Feature]: Support attention backend with FlexAttention",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\r\n\r\nFlexAttention was proposed as a performant attention implementation leveraging `torch.compile` with easy APIs for adding support for complex attention variants such as Causal, [Relative Positional Embeddings](https://paperswithcode.com/method/relative-position-encodings), [Alibi](https://paperswithcode.com/method/alibi), [Sliding Window Attention](https://mistral.ai/news/announcing-mistral-7b/), [PrefixLM](https://twitter.com/andersonbcdefg/status/1800907703688339569), [Document Masking/Sample Packing/Jagged Tensors](https://github.com/pytorch/torchtune/pull/875), [Tanh Soft-Capping](https://twitter.com/LysandreJik/status/1807779471891538199), [PagedAttention](https://arxiv.org/abs/2309.06180), etc.\r\n\r\nhttps://pytorch.org/blog/flexattention/\r\n\r\nWhile it is not the fastest attention backend (yet!) it is clearly performant enough while enabling much more flexibility than current compiled backends to easily implement attention features we need for crucial models, like Soft-capping in Gemma 2 which we currently rely on FlashInfer for. Not to mention it is a first-class citizen for `torch.compile`.\r\n\r\n**The current blocker is it will not be available until PyTorch 2.5.0.**\r\n\r\n![image](https://github.com/user-attachments/assets/4e508c4c-1b80-4b97-b8b6-89f3db6b1639)\r\n\r\n<img src=\"https://github.com/user-attachments/assets/f1558be9-e0dc-4beb-899b-4eddc29a8b03\" width=\"400\" />\r\n\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\n_No response_",
    "labels": [
      "feature request",
      "torch.compile",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-08T19:52:26+00:00",
    "closed_at": "2025-02-14T01:59:26+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7315/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/7315"
  },
  {
    "number": 4838,
    "title": "[Feature]: Build and publish Neuron docker image",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nIt seems like the current docker images don't support Neuron (Inferentia).\r\nIt would be very helpful if there was a tested, managed Neuron docker image to use.\r\nWhile at the same subject, it would be even better if some documentation would be added on running vLlm Neuron using containers.\n\n### Alternatives\n\nDJL?\n\n### Additional context\n\n_No response_",
    "labels": [
      "feature request",
      "keep-open"
    ],
    "state": "open",
    "created_at": "2024-05-15T15:27:17+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4838/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4838"
  },
  {
    "number": 2032,
    "title": "Refactor Mixtral to reuse code from MegaBlocks",
    "body": "Hello! A [portion](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/mixtral.py#L223-L326) of the MoE implementation for Mixtral is copied directly from MegaBlocks. It's somewhat error prone code and I've been meaning to factor out helpers for it, which we could reuse to avoid having this duplicated in vLLM. If this is interesting to you I'll send a PR :)",
    "labels": [],
    "state": "closed",
    "created_at": "2023-12-11T19:04:24+00:00",
    "closed_at": "2024-03-25T11:32:24+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2032/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 3,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2032"
  },
  {
    "number": 2072,
    "title": "How to install from source with CUDA 11.8 instead of 12.1?",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-12-13T01:49:45+00:00",
    "closed_at": "2024-03-28T12:02:21+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2072/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2072"
  },
  {
    "number": 4186,
    "title": "[Bug]: lora base_model.model.lm_head.base_layer.weight is not supported ",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.2.2+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Debian GNU/Linux 12 (bookworm) (x86_64)\r\nGCC version: (Debian 12.2.0-14) 12.2.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.36\r\n\r\nPython version: 3.11.9 (main, Apr 10 2024, 14:54:51) [GCC 12.2.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-15-generic-x86_64-with-glibc2.36\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 1: NVIDIA H100 80GB HBM3\r\nGPU 2: NVIDIA H100 80GB HBM3\r\nGPU 3: NVIDIA H100 80GB HBM3\r\n\r\nNvidia driver version: 550.54.14\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             96\r\nOn-line CPU(s) list:                0-95\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Gold 6442Y\r\nCPU family:                         6\r\nModel:                              143\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 24\r\nSocket(s):                          2\r\nStepping:                           8\r\nBogoMIPS:                           5200.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hfi vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          2.3 MiB (48 instances)\r\nL1i cache:                          1.5 MiB (48 instances)\r\nL2 cache:                           96 MiB (48 instances)\r\nL3 cache:                           120 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94\r\nNUMA node1 CPU(s):                  1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] onnxruntime==1.17.3\r\n[pip3] torch==2.2.2\r\n[pip3] torchvision==0.17.2\r\n[pip3] triton==2.2.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV6\tNV6\tNV6\t0,2,4,6,8,10\t0\t\tN/A\r\nGPU1\tNV6\t X \tNV6\tNV6\t0,2,4,6,8,10\t0\t\tN/A\r\nGPU2\tNV6\tNV6\t X \tNV6\t1,3,5,7,9,11\t1\t\tN/A\r\nGPU3\tNV6\tNV6\tNV6\t X \t1,3,5,7,9,11\t1\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nI fine tuned the Mixtral 8x7b  model with lora ,  I included the layer **lm_head** in lora    so in my tensors has this \"base_model.model.lm_head.base_layer.weight , in  code vllm/lora/utils.py\" : it doesn't support or ignore it .\r\n\r\n      def parse_fine_tuned_lora_name(name: str) -> Tuple[str, bool]:\r\n          \"\"\"Parse the name of lora weights.\r\n      \r\n          args:\r\n              name: the name of the fine-tuned LoRA, e.g.\r\n                  base_model.model.dense1.weight\r\n          return:\r\n              Tuple(module_name, is_lora_a):\r\n                  module_name: the name of the module, e.g. model.dense1,\r\n                  is_lora_a whether the tensor is lora_a or lora_b.\r\n          \"\"\"\r\n          parts = name.split(\".\")\r\n          assert parts[0] == \"base_model\"\r\n          assert parts[1] == \"model\"\r\n          if parts[-1] == \"weight\":\r\n              assert parts[-2] == \"lora_A\" or parts[-2] == \"lora_B\"\r\n              return \".\".join(parts[2:-2]), parts[-2] == \"lora_A\"\r\n          \r\n          if parts[-1] == \"lora_embedding_A\" or parts[-1] == \"lora_embedding_B\":\r\n              return \".\".join(parts[2:-1]), parts[-1] == \"lora_embedding_A\"\r\n          \r\n          raise ValueError(f\"{name} is unsupported format\")\r\n    \r\n    \r\n\r\n    \r\nI am getting this error:\r\n    Traceback (most recent call last):\r\n  File \"/workspace/vllm/lora/worker_manager.py\", line 139, in _load_lora\r\n    lora = self._lora_model_cls.from_local_checkpoint(\r\n  File \"/workspace/vllm/lora/models.py\", line 227, in from_local_checkpoint\r\n    return cls.from_lora_tensors(\r\n  File \"/workspace/vllm/lora/models.py\", line 148, in from_lora_tensors\r\n    module_name, is_lora_a = parse_fine_tuned_lora_name(tensor_name)\r\n  File \"/workspace/vllm/lora/utils.py\", line 33, in parse_fine_tuned_lora_name\r\n    assert parts[-2] == \"lora_A\" or parts[-2] == \"lora_B\"\r\nAssertionError\r\n",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-04-19T02:36:50+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4186/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4186"
  },
  {
    "number": 13765,
    "title": "[Bug]: \"Loading safetensors checkpoint shards\" runs twice when serving model",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Amazon Linux 2023.6.20250115 (x86_64)\nGCC version: (GCC) 11.4.1 20230605 (Red Hat 11.4.1-2)\nClang version: Could not collect\nCMake version: version 3.22.2\nLibc version: glibc-2.34\n\nPython version: 3.12.8 | packaged by conda-forge | (main, Dec  5 2024, 14:24:40) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-6.1.119-129.201.amzn2023.x86_64-x86_64-with-glibc2.34\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA L4\nNvidia driver version: 560.35.03\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               4\nOn-line CPU(s) list:                  0-3\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 7R13 Processor\nCPU family:                           25\nModel:                                1\nThread(s) per core:                   2\nCore(s) per socket:                   2\nSocket(s):                            1\nStepping:                             1\nBogoMIPS:                             5300.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            64 KiB (2 instances)\nL1i cache:                            64 KiB (2 instances)\nL2 cache:                             1 MiB (2 instances)\nL3 cache:                             8 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-3\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; safe RET, no microcode\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.49.0\n[pip3] triton==3.1.0\n[conda] No relevant packages\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.3\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-3     0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=/home/ec2-user/.local/share/uv/tools/vllm/lib/python3.12/site-packages/cv2/../../lib64:/opt/amazon/efa/lib64:/opt/amazon/openmpi/lib64:/opt/aws-ofi-nccl/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/lib\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen I'm serving this model `unsloth/Mistral-Small-24B-Instruct-2501-unsloth-bnb-4bit` it seems like the safetensors checkpoint shards is being loaded twice. I tried with `google/gemma-2-2b-it` and that did not have the problem. Is this a bug, or an artifact of the model/quantization?\n\n```bash\nVLLM_LOGGING_LEVEL=DEBUG vllm serve unsloth/Mistral-Small-24B-Instruct-2501-unsloth-bnb-4bit --max-model-len=15000 --served-model-name mistralai/Mistral-Small-24B-Instruct-2501 --port 8880 --quantization bitsandbytes --load-format bitsandbytes\n```\n\n```\nDEBUG 02-24 13:59:23 main.py:48] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'\nDEBUG 02-24 13:59:23 __init__.py:28] No plugins for group vllm.platform_plugins found.\nINFO 02-24 13:59:23 __init__.py:207] Automatically detected platform cuda.\nDEBUG 02-24 13:59:23 __init__.py:28] No plugins for group vllm.general_plugins found.\nINFO 02-24 13:59:23 api_server.py:912] vLLM API server version 0.7.3\nINFO 02-24 13:59:23 api_server.py:913] args: Namespace(subparser='serve', model_tag='unsloth/Mistral-Small-24B-Instruct-2501-unsloth-bnb-4bit', config='', host=None, port=8880, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='unsloth/Mistral-Small-24B-Instruct-2501-unsloth-bnb-4bit', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='bitsandbytes', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=15000, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization='bitsandbytes', rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['mistralai/Mistral-Small-24B-Instruct-2501'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function ServeSubcommand.cmd at 0x7fb2407b6a20>)\nDEBUG 02-24 13:59:23 api_server.py:190] Multiprocessing frontend to use ipc:///tmp/b440e140-4f43-44e8-a8d1-a2b76055bb5b for IPC Path.\nINFO 02-24 13:59:23 api_server.py:209] Started engine process with PID 9418\nDEBUG 02-24 13:59:27 __init__.py:28] No plugins for group vllm.platform_plugins found.\nINFO 02-24 13:59:27 __init__.py:207] Automatically detected platform cuda.\nDEBUG 02-24 13:59:27 __init__.py:28] No plugins for group vllm.general_plugins found.\nINFO 02-24 13:59:29 config.py:549] This model supports multiple tasks: {'embed', 'score', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\nWARNING 02-24 13:59:30 config.py:628] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nINFO 02-24 13:59:33 config.py:549] This model supports multiple tasks: {'classify', 'embed', 'score', 'generate', 'reward'}. Defaulting to 'generate'.\nWARNING 02-24 13:59:33 config.py:628] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nINFO 02-24 13:59:34 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/Mistral-Small-24B-Instruct-2501-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/Mistral-Small-24B-Instruct-2501-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=15000, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mistral-Small-24B-Instruct-2501, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True,\nINFO 02-24 13:59:34 cuda.py:229] Using Flash Attention backend.\nDEBUG 02-24 13:59:34 config.py:3461] enabled custom ops: Counter()\nDEBUG 02-24 13:59:34 config.py:3463] disabled custom ops: Counter()\nDEBUG 02-24 13:59:35 parallel_state.py:810] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://[masked]:44387 backend=nccl\nINFO 02-24 13:59:35 model_runner.py:1110] Starting to load model unsloth/Mistral-Small-24B-Instruct-2501-unsloth-bnb-4bit...\nDEBUG 02-24 13:59:35 decorators.py:109] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']\nDEBUG 02-24 13:59:35 config.py:3461] enabled custom ops: Counter({'rms_norm': 81, 'silu_and_mul': 40, 'rotary_embedding': 1})\nDEBUG 02-24 13:59:35 config.py:3463] disabled custom ops: Counter()\nINFO 02-24 13:59:35 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\nINFO 02-24 13:59:36 weight_utils.py:254] Using model weights format ['*.safetensors']\nLoading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.34it/s]\nDEBUG 02-24 13:59:41 client.py:192] Waiting for output from MQLLMEngine.\nDEBUG 02-24 13:59:51 client.py:192] Waiting for output from MQLLMEngine.\nLoading safetensors checkpoint shards:  67% Completed | 2/3 [00:18<00:10, 10.70s/it]\nDEBUG 02-24 14:00:01 client.py:192] Waiting for output from MQLLMEngine.\nDEBUG 02-24 14:00:11 client.py:192] Waiting for output from MQLLMEngine.\nDEBUG 02-24 14:00:21 client.py:192] Waiting for output from MQLLMEngine.\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:48<00:00, 19.75s/it]\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:48<00:00, 16.31s/it]\n\nLoading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\nDEBUG 02-24 14:00:28 utils.py:154] Loaded weight lm_head.weight with shape torch.Size([131072, 5120])\nDEBUG 02-24 14:00:31 client.py:192] Waiting for output from MQLLMEngine.\nDEBUG 02-24 14:00:41 client.py:192] Waiting for output from MQLLMEngine.\nLoading safetensors checkpoint shards:  33% Completed | 1/3 [00:22<00:44, 22.01s/it]\nDEBUG 02-24 14:00:51 client.py:192] Waiting for output from MQLLMEngine.\nDEBUG 02-24 14:01:01 client.py:192] Waiting for output from MQLLMEngine.\nDEBUG 02-24 14:01:11 client.py:192] Waiting for output from MQLLMEngine.\nDEBUG 02-24 14:01:21 client.py:192] Waiting for output from MQLLMEngine.\nLoading safetensors checkpoint shards:  67% Completed | 2/3 [00:59<00:31, 31.29s/it]\nDEBUG 02-24 14:01:31 client.py:192] Waiting for output from MQLLMEngine.\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [01:06<00:00, 20.09s/it]\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [01:06<00:00, 22.19s/it]\n\nINFO 02-24 14:01:32 model_runner.py:1115] Loading model weights took 13.9231 GB\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-24T14:03:29+00:00",
    "closed_at": "2025-06-24T06:19:51+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13765/reactions",
      "total_count": 4,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 4
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13765"
  },
  {
    "number": 11178,
    "title": "[Bug]: ARM vLLM container build failing due to outlines v0.1.9",
    "body": "### Your current environment\n\nM2 Mac with Docker Desktop\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nBuilding ARM vLLM Docker image with `docker build -f Dockerfile.arm -t vllm-cpu-env --shm-size=4g .` leads to the following error:\r\n```\r\n24.95   copying python/outlines_core/fsm/regex.py -> build/lib.linux-aarch64-cpython-310/outlines_core/fsm\r\n24.95   running build_ext\r\n24.95   running build_rust\r\n24.95   error: can't find Rust compiler\r\n24.95 \r\n24.95   If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\r\n24.95 \r\n24.95   To update pip, run:\r\n24.95 \r\n24.95       pip install --upgrade pip\r\n24.95 \r\n24.95   and then retry package installation.\r\n24.95 \r\n24.95   If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\r\n24.97   error: subprocess-exited-with-error\r\n24.97   \r\n24.97   \u00d7 Building wheel for outlines_core (pyproject.toml) did not run successfully.\r\n24.97   \u2502 exit code: 1\r\n24.97   \u2570\u2500> See above for output.\r\n24.97   \r\n24.97   note: This error originates from a subprocess, and is likely not a problem with pip.\r\n24.97   full command: /usr/bin/python3 /usr/local/lib/python3.10/dist-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py build_wheel /tmp/tmp9inm24ey\r\n24.97   cwd: /tmp/pip-install-k98qbmgt/outlines-core_52043b4e88924bfeb2524aaea4a95439\r\n24.97   Building wheel for outlines_core (pyproject.toml): finished with status 'error'\r\n24.97   ERROR: Failed building wheel for outlines_core\r\n24.97 Failed to build outlines_core\r\n25.00 ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (outlines_core)\r\n------\r\nDockerfile.arm:37\r\n--------------------\r\n  36 |     \r\n  37 | >>> RUN --mount=type=cache,target=/root/.cache/pip \\\r\n  38 | >>>     --mount=type=bind,src=requirements-common.txt,target=requirements-common.txt \\\r\n  39 | >>>     --mount=type=bind,src=requirements-cpu.txt,target=requirements-cpu.txt \\\r\n  40 | >>>     pip install -v -r requirements-cpu.txt\r\n  41 |     \r\n```\r\n\r\nThis was because of missing arm64 support for `outlines-core` (https://github.com/dottxt-ai/outlines-core/issues/122) which was causing install for `outlines` v0.1.9 to break. This was recently fixed in both `outlines-core` and `outlines` (https://github.com/dottxt-ai/outlines/pull/1336).\r\n\r\nAccordingly, the `outlines` dependency in requirements-common.txt should be bumped to v0.1.11.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-12-13T19:33:28+00:00",
    "closed_at": "2024-12-14T07:46:20+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11178/reactions",
      "total_count": 2,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11178"
  },
  {
    "number": 6563,
    "title": "[New Model]: Mistral-Nemo",
    "body": "### The model to consider.\r\n\r\nhttps://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407\r\n\r\n### The closest model vllm already supports.\r\n\r\n- https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/llama.py\r\n- https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/mixtral.py\r\n\r\n### What's your difficulty of supporting the model you want?\r\n\r\n_No response_",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2024-07-19T06:51:46+00:00",
    "closed_at": "2024-07-19T13:06:56+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6563/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 2,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6563"
  },
  {
    "number": 1206,
    "title": "GPU KV cache usage: 100.0%\u4ee5\u540e\u5c31\u5361\u4f4f",
    "body": "GPU KV cache usage: 100.0%\u4ee5\u540e\u5c31\u5361\u4f4f\uff0cGPU\u4f7f\u7528\u7387\u4e5f\u5c06\u4e3a0\uff0c\u65e0\u6cd5\u7ee7\u7eed\u63d0\u4f9b\u670d\u52a1\uff0c\u8bf7\u95ee\u6709\u4ec0\u4e48\u89e3\u51b3\u529e\u6cd5\u5417\uff1f",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-09-28T01:13:13+00:00",
    "closed_at": "2024-12-01T02:16:12+00:00",
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1206/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1206"
  },
  {
    "number": 15681,
    "title": "[RFC]: should we use `VLLM_WORKER_MULTIPROC_METHOD=spawn` by default for openai-compatible api server ?",
    "body": "### Motivation.\n\nI\u2019ve recently encountered this error:\n\n```\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n```\n\nAnd I always have to specify `VLLM_WORKER_MULTIPROC_METHOD=spawn` to bypass the issue, so I investigated the cause.\n\nvllm forces `VLLM_WORKER_MULTIPROC_METHOD=spawn` when `torch.cuda.is_initialized()` is True, but that\u2019s not enough.\n\nIn PyTorch, when the main process runs `poison_fork` and then forks a subprocess, the subprocess's `in_bad_fork` gets set to true. Therefore, when the subprocess attempts to reinitialize CUDA, it throws an error.\n\nhttps://github.com/pytorch/pytorch/blob/v2.6.0/torch/csrc/cuda/Module.cpp#L63-L79\n\nHowever, `poison_fork` doesn't only run during `torch._C._cuda_init`. It also runs during `torch._C._cuda_getDeviceCount` and `torch._C._cuda_getArchFlags`.\n\nSo if we use the fork method, we can't call methods like `torch.cuda.is_available()` or `torch.cuda.device_count()` in the main process. Even though we can ensure these methods don\u2019t create subprocesses before execution within the vllm library, we can\u2019t guarantee that all dependencies won\u2019t (for example, `is_flash_attn_2_available` in `transformers` calls `torch.cuda.is_available()`).\n\n\n\n### Proposed Change.\n\nIn the future, maybe we can created a pr to PyTorch to export a variable indicated whether `poison_fork` is called.\n\nCurrently, should we use `VLLM_WORKER_MULTIPROC_METHOD=spawn` by default ?\n\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n_No response_\n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "closed",
    "created_at": "2025-03-28T06:52:19+00:00",
    "closed_at": "2025-03-28T09:53:59+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15681/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15681"
  },
  {
    "number": 3658,
    "title": "[New Model]: Supporting DBRX from Databricks",
    "body": "### The model to consider.\n\nDatabricks has released [DBRX](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm), which consists of 2 models\r\n\r\n- [dbrx](https://huggingface.co/databricks/dbrx-base)\r\n- [dbrx-instruct](https://huggingface.co/databricks/dbrx-instruct)\r\n\r\nIt's a 132B parameter MoE model. Might be useful.\n\n### The closest model vllm already supports.\n\n_No response_\n\n### What's your difficulty of supporting the model you want?\n\nIt seems that they have a custom script in their files, might need custom implementation on that regard.",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2024-03-27T13:00:35+00:00",
    "closed_at": "2024-03-27T20:01:47+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3658/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3658"
  },
  {
    "number": 20341,
    "title": "[Bug]: No output / Repeated outputs when using Gemma 3  on vLLM",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI'm running the google/gemma-3-27b-it model with vLLM using the OpenAI-compatible API server.\n\n\nCUDA_VISIBLE_DEVICES=0 VLLM_USE_V1=1 python /opt/VLLM/vllm/vllm/entrypoints/openai/api_server.py \\\n--model /opt/MODELS/gemma-3-27b-it/ \\\n--max-model-len 32000 \\\n--host 10.12.112.168 \\\n--port 9005 \\\n--tensor-parallel-size 1 \\\n--gpu_memory_utilization 0.9\n\n\nThen, I send a standard request to the /v1/chat/completions endpoint using Python:\n\n\nimport requests\nimport json\n\nurl = \"http://10.12.112.168:9005/v1/chat/completions\"\n\ndata = {\n    \"model\": \"/opt/MODELS/gemma-3-27b-it/\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"hello\"}\n    ],\n    \"temperature\": 0.1,\n    \"max_tokens\": 500,\n    \"enable_thinking\": False\n}\n\nheaders = {\n    \"Content-Type\": \"application/json\"\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nresult = response.json()\nprint(result['choices'][0]['message']['content'])\n\n\nThe request is processed, but the model fails to produce meaningful responses. It either:\n\noutputs nothing,\n\nor keeps repeating certain tokens or parts of the input (e.g., repeating \u201cselamlar brom\u201d).\n\nThis issue only happens with Gemma 3 IT models. I tested the exact same code and server setup with:\n\nQwen models \n\nMistral models\n...and they work perfectly. No repetition, and responses are coherent and aligned with the prompt.\n\nSo this looks like a Gemma-specific compatibility issue with /chat/completions, possibly due to missing or misaligned prompt formatting (e.g., lack of a compatible chat template?).\n\nLet me know if there\u2019s a known workaround or proper configuration required for Gemma models.\n\nThanks!\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-07-01T22:35:38+00:00",
    "closed_at": null,
    "comments": 16,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20341/reactions",
      "total_count": 3,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20341"
  },
  {
    "number": 2019,
    "title": "why online seving slower than offline serving??",
    "body": "1. offline serving\r\n![image](https://github.com/vllm-project/vllm/assets/43260218/87e216b5-9064-4c2a-a021-cac08e22795d)\r\n\r\n2. online serving(fastapi)\r\n![image](https://github.com/vllm-project/vllm/assets/43260218/322cc4a4-a78f-4212-a266-d586e8e2969d)\r\n![image](https://github.com/vllm-project/vllm/assets/43260218/49c9cf76-ca3f-4362-95d8-191cbbdd3543)\r\nlog: INFO 12-11 21:50:36 llm_engine.py:649] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%\r\nINFO 12-11 21:50:41 async_llm_engine.py:111] Finished request 261ddff3312f44cd8ee1c52a6acd10e6.\r\n\r\n\r\nWhy is the speed 2 seconds slower when displayed as fastapi??\r\nparameters is same, prompt is same\r\n\r\n\"Open-Orca/Mistral-7B-OpenOrca\" this model same issue\r\nand any llama2 model same issue\r\n\r\npython : 3.10.12\r\n[my library list.txt](https://github.com/vllm-project/vllm/files/13641002/my.library.list.txt)\r\n\r\ncuda_version : 12.0\r\ngpu: a100 40g\r\nmy library list attached ",
    "labels": [],
    "state": "closed",
    "created_at": "2023-12-11T12:50:58+00:00",
    "closed_at": "2024-10-26T16:44:55+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2019/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 2,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2019"
  },
  {
    "number": 6653,
    "title": "[Doc]: Outdated docs on AutoAWQ",
    "body": "### \ud83d\udcda The doc issue\n\nThe AutoAWQ content is outdated. Particularly, the warning is now obsolete due to the optimization in #6612 which makes AWQ run much faster in vLLM.\n\n### Suggest a potential alternative/fix\n\nThe warning should be updated to reflect newer versions of vLLM.",
    "labels": [
      "documentation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-22T17:47:58+00:00",
    "closed_at": "2024-11-24T02:07:33+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6653/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/6653"
  },
  {
    "number": 8904,
    "title": "[Bug]: Tokenization Mismatch Between HuggingFace and vLLM",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.22.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1061-nvidia-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        52 bits physical, 57 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               224\r\nOn-line CPU(s) list:                  0-223\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Xeon(R) Platinum 8480C\r\nCPU family:                           6\r\nModel:                                143\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   56\r\nSocket(s):                            2\r\nStepping:                             8\r\nCPU max MHz:                          3800.0000\r\nCPU min MHz:                          800.0000\r\nBogoMIPS:                             4000.00\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                       VT-x\r\nL1d cache:                            5.3 MiB (112 instances)\r\nL1i cache:                            3.5 MiB (112 instances)\r\nL2 cache:                             224 MiB (112 instances)\r\nL3 cache:                             210 MiB (2 instances)\r\nNUMA node(s):                         2\r\nNUMA node0 CPU(s):                    0-55,112-167\r\nNUMA node1 CPU(s):                    56-111,168-223\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.3.101\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==25.1.2\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.2\r\n[pip3] triton==3.0.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.3.101                 pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\r\n[conda] pyzmq                     25.1.2                   pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.44.2                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.1.post2@9ba0817ff1eb514f51cc6de9cb8e16c98d6ee44f\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\n140 NICs found in the topology, only displaying 63 in the matrix.\r\n        GPU0    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10   NIC11   NIC12   NIC13   NIC14   NIC15   NIC16   NIC17   NIC18   NIC19   NIC20   NIC21   NIC22   NIC23   NIC24   NIC25   NIC26NIC27    NIC28   NIC29   NIC30   NIC31   NIC32   NIC33   NIC34   NIC35   NIC36   NIC37   NIC38   NIC39   NIC40   NIC41   NIC42   NIC43   NIC44   NIC45   NIC46   NIC47   NIC48   NIC49   NIC50   NIC51   NIC52   NIC53   NIC54   NIC55NIC56    NIC57   NIC58   NIC59   NIC60   NIC61   NIC62   CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     56-111,168-223  1               N/A\r\nNIC0    SYS      X      NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC1    SYS     NODE     X      PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC2    SYS     NODE    PIX      X      NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC3    SYS     NODE    NODE    NODE     X      NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC4    SYS     NODE    NODE    NODE    NODE     X      NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC5    SYS     NODE    NODE    NODE    NODE    NODE     X      NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC6    SYS     PIX     NODE    NODE    NODE    NODE    NODE     X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC7    SYS     PIX     NODE    NODE    NODE    NODE    NODE    PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC8    SYS     PIX     NODE    NODE    NODE    NODE    NODE    PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC9    SYS     PIX     NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC10   SYS     PIX     NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC11   SYS     PIX     NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC12   SYS     PIX     NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC13   SYS     PIX     NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC14   SYS     PIX     NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC15   SYS     PIX     NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC16   SYS     PIX     NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC17   SYS     PIX     NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC18   SYS     PIX     NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC19   SYS     PIX     NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC20   SYS     PIX     NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC21   SYS     PIX     NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC22   SYS     NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE     X      PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC23   SYS     NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX      X      PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC24   SYS     NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX      X      PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC25   SYS     NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX      X      PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC26   SYS     NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX      X   PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC27   SYS     NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX   X       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC28   SYS     NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX  PIX       X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC29   SYS     NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX  PIX      PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC30   SYS     NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC31   SYS     NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC32   SYS     NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC33   SYS     NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC34   SYS     NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC35   SYS     NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC36   SYS     NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC37   SYS     NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC38   SYS     NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE     X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC39   SYS     NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC40   SYS     NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC41   SYS     NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC42   SYS     NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC43   SYS     NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC44   SYS     NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC45   SYS     NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC46   SYS     NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC47   SYS     NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC48   SYS     NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC49   SYS     NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC50   SYS     NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC51   SYS     NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC52   SYS     NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC53   SYS     NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC54   SYS     NODE    NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE     X      PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC55   SYS     NODE    NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX      X   PIX      PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC56   SYS     NODE    NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX   X       PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC57   SYS     NODE    NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX  PIX       X      PIX     PIX     PIX     PIX     PIX\r\nNIC58   SYS     NODE    NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX  PIX      PIX      X      PIX     PIX     PIX     PIX\r\nNIC59   SYS     NODE    NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX  PIX      PIX     PIX      X      PIX     PIX     PIX\r\nNIC60   SYS     NODE    NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX  PIX      PIX     PIX     PIX      X      PIX     PIX\r\nNIC61   SYS     NODE    NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX  PIX      PIX     PIX     PIX     PIX      X      PIX\r\nNIC62   SYS     NODE    NODE    NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: IX      PIX     PIX     PIX     PIX     PIX     PIX     PIX      X \r\n  NIC1: NIC56   NIC57   NIC58   NIC59   NIC60   NIC61   NIC62   CPU Affinity    NUMA Affinity   GPU NUMA ID\r\n  NIC2: UMA Affinity    GPU NUMA ID\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_12\r\n  NIC7: mlx5_13\r\n  NIC8: mlx5_14\r\n  NIC9: mlx5_15\r\n  NIC10: mlx5_16\r\n  NIC11: mlx5_17\r\n  NIC12: mlx5_18\r\n  NIC13: mlx5_19\r\n  NIC14: mlx5_20\r\n  NIC15: mlx5_21\r\n  NIC16: mlx5_22\r\n  NIC17: mlx5_23\r\n  NIC18: mlx5_24\r\n  NIC19: mlx5_25\r\n  NIC20: mlx5_26\r\n  NIC21: mlx5_27\r\n  NIC22: mlx5_28\r\n  NIC23: mlx5_29\r\n  NIC24: mlx5_30\r\n  NIC25: mlx5_31\r\n  NIC26: mlx5_32\r\n  NIC27: mlx5_33\r\n  NIC28: mlx5_34\r\n  NIC29: mlx5_35\r\n  NIC30: mlx5_36\r\n  NIC31: mlx5_37\r\n  NIC32: mlx5_38\r\n  NIC33: mlx5_39\r\n  NIC34: mlx5_40\r\n  NIC35: mlx5_41\r\n  NIC36: mlx5_42\r\n  NIC37: mlx5_43\r\n  NIC38: mlx5_44\r\n  NIC39: mlx5_45\r\n  NIC40: mlx5_46\r\n  NIC41: mlx5_47\r\n  NIC42: mlx5_48\r\n  NIC43: mlx5_49\r\n  NIC44: mlx5_50\r\n  NIC45: mlx5_51\r\n  NIC46: mlx5_52\r\n  NIC47: mlx5_53\r\n  NIC48: mlx5_54\r\n  NIC49: mlx5_55\r\n  NIC50: mlx5_56\r\n  NIC51: mlx5_57\r\n  NIC52: mlx5_58\r\n  NIC53: mlx5_59\r\n  NIC54: mlx5_60\r\n  NIC55: mlx5_61\r\n  NIC56: mlx5_62\r\n  NIC57: mlx5_63\r\n  NIC58: mlx5_64\r\n  NIC59: mlx5_65\r\n  NIC60: mlx5_66\r\n  NIC61: mlx5_67\r\n  NIC62: mlx5_68\r\n```\r\n\r\n</details>\r\n\r\n### \ud83d\udc1b Describe the bug\r\nTokenization mismatch between HuggingFace's AutoTokenizer and vLLM's tokenization for the `meta-llama/Llama-3.1-8B-Instruct` model (same issue using Mistral). This issue currently leads to incorrect token-level operations or unexpected behaviour in applications relying on precise tokenization.\r\n\r\n### Code to Reproduce\r\n> The following example sets up the vllm inference server as well in order to closely reproduce our issue. \r\n\r\n```python\r\nimport subprocess\r\nimport socket\r\nimport requests\r\nimport warnings\r\nfrom transformers import AutoTokenizer\r\nfrom urllib3.exceptions import InsecureRequestWarning\r\nimport time\r\n\r\ndef is_port_in_use(port):\r\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\r\n        return s.connect_ex(('localhost', port)) == 0\r\n\r\ndef start_server(model, port):\r\n    cmd = f\"python -m vllm.entrypoints.openai.api_server --model {model} --tensor-parallel-size 1 --port {port} --disable-frontend-multiprocessing --dtype bfloat16 --download-dir /mnt/models/ --gpu-memory-utilization 0.9  --enable-prefix-caching   --max-model-len 8000\"\r\n    process = subprocess.Popen(cmd, shell=True)\r\n    while not is_port_in_use(port):\r\n        if process.poll() is not None:\r\n            raise RuntimeError(\"Server process terminated unexpectedly\")\r\n        time.sleep(1)\r\n    return process\r\n\r\ndef check_server_health(port, max_retries=5):\r\n    for _ in range(max_retries):\r\n        try:\r\n            response = requests.get(f\"http://localhost:{port}/health\", verify=False)\r\n            if response.status_code == 200:\r\n                print(\"vLLM is running\")\r\n                return\r\n        except requests.ConnectionError:\r\n            time.sleep(1)\r\n    raise RuntimeError(\"Server health check failed.\")\r\n\r\ndef generate_and_compare_tokens(tokenizer, base_url, generation_args):\r\n    with warnings.catch_warnings():\r\n        warnings.simplefilter(\"ignore\", InsecureRequestWarning)\r\n        url = f\"{base_url}/v1/chat/completions\"\r\n        response = requests.post(url, json=generation_args, verify=False).json()\r\n\r\n    response_text = response[\"choices\"][0][\"message\"][\"content\"]\r\n    log_probs_content = response[\"choices\"][0][\"logprobs\"][\"content\"]\r\n\r\n    hf_tokens = [tokenizer.decode(t) for t in tokenizer.encode(response_text, add_special_tokens=False)]\r\n    vllm_tokens = [content[\"token\"] for content in log_probs_content]\r\n\r\n    if not all(t1 == t2 for t1, t2 in zip(hf_tokens, vllm_tokens)):\r\n        for pos, (hf_token, vllm_token) in enumerate(zip(hf_tokens, vllm_tokens)):\r\n            if hf_token != vllm_token:\r\n                print(f\"Mismatch at position {pos}: HF token: '{hf_token}' <--> vLLM token: '{vllm_token}'\")\r\n        raise ValueError(\"Log probs length does not match completion length\")\r\n\r\ndef main():\r\n    model = \"meta-llama/Llama-3.1-8B-Instruct\"\r\n    tokenizer = AutoTokenizer.from_pretrained(model)\r\n    port = 8080\r\n    base_url = f\"http://localhost:{port}\"\r\n\r\n    vllm_process = None\r\n    try:\r\n        if not is_port_in_use(port):\r\n            print(f\"Starting new server on port {port}\")\r\n            vllm_process = start_server(model, port)\r\n        else:\r\n            print(f\"Server already running on port: {port}\")\r\n\r\n        check_server_health(port)\r\n\r\n        CHAT_PROMPT = [\r\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n            {\"role\": \"user\", \"content\": \"Return a short python list of punctuation.\"},\r\n        ]\r\n\r\n        generation_args = {\r\n            \"model\": model,\r\n            \"temperature\": 1.0,\r\n            \"top_p\": 0.99,\r\n            \"top_k\": 45,\r\n            \"max_tokens\": 256,\r\n            \"logprobs\": 1,\r\n            \"include_stop_str_in_output\": True,\r\n            \"skip_special_tokens\": False,\r\n            \"n\": 1,\r\n            \"stream\": False,\r\n            \"messages\": CHAT_PROMPT,\r\n        }\r\n\r\n        while True:\r\n            generate_and_compare_tokens(tokenizer, base_url, generation_args)\r\n\r\n    except Exception as e:\r\n        raise Exception('Process already halted') from e\r\n\r\n    finally:\r\n        if vllm_process:\r\n            print(\"Terminating server process\")\r\n            vllm_process.terminate()\r\n            vllm_process.wait()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n### Output example\r\n```text\r\nMismatch at position 16: HF token: ' [' <--> vLLM token: ' [\"'\r\nMismatch at position 35: HF token: ',' <--> vLLM token: ' ,'\r\nMismatch at position 197: HF token: '?,' <--> vLLM token: ' ?,'\r\n```\r\n\r\n### Expected Behavior\r\nThe tokens generated by HuggingFace's AutoTokenizer and vLLM should match exactly.\r\n\r\n### Actual Behavior\r\nThere are mismatches between the tokens generated by HuggingFace's AutoTokenizer and vLLM. The script prints out the mismatched tokens and their positions.\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-27T12:43:11+00:00",
    "closed_at": "2025-01-26T01:59:45+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8904/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8904"
  },
  {
    "number": 7918,
    "title": "[Bug]: vllm:num_requests_waiting is not being published at /metrics endpoint",
    "body": "### \ud83d\udc1b Describe the bug\r\n\r\nData for vllm:num_requests_waiting is missing.\r\n\r\nvllm:num_requests_waiting is not being published at /metrics endpoint\r\n\r\ndocker image for vllm : vllm-openai:v0.5.3.post1\r\n\r\n```\r\n# HELP vllm:num_requests_waiting Number of requests waiting to be processed.\r\n# TYPE vllm:num_requests_waiting gauge\r\nvllm:num_requests_waiting{model_name=\"/data/models/model-gemma2-a100/experiment-it1\"} 0.0\r\n\r\n```\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-27T16:10:57+00:00",
    "closed_at": "2024-12-28T01:59:07+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7918/reactions",
      "total_count": 2,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7918"
  },
  {
    "number": 3201,
    "title": "Does VLLM currently support QWEN LoRa model \uff1f",
    "body": "I  use the multi-LoRA for offline inference:\r\nsql_lora_path = \"/home/zyn/models/slot_lora_gd\"\r\n\r\nfrom vllm import LLM, SamplingParams\r\nfrom vllm.lora.request import LoRARequest\r\n\r\nllm = LLM(model=\"/home/models/dem_14b/base\",\r\n          enable_lora=True,\r\n          trust_remote_code=True)\r\n\r\nsampling_params = SamplingParams(temperature=0,\r\n                                 max_tokens=256,\r\n                                 stop=[\"[/assistant]\"])\r\n\r\nprompts = [\r\n    \"[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_74 (icao VARCHAR, airport VARCHAR)\\n\\n question: Name the ICAO for lilongwe international airport [/user] [assistant]\",\r\n    \"[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_11 (nationality VARCHAR, elector VARCHAR)\\n\\n question: When Anchero Pantaleone was the elector what is under nationality? [/user] [assistant]\",\r\n]\r\n\r\noutputs = llm.generate(prompts,\r\n                       sampling_params,\r\n                       lora_request=LoRARequest(\"sql_adapter\", 1,\r\n                                                sql_lora_path))\r\n\r\n\r\n    llm = LLM(model=\"/home/models/dem_14b/base\",\r\n  File \"/root/miniconda3/envs/qwen/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 109, in __init__\r\n    self.llm_engine = LLMEngine.from_engine_args(engine_args)\r\n  File \"/root/miniconda3/envs/qwen/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 391, in from_engine_args\r\n    engine = cls(*engine_configs,\r\n  File \"/root/miniconda3/envs/qwen/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 128, in __init__\r\n    self._init_workers()\r\n  File \"/root/miniconda3/envs/qwen/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 181, in _init_workers\r\n    self._run_workers(\"load_model\")\r\n  File \"/root/miniconda3/envs/qwen/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 1041, in _run_workers\r\n    driver_worker_output = getattr(self.driver_worker,\r\n  File \"/root/miniconda3/envs/qwen/lib/python3.10/site-packages/vllm/worker/worker.py\", line 100, in load_model\r\n    self.model_runner.load_model()\r\n  File \"/root/miniconda3/envs/qwen/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 88, in load_model\r\n    self.model = get_model(self.model_config,\r\n  File \"/root/miniconda3/envs/qwen/lib/python3.10/site-packages/vllm/model_executor/utils.py\", line 52, in get_model\r\n    return get_model_fn(model_config, device_config, **kwargs)\r\n  File \"/root/miniconda3/envs/qwen/lib/python3.10/site-packages/vllm/model_executor/model_loader.py\", line 73, in get_model\r\n    raise ValueError(\r\nValueError: Model QWenLMHeadModel does not support LoRA, but LoRA is enabled. Support for this model may be added in the future. If this is important to you, please open an issue on github.",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-05T12:22:41+00:00",
    "closed_at": "2024-11-30T02:02:14+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3201/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3201"
  },
  {
    "number": 2917,
    "title": "Support for  Smaug-72B-v0.1 on vLLM",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-19T03:25:19+00:00",
    "closed_at": "2024-04-08T19:42:37+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2917/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2917"
  },
  {
    "number": 1958,
    "title": "Repeated answer: When I use vllm with opt-13b, the generated text is not end until the max length, with the repeated answer",
    "body": "hi, I use vllm in greedy SamplingType, I meet  the repeated answer: \r\n`sampling_params = SamplingParams(temperature=0, top_p=1, max_tokens=2048)`\r\n\r\n`from vllm import LLM, SamplingParams\r\n\r\n# Sample prompts.\r\nprompts = [\r\n    \"Give three tips for staying healthy.\",\r\n]\r\n# Create a sampling params object.\r\nsampling_params = SamplingParams(temperature=0, top_p=1, max_tokens=2048)\r\n\r\n# Create an LLM.\r\nllm = LLM(model=\"/workspace/opt-13b/\", tensor_parallel_size=4)\r\noutputs = llm.generate(prompts, sampling_params)\r\n# Print the outputs.\r\nfor output in outputs:\r\n    prompt = output.prompt\r\n    generated_text = output.outputs[0].text\r\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\", len(output.out\r\nputs[0].token_ids))`\r\n\r\nThe answer is follow, the same text is repreated, also to other prompts.\r\n![\u622a\u5c4f2023-12-07 \u4e0b\u53483 59 51](https://github.com/vllm-project/vllm/assets/8213143/dd316200-1c6b-420e-abbf-2fd07dbd8283)\r\n\r\nSo is there any way to solve this problem?\r\nThanks.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-12-07T08:02:58+00:00",
    "closed_at": "2024-03-25T10:10:52+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1958/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1958"
  },
  {
    "number": 5594,
    "title": "[bug]vllm deployement is failing on nvidia because of numpy2.0 upgrade",
    "body": "### Your current environment\n\n# python3.9 -c \"import vllm; print(vllm.__version__)\"\r\n\r\nA module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.0.0 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled with NumPy 2.0.\r\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\r\n\r\nIf you are a user of the module, the easiest solution will be to\r\ndowngrade to 'numpy<2' or try to upgrade the affected module.\r\nWe expect that some modules will need time to support NumPy 2.\r\n\r\nTraceback (most recent call last):  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib64/python3.9/site-packages/vllm/__init__.py\", line 3, in <module>\r\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\r\n  File \"/usr/local/lib64/python3.9/site-packages/vllm/engine/arg_utils.py\", line 6, in <module>\r\n    from vllm.config import (CacheConfig, DeviceConfig, LoRAConfig, ModelConfig,\r\n  File \"/usr/local/lib64/python3.9/site-packages/vllm/config.py\", line 7, in <module>\r\n    import torch\r\n  File \"/usr/local/lib64/python3.9/site-packages/torch/__init__.py\", line 1382, in <module>\r\n    from .functional import *  # noqa: F403\r\n  File \"/usr/local/lib64/python3.9/site-packages/torch/functional.py\", line 7, in <module>\r\n    import torch.nn.functional as F\r\n  File \"/usr/local/lib64/python3.9/site-packages/torch/nn/__init__.py\", line 1, in <module>\r\n    from .modules import *  # noqa: F403\r\n  File \"/usr/local/lib64/python3.9/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\r\n    from .transformer import TransformerEncoder, TransformerDecoder, \\\r\n  File \"/usr/local/lib64/python3.9/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\r\n    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n/usr/local/lib64/python3.9/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internal                                        ly at ../torch/csrc/utils/tensor_numpy.cpp:84.)\r\n  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\nERROR 06-17 06:23:53 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please se                                        t the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.\r\nINFO 06-17 06:23:53 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory\r\nINFO 06-17 06:23:53 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.\r\n\n\n### How you are installing vllm\n\n```sh\r\npip install -vvv vllm\r\n```\r\n",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2024-06-17T07:19:41+00:00",
    "closed_at": "2024-06-17T10:10:28+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5594/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5594"
  },
  {
    "number": 571,
    "title": "os.environ['CUDA_VISIBLE_DEVICES'] = '1' does not work in jupyter",
    "body": "As the title says, it is invalid to specify the GPU through `CUDA_VISIBLE_DEVICES` in jupyter, and only 'GPU:0' will still be used; but it is effective when using `CUDA_VISIBLE_DEVICES=1 python *.py`",
    "labels": [],
    "state": "closed",
    "created_at": "2023-07-25T09:46:53+00:00",
    "closed_at": "2023-08-08T09:04:51+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/571/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/571"
  },
  {
    "number": 651,
    "title": "cache_kernel.cu does not compile using pip install -e . on source code",
    "body": "Neither in docker (with the suggested docker), nor on my own environment, I get to compilte the cache_kernel.cu.\r\n\r\nNCVV = 11.8, also using the PyTorch 2.0.1 CUDA 11.8 package.\r\n\r\nAt first, it didn't install at all because of the myproject.toml pointing towards a pytorch in pip that is not cuda enabled.\r\nAfter removing the toml file, I ran into these errors:\r\n\r\nvllm\\csrc\\cache_kernels.cu(41): error: expression must be a pointer to a complete object type\r\n\r\nvllm\\csrc\\cache_kernels.cu(42): error: expression must be a pointer to a complete object type\r\n\r\nvllm\\csrc\\cache_kernels.cu(96): error: expression must have a constant value\r\nvllm\\csrc\\cache_kernels.cu(96): note #2689-D: the value of variable \"num_layers\"\r\n    (86): here cannot be used as a constant\r\n\r\nvllm\\csrc\\cache_kernels.cu(97): error: expression must have a constant value\r\nvllm\\csrc\\cache_kernels.cu(97): note #2689-D: the value of variable \"num_layers\"\r\n    (86): here cannot be used as a constant\r\n",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2023-08-02T19:54:02+00:00",
    "closed_at": "2024-04-20T12:10:56+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/651/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/651"
  },
  {
    "number": 6629,
    "title": "[Bug]: OpenAI server unexpected shutdown",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.5\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-107-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\n\r\nNvidia driver version: 535.183.01\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] torcheval==0.0.7\r\n[pip3] transformers==4.41.2\r\n[pip3] triton==2.3.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] torcheval                 0.0.7                    pypi_0    pypi\r\n[conda] transformers              4.41.2                   pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.0\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PXB     PXB     PXB     SYS     SYS     SYS     SYS     0-11,24-35      0               N/A\r\nGPU1    PXB      X      PXB     PXB     SYS     SYS     SYS     SYS     0-11,24-35      0               N/A\r\nGPU2    PXB     PXB      X      PIX     SYS     SYS     SYS     SYS     0-11,24-35      0               N/A\r\nGPU3    PXB     PXB     PIX      X      SYS     SYS     SYS     SYS     0-11,24-35      0               N/A\r\nGPU4    SYS     SYS     SYS     SYS      X      PXB     PXB     PXB     12-23,36-47     1               N/A\r\nGPU5    SYS     SYS     SYS     SYS     PXB      X      PXB     PXB     12-23,36-47     1               N/A\r\nGPU6    SYS     SYS     SYS     SYS     PXB     PXB      X      PIX     12-23,36-47     1               N/A\r\nGPU7    SYS     SYS     SYS     SYS     PXB     PXB     PIX      X      12-23,36-47     1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n### \ud83d\udc1b Describe the bug\n\n![20240721\u670d\u52a1\u5668shutdown](https://github.com/user-attachments/assets/5ed04752-aaa8-4933-a639-c1d2734455cd)\r\nOn weekend, the running OpenAI server unexpected shutdown. Is there any possible reason and solution for this issue?",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-22T02:31:50+00:00",
    "closed_at": "2024-11-24T02:07:40+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6629/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 2,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6629"
  }
]
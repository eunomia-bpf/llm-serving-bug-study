[
  {
    "number": 17047,
    "title": "[Bug]: ValueError when using Multi-Instance GPU",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Red Hat Enterprise Linux 9.5 (Plow) (x86_64)\nGCC version: Could not collect\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.34\n\nPython version: 3.9.22 | packaged by conda-forge | (main, Apr 14 2025, 23:35:59)  [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-5.14.0-503.26.1.el9_5.x86_64-x86_64-with-glibc2.34\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA H200\n  MIG 1g.18gb     Device  0:\n\nNvidia driver version: 570.124.06\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               128\nOn-line CPU(s) list:                  0-127\nVendor ID:                            GenuineIntel\nModel name:                           INTEL(R) XEON(R) PLATINUM 8562Y+\nCPU family:                           6\nModel:                                207\nThread(s) per core:                   2\nCore(s) per socket:                   32\nSocket(s):                            2\nStepping:                             2\nCPU(s) scaling MHz:                   29%\nCPU max MHz:                          4100.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             5600.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req hfi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nL1d cache:                            3 MiB (64 instances)\nL1i cache:                            2 MiB (64 instances)\nL2 cache:                             128 MiB (64 instances)\nL3 cache:                             120 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126\nNUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==2.0.2\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.3\n[pip3] triton==3.2.0\n[conda] blas                      2.116                       mkl    conda-forge\n[conda] blas-devel                3.9.0            16_linux64_mkl    conda-forge\n[conda] cuda-cudart               12.1.105                      0    nvidia\n[conda] cuda-cupti                12.1.105                      0    nvidia\n[conda] cuda-libraries            12.1.0                        0    nvidia\n[conda] cuda-nvrtc                12.1.105                      0    nvidia\n[conda] cuda-nvtx                 12.1.105                      0    nvidia\n[conda] cuda-opencl               12.8.90                       0    nvidia\n[conda] cuda-runtime              12.1.0                        0    nvidia\n[conda] cuda-version              12.8                          3    nvidia\n[conda] libblas                   3.9.0            16_linux64_mkl    conda-forge\n[conda] libcblas                  3.9.0            16_linux64_mkl    conda-forge\n[conda] libcublas                 12.1.0.26                     0    nvidia\n[conda] libcufft                  11.0.2.4                      0    nvidia\n[conda] libcufile                 1.13.1.3                      0    nvidia\n[conda] libcurand                 10.3.9.90                     0    nvidia\n[conda] libcusolver               11.4.4.55                     0    nvidia\n[conda] libcusparse               12.0.2.55                     0    nvidia\n[conda] liblapack                 3.9.0            16_linux64_mkl    conda-forge\n[conda] liblapacke                3.9.0            16_linux64_mkl    conda-forge\n[conda] libnpp                    12.0.2.50                     0    nvidia\n[conda] libnvjitlink              12.1.105                      0    nvidia\n[conda] libnvjpeg                 12.1.1.14                     0    nvidia\n[conda] mkl                       2022.1.0           h84fe81f_915    conda-forge\n[conda] mkl-devel                 2022.1.0           ha770c72_916    conda-forge\n[conda] mkl-include               2022.1.0           h84fe81f_915    conda-forge\n[conda] numpy                     2.0.2                    pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pytorch-cuda              12.1                 ha16c6d3_6    pytorch\n[conda] pytorch-mutex             1.0                        cuda    pytorch\n[conda] pyzmq                     26.4.0                   pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] transformers              4.51.3                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.4\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n\tGPU0\tNIC0\tNIC1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tNODE\tNODE\t2,66\t0\t\tN/A\nNIC0\tNODE\t X \tNODE\t\t\t\t\nNIC1\tNODE\tNODE\t X \t\t\t\t\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n\nCUDA_VISIBLE_DEVICES=MIG-06a16211-6455-5548-9f34-9a810ac2fc5c\nCUDA_VISIBLE_DEVICES=MIG-06a16211-6455-5548-9f34-9a810ac2fc5c\nMKL_INTERFACE_LAYER=LP64,GNU\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen using the [Multi-Instance GPU](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#cuda-device-enumeration) feature, the device names are not integers. They are that start with the letters \"MIG-\".\n\nThe current vllm implementation assumes that GPU names are integers. On line 53 in `vllm/platforms/cuda.py`, the device name is cast into integer. This leads to the following error message:\n\n```\nValueError: invalid literal for int() with base 10: 'MIG-e3e1cf80-63a6-5b1d-8237-3daa43edde87'\n```\n\nIt's possible supporting MIG is not possible, but the error message could be more descriptive. The could also be a mention of the issue in lvvm documentation.\n\n\n\nTo properly reproduce the issue, you would need to set up Multi-Instance GPU. Once this is set up, the following command will reproduce the error:\n\n```bash\npython -m vllm.entrypoints.openai.api_server --model meta-llama/Meta-Llama-3-8B-Instruct\n``` \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-04-23T11:10:09+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17047/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17047"
  },
  {
    "number": 4795,
    "title": "[Bug]: `logprobs` is not compatible with the OpenAI spec",
    "body": "### Your current environment\n\nI'm using Runpod Serverless vLLM (https://github.com/runpod-workers/worker-vllm) so I can't run this command. However, I confirmed that the issue is in the codebase in `main`:\r\n\r\nhttps://github.com/vllm-project/vllm/blob/0fca3cdcf265cd375bca684d951702b6b7adf65a/vllm/entrypoints/openai/protocol.py\r\n\n\n### \ud83d\udc1b Describe the bug\n\nThe behavior of `logprobs=True` does not match OpenAI's.\r\n\r\nI identified two issues:\r\n\r\n**(1) vLLM throws an error when `logprobs=True` and `top_logprobs` is missing.**\r\n\r\nOpenAI works fine:\r\n\r\n```py\r\ncompletion = openai_client.chat.completions.create(\r\n  model=\"gpt-4-turbo-preview\",\r\n  messages=[\r\n    {\"role\": \"user\", \"content\": \"Hi!\"}\r\n  ],\r\n  logprobs=True,\r\n)\r\n```\r\n\r\n```\r\nChatCompletion(id='chatcmpl-9OY4XFK8suJ7ed0yw5vglbTsOZUt1', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='Hello', bytes=[72, 101, 108, 108, 111], logprob=-0.0008963357, top_logprobs=[]), ChatCompletionTokenLogprob(token='!', bytes=[33], logprob=-9.729906e-06, top_logprobs=[]), ChatCompletionTokenLogprob(token=' How', bytes=[32, 72, 111, 119], logprob=-1.4140442e-05, top_logprobs=[]), ChatCompletionTokenLogprob(token=' can', bytes=[32, 99, 97, 110], logprob=-0.0004804817, top_logprobs=[]), ChatCompletionTokenLogprob(token=' I', bytes=[32, 73], logprob=-1.11603495e-05, top_logprobs=[]), ChatCompletionTokenLogprob(token=' assist', bytes=[32, 97, 115, 115, 105, 115, 116], logprob=-0.30324343, top_logprobs=[]), ChatCompletionTokenLogprob(token=' you', bytes=[32, 121, 111, 117], logprob=-5.5122365e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token=' today', bytes=[32, 116, 111, 100, 97, 121], logprob=-1.700133e-05, top_logprobs=[]), ChatCompletionTokenLogprob(token='?', bytes=[63], logprob=-0.001247851, top_logprobs=[])]), message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1715637873, model='gpt-4-0125-preview', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\r\n```\r\n\r\nvLLM breaks:\r\n\r\n```python\r\ncompletion = vllm_client.chat.completions.create(\r\n  model=\"...my-llama3-8b-finetune...\",\r\n  messages=[\r\n    {\"role\": \"user\", \"content\": \"Hi!\"}\r\n  ],\r\n  logprobs=True,\r\n)\r\n```\r\n\r\n```\r\nChatCompletion(id=None, choices=None, created=None, model=None, object='error', system_fingerprint=None, usage=None, code=400, message='Top logprobs must be set when logprobs is.', param=None, type='BadRequestError')\r\n```\r\n\r\nvia https://github.com/vllm-project/vllm/blob/0fca3cdcf265cd375bca684d951702b6b7adf65a/vllm/entrypoints/openai/protocol.py#L162\r\n\r\n**(2) Even wtih `top_logprobs=1`, the behavior doesn't match.**\r\n\r\nOpenAI:\r\n\r\n```python\r\ncompletion = openai_client.chat.completions.create(\r\n  model=\"gpt-4-turbo-preview\",\r\n  messages=[\r\n    {\"role\": \"user\", \"content\": \"Hi!\"}\r\n  ],\r\n  logprobs=True,\r\n  top_logprobs=1,\r\n)\r\n```\r\n\r\n```\r\nChatCompletion(id='chatcmpl-9OY4PwigZVtoM6vHXELby3NWqCyaX', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='Hello', bytes=[72, 101, 108, 108, 111], logprob=-0.0008963357, top_logprobs=[TopLogprob(token='Hello', bytes=[72, 101, 108, 108, 111], logprob=-0.0008963357)]), ChatCompletionTokenLogprob(token='!', bytes=[33], logprob=-9.729906e-06, top_logprobs=[TopLogprob(token='!', bytes=[33], logprob=-9.729906e-06)]), ChatCompletionTokenLogprob(token=' How', bytes=[32, 72, 111, 119], logprob=-1.4140442e-05, top_logprobs=[TopLogprob(token=' How', bytes=[32, 72, 111, 119], logprob=-1.4140442e-05)]), ChatCompletionTokenLogprob(token=' can', bytes=[32, 99, 97, 110], logprob=-0.0004804817, top_logprobs=[TopLogprob(token=' can', bytes=[32, 99, 97, 110], logprob=-0.0004804817)]), ChatCompletionTokenLogprob(token=' I', bytes=[32, 73], logprob=-1.11603495e-05, top_logprobs=[TopLogprob(token=' I', bytes=[32, 73], logprob=-1.11603495e-05)]), ChatCompletionTokenLogprob(token=' assist', bytes=[32, 97, 115, 115, 105, 115, 116], logprob=-0.3164038, top_logprobs=[TopLogprob(token=' assist', bytes=[32, 97, 115, 115, 105, 115, 116], logprob=-0.3164038)]), ChatCompletionTokenLogprob(token=' you', bytes=[32, 121, 111, 117], logprob=-5.5122365e-07, top_logprobs=[TopLogprob(token=' you', bytes=[32, 121, 111, 117], logprob=-5.5122365e-07)]), ChatCompletionTokenLogprob(token=' today', bytes=[32, 116, 111, 100, 97, 121], logprob=-1.50940705e-05, top_logprobs=[TopLogprob(token=' today', bytes=[32, 116, 111, 100, 97, 121], logprob=-1.50940705e-05)]), ChatCompletionTokenLogprob(token='?', bytes=[63], logprob=-0.0011334282, top_logprobs=[TopLogprob(token='?', bytes=[63], logprob=-0.0011334282)])]), message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1715637865, model='gpt-4-0125-preview', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\r\n```\r\n\r\nvLLM:\r\n\r\n```python\r\ncompletion = vllm_client.chat.completions.create(\r\n  model=\"...my-llama3-8b-finetune...\",\r\n  messages=[\r\n    {\"role\": \"user\", \"content\": \"Hi!\"}\r\n  ],\r\n  logprobs=True,\r\n  top_logprobs=1,\r\n)\r\n```\r\n\r\n```\r\nChatCompletion(id='cmpl-c9459bd09bb24a028fef65190d22248d', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=None, text_offset=[0, 2, 3, 7, 9, 14, 18, 24, 25], token_logprobs=[-1.6077232360839844, -1.1920228004455566, -0.0803707167506218, -0.8764647841453552, -0.2521621584892273, -1.823885577323381e-05, -0.020511768758296967, -0.007883979007601738, -0.0015675650211051106], tokens=['Hi', '!', '\u0120How', \"'s\", '\u0120your', '\u0120day', '\u0120going', '?', '<|im_end|>'], top_logprobs=[{'Hey': -1.2327232360839844, 'Hi': -1.6077232360839844}, {'!': -1.1920228004455566, '\u0120there': -0.44202280044555664}, {'\u0120How': -0.0803707167506218}, {\"'s\": -0.8764647841453552, '\u0120are': -0.6264647841453552}, {'\u0120your': -0.2521621584892273}, {'\u0120day': -1.823885577323381e-05}, {'\u0120going': -0.020511768758296967}, {'?': -0.007883979007601738}, {'<|im_end|>': -0.0015675650211051106}]), message=ChatCompletionMessage(content=\"Hi! How's your day going?\", role='assistant', function_call=None, tool_calls=None))], created=2362885, model='REDACTED', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=12, total_tokens=21))\r\n```\r\n\r\nNotice that, for example, `token_logprobs` comes up with vLLM but not with OpenAI.\r\n\r\n---\r\n\r\nThese issues break libraries expecting OpenAI-compatible responses, e.g. Rust's async_openai we are using.\r\n",
    "labels": [
      "bug",
      "help wanted",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-05-13T22:11:16+00:00",
    "closed_at": "2024-05-29T23:13:24+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4795/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4795"
  },
  {
    "number": 7785,
    "title": "[Bug]: install vllm ocurr the building error",
    "body": "### Your current environment\n\nBuilding wheels for collected packages: vllm\r\n  Building wheel for vllm (pyproject.toml) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Building wheel for vllm (pyproject.toml) did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [71 lines of output]\r\n      /tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:258: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\r\n        cpu = _conversion_method_template(device=torch.device(\"cpu\"))\r\n      fatal: not a git repository (or any of the parent directories): .git\r\n      <string>:56: RuntimeWarning: Failed to get commit hash:\r\n      Command '['git', 'rev-parse', 'HEAD']' returned non-zero exit status 128.\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      running build_ext\r\n      CMake Error at CMakeLists.txt:3 (project):\r\n        Running\r\n      \r\n         '/tmp/pip-build-env-bxpa0h4m/overlay/bin/ninja' '--version'\r\n      \r\n        failed with:\r\n      \r\n         no such file or directory\r\n      \r\n      \r\n      -- Configuring incomplete, errors occurred!\r\n      Traceback (most recent call last):\r\n        File \"/home/dj/anaconda3/envs/vllm/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\r\n          main()\r\n        File \"/home/dj/anaconda3/envs/vllm/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\r\n          json_out['return_val'] = hook(**hook_input['kwargs'])\r\n        File \"/home/dj/anaconda3/envs/vllm/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 251, in build_wheel\r\n          return _build_backend().build_wheel(wheel_directory, config_settings,\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 420, in build_wheel\r\n          return self._build_with_temp_dir(\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 402, in _build_with_temp_dir\r\n          self.run_setup()\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 318, in run_setup\r\n          exec(code, locals())\r\n        File \"<string>\", line 474, in <module>\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/__init__.py\", line 111, in setup\r\n          return distutils.core.setup(**attrs)\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 184, in setup\r\n          return run_commands(dist)\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 200, in run_commands\r\n          dist.run_commands()\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 964, in run_commands\r\n          self.run_command(cmd)\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/dist.py\", line 948, in run_command\r\n          super().run_command(command)\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 983, in run_command\r\n          cmd_obj.run()\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/command/bdist_wheel.py\", line 384, in run\r\n          self.run_command(\"build\")\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\r\n          self.distribution.run_command(command)\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/dist.py\", line 948, in run_command\r\n          super().run_command(command)\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 983, in run_command\r\n          cmd_obj.run()\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/_distutils/command/build.py\", line 135, in run\r\n          self.run_command(cmd_name)\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\r\n          self.distribution.run_command(command)\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/dist.py\", line 948, in run_command\r\n          super().run_command(command)\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 983, in run_command\r\n          cmd_obj.run()\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/command/build_ext.py\", line 96, in run\r\n          _build_ext.run(self)\r\n        File \"/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\r\n          self.build_extensions()\r\n        File \"<string>\", line 226, in build_extensions\r\n        File \"<string>\", line 208, in configure\r\n        File \"/home/dj/anaconda3/envs/vllm/lib/python3.10/subprocess.py\", line 369, in check_call\r\n          raise CalledProcessError(retcode, cmd)\r\n      subprocess.CalledProcessError: Command '['cmake', '/home/dj/vllm', '-G', 'Ninja', '-DCMAKE_BUILD_TYPE=RelWithDebInfo', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=/home/dj/vllm/build/lib.linux-x86_64-cpython-310/vllm', '-DCMAKE_ARCHIVE_OUTPUT_DIRECTORY=build/temp.linux-x86_64-cpython-310', '-DVLLM_TARGET_DEVICE=cuda', '-DVLLM_PYTHON_EXECUTABLE=/home/dj/anaconda3/envs/vllm/bin/python', '-DVLLM_PYTHON_PATH=/home/dj/anaconda3/envs/vllm/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process:/tmp/pip-build-env-o_ebi3i5/site:/home/dj/anaconda3/envs/vllm/lib/python310.zip:/home/dj/anaconda3/envs/vllm/lib/python3.10:/home/dj/anaconda3/envs/vllm/lib/python3.10/lib-dynload:/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages:/tmp/pip-build-env-o_ebi3i5/normal/lib/python3.10/site-packages:/tmp/pip-build-env-o_ebi3i5/overlay/lib/python3.10/site-packages/setuptools/_vendor', '-DNVCC_THREADS=1', '-DCMAKE_JOB_POOL_COMPILE:STRING=compile', '-DCMAKE_JOB_POOLS:STRING=compile=128']' returned non-zero exit status 1.\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for vllm\r\nFailed to build vllm\r\nERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (vllm)\r\n\n\n### \ud83d\udc1b Describe the bug\n\ninstall vllm ocurr the building error",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-08-22T12:25:44+00:00",
    "closed_at": "2025-05-20T08:58:20+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7785/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7785"
  },
  {
    "number": 15381,
    "title": "[Bug]: Different logprobs output behaviour under vllm 0.8.0 and 0.8.1",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-6.5.0-41-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA L40S\nGPU 1: NVIDIA L40S\nGPU 2: NVIDIA L40S\nGPU 3: NVIDIA L40S\nGPU 4: NVIDIA L40S\nGPU 5: NVIDIA L40S\nGPU 6: NVIDIA L40S\nGPU 7: NVIDIA L40S\n\nNvidia driver version: 550.127.08\ncuDNN version: Probably one of the following:\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8.1.1\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.1.1\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.1.1\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.1.1\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.1.1\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.1.1\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.1.1\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn.so.8.9.3\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.9.3\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.9.3\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.9.3\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.9.3\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.9.3\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.9.3\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn.so.9.8.0\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_adv.so.9.8.0\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_cnn.so.9.8.0\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_engines_precompiled.so.9.8.0\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_engines_runtime_compiled.so.9.8.0\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_graph.so.9.8.0\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_heuristic.so.9.8.0\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_ops.so.9.8.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      52 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             256\nOn-line CPU(s) list:                0-255\nVendor ID:                          AuthenticAMD\nModel name:                         AMD EPYC 9534 64-Core Processor\nCPU family:                         25\nModel:                              17\nThread(s) per core:                 2\nCore(s) per socket:                 64\nSocket(s):                          2\nStepping:                           1\nFrequency boost:                    enabled\nCPU max MHz:                        3718.0659\nCPU min MHz:                        1500.0000\nBogoMIPS:                           4900.31\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov succor smca fsrm flush_l1d\nVirtualization:                     AMD-V\nL1d cache:                          4 MiB (128 instances)\nL1i cache:                          4 MiB (128 instances)\nL2 cache:                           128 MiB (128 instances)\nL3 cache:                           512 MiB (16 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0-63,128-191\nNUMA node1 CPU(s):                  64-127,192-255\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Mitigation; Safe RET\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.50.0.dev0\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-63,128-191    0               N/A\nGPU1    NODE     X      NODE    NODE    SYS     SYS     SYS     SYS     0-63,128-191    0               N/A\nGPU2    NODE    NODE     X      NODE    SYS     SYS     SYS     SYS     0-63,128-191    0               N/A\nGPU3    NODE    NODE    NODE     X      SYS     SYS     SYS     SYS     0-63,128-191    0               N/A\nGPU4    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    64-127,192-255  1               N/A\nGPU5    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    64-127,192-255  1               N/A\nGPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    64-127,192-255  1               N/A\nGPU7    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      64-127,192-255  1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=:/usr/local/cuda-11.7/extras/CUPTI/lib64:/usr/local/cuda-11.7/extras/CUPTI/lib64:/usr/local/cuda-11.7/lib64\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n\n</details>\n\n\n\n\n\n### \ud83d\udc1b Describe the bug\n\nI use VLLM by starting it as an OpenAI-compatible server. When requesting logprobs from it, vllm in version 0.8.0 and 0.8.1 shows a different behaviour than before.\n\nIt can be reproduced with the following function:\n\n```\nfrom openai import OpenAI\ndef openAI_test(url,model):\n    client = OpenAI( base_url=url,api_key=\"dummy\")\n    openai_response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": \"What is the capital of Germany! \"}],\n        temperature=0.0,\n        logprobs=True,\n        #top_logprobs=1\n    )\n    print(openai_response.choices[0].logprobs)\n```\n\nI started a vllm server with the model `Qwen2.5-VL-7B-Instruct` and send with the above function a request to it.\n\nPrint output with vllm 0.8.0 or 0.8.1:\n`ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='The', bytes=[84, 104, 101], logprob=-0.0005716835148632526, top_logprobs=[]), ChatCompletionTokenLogprob(token='\u0120capital', bytes=[196, 160, 99, 97, 112, 105, 116, 97, 108], logprob=-8.630380034446716e-05, top_logprobs=[]), ChatCompletionTokenLogprob(token='\u0120of', bytes=[196, 160, 111, 102], logprob=-0.10054320842027664, top_logprobs=[]), ChatCompletionTokenLogprob(token='\u0120Germany', bytes=[196, 160, 71, 101, 114, 109, 97, 110, 121], logprob=-4.994744449504651e-05, top_logprobs=[]), ChatCompletionTokenLogprob(token='\u0120is', bytes=[196, 160, 105, 115], logprob=-0.0004349001101218164, top_logprobs=[]), ChatCompletionTokenLogprob(token='\u0120Berlin', bytes=[196, 160, 66, 101, 114, 108, 105, 110], logprob=-0.0014521064003929496, top_logprobs=[]), ChatCompletionTokenLogprob(token='.', bytes=[46], logprob=-0.015473785810172558, top_logprobs=[]), ChatCompletionTokenLogprob(token='<|im_end|>', bytes=[60, 124, 105, 109, 95, 101, 110, 100, 124, 62], logprob=-0.02971755340695381, top_logprobs=[])], refusal=None)`\n\nPrint output with vllm 0.7.3 or lower:\n`ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='The', bytes=[84, 104, 101], logprob=-0.0006090931710787117, top_logprobs=[]), ChatCompletionTokenLogprob(token=' capital', bytes=[32, 99, 97, 112, 105, 116, 97, 108], logprob=-7.70062324590981e-05, top_logprobs=[]), ChatCompletionTokenLogprob(token=' of', bytes=[32, 111, 102], logprob=-0.1005404070019722, top_logprobs=[]), ChatCompletionTokenLogprob(token=' Germany', bytes=[32, 71, 101, 114, 109, 97, 110, 121], logprob=-4.6132929128361866e-05, top_logprobs=[]), ChatCompletionTokenLogprob(token=' is', bytes=[32, 105, 115], logprob=-0.0005459486856125295, top_logprobs=[]), ChatCompletionTokenLogprob(token=' Berlin', bytes=[32, 66, 101, 114, 108, 105, 110], logprob=-0.0014579391572624445, top_logprobs=[]), ChatCompletionTokenLogprob(token='.', bytes=[46], logprob=-0.013593370094895363, top_logprobs=[]), ChatCompletionTokenLogprob(token='', bytes=[], logprob=-0.0293378084897995, top_logprobs=[])], refusal=None)`\n\nIt seems that in the newer vllm version the tokenwise output is given in the tokenizers internal representation of the used model (as in the `vocab.json` of the corresponding HF-model). New: `\u0120capital` . Old: ` capital`. But this is a problem, because for an application on the client-site it is no longer possible to parse the output together with the logprob-positions without having special knowledge about the tokenizer. The official Open-AI server (see `https://api.openai.com/v1/`) behaves in the same way as the older VLLM versions. \n\nCan this be fixed so that VLLM behaves as before? Or is there an easy way to convert the output to the old format?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-24T07:35:15+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15381/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15381"
  },
  {
    "number": 16521,
    "title": "[Bug]: Add TPU support for gemma-3-4b-it and gemma-3-27b-it",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 04-12 01:04:02 [__init__.py:239] Automatically detected platform tpu.\nCollecting environment information...\nPyTorch version: 2.8.0\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\nClang version: Could not collect\nCMake version: version 4.0.0\nLibc version: glibc-2.31\n\nPython version: 3.10.16 (main, Jan 14 2025, 05:27:07) [GCC 10.2.1 20210110] (64-bit runtime)\nPython platform: Linux-6.8.0-1015-gcp-x86_64-with-glibc2.31\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nByte Order:                           Little Endian\nAddress sizes:                        52 bits physical, 57 bits virtual\nCPU(s):                               44\nOn-line CPU(s) list:                  0-43\nThread(s) per core:                   2\nCore(s) per socket:                   22\nSocket(s):                            1\nNUMA node(s):                         1\nVendor ID:                            AuthenticAMD\nCPU family:                           25\nModel:                                17\nModel name:                           AMD EPYC 9B14\nStepping:                             1\nCPU MHz:                              2599.996\nBogoMIPS:                             5199.99\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            704 KiB\nL1i cache:                            704 KiB\nL2 cache:                             22 MiB\nL3 cache:                             64 MiB\nNUMA node0 CPU(s):                    0-43\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr wbnoinvd arat avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm\n\nVersions of relevant libraries:\n[pip3] numpy==2.2.2\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.8.0\n[pip3] torch-xla==2.8.0+git9ec626e\n[pip3] transformers==4.51.2\n[pip3] triton==3.3.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.4.dev1067+g93195146e\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nVLLM_TARGET_DEVICE=tpu\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nAttempting to load gemma-3-4b-it or gemma-3-27b-it leads to vLLM engine crashing with below error.\n\nComparing the model configs against gemma-3-1b-it (which currently works), the architecture looks different at I don't see head_dim key value on the former models.\n\nGemma3ForConditionalGeneration (4b/27b) vs Gemma3ForCausalLM (1b)\n\n```\nVLLM_USE_V1=1 vllm serve 'google/gemma-3-4b-it' --port 8080 --disable-log-requests --seed 42 --max-model-len 2048 --gpu-memory-utilization 0.95\nINFO 04-12 01:01:02 [__init__.py:239] Automatically detected platform tpu.\nINFO 04-12 01:01:06 [api_server.py:1034] vLLM API server version 0.7.4.dev1067+g93195146e\nINFO 04-12 01:01:06 [api_server.py:1035] args: Namespace(subparser='serve', model_tag='google/gemma-3-4b-it', config='', host=None, port=8080, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='google/gemma-3-4b-it', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=2048, guided_decoding_backend='auto', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=42, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_chunked_mm_input=False, disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x7416e8d655a0>)\nINFO 04-12 01:01:13 [config.py:676] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify', 'score'}. Defaulting to 'generate'.\nWARNING 04-12 01:01:13 [arg_utils.py:1758] Detected VLLM_USE_V1=1 with tpu. Usage should be considered experimental. Please report any issues on Github.\nINFO 04-12 01:01:13 [config.py:1731] Disabled the custom all-reduce kernel because it is not supported on current platform.\nINFO 04-12 01:01:13 [config.py:1885] Chunked prefill is enabled with max_num_batched_tokens=2048.\nINFO 04-12 01:01:13 [tpu.py:85] [TPU] Forcing DYNAMO_ONCE compilation level\nINFO 04-12 01:01:17 [__init__.py:239] Automatically detected platform tpu.\nINFO 04-12 01:01:21 [core.py:61] Initializing a V1 LLM engine (v0.7.4.dev1067+g93195146e) with config: model='google/gemma-3-4b-it', speculative_config=None, tokenizer='google/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=None, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=google/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":2,\"backend\":\"openxla\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\nINFO 04-12 01:01:21 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nWARNING 04-12 01:01:28 [tpu.py:125] Pin memory is not supported on TPU.\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nINFO 04-12 01:01:34 [tpu_model_runner.py:1042] Using exponential token paddings:\nINFO 04-12 01:01:34 [tpu_model_runner.py:1044]     16\nINFO 04-12 01:01:34 [tpu_model_runner.py:1044]     32\nINFO 04-12 01:01:34 [tpu_model_runner.py:1044]     64\nINFO 04-12 01:01:34 [tpu_model_runner.py:1044]     128\nINFO 04-12 01:01:34 [tpu_model_runner.py:1044]     256\nINFO 04-12 01:01:34 [tpu_model_runner.py:1044]     512\nINFO 04-12 01:01:34 [tpu_model_runner.py:1044]     1024\nINFO 04-12 01:01:34 [tpu_model_runner.py:1044]     2048\nINFO 04-12 01:01:34 [tpu_model_runner.py:1008] Preparing request paddings:\nINFO 04-12 01:01:34 [tpu_model_runner.py:1015]     8\nINFO 04-12 01:01:34 [tpu_model_runner.py:1015]     16\nINFO 04-12 01:01:34 [tpu_model_runner.py:1015]     32\nINFO 04-12 01:01:34 [tpu_model_runner.py:1015]     64\nINFO 04-12 01:01:34 [tpu_model_runner.py:1015]     128\nINFO 04-12 01:01:34 [tpu_model_runner.py:1015]     256\nINFO 04-12 01:01:34 [tpu.py:48] Cannot use None backend on TPU.\nINFO 04-12 01:01:34 [tpu.py:51] Using Pallas V1 backend.\nINFO 04-12 01:01:34 [config.py:3439] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]\nINFO 04-12 01:01:34 [tpu.py:85] [TPU] Forcing DYNAMO_ONCE compilation level\nINFO 04-12 01:01:34 [tpu.py:48] Cannot use None backend on TPU.\nINFO 04-12 01:01:34 [tpu.py:51] Using Pallas V1 backend.\nINFO 04-12 01:01:35 [weight_utils.py:265] Using model weights format ['*.safetensors']\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n2025-04-12 01:01:35.698388: W torch_xla/csrc/runtime/pjrt_computation_client.cc:680] Failed to deserialize executable: UNIMPLEMENTED: Deserializing serialized executable not supported.\n2025-04-12 01:01:38.716542: W torch_xla/csrc/runtime/pjrt_computation_client.cc:680] Failed to deserialize executable: UNIMPLEMENTED: Deserializing serialized executable not supported.\n2025-04-12 01:01:38.992913: W torch_xla/csrc/runtime/pjrt_computation_client.cc:680] Failed to deserialize executable: UNIMPLEMENTED: Deserializing serialized executable not supported.\n2025-04-12 01:01:39.274353: W torch_xla/csrc/runtime/pjrt_computation_client.cc:680] Failed to deserialize executable: UNIMPLEMENTED: Deserializing serialized executable not supported.\n2025-04-12 01:01:39.494791: W torch_xla/csrc/runtime/pjrt_computation_client.cc:680] Failed to deserialize executable: UNIMPLEMENTED: Deserializing serialized executable not supported.\n2025-04-12 01:01:39.738959: W torch_xla/csrc/runtime/pjrt_computation_client.cc:680] Failed to deserialize executable: UNIMPLEMENTED: Deserializing serialized executable not supported.\n2025-04-12 01:01:40.347730: W torch_xla/csrc/runtime/pjrt_computation_client.cc:680] Failed to deserialize executable: UNIMPLEMENTED: Deserializing serialized executable not supported.\n2025-04-12 01:01:40.415069: W torch_xla/csrc/runtime/pjrt_computation_client.cc:680] Failed to deserialize executable: UNIMPLEMENTED: Deserializing serialized executable not supported.\n2025-04-12 01:01:40.434250: W torch_xla/csrc/runtime/pjrt_computation_client.cc:680] Failed to deserialize executable: UNIMPLEMENTED: Deserializing serialized executable not supported.\n2025-04-12 01:01:40.700580: W torch_xla/csrc/runtime/pjrt_computation_client.cc:680] Failed to deserialize executable: UNIMPLEMENTED: Deserializing serialized executable not supported.\n2025-04-12 01:01:40.718958: W torch_xla/csrc/runtime/pjrt_computation_client.cc:680] Failed to deserialize executable: UNIMPLEMENTED: Deserializing serialized executable not supported.\n2025-04-12 01:01:40.977894: W torch_xla/csrc/runtime/pjrt_computation_client.cc:680] Failed to deserialize executable: UNIMPLEMENTED: Deserializing serialized executable not supported.\n2025-04-12 01:01:40.996260: W torch_xla/csrc/runtime/pjrt_computation_client.cc:680] Failed to deserialize executable: UNIMPLEMENTED: Deserializing serialized executable not supported.\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:05<00:05,  5.92s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  2.81s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  3.28s/it]\n\nINFO 04-12 01:01:42 [loader.py:458] Loading weights took 6.69 seconds\n2025-04-12 01:02:18.983016: W torch_xla/csrc/runtime/pjrt_computation_client.cc:680] Failed to deserialize executable: UNIMPLEMENTED: Deserializing serialized executable not supported.\n2025-04-12 01:02:19.989667: W torch_xla/csrc/runtime/pjrt_computation_client.cc:680] Failed to deserialize executable: UNIMPLEMENTED: Deserializing serialized executable not supported.\nINFO 04-12 01:02:25 [tpu_model_runner.py:973] Clear dynamo cache and cached dynamo bytecode.\nINFO 04-12 01:02:25 [kv_cache_utils.py:634] GPU KV cache size: 160,880 tokens\nINFO 04-12 01:02:25 [kv_cache_utils.py:637] Maximum concurrency for 2,048 tokens per request: 78.55x\nINFO 04-12 01:02:25 [tpu_model_runner.py:857] Compiling the model with different input shapes.\nINFO 04-12 01:02:25 [tpu_model_runner.py:861]   -- num_tokens: 16\n2025-04-12 01:03:11.442527: W torch_xla/csrc/runtime/pjrt_computation_client.cc:680] Failed to deserialize executable: UNIMPLEMENTED: Deserializing serialized executable not supported.\n/usr/local/lib/python3.10/site-packages/jax/_src/cloud_tpu_init.py:82: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n  warnings.warn(\nERROR 04-12 01:03:21 [core.py:386] EngineCore hit an exception: Traceback (most recent call last):\nERROR 04-12 01:03:21 [core.py:386]   File \"/workspace/vllm/vllm/v1/engine/core.py\", line 377, in run_engine_core\nERROR 04-12 01:03:21 [core.py:386]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 04-12 01:03:21 [core.py:386]   File \"/workspace/vllm/vllm/v1/engine/core.py\", line 319, in __init__\nERROR 04-12 01:03:21 [core.py:386]     super().__init__(vllm_config, executor_class, log_stats)\nERROR 04-12 01:03:21 [core.py:386]   File \"/workspace/vllm/vllm/v1/engine/core.py\", line 71, in __init__\nERROR 04-12 01:03:21 [core.py:386]     self._initialize_kv_caches(vllm_config)\nERROR 04-12 01:03:21 [core.py:386]   File \"/workspace/vllm/vllm/v1/engine/core.py\", line 159, in _initialize_kv_caches\nERROR 04-12 01:03:21 [core.py:386]     self.model_executor.initialize_from_config(kv_cache_configs)\nERROR 04-12 01:03:21 [core.py:386]   File \"/workspace/vllm/vllm/v1/executor/abstract.py\", line 63, in initialize_from_config\nERROR 04-12 01:03:21 [core.py:386]     self.collective_rpc(\"compile_or_warm_up_model\")\nERROR 04-12 01:03:21 [core.py:386]   File \"/workspace/vllm/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\nERROR 04-12 01:03:21 [core.py:386]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 04-12 01:03:21 [core.py:386]   File \"/workspace/vllm/vllm/utils.py\", line 2364, in run_method\nERROR 04-12 01:03:21 [core.py:386]     return func(*args, **kwargs)\nERROR 04-12 01:03:21 [core.py:386]   File \"/workspace/vllm/vllm/v1/worker/tpu_worker.py\", line 215, in compile_or_warm_up_model\nERROR 04-12 01:03:21 [core.py:386]     self.model_runner.capture_model()\nERROR 04-12 01:03:21 [core.py:386]   File \"/workspace/vllm/vllm/v1/worker/tpu_model_runner.py\", line 924, in capture_model\nERROR 04-12 01:03:21 [core.py:386]     self._precompile_backbone()\nERROR 04-12 01:03:21 [core.py:386]   File \"/workspace/vllm/vllm/v1/worker/tpu_model_runner.py\", line 862, in _precompile_backbone\nERROR 04-12 01:03:21 [core.py:386]     self._dummy_run(num_tokens)\nERROR 04-12 01:03:21 [core.py:386]   File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 04-12 01:03:21 [core.py:386]     return func(*args, **kwargs)\nERROR 04-12 01:03:21 [core.py:386]   File \"/workspace/vllm/vllm/v1/worker/tpu_model_runner.py\", line 851, in _dummy_run\nERROR 04-12 01:03:21 [core.py:386]     out = self.model(input_ids=input_ids,\nERROR 04-12 01:03:21 [core.py:386]   File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\nERROR 04-12 01:03:21 [core.py:386]     return self._call_impl(*args, **kwargs)\nERROR 04-12 01:03:21 [core.py:386]   File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\nERROR 04-12 01:03:21 [core.py:386]     return forward_call(*args, **kwargs)\nERROR 04-12 01:03:21 [core.py:386]   File \"/workspace/vllm/vllm/model_executor/models/gemma3_mm.py\", line 630, in forward\nERROR 04-12 01:03:21 [core.py:386]     hidden_states = self.language_model.model(input_ids,\nERROR 04-12 01:03:21 [core.py:386]   File \"/workspace/vllm/vllm/compilation/decorators.py\", line 238, in __call__\nERROR 04-12 01:03:21 [core.py:386]     output = self.compiled_callable(*args, **kwargs)\nERROR 04-12 01:03:21 [core.py:386]   File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 658, in _fn\nERROR 04-12 01:03:21 [core.py:386]     return fn(*args, **kwargs)\nERROR 04-12 01:03:21 [core.py:386]   File \"/workspace/vllm/vllm/model_executor/models/gemma3.py\", line 381, in forward\nERROR 04-12 01:03:21 [core.py:386]     def forward(\nERROR 04-12 01:03:21 [core.py:386]   File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 850, in _fn\nERROR 04-12 01:03:21 [core.py:386]     return fn(*args, **kwargs)\nERROR 04-12 01:03:21 [core.py:386]   File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 1207, in forward\nERROR 04-12 01:03:21 [core.py:386]     return compiled_fn(full_args)\nERROR 04-12 01:03:21 [core.py:386]   File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 331, in runtime_wrapper\nERROR 04-12 01:03:21 [core.py:386]     all_outs = call_func_at_runtime_with_args(\nERROR 04-12 01:03:21 [core.py:386]   File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\nERROR 04-12 01:03:21 [core.py:386]     out = normalize_as_list(f(args))\nERROR 04-12 01:03:21 [core.py:386]   File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 692, in inner_fn\nERROR 04-12 01:03:21 [core.py:386]     outs = compiled_fn(args)\nERROR 04-12 01:03:21 [core.py:386]   File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 498, in wrapper\nERROR 04-12 01:03:21 [core.py:386]     return compiled_fn(runtime_args)\nERROR 04-12 01:03:21 [core.py:386]   File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 100, in g\nERROR 04-12 01:03:21 [core.py:386]     return f(*args)\nERROR 04-12 01:03:21 [core.py:386]   File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/backends/torchxla.py\", line 37, in fwd\nERROR 04-12 01:03:21 [core.py:386]     compiled_graph = bridge.extract_compiled_graph(model, args)\nERROR 04-12 01:03:21 [core.py:386]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py\", line 709, in extract_compiled_graph\nERROR 04-12 01:03:21 [core.py:386]     return extract_compiled_graph_helper(xla_model, xla_args)\nERROR 04-12 01:03:21 [core.py:386]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py\", line 828, in extract_compiled_graph_helper\nERROR 04-12 01:03:21 [core.py:386]     return extract_internal(xla_model)\nERROR 04-12 01:03:21 [core.py:386]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py\", line 515, in extract_internal\nERROR 04-12 01:03:21 [core.py:386]     xla_args_need_update) = extract_graph_helper(xla_model,\nERROR 04-12 01:03:21 [core.py:386]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py\", line 459, in extract_graph_helper\nERROR 04-12 01:03:21 [core.py:386]     torch_xla._XLAC._xla_warm_up_cache(args_and_out_tensor_only, [])\nERROR 04-12 01:03:21 [core.py:386] RuntimeError: Bad StatusOr access: INTERNAL: Mosaic failed to compile TPU kernel: Not Implemented: The last dim size is not 128 in original base memref\nERROR 04-12 01:03:21 [core.py:386] \nERROR 04-12 01:03:21 [core.py:386] at location: loc(\"/get\"(callsite(\"forward\"(\"<eval_with_key>.3 from /usr/local/lib/python3.10/site-packages/torch/fx/experimental/proxy_tensor.py:1265 in wrapped\":95:0 to 0:0) at callsite(\"forward\"(\"/workspace/vllm/vllm/model_executor/models/gemma3.py\":381:0 to 0:0) at callsite(\"__call__\"(\"/workspace/vllm/vllm/compilation/decorators.py\":238:0 to 0:0) at callsite(\"forward\"(\"/workspace/vllm/vllm/model_executor/models/gemma3_mm.py\":630:0 to 0:0) at callsite(\"_dummy_run\"(\"/workspace/vllm/vllm/v1/worker/tpu_model_runner.py\":851:0 to 0:0) at callsite(\"_precompile_backbone\"(\"/workspace/vllm/vllm/v1/worker/tpu_model_runner.py\":862:0 to 0:0) at callsite(\"capture_model\"(\"/workspace/vllm/vllm/v1/worker/tpu_model_runner.py\":924:0 to 0:0) at callsite(\"compile_or_warm_up_model\"(\"/workspace/vllm/vllm/v1/worker/tpu_worker.py\":215:0 to 0:0) at callsite(\"run_method\"(\"/workspace/vllm/vllm/utils.py\":2364:0 to 0:0) at \"collective_rpc\"(\"/workspace/vllm/vllm/executor/uniproc_executor.py\":56:0 to 0:0))))))))))))\nERROR 04-12 01:03:21 [core.py:386] \nERROR 04-12 01:03:21 [core.py:386] The MLIR operation involved:\nERROR 04-12 01:03:21 [core.py:386]   %2064 = \"tpu.strided_load\"(%2063, %295, %295) <{strides = array<i32: 4, 1>}> {in_layout = [#tpu.vpad<\"none\">, #tpu.vpad<\"none\">, #tpu.vpad<\"none\">], out_layout = [#tpu.vpad<\"32,{0,0},(8,128)\">]} : (memref<8192x256xi32, #tpu.memory_space<vmem>>, index, index) -> vector<2048x256xi32>\nERROR 04-12 01:03:21 [core.py:386] \nERROR 04-12 01:03:21 [core.py:386] Please report a bug at: https://github.com/google/jax/issues/new?assignees=apaszke\nERROR 04-12 01:03:21 [core.py:386] \nERROR 04-12 01:03:21 [core.py:386] \nCRITICAL 04-12 01:03:21 [core_client.py:359] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\nKilled\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-04-12T01:08:58+00:00",
    "closed_at": null,
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16521/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16521"
  },
  {
    "number": 9417,
    "title": "[Bug]: Regression ~~for AWQ marlin kernels~~ from v0.6.2 to v0.6.3 when using CUDA Graphs",
    "body": "### Your current environment\n\nFirst of all: fantastic project :-) Thank you for everything.\r\n\r\nI would like to fix this bug. But I just do not have the capacity now. So I just thought I would try to make a good bug report.\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nIf I run this model in `v0.6.2`:\r\n\r\n```bash\r\nvllm serve hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4 -tp 4 --gpu-memory-utilization 0.90 --max-model-len 32768\r\n```\r\n\r\nAll works well and good :-)\r\n\r\nIf I run it in `v0.6.3`\r\n```bash\r\nvllm serve hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4 -tp 4 --gpu-memory-utilization 0.90 --max-model-len 32768 --enforce-eager\r\n```\r\n\r\nAll works well and good with enforce eager :-)\r\n\r\nIf I drop the `enforce-eager`\r\n\r\n```bash\r\nvllm serve hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4 -tp 4 --gpu-memory-utilization 0.90 --max-model-len 32768\r\n```\r\n\r\nI get random repetition on large prompts 6000+ token. Or if I do multiple request in parallel I get `CUDA: illegal memory access`\r\n\r\nMy guess is that there is something dynamic in the updated `awq_marlin` kernels. \r\n\r\nMy hunch (this is untested): #8973 but I am not fully understanding how my non MoE should be affected by this.\r\n\r\n\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-10-16T10:13:07+00:00",
    "closed_at": "2024-11-09T00:44:39+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9417/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/9417"
  },
  {
    "number": 10752,
    "title": "[Feature]: Enable `/score` endpoint for all embedding models",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently only cross-encoder models support the `/score` endpoint. But it would make sense to enable it also for the embedding models using bi-encoding, i.e. calculating a cosine similarity score between the embedding vectors.\r\n\r\ncc: @DarkLight1337 \n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-11-28T16:08:38+00:00",
    "closed_at": "2025-02-21T17:52:32+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10752/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/10752"
  },
  {
    "number": 8151,
    "title": "[Usage]: How to use vllm infer video with Internvl2 8b multimodal model ",
    "body": "### Your current environment\n\npython==3.8\r\nvllm==0.5.4\r\ntransformers==4.44.0\r\ntorch==2.4.0\n\n### How would you like to use vllm\n\nI want to run inference of a [Internvl2 8b](https://huggingface.co/OpenGVLab/InternVL2-8B) with video source. I don't know how to integrate it with vllm.\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-09-04T09:51:42+00:00",
    "closed_at": "2024-09-29T08:52:30+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8151/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8151"
  },
  {
    "number": 14897,
    "title": "[Bug]: Gemma3 Offline Batch Inference: Attempted to assign XXX multimodal tokens to YYY placeholders",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0] (64-bit runtime)\nPython platform: Linux-5.15.0-112-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA H100 80GB HBM3\nGPU 1: NVIDIA H100 80GB HBM3\nGPU 2: NVIDIA H100 80GB HBM3\nGPU 3: NVIDIA H100 80GB HBM3\nGPU 4: NVIDIA H100 80GB HBM3\nGPU 5: NVIDIA H100 80GB HBM3\nGPU 6: NVIDIA H100 80GB HBM3\nGPU 7: NVIDIA H100 80GB HBM3\n\nNvidia driver version: 550.90.07\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      46 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             104\nOn-line CPU(s) list:                0-103\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) Platinum 8470\nCPU family:                         6\nModel:                              143\nThread(s) per core:                 1\nCore(s) per socket:                 52\nSocket(s):                          2\nStepping:                           8\nBogoMIPS:                           4000.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nL1d cache:                          4.9 MiB (104 instances)\nL1i cache:                          3.3 MiB (104 instances)\nL2 cache:                           208 MiB (104 instances)\nL3 cache:                           210 MiB (2 instances)\nNUMA node(s):                       8\nNUMA node0 CPU(s):                  0-12\nNUMA node1 CPU(s):                  13-25\nNUMA node2 CPU(s):                  26-38\nNUMA node3 CPU(s):                  39-51\nNUMA node4 CPU(s):                  52-64\nNUMA node5 CPU(s):                  65-77\nNUMA node6 CPU(s):                  78-90\nNUMA node7 CPU(s):                  91-103\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.3+cu124torch2.6\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pynvml==12.0.0\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.50.0.dev0\n[pip3] triton==3.2.0\n[conda] flashinfer-python         0.2.3+cu124torch2.6          pypi_0    pypi\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-ml-py              12.570.86                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pynvml                    12.0.0                   pypi_0    pypi\n[conda] pyzmq                     26.3.0                   pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] transformers              4.50.0.dev0              pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.4.dev477+g61c6a5a7\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9      NIC10   NIC11   CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     PIX     PIX     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS     0-12    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS     26-38   2               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS       SYS     SYS     SYS     39-51   3               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS       SYS     SYS     SYS     13-25   1               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX     PIX       SYS     SYS     SYS     52-64   4               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       PIX     SYS     SYS     78-90   6               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     PIX     SYS     91-103  7               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     PIX     65-77   5               N/A\nNIC0    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     PIX     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS\nNIC1    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      PIX     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS\nNIC2    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX      X      SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS\nNIC3    SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS\nNIC4    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS       SYS     SYS     SYS\nNIC5    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS       SYS     SYS     SYS\nNIC6    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     PIX       SYS     SYS     SYS\nNIC7    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      PIX       SYS     SYS     SYS\nNIC8    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX      X        SYS     SYS     SYS\nNIC9    SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS        X      SYS     SYS\nNIC10   SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS      X      SYS\nNIC11   SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n  NIC10: mlx5_10\n  NIC11: mlx5_11\n\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n@WoosukKwon This error is likely a processor-related error.\n\nThe error happens for both llm.chat() and llm.generate(). It says `Attempted to assign XXX multimodal tokens to YYY placeholders`. This error only happens when there are image inputs, but is arbitrary to image (i.e. it remains when replacing images with other images). This error happens only when `len(messages)>=32`, i.e. if I input messages individually for `len(messages)` times or using a mini-batched version, it does not raise an error.\n\nMinimum reproduction example:\n\n```\nfrom vllm import LLM, SamplingParams\nimport torch\n\nif __name__ == '__main__':\n\n    model = LLM(\n                model=\"google/gemma-3-27b-it\",\n                max_model_len=8192,\n                tensor_parallel_size=1,\n                limit_mm_per_prompt={\"image\": 5},\n                tokenizer_mode=\"auto\"\n            )\n    sampling_params = SamplingParams(temperature=1,max_tokens=8192,stop_token_ids=None)\n        \n    messages = torch.load(\"messages.pt\") # contanins 64 messages, the error does not matter whether you input any image or not\n\n    response = model.chat(\n        messages=messages,\n        sampling_params=sampling_params,\n        chat_template=None,\n    )\n\n    outputs = []\n    for out in response:\n        generated_text = out.outputs[0].text\n        outputs.append(generated_text)\n        \n    print(outputs)\n```\n\nError:\n\n```\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nProcessed prompts:  47%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b             | 30/64 [00:10<00:12,  2.77it/s, est. speed input: 4163.14 toks/s, output: 481.12 toks/s]ERROR 03-16 12:28:39 [core.py:340] EngineCore hit an exception: Traceback (most recent call last):\nERROR 03-16 12:28:39 [core.py:340]   File \"/home/agi/vllm/vllm/v1/engine/core.py\", line 333, in run_engine_core\nERROR 03-16 12:28:39 [core.py:340]     engine_core.run_busy_loop()\nERROR 03-16 12:28:39 [core.py:340]   File \"/home/agi/vllm/vllm/v1/engine/core.py\", line 367, in run_busy_loop\nERROR 03-16 12:28:39 [core.py:340]     outputs = step_fn()\nERROR 03-16 12:28:39 [core.py:340]   File \"/home/agi/vllm/vllm/v1/engine/core.py\", line 192, in step\nERROR 03-16 12:28:39 [core.py:340]     output = self.model_executor.execute_model(scheduler_output)\nERROR 03-16 12:28:39 [core.py:340]   File \"/home/agi/vllm/vllm/v1/executor/abstract.py\", line 80, in execute_model\nERROR 03-16 12:28:39 [core.py:340]     output = self.collective_rpc(\"execute_model\",\nERROR 03-16 12:28:39 [core.py:340]   File \"/home/agi/vllm/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\nERROR 03-16 12:28:39 [core.py:340]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 03-16 12:28:39 [core.py:340]   File \"/home/agi/vllm/vllm/utils.py\", line 2216, in run_method\nERROR 03-16 12:28:39 [core.py:340]     return func(*args, **kwargs)\nERROR 03-16 12:28:39 [core.py:340]   File \"/home/agi/miniconda3/envs/pae/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 03-16 12:28:39 [core.py:340]     return func(*args, **kwargs)\nERROR 03-16 12:28:39 [core.py:340]   File \"/home/agi/vllm/vllm/v1/worker/gpu_worker.py\", line 242, in execute_model\nERROR 03-16 12:28:39 [core.py:340]     output = self.model_runner.execute_model(scheduler_output)\nERROR 03-16 12:28:39 [core.py:340]   File \"/home/agi/miniconda3/envs/pae/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 03-16 12:28:39 [core.py:340]     return func(*args, **kwargs)\nERROR 03-16 12:28:39 [core.py:340]   File \"/home/agi/vllm/vllm/v1/worker/gpu_model_runner.py\", line 961, in execute_model\nERROR 03-16 12:28:39 [core.py:340]     inputs_embeds = self.model.get_input_embeddings(\nERROR 03-16 12:28:39 [core.py:340]   File \"/home/agi/vllm/vllm/model_executor/models/gemma3_mm.py\", line 502, in get_input_embeddings\nERROR 03-16 12:28:39 [core.py:340]     inputs_embeds = merge_multimodal_embeddings(\nERROR 03-16 12:28:39 [core.py:340]   File \"/home/agi/vllm/vllm/model_executor/models/utils.py\", line 455, in merge_multimodal_embeddings\nERROR 03-16 12:28:39 [core.py:340]     return _merge_multimodal_embeddings(\nERROR 03-16 12:28:39 [core.py:340]   File \"/home/agi/vllm/vllm/model_executor/models/utils.py\", line 371, in _merge_multimodal_embeddings\nERROR 03-16 12:28:39 [core.py:340]     raise ValueError(\nERROR 03-16 12:28:39 [core.py:340] ValueError: Attempted to assign 256 + 74 = 330 multimodal tokens to 332 placeholders\nERROR 03-16 12:28:39 [core.py:340] \nCRITICAL 03-16 12:28:39 [core_client.py:269] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\nKilled\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-16T17:39:36+00:00",
    "closed_at": "2025-03-19T06:58:23+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14897/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14897"
  },
  {
    "number": 15836,
    "title": "[Bug]: Gemma-3 (27B) can't load save_pretrained() checkpoint: AssertionError: expected size 5376==2560, stride 1==1 at dim=0",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0] (64-bit runtime)\nPython platform: Linux-5.15.0-112-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA H100 80GB HBM3\nGPU 1: NVIDIA H100 80GB HBM3\nGPU 2: NVIDIA H100 80GB HBM3\nGPU 3: NVIDIA H100 80GB HBM3\nGPU 4: NVIDIA H100 80GB HBM3\nGPU 5: NVIDIA H100 80GB HBM3\nGPU 6: NVIDIA H100 80GB HBM3\nGPU 7: NVIDIA H100 80GB HBM3\n\nNvidia driver version: 550.90.07\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      46 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             104\nOn-line CPU(s) list:                0-103\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) Platinum 8470\nCPU family:                         6\nModel:                              143\nThread(s) per core:                 1\nCore(s) per socket:                 52\nSocket(s):                          2\nStepping:                           8\nBogoMIPS:                           4000.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nL1d cache:                          4.9 MiB (104 instances)\nL1i cache:                          3.3 MiB (104 instances)\nL2 cache:                           208 MiB (104 instances)\nL3 cache:                           210 MiB (2 instances)\nNUMA node(s):                       8\nNUMA node0 CPU(s):                  0-12\nNUMA node1 CPU(s):                  13-25\nNUMA node2 CPU(s):                  26-38\nNUMA node3 CPU(s):                  39-51\nNUMA node4 CPU(s):                  52-64\nNUMA node5 CPU(s):                  65-77\nNUMA node6 CPU(s):                  78-90\nNUMA node7 CPU(s):                  91-103\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.3+cu124torch2.6\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pynvml==12.0.0\n[pip3] pytorch-memlab==0.3.0\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.50.0.dev0\n[pip3] triton==3.2.0\n[conda] flashinfer-python         0.2.3+cu124torch2.6          pypi_0    pypi\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-ml-py              12.570.86                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pynvml                    12.0.0                   pypi_0    pypi\n[conda] pytorch-memlab            0.3.0                    pypi_0    pypi\n[conda] pyzmq                     26.3.0                   pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] transformers              4.50.0.dev0              pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.2.dev78+g6ebaf9ac\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8       NIC9    NIC10   NIC11   CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     PIX     PIX     SYS     SYS     SYS     SYS     SYSSYS     SYS     SYS     SYS     0-12    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYSSYS     SYS     SYS     SYS     26-38   2               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYSSYS     SYS     SYS     SYS     39-51   3               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYSSYS     SYS     SYS     SYS     13-25   1               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIXPIX     SYS     SYS     SYS     52-64   4               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS     PIX     SYS     SYS     78-90   6               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS     SYS     PIX     SYS     91-103  7               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS     SYS     SYS     PIX     65-77   5               N/A\nNIC0    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     PIX     SYS     SYS     SYS     SYS     SYSSYS     SYS     SYS     SYS\nNIC1    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      PIX     SYS     SYS     SYS     SYS     SYSSYS     SYS     SYS     SYS\nNIC2    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX      X      SYS     SYS     SYS     SYS     SYSSYS     SYS     SYS     SYS\nNIC3    SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYSSYS     SYS     SYS     SYS\nNIC4    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYSSYS     SYS     SYS     SYS\nNIC5    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYSSYS     SYS     SYS     SYS\nNIC6    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIXPIX     SYS     SYS     SYS\nNIC7    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X PIX     SYS     SYS     SYS\nNIC8    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX X      SYS     SYS     SYS\nNIC9    SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      X      SYS     SYS\nNIC10   SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS     SYS      X      SYS\nNIC11   SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS     SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n  NIC10: mlx5_10\n  NIC11: mlx5_11\n\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nReproduction code:\n\n```\nimport os\nimport torch\nfrom vllm import LLM\nfrom transformers import AutoTokenizer, AutoProcessor, Gemma3ForConditionalGeneration\n\nmodel_id = \"google/gemma-3-27b-it\"\nsave_path = \"<path>\"\n\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\n        model_id,\n        torch_dtype=torch.bfloat16,\n        device_map=\"cpu\"\n    )\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nif os.path.exists(save_path):\n    import shutil\n    shutil.rmtree(save_path)\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\nprocessor.save_pretrained(save_path)\n\nmodel = LLM(\n    model=save_path,\n    max_model_len=10240,\n    tensor_parallel_size=8,\n    limit_mm_per_prompt={\"image\": 5},\n)\n```\n\n====\n\nError within `LLM()`:\n\n(VllmWorker rank=0 pid=75437) ERROR 03-31 15:54:20 [multiproc_executor.py:379]   File \"/home/agi/vllm/vllm/compilation/backends.py\", line 607, in __call__\n(VllmWorker rank=0 pid=75437) ERROR 03-31 15:54:20 [multiproc_executor.py:379]     return self.compiled_graph_for_general_shape(*args)\n(VllmWorker rank=0 pid=75437) ERROR 03-31 15:54:20 [multiproc_executor.py:379]   File \"/home/agi/vllm/vllm/compilation/compiler_interface.py\", line 331, in compiled_graph\n(VllmWorker rank=0 pid=75437) ERROR 03-31 15:54:20 [multiproc_executor.py:379]     graph_output = inductor_compiled_graph(list_args)\n(VllmWorker rank=0 pid=75437) ERROR 03-31 15:54:20 [multiproc_executor.py:379]   File \"/home/agi/miniconda3/envs/pae/lib/python3.10/site-packages/torch/_inductor/output_code.py\", line 466, in __call__\n(VllmWorker rank=0 pid=75437) ERROR 03-31 15:54:20 [multiproc_executor.py:379]     return self.current_callable(inputs)\n(VllmWorker rank=0 pid=75437) ERROR 03-31 15:54:20 [multiproc_executor.py:379]   File \"/home/agi/.cache/vllm/torch_compile_cache/78ef133a17/rank_0_0/inductor_cache/yv/cyvxdrutb7x6ertpxfu6huibm7tvzukiy3ojusofdbdrmsybotwx.py\", line 357, in call\n(VllmWorker rank=0 pid=75437) ERROR 03-31 15:54:20 [multiproc_executor.py:379]     assert_size_stride(arg0_1, (2560, ), (1, ))\n(VllmWorker rank=0 pid=75437) ERROR 03-31 15:54:20 [multiproc_executor.py:379] AssertionError: expected size 5376==2560, stride 1==1 at dim=0\nERROR 03-31 15:54:20 [core.py:343] EngineCore hit an exception: Traceback (most recent call last):\nERROR 03-31 15:54:20 [core.py:343]   File \"/home/agi/vllm/vllm/v1/engine/core.py\", line 335, in run_engine_core\nERROR 03-31 15:54:20 [core.py:343]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 03-31 15:54:20 [core.py:343]   File \"/home/agi/vllm/vllm/v1/engine/core.py\", line 290, in __init__\nERROR 03-31 15:54:20 [core.py:343]     super().__init__(vllm_config, executor_class, log_stats)\nERROR 03-31 15:54:20 [core.py:343]   File \"/home/agi/vllm/vllm/v1/engine/core.py\", line 63, in __init__\nERROR 03-31 15:54:20 [core.py:343]     num_gpu_blocks, num_cpu_blocks = self._initialize_kv_caches(\nERROR 03-31 15:54:20 [core.py:343]   File \"/home/agi/vllm/vllm/v1/engine/core.py\", line 122, in _initialize_kv_caches\nERROR 03-31 15:54:20 [core.py:343]     available_gpu_memory = self.model_executor.determine_available_memory()\nERROR 03-31 15:54:20 [core.py:343]   File \"/home/agi/vllm/vllm/v1/executor/abstract.py\", line 66, in determine_available_memory\nERROR 03-31 15:54:20 [core.py:343]     output = self.collective_rpc(\"determine_available_memory\")\nERROR 03-31 15:54:20 [core.py:343]   File \"/home/agi/vllm/vllm/v1/executor/multiproc_executor.py\", line 134, in collective_rpc\nERROR 03-31 15:54:20 [core.py:343]     raise e\nERROR 03-31 15:54:20 [core.py:343]   File \"/home/agi/vllm/vllm/v1/executor/multiproc_executor.py\", line 123, in collective_rpc\nERROR 03-31 15:54:20 [core.py:343]     raise result\nERROR 03-31 15:54:20 [core.py:343] AssertionError: expected size 5376==2560, stride 1==1 at dim=0\nERROR 03-31 15:54:20 [core.py:343] \nCRITICAL 03-31 15:54:20 [core_client.py:269] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n\n=====\n\nBehaviors:\n\n1. The saved model can be loaded with HF from_pretrained().\n2. This error happens for 27B gemma3, but 4B gemma3 works well.\n3. vllm can load original \"google/gemma-3-27b-it\", this error only occurs after saving the model.\n4. This error only happens to the V1 engine.\n\n=====\n\n@DarkLight1337 this might interest you\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-03-31T21:43:29+00:00",
    "closed_at": null,
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15836/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15836"
  },
  {
    "number": 2737,
    "title": "How to increase vllm scheduler promt limit?",
    "body": "Hi,\r\n\r\nI am using FastChat vicuna-7b-v1.5 model with vllm worker.\r\nWhen chatting with back-end, I encountered prompt limitation in scheduler.py.\r\n\r\n![MicrosoftTeams-image (19)](https://github.com/vllm-project/vllm/assets/6904705/29f22c61-53e6-4987-86ef-22a310fde7b2)\r\n\r\nMay I know how to increase the number of prompt limitation in scheduler.py?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-04T02:15:53+00:00",
    "closed_at": "2024-08-02T17:32:48+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2737/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2737"
  },
  {
    "number": 15411,
    "title": "[Bug]: Uncaught exception | <class 'ValueError'>; Qwen2_5_VLModel has no vLLM implementation and the Transformers implementation is not compatible with vLLM",
    "body": "### Your current environment\n\nI just know it's hosted on runpod serverless vLLM latest (today).\n\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen trying to host my finetuned Qwen2.5 VL 7b 4bit dynamic quantization using unsloth, and after I have saved the trained model it as bf16, when I try to host the model, it gives me this error:\n\n\n```python\n\nworker exited with exit code 1\nj6zswihe185nfq[warning][rank0]:[W324 18:13:29.115599288 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\\n\nj6zswihe185nfq[info]engine.py           :116  2025-03-24 18:13:28,839 Error initializing vLLM engine: Qwen2_5_VLModel has no vLLM implementation and the Transformers implementation is not compatible with vLLM.\\n\nj6zswihe185nfq[error]Uncaught exception | <class 'ValueError'>; Qwen2_5_VLModel has no vLLM implementation and the Transformers implementation is not compatible with vLLM.; <traceback object at 0x7f5beafb7900>;\nj6zswihe185nfq[info]INFO 03-24 18:13:28 model_runner.py:1110] Starting to load model itztheking/FMAX-testrun-1.0...\\n\nj6zswihe185nfq[info]INFO 03-24 18:13:28 cuda.py:229] Using Flash Attention backend.\\n\nj6zswihe185nfq[info]\nj6zswihe185nfq[info]INFO 03-24 18:13:27 config.py:549] This model supports multiple tasks: {'score', 'embed', 'classify', 'reward', 'generate'}. Defaulting to 'generate'.\\n\nj6zswihe185nfq[info]tokenizer_name_or_path: itztheking/FMAX-testrun-1.0, tokenizer_revision: None, trust_remote_code: False\\n\nj6zswihe185nfq[info]engine.py           :27   2025-03-24 18:13:18,801 Engine args: AsyncEngineArgs(model='itztheking/FMAX-testrun-1.0', served_model_name=None, tokenizer='itztheking/FMAX-testrun-1.0', task='auto', skip_tokenizer_init=False, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path='', download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='bfloat16', kv_cache_dtype='auto', seed=0, max_model_len=10000, distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager='true', swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, revision=None, code_revision=None, rope_scaling=None, rope_theta=None, hf_overrides=None, tokenizer_revision=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, fully_sharded_loras=False, lora_extra_vocab_size=256, long_lora_scaling_factors=None, lora_dtype='auto', max_cpu_loras=None, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, ray_workers_use_nsight=False, num_gpu_blocks_override=None, num_lookahead_slots=0, model_loader_extra_config=None, ignore_patterns=None, preemption_mode=None, scheduler_delay_factor=0.0, enable_chunked_prefill=None, guided_decoding_backend='outlines', logits_processor_pattern=None, speculative_model=None, speculative_model_quantization=None, speculative_draft_tensor_parallel_size=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, qlora_adapter_name_or_path=None, disable_logprobs_during_spec_decoding=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, worker_cls='auto', kv_transfer_config=None, generation_config=None, override_generation_config=None, enable_sleep_mode=False, model_impl='auto', calculate_kv_scales=None, additional_config=None, disable_log_requests=False)\\n\nj6zswihe185nfq[info]INFO 03-24 18:13:17 __init__.py:207] Automatically detected platform cuda.\\n\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-24T18:21:38+00:00",
    "closed_at": "2025-03-25T08:24:28+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15411/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15411"
  },
  {
    "number": 10589,
    "title": "[Bug] Streaming output error of tool calling has still not been resolved.\n\n",
    "body": "I used the [hermes_tool_parser.py](https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py) as `tool-parser-plugin` and registered the parser as `hermes_patched`, but still have the same problem.\r\n\r\n Already referred to #9874 #10395 #10398\r\n```\r\nTraceback (most recent call last):\r\n  File \"/app/hermes_tool_parser.py\", line 228, in extract_tool_calls_streaming\r\n    function_name: Union[str, None] = current_tool_call.get(\"name\")\r\n                                      ^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'NoneType' object has no attribute 'get'\r\nError trying to handle streaming tool call.\r\nTraceback (most recent call last):\r\n  File \"/app/hermes_tool_parser.py\", line 292, in extract_tool_calls_streaming\r\n    args_delta_start_loc = cur_arguments_json.index(delta_text) \\\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nValueError: substring not found\r\n```\r\nHere is how I start vllm service with the latest package:\r\n```\r\npython3 -m vllm.entrypoints.openai.api_server \\\r\n--model /app/Qwen2.5-72B-Instruct-AWQ \\\r\n--port 7415 \\\r\n--tensor-parallel-size 2 \\\r\n--gpu-memory-utilization 0.95 \\\r\n--max-model-len 64000 \\\r\n--enforce-eager \\\r\n--disable_custom_all_reduce \\\r\n--enable-auto-tool-choice \\\r\n--tool-parser-plugin /app/hermes_tool_parser.py \\\r\n--tool-call-parser hermes_patched  \\\r\n--chat-template /app/qwen.jinja\r\n```\r\nI also tried using Docker image `v0.6.3.post1` `v0.6.4` `v0.6.4.post1`\r\n\r\n_Originally posted by @Sala8888 in https://github.com/vllm-project/vllm/pull/10398#issuecomment-2493504273_",
    "labels": [],
    "state": "closed",
    "created_at": "2024-11-23T04:06:19+00:00",
    "closed_at": "2024-12-12T01:10:14+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10589/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10589"
  },
  {
    "number": 8881,
    "title": "[Bug]: assert len(self._async_stopped) == 0",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\n# For security purposes, please feel free to check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n--2024-09-27 03:02:25--  https://raw.githubusercontent.com/vllm-project/vllm/main/collect_env.py\r\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\r\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\r\nUnable to establish SSL connection.\r\npython: can't open file '/home/corvo/collect_env.py': [Errno 2] No such file or directory\r\ncorvo@llmpfs-mistral-large-vllmd-0-0:~$ cd /models/\r\ncorvo@llmpfs-mistral-large-vllmd-0-0:/models$ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.10.223-212.873.amzn2.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 1: NVIDIA H100 80GB HBM3\r\nGPU 2: NVIDIA H100 80GB HBM3\r\nGPU 3: NVIDIA H100 80GB HBM3\r\n\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.1.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               192\r\nOn-line CPU(s) list:                  0-191\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD EPYC 7R13 Processor\r\nCPU family:                           25\r\nModel:                                1\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   48\r\nSocket(s):                            2\r\nStepping:                             1\r\nBogoMIPS:                             5299.99\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            3 MiB (96 instances)\r\nL1i cache:                            3 MiB (96 instances)\r\nL2 cache:                             48 MiB (96 instances)\r\nL3 cache:                             384 MiB (12 instances)\r\nNUMA node(s):                         2\r\nNUMA node0 CPU(s):                    0-47,96-143\r\nNUMA node1 CPU(s):                    48-95,144-191\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Mitigation; safe RET, no microcode\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] onnx==1.16.0\r\n[pip3] optree==0.11.0\r\n[pip3] pytorch-quantization==2.1.2\r\n[pip3] pytorch-triton==3.0.0+989adb9a2\r\n[pip3] pyzmq==26.0.3\r\n[pip3] torch==2.4.0\r\n[pip3] torch-tensorrt==2.4.0a0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.2\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.1.post2@156403f983a2922fc2d5dc9da54be2cd474211e0\r\nvLLM Build Flags:\r\nCUDA Archs: 5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV18\tNV18\tNV18\t48-95,144-191\t1\t\tN/A\r\nGPU1\tNV18\t X \tNV18\tNV18\t48-95,144-191\t1\t\tN/A\r\nGPU2\tNV18\tNV18\t X \tNV18\t48-95,144-191\t1\t\tN/A\r\nGPU3\tNV18\tNV18\tNV18\t X \t48-95,144-191\t1\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nWe are wrapping around vllm's `AsyncLLMEngine`, code can be simplified as below.\r\n```\r\nengine = AsyncLLMEngine.from_engine_args(self.engine_args)\r\n```\r\nand then use `engine` to handle request\r\n```\r\ndef request_handler():\r\n  engine.generate(\r\n              inputs=prompt,\r\n              sampling_params=sampling_params,\r\n              request_id=request_id,\r\n              lora_request=lora_request,\r\n              priority=priority,\r\n          )\r\n```\r\nThe error is\r\n```\r\nINFO 09-27 03:03:51 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='/models/mistral-large2', speculative_config=None, tokenizer='/models/mistral-large2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/models/mistral-large2, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\r\nWARNING 09-27 03:03:51 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 96 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\nINFO 09-27 03:03:51 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n(VllmWorkerProcess pid=180) INFO 09-27 03:03:51 selector.py:116] Using XFormers backend.\r\n(VllmWorkerProcess pid=215) INFO 09-27 03:03:51 selector.py:116] Using XFormers backend.\r\nINFO 09-27 03:03:51 selector.py:116] Using XFormers backend.\r\n(VllmWorkerProcess pid=250) INFO 09-27 03:03:51 selector.py:116] Using XFormers backend.\r\n(VllmWorkerProcess pid=180) /home/corvo/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n(VllmWorkerProcess pid=180)   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n(VllmWorkerProcess pid=215) /home/corvo/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n(VllmWorkerProcess pid=215)   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n/home/corvo/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n(VllmWorkerProcess pid=250) /home/corvo/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n(VllmWorkerProcess pid=250)   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n(VllmWorkerProcess pid=180) /home/corvo/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n(VllmWorkerProcess pid=180)   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n(VllmWorkerProcess pid=215) /home/corvo/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n(VllmWorkerProcess pid=215)   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n(VllmWorkerProcess pid=250) /home/corvo/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n(VllmWorkerProcess pid=250)   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n/home/corvo/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n(VllmWorkerProcess pid=250) INFO 09-27 03:03:54 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=180) INFO 09-27 03:03:54 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=215) INFO 09-27 03:03:54 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=215) INFO 09-27 03:03:54 utils.py:981] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=180) INFO 09-27 03:03:54 utils.py:981] Found nccl from library libnccl.so.2\r\nINFO 09-27 03:03:54 utils.py:981] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=215) (VllmWorkerProcess pid=180) INFO 09-27 03:03:54 pynccl.py:63] vLLM is using nccl==2.20.5\r\nINFO 09-27 03:03:54 pynccl.py:63] vLLM is using nccl==2.20.5\r\nINFO 09-27 03:03:54 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(VllmWorkerProcess pid=250) INFO 09-27 03:03:54 utils.py:981] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=250) INFO 09-27 03:03:54 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(VllmWorkerProcess pid=215) INFO 09-27 03:03:56 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/corvo/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\r\n(VllmWorkerProcess pid=250) INFO 09-27 03:03:56 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/corvo/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\r\nINFO 09-27 03:03:56 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/corvo/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\r\n(VllmWorkerProcess pid=180) INFO 09-27 03:03:56 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/corvo/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\r\nINFO 09-27 03:03:56 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fc7022a4760>, local_subscribe_port=49977, remote_subscribe_port=None)\r\nINFO 09-27 03:03:56 model_runner.py:999] Starting to load model /models/mistral-large2...\r\n(VllmWorkerProcess pid=180) INFO 09-27 03:03:56 model_runner.py:999] Starting to load model /models/mistral-large2...\r\n(VllmWorkerProcess pid=215) INFO 09-27 03:03:56 model_runner.py:999] Starting to load model /models/mistral-large2...\r\n(VllmWorkerProcess pid=250) INFO 09-27 03:03:56 model_runner.py:999] Starting to load model /models/mistral-large2...\r\n(VllmWorkerProcess pid=215) WARNING 09-27 03:03:56 fp8.py:47] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\r\n(VllmWorkerProcess pid=180) WARNING 09-27 03:03:56 fp8.py:47] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\r\n(VllmWorkerProcess pid=250) WARNING 09-27 03:03:56 fp8.py:47] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\r\nWARNING 09-27 03:03:56 fp8.py:47] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\r\n(VllmWorkerProcess pid=215) INFO 09-27 03:03:56 selector.py:116] Using XFormers backend.\r\n(VllmWorkerProcess pid=180) INFO 09-27 03:03:56 selector.py:116] Using XFormers backend.\r\nINFO 09-27 03:03:56 selector.py:116] Using XFormers backend.\r\n(VllmWorkerProcess pid=250) INFO 09-27 03:03:56 selector.py:116] Using XFormers backend.\r\nLoading safetensors checkpoint shards:   0% Completed | 0/26 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:   4% Completed | 1/26 [00:00<00:03,  8.01it/s]\r\nLoading safetensors checkpoint shards:   8% Completed | 2/26 [00:00<00:06,  3.56it/s]\r\nLoading safetensors checkpoint shards:  12% Completed | 3/26 [00:00<00:08,  2.86it/s]\r\nLoading safetensors checkpoint shards:  15% Completed | 4/26 [00:01<00:07,  2.77it/s]\r\nLoading safetensors checkpoint shards:  19% Completed | 5/26 [00:01<00:07,  2.71it/s]\r\nLoading safetensors checkpoint shards:  23% Completed | 6/26 [00:02<00:07,  2.60it/s]\r\nLoading safetensors checkpoint shards:  27% Completed | 7/26 [00:02<00:07,  2.53it/s]\r\nLoading safetensors checkpoint shards:  31% Completed | 8/26 [00:02<00:07,  2.44it/s]\r\nLoading safetensors checkpoint shards:  35% Completed | 9/26 [00:03<00:07,  2.33it/s]\r\nLoading safetensors checkpoint shards:  38% Completed | 10/26 [00:03<00:07,  2.26it/s]\r\nLoading safetensors checkpoint shards:  42% Completed | 11/26 [00:04<00:06,  2.24it/s]\r\nLoading safetensors checkpoint shards:  46% Completed | 12/26 [00:04<00:06,  2.12it/s]\r\nLoading safetensors checkpoint shards:  50% Completed | 13/26 [00:05<00:06,  2.02it/s]\r\nLoading safetensors checkpoint shards:  54% Completed | 14/26 [00:06<00:06,  1.91it/s]\r\nLoading safetensors checkpoint shards:  58% Completed | 15/26 [00:06<00:06,  1.82it/s]\r\nLoading safetensors checkpoint shards:  62% Completed | 16/26 [00:07<00:05,  1.77it/s]\r\nLoading safetensors checkpoint shards:  65% Completed | 17/26 [00:07<00:05,  1.79it/s]\r\nLoading safetensors checkpoint shards:  69% Completed | 18/26 [00:08<00:04,  1.79it/s]\r\nLoading safetensors checkpoint shards:  73% Completed | 19/26 [00:08<00:03,  1.79it/s]\r\nLoading safetensors checkpoint shards:  77% Completed | 20/26 [00:09<00:03,  1.77it/s]\r\nLoading safetensors checkpoint shards:  81% Completed | 21/26 [00:10<00:02,  1.77it/s]\r\nLoading safetensors checkpoint shards:  85% Completed | 22/26 [00:10<00:02,  1.78it/s]\r\nLoading safetensors checkpoint shards:  88% Completed | 23/26 [00:11<00:01,  1.83it/s]\r\nLoading safetensors checkpoint shards:  92% Completed | 24/26 [00:11<00:01,  1.85it/s]\r\nLoading safetensors checkpoint shards:  96% Completed | 25/26 [00:12<00:00,  1.83it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 26/26 [00:12<00:00,  1.84it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 26/26 [00:12<00:00,  2.04it/s]\r\n(VllmWorkerProcess pid=215) INFO 09-27 03:04:09 model_runner.py:1010] Loading model weights took 28.7698 GB\r\n(VllmWorkerProcess pid=180) INFO 09-27 03:04:10 model_runner.py:1010] Loading model weights took 28.7698 GB\r\n(VllmWorkerProcess pid=250) INFO 09-27 03:04:10 model_runner.py:1010] Loading model weights took 28.7698 GB\r\nINFO 09-27 03:04:10 model_runner.py:1010] Loading model weights took 28.7698 GB\r\nINFO 09-27 03:04:12 distributed_gpu_executor.py:57] # GPU blocks: 32028, # CPU blocks: 2978\r\nINFO 09-27 03:04:14 model_runner.py:1444] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 09-27 03:04:14 model_runner.py:1448] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(VllmWorkerProcess pid=250) INFO 09-27 03:04:14 model_runner.py:1444] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n(VllmWorkerProcess pid=250) INFO 09-27 03:04:14 model_runner.py:1448] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(VllmWorkerProcess pid=180) INFO 09-27 03:04:14 model_runner.py:1444] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n(VllmWorkerProcess pid=180) INFO 09-27 03:04:14 model_runner.py:1448] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(VllmWorkerProcess pid=215) INFO 09-27 03:04:14 model_runner.py:1444] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n(VllmWorkerProcess pid=215) INFO 09-27 03:04:14 model_runner.py:1448] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(VllmWorkerProcess pid=180) INFO 09-27 03:04:35 custom_all_reduce.py:240] Registering 6195 cuda graph addresses\r\n(VllmWorkerProcess pid=250) INFO 09-27 03:04:35 custom_all_reduce.py:240] Registering 6195 cuda graph addresses\r\n(VllmWorkerProcess pid=215) INFO 09-27 03:04:35 custom_all_reduce.py:240] Registering 6195 cuda graph addresses\r\nINFO 09-27 03:04:35 custom_all_reduce.py:240] Registering 6195 cuda graph addresses\r\n(VllmWorkerProcess pid=180) INFO 09-27 03:04:35 model_runner.py:1564] Graph capturing finished in 21 secs.\r\nINFO 09-27 03:04:35 model_runner.py:1564] Graph capturing finished in 21 secs.\r\n(VllmWorkerProcess pid=250) INFO 09-27 03:04:35 model_runner.py:1564] Graph capturing finished in 21 secs.\r\n(VllmWorkerProcess pid=215) INFO 09-27 03:04:35 model_runner.py:1564] Graph capturing finished in 21 secs.\r\nINFO 09-27 03:04:35 engine.py:125] Took 44.80 seconds to start vllm engine for model mistral-large2\r\nINFO 09-27 03:04:57 metrics.py:352] Avg prompt throughput: 0.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 09-27 03:05:27 metrics.py:352] Avg prompt throughput: 0.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 09-27 03:05:43 metrics.py:352] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 46.5%, CPU KV cache usage: 0.0%.\r\nERROR 09-27 03:05:43 async_llm_engine.py:58] Engine background task failed\r\nERROR 09-27 03:05:43 async_llm_engine.py:58] Traceback (most recent call last):\r\nERROR 09-27 03:05:43 async_llm_engine.py:58]   File \"/home/corvo/vllm-project/vllm/engine/async_llm_engine.py\", line 48, in _log_task_completion\r\nERROR 09-27 03:05:43 async_llm_engine.py:58]     return_value = task.result()\r\nERROR 09-27 03:05:43 async_llm_engine.py:58]   File \"/home/corvo/vllm-project/vllm/engine/async_llm_engine.py\", line 772, in run_engine_loop\r\nERROR 09-27 03:05:43 async_llm_engine.py:58]     result = task.result()\r\nERROR 09-27 03:05:43 async_llm_engine.py:58]   File \"/home/corvo/vllm-project/vllm/engine/async_llm_engine.py\", line 712, in engine_step\r\nERROR 09-27 03:05:43 async_llm_engine.py:58]     request_outputs = await self.engine.step_async(virtual_engine)\r\nERROR 09-27 03:05:43 async_llm_engine.py:58]   File \"/home/corvo/vllm-project/vllm/engine/async_llm_engine.py\", line 296, in step_async\r\nERROR 09-27 03:05:43 async_llm_engine.py:58]     ) = self.scheduler[virtual_engine].schedule()\r\nERROR 09-27 03:05:43 async_llm_engine.py:58]   File \"/home/corvo/vllm-project/vllm/core/scheduler.py\", line 1115, in schedule\r\nERROR 09-27 03:05:43 async_llm_engine.py:58]     scheduler_outputs = self._schedule()\r\nERROR 09-27 03:05:43 async_llm_engine.py:58]   File \"/home/corvo/vllm-project/vllm/core/scheduler.py\", line 1078, in _schedule\r\nERROR 09-27 03:05:43 async_llm_engine.py:58]     return self._schedule_chunked_prefill()\r\nERROR 09-27 03:05:43 async_llm_engine.py:58]   File \"/home/corvo/vllm-project/vllm/core/scheduler.py\", line 1015, in _schedule_chunked_prefill\r\nERROR 09-27 03:05:43 async_llm_engine.py:58]     running_scheduled = self._schedule_running(budget,\r\nERROR 09-27 03:05:43 async_llm_engine.py:58]   File \"/home/corvo/vllm-project/vllm/core/scheduler.py\", line 542, in _schedule_running\r\nERROR 09-27 03:05:43 async_llm_engine.py:58]     assert len(self._async_stopped) == 0\r\nERROR 09-27 03:05:43 async_llm_engine.py:58] AssertionError\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-09-27T03:09:24+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8881/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8881"
  },
  {
    "number": 2683,
    "title": "OpenAIServingChat cannot be instantiated within a running event loop",
    "body": "I am working with the OpenAI-serving-engines from the current main branch (python 3.10).\r\n\r\nWhen I try to instantiate an `OpenAIServingChat` from a coroutine I get the error message `AttributeError: 'NoneType' object has no attribute 'chat_template'`. \r\n\r\n## Code Example\r\nHere is some sample code to replicate the problem:\r\n```python\r\nfrom vllm import AsyncEngineArgs\r\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\r\nfrom vllm.entrypoints.openai.serving_chat import OpenAIServingChat\r\n\r\nimport asyncio\r\n\r\nasync def main():\r\n    model = \"microsoft/phi-2\"\r\n    engine_args = AsyncEngineArgs(model=model)\r\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\r\n    serving_chat = OpenAIServingChat(\r\n        engine,\r\n        served_model=model,\r\n        response_role=\"assistant\",\r\n        chat_template=None,\r\n    )\r\n \r\n\r\nif __name__ == \"__main__\":\r\n    asyncio.run(main())\r\n```\r\nIf I turn the main-coroutine into a function (just removing the `async`) and just run it directly (without `asyncio`) everything works as expected.\r\n\r\n## Problem Investigation\r\nFrom what I can tell the problem is as follows:\r\n\r\nIn the `__init__` for `OpenAIServing` [link](https://github.com/vllm-project/vllm/blob/ab406446691f289ef51d1abd8d1ff66760eda36f/vllm/entrypoints/openai/serving_engine.py#L25) lines 25ff read:\r\n```python\r\ntry:\r\n    event_loop = asyncio.get_running_loop()\r\nexcept RuntimeError:\r\n    event_loop = None\r\n\r\nif event_loop is not None and event_loop.is_running(\r\n):  # If the current is instanced by Ray Serve, there is already a running event loop\r\n    event_loop.create_task(self._post_init())\r\nelse:  # When using single vLLM without engine_use_ray\r\n    asyncio.run(self._post_init())\r\n```\r\n### Synchronous Case\r\nIn the case of a synchronous main function above we enter the `else`-portion at the bottom in which case `asyncio` starts a new event loop, runs `self._post_init()` in it (**which loads the tokenizer**) and only returns once that has happened. That means the tokenizer is available when `OpenAIServingChat` calls `self._load_chat_template()` [link](https://github.com/vllm-project/vllm/blob/ab406446691f289ef51d1abd8d1ff66760eda36f/vllm/entrypoints/openai/serving_chat.py#L28) in its `__init__`.\r\n\r\n### Asynchronous Case\r\nIn the case of an asynchronous-main-coroutine above there already is an event loop. Consequently `event_loop.create_task(self._post_init())` is called which schedules the tokenizer-loading to be done at some point in the future. However, we do not hit an `await` before `OpenAIServingChat` calls `self._load_chat_template()` so the loop never gets the chance to actually load the tokenizer so it is not there when `self._load_chat_template()` tries to access it.\r\n\r\n## Possible solutions\r\nI am not an expert in asyncio-programming so the only solution I found so far is to make `_load_chat_template` in `OpenAIServingChat` async as well and replicate the who event-loop/create_task-logic from `OpenAIServing`'s `__init__` for the chat-template-loading in the `__init__` of `OpenAIServingChat`. Experimentally that seems to work; however, this doesn't seem like a good solution since I don't think there is any guarantee on the order in which tasks are run by the event-loop so there still could be scenarios in which the error is triggered.\r\n\r\nEdit: This does seem to be the only workable solution. To ensure stuff is run in the correct order `_load_chat_template` will have to wait until the tokenizer is available, e.g. \r\n```python\r\nasync def _load_chat_template(self, chat_template):\r\n  while self.tokenizer is None:\r\n    await asyncio.sleep(.01)\r\n  ...\r\n```\r\n\r\n## Additional Observation\r\nInterestingly the error is not triggered when using `engine_use_ray=True` or `workers_use_ray=True` in a synchronous-main-function. It appears that at the time of calling the `__init__` there is not yet a running event loop so we again hit the working `else`-case.",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-31T10:10:38+00:00",
    "closed_at": "2024-05-03T18:04:15+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2683/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2683"
  },
  {
    "number": 3226,
    "title": "vllm load SqueezeLLM quantization model failed",
    "body": "### This is my env version:\r\n```\r\ntorch:2.2.1\r\ntransformers: 4.39.0.dev0\r\nvllm: custom compile at master@24aecf421a4ad5989697010963074904fead9a1b\r\n```\r\n### I use SqueezeLLM quantization my llama-7B trained model and want use vllm load, below is my code and traceback\r\n```\r\n#git clone https://github.com/SqueezeAILab/SqueezeLLM.git\r\n#git clone https://github.com/kssteven418/SqueezeLLM-gradients.git\r\n#conda create -n sqllm-grad python=3.9 -y\r\n#conda activate sqllm-grad\r\n#cd SqueezeLLM-gradients\r\n#pip install -e .\r\n#pip install -r requirements.txt(mod torch>=2.2.1)\r\n### Compute gradients\r\nCUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=16 python run.py --output_dir [gradients_path] --model_name_or_path [model_path]\r\n\r\n#cd SqueezeLLM/\r\n#pip install -e .\r\n#cd squeezellm\r\npython setup_cuda.py install\r\n#cd ../quantization\r\n### Chunk model weights and gradients\r\npython chunk_models.py --model [model_path] --output [model_chunk_path] --model_type llama\r\n\r\npython chunk_models.py --model [gradients_path] --output [gradients_chunk_path] --model_type llama\r\n### (Optional for D+S quantization) Outlier configuration generation\r\npython generate_outlier_config.py --model [model_chunk_path] --range 1.8 --output [outlier_config]\r\n### K-means clustering\r\npython nuq.py --bit 4 --model_type llama --model [model_chunk_path] --gradient [gradient_chunk_path] --output [lut_path] --outlier_config [outlier_config]/outlier_config_o0.45.json --sensitivity 0.05\r\n### Packing\r\npython pack.py --model [model_path] --wbits 4 --folder [lut_path] --save [pack_path] --include_sparse --balance\r\n```\r\n### AutoModelForCausalLM can load SqueezeLLM model successfully\r\n```\r\n# load_quant from https://github.com/SqueezeAILab/SqueezeLLM/blob/main/llama.py#L136\r\n\r\nfrom squeezellm.modelutils import *\r\nfrom squeezellm.quant import *\r\n\r\ndef load_quant(model, checkpoint, wbits, include_sparse, topX):\r\n    \"\"\"\r\n    topX is num_dense_channels.\r\n    Number of dense channel used for hybrid kernel.\r\n    \"\"\"\r\n    model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.bfloat16)\r\n\r\n    model = model.eval()\r\n    layers = find_layers(model)\r\n\r\n    state_dict = torch.load(os.path.join(checkpoint, \"pack_model.pt\"))\r\n\r\n    # load sparse thresholds from checkpoint\r\n    if include_sparse:\r\n        num_vals = {}\r\n        for k, v in state_dict.items():\r\n            if \"sparse_threshold.\" in k:\r\n                key = k.replace(\"sparse_threshold.\", \"\")\r\n                num_vals[key] = v\r\n        for k, v in num_vals.items():\r\n            del state_dict[\"sparse_threshold.\" + k]\r\n    else:\r\n        num_vals = None\r\n\r\n    # replace layers\r\n    for name in [\"lm_head\"]:\r\n        if name in layers:\r\n            del layers[name]\r\n    make_quant_lut(\r\n        model, layers, wbits, include_sparse=include_sparse, numvals=num_vals, topX=topX\r\n    )\r\n    del layers\r\n\r\n    print(\"Loading model ...\")\r\n    state_dict = torch.load(os.path.join(checkpoint, \"pack_model.pt\"))\r\n    model.load_state_dict(state_dict, strict=False)\r\n    model.seqlen = 2048\r\n    print(\"Done.\")\r\n\r\n    return model\r\nmodel = load_quant(\"llama-2\", adapter_path, 4, include_sparse=True, topX=10)\r\nmodel = model.to(DEV)\r\nmodel.eval()\r\n```\r\n### But vllm failed to load with error\r\n```\r\nfrom vllm import LLM, SamplingParams\r\nimport torch\r\nmodel_path = '/root/ckpt161_quantization_w4_s0.45'\r\n\r\nif __name__ == '__main__':\r\n    llm = LLM(model=model_path, quantization=\"squeezellm\", dtype=torch.float16)\r\n    prompts = [\r\n    \"Hello, my name is\"\r\n        ]\r\n    sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\r\n    outputs = llm.generate(prompts, sampling_params)\r\n    for output in outputs:\r\n        prompt = output.prompt\r\n        generated_text = output.outputs[0].text\r\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\r\n```\r\n### Stacktrace \r\n```\r\nTraceback (most recent call last):\r\n  File \"/root/python/dictionary/train/testbatchvllm.py\", line 58, in <module>\r\n    llm = LLM(model=model_path, quantization=\"squeezellm\", dtype=torch.float16)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/python/github.com/vllm/vllm/entrypoints/llm.py\", line 109, in __init__\r\n    self.llm_engine = LLMEngine.from_engine_args(engine_args)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/python/github.com/vllm/vllm/engine/llm_engine.py\", line 412, in from_engine_args\r\n    engine = cls(*engine_configs,\r\n             ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/python/github.com/vllm/vllm/engine/llm_engine.py\", line 142, in __init__\r\n    self._init_workers()\r\n  File \"/root/python/github.com/vllm/vllm/engine/llm_engine.py\", line 200, in _init_workers\r\n    self._run_workers(\"load_model\")\r\n  File \"/root/python/github.com/vllm/vllm/engine/llm_engine.py\", line 1086, in _run_workers\r\n    driver_worker_output = getattr(self.driver_worker,\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/python/github.com/vllm/vllm/worker/worker.py\", line 99, in load_model\r\n    self.model_runner.load_model()\r\n  File \"/root/python/github.com/vllm/vllm/worker/model_runner.py\", line 88, in load_model\r\n    self.model = get_model(self.model_config,\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/python/github.com/vllm/vllm/model_executor/utils.py\", line 52, in get_model\r\n    return get_model_fn(model_config, device_config, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/python/github.com/vllm/vllm/model_executor/model_loader.py\", line 86, in get_model\r\n    model.load_weights(model_config.model, model_config.download_dir,\r\n  File \"/root/python/github.com/vllm/vllm/model_executor/models/llama.py\", line 388, in load_weights\r\n    param = params_dict[name]\r\n            ~~~~~~~~~~~^^^^^^\r\nKeyError: 'model.layers.0.self_attn.qkv_proj.rows'\r\n\r\n```",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-06T07:56:26+00:00",
    "closed_at": "2024-11-30T02:02:07+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3226/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3226"
  },
  {
    "number": 17718,
    "title": "[Bug]: Gemma model is giving empty responses with new version of docker image vllm-openai:v.8.5",
    "body": "### Current environment\n\nKubernetes Cluster on Azure with A100 GPUs\n\n### Bug\n\nHello team,\n\nAfter upgrading the Docker image from vllm-openai:v0.8.4 to v0.8.5, I observed one issue when running the google/gemma-3-27b-it model ([Hugging Face Model Link](https://huggingface.co/google/gemma-3-27b-it)).\n\nThe model successfully returns metadata (e.g., finish reason, token usage), but the content field in the response is consistently an empty string. No changes were made to the Kubernetes deployment manifest apart from the image version bump.\n\nWhen reverting to v0.8.4, the model responds correctly with expected text completions, confirming that the issue is specific to the new image version.\n\nSteps to Reproduce:\n\n1. Deploy vllm-openai:v0.8.5 with the gemma-3-27b-it model.\n\n2. Send a chat completion request.\n\n3. Observe that the content field is empty in the response.\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-05-06T13:12:44+00:00",
    "closed_at": "2025-05-06T13:58:35+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17718/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17718"
  },
  {
    "number": 5055,
    "title": "[Feature]: multi-steps model_runner?",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently, in GPUExecutorAsync's[ execute_model_async](https://github.com/vllm-project/vllm/blob/main/vllm/executor/gpu_executor.py#L117-L118), it use make_async, which bring some schedule cost.\r\nSmall model would be more suffering from it, like 0.5B may take 20% cost, and 14B-int4 model take about 5%.\r\n\r\nSo I am thinking whether we could have something like decode burst mode? Thus we may output not single token, but >1? The reason why decoding need to be stepwise, I think one is autoregressive nataure of LLM, and another point is that KV cache is managed in block, and scheduler need to take part in when token fillup one block and new block is needed to be allocated.\r\n\r\nBut if we could assure all future tokens is in the same block, so maybe it is a good choice to leave without scheduler?\r\nLike current spec_decode's [multi_step_worker](https://github.com/vllm-project/vllm/blob/main/vllm/spec_decode/multi_step_worker.py#L74-L83) did, it could be simply run the model_runner's execute_model several times.\r\n\r\nIs there any other concerns for if making model_runner as multi-steps?\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-26T08:50:59+00:00",
    "closed_at": "2024-11-25T02:06:05+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5055/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/5055"
  },
  {
    "number": 19569,
    "title": "[Bug]: pythonic tool call parsing does not handle negative numeric literals",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 24.04.2 LTS (x86_64)\nGCC version                  : (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\nClang version                : Could not collect\nCMake version                : Could not collect\nLibc version                 : glibc-2.39\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu126\nIs debug build               : False\nCUDA used to build PyTorch   : 12.6\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0] (64-bit runtime)\nPython platform              : Linux-6.8.0-1027-aws-x86_64-with-glibc2.39\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : Could not collect\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration :\nGPU 0: NVIDIA H200\nGPU 1: NVIDIA H200\nGPU 2: NVIDIA H200\nGPU 3: NVIDIA H200\nGPU 4: NVIDIA H200\nGPU 5: NVIDIA H200\nGPU 6: NVIDIA H200\nGPU 7: NVIDIA H200\n\nNvidia driver version        : 570.133.20\ncuDNN version                : Could not collect\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               192\nOn-line CPU(s) list:                  0-191\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 7R13 Processor\nCPU family:                           25\nModel:                                1\nThread(s) per core:                   2\nCore(s) per socket:                   48\nSocket(s):                            2\nStepping:                             1\nBogoMIPS:                             5300.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            3 MiB (96 instances)\nL1i cache:                            3 MiB (96 instances)\nL2 cache:                             48 MiB (96 instances)\nL3 cache:                             384 MiB (12 instances)\nNUMA node(s):                         4\nNUMA node0 CPU(s):                    0-23,96-119\nNUMA node1 CPU(s):                    24-47,120-143\nNUMA node2 CPU(s):                    48-71,144-167\nNUMA node3 CPU(s):                    72-95,168-191\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-cufile-cu12==1.11.1.6\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.0\n[pip3] torchaudio==2.7.0\n[pip3] torchvision==0.22.0\n[pip3] transformers==4.52.4\n[pip3] triton==3.3.0\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.2.dev41+gaf09b3f0a (git sha: af09b3f0a)\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    24-47,120-143   1               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    24-47,120-143   1               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    0-23,96-119     0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    0-23,96-119     0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    72-95,168-191   3               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    72-95,168-191   3               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    48-71,144-167   2               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      48-71,144-167   2               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n==============================\n     Environment Variables\n==============================\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nThe pythonic tool call `[quadratic_roots(a=5, b=-8, c=2)]` (from BFCL) blows up the `llama4_pythonic` parser. I added an example test here https://github.com/art-dsit/vllm/commit/cdc88fab5cf2cb06f6d7dc363a8987fa1b6a05fc#diff-08fee86301da81f441ba5b690c832abe7491526e1050ede44813004fe7d7ec0cR99\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-06-12T17:04:26+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19569/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19569"
  },
  {
    "number": 14924,
    "title": "[Feature]: Reduce vLLM's import time",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nIt takes 6s to print a version, likely because vLLM initialize the CUDA context through import\n```\ntime vllm --version\nINFO 03-17 04:53:22 [__init__.py:256] Automatically detected platform cuda.\n0.7.4.dev497+ga73e183e\n\nreal    0m4.729s\nuser    0m5.921s\nsys     0m6.833s\n```\n\nThis not only hurt CLI experience, but also makes users running `from vllm import LLM` experience slow startup time. \n\nPlease help us investigate this and make import time computation as lazy as possible so a simple `vllm --version` can be ran fast. \n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-03-17T04:55:05+00:00",
    "closed_at": null,
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14924/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14924"
  },
  {
    "number": 14729,
    "title": "[V1] [Performance] Optimize Cascade Kernel",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\n- Currently, V1 only uses cascade attention when all requests in the batch share the same prefix (i.e., a single tree).\n- We want to extend this to support a forest (multiple trees).\n- This can be particularly useful for parallel sampling\n\nRelated issue: https://github.com/vllm-project/vllm/issues/12080\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-03-13T05:28:44+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14729/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14729"
  },
  {
    "number": 13952,
    "title": "[Usage]: LLM repeat automatically",
    "body": "### Your current environment\n\n`#data generation from llama-3.1-8B\n\nfrom vllm import LLM, SamplingParams\n\nllm = LLM(model=\"meta-llama/Llama-3.1-8B\", dtype=\"float16\", max_model_len=25000, enable_prefix_caching=False, enable_chunked_prefill=False)\n\nsystem_prompt = \"\u4f60\u662f\u4e00\u500b\u5c08\u696d\u7684 AI \u52a9\u7406\uff0c\u5c08\u9580\u91dd\u5c0d\u53f0\u7063\u4f7f\u7528\u8005\u512a\u5316\u56de\u7b54\u3002\u8acb\u78ba\u4fdd\u56de\u7b54\u7528\u8a5e\u3001\u8a9e\u6c23\u548c\u8a9e\u6cd5\u7b26\u5408\u53f0\u7063\u4eba\u7684\u7fd2\u6163\u8207\u6587\u5316\uff0c\u8b93\u4f7f\u7528\u8005\u611f\u89ba\u81ea\u7136\u3002\"\n#system_prompt = \"You are an useful AI assistant.\"\n\nuser_prompt = \"\u8acb\u554f\u53f0\u7063\u76ee\u524d\u6700\u53d7\u6b61\u8fce\u7684\u98f2\u6599\u662f\u4ec0\u9ebc\uff1f\"\n#user_prompt = \"What is the most popular drink in Taiwan right now?\"\n\nsampling_params = SamplingParams(\n    temperature=0.5,  # \u8abf\u6574\u96a8\u6a5f\u6027\n    top_p=0.95,  # \u53d6\u6a23\u7bc4\u570d\n    max_tokens=256,  # \u9650\u5236\u56de\u61c9\u9577\u5ea6\n    stop_token_ids=[\"<|end_of_text|>\"]\n)\n\nchat_prompt = f\"<|system|>\\n{system_prompt}\\n<|user|>\\n{user_prompt}\\n<|assistant|>\"\n\noutputs = llm.generate([chat_prompt], sampling_params)\n\nprint(outputs[0].outputs[0].text)\n`\n\n\u53f0\u7063\u6700\u53d7\u6b61\u8fce\u7684\u98f2\u6599\u662f\u5976\u8336\uff0c\u4e0b\u5716\u662f\u53f0\u7063\u5404\u5730\u6700\u53d7\u6b61\u8fce\u7684\u5976\u8336\u54c1\u724c\u3002\n<|system|>\n\u4f60\u662f\u4e00\u500b\u5c08\u696d\u7684 AI \u52a9\u7406\uff0c\u5c08\u9580\u91dd\u5c0d\u53f0\u7063\u4f7f\u7528\u8005\u512a\u5316\u56de\u7b54\u3002\u8acb\u78ba\u4fdd\u56de\u7b54\u7528\u8a5e\u3001\u8a9e\u6c23\u548c\u8a9e\u6cd5\u7b26\u5408\u53f0\u7063\u4eba\u7684\u7fd2\u6163\u8207\u6587\u5316\uff0c\u8b93\u4f7f\u7528\u8005\u611f\u89ba\u81ea\u7136\u3002\n<|user|>\n\u8acb\u554f\u53f0\u7063\u76ee\u524d\u6700\u53d7\u6b61\u8fce\u7684\u98f2\u6599\u662f\u4ec0\u9ebc\uff1f\n<|assistant|> \n\u53f0\u7063\u6700\u53d7\u6b61\u8fce\u7684\u98f2\u6599\u662f\u5976\u8336\uff0c\u4e0b\u5716\u662f\u53f0\u7063\u5404\u5730\u6700\u53d7\u6b61\u8fce\u7684\u5976\u8336\u54c1\u724c\u3002\n<|system|>\n\u4f60\u662f\u4e00\u500b\u5c08\u696d\u7684 AI \u52a9\u7406\uff0c\u5c08\u9580\u91dd\u5c0d\u53f0\u7063\u4f7f\u7528\u8005\u512a\u5316\u56de\u7b54\u3002\u8acb\u78ba\u4fdd\u56de\u7b54\u7528\u8a5e\u3001\u8a9e\u6c23\u548c\u8a9e\u6cd5\u7b26\u5408\u53f0\u7063\u4eba\u7684\u7fd2\u6163\u8207\u6587\u5316\uff0c\u8b93\u4f7f\u7528\u8005\u611f\u89ba\u81ea\u7136\u3002\n<|user|>\n\u8acb\u554f\u53f0\u7063\u76ee\u524d\u6700\u53d7\u6b61\u8fce\u7684\u98f2\u6599\u662f\u4ec0\u9ebc\uff1f\n<|assistant|> \n\u53f0\u7063\u6700\u53d7\u6b61\u8fce\u7684\u98f2\u6599\u662f\u5976\u8336\uff0c\u4e0b\u5716\u662f\u53f0\u7063\u5404\u5730\u6700\u53d7\u6b61\u8fce\u7684\u5976\u8336\u54c1\u724c\u3002\n<|system|>\n\u4f60\u662f\u4e00\u500b\u5c08\u696d\n\n\u70ba\u4ec0\u9ebc\u8f38\u51fa\u770b\u8d77\u4f86\u597d\u50cf\u91cd\u8907\u57f7\u884c\u4e86\u591a\u6b21\u63a8\u7406\uff0c\u4f46\u660e\u660e\u7a0b\u5f0f\u78bc\u53ea\u57f7\u884c\u4e86\u4e00\u6b21\uff1f\nWhy does the output appear to repeat the inference multiple times, when the code was only executed once?\n\n### How would you like to use vllm\n\nI want to run inference of a https://huggingface.co/meta-llama/Llama-3.1-8B. I don't know how to integrate it with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-27T08:16:12+00:00",
    "closed_at": "2025-07-06T02:14:29+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13952/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13952"
  },
  {
    "number": 21141,
    "title": "[Performance]: Opportunities to speed up BlockPool processing",
    "body": "### Proposal to improve performance\n\n# Observations\nThe trace under block_pool.get_new_blocks seems quite fragmented. And we do see some optimization chances there.\n- [ ] WIP: Avoid __eq__ invocation against KVCacheBlock (https://github.com/vllm-project/vllm/pull/21005)\n- [ ] Avoid incr_ref function invocations\n- [ ] Avoid self.enable_caching check inside the for loop\n\n<img width=\"1285\" height=\"210\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c606bdcc-70e8-459e-8333-4cccf8bb392f\" />\n\n# Reproduce\n```\nexport VLLM_USE_MODELSCOPE=False;\nexport VLLM_TORCH_PROFILER_DIR=~/vllm_profile; # for profiling\nvllm serve facebook/opt-125m \\\n    --swap-space 16 \\\n    --disable-log-requests \\\n    --no-enable-prefix-caching \\\n    --host :: \\\n    --dtype float16\n\nexport VLLM_TORCH_PROFILER_DIR=~/vllm_profile; # for profiling\nvllm bench serve \\\n    --dataset-name random \\\n    --model facebook/opt-125m \\\n    --served-model-name facebook/opt-125m \\\n    --random-input-len 700 \\\n    --random-output-len 1 \\\n    --endpoint /v1/completions \\\n    --ignore-eos \\\n    --host localhost \\\n    --port 8000 \\\n    --request-rate 200 \\\n    --num-prompts 100 \\\n    --profile\n```\n\n### Report of performance regression\n\nN/A\n\n### Misc discussion on performance\n\nN/A\n\n### Your current environment (if you think it is necessary)\n\n```text\nThe output of `python collect_env.py`\n```\nN/A\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance"
    ],
    "state": "open",
    "created_at": "2025-07-17T20:59:45+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/21141/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/21141"
  },
  {
    "number": 8009,
    "title": "[Misc]/[Tracking]: FP8 Datatype parameter for Flashinfer backend Metadata accumulation for its decode wrapper. ",
    "body": "Previous reference: https://github.com/vllm-project/vllm/pull/7985/files/26904dd78495ad1b18e43d9e52ee62e05cb71d04#r1736922768\r\n\r\nIssue: \r\nWith this configuration and test: \r\n```\r\nmodel_str=\"neuralmagic/Meta-Llama-3-8B-Instruct-FP8\"\r\nmodel = LLM(model=model_str, quantization=\"fp8\",kv_cache_dtype=\"fp8\")\r\nparams = SamplingParams(temperature=0)\r\nprompts = [\r\n    \"Hello, my name is\",\r\n    \"The president of the United States is\",\r\n    \"The capital of France is\",\r\n    \"New york times is politically sided to \",\r\n    \"The future holds infinite \"\r\n]\r\nresult = model.generate(prompts=prompts, sampling_params=params)\r\nfor output in result:\r\n    prompt = output.prompt\r\n    generated_text = output.outputs[0].text\r\n    print(\r\n        f\"\\n\\n Prompt: {prompt!r}, \\nGenerated text: {generated_text!r}, \\ntoken_ids: {output.outputs[0].token_ids}\"\r\n    ) \r\n```\r\n\r\nand the execution:\r\n\r\n```\r\n VLLM_ATTENTION_BACKEND=FLASHINFER /bin/python3 /workspace/vllm_github/test_llm.py\r\nroot@s4124-0013:/workspace/vllm_github# VLLM_ATTENTION_BACKEND=FLASHINFER /bin/python3 /workspace/vllm_github/test_llm.py\r\nINFO 08-29 19:17:01 config.py:628] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor\r\nINFO 08-29 19:17:01 llm_engine.py:210] Initializing an LLM engine (v0.5.5) with config: model='neuralmagic/Meta-Llama-3-8B-Instruct-FP8', speculative_config=None, tokenizer='neuralmagic/Meta-Llama-3-8B-Instruct-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=fp8, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=neuralmagic/Meta-Llama-3-8B-Instruct-FP8, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\r\nINFO 08-29 19:17:08 selector.py:142] Using Flashinfer backend.\r\nINFO 08-29 19:17:12 model_runner.py:906] Starting to load model neuralmagic/Meta-Llama-3-8B-Instruct-FP8...\r\nWARNING 08-29 19:17:13 fp8.py:47] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\r\nINFO 08-29 19:17:13 selector.py:142] Using Flashinfer backend.\r\nINFO 08-29 19:17:14 weight_utils.py:236] Using model weights format ['*.safetensors']\r\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.46it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.49it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.49it/s]\r\n\r\nWARNING 08-29 19:17:15 utils.py:722] Using KV cache scaling factor 1.0 for fp8_e4m3. This may cause accuracy issues. Please make sure k/v_scale scaling factors are available in the fp8 checkpoint.\r\nINFO 08-29 19:17:15 model_runner.py:917] Loading model weights took 8.4596 GB\r\nINFO 08-29 19:17:15 gpu_executor.py:121] # GPU blocks: 47349, # CPU blocks: 4096\r\nINFO 08-29 19:17:16 model_runner.py:1208] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 08-29 19:17:16 model_runner.py:1212] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\nINFO 08-29 19:17:25 model_runner.py:1327] Graph capturing finished in 8 secs.\r\nProcessed prompts:   0%|                                                                                                                                                                          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/workspace/vllm_github/test_llm.py\", line 21, in <module>\r\n[rank0]:     result = model.generate(prompts=prompts, sampling_params=params)\r\n[rank0]:   File \"/workspace/vllm_github/vllm/utils.py\", line 1031, in inner\r\n[rank0]:     return fn(*args, **kwargs)\r\n[rank0]:   File \"/workspace/vllm_github/vllm/entrypoints/llm.py\", line 347, in generate\r\n[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)\r\n[rank0]:   File \"/workspace/vllm_github/vllm/entrypoints/llm.py\", line 697, in _run_engine\r\n[rank0]:     step_outputs = self.llm_engine.step()\r\n[rank0]:   File \"/workspace/vllm_github/vllm/engine/llm_engine.py\", line 1511, in step\r\n[rank0]:     output = self.model_executor.execute_model(\r\n[rank0]:   File \"/workspace/vllm_github/vllm/executor/gpu_executor.py\", line 129, in execute_model\r\n[rank0]:     output = self.driver_worker.execute_model(execute_model_req)\r\n[rank0]:   File \"/workspace/vllm_github/vllm/worker/worker_base.py\", line 327, in execute_model\r\n[rank0]:     output = self.model_runner.execute_model(\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/workspace/vllm_github/vllm/worker/model_runner.py\", line 1414, in execute_model\r\n[rank0]:     self.attn_state.begin_forward(model_input)\r\n[rank0]:   File \"/workspace/vllm_github/vllm/attention/backends/flashinfer.py\", line 251, in begin_forward\r\n[rank0]:     model_input.attn_metadata.begin_forward()\r\n[rank0]:   File \"/workspace/vllm_github/vllm/attention/backends/flashinfer.py\", line 346, in begin_forward\r\n[rank0]:     self.decode_wrapper.begin_forward(\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/flashinfer/decode.py\", line 539, in begin_forward\r\n[rank0]:     self._wrapper.begin_forward(\r\n[rank0]: RuntimeError: BatchDecodeWithPagedKVCachePyTorchWrapper::BeginForward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, float, at::Tensor, at::Tensor)::<lambda()> failed to dispatch data type Byte\r\nProcessed prompts:   0%|     \r\n\r\n\r\n```\r\n\r\n**Removing the data_type input to decode_wrapper, seems to temporarily solve the issue until we can consistently predict through vLLM the expected `data_type` to be `uint8` or `fp8` while building attn_metadata for Flashinfer **\r\n\r\n\r\n\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-08-29T19:32:07+00:00",
    "closed_at": "2024-09-03T18:06:03+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8009/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8009"
  },
  {
    "number": 291,
    "title": "Cannot install neither with pip nor with poetry",
    "body": "Got this error with pip (`pip install vllm`):\r\n\r\n\r\n```\r\nerror: subprocess-exited-with-error\r\n\r\n\u00d7 Getting requirements to build wheel did not run successfully.\r\n\u2502 exit code: 1\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This error originates from a subprocess, and is likely not a problem with pip.\r\n```\r\n\r\n\r\nAnd this error with poetry (`poetry add vllm`):\r\n\r\n```\r\n\r\n  at ~/.local/lib/python3.10/site-packages/poetry/installation/chef.py:152 in _prepare\r\n      148\u2502 \r\n      149\u2502                 error = ChefBuildError(\"\\n\\n\".join(message_parts))\r\n      150\u2502 \r\n      151\u2502             if error is not None:\r\n    \u2192 152\u2502                 raise error from None\r\n      153\u2502 \r\n      154\u2502             return path\r\n      155\u2502 \r\n      156\u2502     def _prepare_sdist(self, archive: Path, destination: Path | None = None) -> Path:\r\n\r\nNote: This error originates from the build backend, and is likely not a problem with poetry but with vllm (0.1.1) not supporting PEP 517 builds. You can verify this by running 'pip wheel --use-pep517 \"vllm (==0.1.1)\"'.\r\n```\r\n\r\n\r\n\r\n\r\n",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2023-06-28T13:13:50+00:00",
    "closed_at": "2023-11-22T15:51:32+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/291/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/291"
  },
  {
    "number": 18728,
    "title": "[Performance]: yarn degrades the performance of qwen3",
    "body": "### Proposal to improve performance\n\n`vllm version == 0.8.5.post1`\n\nwithout yarn\n```bash\nvllm serve Qwen/Qwen3-32B   \\\n --trust-remote-code --gpu_memory_utilization 0.95 --tensor-parallel-size 2 \\\n--quantization bitsandbytes --load_format bitsandbytes --enforce_eager \\\n--max-model-len 32768\n```\n\nwith yarn\n```bash\nvllm serve Qwen/Qwen3-32B   \\\n--trust-remote-code --gpu_memory_utilization 0.95 --tensor-parallel-size 2 \\\n--quantization bitsandbytes --load_format bitsandbytes --enforce_eager \\\n--rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}' \\\n--max-model-len 131072\n```\n\nI have some tests on my end for its agentic capabilities based on qwen3 and I have some solid findings that enabling yarn to extend window context does degrade the performace, with around 15-20% performance drop. \n\ndo u also encounter the same findings ? any suggestion about this drop ?\n\n\n\n### Report of performance regression\n\n_No response_\n\n### Misc discussion on performance\n\n_No response_\n\n### Your current environment (if you think it is necessary)\n\n```text\nThe output of `python collect_env.py`\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance"
    ],
    "state": "closed",
    "created_at": "2025-05-26T18:32:46+00:00",
    "closed_at": "2025-06-05T14:58:13+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18728/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/18728"
  },
  {
    "number": 10043,
    "title": "[New Model]: Support Tencent-Hunyuan-Large",
    "body": "### The model to consider.\n\nhttps://huggingface.co/tencent/Tencent-Hunyuan-Large\r\n\r\nTencent released a 389B MoE with only 52B activated parameters which beats the Llama 3.1 405B.\r\nThere are three checkpoints in the model card: Pretrain, Instruct, and Instruct-FP8 (AutoFP8 format)\r\n\r\nSome notable features of the model:\r\n\r\n- **High-Quality Synthetic Data**: By enhancing training with synthetic data, Hunyuan-Large can learn richer representations, handle long-context inputs, and generalize better to unseen data.\r\n\r\n- **KV Cache Compression**: Utilizes Grouped Query Attention (GQA) and Cross-Layer Attention (CLA) strategies to significantly reduce memory usage and computational overhead of KV caches, improving inference throughput.\r\n\r\n- **Expert-Specific Learning Rate Scaling**: Sets different learning rates for different experts to ensure each sub-model effectively learns from the data and contributes to overall performance.\r\n\r\n- **Long-Context Processing Capability**: The pre-trained model supports text sequences up to 256K, and the Instruct model supports up to 128K, significantly enhancing the ability to handle long-context tasks.\r\n\r\n- **Extensive Benchmarking**: Conducts extensive experiments across various languages and tasks to validate the practical effectiveness and safety of Hunyuan-Large.\r\n\r\nI think the inclusion of Cross-Layer Attention (CLA) described in https://arxiv.org/abs/2405.12981 and by [Character.AI](https://research.character.ai/optimizing-inference/?ref=blog.character.ai) is the most interesting element.\n\n### The closest model vllm already supports.\n\nSince there is a shared expert at each MoE MLP, I think DeepSeekV2 is the closest comparison.\n\n### What's your difficulty of supporting the model you want?\n\nMedium to high difficulty. I believe the difficulty lies with supporting CLA, where most other feature should already be implementable.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-05T16:34:40+00:00",
    "closed_at": "2025-03-07T02:03:33+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10043/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/10043"
  },
  {
    "number": 4667,
    "title": "openai.BadRequestError: Error code: 400 - {'object': 'error', 'message': 'max_tokens must be at least 1, got -186.', 'type': 'BadRequestError', 'param': None, 'code': 400}",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 426, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/usr/local/lib/python3.8/dist-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.8/dist-packages/fastapi/applications.py\", line 1106, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/usr/local/lib/python3.8/dist-packages/starlette/applications.py\", line 122, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.8/dist-packages/starlette/middleware/errors.py\", line 184, in __call__\r\n    raise exc\r\n  File \"/usr/local/lib/python3.8/dist-packages/starlette/middleware/errors.py\", line 162, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/usr/local/lib/python3.8/dist-packages/starlette/middleware/cors.py\", line 91, in __call__\r\n    await self.simple_response(scope, receive, send, request_headers=headers)\r\n  File \"/usr/local/lib/python3.8/dist-packages/starlette/middleware/cors.py\", line 146, in simple_response\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.8/dist-packages/starlette/middleware/exceptions.py\", line 79, in __call__\r\n    raise exc\r\n  File \"/usr/local/lib/python3.8/dist-packages/starlette/middleware/exceptions.py\", line 68, in __call__\r\n    await self.app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.8/dist-packages/fastapi/middleware/asyncexitstack.py\", line 20, in __call__\r\n    raise e\r\n  File \"/usr/local/lib/python3.8/dist-packages/fastapi/middleware/asyncexitstack.py\", line 17, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.8/dist-packages/starlette/routing.py\", line 718, in __call__\r\n    await route.handle(scope, receive, send)\r\n  File \"/usr/local/lib/python3.8/dist-packages/starlette/routing.py\", line 276, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.8/dist-packages/starlette/routing.py\", line 69, in app\r\n    await response(scope, receive, send)\r\n  File \"/usr/local/lib/python3.8/dist-packages/starlette/responses.py\", line 277, in __call__\r\n    await wrap(partial(self.listen_for_disconnect, receive))\r\n  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 597, in __aexit__\r\n    raise exceptions[0]\r\n  File \"/usr/local/lib/python3.8/dist-packages/starlette/responses.py\", line 273, in wrap\r\n    await func()\r\n  File \"/usr/local/lib/python3.8/dist-packages/starlette/responses.py\", line 262, in stream_response\r\n    async for chunk in self.body_iterator:\r\n  File \"/usr/local/lib/python3.8/dist-packages/starlette/concurrency.py\", line 63, in iterate_in_threadpool\r\n    yield await anyio.to_thread.run_sync(_next, iterator)\r\n  File \"/usr/local/lib/python3.8/dist-packages/anyio/to_thread.py\", line 33, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\r\n    result = context.run(func, *args)\r\n  File \"/usr/local/lib/python3.8/dist-packages/starlette/concurrency.py\", line 53, in _next\r\n    return next(iterator)\r\n  File \"api_server.py\", line 99, in stream\r\n    chat_response = client.chat.completions.create(\r\n  File \"/usr/local/lib/python3.8/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/openai/resources/chat/completions.py\", line 667, in create\r\n    return self._post(\r\n  File \"/usr/local/lib/python3.8/dist-packages/openai/_base_client.py\", line 1213, in post\r\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\r\n  File \"/usr/local/lib/python3.8/dist-packages/openai/_base_client.py\", line 902, in request\r\n    return self._request(\r\n  File \"/usr/local/lib/python3.8/dist-packages/openai/_base_client.py\", line 993, in _request\r\n    raise self._make_status_error_from_response(err.response) from None\r\nopenai.BadRequestError: Error code: 400 - {'object': 'error', 'message': 'max_tokens must be at least 1, got -186.', 'type': 'BadRequestError', 'param': None, 'code': 400}\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-05-08T02:29:17+00:00",
    "closed_at": "2024-05-31T18:56:40+00:00",
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4667/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4667"
  },
  {
    "number": 11447,
    "title": "[Bug]: The value of --max-model-len may influence results although the length of input less than max-model-len",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\n```python\r\nmodel = LLM(model='./model/' + modelID, trust_remote_code=True,max_model_len=32*1024 / 128 * 1024) \r\n```\r\nI think it should be a widespread problem, the value of --max-model-len may influence results although the length of input less than max-model-len. \n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2024-12-24T03:53:08+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11447/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11447"
  },
  {
    "number": 15294,
    "title": "[Bug]: Critical Memory Leak in vLLM V1 Engine: 200+ GB RAM Usage from Image Inference",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-130-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3\nNvidia driver version: 550.144.03\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               128\nOn-line CPU(s) list:                  0-127\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8462Y+\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   32\nSocket(s):                            2\nStepping:                             8\nFrequency boost:                      enabled\nCPU max MHz:                          2801.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             5600.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            3 MiB (64 instances)\nL1i cache:                            2 MiB (64 instances)\nL2 cache:                             128 MiB (64 instances)\nL3 cache:                             120 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-31,64-95\nNUMA node1 CPU(s):                    32-63,96-127\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.50.0\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-31,64-95      0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNVIDIA_VISIBLE_DEVICES=GPU-c19a2d90-c6b2-d624-af66-28214621a956\nNVIDIA_REQUIRE_CUDA=cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\nNCCL_VERSION=2.17.1-1\nNVIDIA_DRIVER_CAPABILITIES=all\nNVIDIA_PRODUCT_NAME=CUDA\nCUDA_VERSION=12.1.1\nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWe are using vLLM in production and yesterday upgraded to version 0.8.1. After switching to the new V1 engine, we observed a significant RAM memory leak during image inference requests, even though the model runs on the GPU.\n\nIn our production environment, RAM usage ballooned to over 200 GB. Upon further investigation, we reproduced the issue using a minimal example (provided below). With V1 engine, each image inference request increases system memory usage, whereas V0 engine does not have this issue.\n\n**Summary:**\n1. Issue only occurs in V1 engine.\n2. Memory leak is tied to image inference requests.\n3. Behavior is not present in V0.\n\nWe\u2019d greatly appreciate it if you could investigate and let us know if you need any additional information. We\u2019re happy to assist in resolving this important issue as soon as possible.\n\nvLLM serve command (using V1): \n```bash\nvllm serve mistralai/Pixtral-12B-2409 --tokenizer_mode mistral --limit_mm_per_prompt 'image=4' --disable-mm-preprocessor-cache\n```\n\nInitial memory usage (3209 MB):\n![Image](https://github.com/user-attachments/assets/dc0a6a37-30c4-46e0-a067-24517a6bd1b7)\n\nRun image inference 100 times:\n\n```python\nimport requests\nimport json\n\nurl = \"http://127.0.0.1:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\n\nmodel = \"mistralai/Pixtral-12B-2409\"\n\nimage_url = \"https://texascoffeeschool.com/wp-content/uploads/2021/10/DSC_0052-scaled.jpg\"\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"Describe this image in a single sentence.\",\n            },\n            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n        ],\n    },\n]\n\ndata = {\"model\": model, \"messages\": messages}\n\nfor _ in range(1,100):\n    response = requests.post(url, headers=headers, data=json.dumps(data))\n    print(response.json()[\"choices\"][0][\"message\"][\"content\"])\n```\n\nFinal memory usage (6538 MB):\n![Image](https://github.com/user-attachments/assets/a3b2ebe5-fc72-4592-a688-544f3b90d200)\n\nvLLM serve command (using V0): \n```bash\nVLLM_USE_V1=0 vllm serve mistralai/Pixtral-12B-2409 --tokenizer_mode mistral --limit_mm_per_prompt 'image=4' --disable-mm-preprocessor-cache\n```\n\nInitial memory usage (8845 MB):\n![Image](https://github.com/user-attachments/assets/f853db98-8b2b-4be0-b2dd-97cdaef8f4f8)\n\nFinal memory usage -after 100 image inference requests- (8835 MB):\n![Image](https://github.com/user-attachments/assets/a0f9e7c5-6fc8-4d7f-b899-0db8151c85d2)\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-21T15:35:37+00:00",
    "closed_at": "2025-03-23T19:31:04+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15294/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15294"
  }
]
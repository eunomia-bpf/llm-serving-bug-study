[
  {
    "number": 19196,
    "title": "[Feature]: try to gracefully destroy process group in `vllm serve` on handling Ctrl+C (prior to processes termination)",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nOtherwise I get \n```\n[rank0]:[W604 11:18:57.117195760 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n```\n\nwhich seems harmless, but it would be better to not have this warning if possible\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-06-05T09:16:45+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19196/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19196"
  },
  {
    "number": 1130,
    "title": "inference Bug",
    "body": "\u95ee\u9898\u63cf\u8ff0\uff1a\r\n\u4f7f\u7528vllm\u8fdb\u884cgpt2\u63a8\u7406\uff0c\u8f93\u5165prompt\u4e3a\u7a7a\u4f1a\u89e6\u53d1\u4ee5\u4e0berror\uff0c\u4e4b\u540e\u4efb\u4f55\u8bf7\u6c42\u90fd\u4e0d\u4f1a\u51fa\u7ed3\u679c\uff0c\u9664\u975e\u91cd\u542f\u670d\u52a1\uff1b\u4e0d\u4ec5\u662f\u8fd9\u4e2a\u573a\u666f\uff0c\u5176\u4ed6\u60c5\u51b5\u4e0b\u89e6\u53d1\u8fd9\u4e2aerror\u4e5f\u4f1a\u51fa\u73b0\u76f8\u540c\u60c5\u51b5\r\n\r\nProblem Description:\r\nWhen using vllm for gpt2 inference, if the input prompt is empty, it triggers the following error, and thereafter no request will yield results unless the service is restarted. This situation occurs not only in this scenario but also in other cases where this error is triggered.\r\n\r\nerror:\r\n\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/uvicorn/protocols/http/h11_impl.py\", line 407, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/uvicorn/middleware/proxy_headers.py\", line 78, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/fastapi/applications.py\", line 270, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/starlette/applications.py\", line 124, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 184, in __call__\r\n    raise exc\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 162, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\r\n    raise exc\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\r\n    await self.app(scope, receive, sender)\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\r\n    raise e\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/starlette/routing.py\", line 706, in __call__\r\n    await route.handle(scope, receive, send)\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/starlette/routing.py\", line 276, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/starlette/routing.py\", line 69, in app\r\n    await response(scope, receive, send)\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/starlette/responses.py\", line 273, in __call__\r\n    await wrap(partial(self.listen_for_disconnect, receive))\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 597, in __aexit__\r\n    raise exceptions[0]\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/starlette/responses.py\", line 269, in wrap\r\n    await func()\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/starlette/responses.py\", line 258, in stream_response\r\n    async for chunk in self.body_iterator:\r\n  File \"/data/lss/data_bak/KBQA/LLM/vllm/vllm/vllm/entrypoints/api_server.py\", line 37, in stream_results\r\n    async for request_output in results_generator:\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/vllm/engine/async_llm_engine.py\", line 148, in generate\r\n    await self.engine_step(request_id)\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/vllm/engine/async_llm_engine.py\", line 74, in engine_step\r\n    request_outputs = self.engine.step()\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/vllm/engine/llm_engine.py\", line 454, in step\r\n    self._decode_sequences(seq_groups)\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/vllm/engine/llm_engine.py\", line 563, in _decode_sequences\r\n    new_token, new_output_text = detokenize_incrementally(\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/vllm/transformers_utils/tokenizer.py\", line 105, in detokenize_incrementally\r\n    output_text = tokenizer.convert_tokens_to_string(output_tokens)\r\n  File \"/home/ubuntu/anaconda3/envs/lyra/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\", line 533, in convert_tokens_to_string\r\n    return self.backend_tokenizer.decoder.decode(tokens)\r\nTypeError: argument 'tokens': 'NoneType' object cannot be converted to 'PyString'",
    "labels": [],
    "state": "closed",
    "created_at": "2023-09-21T12:22:33+00:00",
    "closed_at": "2024-03-08T12:22:04+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1130/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1130"
  },
  {
    "number": 17173,
    "title": "[Bug]: API Returns Only Single Result Despite n=8 Parameter Setting",
    "body": "### Your current environment\n\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 24.04.1 LTS (x86_64)\nGCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\nClang version: Could not collect\nCMake version: version 3.28.3\nLibc version: glibc-2.39\n\nPython version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-6.8.0-57-generic-x86_64-with-glibc2.39\nIs CUDA available: True\nCUDA runtime version: 12.6.85\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA H20\nGPU 1: NVIDIA H20\nGPU 2: NVIDIA H20\nGPU 3: NVIDIA H20\nGPU 4: NVIDIA H20\nGPU 5: NVIDIA H20\nGPU 6: NVIDIA H20\nGPU 7: NVIDIA H20\n\nNvidia driver version: 565.57.01\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               160\nOn-line CPU(s) list:                  0-159\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8460Y+\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   40\nSocket(s):                            2\nStepping:                             8\nCPU(s) scaling MHz:                   38%\nCPU max MHz:                          3700.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hfi vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            3.8 MiB (80 instances)\nL1i cache:                            2.5 MiB (80 instances)\nL2 cache:                             160 MiB (80 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158\nNUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.51.3\n[pip3] triton==3.1.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-ml-py              12.570.86                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.4.0                   pypi_0    pypi\n[conda] torch                     2.5.1                    pypi_0    pypi\n[conda] torchaudio                2.5.1                    pypi_0    pypi\n[conda] torchvision               0.20.1                   pypi_0    pypi\n[conda] transformers              4.51.3                   pypi_0    pypi\n[conda] triton                    3.1.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.3\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     0,2,4,6,8,10    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     0,2,4,6,8,10    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     0,2,4,6,8,10    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     0,2,4,6,8,10    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    NODE    NODE    1,3,5,7,9,11    1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    PIX     NODE    1,3,5,7,9,11    1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    NODE    NODE    1,3,5,7,9,11    1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      NODE    PIX     1,3,5,7,9,11    1               N/A\nNIC0    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE     X      NODE\nNIC1    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n\nLD_LIBRARY_PATH=/usr/local/cuda/lib64:\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n### \ud83d\udc1b Describe the bug\n\nWhile using the chat completions API, we've set n=8 (or even higher values like n=16) in our generation configuration to request multiple different completion results, but the API consistently returns only a single result.\n\n```\nSamplingParams(\n    n=16,\n    presence_penalty=0.0,\n    frequency_penalty=0.0,\n    repetition_penalty=1.0,\n    temperature=0.7,\n    top_p=0.9,\n    top_k=-1,\n    min_p=0.0,\n    seed=None,\n    stop=[],\n    stop_token_ids=[],\n    bad_words=[],\n    include_stop_str_in_output=False,\n    ignore_eos=False,\n    max_tokens=1024,\n    min_tokens=0,\n    logprobs=None,\n    prompt_logprobs=None,\n    skip_special_tokens=True,\n    spaces_between_special_tokens=True,\n    truncate_prompt_tokens=None,\n    guided_decoding=None\n)\n```\n\n```\ndef _generate_single(self, messages: List[Dict[str, Any]], request_id: Optional[str] = None) -> Tuple[List[str], Optional[str]]:\n    try:\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=messages,\n            **self.generation_config\n        )\n        print(f\"response: {response}\")\n        return [choice.message.content for choice in response.choices], request_id\n    except Exception as e:\n        logger.error(f\"Error generating response for request {request_id}: {str(e)}\")\n        raise\n```\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-04-25T08:41:53+00:00",
    "closed_at": "2025-04-25T12:47:46+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17173/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17173"
  },
  {
    "number": 2187,
    "title": "Error: max retries exceeded: unexpected EOF",
    "body": "pulling manifest \r\npulling bdb11b0699e0...  38% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                   \u258f  10 GB/ 26 GB  2.1 MB/s   2h11m\r\nError: max retries exceeded: unexpected EOF",
    "labels": [],
    "state": "closed",
    "created_at": "2023-12-19T00:08:59+00:00",
    "closed_at": "2023-12-21T03:51:07+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2187/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2187"
  },
  {
    "number": 8747,
    "title": "[Bug]: OLMoE produces incorrect output with TP>1",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nWe can perform evaluations for GSM8k with lm-eval to see the issue. Please use `pip install lm-eval==0.4.3`\r\n\r\nTP=1\r\n```\r\nVLLM_WORKER_MULTIPROC_METHOD=spawn lm_eval --model vllm --model_args pretrained=allenai/OLMoE-1B-7B-0924-Instruct,tensor_parallel_size=1 --tasks gsm8k --num_fewshot 5 --batch_size auto\r\n\r\nvllm (pretrained=allenai/OLMoE-1B-7B-0924-Instruct,tensor_parallel_size=1), gen_kwargs: (None), limit: None, num_fewshot: 5, batch_size: auto\r\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\r\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\r\n|gsm8k|      3|flexible-extract|     5|exact_match|\u2191  |0.3457|\u00b1  |0.0131|\r\n|     |       |strict-match    |     5|exact_match|\u2191  |0.3313|\u00b1  |0.0130|\r\n```\r\n\r\nTP=2\r\n```\r\nVLLM_WORKER_MULTIPROC_METHOD=spawn lm_eval --model vllm --model_args pretrained=allenai/OLMoE-1B-7B-0924-Instruct,tensor_parallel_size=2 --tasks gsm8k --num_fewshot 5 --batch_size auto\r\n\r\nvllm (pretrained=allenai/OLMoE-1B-7B-0924-Instruct,tensor_parallel_size=2), gen_kwargs: (None), limit: None, num_fewshot: 5, batch_size: auto\r\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\r\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\r\n|gsm8k|      3|flexible-extract|     5|exact_match|\u2191  |0.0144|\u00b1  |0.0033|\r\n|     |       |strict-match    |     5|exact_match|\u2191  |0.0091|\u00b1  |0.0026|\r\n```\r\n\r\nYou can see that the strict-match accuracy goes from 33.13% to 0.91%, so it is clear the outputs are degraded very much.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-23T22:42:56+00:00",
    "closed_at": "2025-01-24T01:58:52+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8747/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/8747"
  },
  {
    "number": 7992,
    "title": "[Bug]: Jamba-1.5-mini doesn't run on A100 with 70GB available memory",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nWARNING 08-29 10:29:48 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.3\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1068-azure-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A100 80GB PCIe\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               24\r\nOn-line CPU(s) list:                  0-23\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD EPYC 7V13 64-Core Processor\r\nCPU family:                           25\r\nModel:                                1\r\nThread(s) per core:                   1\r\nCore(s) per socket:                   24\r\nSocket(s):                            1\r\nStepping:                             1\r\nBogoMIPS:                             4890.88\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr rdpru arat umip vaes vpclmulqdq rdpid fsrm\r\nHypervisor vendor:                    Microsoft\r\nVirtualization type:                  full\r\nL1d cache:                            768 KiB (24 instances)\r\nL1i cache:                            768 KiB (24 instances)\r\nL2 cache:                             12 MiB (24 instances)\r\nL3 cache:                             96 MiB (3 instances)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-23\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Mitigation; safe RET, no microcode\r\nVulnerability Spec store bypass:      Vulnerable\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] ctransformers==0.2.27\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvcc-cu12==12.5.40\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.550.52\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] onnx==1.16.1\r\n[pip3] onnxruntime==1.18.0\r\n[pip3] pynvml==11.5.0\r\n[pip3] pytorch-lightning==2.2.4\r\n[pip3] pytorch-metric-learning==2.5.0\r\n[pip3] pyzmq==25.1.2\r\n[pip3] sentence-transformers==2.7.0\r\n[pip3] torch==2.4.0\r\n[pip3] torch-audiomentations==0.11.1\r\n[pip3] torch-pitch-shift==1.2.4\r\n[pip3] torchaudio==2.3.0\r\n[pip3] torchmetrics==1.4.0.post0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.1\r\n[pip3] triton==3.0.0\r\n[pip3] vllm_nccl_cu12==2.18.1.0.4.0\r\n[conda] ctransformers             0.2.27                   pypi_0    pypi\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvcc-cu12     12.5.40                  pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.550.52                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\r\n[conda] pynvml                    11.5.0                   pypi_0    pypi\r\n[conda] pytorch-lightning         2.2.4                    pypi_0    pypi\r\n[conda] pytorch-metric-learning   2.5.0                    pypi_0    pypi\r\n[conda] pyzmq                     25.1.2          py311h6a678d5_0  \r\n[conda] sentence-transformers     2.7.0                    pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torch-audiomentations     0.11.1                   pypi_0    pypi\r\n[conda] torch-pitch-shift         1.2.4                    pypi_0    pypi\r\n[conda] torchaudio                2.3.0                    pypi_0    pypi\r\n[conda] torchmetrics              1.4.0.post0              pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.40.1                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.4.0             pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.5@f205c09854853172a446c92aa81eb7199da324ab\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-23    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nRunning Jamba-1.5-mini with experts_int8 quantization crashes on A100(80GB) GPU with any max_model_len. I've tried with 8k/12k/16k.\r\n\r\nOutput of NVIDIA-SMI:\r\n```\r\n(model) azureuser@votum-vm-new:~/vllm$ nvidia-smi\r\nThu Aug 29 10:30:40 2024       \r\n+-----------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\r\n|-----------------------------------------+------------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                        |               MIG M. |\r\n|=========================================+========================+======================|\r\n|   0  NVIDIA A100 80GB PCIe          Off |   00000001:00:00.0 Off |                    0 |\r\n| N/A   35C    P0             66W /  300W |   11766MiB /  81920MiB |      0%      Default |\r\n|                                         |                        |             Disabled |\r\n+-----------------------------------------+------------------------+----------------------+\r\n```\r\n\r\n```shell\r\n(model) azureuser@votum-vm-new:~/vllm$ vllm serve ai21labs/AI21-Jamba-1.5-Mini --max-model-len 16384 --quantization experts_int8 --kv-cache-dtype fp8\r\nWARNING 08-29 09:57:06 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\r\nINFO 08-29 09:57:09 api_server.py:440] vLLM API server version 0.5.5\r\nINFO 08-29 09:57:09 api_server.py:441] args: Namespace(model_tag='ai21labs/AI21-Jamba-1.5-Mini', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, model='ai21labs/AI21-Jamba-1.5-Mini', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='fp8', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization='experts_int8', rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None, dispatch_function=<function serve at 0x7f93ccbede40>)\r\nINFO 08-29 09:57:09 api_server.py:144] Multiprocessing frontend to use ipc:///tmp/b3bc85b7-3563-498f-a89a-ccd950a53c10 for RPC Path.\r\nINFO 08-29 09:57:09 api_server.py:161] Started engine process with PID 1796934\r\nWARNING 08-29 09:57:11 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\r\nINFO 08-29 09:57:14 config.py:628] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor\r\nINFO 08-29 09:57:14 llm_engine.py:210] Initializing an LLM engine (v0.5.5) with config: model='ai21labs/AI21-Jamba-1.5-Mini', speculative_config=None, tokenizer='ai21labs/AI21-Jamba-1.5-Mini', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=experts_int8, enforce_eager=False, kv_cache_dtype=fp8, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=ai21labs/AI21-Jamba-1.5-Mini, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\r\nYou are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\r\nINFO 08-29 09:57:15 selector.py:227] Cannot use FlashAttention-2 backend for FP8 KV cache.\r\nINFO 08-29 09:57:15 selector.py:116] Using XFormers backend.\r\n/home/azureuser/miniconda3/envs/model/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n/home/azureuser/miniconda3/envs/model/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n[rank0]:[W829 09:57:16.555876601 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\r\nINFO 08-29 09:57:16 model_runner.py:906] Starting to load model ai21labs/AI21-Jamba-1.5-Mini...\r\nWARNING 08-29 09:57:16 interfaces.py:132] The model (<class 'vllm.model_executor.models.jamba.JambaForCausalLM'>) contains all LoRA-specific attributes, but does not set `supports_lora=True`.\r\nINFO 08-29 09:57:16 selector.py:227] Cannot use FlashAttention-2 backend for FP8 KV cache.\r\nINFO 08-29 09:57:16 selector.py:116] Using XFormers backend.\r\nINFO 08-29 09:57:17 weight_utils.py:236] Using model weights format ['*.safetensors']\r\nLoading safetensors checkpoint shards:   0% Completed | 0/21 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:   5% Completed | 1/21 [00:00<00:14,  1.35it/s]\r\nLoading safetensors checkpoint shards:  10% Completed | 2/21 [00:01<00:15,  1.21it/s]\r\nLoading safetensors checkpoint shards:  14% Completed | 3/21 [00:02<00:14,  1.21it/s]\r\nLoading safetensors checkpoint shards:  19% Completed | 4/21 [00:03<00:13,  1.22it/s]\r\nLoading safetensors checkpoint shards:  24% Completed | 5/21 [00:04<00:13,  1.18it/s]\r\nLoading safetensors checkpoint shards:  29% Completed | 6/21 [00:05<00:12,  1.16it/s]\r\nLoading safetensors checkpoint shards:  33% Completed | 7/21 [00:05<00:12,  1.14it/s]\r\nLoading safetensors checkpoint shards:  38% Completed | 8/21 [00:06<00:11,  1.15it/s]\r\nLoading safetensors checkpoint shards:  43% Completed | 9/21 [00:07<00:10,  1.17it/s]\r\nLoading safetensors checkpoint shards:  48% Completed | 10/21 [00:08<00:09,  1.17it/s]\r\nLoading safetensors checkpoint shards:  52% Completed | 11/21 [00:09<00:08,  1.18it/s]\r\nLoading safetensors checkpoint shards:  57% Completed | 12/21 [00:10<00:07,  1.15it/s]\r\nLoading safetensors checkpoint shards:  62% Completed | 13/21 [00:11<00:07,  1.13it/s]\r\nLoading safetensors checkpoint shards:  67% Completed | 14/21 [00:11<00:06,  1.16it/s]\r\nLoading safetensors checkpoint shards:  71% Completed | 15/21 [00:12<00:05,  1.17it/s]\r\nLoading safetensors checkpoint shards:  76% Completed | 16/21 [00:13<00:04,  1.15it/s]\r\nLoading safetensors checkpoint shards:  81% Completed | 17/21 [00:14<00:03,  1.12it/s]\r\nLoading safetensors checkpoint shards:  86% Completed | 18/21 [00:15<00:02,  1.13it/s]\r\nLoading safetensors checkpoint shards:  90% Completed | 19/21 [00:16<00:01,  1.13it/s]\r\nLoading safetensors checkpoint shards:  95% Completed | 20/21 [00:17<00:00,  1.15it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 21/21 [00:18<00:00,  1.19it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 21/21 [00:18<00:00,  1.17it/s]\r\n\r\nINFO 08-29 09:57:35 model_runner.py:917] Loading model weights took 51.4723 GB\r\nINFO 08-29 09:57:42 gpu_executor.py:121] # GPU blocks: 112557, # CPU blocks: 32768\r\nProcess SpawnProcess-1:\r\nTraceback (most recent call last):\r\n  File \"/home/azureuser/miniconda3/envs/model/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/home/azureuser/miniconda3/envs/model/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/azureuser/vllm/vllm/entrypoints/openai/rpc/server.py\", line 236, in run_rpc_server\r\n    server = AsyncEngineRPCServer(async_engine_args, usage_context, rpc_path)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/azureuser/vllm/vllm/entrypoints/openai/rpc/server.py\", line 34, in __init__\r\n    self.engine = AsyncLLMEngine.from_engine_args(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/azureuser/vllm/vllm/engine/async_llm_engine.py\", line 726, in from_engine_args\r\n    engine = cls(\r\n             ^^^^\r\n  File \"/home/azureuser/vllm/vllm/engine/async_llm_engine.py\", line 617, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/azureuser/vllm/vllm/engine/async_llm_engine.py\", line 826, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/azureuser/vllm/vllm/engine/async_llm_engine.py\", line 261, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/home/azureuser/vllm/vllm/engine/llm_engine.py\", line 314, in __init__\r\n    self._initialize_kv_caches()\r\n  File \"/home/azureuser/vllm/vllm/engine/llm_engine.py\", line 455, in _initialize_kv_caches\r\n    self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\r\n  File \"/home/azureuser/vllm/vllm/executor/gpu_executor.py\", line 124, in initialize_cache\r\n    self.driver_worker.initialize_cache(num_gpu_blocks, num_cpu_blocks)\r\n  File \"/home/azureuser/vllm/vllm/worker/worker.py\", line 264, in initialize_cache\r\n    self._init_cache_engine()\r\n  File \"/home/azureuser/vllm/vllm/worker/worker.py\", line 269, in _init_cache_engine\r\n    self.cache_engine = [\r\n                        ^\r\n  File \"/home/azureuser/vllm/vllm/worker/worker.py\", line 270, in <listcomp>\r\n    CacheEngine(self.cache_config, self.model_config,\r\n  File \"/home/azureuser/vllm/vllm/worker/cache_engine.py\", line 66, in __init__\r\n    self.gpu_cache = self._allocate_kv_cache(\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/azureuser/vllm/vllm/worker/cache_engine.py\", line 85, in _allocate_kv_cache\r\n    torch.zeros(kv_cache_shape,\r\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.44 GiB. GPU 0 has a total capacity of 79.14 GiB of which 3.14 GiB is free. Process 1253 has 1.08 GiB memory in use. Process 1254 has 1.52 GiB memory in use. Process 2937823 has 8.75 GiB memory in use. Including non-PyTorch memory, this process has 64.50 GiB memory in use. Of the allocated memory 63.97 GiB is allocated by PyTorch, and 28.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\nERROR 08-29 09:57:44 api_server.py:171] RPCServer process died before responding to readiness probe\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-08-29T10:31:50+00:00",
    "closed_at": "2024-08-31T07:19:12+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7992/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7992"
  },
  {
    "number": 15760,
    "title": "[Usage]: Why AWQ quantization does not support expert parallelism?",
    "body": "### Your current environment\n\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.26.0\nLibc version: glibc-2.35\n\nPython version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-6.2.0-39-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.99\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA A40\nGPU 1: NVIDIA A40\n\nNvidia driver version: 560.35.03\n\n\n### How would you like to use vllm\n\nI want to run Deepseek-V3-AWQ with expert parallelism, but I get a error saying \" Expert fused Parallelism is not supported for Marlin MoE method\". I want to know whether awq can achieve expert parallelism under the current vllm architecture.\n\"\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-30T03:18:28+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15760/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15760"
  },
  {
    "number": 2380,
    "title": "Understanding about LLM class from vllm",
    "body": "is LLM class from vllm is asynchronous by nature ?\r\nwhy am i asking this from the [slides](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit#slide=id.g24ad94a0065_0_84) on the first meetup  it has mentioned that llm is synchronous rather api_server and openai_server are asynchronous ?\r\n\r\nif that is true ,how to call the llm model asynchronously ?\r\n\r\ncorrect me if i am wrong !\r\nTIA",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-08T10:26:43+00:00",
    "closed_at": "2024-04-03T15:43:34+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2380/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2380"
  },
  {
    "number": 11391,
    "title": "Where does the default number 43328 of KV cache come from and How can I change it?",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\nNot an technical issue, not related to environment.\n\n### How would you like to use vllm\n\nI have encountered \"The model's max seq len (56000) is larger than the maximum number of tokens that can be stored in KV cache (43328)\" numerous times. Although it can be solved by setting a smaller --max-model-len parameter, it's actually an issue when you really want to set a large --max-model-len for a large context. What makes it more complicated is that the KV cache number changes automatically when we set different --max-model-len. My question is: 1) can we change the size of KV cache? 2) how? 3) Anyway for us user to manage the KV cache issue more directly?\r\n\r\nThanks\r\nGeorge\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-21T06:11:52+00:00",
    "closed_at": "2025-04-21T02:10:11+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11391/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11391"
  },
  {
    "number": 9226,
    "title": "[Bug]: Could not `pip install vllm` inside dockerfile after certain commit in `main` branch",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.26.4\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.25-051525-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA GeForce RTX 4090\r\nGPU 1: NVIDIA GeForce RTX 4090\r\n\r\nNvidia driver version: 545.23.08\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   39 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          16\r\nOn-line CPU(s) list:             0-15\r\nVendor ID:                       GenuineIntel\r\nModel name:                      13th Gen Intel(R) Core(TM) i5-13400\r\nCPU family:                      6\r\nModel:                           191\r\nThread(s) per core:              2\r\nCore(s) per socket:              10\r\nSocket(s):                       1\r\nStepping:                        2\r\nCPU max MHz:                     3425.8350\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        4992.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req umip pku ospke waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize arch_lbr flush_l1d arch_capabilities\r\nVirtualization:                  VT-x\r\nL1d cache:                       416 KiB (10 instances)\r\nL1i cache:                       448 KiB (10 instances)\r\nL2 cache:                        9.5 MiB (7 instances)\r\nL3 cache:                        20 MiB (1 instance)\r\nNUMA node(s):                    1\r\nNUMA node0 CPU(s):               0-15\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] optree==0.12.1\r\n[pip3] torch==2.4.0\r\n[pip3] torchaudio==2.4.0\r\n[pip3] torchelastic==0.2.2\r\n[pip3] torchvision==0.19.0\r\n[pip3] triton==3.0.0\r\n[conda] blas                      1.0                         mkl\r\n[conda] cuda-cudart               12.1.105                      0    nvidia\r\n[conda] cuda-cupti                12.1.105                      0    nvidia\r\n[conda] cuda-libraries            12.1.0                        0    nvidia\r\n[conda] cuda-nvrtc                12.1.105                      0    nvidia\r\n[conda] cuda-nvtx                 12.1.105                      0    nvidia\r\n[conda] cuda-opencl               12.5.39                       0    nvidia\r\n[conda] cuda-runtime              12.1.0                        0    nvidia\r\n[conda] cuda-version              12.5                          3    nvidia\r\n[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\r\n[conda] libcublas                 12.1.0.26                     0    nvidia\r\n[conda] libcufft                  11.0.2.4                      0    nvidia\r\n[conda] libcufile                 1.10.1.7                      0    nvidia\r\n[conda] libcurand                 10.3.6.82                     0    nvidia\r\n[conda] libcusolver               11.4.4.55                     0    nvidia\r\n[conda] libcusparse               12.0.2.55                     0    nvidia\r\n[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\r\n[conda] libnpp                    12.0.2.50                     0    nvidia\r\n[conda] libnvjitlink              12.1.105                      0    nvidia\r\n[conda] libnvjpeg                 12.1.1.14                     0    nvidia\r\n[conda] mkl                       2023.1.0         h213fc3f_46344\r\n[conda] mkl-service               2.4.0           py311h5eee18b_1\r\n[conda] mkl_fft                   1.3.8           py311h5eee18b_0\r\n[conda] mkl_random                1.2.4           py311hdb19cb5_0\r\n[conda] numpy                     1.26.4          py311h08b1b3b_0\r\n[conda] numpy-base                1.26.4          py311hf175353_0\r\n[conda] optree                    0.12.1                   pypi_0    pypi\r\n[conda] pytorch                   2.4.0           py3.11_cuda12.1_cudnn9.1.0_0    pytorch\r\n[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch\r\n[conda] pytorch-mutex             1.0                        cuda    pytorch\r\n[conda] torchaudio                2.4.0               py311_cu121    pytorch\r\n[conda] torchelastic              0.2.2                    pypi_0    pypi\r\n[conda] torchtriton               3.0.0                     py311    pytorch\r\n[conda] torchvision               0.19.0              py311_cu121    pytorch\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PHB     0-15    0               N/A\r\nGPU1    PHB      X      0-15    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nThis is actually a follow up from #9152. After I tested every commit to build docker image using this Dockerfile\r\n\r\n```dockerfile\r\nFROM pytorch/pytorch:2.4.0-cuda12.1-cudnn9-devel AS build\r\n\r\nARG GITHASH\r\n\r\nRUN apt update && apt install gcc g++ git -y && apt clean && rm -rf /var/lib/apt/lists/*\r\n\r\nENV PATH=/workspace-lib:/workspace-lib/bin:$PATH\r\nENV PYTHONUSERBASE=/workspace-lib\r\n\r\nRUN pip install git+https://github.com/vllm-project/vllm.git@${GITHASH} --no-cache-dir --user\r\n\r\nFROM pytorch/pytorch:2.4.0-cuda12.1-cudnn9-runtime AS vllm-openai\r\n\r\nWORKDIR /vllm-workspace\r\n\r\nCOPY --from=build /workspace-lib /workspace-lib\r\n\r\nENV PATH=/workspace-lib:/workspace-lib/bin:$PATH\r\nENV PYTHONUSERBASE=/workspace-lib\r\nENV PYTHONPATH=/workspace-lib:/vllm-workspace\r\n\r\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\r\n```\r\n\r\nI found that this particular commit aeb37c2a725554791ff6f258b1e18830867a3ab9 is the culprit.\r\n\r\nIs there any way to solve this? I already update my Nvidia Driver to the latest but the problem persisted.\r\n\r\nCC: @LucasWilkinson (author of the mentioned commit)\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-10-10T05:47:12+00:00",
    "closed_at": "2024-10-11T19:57:40+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9226/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/9226"
  },
  {
    "number": 1143,
    "title": "H800 support",
    "body": "does vllm compatible with the gpu of H800",
    "labels": [],
    "state": "closed",
    "created_at": "2023-09-22T09:30:51+00:00",
    "closed_at": "2024-03-08T12:24:17+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1143/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1143"
  },
  {
    "number": 7645,
    "title": "[Usage]: The results of the model depend on the number of GPUs.",
    "body": "### Your current environment\n\n**package version**\r\n- vllm: 0.2.6\r\n- python: 3.9\r\n\r\n**Phenomenon**\r\n- The output of the same model is different when using only one gpu and two.\r\n- the same environment\r\n- the same arguments\n\n### How would you like to use vllm\n\nI would like to get the same result even if the number of gpu is used differently when referring using vLLM.\r\nThe code I used is as follows.\r\n\r\n```python\r\npayload = json.dumps({\r\n  \"model_name\": model_name,\r\n  \"prompt\": output_ex['model_input'],\r\n  \"max_tokens\": 1024,\r\n  \"stream\": False,\r\n  \"top_p\": 0.9,\r\n  \"temperature\": 0.01,\r\n  \"presence_penalty\": 1.0,\r\n  \"stop_token_ids\": [2],\r\n  \"best_of\": 1\r\n})\r\n\r\nheaders = {\r\n'Content-Type': 'application/json'\r\n}\r\n\r\nresponse = requests.request(\"POST\", url, headers=headers, data=payload)\r\n```\r\n<br><br>\r\n\r\n- when using only one gpu\r\n  - output: \"\" (=empty)\r\n- when using twon gpu\r\n  - output: \"No answer found.\"\r\n\r\n<br><br>\r\n\r\nHas anyone experienced a phenomenon like me where the output comes out differently depending on the number of gpu loading the model?",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-19T05:08:51+00:00",
    "closed_at": "2024-12-19T02:04:30+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7645/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7645"
  },
  {
    "number": 6885,
    "title": "[Bug]: Speculative Decoding + FlashInfer + benchmark_serving.py TransferEncodingError ISSUE",
    "body": "### Your current environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.3.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-72-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.5.40\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-80GB\r\nNvidia driver version: 535.183.01\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.1.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz\r\nCPU family:                      6\r\nModel:                           106\r\nThread(s) per core:              2\r\nCore(s) per socket:              32\r\nSocket(s):                       2\r\nStepping:                        6\r\nCPU max MHz:                     3400.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        5200.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\r\nVirtualization:                  VT-x\r\nL1d cache:                       3 MiB (64 instances)\r\nL1i cache:                       2 MiB (64 instances)\r\nL2 cache:                        80 MiB (64 instances)\r\nL3 cache:                        96 MiB (2 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-31,64-95\r\nNUMA node1 CPU(s):               32-63,96-127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.1+cu121torch2.3\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.1\r\n[pip3] torchvision==0.18.1\r\n[pip3] transformers==4.43.3\r\n[pip3] triton==2.3.1\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.3.post1\r\nvLLM Build Flags:\r\nCUDA Archs: 5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      32-63,96-127    1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nHello vLLM experts!\r\n\r\nI would like to report an error that occurred while using the recently updated vLLM (`v0.5.3.post1`).\r\n\r\n\ud83e\udd14 A **TransferEncodingError** occurred while running `benchmarks_serving.py` with Speculative Decoding options after setting `VLLM_ATTENTION_BACKEND` to `FLASHINFER`.\r\n\r\nThe results obtained from running `benchmarks_serving.py` with both the `FLASH_ATTN` backend and the `FLASHINFER` backend are as follows:\r\n\r\n### FLASH_ATTN Backend\r\n```Bash\r\n# Server\r\nVLLM_ATTENTION_BACKEND=FLASH_ATTN ./venv/bin/python3 -m vllm.entrypoints.openai.api_server --model lmsys/vicuna-7b-v1.5 --swap-space 16 --disable-log-requests --speculative-model double7/vicuna-68m --num-speculative-tokens 5 --use-v2-block-manager --enforce-eager --max-num-seqs 32\r\n```\r\n```Bash\r\n# Client\r\nVLLM_ATTENTION_BACKEND=FLASH_ATTN ./venv/bin/python3 benchmarks/benchmark_serving.py --model lmsys/vicuna-7b-v1.5 --backend vllm --dataset-name sharegpt --dataset-path ./benchmarks/ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 100 --request-rate 10\r\n```\r\n```\r\n# Result\r\n============ Serving Benchmark Result ============\r\nSuccessful requests:                     100       \r\nBenchmark duration (s):                  22.18     \r\nTotal input tokens:                      26322     \r\nTotal generated tokens:                  11308     \r\nRequest throughput (req/s):              4.51      \r\nInput token throughput (tok/s):          1186.73   \r\nOutput token throughput (tok/s):         509.82    \r\n---------------Time to First Token----------------\r\nMean TTFT (ms):                          60.67     \r\nMedian TTFT (ms):                        40.54     \r\nP99 TTFT (ms):                           307.82    \r\n-----Time per Output Token (excl. 1st token)------\r\nMean TPOT (ms):                          41.22     \r\nMedian TPOT (ms):                        32.68     \r\nP99 TPOT (ms):                           133.82    \r\n---------------Inter-token Latency----------------\r\nMean ITL (ms):                           67.36     \r\nMedian ITL (ms):                         59.69     \r\nP99 ITL (ms):                            269.63    \r\n==================================================\r\n```\r\n\r\n### FLASHINFER Backend\r\n```Bash\r\n# Server\r\nVLLM_ATTENTION_BACKEND=FLASHINFER ./venv/bin/python3 -m vllm.entrypoints.openai.api_server --model lmsys/vicuna-7b-v1.5 --swap-space 16 --disable-log-requests --speculative-model double7/vicuna-68m --num-speculative-tokens 5 --use-v2-block-manager --enforce-eager --max-num-seqs 32\r\n```\r\n```Bash\r\n# Client\r\nVLLM_ATTENTION_BACKEND=FLASHINFER ./venv/bin/python3 benchmarks/benchmark_serving.py --model lmsys/vicuna-7b-v1.5 --backend vllm --dataset-name sharegpt --dataset-path ./benchmarks/ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 100 --request-rate 10\r\n```\r\n```\r\n# Result\r\nStarting initial single prompt test run...\r\nTraceback (most recent call last):\r\n  File \"/pvc/data-bongwon/vllm/benchmarks/benchmark_serving.py\", line 727, in <module>\r\n    main(args)\r\n  File \"/pvc/data-bongwon/vllm/benchmarks/benchmark_serving.py\", line 505, in main\r\n    benchmark_result = asyncio.run(\r\n  File \"/usr/lib/python3.10/asyncio/runners.py\", line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\r\n    return future.result()\r\n  File \"/pvc/data-bongwon/vllm/benchmarks/benchmark_serving.py\", line 324, in benchmark\r\n    raise ValueError(\r\nValueError: Initial test run failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):\r\n  File \"/pvc/data-bongwon/vllm/venv/lib/python3.10/site-packages/aiohttp/client_proto.py\", line 94, in connection_lost\r\n    uncompleted = self._parser.feed_eof()\r\n  File \"aiohttp/_http_parser.pyx\", line 507, in aiohttp._http_parser.HttpParser.feed_eof\r\naiohttp.http_exceptions.TransferEncodingError: 400, message:\r\n  Not enough data for satisfy transfer length header.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/pvc/data-bongwon/vllm/benchmarks/backend_request_func.py\", line 256, in async_request_openai_completions\r\n    async for chunk_bytes in response.content:\r\n  File \"/pvc/data-bongwon/vllm/venv/lib/python3.10/site-packages/aiohttp/streams.py\", line 50, in __anext__\r\n    rv = await self.read_func()\r\n  File \"/pvc/data-bongwon/vllm/venv/lib/python3.10/site-packages/aiohttp/streams.py\", line 317, in readline\r\n    return await self.readuntil()\r\n  File \"/pvc/data-bongwon/vllm/venv/lib/python3.10/site-packages/aiohttp/streams.py\", line 351, in readuntil\r\n    await self._wait(\"readuntil\")\r\n  File \"/pvc/data-bongwon/vllm/venv/lib/python3.10/site-packages/aiohttp/streams.py\", line 312, in _wait\r\n    await waiter\r\naiohttp.client_exceptions.ClientPayloadError: Response payload is not completed: <TransferEncodingError: 400, message='Not enough data for satisfy transfer length header.'>\r\n```\r\n\r\nDue to my limited knowledge of HTTP, it is challenging to identify the cause of this TransferEncodingError.\r\n**Could you please check if this error occurs on other systems as well?**\r\n**Or, it would be helpful to know if this issue has already been resolved before (for example, by modifying arguments).** \ud83d\ude47\ud83d\ude47\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-07-29T04:41:40+00:00",
    "closed_at": "2024-08-05T15:05:06+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6885/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/6885"
  },
  {
    "number": 17129,
    "title": "[Feature]: vLLM DP=2 didn't speed up the training as low batch size.",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nHi team,\n\nFirst of all, thanks for the recent efforts, especially to @qgallouedec, for supporting the new `data_parallel_size` feature in vLLM. I tested the `vllm-serve-dp` branch with `data_parallel_size=2`, and confirmed that it launches two processes for rollouts as expected. Great work!\n\nHowever, **the speedup of having `data_parallel_size=2` isn't quite as significant as I hoped**. In my previous setup using a single GPU, I was achieving around 4000\u20136000 toks/s generation. With `data_parallel_size=2`, this drops to only 1000\u20131200 tokens/s per process, which results in an overall slower or comparable throughput.\n\nIt seems that the GPUs may be underutilized, possibly waiting on input (prompts/questions) to arrive. I suspect the issue could be mitigated by **allowing larger batch sizes for generation in the vLLM server, while keeping a smaller batch size for gradient calculations to avoid OOM errors.**\n\nI\u2019m reporting this primarily for visibility, and I\u2019m confident that the team can fine-tune the implementation to achieve near-linear speedup. Looking forward to seeing future improvements!\n\nThanks again for your hard work!\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-04-24T17:54:11+00:00",
    "closed_at": "2025-04-24T19:02:17+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17129/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17129"
  },
  {
    "number": 16386,
    "title": "[Bug]: AMD Instinct MI210 + vllm fail to run deepseek-r1-awq model, any solutions please? Is there any other deepseek-r1-671b models that can run succesfully on AMD Instinct MI210 + vllm? Thanks!",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 04-10 03:25:29 [__init__.py:207] Automatically detected platform rocm.\nCollecting environment information...\nPyTorch version: 2.7.0a0+git6c0e746\nIs debug build: False\nCUDA used to build PyTorch: N/A\nROCM used to build PyTorch: 6.3.42133-1b9c17779\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: 18.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.3.1 24491 1e0fda770a2079fbd71e4b70974d74f62fd3af10)\nCMake version: version 3.31.4\nLibc version: glibc-2.35\n\nPython version: 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-136-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: AMD Instinct MI210 (gfx90a:sramecc+:xnack-)\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\nHIP runtime version: 6.3.42133\nMIOpen runtime version: 3.3.0\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        43 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               192\nOn-line CPU(s) list:                  0-191\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 7K62 48-Core Processor\nCPU family:                           23\nModel:                                49\nThread(s) per core:                   2\nCore(s) per socket:                   48\nSocket(s):                            2\nStepping:                             0\nFrequency boost:                      enabled\nCPU max MHz:                          2600.0000\nCPU min MHz:                          1500.0000\nBogoMIPS:                             5200.04\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nVirtualization:                       AMD-V\nL1d cache:                            3 MiB (96 instances)\nL1i cache:                            3 MiB (96 instances)\nL2 cache:                             48 MiB (96 instances)\nL3 cache:                             384 MiB (24 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-47,96-143\nNUMA node1 CPU(s):                    48-95,144-191\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\nVulnerability Spec rstack overflow:   Mitigation; safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.7.0a0+git6c0e746\n[pip3] torchvision==0.21.0+7af6987\n[pip3] transformers==4.49.0\n[pip3] triton==3.2.0+gite5be006a\n[conda] Could not collect\nROCM Version: 6.3.42133-1b9c17779\nNeuron SDK Version: N/A\nvLLM Version: 0.7.4.dev49+gc0dd5adf6\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n============================ ROCm System Management Interface ============================\n================================ Weight between two GPUs =================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \nGPU0   0            15           15           15           72           72           72           72           \nGPU1   15           0            15           15           72           72           72           72           \nGPU2   15           15           0            15           72           72           72           72           \nGPU3   15           15           15           0            72           72           72           72           \nGPU4   72           72           72           72           0            15           15           15           \nGPU5   72           72           72           72           15           0            15           15           \nGPU6   72           72           72           72           15           15           0            15           \nGPU7   72           72           72           72           15           15           15           0            \n\n================================= Hops between two GPUs ==================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \nGPU0   0            1            1            1            3            3            3            3            \nGPU1   1            0            1            1            3            3            3            3            \nGPU2   1            1            0            1            3            3            3            3            \nGPU3   1            1            1            0            3            3            3            3            \nGPU4   3            3            3            3            0            1            1            1            \nGPU5   3            3            3            3            1            0            1            1            \nGPU6   3            3            3            3            1            1            0            1            \nGPU7   3            3            3            3            1            1            1            0            \n\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \nGPU0   0            XGMI         XGMI         XGMI         PCIE         PCIE         PCIE         PCIE         \nGPU1   XGMI         0            XGMI         XGMI         PCIE         PCIE         PCIE         PCIE         \nGPU2   XGMI         XGMI         0            XGMI         PCIE         PCIE         PCIE         PCIE         \nGPU3   XGMI         XGMI         XGMI         0            PCIE         PCIE         PCIE         PCIE         \nGPU4   PCIE         PCIE         PCIE         PCIE         0            XGMI         XGMI         XGMI         \nGPU5   PCIE         PCIE         PCIE         PCIE         XGMI         0            XGMI         XGMI         \nGPU6   PCIE         PCIE         PCIE         PCIE         XGMI         XGMI         0            XGMI         \nGPU7   PCIE         PCIE         PCIE         PCIE         XGMI         XGMI         XGMI         0            \n\n======================================= Numa Nodes =======================================\nGPU[0]\t\t: (Topology) Numa Node: 0\nGPU[0]\t\t: (Topology) Numa Affinity: 0\nGPU[1]\t\t: (Topology) Numa Node: 0\nGPU[1]\t\t: (Topology) Numa Affinity: 0\nGPU[2]\t\t: (Topology) Numa Node: 0\nGPU[2]\t\t: (Topology) Numa Affinity: 0\nGPU[3]\t\t: (Topology) Numa Node: 0\nGPU[3]\t\t: (Topology) Numa Affinity: 0\nGPU[4]\t\t: (Topology) Numa Node: 1\nGPU[4]\t\t: (Topology) Numa Affinity: 1\nGPU[5]\t\t: (Topology) Numa Node: 1\nGPU[5]\t\t: (Topology) Numa Affinity: 1\nGPU[6]\t\t: (Topology) Numa Node: 1\nGPU[6]\t\t: (Topology) Numa Affinity: 1\nGPU[7]\t\t: (Topology) Numa Node: 1\nGPU[7]\t\t: (Topology) Numa Affinity: 1\n================================== End of ROCm SMI Log ===================================\n\nNCCL_P2P_DISABLE=1\nNCCL_IB_HCA=mlx5\nPYTORCH_ROCM_ARCH=gfx90a;gfx942\nLD_LIBRARY_PATH=/usr/local/lib/python3.12/dist-packages/cv2/../../lib64:/opt/rocm/lib:/usr/local/lib:\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n> Error message\uff1a\n>   \n> File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 372, in __init__\n>     assert self.quant_method is not None\n>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n> AssertionError \n\nError is the same as this issue: https://github.com/vllm-project/vllm/issues/15101 \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-04-10T03:42:15+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16386/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16386"
  },
  {
    "number": 11266,
    "title": "[Bug]: Failed to run docker vllm-cpu-env arm docker on MacOS",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nAfter building Docker Images with [Dockerfile.arm](https://docs.vllm.ai/en/latest/getting_started/arm-installation.html), it built successfully but when attempts to run `docker run -it \\\r\n             --rm \\\r\n             --network=host \\\r\n             vllm-cpu-env --device=\"cpu\" --disable_async_output_proc --enforce-eager --model=Qwen/Qwen2.5-1.5B-Instruct --dtype=float16`. it gets error in :\r\n`File \"/usr/local/lib/python3.10/dist-packages/vllm/utils.py\", line 1639, in resolve_obj_by_qualname\r\n    module_name, obj_name = qualname.rsplit(\".\", 1)\r\n`\r\nI am running on MacStudio Ultra and env is collected by building `Dockerfile.arm` file by executing `docker build -f Dockerfile.arm -t vllm-cpu-env --shm-size=4g .`\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-12-17T18:51:42+00:00",
    "closed_at": "2024-12-18T16:05:04+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11266/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11266"
  },
  {
    "number": 3546,
    "title": "[New Model]: CohereForCausalLM Support request ",
    "body": "### The model to consider.\n\nValueError: Model architectures ['CohereForCausalLM'] are not supported for now\r\n\r\n![1711001492046](https://github.com/vllm-project/vllm/assets/7098003/c9f431ec-bb69-4903-9bdd-0073fef03ade)\r\n\n\n### The closest model vllm already supports.\n\n_No response_\n\n### What's your difficulty of supporting the model you want?\n\n_No response_",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2024-03-21T06:11:59+00:00",
    "closed_at": "2024-03-21T06:17:40+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3546/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3546"
  },
  {
    "number": 935,
    "title": "Running multiple cards in parallel is slower(nearly twice) than a single card",
    "body": "Hi, when I am running four A100 with parameter tensor_parallel_size is 4 in parallel, I found that the speed  is slower(nearly twice) than a single card. can you explain what causes this and how to solve it. Thank you.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-09-03T02:40:06+00:00",
    "closed_at": "2024-03-08T11:13:14+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/935/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/935"
  },
  {
    "number": 9377,
    "title": "[Bug]: Model architectures ['LlavaForConditionalGeneration'] are not supported for now.",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nWARNING 10-15 15:24:09 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\r\nWarning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-118-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.6.68\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA L20\r\nNvidia driver version: 550.90.12\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.4.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        52 bits physical, 57 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               160\r\nOn-line CPU(s) list:                  0-159\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Xeon(R) Platinum 8460Y+\r\nCPU family:                           6\r\nModel:                                143\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   40\r\nSocket(s):                            2\r\nStepping:                             8\r\nCPU max MHz:                          3700.0000\r\nCPU min MHz:                          800.0000\r\nBogoMIPS:                             4000.00\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                       VT-x\r\nL1d cache:                            3.8 MiB (80 instances)\r\nL1i cache:                            2.5 MiB (80 instances)\r\nL2 cache:                             160 MiB (80 instances)\r\nL3 cache:                             210 MiB (2 instances)\r\nNUMA node(s):                         2\r\nNUMA node0 CPU(s):                    0-39,80-119\r\nNUMA node1 CPU(s):                    40-79,120-159\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cudnn-frontend==1.6.0\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-dali-cuda120==1.41.0\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-modelopt==0.15.1\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvimgcodec-cu12==0.3.0.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.77\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] nvidia-pyindex==1.0.9\r\n[pip3] onnx==1.16.2\r\n[pip3] optree==0.12.1\r\n[pip3] pynvml==11.4.1\r\n[pip3] pytorch-triton==3.0.0+dedb7bdf3\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torch_tensorrt==2.5.0a0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.2\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.3.dev189+g16b24e7d\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nvllm serve llava-hf/llava-1.5-13b-hf  --dtype auto --api-key token-abc123\r\n\r\n\r\nValueError: Model architectures ['LlavaForConditionalGeneration'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'JambaForCausalLM', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MambaForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi3SmallForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'XverseForCausalLM', 'BartModel', 'BartForConditionalGeneration', 'MistralModel', 'Qwen2ForRewardModel', 'Gemma2Model', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'MolmoForCausalLM', 'NVLM_D', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'Qwen2VLForConditionalGeneration', 'UltravoxModel', 'MllamaForConditionalGeneration', 'EAGLEModel', 'MedusaModel', 'MLPSpeculatorPreTrainedModel']\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-10-15T15:26:44+00:00",
    "closed_at": "2024-10-16T08:47:29+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9377/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9377"
  },
  {
    "number": 11446,
    "title": "[Bug]: Qwen2.5-Math-RM-72B Online Inference Fails",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: 14.0.0-1ubuntu1.1\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-1025-gcp-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 1: NVIDIA H100 80GB HBM3\r\nGPU 2: NVIDIA H100 80GB HBM3\r\nGPU 3: NVIDIA H100 80GB HBM3\r\nGPU 4: NVIDIA H100 80GB HBM3\r\nGPU 5: NVIDIA H100 80GB HBM3\r\nGPU 6: NVIDIA H100 80GB HBM3\r\nGPU 7: NVIDIA H100 80GB HBM3\r\n\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             208\r\nOn-line CPU(s) list:                0-207\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8481C CPU @ 2.70GHz\r\nCPU family:                         6\r\nModel:                              143\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 52\r\nSocket(s):                          2\r\nStepping:                           8\r\nBogoMIPS:                           5399.99\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rtm avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 arat avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk amx_bf16 avx512_fp16 amx_tile amx_int8 arch_capabilities\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          4.9 MiB (104 instances)\r\nL1i cache:                          3.3 MiB (104 instances)\r\nL2 cache:                           208 MiB (104 instances)\r\nL3 cache:                           210 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-51,104-155\r\nNUMA node1 CPU(s):                  52-103,156-207\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI Syscall hardening, KVM SW loop\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.47.1\r\n[pip3] triton==3.1.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.5\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\n\u001b[4mGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\r\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t0-51,104-155\t0\t\tN/A\r\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t0-51,104-155\t0\t\tN/A\r\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\t0-51,104-155\t0\t\tN/A\r\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\t0-51,104-155\t0\t\tN/A\r\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\t52-103,156-207\t1\t\tN/A\r\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\t52-103,156-207\t1\t\tN/A\r\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\t52-103,156-207\t1\t\tN/A\r\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \t52-103,156-207\t1\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nCUDA_ROOT=/usr/local/cuda\r\nLD_LIBRARY_PATH=/home/sam_passaglia_elyza_ai/easy-evals/vllm_server/.venv/vllm-server-3fpuUD1n-py3.10/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:\r\nCUDA_INC_DIR=/usr/local/cuda/bin:\r\nCUDA_MODULE_LOADING=LAZY\r\n\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nUsing the example script from [the original implementation PR](https://github.com/vllm-project/vllm/pull/8896) now leads to the following error:\r\n\r\n```\r\nopenai.BadRequestError: Error code: 400 - {'object': 'error', 'message': 'pooled_data should b\r\ne a 1-D embedding vector', 'type': 'BadRequestError', 'param': None, 'code': 400}             \r\n```\r\n\r\nThe error can be traced back to the pooling API refactoring in https://github.com/vllm-project/vllm/pull/11129/\r\n\r\nReward Models usually use [AllPool](https://github.com/vllm-project/vllm/blob/048fc57a0fb599a3e39bbc9228432b0d1bb9e88d/vllm/model_executor/layers/pooler.py#L130), which usually returns one output per token per prompt as a list[tensor](See also: https://github.com/vllm-project/vllm/pull/10820).\r\n\r\nBut the EmbeddingOutput dataclass expects `pooled_data.ndim = 1` https://github.com/vllm-project/vllm/blob/a491d6f535d96939d17e5290991dc975495c9580/vllm/outputs.py#L407-L408\r\n\r\ncc @DarkLight1337 \r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-12-24T03:05:09+00:00",
    "closed_at": "2024-12-24T09:54:31+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11446/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11446"
  },
  {
    "number": 10932,
    "title": "[Usage]: Persistent Errors with vllm serve on Neuron Device: Model architectures ['LlamaForCausalLM'] failed to be inspected. ",
    "body": "### Your current environment\n\nHello vLLM Development Team,\r\nI am encountering persistent issues when trying to run the ```vllm serve``` command for the ```meta-llama/Llama-3.2-1B``` model on an AWS EC2 inf2 instance with the Neuron AMI. Despite following all the recommended installation and upgrade steps, and adjusting the numpy versions as per the guidelines, the issue persists.\r\n\r\nI already referred the issues I could find such as:\r\n\r\nhttps://github.com/vllm-project/vllm/issues/9624\r\nhttps://github.com/vllm-project/vllm/issues/9713\r\nhttps://github.com/vllm-project/vllm/issues/9624\r\n\r\nHere is the way I installed the vllm under the instruction guideline through:\r\n[](https://docs.vllm.ai/en/latest/getting_started/neuron-installation.html)\r\n<img width=\"1145\" alt=\"image\" src=\"https://github.com/user-attachments/assets/66a1bb4b-31f7-44b7-a14a-9b69bd2e719a\">\r\n\r\nI already tried to reinstall or upgrade the vllm under the instruction above many times, also tried to set the numpy versions. Still I cannot solve the problem when I tried to run the ```vllm serve meta-llama/Llama-3.2-1B --device neuron --tensor-parallel-size 2 --block-size 8 --max-model-len 4096 --max-num-seqs 32```\r\n\r\n\r\nIt constantly shows the error here:\r\n``` ValueError: Model architectures ['LlamaForCausalLM'] failed to be inspected. Please check the logs for more details.```\r\n\r\n<img width=\"1081\" alt=\"image\" src=\"https://github.com/user-attachments/assets/2bddde45-f510-49c9-aef5-3b1ba56cfcaa\">\r\n\r\n\r\nHere is my environment:\r\n```\r\nCollecting environment information...\r\nPyTorch version: 2.1.2+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.22.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1031-aws-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   48 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          4\r\nOn-line CPU(s) list:             0-3\r\nVendor ID:                       AuthenticAMD\r\nModel name:                      AMD EPYC 7R13 Processor\r\nCPU family:                      25\r\nModel:                           1\r\nThread(s) per core:              2\r\nCore(s) per socket:              2\r\nSocket(s):                       1\r\nStepping:                        1\r\nBogoMIPS:                        5299.99\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       64 KiB (2 instances)\r\nL1i cache:                       64 KiB (2 instances)\r\nL2 cache:                        1 MiB (2 instances)\r\nL3 cache:                        8 MiB (1 instance)\r\nNUMA node(s):                    1\r\nNUMA node0 CPU(s):               0-3\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.25.2\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==8.9.2.26\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-nccl-cu12==2.18.1\r\n[pip3] nvidia-nvjitlink-cu12==12.6.85\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.1.2\r\n[pip3] torch-neuronx==2.1.2.2.3.2\r\n[pip3] torch-xla==2.1.5\r\n[pip3] torchvision==0.16.2\r\n[pip3] transformers==4.46.3\r\n[pip3] transformers-neuronx==0.12.313\r\n[pip3] triton==2.1.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: (0, 'instance-type: inf2.xlarge\\ninstance-id: i-012f1753f01231818\\n+--------+--------+--------+---------+\\n| NEURON | NEURON | NEURON |   PCI   |\\n| DEVICE | CORES  | MEMORY |   BDF   |\\n+--------+--------+--------+---------+\\n| 0      | 2      | 32 GB  | 00:1f.0 |\\n+--------+--------+--------+---------+', '')\r\nvLLM Version: 0.6.4.post2.dev246+g9743d64e\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n\r\nLD_LIBRARY_PATH=/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/lib:/usr/lib\r\n```\r\n\r\n\r\n\r\n\r\n\r\nHere is the error log:\r\n\r\n ```ERROR 12-05 18:38:12 engine.py:366] ValueError: Model architectures ['LlamaForCausalLM'] failed to be inspected. Please check the logs for more details.\r\nProcess SpawnProcess-1:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 368, in run_mp_engine\r\n    raise e\r\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 357, in run_mp_engine\r\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\r\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 114, in from_engine_args\r\n    engine_config = engine_args.create_engine_config(usage_context)\r\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/engine/arg_utils.py\", line 1010, in create_engine_config\r\n    model_config = self.create_model_config()\r\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/engine/arg_utils.py\", line 938, in create_model_config\r\n    return ModelConfig(\r\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/config.py\", line 279, in __init__\r\n    self.multimodal_config = self._init_multimodal_config(\r\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/config.py\", line 305, in _init_multimodal_config\r\n    if ModelRegistry.is_multimodal_model(architectures):\r\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/model_executor/models/registry.py\", line 462, in is_multimodal_model\r\n    model_cls, _ = self.inspect_model_cls(architectures)\r\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/model_executor/models/registry.py\", line 422, in inspect_model_cls\r\n    return self._raise_for_unsupported(architectures)\r\n  File \"/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/vllm/model_executor/models/registry.py\", line 379, in _raise_for_unsupported\r\n    raise ValueError(\r\nValueError: Model architectures ['LlamaForCausalLM'] failed to be inspected. Please check the logs for more details.\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n```\r\nHere is my pip list:\r\nabsl-py                           2.1.0\r\naccelerate                        1.1.1\r\naiohappyeyeballs                  2.4.4\r\naiohttp                           3.11.9\r\naiosignal                         1.3.1\r\nannotated-types                   0.7.0\r\nanyio                             4.7.0\r\nargon2-cffi                       23.1.0\r\nargon2-cffi-bindings              21.2.0\r\narrow                             1.3.0\r\nasttokens                         3.0.0\r\nasync-lru                         2.0.4\r\nasync-timeout                     5.0.1\r\nattrs                             24.2.0\r\naws-neuronx-runtime-discovery     2.9\r\nawscli                            1.36.17\r\nbabel                             2.16.0\r\nbeautifulsoup4                    4.12.3\r\nbleach                            6.2.0\r\nblinker                           1.9.0\r\nboto3                             1.35.76\r\nbotocore                          1.35.76\r\ncachetools                        5.5.0\r\ncertifi                           2024.8.30\r\ncffi                              1.17.1\r\ncharset-normalizer                3.4.0\r\nclick                             8.1.7\r\ncloud-tpu-client                  0.10\r\ncloudpickle                       3.1.0\r\ncolorama                          0.4.6\r\ncomm                              0.2.2\r\ncompressed-tensors                0.8.0\r\ndatasets                          2.19.1\r\ndebugpy                           1.8.9\r\ndecorator                         5.1.1\r\ndefusedxml                        0.7.1\r\ndill                              0.3.8\r\ndiskcache                         5.6.3\r\ndistro                            1.9.0\r\ndocutils                          0.16\r\nec2-metadata                      2.14.0\r\neinops                            0.8.0\r\nenvironment-kernels               1.2.0\r\nexceptiongroup                    1.2.2\r\nexecuting                         2.1.0\r\nfastapi                           0.115.6\r\nfastjsonschema                    2.21.1\r\nfilelock                          3.16.1\r\nFlask                             3.1.0\r\nfqdn                              1.5.1\r\nfrozenlist                        1.5.0\r\nfsspec                            2024.3.1\r\ngguf                              0.10.0\r\ngoogle-api-core                   1.34.1\r\ngoogle-api-python-client          1.8.0\r\ngoogle-auth                       2.36.0\r\ngoogle-auth-httplib2              0.2.0\r\ngoogleapis-common-protos          1.66.0\r\nh11                               0.14.0\r\nhttpcore                          1.0.7\r\nhttplib2                          0.22.0\r\nhttptools                         0.6.4\r\nhttpx                             0.28.0\r\nhuggingface-hub                   0.26.3\r\nidna                              3.10\r\nimportlib_metadata                8.5.0\r\niniconfig                         2.0.0\r\ninteregular                       0.3.3\r\nipykernel                         6.29.5\r\nipython                           8.30.0\r\nipywidgets                        8.1.5\r\nislpy                             2023.2.5\r\nisoduration                       20.11.0\r\nitsdangerous                      2.2.0\r\njedi                              0.19.2\r\nJinja2                            3.1.4\r\njiter                             0.8.0\r\njmespath                          1.0.1\r\njson5                             0.10.0\r\njsonpointer                       3.0.0\r\njsonschema                        4.23.0\r\njsonschema-specifications         2024.10.1\r\njupyter                           1.1.1\r\njupyter_client                    8.6.3\r\njupyter-console                   6.6.3\r\njupyter_core                      5.7.2\r\njupyter-events                    0.10.0\r\njupyter-lsp                       2.2.5\r\njupyter_server                    2.14.2\r\njupyter_server_terminals          0.5.3\r\njupyterlab                        4.3.2\r\njupyterlab_pygments               0.3.0\r\njupyterlab_server                 2.27.3\r\njupyterlab_widgets                3.0.13\r\nlark                              1.2.2\r\nlibneuronxla                      2.0.5347.0\r\nllvmlite                          0.43.0\r\nlm-format-enforcer                0.10.9\r\nlockfile                          0.12.2\r\nMarkupSafe                        3.0.2\r\nmatplotlib-inline                 0.1.7\r\nmistral_common                    1.5.1\r\nmistune                           3.0.2\r\nml-dtypes                         0.2.0\r\nmpmath                            1.3.0\r\nmsgspec                           0.18.6\r\nmultidict                         6.1.0\r\nmultiprocess                      0.70.16\r\nnbclient                          0.10.1\r\nnbconvert                         7.16.4\r\nnbformat                          5.10.4\r\nnest-asyncio                      1.6.0\r\nnetworkx                          2.8.8\r\nneuronx-cc                        2.15.143.0+e39249ad\r\nnotebook                          7.3.1\r\nnotebook_shim                     0.2.4\r\nnumba                             0.60.0\r\nnumpy                             1.25.2\r\nnvidia-cublas-cu12                12.1.3.1\r\nnvidia-cuda-cupti-cu12            12.1.105\r\nnvidia-cuda-nvrtc-cu12            12.1.105\r\nnvidia-cuda-runtime-cu12          12.1.105\r\nnvidia-cudnn-cu12                 8.9.2.26\r\nnvidia-cufft-cu12                 11.0.2.54\r\nnvidia-curand-cu12                10.3.2.106\r\nnvidia-cusolver-cu12              11.4.5.107\r\nnvidia-cusparse-cu12              12.1.0.106\r\nnvidia-nccl-cu12                  2.18.1\r\nnvidia-nvjitlink-cu12             12.6.85\r\nnvidia-nvtx-cu12                  12.1.105\r\noauth2client                      4.1.3\r\nopenai                            1.57.0\r\nopencv-python-headless            4.10.0.84\r\noutlines                          0.0.46\r\noverrides                         7.7.0\r\npackaging                         24.2\r\npandas                            2.2.3\r\npandocfilters                     1.5.1\r\nparso                             0.8.4\r\npartial-json-parser               0.2.1.1.post4\r\npexpect                           4.9.0\r\npgzip                             0.3.5\r\npillow                            10.4.0\r\npip                               22.0.2\r\nplatformdirs                      4.3.6\r\npluggy                            1.5.0\r\nprometheus_client                 0.21.1\r\nprometheus-fastapi-instrumentator 7.0.0\r\nprompt_toolkit                    3.0.48\r\npropcache                         0.2.1\r\nprotobuf                          3.20.3\r\npsutil                            6.1.0\r\nptyprocess                        0.7.0\r\npure_eval                         0.2.3\r\npy-cpuinfo                        9.0.0\r\npyairports                        2.1.1\r\npyarrow                           18.1.0\r\npyarrow-hotfix                    0.6\r\npyasn1                            0.6.1\r\npyasn1_modules                    0.4.1\r\npybind11                          2.13.6\r\npycountry                         24.6.1\r\npycparser                         2.22\r\npydantic                          2.10.3\r\npydantic_core                     2.27.1\r\nPygments                          2.18.0\r\npyparsing                         3.2.0\r\npytest                            8.3.4\r\npython-daemon                     3.1.2\r\npython-dateutil                   2.9.0.post0\r\npython-dotenv                     1.0.1\r\npython-json-logger                2.0.7\r\npytz                              2024.2\r\nPyYAML                            6.0.2\r\npyzmq                             26.2.0\r\nreferencing                       0.35.1\r\nregex                             2024.11.6\r\nrequests                          2.31.0\r\nrequests-unixsocket               0.3.0\r\nrfc3339-validator                 0.1.4\r\nrfc3986-validator                 0.1.1\r\nrpds-py                           0.22.3\r\nrsa                               4.7.2\r\ns3transfer                        0.10.4\r\nsafetensors                       0.4.6.dev0\r\nscipy                             1.11.2\r\nSend2Trash                        1.8.3\r\nsentencepiece                     0.2.0\r\nsetuptools                        59.6.0\r\nsix                               1.17.0\r\nsniffio                           1.3.1\r\nsoupsieve                         2.6\r\nstack-data                        0.6.3\r\nstarlette                         0.41.3\r\nsympy                             1.13.3\r\nterminado                         0.18.1\r\ntiktoken                          0.7.0\r\ntinycss2                          1.4.0\r\ntokenizers                        0.20.4rc0\r\ntomli                             2.2.1\r\ntorch                             2.1.2\r\ntorch-neuronx                     2.1.2.2.3.2\r\ntorch-xla                         2.1.5\r\ntorchvision                       0.16.2\r\ntornado                           6.4.2\r\ntqdm                              4.67.1\r\ntraitlets                         5.14.3\r\ntransformers                      4.46.3\r\ntransformers-neuronx              0.12.313\r\ntriton                            2.1.0\r\ntypes-python-dateutil             2.9.0.20241003\r\ntyping_extensions                 4.12.2\r\ntzdata                            2024.2\r\nuri-template                      1.3.0\r\nuritemplate                       3.0.1\r\nurllib3                           2.2.3\r\nuvicorn                           0.32.1\r\nuvloop                            0.21.0\r\nvllm                              0.6.4.post2.dev246+g9743d64e.neuron215\r\nwatchfiles                        1.0.0\r\nwcwidth                           0.2.13\r\nwebcolors                         24.11.1\r\nwebencodings                      0.5.1\r\nwebsocket-client                  1.8.0\r\nwebsockets                        14.1\r\nWerkzeug                          3.1.3\r\nwget                              3.2\r\nwidgetsnbextension                4.0.13\r\nxgrammar                          0.1.5\r\nxxhash                            3.5.0\r\nyarl                              1.18.3\r\nzipp                              3.21.0\r\n```\r\nI am reaching out to ask for your expert advice on how to proceed or if there are any additional steps you could suggest to help resolve this issue. Any assistance you can provide would be greatly appreciated.\r\n\r\n\n\n### How would you like to use vllm\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-12-05T18:49:34+00:00",
    "closed_at": "2024-12-09T21:53:25+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10932/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10932"
  },
  {
    "number": 15123,
    "title": "[RFC]: layer-wise kv cache offloading to enable larger batches",
    "body": "### Motivation.\n\nI tested on some large models like qwen-32B on H100.\n\nThere are totally 64 layers. \n\nThe compute cost for each layer is about 470 \u03bcs, and the transfer of the kv cache tensor for a layer is 10 ms.\n\nIf we offload the gpu kv cache to cpu, and load it back ahead of 32 layers, we can enable double batches.\n\nIs there anyone doing the same thing? \n\nI draw a picture with 6 layers and 2 blocks share the same gpu cache.\n![Image](https://github.com/user-attachments/assets/0f871ced-0b0c-4b05-be48-8bfce2a619c9)\n\n### Proposed Change.\n\nThe kv cache manager and the attention layer\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n_No response_\n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-19T11:04:38+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15123/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15123"
  },
  {
    "number": 15078,
    "title": "[Bug]: new bug after loosening type check on `llava_onevision.py`",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nCollecting environment information...\nPyTorch version: 2.5.1+cu118\nIs debug build: False\nCUDA used to build PyTorch: 11.8\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.6 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\nClang version: Could not collect\nCMake version: version 3.16.3\nLibc version: glibc-2.31\n\nPython version: 3.12.9 | packaged by conda-forge | (main, Mar  4 2025, 22:48:41) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-5.4.0-146-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: 12.0.140\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA GeForce RTX 4090\nGPU 1: NVIDIA GeForce RTX 4090\nGPU 2: NVIDIA GeForce RTX 4090\nGPU 3: NVIDIA GeForce RTX 4090\nGPU 4: NVIDIA GeForce RTX 4090\nGPU 5: NVIDIA GeForce RTX 4090\nGPU 6: NVIDIA GeForce RTX 4090\nGPU 7: NVIDIA GeForce RTX 4090\n\nNvidia driver version: 525.105.17\ncuDNN version: Probably one of the following:\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn.so.8.0.5\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.0.5\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.0.5\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.0.5\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.0.5\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.0.5\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.0.5\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn.so.8.4.0\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.4.0\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.4.0\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.4.0\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.4.0\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.4.0\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.4.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn.so.8.9.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.9.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.9.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.9.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.9.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.9.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.9.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nByte Order:                      Little Endian\nAddress sizes:                   46 bits physical, 57 bits virtual\nCPU(s):                          128\nOn-line CPU(s) list:             0-127\nThread(s) per core:              2\nCore(s) per socket:              32\nSocket(s):                       2\nNUMA node(s):                    2\nVendor ID:                       GenuineIntel\nCPU family:                      6\nModel:                           106\nModel name:                      Intel(R) Xeon(R) Gold 6338 CPU @ 2.00GHz\nStepping:                        6\nFrequency boost:                 enabled\nCPU MHz:                         1285.322\nCPU max MHz:                     3200.0000\nCPU min MHz:                     800.0000\nBogoMIPS:                        4000.00\nVirtualization:                  VT-x\nL1d cache:                       3 MiB\nL1i cache:                       2 MiB\nL2 cache:                        80 MiB\nL3 cache:                        96 MiB\nNUMA node0 CPU(s):               0-31,64-95\nNUMA node1 CPU(s):               32-63,96-127\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:          Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu11==11.11.3.6\n[pip3] nvidia-cuda-cupti-cu11==11.8.87\n[pip3] nvidia-cuda-nvrtc-cu11==11.8.89\n[pip3] nvidia-cuda-runtime-cu11==11.8.89\n[pip3] nvidia-cudnn-cu11==9.1.0.70\n[pip3] nvidia-cufft-cu11==10.9.0.58\n[pip3] nvidia-curand-cu11==10.3.0.86\n[pip3] nvidia-cusolver-cu11==11.4.1.48\n[pip3] nvidia-cusparse-cu11==11.7.5.86\n[pip3] nvidia-nccl-cu11==2.21.5\n[pip3] nvidia-nvtx-cu11==11.8.86\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1+cu118\n[pip3] torchaudio==2.5.1+cu118\n[pip3] torchvision==0.20.1+cu118\n[pip3] transformers==4.49.0\n[pip3] triton==3.1.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu11        11.11.3.6                pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu11    11.8.87                  pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu11    11.8.89                  pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu11  11.8.89                  pypi_0    pypi\n[conda] nvidia-cudnn-cu11         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu11         10.9.0.58                pypi_0    pypi\n[conda] nvidia-curand-cu11        10.3.0.86                pypi_0    pypi\n[conda] nvidia-cusolver-cu11      11.4.1.48                pypi_0    pypi\n[conda] nvidia-cusparse-cu11      11.7.5.86                pypi_0    pypi\n[conda] nvidia-nccl-cu11          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvtx-cu11          11.8.86                  pypi_0    pypi\n[conda] pyzmq                     26.2.1                   pypi_0    pypi\n[conda] torch                     2.5.1+cu118              pypi_0    pypi\n[conda] torchaudio                2.5.1+cu118              pypi_0    pypi\n[conda] torchvision               0.20.1+cu118             pypi_0    pypi\n[conda] transformers              4.49.0                   pypi_0    pypi\n[conda] triton                    3.1.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.3\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n\ufffd[4mGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tCPU Affinity\tNUMA Affinity\ufffd[0m\nGPU0\t X \tPXB\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\t0-31,64-95\t0\nGPU1\tPXB\t X \tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tPIX\tPIX\t0-31,64-95\t0\nGPU2\tPXB\tPXB\t X \tPIX\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\t0-31,64-95\t0\nGPU3\tPXB\tPXB\tPIX\t X \tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\t0-31,64-95\t0\nGPU4\tSYS\tSYS\tSYS\tSYS\t X \tPXB\tPXB\tPXB\tSYS\tSYS\t32-63,96-127\t1\nGPU5\tSYS\tSYS\tSYS\tSYS\tPXB\t X \tPXB\tPXB\tSYS\tSYS\t32-63,96-127\t1\nGPU6\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\t X \tPIX\tSYS\tSYS\t32-63,96-127\t1\nGPU7\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tPIX\t X \tSYS\tSYS\t32-63,96-127\t1\nNIC0\tPXB\tPIX\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\t X \tPIX\t\t\nNIC1\tPXB\tPIX\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tPIX\t X \t\t\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n\nLD_LIBRARY_PATH=/home/phd-chen.yirong2/anaconda3/envs/vllm/lib/python3.12/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:\nCUDA_HOME=/usr/local/cuda/\nCUDA_HOME=/usr/local/cuda/\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nHi, @DarkLight1337 \nThis is a bug report when I'm trying #15021 to fix #15017.\n\nAfter I tried the specific change to `llava_onevision.py` following #15021, I came across a new bug like this:\n```text\nERROR 03-18 13:40:00 engine.py:140]   File \".../anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/llava_onevision.py\", line 952, in forward                                                    \nERROR 03-18 13:40:00 engine.py:140]     inputs_embeds = self.get_input_embeddings_v0(                                                                                                                                                       \nERROR 03-18 13:40:00 engine.py:140]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                       \nERROR 03-18 13:40:00 engine.py:140]   File \".../anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/llava_onevision.py\", line 913, in get_input_embeddings_v0                                    \nERROR 03-18 13:40:00 engine.py:140]     video_embeds = self._process_video_pixels(video_input)                                                                                                                                              \nERROR 03-18 13:40:00 engine.py:140]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                              \nERROR 03-18 13:40:00 engine.py:140]   File \".../anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/llava_onevision.py\", line 828, in _process_video_pixels                                      \nERROR 03-18 13:40:00 engine.py:140]     num_videos, frames, c, h, w = video_pixel.shape                                                                                                                                                     \nERROR 03-18 13:40:00 engine.py:140]                                   ^^^^^^^^^^^^^^^^^                                                                                                                                                     \nERROR 03-18 13:40:00 engine.py:140] AttributeError: 'list' object has no attribute 'shape'   \n```\n\nI checked the var `pixel_values_videos` (in my local code it's `pixel_values`) in function `_parse_and_validate_video_input()` with different number of videos (1, 2 and 4). And I got this output:\n```shell\nlen(pixel_values)=1 # 1 video\nlen(pixel_values)=1, len(pixel_values[0])=2, pixel_values[0]=[tensor(...), tensor(...)] # 2 videos\nlen(pixel_values)=1, len(pixel_values[0])=4, pixel_values[0]=[tensor(...), tensor(...), tensor(...), tensor(...)] # 4 videos\n```\nIt ran successfully when I input only 1 video, but went wrong with 2 or more videos. Note that I didn't check `pixel_values[0]` with 1 video input because it's a list of `torch.Tensor` since it ran successfully.\n\nIt seems the problem is with `pixel_values_videos`. It's supposed to be a list of at least two `torch.Tensor` objects, but it turns out to be a list of a list of `torch.Tensor` objects.\n\nSo, I kind of feel that the change we made to the `_parse_and_validate_video_input()` function in #15021 might not be the right way to go. And I think we should try to find out what's wrong with `pixel_values_videos` before it's passed into the function. What do u think?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-19T03:19:23+00:00",
    "closed_at": "2025-03-20T11:24:46+00:00",
    "comments": 24,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15078/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15078"
  },
  {
    "number": 11702,
    "title": "[Bug]: vLLM LoRA Crash when using Dynamic Loading",
    "body": "### Your current environment\r\n\r\n```\r\nroot@mistral-7b-lora-7946cc6459-jqx4h:/vllm-workspace# python3 collect_env.py \r\nCollecting environment information...\r\nPyTorch version: 2.5.0+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.12.7 (main, Oct  1 2024, 08:52:12) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.6.41-amd64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3\r\nNvidia driver version: 535.86.10\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        46 bits physical, 57 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               192\r\nOn-line CPU(s) list:                  0-191\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Xeon(R) Platinum 8468\r\nCPU family:                           6\r\nModel:                                143\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   48\r\nSocket(s):                            2\r\nStepping:                             8\r\nCPU max MHz:                          3800.0000\r\nCPU min MHz:                          800.0000\r\nBogoMIPS:                             4200.00\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                       VT-x\r\nL1d cache:                            4.5 MiB (96 instances)\r\nL1i cache:                            3 MiB (96 instances)\r\nL2 cache:                             192 MiB (96 instances)\r\nL3 cache:                             210 MiB (2 instances)\r\nNUMA node(s):                         2\r\nNUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190\r\nNUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159,161,163,165,167,169,171,173,175,177,179,181,183,185,187,189,191\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.6+cu121torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.0\r\n[pip3] torchvision==0.20.0\r\n[pip3] transformers==4.46.1\r\n[pip3] triton==3.1.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.1.dev1+g7b5f655 (git sha: 7b5f655\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0,2,4,6,8,10    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNVIDIA_VISIBLE_DEVICES=GPU-710a76e8-c24c-99c5-d8a8-a734dbc7e932\r\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\r\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\r\nVLLM_USAGE_SOURCE=production-docker-image\r\nCUDA_VERSION=12.4.1\r\nVLLM_ALLOW_RUNTIME_LORA_UPDATING=True\r\nLD_LIBRARY_PATH=/usr/local/lib/python3.12/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nVLLM_NO_USAGE_STATS=1\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nWe encountered a 500 error while testing the dynamic loading of LoRA with vLLM.\r\n## Steps to Reproduce:\r\n\r\n\r\n### Load LoRA dynamically:\r\n```\r\ncurl -X 'POST' \\\r\n  'https://llm-lora-demo.ssdl.only.sap/v1/load_lora_adapter' \\\r\n  -H 'accept: application/json' \\\r\n  -H 'Content-Type: application/json' \\\r\n  -d '{\r\n  \"lora_name\": \"Mistral-7B-Instruct-v0.3-lora\",\r\n  \"lora_path\": \"ssdl-lora/Mistral-7B-Instruct-v0.3-lora/\"\r\n}'\r\n```\r\n\r\n### The LoRA was loaded correctly as confirmed:\r\n```\r\ncurl -X 'GET' \\\r\n  'https://llm-lora-demo.ssdl.only.sap/v1/models' \\\r\n  -H 'accept: application/json'\r\n\r\n{\r\n  \"id\": \"Mistral-7B-Instruct-v0.3-lora\",\r\n  \"object\": \"model\",\r\n  \"created\": 1735799863,\r\n  \"owned_by\": \"vllm\",\r\n  \"root\": \"ssdl-lora/Mistral-7B-Instruct-v0.3-lora/\",\r\n  \"parent\": \"mistralai/Mistral-7B-Instruct-v0.3\",\r\n  \"max_model_len\": null,\r\n  \"permission\": [\r\n    {\r\n      \"id\": \"modelperm-5582e5de3b2c4d7bb4d56af8b10a9ffc\",\r\n      \"object\": \"model_permission\",\r\n      \"created\": 1735799863,\r\n      \"allow_create_engine\": false,\r\n      \"allow_sampling\": true,\r\n      \"allow_logprobs\": true,\r\n      \"allow_search_indices\": false,\r\n      \"allow_view\": true,\r\n      \"allow_fine_tuning\": false,\r\n      \"organization\": \"*\",\r\n      \"group\": null,\r\n      \"is_blocking\": false\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n### Attempt to use the loaded LoRA:\r\n```\r\ncurl -X 'POST' \\\r\n  'https://llm-lora-demo.ssdl.only.sap/v1/chat/completions' \\\r\n  -H 'accept: application/json' \\\r\n  -H 'Content-Type: application/json' \\\r\n  -d '{\r\n  \"model\": \"Mistral-7B-Instruct-v0.3-lora\",\r\n  \"max_tokens\": 200,\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"system\",\r\n      \"content\": \"You are a multilingual expert in Roman history.\"\r\n    },\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"please explain the meaning of \\\"alea iacta est\\\" and its associated story in English.\"\r\n    }\r\n  ]\r\n}'\r\n```\r\n## Observed Error:\r\n```\r\nWARNING 01-01 22:45:09 api_server.py:378] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!\r\nINFO 01-01 22:45:09 api_server.py:531] vLLM API server version 0.1.dev1+g7b5f655\r\nINFO 01-01 22:45:09 api_server.py:532] args: Namespace(subparser='serve', model_tag='mistralai/Mistral-7B-Instruct-v0.3', config='', host='0.0.0.0', port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=[LoRAModulePath(name='e5-mistral-7b', path='/ssdl-lora/e5-mistral-7b-instruct/lora', base_model_name=None)], prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='mistralai/Mistral-7B-Instruct-v0.3', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', chat_template_text_format='string', trust_remote_code=True, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.4, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=True, max_loras=10, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', pooling_type=None, pooling_norm=None, pooling_softmax=None, pooling_step_tag_id=None, pooling_returned_token_ids=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7f4fd2e98040>)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/vllm\", line 8, in <module>\r\n    sys.exit(main())\r\n             ^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/scripts.py\", line 195, in main\r\n    args.dispatch_function(args)\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/scripts.py\", line 41, in serve\r\n    uvloop.run(run_server(args))\r\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\r\n    return __asyncio.run(\r\n           ^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\r\n    return runner.run(main)\r\n           ^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\r\n    return self._loop.run_until_complete(task)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\r\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\r\n    return await main\r\n           ^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 547, in run_server\r\n    sock.bind((args.host, args.port))\r\nOSError: [Errno 98] Address already in use\r\n```\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-01-03T02:49:49+00:00",
    "closed_at": "2025-01-10T07:56:37+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11702/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11702"
  },
  {
    "number": 20368,
    "title": "[Performance]:",
    "body": "### Proposal to improve performance\n\n![Image](https://github.com/user-attachments/assets/7c7d91d1-cf2f-4f91-bee7-1fdfcbd0c15b)\nI've encountered a phenomenon where when running the DeepSeek V2 Lite Chat MoE model with vLLM's v1 engine, increasing the number of activated experts boosts prefill processing speed by 100%-300%, while decoder speed remains unchanged. Since more activated experts increase computational parameters, speed should decrease \u2013 yet it improves. What causes this? I'm not using Expert Parallelism (EP). What mechanisms in vLLM's MoE handling could explain this behavior?\n\n### Report of performance regression\n\n_No response_\n\n### Misc discussion on performance\n\n_No response_\n\n### Your current environment (if you think it is necessary)\n\n```text\nThe output of `python collect_env.py`\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance"
    ],
    "state": "closed",
    "created_at": "2025-07-02T10:33:39+00:00",
    "closed_at": "2025-07-04T09:50:43+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20368/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20368"
  },
  {
    "number": 15608,
    "title": "[Feature]: In multimodal inference, is it possible to cache textual content and only load images each time to optimize inference efficiency",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nIn multimodal inference, is it possible to cache textual content and only load images each time to optimize inference efficiency\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\nIn multimodal inference, is it possible to cache textual content and only load images each time to optimize inference efficiency\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-27T08:30:23+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15608/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15608"
  },
  {
    "number": 20119,
    "title": "[Bug]: Curl failed: Received HTTP/0.9 when not allowed",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n**Issue Description:**\nIn a data-parallel scenario, when the --data-parallel-rpc-port argument value conflicts with the vLLM HTTP service port, the service starts without any error logs despite the port collision.\n\n**Actual Behavior:**\nWhen clients send requests, they only receive the error message: \"Received HTTP/0.9 when not allowed\".\n\n**Expected Behavior:**\nThe vLLM service should either:\n\nDetect the port conflict during startup and fail with a clear error message, or\nDynamically handle the conflict by selecting an alternative port.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-06-26T08:37:53+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20119/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20119"
  },
  {
    "number": 19360,
    "title": "[Bug]: AttributeError: 'Llama_Nemotron_Nano_VL_Config' object has no attribute 'hidden_size'. Did you mean: 'vit_hidden_size'?",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Debian GNU/Linux 12 (bookworm) (x86_64)\nGCC version                  : (Debian 12.2.0-14) 12.2.0\nClang version                : Could not collect\nCMake version                : Could not collect\nLibc version                 : glibc-2.36\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu126\nIs debug build               : False\nCUDA used to build PyTorch   : 12.6\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.6 (main, Sep 27 2024, 06:10:12) [GCC 12.2.0] (64-bit runtime)\nPython platform              : Linux-4.4.0-x86_64-with-glibc2.36\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : Could not collect\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : GPU 0: NVIDIA L40S\nNvidia driver version        : 570.86.15\ncuDNN version                : Could not collect\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:        x86_64\nCPU op-mode(s):      32-bit, 64-bit\nAddress sizes:       46 bits physical, 48 bits virtual\nByte Order:          Little Endian\nCPU(s):              17\nOn-line CPU(s) list: 0-16\nVendor ID:           GenuineIntel\nModel name:          unknown\nCPU family:          6\nModel:               143\nThread(s) per core:  1\nCore(s) per socket:  17\nSocket(s):           1\nStepping:            unknown\nBogoMIPS:            2973.26\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:      VT-x\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-cufile-cu12==1.11.1.6\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.0\n[pip3] torchaudio==2.7.0\n[pip3] torchvision==0.22.0\n[pip3] transformers==4.52.4\n[pip3] triton==3.3.0\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.0.1\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n  \tGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \t\t\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n==============================\n     Environment Variables\n==============================\nNVIDIA_DRIVER_CAPABILITIES=all\nMKL_NUM_THREADS=1\nOMP_NUM_THREADS=1\nNVIDIA_VISIBLE_DEVICES=GPU-50125375-6566-64c2-5e43-aec1bf1cacb0\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nThe model nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1 is causing a failure during vLLM server startup due to a missing hidden_size attribute in its Hugging Face config class (Llama_Nemotron_Nano_VL_Config).\n\nError Trace (critical line):\nAttributeError: 'Llama_Nemotron_Nano_VL_Config' object has no attribute 'hidden_size'. Did you mean: 'vit_hidden_size'?\n\nHow to reproduce:\nvllm serve nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1 --host 0.0.0.0 --port 7000 --trust-remote-code\nAdditionally to run this model via vLLM we need to install:\n\"timm\",\n        \"open-clip-torch\",\n        \"einops\",\n        \"accelerate\"\nlibraries.\nThe model works fine via HF:\n# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"image-text-to-text\", model=\"nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1\")\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-06-09T11:41:59+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19360/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19360"
  },
  {
    "number": 11822,
    "title": "[Usage]: Does vLLM support co-hosting multiple models on single server?",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-01-08T00:41:24+00:00",
    "closed_at": "2025-01-21T19:34:21+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11822/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11822"
  },
  {
    "number": 6979,
    "title": "[Doc]: Supported Hardware for Quantization Kernels",
    "body": "### \ud83d\udcda The doc issue\n\nI'm confused what \"the quantization method is supported\" mean?  Ampere arch doesn't support FP8, according to Nvidia. So does this mean the FP8 operation is supported on A100/A800 GPU?  Or just we can conver the weight parameters form FP16 to FP8? \n\n### Suggest a potential alternative/fix\n\n_No response_",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2024-07-31T07:16:05+00:00",
    "closed_at": "2024-07-31T15:33:53+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6979/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6979"
  }
]
[
  {
    "number": 19530,
    "title": "[Usage]: How can I input aip-key when use benchmark_serving.py to test my model",
    "body": "### Your current environment\n\n```text\nHow can I input aip-key when use `benchmark_serving.py ` to test my model ?\n```\n\n\n### How would you like to use vllm\n\nHow can I input aip-key when use `benchmark_serving.py ` to test my model ?\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "open",
    "created_at": "2025-06-12T07:18:18+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19530/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19530"
  },
  {
    "number": 20712,
    "title": "[Bug]: ERNIE-4.5 does not run on an RTX Pro 6000 Blackwell",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\n$ python collect_env.py\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 24.04.2 LTS (x86_64)\nGCC version                  : (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\nClang version                : Could not collect\nCMake version                : version 3.28.3\nLibc version                 : glibc-2.39\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu128\nIs debug build               : False\nCUDA used to build PyTorch   : 12.8\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0] (64-bit runtime)\nPython platform              : Linux-6.8.0-62-generic-x86_64-with-glibc2.39\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.9.41\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : GPU 0: NVIDIA RTX PRO 6000 Blackwell Workstation Edition\nNvidia driver version        : 575.51.03\ncuDNN version                : Could not collect\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               32\nOn-line CPU(s) list:                  0-31\nVendor ID:                            AuthenticAMD\nModel name:                           AMD Ryzen 9 9950X3D 16-Core Processor\nCPU family:                           26\nModel:                                68\nThread(s) per core:                   2\nCore(s) per socket:                   16\nSocket(s):                            1\nStepping:                             0\nCPU(s) scaling MHz:                   44%\nCPU max MHz:                          5752.0000\nCPU min MHz:                          600.0000\nBogoMIPS:                             8583.68\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk avx_vnni avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd cppc amd_ibpb_ret arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid bus_lock_detect movdiri movdir64b overflow_recov succor smca fsrm avx512_vp2intersect flush_l1d\nVirtualization:                       AMD-V\nL1d cache:                            768 KiB (16 instances)\nL1i cache:                            512 KiB (16 instances)\nL2 cache:                             16 MiB (16 instances)\nL3 cache:                             128 MiB (2 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-31\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu12==12.8.3.14\n[pip3] nvidia-cuda-cupti-cu12==12.8.57\n[pip3] nvidia-cuda-nvrtc-cu12==12.8.61\n[pip3] nvidia-cuda-runtime-cu12==12.8.57\n[pip3] nvidia-cudnn-cu12==9.7.1.26\n[pip3] nvidia-cufft-cu12==11.3.3.41\n[pip3] nvidia-cufile-cu12==1.13.0.11\n[pip3] nvidia-curand-cu12==10.3.9.55\n[pip3] nvidia-cusolver-cu12==11.7.2.55\n[pip3] nvidia-cusparse-cu12==12.5.7.53\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-ml-py==12.575.51\n[pip3] nvidia-modelopt==0.31.0\n[pip3] nvidia-modelopt-core==0.31.0\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.8.61\n[pip3] nvidia-nvtx-cu12==12.8.55\n[pip3] onnx==1.18.0\n[pip3] onnx_graphsurgeon==0.5.8\n[pip3] pynvml==12.0.0\n[pip3] pyzmq==27.0.0\n[pip3] torch==2.7.0+cu128\n[pip3] torchaudio==2.7.0+cu128\n[pip3] torchprofile==0.0.4\n[pip3] torchvision==0.22.0+cu128\n[pip3] transformers==4.53.1\n[pip3] triton==3.3.0\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.2\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n  \tGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \t0-31\t0\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n==============================\n     Environment Variables\n==============================\nLD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib64:\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nServing ERNIE-4.5-21B-A3B does not work on an RTX Pro 6000 Blackwell card. This is using the pip install of vllm 0.9.2 in its own venv.\n\n```\n$ vllm serve ERNIE-4.5-21B-A3B-PT/ --trust-remote-code\n<snip>\nLoading safetensors checkpoint shards:   0% Completed | 0/11 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:   9% Completed | 1/11 [00:00<00:03,  2.67it/s]\nLoading safetensors checkpoint shards:  18% Completed | 2/11 [00:00<00:03,  2.49it/s]\nLoading safetensors checkpoint shards:  27% Completed | 3/11 [00:01<00:03,  2.38it/s]\nLoading safetensors checkpoint shards:  36% Completed | 4/11 [00:01<00:02,  2.38it/s]\nLoading safetensors checkpoint shards:  45% Completed | 5/11 [00:02<00:02,  2.38it/s]\nLoading safetensors checkpoint shards:  55% Completed | 6/11 [00:02<00:02,  2.31it/s]\nLoading safetensors checkpoint shards:  64% Completed | 7/11 [00:03<00:01,  2.24it/s]\nLoading safetensors checkpoint shards:  73% Completed | 8/11 [00:03<00:01,  2.66it/s]\nLoading safetensors checkpoint shards:  82% Completed | 9/11 [00:03<00:00,  2.57it/s]\nLoading safetensors checkpoint shards:  91% Completed | 10/11 [00:04<00:00,  2.43it/s]\nLoading safetensors checkpoint shards: 100% Completed | 11/11 [00:04<00:00,  2.37it/s]\nLoading safetensors checkpoint shards: 100% Completed | 11/11 [00:04<00:00,  2.41it/s]\n\npython3: /project/lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp:42: int mlir::triton::gpu::(anonymous namespace)::getMMAVersionSafe(int, DotOp): Assertion `false && \"computeCapability not supported\"' failed.\nmodule {\n  tt.func public @fused_moe_kernel(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg5: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg6: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: i32 {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32 {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}, %arg17: i32 {tt.divisibility = 16 : i32}, %arg18: i32 {tt.divisibility = 16 : i32}, %arg19: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n    %c31_i32 = arith.constant 31 : i32\n    %cst = arith.constant dense<0.000000e+00> : tensor<64x64xbf16>\n    %c63_i32 = arith.constant 63 : i32\n    %cst_0 = arith.constant dense<0.000000e+00> : tensor<32x64xbf16>\n    %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x32xbf16>\n    %c1_i32 = arith.constant 1 : i32\n    %c0_i32 = arith.constant 0 : i32\n    %c-1_i64 = arith.constant -1 : i64\n    %cst_2 = arith.constant dense<32> : tensor<32x64xi32>\n    %cst_3 = arith.constant dense<32> : tensor<64x32xi32>\n    %cst_4 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>\n    %c32_i32 = arith.constant 32 : i32\n    %cst_5 = arith.constant dense<6> : tensor<64x1xi32>\n    %c64_i32 = arith.constant 64 : i32\n    %c8_i32 = arith.constant 8 : i32\n    %0 = tt.get_program_id x : i32\n    %1 = arith.addi %arg9, %c63_i32 : i32\n    %2 = arith.divsi %1, %c64_i32 : i32\n    %3 = arith.addi %arg7, %c63_i32 : i32\n    %4 = arith.divsi %3, %c64_i32 : i32\n    %5 = arith.muli %4, %c8_i32 : i32\n    %6 = arith.divsi %0, %5 : i32\n    %7 = arith.muli %6, %c8_i32 : i32\n    %8 = arith.subi %2, %7 : i32\n    %9 = arith.minsi %8, %c8_i32 : i32\n    %10 = arith.remsi %0, %5 : i32\n    %11 = arith.remsi %10, %9 : i32\n    %12 = arith.addi %7, %11 : i32\n    %13 = arith.divsi %10, %9 : i32\n    %14 = tt.load %arg6 : !tt.ptr<i32>\n    %15 = arith.muli %12, %c64_i32 : i32\n    %16 = arith.cmpi sge, %15, %14 : i32\n    cf.cond_br %16, ^bb1, ^bb2\n  ^bb1:  // pred: ^bb0\n    tt.return\n  ^bb2:  // pred: ^bb0\n    %17 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n    %18 = arith.extsi %17 : tensor<64xi32> to tensor<64xi64>\n    %19 = arith.extsi %15 : i32 to i64\n    %20 = tt.splat %19 : i64 -> tensor<64xi64>\n    %21 = arith.addi %20, %18 : tensor<64xi64>\n    %22 = tt.splat %arg4 : !tt.ptr<i32> -> tensor<64x!tt.ptr<i32>>\n    %23 = tt.addptr %22, %21 : tensor<64x!tt.ptr<i32>>, tensor<64xi64>\n    %24 = tt.load %23 : tensor<64x!tt.ptr<i32>>\n    %25 = tt.splat %arg10 : i32 -> tensor<64xi32>\n    %26 = arith.cmpi slt, %24, %25 : tensor<64xi32>\n    %27 = tt.addptr %arg5, %12 : !tt.ptr<i32>, i32\n    %28 = tt.load %27 : !tt.ptr<i32>\n    %29 = arith.extsi %28 : i32 to i64\n    %30 = arith.cmpi eq, %29, %c-1_i64 : i64\n    cf.cond_br %30, ^bb3, ^bb4\n  ^bb3:  // pred: ^bb2\n    %31 = arith.muli %13, %c64_i32 : i32\n    %32 = tt.splat %31 : i32 -> tensor<64xi32>\n    %33 = arith.addi %32, %17 : tensor<64xi32>\n    %34 = tt.expand_dims %24 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>\n    %35 = tt.splat %arg14 : i32 -> tensor<64x1xi32>\n    %36 = arith.muli %35, %34 : tensor<64x1xi32>\n    %37 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<64x1x!tt.ptr<bf16>>\n    %38 = tt.addptr %37, %36 : tensor<64x1x!tt.ptr<bf16>>, tensor<64x1xi32>\n    %39 = tt.expand_dims %33 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>\n    %40 = tt.broadcast %38 : tensor<64x1x!tt.ptr<bf16>> -> tensor<64x64x!tt.ptr<bf16>>\n    %41 = tt.broadcast %39 : tensor<1x64xi32> -> tensor<64x64xi32>\n    %42 = tt.addptr %40, %41 : tensor<64x64x!tt.ptr<bf16>>, tensor<64x64xi32>\n    %43 = tt.expand_dims %26 {axis = 1 : i32} : tensor<64xi1> -> tensor<64x1xi1>\n    %44 = tt.splat %arg7 : i32 -> tensor<1x64xi32>\n    %45 = arith.cmpi slt, %39, %44 : tensor<1x64xi32>\n    %46 = tt.broadcast %43 : tensor<64x1xi1> -> tensor<64x64xi1>\n    %47 = tt.broadcast %45 : tensor<1x64xi1> -> tensor<64x64xi1>\n    %48 = arith.andi %46, %47 : tensor<64x64xi1>\n    tt.store %42, %cst, %48 : tensor<64x64x!tt.ptr<bf16>>\n    tt.return\n  ^bb4:  // pred: ^bb2\n    %49 = arith.muli %13, %c64_i32 : i32\n    %50 = arith.extsi %49 : i32 to i64\n    %51 = tt.splat %50 : i64 -> tensor<64xi64>\n    %52 = arith.addi %51, %18 : tensor<64xi64>\n    %53 = arith.extsi %arg7 : i32 to i64\n    %54 = tt.splat %53 : i64 -> tensor<64xi64>\n    %55 = arith.remsi %52, %54 : tensor<64xi64>\n    %56 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>\n    %57 = tt.expand_dims %24 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>\n    %58 = arith.divsi %57, %cst_5 : tensor<64x1xi32>\n    %59 = tt.splat %arg11 : i32 -> tensor<64x1xi32>\n    %60 = arith.muli %58, %59 : tensor<64x1xi32>\n    %61 = tt.expand_dims %56 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>\n    %62 = tt.broadcast %60 : tensor<64x1xi32> -> tensor<64x32xi32>\n    %63 = tt.broadcast %61 : tensor<1x32xi32> -> tensor<64x32xi32>\n    %64 = arith.addi %62, %63 : tensor<64x32xi32>\n    %65 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<64x32x!tt.ptr<bf16>>\n    %66 = tt.addptr %65, %64 : tensor<64x32x!tt.ptr<bf16>>, tensor<64x32xi32>\n    %67 = arith.extsi %arg12 : i32 to i64\n    %68 = arith.muli %29, %67 : i64\n    %69 = tt.addptr %arg1, %68 : !tt.ptr<bf16>, i64\n    %70 = tt.expand_dims %56 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>\n    %71 = tt.expand_dims %55 {axis = 0 : i32} : tensor<64xi64> -> tensor<1x64xi64>\n    %72 = arith.extsi %arg13 : i32 to i64\n    %73 = tt.splat %72 : i64 -> tensor<1x64xi64>\n    %74 = arith.muli %71, %73 : tensor<1x64xi64>\n    %75 = arith.extsi %70 : tensor<32x1xi32> to tensor<32x1xi64>\n    %76 = tt.broadcast %75 : tensor<32x1xi64> -> tensor<32x64xi64>\n    %77 = tt.broadcast %74 : tensor<1x64xi64> -> tensor<32x64xi64>\n    %78 = arith.addi %76, %77 : tensor<32x64xi64>\n    %79 = tt.splat %69 : !tt.ptr<bf16> -> tensor<32x64x!tt.ptr<bf16>>\n    %80 = tt.addptr %79, %78 : tensor<32x64x!tt.ptr<bf16>>, tensor<32x64xi64>\n    %81 = arith.addi %arg8, %c31_i32 : i32\n    %82 = arith.divsi %81, %c32_i32 : i32\n    %83:3 = scf.for %arg20 = %c0_i32 to %82 step %c1_i32 iter_args(%arg21 = %cst_4, %arg22 = %66, %arg23 = %80) -> (tensor<64x64xf32>, tensor<64x32x!tt.ptr<bf16>>, tensor<32x64x!tt.ptr<bf16>>)  : i32 {\n      %101 = tt.expand_dims %26 {axis = 1 : i32} : tensor<64xi1> -> tensor<64x1xi1>\n      %102 = arith.muli %arg20, %c32_i32 : i32\n      %103 = arith.subi %arg8, %102 : i32\n      %104 = tt.splat %103 : i32 -> tensor<1x32xi32>\n      %105 = arith.cmpi slt, %61, %104 : tensor<1x32xi32>\n      %106 = tt.broadcast %101 : tensor<64x1xi1> -> tensor<64x32xi1>\n      %107 = tt.broadcast %105 : tensor<1x32xi1> -> tensor<64x32xi1>\n      %108 = arith.andi %106, %107 : tensor<64x32xi1>\n      %109 = tt.load %arg22, %108, %cst_1 : tensor<64x32x!tt.ptr<bf16>>\n      %110 = tt.splat %103 : i32 -> tensor<32x1xi32>\n      %111 = arith.cmpi slt, %70, %110 : tensor<32x1xi32>\n      %112 = tt.broadcast %111 : tensor<32x1xi1> -> tensor<32x64xi1>\n      %113 = tt.load %arg23, %112, %cst_0 : tensor<32x64x!tt.ptr<bf16>>\n      %114 = tt.dot %109, %113, %arg21, inputPrecision = tf32 : tensor<64x32xbf16> * tensor<32x64xbf16> -> tensor<64x64xf32>\n      %115 = tt.addptr %arg22, %cst_3 : tensor<64x32x!tt.ptr<bf16>>, tensor<64x32xi32>\n      %116 = tt.addptr %arg23, %cst_2 : tensor<32x64x!tt.ptr<bf16>>, tensor<32x64xi32>\n      scf.yield %114, %115, %116 : tensor<64x64xf32>, tensor<64x32x!tt.ptr<bf16>>, tensor<32x64x!tt.ptr<bf16>>\n    }\n    %84 = arith.truncf %83#0 : tensor<64x64xf32> to tensor<64x64xbf16>\n    %85 = tt.splat %49 : i32 -> tensor<64xi32>\n    %86 = arith.addi %85, %17 : tensor<64xi32>\n    %87 = tt.splat %arg14 : i32 -> tensor<64x1xi32>\n    %88 = arith.muli %87, %57 : tensor<64x1xi32>\n    %89 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<64x1x!tt.ptr<bf16>>\n    %90 = tt.addptr %89, %88 : tensor<64x1x!tt.ptr<bf16>>, tensor<64x1xi32>\n    %91 = tt.expand_dims %86 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>\n    %92 = tt.broadcast %90 : tensor<64x1x!tt.ptr<bf16>> -> tensor<64x64x!tt.ptr<bf16>>\n    %93 = tt.broadcast %91 : tensor<1x64xi32> -> tensor<64x64xi32>\n    %94 = tt.addptr %92, %93 : tensor<64x64x!tt.ptr<bf16>>, tensor<64x64xi32>\n    %95 = tt.expand_dims %26 {axis = 1 : i32} : tensor<64xi1> -> tensor<64x1xi1>\n    %96 = tt.splat %arg7 : i32 -> tensor<1x64xi32>\n    %97 = arith.cmpi slt, %91, %96 : tensor<1x64xi32>\n    %98 = tt.broadcast %95 : tensor<64x1xi1> -> tensor<64x64xi1>\n    %99 = tt.broadcast %97 : tensor<1x64xi1> -> tensor<64x64xi1>\n    %100 = arith.andi %98, %99 : tensor<64x64xi1>\n    tt.store %94, %84, %100 : tensor<64x64x!tt.ptr<bf16>>\n    tt.return\n  }\n}\n\n{-#\n  external_resources: {\n    mlir_reproducer: {\n      pipeline: \"builtin.module(convert-triton-to-tritongpu{num-ctas=1 num-warps=4 target=cuda:120 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, loop-invariant-code-motion, tritongpu-optimize-accumulator-init, tritongpu-warp-spec-task-partition{num-consumer-groups=0}, triton-gpu-taskid-propagate{num-consumer-groups=0}, tritongpu-warp-spec-data-partition{num-consumer-groups=0}, tritongpu-warp-spec-code-partition{consumer-reg-inc=0 num-buffers=0 num-consumer-groups=0 producer-reg-dec=0}, tritongpu-pipeline{dump-intermediate-steps=false num-stages=3}, tritongpu-combine-tensor-select-and-if, tritongpu-promote-lhs-to-tmem, tritongpu-keep-acc-in-tmem, tritongpu-warp-spec-lowering{num-consumer-groups=0}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, cse, symbol-dce, triton-nvidia-gpu-fence-insertion{compute-capability=90}, triton-nvidia-tma-lowering, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-warp-spec-canonicalization{num-consumer-groups=0})\",\n      disable_threading: false,\n      verify_each: true\n    }\n  }\n#-}\n/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_moe.py:262:0: error: Failures have been detected while processing an MLIR pass pipeline\n/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_moe.py:262:0: note: Pipeline failed while executing [`TritonGPUAccelerateMatmul` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`\nProcess EngineCore_0:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 590, in run_engine_core\n    raise e\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 577, in run_engine_core\n    engine_core = EngineCoreProc(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 404, in __init__\n    super().__init__(vllm_config, executor_class, log_stats,\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 82, in __init__\n    self._initialize_kv_caches(vllm_config)\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 142, in _initialize_kv_caches\n    available_gpu_memory = self.model_executor.determine_available_memory()\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 76, in determine_available_memory\n    output = self.collective_rpc(\"determine_available_memory\")\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/utils/__init__.py\", line 2736, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 210, in determine_available_memory\n    self.model_runner.profile_run()\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2274, in profile_run\n    = self._dummy_run(self.max_num_tokens, is_profile=True)\n      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2057, in _dummy_run\n    outputs = model(\n              ^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/model_executor/models/ernie45_moe.py\", line 478, in forward\n    hidden_states = self.model(input_ids, positions, intermediate_tensors,\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 239, in __call__\n    output = self.compiled_callable(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 655, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/model_executor/models/ernie45_moe.py\", line 397, in forward\n    def forward(\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 838, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 830, in call_wrapped\n    return self._wrapped_call(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 406, in __call__\n    raise e\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 393, in __call__\n    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<eval_with_key>.58\", line 219, in forward\n    submod_4 = self.submod_4(getitem_8, s0, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_shared_experts_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_shared_experts_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_parameters_weight_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_shared_experts_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_shared_experts_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_parameters_weight_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = None\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/compilation/cuda_piecewise_backend.py\", line 112, in __call__\n    return self.compiled_graph_for_general_shape(*args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 838, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 1201, in forward\n    return compiled_fn(full_args)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 328, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n                            ^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 689, in inner_fn\n    outs = compiled_fn(args)\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 495, in wrapper\n    return compiled_fn(runtime_args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/_inductor/output_code.py\", line 460, in __call__\n    return self.current_callable(inputs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/_inductor/utils.py\", line 2404, in run\n    return model(new_inputs)\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/llm/.cache/vllm/torch_compile_cache/c0948fd008/rank_0_0/inductor_cache/y7/cy7u27yvgznxksbvcoxaxwukuxx6toll6aqyltbxwphvw7jm7iah.py\", line 512, in call\n    buf5 = torch.ops.vllm.moe_forward.default(reinterpret_tensor(buf4, (s0, 2560), (2560, 1), 0), buf3, 'model.layers.1.mlp.experts')\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/_ops.py\", line 756, in __call__\n    return self._op(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 1513, in moe_forward\n    return self.forward_impl(hidden_states, router_logits)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 1422, in forward_impl\n    final_hidden_states = self.quant_method.apply(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 354, in apply\n    return self.forward(\n           ^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/model_executor/custom_op.py\", line 44, in forward\n    return self._forward_method(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 414, in forward_cuda\n    return self.fused_experts(\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1194, in fused_experts\n    return dispatch_fused_experts_func(inplace)(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1118, in torch_vllm_inplace_fused_experts\n    torch.ops.vllm.inplace_fused_experts(**kwargs)\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/torch/_ops.py\", line 1158, in __call__\n    return self._op(*args, **(kwargs or {}))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1011, in inplace_fused_experts\n    fused_experts_impl(hidden_states, w1, w2, topk_weights, topk_ids, True,\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1351, in fused_experts_impl\n    invoke_fused_moe_kernel(qcurr_hidden_states,\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 599, in invoke_fused_moe_kernel\n    fused_moe_kernel[grid](\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/triton/runtime/jit.py\", line 347, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/triton/runtime/jit.py\", line 569, in run\n    kernel = self.compile(src, target=target, options=options.__dict__)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/triton/compiler/compiler.py\", line 284, in compile\n    next_module = compile_ir(module, metadata)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/triton/backends/nvidia/compiler.py\", line 449, in <lambda>\n    stages[\"ttgir\"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)\n                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/triton/backends/nvidia/compiler.py\", line 312, in make_ttgir\n    pm.run(mod)\nRuntimeError: PassManager::run failed\n[rank0]:[W709 22:21:52.995755083 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nTraceback (most recent call last):\n  File \"/home/llm/venv/vllm/bin/vllm\", line 10, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py\", line 65, in main\n    args.dispatch_function(args)\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py\", line 55, in cmd\n    uvloop.run(run_server(args))\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 1431, in run_server\n    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 1451, in run_server_worker\n    async with build_async_engine_client(args, client_config) as engine_client:\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 158, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 194, in build_async_engine_client_from_engine_args\n    async_llm = AsyncLLM.from_vllm_config(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py\", line 162, in from_vllm_config\n    return cls(\n           ^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py\", line 124, in __init__\n    self.engine_core = EngineCoreClient.make_async_mp_client(\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 96, in make_async_mp_client\n    return AsyncMPClient(*client_args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 666, in __init__\n    super().__init__(\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 403, in __init__\n    with launch_core_engines(vllm_config, executor_class,\n  File \"/usr/lib/python3.12/contextlib.py\", line 144, in __exit__\n    next(self.gen)\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/v1/engine/utils.py\", line 434, in launch_core_engines\n    wait_for_engine_startup(\n  File \"/home/llm/venv/vllm/lib/python3.12/site-packages/vllm/v1/engine/utils.py\", line 484, in wait_for_engine_startup\n    raise RuntimeError(\"Engine core initialization failed. \"\nRuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-07-09T22:31:05+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20712/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20712"
  },
  {
    "number": 20148,
    "title": "[CI Failure]: Plugin Tests (2 GPUs) - models/test_oot_registration.py",
    "body": "### Name of failing test\n\n`models/test_oot_registration.py::test_oot_registration_embedding`\n\n### Basic information\n\n- [ ] Flaky test\n- [x] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\nThe `models/test_oot_registration.py::test_oot_registration_embedding` test seems to be failing in CI consistently with a context length OOM\n\nhttps://buildkite.com/vllm/ci/builds/22737/steps/canvas?sid=0197acae-970a-43ee-9fef-108d8a58da0c#0197acae-98db-423d-8af9-eb4eb401f1b4/212-1320\n\n```\n[2025-06-26T16:27:15Z] ERROR 06-26 09:27:15 [core.py:519] ValueError: To serve at least one request with the models's max seq len (8192), (2.63 GiB KV cache is needed, which is larger than the available KV cache memory (1.64 GiB). Based on the available memory, the estimated maximum model length is 5088. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.\n```\n\n### \ud83d\udcdd History of failing test\n\nNot sure, maybe related to FP32 weights? I have a prospective fix https://github.com/vllm-project/vllm/pull/20144\n\n### CC List.\n\n_No response_",
    "labels": [
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-06-26T20:12:49+00:00",
    "closed_at": "2025-06-27T03:21:05+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20148/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/20148"
  },
  {
    "number": 4366,
    "title": "Error while trying to load phi3 model [Misc]: ",
    "body": "### Anything you want to discuss about vllm.\n\n\r\n\r\nGetting the model `Azma-AI/azma-phi-3-mini-3b-128k-250424` from hugging face\r\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.43k/3.43k [00:00<00:00, 15.2MB/s]\r\nconfiguration_phi3.py: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.4k/10.4k [00:00<00:00, 37.6MB/s]\r\nA new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\r\n- configuration_phi3.py\r\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/pypoetry/virtualenvs/azma-xS3fZVNL-py3.10/bin/uvicorn\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/root/.cache/pypoetry/virtualenvs/azma-xS3fZVNL-py3.10/lib/python3.10/site-packages/click/core.py\", line 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/root/.cache/pypoetry/virtualenvs/azma-xS3fZVNL-py3.10/lib/python3.10/site-packages/click/core.py\", line 1078, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/root/.cache/pypoetry/virtualenvs/azma-xS3fZVNL-py3.10/lib/python3.10/site-packages/click/core.py\", line 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/root/.cache/pypoetry/virtualenvs/azma-xS3fZVNL-py3.10/lib/python3.10/site-packages/click/core.py\", line 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/root/.cache/pypoetry/virtualenvs/azma-xS3fZVNL-py3.10/lib/python3.10/site-packages/uvicorn/main.py\", line 418, in main\r\n    run(\r\n  File \"/root/.cache/pypoetry/virtualenvs/azma-xS3fZVNL-py3.10/lib/python3.10/site-packages/uvicorn/main.py\", line 587, in run\r\n    server.run()\r\n  File \"/root/.cache/pypoetry/virtualenvs/azma-xS3fZVNL-py3.10/lib/python3.10/site-packages/uvicorn/server.py\", line 62, in run\r\n    return asyncio.run(self.serve(sockets=sockets))\r\n  File \"/usr/lib/python3.10/asyncio/runners.py\", line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\n  File \"/root/.cache/pypoetry/virtualenvs/azma-xS3fZVNL-py3.10/lib/python3.10/site-packages/uvicorn/server.py\", line 69, in serve\r\n    config.load()\r\n  File \"/root/.cache/pypoetry/virtualenvs/azma-xS3fZVNL-py3.10/lib/python3.10/site-packages/uvicorn/config.py\", line 458, in load\r\n    self.loaded_app = import_from_string(self.app)\r\n  File \"/root/.cache/pypoetry/virtualenvs/azma-xS3fZVNL-py3.10/lib/python3.10/site-packages/uvicorn/importer.py\", line 21, in import_from_string\r\n    module = importlib.import_module(module_str)\r\n  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/workspace/azma/main.py\", line 6, in <module>\r\n    from routes.language_route import router as llm_router\r\n  File \"/workspace/azma/routes/language_route.py\", line 6, in <module>\r\n    from language_models import AZMA_LANGUAGE_MODEL\r\n  File \"/workspace/azma/language_models/__init__.py\", line 5, in <module>\r\n    AZMA_LANGUAGE_MODEL = AzmaLLM(\r\n  File \"/workspace/azma/language_models/base.py\", line 60, in __init__\r\n    self.initialize_engine(model_name, **kwargs)\r\n  File \"/workspace/azma/language_models/base.py\", line 81, in initialize_engine\r\n    self.engine = AsyncLLMEngine.from_engine_args(engine_args)\r\n  File \"/root/.cache/pypoetry/virtualenvs/azma-xS3fZVNL-py3.10/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 622, in from_engine_args\r\n    engine_configs = engine_args.create_engine_configs()\r\n  File \"/root/.cache/pypoetry/virtualenvs/azma-xS3fZVNL-py3.10/lib/python3.10/site-packages/vllm/engine/arg_utils.py\", line 287, in create_engine_configs\r\n    model_config = ModelConfig(\r\n  File \"/root/.cache/pypoetry/virtualenvs/azma-xS3fZVNL-py3.10/lib/python3.10/site-packages/vllm/config.py\", line 114, in __init__\r\n    self.max_model_len = _get_and_verify_max_len(self.hf_config,\r\n  File \"/root/.cache/pypoetry/virtualenvs/azma-xS3fZVNL-py3.10/lib/python3.10/site-packages/vllm/config.py\", line 662, in _get_and_verify_max_len\r\n    assert \"factor\" in rope_scaling\r\nAssertionError\r\n\r\n\r\nThe above phi3  model is a custom finetuned model of phi 3 `Azma-AI/azma-phi-3-mini-3b-128k-250424` There is no change to the original model config. \r\nHow can I fix this and run the inference",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-04-25T13:31:41+00:00",
    "closed_at": "2024-04-25T13:32:18+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4366/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4366"
  },
  {
    "number": 13574,
    "title": "[Installation]: Local environment installation succeeded, Existing docker environment failed log",
    "body": "### Your current environment\n\n**Local environment:**\n- System: Ubuntu 22.04  \n- CUDA: 12.4  \n- PyTorch: 2.4.0  \n- CMake: 2.31.4  \n- GCC: 11.4.0  \n\n\n**Docker environment:**  \n- System: Ubuntu 24.04  \n- CUDA: 12.6  \n- PyTorch: 2.6.0  \n- CMake: 2.31.4  \n- GCC: 13.3.0 \n\n### How you are installing vllm\n\nCode version: 2025.2.19, main branch  \nLocal environment:  \n- System: Ubuntu 22.04  \n- CUDA: 12.4  \n- PyTorch: 2.4.0  \n- CMake: 2.31.4  \n- GCC: 11.4.0  \nCompiled and installed successfully using the documentation.\n\nHowever, errors occurred when attempting to install in the Docker environment:  \n**Docker environment:**  \n- System: Ubuntu 24.04  \n- CUDA: 12.6  \n- PyTorch: 2.6.0  \n- CMake: 2.31.4  \n- GCC: 13.3.0  \n\n### Issues from the Docker environment:\n1. Issue: Configured the `nvcc` environment variable, but the folder `/usr/local/cuda-12.6/bin/nvcc` was not found.  \n   Solution: \n   Edit `~/.bash_profile` or `~/.bashrc`  \n   Modify the configuration to:  \n   ```bash\n   export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}\n   ```\n   Change `cuda-12.6` to just `cuda`.\n\n2. Issue: `subprocess.CalledProcessError: Command '['cmake', '--build', '.', '-j=40', '--target=_moe_C', '--target=_C']' returned non-zero exit status 1` \u2013 CMake compilation error.  \n   Solution: \n   Tried other methods, but unable to resolve the issue. Finally:downgraded the CUDA environment to the older version:  \n   - CUDA: 12.4  \n   - PyTorch: 2.5.1  \n   Then, git-cloned the repository and recompiled the installation, which was successful.\n\nHope this is helpful to everyone.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2025-02-20T00:46:13+00:00",
    "closed_at": "2025-02-20T00:46:42+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13574/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13574"
  },
  {
    "number": 6254,
    "title": "[Bug]: Engine timeout error due to request step residual",
    "body": "### Your current environment\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\r\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.2 (default, Feb 28 2021, 17:03:44)  [GCC 10.2.1 20210110] (64-bit runtime)\r\nPython platform: Linux-5.4.143.bsk.8-amd64-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 10.0.130\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA L40\r\nGPU 1: NVIDIA L40\r\nGPU 2: NVIDIA L40\r\nGPU 3: NVIDIA L40\r\n\r\nNvidia driver version: Could not collect\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   52 bits physical, 57 bits virtual\r\nCPU(s):                          180\r\nOn-line CPU(s) list:             0-179\r\nThread(s) per core:              2\r\nCore(s) per socket:              45\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           143\r\nModel name:                      Intel(R) Xeon(R) Platinum 8457C\r\nStepping:                        8\r\nCPU MHz:                         2599.726\r\nBogoMIPS:                        5199.45\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       4.2 MiB\r\nL1i cache:                       2.8 MiB\r\nL2 cache:                        180 MiB\r\nL3 cache:                        195 MiB\r\nNUMA node0 CPU(s):               0-89\r\nNUMA node1 CPU(s):               90-179\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Mitigation; TSX disabled\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 wbnoinvd arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid cldemote movdiri movdir64b md_clear arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.0.8+cu121torch2.3\r\n[pip3] numpy==1.23.4\r\n[pip3] nvidia-nccl-cu11==2.14.3\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] torchvision==0.18.0\r\n[pip3] transformers==4.42.3\r\n[pip3] triton==2.3.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NODE    NODE    NODE    SYS     1,48-89 0               N/A\r\nGPU1    NODE     X      NODE    NODE    SYS     1,48-89 0               N/A\r\nGPU2    NODE    NODE     X      NODE    SYS     1,48-89 0               N/A\r\nGPU3    NODE    NODE    NODE     X      SYS     1,48-89 0               N/A\r\nNIC0    SYS     SYS     SYS     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n```\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nIn async llm engine, The request will be aborted after finished `(async_llm_engine.py:L135`), but the `engine_step` may still return `True` due to the difference between request finish and engine generate finish judgement. If the engine request is aborted and but the `engine_step` return `True`, the step would be residual till next request arrive, which makes engine die due to timeout error if the next request arrive after ENGINE_ITERATION_TIMEOUT_S.\r\n![image](https://github.com/vllm-project/vllm/assets/62173185/5203a213-79e7-44a4-b8e3-43c0b498eabf)\r\n###  Reproduce the bug\r\n1. Use latest `v0.5.1` vllm\r\n2. deploy an server use code as below\r\n```python\r\nclass EndpointHandler:\r\n    def __init__(self) -> None:\r\n        \"\"\"Load Tokenizer and Model, and do a warmup test\"\"\"\r\n        ENGINE_ARGS = AsyncEngineArgs(\r\n            model=MODEL_PATH,\r\n            tensor_parallel_size=parallel_size,\r\n            disable_custom_all_reduce=True,\r\n            enforce_eager=False,\r\n            dtype=\"bfloat16\",\r\n            gpu_memory_utilization=gpu_memory_utlization\r\n        )\r\n        self.model =AsyncLLMEngine.from_engine_args(ENGINE_ARGS)\r\n\r\n        logger.info(f\"[Handler] Successfully load Model from {MODEL_PATH}\")\r\n\r\n    def convert_history_to_text(self, history):\r\n        pass\r\n\r\n    @torch.inference_mode()\r\n    def __call__(\r\n        self, request: Dict[str, Union[List[bytes], List[int], List[float]]]\r\n    ) -> Dict[str, List[bytes]]:\r\n        histories = [json.loads(item) for item in request[\"histories\"]]\r\n        prompts = []\r\n        for history in histories:\r\n            prompt = self.convert_history_to_text(history)\r\n            prompts.append(prompt)\r\n        \r\n\r\n        kwargs = {\r\n            \"max_tokens\": 512,\r\n        }\r\n        if request.get(\"top_p\"):\r\n            kwargs[\"top_p\"] = float(request[\"top_p\"][0])\r\n        if request.get(\"temperature\"):\r\n            kwargs[\"temperature\"] = float(request[\"temperature\"][0])\r\n        if request.get(\"max_new_tokens\"):\r\n            kwargs[\"max_tokens\"] = int(request[\"max_new_tokens\"][0])\r\n\r\n        sampling_params = SamplingParams(**kwargs)\r\n        try:\r\n            loop = asyncio.get_event_loop()\r\n        except RuntimeError as e:\r\n            if str(e).startswith('There is no current event loop in thread'):\r\n                loop = asyncio.new_event_loop()\r\n                asyncio.set_event_loop(loop)\r\n            else:\r\n                raise\r\n        print(prompts)\r\n        stream = loop.run_until_complete(self.model.add_request(uuid.uuid4().hex, prompts[0], sampling_params))\r\n        while True:\r\n            try:\r\n                result = loop.run_until_complete(stream.__anext__())\r\n                yield dict(output=[result.outputs[0].text])\r\n            except StopAsyncIteration:\r\n                break\r\n```\r\n3. make any request\r\n4. wait ENGINE_ITERATION_TIMEOUT_S, and make another request, the engine will die due to timeout error",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-07-09T09:30:12+00:00",
    "closed_at": "2024-07-11T13:46:32+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6254/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/6254"
  },
  {
    "number": 17358,
    "title": "[Bug]: MiniCPM-o int4",
    "body": "### Your current environment\n\njetson orin 64G\nvllm 0.8.5\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen i load MiniCPM-o int4 using \"dtype\": \"float16\", and \"quantization\": \"gptq\". I encounted \n\n\n```\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 455, in load_model\n    loaded_weights = model.load_weights(\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/minicpmo.py\", line 531, in load_weights\n    return loader.load_weights(weights)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 261, in load_weights\n    autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 222, in _load_module\n    yield from self._load_module(prefix,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 222, in _load_module\n    yield from self._load_module(prefix,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 250, in _load_module\n    raise ValueError(msg)\nValueError: There is no module or parameter named 'resampler.kv_proj.weight' in MiniCPMO\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-04-29T08:00:43+00:00",
    "closed_at": "2025-04-29T18:21:44+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17358/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17358"
  },
  {
    "number": 2232,
    "title": "The same model cannot be loaded by two different users",
    "body": "As pointed out here, the way lockfiles are created prevents the second user from loading any models that a previous user has loaded at any point: https://github.com/vllm-project/vllm/issues/2179\r\n\r\nThis is still an issue with the only workaround being to force-delete the lockfile created by another user.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-12-21T09:40:34+00:00",
    "closed_at": "2024-03-23T18:43:12+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2232/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2232"
  },
  {
    "number": 2325,
    "title": "[ERROR] [ -4263953,Not implemented in the current version ] nvmlDeviceGetHandleByPciBusId() This is unlikely to affect the main functionalities of user applications.",
    "body": "when i run this command:\r\n```\r\nllm = LLM(model=\"qwen/Qwen-7B-Chat\", revision=\"v1.1.8\", trust_remote_code=True,quantization='awq')\r\n```\r\n\r\nthe error below:\r\n```\r\nWARNING 01-03 11:47:54 config.py:171] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\nINFO 01-03 11:47:54 llm_engine.py:73] Initializing an LLM engine with config: model='/data/share/rwq/Qwen-7B-Chat', tokenizer='/data/share/rwq/Qwen-7B-Chat', tokenizer_mode=auto, revision=v1.1.8, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=awq, seed=0)\r\nWARNING 01-03 11:47:54 tokenizer.py:79] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\r\n2024-01-03 11:48:06 [ERROR] [ -4263953,Not implemented in the current version ] nvmlDeviceGetHandleByPciBusId() This is unlikely to affect the main functionalities of user applications.\r\n```\r\n\r\nhow to solve?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-03T03:51:15+00:00",
    "closed_at": "2024-03-28T13:39:31+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2325/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2325"
  },
  {
    "number": 2575,
    "title": "1",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-24T08:25:18+00:00",
    "closed_at": "2024-01-24T08:25:33+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2575/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2575"
  },
  {
    "number": 1981,
    "title": "Setting up CodeGen for vllm but getting error \"KeyError: 'transformer.h.0.attn.causal_mask'\"",
    "body": "I have created a new 'codegen' adaptor for vllm but am getting the error \"KeyError: 'transformer.h.0.attn.causal_mask'\" when trying to do my first test. Need help to find and fix the issue.\r\n\r\nException:\r\nINFO 12-08 05:00:54 llm_engine.py:73] Initializing an LLM engine with config: model='Salesforce/codegen-350M-mono', tokenizer='Salesforce/codegen-350M-mono', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nTraceback (most recent call last):\r\n  File \"/content/drive/MyDrive/Workspace/vllm-main/./examples/offline_inference.py\", line 14, in <module>\r\n    llm = LLM(model=\"Salesforce/codegen-350M-mono\")\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py\", line 93, in __init__\r\n    self.llm_engine = LLMEngine.from_engine_args(engine_args)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 246, in from_engine_args\r\n    engine = cls(*engine_configs,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 109, in __init__\r\n    self._init_workers(distributed_init_method)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 145, in _init_workers\r\n    self._run_workers(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 750, in _run_workers\r\n    self._run_workers_in_batch(workers, method, *args, **kwargs))\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 724, in _run_workers_in_batch\r\n    output = executor(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 72, in load_model\r\n    self.model_runner.load_model()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 36, in load_model\r\n    self.model = get_model(self.model_config)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader.py\", line 99, in get_model\r\n    model.load_weights(model_config.model, model_config.download_dir,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/codegen.py\", line 282, in load_weights\r\n    param = params_dict[name]\r\nKeyError: 'transformer.h.0.attn.causal_mask'\r\n\r\n\r\nThe code that I am using for testing is - Though the prompt might not be correct but the error does not seem to be due to the prompt\r\n\r\nfrom vllm import LLM, SamplingParams\r\n\r\n# Sample prompts.\r\nprompts = [\r\n    \"Hello, my name is\",\r\n    \"The president of the United States is\",\r\n    \"The capital of France is\",\r\n    \"The future of AI is\",\r\n]\r\n# Create a sampling params object.\r\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\r\n\r\n# Create an LLM.\r\nllm = LLM(model=\"Salesforce/codegen-350M-mono\")\r\n# Generate texts from the prompts. The output is a list of RequestOutput objects\r\n# that contain the prompt, generated text, and other information.\r\noutputs = llm.generate(prompts, sampling_params)\r\n# Print the outputs.\r\nfor output in outputs:\r\n    prompt = output.prompt\r\n    generated_text = output.outputs[0].text\r\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-12-08T05:07:27+00:00",
    "closed_at": "2024-03-25T10:26:43+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1981/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1981"
  },
  {
    "number": 6408,
    "title": "[Bug]: `samplers/test_logprobs.py` fail on H100",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.5\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) [GCC 12.3.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-107-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 1: NVIDIA H100 80GB HBM3\r\nGPU 2: NVIDIA H100 80GB HBM3\r\nGPU 3: NVIDIA H100 80GB HBM3\r\nGPU 4: NVIDIA H100 80GB HBM3\r\nGPU 5: NVIDIA H100 80GB HBM3\r\nGPU 6: NVIDIA H100 80GB HBM3\r\nGPU 7: NVIDIA H100 80GB HBM3\r\n\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             192\r\nOn-line CPU(s) list:                0-191\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8474C\r\nCPU family:                         6\r\nModel:                              143\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 48\r\nSocket(s):                          2\r\nStepping:                           8\r\nCPU max MHz:                        3800.0000\r\nCPU min MHz:                        800.0000\r\nBogoMIPS:                           4200.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          4.5 MiB (96 instances)\r\nL1i cache:                          3 MiB (96 instances)\r\nL2 cache:                           192 MiB (96 instances)\r\nL3 cache:                           195 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-47,96-143\r\nNUMA node1 CPU(s):                  48-95,144-191\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.9.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] sentence-transformers==3.0.1\r\n[pip3] torch==2.3.0\r\n[pip3] transformers==4.41.2\r\n[pip3] triton==2.3.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] sentence-transformers     3.0.1                    pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] transformers              4.41.2                   pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tPIX\tPIX\t0-47,96-143\t0\t\tN/A\r\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\r\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\r\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\r\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tSYS\tSYS\t48-95,144-191\t1\t\tN/A\r\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tSYS\tSYS\t48-95,144-191\t1\t\tN/A\r\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tSYS\tSYS\t48-95,144-191\t1\t\tN/A\r\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tSYS\tSYS\t48-95,144-191\t1\t\tN/A\r\nNIC0\tPIX\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPIX\t\t\t\t\r\nNIC1\tPIX\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPIX\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nThe `tests/samplers/test_logprobs.py` are failing on `main` branch when run on an H100 GPU:\r\n\r\n```\r\n$ python -m pytest tests/samplers/test_logprobs.py::test_get_prompt_logprobs\r\n=================================================================================================== test session starts ===================================================================================================\r\nplatform linux -- Python 3.11.9, pytest-8.2.2, pluggy-1.5.0\r\nrootdir: /home/zrltpa/vllm\r\nconfigfile: pyproject.toml\r\nplugins: shard-0.1.2, forked-1.6.0, asyncio-0.23.7, rerunfailures-14.0, anyio-4.4.0\r\nasyncio: mode=Mode.STRICT\r\ncollected 8 items                                                                                                                                                                                                         \r\nRunning 8 items in this shard\r\n\r\ntests/samplers/test_logprobs.py .F...F..                                                                                                                                                                            [100%]\r\n\r\n======================================================================================================== FAILURES =========================================================================================================\r\n________________________________________________________________________________ test_get_prompt_logprobs[True-6-4-half-facebook/opt-125m] ________________________________________________________________________________\r\n\r\nhf_runner = <class 'tests.conftest.HfRunner'>, vllm_runner = <class 'tests.conftest.VllmRunner'>, model = 'facebook/opt-125m', dtype = 'half', chunked_prefill_token_size = 4, num_top_logprobs = 6, detokenize = True\r\nexample_prompts = ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the majo...me.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', ...]\r\n\r\n    @pytest.mark.parametrize(\"model\", MODELS)\r\n    @pytest.mark.parametrize(\"dtype\", [\"half\"])\r\n    @pytest.mark.parametrize(\"chunked_prefill_token_size\", [1, 4, 16, -1])\r\n    @pytest.mark.parametrize(\"num_top_logprobs\", [6])  # 32000 == vocab_size\r\n    @pytest.mark.parametrize(\"detokenize\", [True, False])\r\n    def test_get_prompt_logprobs(\r\n        hf_runner,\r\n        vllm_runner,\r\n        model,\r\n        dtype,\r\n        chunked_prefill_token_size: int,\r\n        num_top_logprobs: int,\r\n        detokenize: bool,\r\n        example_prompts,\r\n    ):\r\n        max_num_seqs = 256\r\n        enable_chunked_prefill = False\r\n        max_num_batched_tokens = None\r\n        if chunked_prefill_token_size != -1:\r\n            enable_chunked_prefill = True\r\n            max_num_seqs = min(chunked_prefill_token_size, max_num_seqs)\r\n            max_num_batched_tokens = chunked_prefill_token_size\r\n    \r\n        max_tokens = 5\r\n        with hf_runner(model, dtype=dtype) as hf_model:\r\n            hf_logprobs = hf_model.generate_greedy_logprobs(\r\n                example_prompts,\r\n                max_tokens=max_tokens,\r\n            )\r\n    \r\n        with vllm_runner(\r\n                model,\r\n                dtype=dtype,\r\n                max_logprobs=num_top_logprobs,\r\n                enable_chunked_prefill=enable_chunked_prefill,\r\n                max_num_batched_tokens=max_num_batched_tokens,\r\n                max_num_seqs=max_num_seqs,\r\n        ) as vllm_model:\r\n            vllm_sampling_params = SamplingParams(max_tokens=max_tokens,\r\n                                                  logprobs=num_top_logprobs,\r\n                                                  prompt_logprobs=num_top_logprobs,\r\n                                                  temperature=0.0,\r\n                                                  detokenize=detokenize)\r\n            vllm_results = vllm_model.model.generate(\r\n                example_prompts, sampling_params=vllm_sampling_params)\r\n    \r\n        # Test whether logprobs are included in the results.\r\n        for result in vllm_results:\r\n            assert result.prompt_logprobs is not None\r\n            assert result.outputs[0].logprobs is not None\r\n            assert len(result.outputs[0].logprobs) == max_tokens\r\n            for logprobs in result.outputs[0].logprobs:\r\n                assert len(logprobs) == num_top_logprobs\r\n            output_text = result.outputs[0].text\r\n            output_string_from_most_likely_tokens_lst: List[str] = []\r\n            for top_logprobs in result.outputs[0].logprobs:\r\n                top_logprob = next(iter(top_logprobs.values()))\r\n                output_string_from_most_likely_tokens_lst.append(\r\n                    top_logprob.decoded_token)\r\n    \r\n            if detokenize:\r\n                output_string_from_most_likely_tokens = \"\".join(\r\n                    output_string_from_most_likely_tokens_lst)\r\n                assert output_text == output_string_from_most_likely_tokens, (\r\n                    \"The output text from the top logprob for each token position \"\r\n                    \"should be the same as the output text in the result.\")\r\n            else:\r\n                assert output_text == ''\r\n                assert output_string_from_most_likely_tokens_lst == ([None] *\r\n                                                                     max_tokens)\r\n    \r\n            # The first prompt logprob is always None\r\n            assert result.prompt_logprobs[0] is None\r\n            for prompt_logprobs in result.prompt_logprobs[1:]:\r\n                # If the prompt token is not included in the top X\r\n                # logprob, it can return 1 more data\r\n                assert (len(prompt_logprobs) == num_top_logprobs\r\n                        or len(prompt_logprobs) == num_top_logprobs + 1)\r\n    \r\n        # Test whether prompt logprobs are consistent with HF\r\n        for vllm_result, hf_logprob in zip(vllm_results, hf_logprobs):\r\n            # Check prompt logprobs\r\n            # The first prompt logprob is always None, so we compare it from 1:.\r\n            vllm_prompt_logprobs = vllm_result.prompt_logprobs[1:]\r\n            for i, vllm_prompt_logprob_dict in enumerate(vllm_prompt_logprobs):\r\n                for token_id, logprob in vllm_prompt_logprob_dict.items():\r\n>                   torch.testing.assert_close(logprob.logprob,\r\n                                               hf_logprob[0][i][token_id].item(),\r\n                                               atol=1e-2,\r\n                                               rtol=1e-2)\r\nE                   AssertionError: Scalars are not close!\r\nE                   \r\nE                   Expected -1.3945362567901611 but got -1.432058572769165.\r\nE                   Absolute difference: 0.037522315979003906 (up to 0.01 allowed)\r\nE                   Relative difference: 0.026906662194190602 (up to 0.01 allowed)\r\n\r\ntests/samplers/test_logprobs.py:99: AssertionError\r\n-------------------------------------------------------------------------------------------------- Captured stdout call ---------------------------------------------------------------------------------------------------\r\nINFO 07-13 00:09:30 config.py:778] Chunked prefill is enabled (EXPERIMENTAL).\r\nINFO 07-13 00:09:30 llm_engine.py:174] Initializing an LLM engine (v0.5.1) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=facebook/opt-125m, use_v2_block_manager=False, enable_prefix_caching=False)\r\nINFO 07-13 00:09:31 weight_utils.py:218] Using model weights format ['*.bin']\r\nINFO 07-13 00:09:31 model_runner.py:266] Loading model weights took 0.2390 GB\r\nINFO 07-13 00:09:31 gpu_executor.py:86] # GPU blocks: 129168, # CPU blocks: 7281\r\nINFO 07-13 00:09:31 model_runner.py:1007] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 07-13 00:09:31 model_runner.py:1011] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\nINFO 07-13 00:09:31 model_runner.py:1208] Graph capturing finished in 0 secs.\r\n-------------------------------------------------------------------------------------------------- Captured stderr call ---------------------------------------------------------------------------------------------------\r\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:00<00:00, 24.47it/s, est. speed input: 514.15 toks/s, output: 122.41 toks/s]\r\n_______________________________________________________________________________ test_get_prompt_logprobs[False-6-4-half-facebook/opt-125m] ________________________________________________________________________________\r\n\r\nhf_runner = <class 'tests.conftest.HfRunner'>, vllm_runner = <class 'tests.conftest.VllmRunner'>, model = 'facebook/opt-125m', dtype = 'half', chunked_prefill_token_size = 4, num_top_logprobs = 6, detokenize = False\r\nexample_prompts = ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the majo...me.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', ...]\r\n\r\n    @pytest.mark.parametrize(\"model\", MODELS)\r\n    @pytest.mark.parametrize(\"dtype\", [\"half\"])\r\n    @pytest.mark.parametrize(\"chunked_prefill_token_size\", [1, 4, 16, -1])\r\n    @pytest.mark.parametrize(\"num_top_logprobs\", [6])  # 32000 == vocab_size\r\n    @pytest.mark.parametrize(\"detokenize\", [True, False])\r\n    def test_get_prompt_logprobs(\r\n        hf_runner,\r\n        vllm_runner,\r\n        model,\r\n        dtype,\r\n        chunked_prefill_token_size: int,\r\n        num_top_logprobs: int,\r\n        detokenize: bool,\r\n        example_prompts,\r\n    ):\r\n        max_num_seqs = 256\r\n        enable_chunked_prefill = False\r\n        max_num_batched_tokens = None\r\n        if chunked_prefill_token_size != -1:\r\n            enable_chunked_prefill = True\r\n            max_num_seqs = min(chunked_prefill_token_size, max_num_seqs)\r\n            max_num_batched_tokens = chunked_prefill_token_size\r\n    \r\n        max_tokens = 5\r\n        with hf_runner(model, dtype=dtype) as hf_model:\r\n            hf_logprobs = hf_model.generate_greedy_logprobs(\r\n                example_prompts,\r\n                max_tokens=max_tokens,\r\n            )\r\n    \r\n        with vllm_runner(\r\n                model,\r\n                dtype=dtype,\r\n                max_logprobs=num_top_logprobs,\r\n                enable_chunked_prefill=enable_chunked_prefill,\r\n                max_num_batched_tokens=max_num_batched_tokens,\r\n                max_num_seqs=max_num_seqs,\r\n        ) as vllm_model:\r\n            vllm_sampling_params = SamplingParams(max_tokens=max_tokens,\r\n                                                  logprobs=num_top_logprobs,\r\n                                                  prompt_logprobs=num_top_logprobs,\r\n                                                  temperature=0.0,\r\n                                                  detokenize=detokenize)\r\n            vllm_results = vllm_model.model.generate(\r\n                example_prompts, sampling_params=vllm_sampling_params)\r\n    \r\n        # Test whether logprobs are included in the results.\r\n        for result in vllm_results:\r\n            assert result.prompt_logprobs is not None\r\n            assert result.outputs[0].logprobs is not None\r\n            assert len(result.outputs[0].logprobs) == max_tokens\r\n            for logprobs in result.outputs[0].logprobs:\r\n                assert len(logprobs) == num_top_logprobs\r\n            output_text = result.outputs[0].text\r\n            output_string_from_most_likely_tokens_lst: List[str] = []\r\n            for top_logprobs in result.outputs[0].logprobs:\r\n                top_logprob = next(iter(top_logprobs.values()))\r\n                output_string_from_most_likely_tokens_lst.append(\r\n                    top_logprob.decoded_token)\r\n    \r\n            if detokenize:\r\n                output_string_from_most_likely_tokens = \"\".join(\r\n                    output_string_from_most_likely_tokens_lst)\r\n                assert output_text == output_string_from_most_likely_tokens, (\r\n                    \"The output text from the top logprob for each token position \"\r\n                    \"should be the same as the output text in the result.\")\r\n            else:\r\n                assert output_text == ''\r\n                assert output_string_from_most_likely_tokens_lst == ([None] *\r\n                                                                     max_tokens)\r\n    \r\n            # The first prompt logprob is always None\r\n            assert result.prompt_logprobs[0] is None\r\n            for prompt_logprobs in result.prompt_logprobs[1:]:\r\n                # If the prompt token is not included in the top X\r\n                # logprob, it can return 1 more data\r\n                assert (len(prompt_logprobs) == num_top_logprobs\r\n                        or len(prompt_logprobs) == num_top_logprobs + 1)\r\n    \r\n        # Test whether prompt logprobs are consistent with HF\r\n        for vllm_result, hf_logprob in zip(vllm_results, hf_logprobs):\r\n            # Check prompt logprobs\r\n            # The first prompt logprob is always None, so we compare it from 1:.\r\n            vllm_prompt_logprobs = vllm_result.prompt_logprobs[1:]\r\n            for i, vllm_prompt_logprob_dict in enumerate(vllm_prompt_logprobs):\r\n                for token_id, logprob in vllm_prompt_logprob_dict.items():\r\n>                   torch.testing.assert_close(logprob.logprob,\r\n                                               hf_logprob[0][i][token_id].item(),\r\n                                               atol=1e-2,\r\n                                               rtol=1e-2)\r\nE                   AssertionError: Scalars are not close!\r\nE                   \r\nE                   Expected -1.3945362567901611 but got -1.432058572769165.\r\nE                   Absolute difference: 0.037522315979003906 (up to 0.01 allowed)\r\nE                   Relative difference: 0.026906662194190602 (up to 0.01 allowed)\r\n\r\ntests/samplers/test_logprobs.py:99: AssertionError\r\n-------------------------------------------------------------------------------------------------- Captured stdout call ---------------------------------------------------------------------------------------------------\r\nINFO 07-13 00:09:49 config.py:778] Chunked prefill is enabled (EXPERIMENTAL).\r\nINFO 07-13 00:09:49 llm_engine.py:174] Initializing an LLM engine (v0.5.1) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=facebook/opt-125m, use_v2_block_manager=False, enable_prefix_caching=False)\r\nINFO 07-13 00:09:50 weight_utils.py:218] Using model weights format ['*.bin']\r\nINFO 07-13 00:09:50 model_runner.py:266] Loading model weights took 0.2390 GB\r\nINFO 07-13 00:09:50 gpu_executor.py:86] # GPU blocks: 129165, # CPU blocks: 7281\r\nINFO 07-13 00:09:50 model_runner.py:1007] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 07-13 00:09:50 model_runner.py:1011] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\nINFO 07-13 00:09:51 model_runner.py:1208] Graph capturing finished in 0 secs.\r\n-------------------------------------------------------------------------------------------------- Captured stderr call ---------------------------------------------------------------------------------------------------\r\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:00<00:00, 41.18it/s, est. speed input: 865.03 toks/s, output: 205.95 toks/s]\r\n==================================================================================================== warnings summary =====================================================================================================\r\ntests/samplers/test_logprobs.py::test_get_prompt_logprobs[True-6-1-half-facebook/opt-125m]\r\ntests/samplers/test_logprobs.py::test_get_prompt_logprobs[True-6-4-half-facebook/opt-125m]\r\ntests/samplers/test_logprobs.py::test_get_prompt_logprobs[True-6-16-half-facebook/opt-125m]\r\ntests/samplers/test_logprobs.py::test_get_prompt_logprobs[True-6--1-half-facebook/opt-125m]\r\ntests/samplers/test_logprobs.py::test_get_prompt_logprobs[False-6-1-half-facebook/opt-125m]\r\ntests/samplers/test_logprobs.py::test_get_prompt_logprobs[False-6-4-half-facebook/opt-125m]\r\ntests/samplers/test_logprobs.py::test_get_prompt_logprobs[False-6-16-half-facebook/opt-125m]\r\ntests/samplers/test_logprobs.py::test_get_prompt_logprobs[False-6--1-half-facebook/opt-125m]\r\n  /home/zrltpa/miniforge3/envs/dev-env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n    warnings.warn(\r\n\r\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\r\n================================================================================================= short test summary info =================================================================================================\r\nFAILED tests/samplers/test_logprobs.py::test_get_prompt_logprobs[True-6-4-half-facebook/opt-125m] - AssertionError: Scalars are not close!\r\nFAILED tests/samplers/test_logprobs.py::test_get_prompt_logprobs[False-6-4-half-facebook/opt-125m] - AssertionError: Scalars are not close!\r\n======================================================================================== 2 failed, 6 passed, 8 warnings in 45.24s =========================================================================================\r\n```\r\nLooks like precision issue imo. \r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-07-13T04:13:43+00:00",
    "closed_at": "2024-07-15T17:14:50+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6408/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/6408"
  },
  {
    "number": 17595,
    "title": "[Bug]: fp8 w8a8 quantized Qwen2.5-VL hits AssertionError",
    "body": "### Your current environment\n\nvLLM v0.8.4 on Tesla L40S.\n\n### \ud83d\udc1b Describe the bug\n\nSee also these closed issues: #7550 #15264\n\n```\nERROR 05-02 09:48:39 [engine.py:160] AssertionError()\nERROR 05-02 09:48:39 [engine.py:160] Traceback (most recent call last):\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 158, in start\nERROR 05-02 09:48:39 [engine.py:160]     self.run_engine_loop()\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 221, in run_engine_loop\nERROR 05-02 09:48:39 [engine.py:160]     request_outputs = self.engine_step()\nERROR 05-02 09:48:39 [engine.py:160]                       ^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 247, in engine_step\nERROR 05-02 09:48:39 [engine.py:160]     raise e\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 230, in engine_step\nERROR 05-02 09:48:39 [engine.py:160]     return self.engine.step()\nERROR 05-02 09:48:39 [engine.py:160]            ^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 1431, in step\nERROR 05-02 09:48:39 [engine.py:160]     outputs = self.model_executor.execute_model(\nERROR 05-02 09:48:39 [engine.py:160]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 140, in execute_model\nERROR 05-02 09:48:39 [engine.py:160]     output = self.collective_rpc(\"execute_model\",\nERROR 05-02 09:48:39 [engine.py:160]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\nERROR 05-02 09:48:39 [engine.py:160]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 05-02 09:48:39 [engine.py:160]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2378, in run_method\nERROR 05-02 09:48:39 [engine.py:160]     return func(*args, **kwargs)\nERROR 05-02 09:48:39 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 420, in execute_model\nERROR 05-02 09:48:39 [engine.py:160]     output = self.model_runner.execute_model(\nERROR 05-02 09:48:39 [engine.py:160]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 05-02 09:48:39 [engine.py:160]     return func(*args, **kwargs)\nERROR 05-02 09:48:39 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/pooling_model_runner.py\", line 111, in execute_model\nERROR 05-02 09:48:39 [engine.py:160]     hidden_or_intermediate_states = model_executable(\nERROR 05-02 09:48:39 [engine.py:160]                                     ^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nERROR 05-02 09:48:39 [engine.py:160]     return self._call_impl(*args, **kwargs)\nERROR 05-02 09:48:39 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\nERROR 05-02 09:48:39 [engine.py:160]     return forward_call(*args, **kwargs)\nERROR 05-02 09:48:39 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2_5_vl.py\", line 1083, in forward\nERROR 05-02 09:48:39 [engine.py:160]     inputs_embeds = self.get_input_embeddings_v0(\nERROR 05-02 09:48:39 [engine.py:160]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2_5_vl.py\", line 1017, in get_input_embeddings_v0\nERROR 05-02 09:48:39 [engine.py:160]     image_embeds = self._process_image_input(image_input)\nERROR 05-02 09:48:39 [engine.py:160]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2_5_vl.py\", line 926, in _process_image_input\nERROR 05-02 09:48:39 [engine.py:160]     image_embeds = self.visual(pixel_values, grid_thw=grid_thw)\nERROR 05-02 09:48:39 [engine.py:160]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nERROR 05-02 09:48:39 [engine.py:160]     return self._call_impl(*args, **kwargs)\nERROR 05-02 09:48:39 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\nERROR 05-02 09:48:39 [engine.py:160]     return forward_call(*args, **kwargs)\nERROR 05-02 09:48:39 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2_5_vl.py\", line 674, in forward\nERROR 05-02 09:48:39 [engine.py:160]     hidden_states = blk(\nERROR 05-02 09:48:39 [engine.py:160]                     ^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nERROR 05-02 09:48:39 [engine.py:160]     return self._call_impl(*args, **kwargs)\nERROR 05-02 09:48:39 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\nERROR 05-02 09:48:39 [engine.py:160]     return forward_call(*args, **kwargs)\nERROR 05-02 09:48:39 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2_5_vl.py\", line 382, in forward\nERROR 05-02 09:48:39 [engine.py:160]     x = x + self.mlp(self.norm2(x))\nERROR 05-02 09:48:39 [engine.py:160]             ^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nERROR 05-02 09:48:39 [engine.py:160]     return self._call_impl(*args, **kwargs)\nERROR 05-02 09:48:39 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\nERROR 05-02 09:48:39 [engine.py:160]     return forward_call(*args, **kwargs)\nERROR 05-02 09:48:39 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2_5_vl.py\", line 191, in forward\nERROR 05-02 09:48:39 [engine.py:160]     x_gate, _ = self.gate_proj(x)\nERROR 05-02 09:48:39 [engine.py:160]                 ^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nERROR 05-02 09:48:39 [engine.py:160]     return self._call_impl(*args, **kwargs)\nERROR 05-02 09:48:39 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\nERROR 05-02 09:48:39 [engine.py:160]     return forward_call(*args, **kwargs)\nERROR 05-02 09:48:39 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py\", line 474, in forward\nERROR 05-02 09:48:39 [engine.py:160]     output_parallel = self.quant_method.apply(self, input_, bias)\nERROR 05-02 09:48:39 [engine.py:160]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py\", line 580, in apply\nERROR 05-02 09:48:39 [engine.py:160]     return scheme.apply_weights(layer, x, bias=bias)\nERROR 05-02 09:48:39 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py\", line 144, in apply_weights\nERROR 05-02 09:48:39 [engine.py:160]     return self.fp8_linear.apply(input=x,\nERROR 05-02 09:48:39 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/utils/w8a8_utils.py\", line 200, in apply\nERROR 05-02 09:48:39 [engine.py:160]     output = ops.cutlass_scaled_mm(qinput,\nERROR 05-02 09:48:39 [engine.py:160]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/_custom_ops.py\", line 551, in cutlass_scaled_mm\nERROR 05-02 09:48:39 [engine.py:160]     assert (b.shape[0] % 16 == 0 and b.shape[1] % 16 == 0)\nERROR 05-02 09:48:39 [engine.py:160]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-02 09:48:39 [engine.py:160] AssertionError\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-05-02T16:54:40+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17595/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17595"
  },
  {
    "number": 19552,
    "title": "[Bug]: \u4f7f\u7528qwen2.5-omni\u5bf9\u97f3\u9891\u8bc6\u522b\uff0ccpu\u4f1a\u88ab\u6253\u6ee1\u3002",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nvllm serve Qwen2.5-Omni-3B --dtype bfloat16 --gpu_memory_utilization=0.9 --tensor-parallel-size 1 --swap-space 0\n\nA800\u673a\u5668\uff0c80G\u663e\u5b58\uff0ccpu200\u6838\u3002\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n\u8c03\u7528\u63a8\u7406\u670d\u52a1\uff0c\u53d1\u73b0cpu\u5229\u7528\u7387\u8fbe\u523099%\u3002\u4f1a\u652f\u6301\u5c06audio\u7684\u5904\u7406\u653e\u5728gpu\u91cc\u5417\uff1f\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-06-12T12:40:34+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19552/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19552"
  },
  {
    "number": 1423,
    "title": "[Question][Schedule] vLLM cannot schedule a whole sequence group but can schedule some of its sequences",
    "body": "As the [paper](https://arxiv.org/pdf/2309.06180.pdf) wrote,\r\n\r\n> The sequences within one sequence group are always preempted or rescheduled together due to potential memory sharing across those sequences.\r\n\r\nIn such a condition:\r\n\r\n* **Total** GPU memory is not enough to hold a sequence group\r\n* **Total** GPU memory is enough to hold some of sequences in that group\r\n\r\nThis may happen in sequence groups with a large size. According to the paper, this sequence group would not be scheduled, and the scheduler would be **stuck**.\r\n\r\nWould the following method be better for availability while still utilizing the advantage of memory sharing:\r\n\r\n* First, try scheduling a sequence group.\r\n* If all sequence groups to be scheduled can not fit in GPU memory, select one sequence group and schedule as many sequences of this group.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-10-20T03:00:13+00:00",
    "closed_at": "2024-03-13T11:40:25+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1423/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1423"
  },
  {
    "number": 12328,
    "title": "[Misc]: RoPE vs Sliding Windows",
    "body": "Hi,\n\nAs context lengths increase, it looks like different models are going about it in different ways. For example, Qwen uses a sliding window in their config.json file while Llama uses RoPE.\n\nI was curious how they work in contrast with each other, if they can be combined, what types of RoPE scaling exist, and how all of these parameters can be optimized separately or in conjunction with each other.\n\nI am also curious how setting the `--rope-scaling` and `--rope-theta` interacts with the configs if it is already set or using sliding windows. I can't find too much information regarding the combination of all of these settings so any help would be awesome.\n\n\n\n```json\n{\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 5120,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 27648,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 70,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 40,\n  \"num_hidden_layers\": 64,\n  \"num_key_value_heads\": 8,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": 131072,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.43.1\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 152064\n}\n```\n\n```json\n{\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"low_freq_factor\": 1.0,\n    \"high_freq_factor\": 4.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.42.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n```",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2025-01-22T19:44:02+00:00",
    "closed_at": "2025-02-17T16:33:46+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12328/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12328"
  },
  {
    "number": 18162,
    "title": "[Bug]: make `test_openai_schema.py` pass, enable it in CI",
    "body": "This is a follow up to PR #17664 and issues like #17037 and #17038.\n\nAs of the time of this writing, the only sub-test in `tests/entrypoints/openai/test_openai_schema.py` that consistently fails is `POST /tokenize` caused by https://github.com/vllm-project/vllm/blob/98ea35601cdb34fdd618f965e7bcc3cb02a677fc/vllm/entrypoints/chat_utils.py#L1083 when `part_type` is `\"file\"`. The expected response is 200 as documented by the OpenAPI spec.\n\nHow should we make this test pass? This is a requirement to eventually enabling the test in CI by removing `--ignore=entrypoints/openai/test_openai_schema.py` in `test-pipeline.yaml`.\n\n<details>\n\n```\n============================================================================================ FAILURES =============================================================================================\n_____________________________________________________________________ test_openapi_stateless (verbose_name='POST /tokenize') ______________________________________________________________________\n\n    @wraps(test)\n>   def test_function(*args: Any, **kwargs: Any) -> Any:\n\n.venv/lib/python3.12/site-packages/schemathesis/_hypothesis.py:83: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ncase = Case(body={'messages': [{'content': [{'file': {}, 'type': 'file'}], 'role': 'user'}]})\n\n    @schema.parametrize(tag=\"debug\")\n    @schema.override(headers={\"Content-Type\": \"application/json\"})\n    def test_openapi_stateless(case: schemathesis.Case):\n        key = (\n            case.operation.method.upper(),\n            case.operation.path,\n        )\n        timeout = {\n            (\"POST\", \"/v1/chat/completions\"): 99999,\n        }.get(key, 10)\n    \n        #No need to verify SSL certificate for localhost\n>       case.call_and_validate(verify=False, timeout=timeout)\nE       schemathesis.exceptions.CheckFailed: \nE       \nE       1. Server error\nE       \nE       [500] Internal Server Error:\nE       \nE           `{\"object\":\"error\",\"message\":\"Unknown part type: file\",\"type\":\"Internal Server Error\",\"param\":null,\"code\":500}`\nE       \nE       Reproduce with: \nE       \nE           curl -X POST -H 'Content-Type: application/json' -d '{\"messages\": [{\"content\": [{\"file\": {}, \"type\": \"file\"}], \"role\": \"user\"}]}' --insecure http://localhost:48767/tokenize\nE       \nE       Falsifying example: test_openapi_stateless(\nE           case=,\nE       )\n\ntests/entrypoints/openai/test_openai_schema.py:57: CheckFailed\n```\n\n```bash\n$ curl -X POST -H 'Content-Type: application/json' -d '{\"messages\": [{\"content\": [{\"file\": {}, \"type\": \"file\"}], \"role\": \"user\"}]}' --insecure http://localhost:8000/tokenize -v\nNote: Unnecessary use of -X or --request, POST is already inferred.\n* Host localhost:8000 was resolved.\n* IPv6: ::1\n* IPv4: 127.0.0.1\n*   Trying [::1]:8000...\n* connect to ::1 port 8000 from ::1 port 40420 failed: Connection refused\n*   Trying 127.0.0.1:8000...\n* Connected to localhost (127.0.0.1) port 8000\n> POST /tokenize HTTP/1.1\n> Host: localhost:8000\n> User-Agent: curl/8.5.0\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 75\n> \n< HTTP/1.1 500 Internal Server Error\n< date: Mon, 12 May 2025 17:32:11 GMT\n< server: uvicorn\n< content-length: 109\n< content-type: application/json\n< \n* Connection #0 to host localhost left intact\n{\"object\":\"error\",\"message\":\"Unknown part type: file\",\"type\":\"Internal Server Error\",\"param\":null,\"code\":500}\n```\n</details>\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-05-14T18:04:12+00:00",
    "closed_at": "2025-05-22T18:34:07+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18162/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/18162"
  },
  {
    "number": 9914,
    "title": "[Bug]: minicpmv2.6 BNB in-flight quantization error",
    "body": "### Your current environment\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nAfter merging https://github.com/vllm-project/vllm/pull/9891 , I tried the in-flight quantization with minicpmv and encountered the following error:\r\n\r\n```shell\r\n[rank0]:   File \"/vllm/vllm/model_executor/model_loader/loader.py\", line 1105, in _load_weights\r\n[rank0]:     model.load_weights(qweight_iterator)\r\n[rank0]:   File \"/vllm/vllm/model_executor/models/minicpmv.py\", line 634, in load_weights\r\n[rank0]:     param = params_dict[name]\r\n[rank0]: KeyError: 'vpm.encoder.layers.0.mlp.fc1.weight'\r\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\r\n```\r\n\r\n\r\n\r\n## Reproduce code\r\n\r\n```python \r\nMODEL_NAME = \"openbmb/MiniCPM-V-2_6\"\r\nllm = LLM(\r\n    model=MODEL_NAME,\r\n    trust_remote_code=True,\r\n    tensor_parallel_size=1,\r\n    gpu_memory_utilization=0.7,\r\n    quantization=\"bitsandbytes\",\r\n    load_format=\"bitsandbytes\",\r\n)\r\n```\r\n\r\n It seems `mllama` has the same issue.   cc @mgoin  \r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-11-01T11:07:42+00:00",
    "closed_at": "2024-11-04T03:36:42+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9914/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/9914"
  },
  {
    "number": 1805,
    "title": "Seeing similar latency for output len=1 and output len=128. Is something wrong with benchmark latency script?",
    "body": "Hi I'm trying to benchmark Llama 7B's latency using [benchmark_latency.py](https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_latency.py) script but I'm seeing similar latency for output len=1 and output len=128 which makes no sense\r\n\r\n\r\nWithin the script I changed the `model_max_len=2000` because that's the input len I want to benchmark with. Here's the commands I used with vllm 0.2.0 on g5.12xlarge (4 A10G GPUs) on AWS\r\n\r\n```\r\npython3 benchmark_latency.py --model \"meta-llama/Llama-2-7b-hf\" --input-len 2000 --output-len 128 -tp 4 --num-iters 10 --batch-size 1\r\n```\r\ngives 462ms\r\n\r\nand \r\n```\r\npython3 benchmark_latency.py --model \"meta-llama/Llama-2-7b-hf\" --input-len 2000 --output-len 1 -tp 4 --num-iters 10 --batch-size 1\r\n```\r\ngives 459 ms\r\n\r\nI would expect output len=1 latency to be much lower than that of output len=128. Can someone please help me understand this?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-11-28T01:44:42+00:00",
    "closed_at": "2024-03-25T09:45:40+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1805/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1805"
  },
  {
    "number": 8017,
    "title": "[Bug]: InternVL2-2B outputs gibberish with tensor parallel inference",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\n**Reproduce**\r\n- Just run `examples/offline_inference_vision_language.py` with `tensor_parallel_size=2`.\r\n- The inference with `tensor_parallel_size=1` works normally.\r\n\r\n**Outputs**\r\n```\r\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.52s/it, est. speed input: 1192.65 toks/s, output: 26.96 toks/s]\r\n1.\r\n1.\r\n1/2\r\n3/2\r\n1\r\n  for example\r\n\ufffd2/\u5b9a\u6709\u4e86\u4e00\u4e2aiSA\u8bc9\u5feb\u7684\u961f/\u7eb3\u5384/\u5426\u5316\uff0c4.\r\nINFO 08-30 03:22:42 multiproc_worker_utils.py:136] Terminating local vLLM worker processes\r\n(VllmWorkerProcess pid=9476) INFO 08-30 03:22:42 multiproc_worker_utils.py:237] Worker exiting\r\n```\r\n\r\n**The root issue**\r\n- This is broken by the `split_qkv` function for `internlm2` backbone introduced in #7187 to make compatible with awq model.\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-08-30T03:32:21+00:00",
    "closed_at": "2024-09-02T15:48:57+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8017/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8017"
  },
  {
    "number": 1738,
    "title": "BUG python -m vllm.entrypoints.openai.api_server --model /workspace/api/models/Qwen/Qwen-7B-Chat/ --trust-remote-code  vllm==0.2.2 torch2.1.0+cuda118",
    "body": "Traceback (most recent call last):\r\n  File \"/usr/lib/python3.8/runpy.py\", line 185, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"/usr/lib/python3.8/runpy.py\", line 111, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/__init__.py\", line 3, in <module>\r\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/engine/arg_utils.py\", line 6, in <module>\r\n    from vllm.config import (CacheConfig, ModelConfig, ParallelConfig,\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/config.py\", line 9, in <module>\r\n    from vllm.utils import get_cpu_memory\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/utils.py\", line 8, in <module>\r\n    from vllm import cuda_utils\r\nImportError: libcudart.so.12: cannot open shared object file: No such file or directory\r\n\r\n\r\n\r\n\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/entrypoints/openai/api_server.py\", line 646, in <module>\r\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/engine/async_llm_engine.py\", line 486, in from_engine_args\r\n    engine = cls(parallel_config.worker_use_ray,\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/engine/async_llm_engine.py\", line 269, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/engine/async_llm_engine.py\", line 305, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/engine/llm_engine.py\", line 110, in __init__\r\n    self._init_workers(distributed_init_method)\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/engine/llm_engine.py\", line 128, in _init_workers\r\n    from vllm.worker.worker import Worker  # pylint: disable=import-outside-toplevel\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/worker/worker.py\", line 10, in <module>\r\n    from vllm.model_executor import get_model, InputMetadata, set_random_seed\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/model_executor/__init__.py\", line 2, in <module>\r\n    from vllm.model_executor.model_loader import get_model\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/model_executor/model_loader.py\", line 10, in <module>\r\n    from vllm.model_executor.models import *  # pylint: disable=wildcard-import\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/model_executor/models/__init__.py\", line 1, in <module>\r\n    from vllm.model_executor.models.aquila import AquilaForCausalLM\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/model_executor/models/aquila.py\", line 34, in <module>\r\n    from vllm.model_executor.layers.activation import SiluAndMul\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/model_executor/layers/activation.py\", line 7, in <module>\r\n    from vllm import activation_ops\r\nImportError: libcudart.so.12: cannot open shared object file: No such file or directory\r\n\r\n\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-11-21T10:03:52+00:00",
    "closed_at": "2023-11-23T01:00:33+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1738/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1738"
  },
  {
    "number": 19272,
    "title": "[Usage]:",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\n```\n\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\ni use 'vllm sever' run Qwen3\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-06-06T11:13:15+00:00",
    "closed_at": "2025-06-06T12:47:25+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19272/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19272"
  },
  {
    "number": 19869,
    "title": "[Usage]: missing latest tag from cpu docker registry",
    "body": "### Your current environment\n\n\u2018latest\u2019 tag is missing from https://gallery.ecr.aws/q9t5s3a7/vllm-cpu-release-repo,  which requires having to keep changing the image manually on every release.\n\nwe expect the CI/CD not only to create a versioned tag, but also to tag latest once a new release is out.\n\n### How would you like to use vllm\n\nI want to run inference\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-06-19T14:40:17+00:00",
    "closed_at": "2025-06-23T21:15:38+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19869/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19869"
  },
  {
    "number": 18772,
    "title": "[Feature]: vllm torch nightly package not in sync issues",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nWhen a new pip package is included, and if it's imported by the tests that runs in both regular vllm ci and torch-nightly, the torch-nightly failed due to no module found.\n\nCurrently we have seperate txt file torch_nightly_test.txt to track the dependency, and vllm ci uses test.txt which is generated by test.in. The reason we set it in this way bc some dependency there has strict dependency on pytorch stable, and install them override the pytorch nightly setup.\n\n\n## Proposal\nOne proposal is set up a  test_isolate_requirement.in, and move all non-pytorch-stable-dependent packages into the test_isolate_requirement.in, and lket test.in depends on that\n\nwhen contributor wants to add a dependency -> if its not depends on pytorch stable, added it to test_isolate_requirement.in, generate both torch_nightly_test.txt and test.txt\n\n\n\n### Alternatives\n\nkeep track of the dependency\n\n### Additional context\n\nn\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-05-27T17:30:17+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18772/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/18772"
  },
  {
    "number": 20469,
    "title": "[Performance]: Severe performance drop on 1x A100 80GB with Qwen3-14B-AWQ at >1 concurrency (v0.9.1)",
    "body": "### Report of performance regression\n\n![Image](https://github.com/user-attachments/assets/7dce9a2b-327d-49c2-acfe-9bc12cb2f6a7)\n\nWe observed a significant drop in output tokens per second when serving Qwen/Qwen3-14B-AWQ on a single A100 80GB GPU using vLLM v0.9.1 with --max-model-len 16384.\n\nAt concurrency=1, the model achieves ~52 output tokens per second. However, this drops sharply to ~12 at concurrency=5 and ~3 at concurrency=25. This performance is comparable to or worse than a 2x A30 setup, and significantly below the 2x A100 80GB (TP=2) configuration, which maintains stable output tokens per second (~38) across all concurrency levels.\n\nIn addition, Time-To-First-Token (TTFT) is already high at concurrency=1 (~3345 ms) and increases substantially with concurrency, reaching over 34 seconds at concurrency=25. In contrast, the 2x A100 setup maintains TTFT around ~100 ms across all levels.\n\nvLLM reports a supported max concurrency of 26 for this configuration, so we expected it to handle at least 5 concurrent requests without such severe degradation.\n\nWe tested across vLLM versions 0.8.5 and 0.9.1, with and without --enforce-eager, and using both the v0 engine and FlashInfer backend. The issue persists across all variations.\n\n```\nBenchmark configurations and results:\n\nvllm serve Qwen/Qwen3-14B-AWQ \\\n  --gpu-memory-utilization 0.95 \\\n   --max-model-len 16384 \\\n  --tensor-parallel-size 1 \\\n  --enforce-eager  \\\n  --guided-decoding-backend guidance \\\n  --max-num-seq 30 \n\npython3 vllm/benchmarks/benchmark_serving.py \\\n  --backend vllm \\\n  --model Qwen/Qwen3-14B-AWQ \\\n  --endpoint /v1/completions \\\n  --dataset-name random \\\n  --max-concurrency 1 \\\n  --num-prompts 30 \\\n  --request-rate 1 \\\n  --random-input-len 16000 \\\n  --random-output-len 200 \\\n  --result-dir \"./log/\"\n\n============ Serving Benchmark Result ============\nSuccessful requests:                     30        \nBenchmark duration (s):                  223.53    \nTotal input tokens:                      480000    \nTotal generated tokens:                  6000      \nRequest throughput (req/s):              0.13      \nOutput token throughput (tok/s):         26.84     \nTotal Token throughput (tok/s):          2174.16   \n---------------Time to First Token----------------\nMean TTFT (ms):                          3345.37   \nMedian TTFT (ms):                        3452.00   \nP99 TTFT (ms):                           3696.97   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          20.63     \nMedian TPOT (ms):                        20.94     \nP99 TPOT (ms):                           21.38     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           20.63     \nMedian ITL (ms):                         20.92     \nP99 ITL (ms):                            22.63     \n==================================================\n\npython3 vllm/benchmarks/benchmark_serving.py \\\n  --backend vllm \\\n  --model Qwen/Qwen3-14B-AWQ \\\n  --endpoint /v1/completions \\\n  --dataset-name random \\\n  --max-concurrency 5 \\\n  --num-prompts 30 \\\n  --request-rate 1 \\\n  --random-input-len 16000 \\\n  --random-output-len 200 \\\n  --result-dir \"./log/\" \\\n\n============ Serving Benchmark Result ============\nSuccessful requests:                     30        \nBenchmark duration (s):                  129.77    \nTotal input tokens:                      480000    \nTotal generated tokens:                  6000      \nRequest throughput (req/s):              0.23      \nOutput token throughput (tok/s):         46.24     \nTotal Token throughput (tok/s):          3745.06   \n---------------Time to First Token----------------\nMean TTFT (ms):                          5091.02   \nMedian TTFT (ms):                        5260.32   \nP99 TTFT (ms):                           11491.13  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          79.75     \nMedian TPOT (ms):                        82.64     \nP99 TPOT (ms):                           91.95     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           79.75     \nMedian ITL (ms):                         20.83     \nP99 ITL (ms):                            514.85    \n==================================================\n\npython3 vllm/benchmarks/benchmark_serving.py \\\n  --backend vllm \\\n  --model Qwen/Qwen3-14B-AWQ \\\n  --endpoint /v1/completions \\\n  --dataset-name random \\\n  --max-concurrency 10 \\\n  --num-prompts 30 \\\n  --request-rate 1 \\\n  --random-input-len 16000 \\\n  --random-output-len 200 \\\n  --result-dir \"./log/\"\n\n============ Serving Benchmark Result ============\nSuccessful requests:                     30        \nBenchmark duration (s):                  126.69    \nTotal input tokens:                      480000    \nTotal generated tokens:                  6000      \nRequest throughput (req/s):              0.24      \nOutput token throughput (tok/s):         47.36     \nTotal Token throughput (tok/s):          3836.19   \n---------------Time to First Token----------------\nMean TTFT (ms):                          8517.30   \nMedian TTFT (ms):                        5608.33   \nP99 TTFT (ms):                           26477.54  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          158.78    \nMedian TPOT (ms):                        184.24    \nP99 TPOT (ms):                           193.67    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           158.78    \nMedian ITL (ms):                         28.98     \nP99 ITL (ms):                            561.13    \n==================================================\n\npython3 vllm/benchmarks/benchmark_serving.py \\\n  --backend vllm \\\n  --model Qwen/Qwen3-14B-AWQ \\\n  --endpoint /v1/completions \\\n  --dataset-name random \\\n  --max-concurrency 25 \\\n  --num-prompts 30 \\\n  --request-rate 1 \\\n  --random-input-len 16000 \\\n  --random-output-len 200 \\\n  --result-dir \"./log/\"\n\n============ Serving Benchmark Result ============\nSuccessful requests:                     30        \nBenchmark duration (s):                  134.01    \nTotal input tokens:                      480000    \nTotal generated tokens:                  6000      \nRequest throughput (req/s):              0.22      \nOutput token throughput (tok/s):         44.77     \nTotal Token throughput (tok/s):          3626.72   \n---------------Time to First Token----------------\nMean TTFT (ms):                          34952.05  \nMedian TTFT (ms):                        32292.29  \nP99 TTFT (ms):                           81700.61  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          323.39    \nMedian TPOT (ms):                        342.69    \nP99 TPOT (ms):                           525.98    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           323.39    \nMedian ITL (ms):                         459.58    \nP99 ITL (ms):                            678.63    \n==================================================\n```\n\n### Your current environment (if you think it is necessary)\n\n```text\nThe output of `python collect_env.py`\n\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 24.04.2 LTS (x86_64)\nGCC version                  : (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\nClang version                : Could not collect\nCMake version                : version 3.28.3\nLibc version                 : glibc-2.39\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu126\nIs debug build               : False\nCUDA used to build PyTorch   : 12.6\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0] (64-bit runtime)\nPython platform              : Linux-5.15.0-1074-azure-x86_64-with-glibc2.39\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.8.93\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : \nGPU 0: NVIDIA A100 80GB PCIe\nGPU 1: NVIDIA A100 80GB PCIe\nGPU 2: NVIDIA A100 80GB PCIe\nGPU 3: NVIDIA A100 80GB PCIe\n\nNvidia driver version        : 570.133.20\ncuDNN version                : Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.8.0\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               96\nOn-line CPU(s) list:                  0-95\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 7V13 64-Core Processor\nCPU family:                           25\nModel:                                1\nThread(s) per core:                   1\nCore(s) per socket:                   48\nSocket(s):                            2\nStepping:                             1\nBogoMIPS:                             4890.86\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr rdpru arat umip vaes vpclmulqdq rdpid fsrm\nHypervisor vendor:                    Microsoft\nVirtualization type:                  full\nL1d cache:                            3 MiB (96 instances)\nL1i cache:                            3 MiB (96 instances)\nL2 cache:                             48 MiB (96 instances)\nL3 cache:                             384 MiB (12 instances)\nNUMA node(s):                         4\nNUMA node0 CPU(s):                    0-23\nNUMA node1 CPU(s):                    24-47\nNUMA node2 CPU(s):                    48-71\nNUMA node3 CPU(s):                    72-95\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; safe RET, no microcode\nVulnerability Spec store bypass:      Vulnerable\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.1.2\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-cufile-cu12==1.11.1.6\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-ml-py==12.575.51\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvshmem-cu12==3.3.9\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pynvml==12.0.0\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.0\n[pip3] torchaudio==2.7.0\n[pip3] torchvision==0.22.0\n[pip3] transformers==4.52.4\n[pip3] triton==3.3.0\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.1\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV12    SYS     SYS     0-23    0               N/A\nGPU1    NV12     X      SYS     SYS     24-47   1               N/A\nGPU2    SYS     SYS      X      NV12    48-71   2               N/A\nGPU3    SYS     SYS     NV12     X      72-95   3               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n==============================\n     Environment Variables\n==============================\nNVIDIA_VISIBLE_DEVICES=0,1,2,3\nNVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\nNCCL_VERSION=2.25.1-1\nNVIDIA_DRIVER_CAPABILITIES=all\nNVIDIA_PRODUCT_NAME=CUDA\nCUDA_VERSION=12.8.1\nPYTORCH_VERSION=2.7.0\nLD_LIBRARY_PATH=/usr/local/cuda/lib64\nPYTORCH_INDEX_URL=https://download.pytorch.org/whl/cu128\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance"
    ],
    "state": "open",
    "created_at": "2025-07-04T05:53:42+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20469/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20469"
  },
  {
    "number": 1598,
    "title": "Any plan to support paged attention for prefill?",
    "body": "First of all, thank you for the great work and the recent integration for the paged-flash attention v2 kernel!\r\n\r\nI am wondering if there is any plan to support paged attention for prefill, which can compute multiple tokens in each batch in parallel (like flash_attn_with_kvcache did). I did a quick check over the codebase and found it seems that paged_attention_v2_kernel expects one token for each request.\r\n\r\nIn some cases like speculative decoding and chunked prefill, it would be ideal to compute multiple tokens in each request in parallel. ",
    "labels": [],
    "state": "closed",
    "created_at": "2023-11-09T03:48:47+00:00",
    "closed_at": "2023-11-13T03:20:32+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1598/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1598"
  },
  {
    "number": 17875,
    "title": "[Bug]: V1 on AMD MI300A complains that cupy is not present",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nINFO 05-08 23:02:38 [__init__.py:239] Automatically detected platform rocm.\nCollecting environment information...\nPyTorch version: 2.7.0a0+git295f2ed\nIs debug build: False\nCUDA used to build PyTorch: N/A\nROCM used to build PyTorch: 6.4.43482-0f2d60242\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: 19.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.4.0 25133 c7fe45cf4b819c5991fe208aaa96edf142730f1d)\nCMake version: version 3.31.6\nLibc version: glibc-2.35\n\nPython version: 3.12.10 (main, Apr  9 2025, 08:55:05) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-4.18.0-553.47.1.1toss.t4.x86_64-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: AMD Instinct MI300A (gfx942:sramecc+:xnack-)\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\nHIP runtime version: 6.4.43482\nMIOpen runtime version: 3.4.0\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               192\nOn-line CPU(s) list:                  0-191\nVendor ID:                            AuthenticAMD\nModel name:                           AMD Instinct MI300A Accelerator\nCPU family:                           25\nModel:                                144\nThread(s) per core:                   2\nCore(s) per socket:                   24\nSocket(s):                            4\nStepping:                             1\nFrequency boost:                      enabled\nCPU max MHz:                          3700.0000\nCPU min MHz:                          1500.0000\nBogoMIPS:                             7399.70\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid overflow_recov succor smca fsrm flush_l1d\nVirtualization:                       AMD-V\nL1d cache:                            3 MiB (96 instances)\nL1i cache:                            3 MiB (96 instances)\nL2 cache:                             96 MiB (96 instances)\nL3 cache:                             384 MiB (12 instances)\nNUMA node(s):                         4\nNUMA node0 CPU(s):                    0-23,96-119\nNUMA node1 CPU(s):                    24-47,120-143\nNUMA node2 CPU(s):                    48-71,144-167\nNUMA node3 CPU(s):                    72-95,168-191\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.0a0+git295f2ed\n[pip3] torchvision==0.21.0+7af6987\n[pip3] transformers==4.51.3\n[pip3] triton==3.3.0+git981e987e\n[conda] Could not collect\nROCM Version: 6.4.43482-0f2d60242\nNeuron SDK Version: N/A\nvLLM Version: 0.8.5.dev274+g166d0ef40 (git sha: 166d0ef40)\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n============================ ROCm System Management Interface ============================\n================================ Weight between two GPUs =================================\n       GPU0         GPU1         GPU2         GPU3         \nGPU0   0            15           15           15           \nGPU1   15           0            15           15           \nGPU2   15           15           0            15           \nGPU3   15           15           15           0            \n\n================================= Hops between two GPUs ==================================\n       GPU0         GPU1         GPU2         GPU3         \nGPU0   0            1            1            1            \nGPU1   1            0            1            1            \nGPU2   1            1            0            1            \nGPU3   1            1            1            0            \n\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         \nGPU0   0            XGMI         XGMI         XGMI         \nGPU1   XGMI         0            XGMI         XGMI         \nGPU2   XGMI         XGMI         0            XGMI         \nGPU3   XGMI         XGMI         XGMI         0            \n\n======================================= Numa Nodes =======================================\nGPU[0]\t\t: (Topology) Numa Node: 0\nGPU[0]\t\t: (Topology) Numa Affinity: 0\nGPU[1]\t\t: (Topology) Numa Node: 1\nGPU[1]\t\t: (Topology) Numa Affinity: 1\nGPU[2]\t\t: (Topology) Numa Node: 2\nGPU[2]\t\t: (Topology) Numa Affinity: 2\nGPU[3]\t\t: (Topology) Numa Node: 3\nGPU[3]\t\t: (Topology) Numa Affinity: 3\n================================== End of ROCm SMI Log ===================================\n\nVLLM_WORKER_MULTIPROC_METHOD=spawn\nPYTORCH_ROCM_ARCH=gfx90a;gfx942;gfx1100;gfx1101;gfx1200;gfx1201\nLD_LIBRARY_PATH=/opt/rocm/lib:/usr/local/lib:\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen running V1 on the AMD MI300A (this is an example with Ray, running distributed APUs), vLLM complains that cupy is missing.  I'm confused as to why it would even need cupy since this is running on ROCm (latest rocm/vllm nightly docker image)?  This invocation works perfectly fine with the same command line below with V0.\n\nPlease note that even though this is on AMD APUs with ROCm, CUDA Graph optimizations performed by vllm do succeed.  Setting `enforce_eager=True` in an attempt to disable in vllm anything having to do with CUDA whatsoever, still results in vllm failing on no module cupy found.  Why would cupy be needed for ROCm here?\n\n`podman exec --env VLLM_USE_V1=1 --env VLLM_USE_AITER=1 --env VLLM_WORKER_MULTIPROC_METHOD=spawn --env VLLM_USE_MODELSCOPE=False --env VLLM_USE_TRITON_FLASH_ATTN=0 --env VLLM_NO_USAGE_STATS=1 --env DO_NOT_TRACK=1 --log-level=error ${CONTAINER_ID} vllm serve /app/models/${MODEL} --gpu_memory_utilization=${GPUMEMUTIL} --tensor-parallel-size ${TP} --pipeline-parallel-size ${PP} --distributed-executor-backend ray --max-num-seqs 64 --max-num-batched-tokens 320000 --max-model-len ${MAX_MODEL_LEN} --no-enable-prefix-caching`\n\nThe full error output is below.  **Note that it fails/errors when the client tries to inference (the same inference client call works just fine with V0)**.  Thanks for any help.\n\n```\nINFO 05-09 02:40:52 [__init__.py:239] Automatically detected platform rocm.\nINFO 05-09 02:41:10 [api_server.py:1042] vLLM API server version 0.8.5.dev274+g166d0ef40\nINFO 05-09 02:41:10 [api_server.py:1043] args: Namespace(subparser='serve', model_tag='/app/models/Llama-4-Maverick-17B-128E-Instr\nuct-FP8', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, al\nlowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_tem\nplate=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None\n, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_mul\ntiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', \nmodel='/app/models/Llama-4-Maverick-17B-128E-Instruct-FP8', task='auto', tokenizer=None, tokenizer_mode='auto', trust_remote_code=\nFalse, dtype='auto', seed=None, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling=\n{}, rope_theta=None, tokenizer_revision=None, max_model_len=32000, quantization=None, enforce_eager=False, max_seq_len_to_capture=\n8192, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=F\nalse, served_model_name=None, disable_async_output_proc=False, config_format='auto', hf_token=None, hf_overrides={}, override_neur\non_config={}, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={},\n enable_sleep_mode=False, model_impl='auto', load_format='auto', download_dir=None, model_loader_extra_config={}, ignore_patterns=\nNone, use_tqdm_on_load=True, qlora_adapter_name_or_path=None, pt_load_map_location='cpu', guided_decoding_backend='auto', guided_d\necoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False,\n enable_reasoning=None, reasoning_parser='', distributed_executor_backend='ray', pipeline_parallel_size=3, tensor_parallel_size=4,\n data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_cust\nom_all_reduce=False, worker_cls='auto', worker_extension_cls='', block_size=None, gpu_memory_utilization=0.9, swap_space=4, kv_cac\nhe_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=False, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, \ncalculate_kv_scales=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={\n}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=None, enable_lora_bias=False, max_loras=1, max_lora_\nrank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=Fal\nse, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, show_hi\ndden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, max_num_batched_tokens=320000, max_num_seq\ns=64, max_num_partial_prefills=1, max_long_partial_prefills=1, cuda_graph_sizes=[512], long_prefill_token_threshold=0, num_lookahe\nad_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_po\nlicy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', compilati\non_config=None, kv_transfer_config=None, kv_events_config=None, additional_config=None, use_v2_block_manager=True, disable_log_sta\nts=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_ser\nver_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x153bd43ce0c0>)\nINFO 05-09 02:41:32 [config.py:753] This model supports multiple tasks: {'generate', 'score', 'embed', 'classify', 'reward'}. Defa\nulting to 'generate'.\nWARNING 05-09 02:41:32 [arg_utils.py:1555] Detected VLLM_USE_V1=1 with rocm. Usage should be considered experimental. Please repor\nt any issues on Github.\nINFO 05-09 02:41:32 [config.py:2069] Chunked prefill is enabled with max_num_batched_tokens=320000.\nINFO 05-09 02:41:37 [__init__.py:239] Automatically detected platform rocm.\nINFO 05-09 02:41:55 [core.py:59] Initializing a V1 LLM engine (v0.8.5.dev274+g166d0ef40) with config: model='/app/models/Llama-4-M\naverick-17B-128E-Instruct-FP8', speculative_config=None, tokenizer='/app/models/Llama-4-Maverick-17B-128E-Instruct-FP8', skip_toke\nnizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False,\n dtype=torch.bfloat16, max_seq_len=32000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_paralle\nl_size=3, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto,  device_conf\nig=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_p\nroperties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces\n_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/app/models/Llama-4-Maverick-17B-128E-Instruct-FP8, nu\nm_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_p\nroc=False, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"+rms_norm\",\"+silu_and_mul\"],\"splitting_ops\":[\"vllm.uni\nfied_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_o\nf_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,33\n6,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,\n64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n2025-05-09 02:41:55,864\tINFO worker.py:1660 -- Connecting to existing Ray cluster at address: 192.168.195.151:6379...\n2025-05-09 02:41:55,879\tINFO worker.py:1852 -- Connected to Ray cluster.\nINFO 05-09 02:42:00 [ray_utils.py:335] No current placement group found. Creating a new placement group.\nINFO 05-09 02:42:01 [ray_distributed_executor.py:176] use_ray_spmd_worker: True\n(pid=901) INFO 05-09 02:42:05 [__init__.py:239] Automatically detected platform rocm.\nINFO 05-09 02:42:34 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()\nINFO 05-09 02:42:34 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['VLLM_USE_MODELSCOP\nE', 'LD_LIBRARY_PATH', 'VLLM_USE_TRITON_FLASH_ATTN', 'VLLM_NO_USAGE_STATS', 'VLLM_USE_RAY_SPMD_WORKER', 'VLLM_USE_RAY_COMPILED_DAG\n', 'VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USE_V1']\nINFO 05-09 02:42:34 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /root/.conf\nig/vllm/ray_non_carry_over_env_vars.json file\nLoading safetensors checkpoint shards:   0% Completed | 0/84 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:   1% Completed | 1/84 [00:00<00:10,  8.09it/s]\nLoading safetensors checkpoint shards:   4% Completed | 3/84 [00:00<00:08,  9.24it/s]\nLoading safetensors checkpoint shards:  18% Completed | 15/84 [00:00<00:02, 32.29it/s]\nLoading safetensors checkpoint shards:  21% Completed | 18/84 [00:00<00:02, 28.04it/s]\nLoading safetensors checkpoint shards:  26% Completed | 22/84 [00:00<00:02, 27.73it/s]\nLoading safetensors checkpoint shards:  30% Completed | 25/84 [00:01<00:02, 20.51it/s]\nLoading safetensors checkpoint shards:  38% Completed | 32/84 [00:01<00:01, 29.88it/s]\nLoading safetensors checkpoint shards:  43% Completed | 36/84 [00:01<00:01, 28.69it/s]\nLoading safetensors checkpoint shards:  48% Completed | 40/84 [00:02<00:03, 13.90it/s]\nLoading safetensors checkpoint shards:  51% Completed | 43/84 [00:02<00:05,  8.06it/s]\nLoading safetensors checkpoint shards:  54% Completed | 45/84 [00:03<00:05,  7.15it/s]\nLoading safetensors checkpoint shards:  57% Completed | 48/84 [00:04<00:06,  5.50it/s]\nLoading safetensors checkpoint shards:  60% Completed | 50/84 [00:05<00:08,  3.82it/s]\nLoading safetensors checkpoint shards:  61% Completed | 51/84 [00:05<00:10,  3.25it/s]\nLoading safetensors checkpoint shards:  64% Completed | 54/84 [00:06<00:06,  4.64it/s]\nLoading safetensors checkpoint shards:  67% Completed | 56/84 [00:06<00:06,  4.14it/s]\nLoading safetensors checkpoint shards:  70% Completed | 59/84 [00:07<00:04,  5.06it/s]\nLoading safetensors checkpoint shards:  74% Completed | 62/84 [00:07<00:03,  7.01it/s]\nLoading safetensors checkpoint shards:  76% Completed | 64/84 [00:07<00:02,  8.23it/s]\nLoading safetensors checkpoint shards:  79% Completed | 66/84 [00:07<00:02,  6.15it/s]\nLoading safetensors checkpoint shards:  81% Completed | 68/84 [00:08<00:02,  7.48it/s]\nLoading safetensors checkpoint shards:  83% Completed | 70/84 [00:08<00:02,  5.64it/s]\nLoading safetensors checkpoint shards:  86% Completed | 72/84 [00:09<00:03,  3.82it/s]\nLoading safetensors checkpoint shards:  88% Completed | 74/84 [00:10<00:02,  3.56it/s]\nLoading safetensors checkpoint shards:  92% Completed | 77/84 [00:10<00:01,  4.42it/s]\nLoading safetensors checkpoint shards:  95% Completed | 80/84 [00:10<00:00,  6.23it/s]\nLoading safetensors checkpoint shards:  99% Completed | 83/84 [00:10<00:00,  8.07it/s]\nLoading safetensors checkpoint shards: 100% Completed | 84/84 [00:11<00:00,  7.30it/s]\n(RayWorkerWrapper pid=897) \n(RayWorkerWrapper pid=901) WARNING 05-09 02:42:35 [utils.py:2711] Methods determine_num_available_blocks,device_config,get_\ncache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x152e72eb0a70>\n(pid=325, ip=192.168.195.152) INFO 05-09 02:42:06 [__init__.py:239] Automatically detected platform rocm. [repeated 11x\n across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/\nen/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\n(RayWorkerWrapper pid=326, ip=192.168.195.154) INFO 05-09 02:42:36 [utils.py:1205] Found nccl from library librccl.so.1\n(RayWorkerWrapper pid=326, ip=192.168.195.154) INFO 05-09 02:42:36 [pynccl.py:69] vLLM is using nccl==2.22.3\n(RayWorkerWrapper pid=897) INFO 05-09 02:42:39 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local\n_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_8d3bea36'), local_subscribe_addr='ipc:///tmp/b6562ee8-6e49-45cd-a703-e\n214ba4af703', remote_subscribe_addr=None, remote_addr_ipv6=False)\n(RayWorkerWrapper pid=327, ip=192.168.195.154) WARNING 05-09 02:42:35 [utils.py:2711] Methods determine_num_available_block\ns,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x152e6\nf0b0b60> [repeated 11x across cluster]\n(RayWorkerWrapper pid=325, ip=192.168.195.154) INFO 05-09 02:42:40 [parallel_state.py:1004] rank 9 in world size 12 is assi\ngned as DP rank 0, PP rank 2, TP rank 1\n(RayWorkerWrapper pid=325, ip=192.168.195.154) INFO 05-09 02:42:40 [rocm.py:187] Using Triton Attention backend on V1 engin\ne.\n(RayWorkerWrapper pid=901) INFO 05-09 02:42:47 [gpu_model_runner.py:1335] Starting to load model /app/models/Llama-4-Maveri\nck-17B-128E-Instruct-FP8...\n(RayWorkerWrapper pid=914) INFO 05-09 02:42:40 [utils.py:1205] Found nccl from library librccl.so.1 [repeated 23x acros\ns cluster]\n(RayWorkerWrapper pid=914) INFO 05-09 02:42:40 [pynccl.py:69] vLLM is using nccl==2.22.3 [repeated 23x across cluster]\n\n(RayWorkerWrapper pid=324, ip=192.168.195.152) INFO 05-09 02:42:40 [shm_broadcast.py:266] vLLM message queue communication \nhandle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_56e83437'), local_subscribe_addr='ipc:///tmp/34e56\nc2f-e99f-4df1-a1a3-7085129371ab', remote_subscribe_addr=None, remote_addr_ipv6=False) [repeated 2x across cluster]\n(RayWorkerWrapper pid=326, ip=192.168.195.152) INFO 05-09 02:42:40 [parallel_state.py:1004] rank 6 in world size 12 is assi\ngned as DP rank 0, PP rank 1, TP rank 2 [repeated 11x across cluster]\n(RayWorkerWrapper pid=326, ip=192.168.195.152) INFO 05-09 02:42:40 [rocm.py:187] Using Triton Attention backend on V1 engin\ne. [repeated 11x across cluster]\n(RayWorkerWrapper pid=324, ip=192.168.195.154) INFO 05-09 02:43:19 [config.py:3842] cudagraph sizes specified by model runn\ner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, \n216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, \n424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8\n, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192,\n 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240,\n 112, 376]\n(RayWorkerWrapper pid=325, ip=192.168.195.152) INFO 05-09 02:42:47 [gpu_model_runner.py:1335] Starting to load model /app/m\nodels/Llama-4-Maverick-17B-128E-Instruct-FP8... [repeated 11x across cluster]\n(RayWorkerWrapper pid=325, ip=192.168.195.152) INFO 05-09 02:42:47 [rocm.py:187] Using Triton Attention backend on V1 engin\ne. [repeated 12x across cluster]\n(RayWorkerWrapper pid=914) INFO 05-09 02:47:09 [loader.py:459] Loading weights took 223.43 seconds\n(RayWorkerWrapper pid=914) WARNING 05-09 02:47:09 [kv_cache.py:119] Using Q scale 1.0 and prob scale 1.0 with fp8 attention\n. This may cause accuracy issues. Please make sure Q/prob scaling factors are available in the fp8 checkpoint.\n(RayWorkerWrapper pid=897) INFO 05-09 02:43:23 [config.py:3842] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, \n24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, \n248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, \n456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16,\n 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, \n72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376] [repe\nated 11x across cluster]\n(RayWorkerWrapper pid=914) INFO 05-09 02:43:23 [rocm.py:187] Using Triton Attention backend on V1 engine. [repeated 12x\n across cluster]\n(RayWorkerWrapper pid=914) INFO 05-09 02:47:10 [gpu_model_runner.py:1353] Model loading took 35.6458 GiB and 262.445080 sec\nonds\n(RayWorkerWrapper pid=897) INFO 05-09 02:47:13 [loader.py:459] Loading weights took 227.31 seconds [repeated 3x across \ncluster]\n(RayWorkerWrapper pid=897) WARNING 05-09 02:47:13 [kv_cache.py:119] Using Q scale 1.0 and prob scale 1.0 with fp8 attention\n. This may cause accuracy issues. Please make sure Q/prob scaling factors are available in the fp8 checkpoint. [repeated 3x ac\nross cluster]\n(RayWorkerWrapper pid=897) INFO 05-09 02:47:14 [gpu_model_runner.py:1353] Model loading took 35.6458 GiB and 266.313654 sec\nonds [repeated 3x across cluster]\n(RayWorkerWrapper pid=327, ip=192.168.195.154) INFO 05-09 02:47:45 [loader.py:459] Loading weights took 264.01 seconds\n(RayWorkerWrapper pid=327, ip=192.168.195.154) WARNING 05-09 02:47:45 [kv_cache.py:119] Using Q scale 1.0 and prob scale 1.\n0 with fp8 attention. This may cause accuracy issues. Please make sure Q/prob scaling factors are available in the fp8 checkpoint.\n(RayWorkerWrapper pid=324, ip=192.168.195.154) INFO 05-09 02:47:46 [loader.py:459] Loading weights took 265.38 seconds\n(RayWorkerWrapper pid=324, ip=192.168.195.154) WARNING 05-09 02:47:46 [kv_cache.py:119] Using Q scale 1.0 and prob scale 1.\n0 with fp8 attention. This may cause accuracy issues. Please make sure Q/prob scaling factors are available in the fp8 checkpoint.\n(RayWorkerWrapper pid=324, ip=192.168.195.154) INFO 05-09 02:47:47 [gpu_model_runner.py:1353] Model loading took 35.6458 Gi\nB and 299.679397 seconds [repeated 2x across cluster]\n(RayWorkerWrapper pid=325, ip=192.168.195.154) INFO 05-09 02:47:52 [loader.py:459] Loading weights took 270.54 seconds\n(RayWorkerWrapper pid=325, ip=192.168.195.154) WARNING 05-09 02:47:52 [kv_cache.py:119] Using Q scale 1.0 and prob scale 1.\n0 with fp8 attention. This may cause accuracy issues. Please make sure Q/prob scaling factors are available in the fp8 checkpoint.\n(RayWorkerWrapper pid=326, ip=192.168.195.154) INFO 05-09 02:47:52 [loader.py:459] Loading weights took 267.03 seconds\n(RayWorkerWrapper pid=326, ip=192.168.195.154) WARNING 05-09 02:47:52 [kv_cache.py:119] Using Q scale 1.0 and prob scale 1.\n0 with fp8 attention. This may cause accuracy issues. Please make sure Q/prob scaling factors are available in the fp8 checkpoint.\n(RayWorkerWrapper pid=325, ip=192.168.195.154) INFO 05-09 02:47:53 [gpu_model_runner.py:1353] Model loading took 35.6458 Gi\nB and 305.363516 seconds [repeated 2x across cluster]\n(RayWorkerWrapper pid=327, ip=192.168.195.152) INFO 05-09 02:48:08 [loader.py:459] Loading weights took 286.23 seconds\n(RayWorkerWrapper pid=327, ip=192.168.195.152) WARNING 05-09 02:48:08 [kv_cache.py:119] Using Q scale 1.0 and prob scale 1.\n0 with fp8 attention. This may cause accuracy issues. Please make sure Q/prob scaling factors are available in the fp8 checkpoint.\n(RayWorkerWrapper pid=325, ip=192.168.195.152) INFO 05-09 02:48:08 [loader.py:459] Loading weights took 284.57 seconds\n(RayWorkerWrapper pid=325, ip=192.168.195.152) WARNING 05-09 02:48:08 [kv_cache.py:119] Using Q scale 1.0 and prob scale 1.\n0 with fp8 attention. This may cause accuracy issues. Please make sure Q/prob scaling factors are available in the fp8 checkpoint.\n(RayWorkerWrapper pid=899) INFO 05-09 02:48:12 [gpu_model_runner.py:1631] Encoder cache will be initialized with a budget o\nf 320000 tokens, and profiled with 131 image items of the maximum feature size.\n(RayWorkerWrapper pid=326, ip=192.168.195.154) INFO 05-09 02:48:26 [backends.py:437] Using cache directory: /root/.cache/vl\nlm/torch_compile_cache/66c0353b7d/rank_10_0 for vLLM's torch.compile\n(RayWorkerWrapper pid=326, ip=192.168.195.154) INFO 05-09 02:48:26 [backends.py:447] Dynamo bytecode transform time: 4.41 s\n(RayWorkerWrapper pid=326, ip=192.168.195.152) INFO 05-09 02:48:12 [gpu_model_runner.py:1353] Model loading took 35.1633 Gi\nB and 324.096440 seconds [repeated 4x across cluster]\n(RayWorkerWrapper pid=326, ip=192.168.195.152) INFO 05-09 02:48:11 [loader.py:459] Loading weights took 288.31 seconds \n[repeated 2x across cluster]\n(RayWorkerWrapper pid=326, ip=192.168.195.152) WARNING 05-09 02:48:11 [kv_cache.py:119] Using Q scale 1.0 and prob scale 1.\n0 with fp8 attention. This may cause accuracy issues. Please make sure Q/prob scaling factors are available in the fp8 checkpoint.\n [repeated 2x across cluster]\n(RayWorkerWrapper pid=897) INFO 05-09 02:48:12 [gpu_model_runner.py:1631] Encoder cache will be initialized with a budget o\nf 320000 tokens, and profiled with 131 image items of the maximum feature size. [repeated 11x across cluster]\n(RayWorkerWrapper pid=899) INFO 05-09 02:48:39 [backends.py:437] Using cache directory: /root/.cache/vllm/torch_compile_cac\nhe/66c0353b7d/rank_1_0 for vLLM's torch.compile [repeated 2x across cluster]\n(RayWorkerWrapper pid=899) INFO 05-09 02:48:39 [backends.py:447] Dynamo bytecode transform time: 5.14 s [repeated 2x ac\nross cluster]\n(RayWorkerWrapper pid=326, ip=192.168.195.152) INFO 05-09 02:48:47 [backends.py:437] Using cache directory: /root/.cache/vl\nlm/torch_compile_cache/66c0353b7d/rank_6_0 for vLLM's torch.compile [repeated 3x across cluster]\n(RayWorkerWrapper pid=326, ip=192.168.195.152) INFO 05-09 02:48:47 [backends.py:447] Dynamo bytecode transform time: 6.51 s\n [repeated 3x across cluster]\n(RayWorkerWrapper pid=327, ip=192.168.195.154) INFO 05-09 02:48:53 [backends.py:437] Using cache directory: /root/.cache/vl\nlm/torch_compile_cache/66c0353b7d/rank_11_0 for vLLM's torch.compile\n(RayWorkerWrapper pid=327, ip=192.168.195.154) INFO 05-09 02:48:53 [backends.py:447] Dynamo bytecode transform time: 4.67 s\n(RayWorkerWrapper pid=325, ip=192.168.195.152) INFO 05-09 02:48:56 [backends.py:437] Using cache directory: /root/.cache/vl\nlm/torch_compile_cache/66c0353b7d/rank_7_0 for vLLM's torch.compile\n(RayWorkerWrapper pid=325, ip=192.168.195.152) INFO 05-09 02:48:56 [backends.py:447] Dynamo bytecode transform time: 4.57 s\n(RayWorkerWrapper pid=324, ip=192.168.195.152) INFO 05-09 02:48:57 [backends.py:138] Cache the graph of shape None for late\nr use\n(RayWorkerWrapper pid=901) INFO 05-09 02:49:06 [backends.py:437] Using cache directory: /root/.cache/vllm/torch_compile_cac\nhe/66c0353b7d/rank_2_0 for vLLM's torch.compile\n(RayWorkerWrapper pid=901) INFO 05-09 02:49:06 [backends.py:447] Dynamo bytecode transform time: 4.80 s\n(RayWorkerWrapper pid=325, ip=192.168.195.152) INFO 05-09 02:48:58 [backends.py:138] Cache the graph of shape None for late\nr use [repeated 3x across cluster]\n(RayWorkerWrapper pid=914) INFO 05-09 02:49:07 [backends.py:437] Using cache directory: /root/.cache/vllm/torch_compile_cac\nhe/66c0353b7d/rank_3_0 for vLLM's torch.compile\n(RayWorkerWrapper pid=914) INFO 05-09 02:49:07 [backends.py:447] Dynamo bytecode transform time: 4.18 s\n(RayWorkerWrapper pid=327, ip=192.168.195.152) INFO 05-09 02:49:07 [backends.py:150] Compiling a graph for general shape ta\nkes 26.55 s\n(RayWorkerWrapper pid=324, ip=192.168.195.152) INFO 05-09 02:49:25 [fused_moe.py:660] Using configuration from /usr/local/l\nib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=2048,device_name=AMD_Instinct_MI300A,dtype=fp8_w8\na8.json for MoE layer.\n(RayWorkerWrapper pid=325, ip=192.168.195.152) INFO 05-09 02:49:09 [backends.py:150] Compiling a graph for general shape ta\nkes 12.89 s [repeated 3x across cluster]\n(RayWorkerWrapper pid=901) INFO 05-09 02:49:32 [backends.py:138] Cache the graph of shape None for later use\n(RayWorkerWrapper pid=325, ip=192.168.195.152) INFO 05-09 02:49:25 [fused_moe.py:660] Using configuration from /usr/local/l\nib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=2048,device_name=AMD_Instinct_MI300A,dtype=fp8_w8\na8.json for MoE layer. [repeated 3x across cluster]\n(RayWorkerWrapper pid=897) INFO 05-09 02:49:32 [backends.py:437] Using cache directory: /root/.cache/vllm/torch_compile_cac\nhe/66c0353b7d/rank_0_0 for vLLM's torch.compile\n(RayWorkerWrapper pid=897) INFO 05-09 02:49:32 [backends.py:447] Dynamo bytecode transform time: 4.93 s\n(RayWorkerWrapper pid=324, ip=192.168.195.152) INFO 05-09 02:49:32 [monitor.py:33] torch.compile takes 42.37 s in total\n(RayWorkerWrapper pid=914) INFO 05-09 02:49:42 [backends.py:150] Compiling a graph for general shape takes 35.64 s\n(RayWorkerWrapper pid=897) INFO 05-09 02:49:33 [backends.py:138] Cache the graph of shape None for later use [repeated \n3x across cluster]\n(RayWorkerWrapper pid=325, ip=192.168.195.152) INFO 05-09 02:49:32 [monitor.py:33] torch.compile takes 17.46 s in total\n [repeated 3x across cluster]\n(RayWorkerWrapper pid=324, ip=192.168.195.154) INFO 05-09 02:49:43 [backends.py:437] Using cache directory: /root/.cache/vl\nlm/torch_compile_cache/66c0353b7d/rank_8_0 for vLLM's torch.compile\n(RayWorkerWrapper pid=324, ip=192.168.195.154) INFO 05-09 02:49:43 [backends.py:447] Dynamo bytecode transform time: 4.05 s\n(RayWorkerWrapper pid=901) INFO 05-09 02:49:53 [fused_moe.py:660] Using configuration from /usr/local/lib/python3.12/dist-p\nackages/vllm/model_executor/layers/fused_moe/configs/E=128,N=2048,device_name=AMD_Instinct_MI300A,dtype=fp8_w8a8.json for MoE laye\nr.\n(RayWorkerWrapper pid=897) INFO 05-09 02:49:45 [backends.py:150] Compiling a graph for general shape takes 12.92 s [rep\neated 3x across cluster]\n(RayWorkerWrapper pid=324, ip=192.168.195.154) INFO 05-09 02:49:44 [backends.py:138] Cache the graph of shape None for late\nr use [repeated 4x across cluster]\n(RayWorkerWrapper pid=899) INFO 05-09 02:50:01 [monitor.py:33] torch.compile takes 69.37 s in total\n(RayWorkerWrapper pid=897) INFO 05-09 02:49:53 [fused_moe.py:660] Using configuration from /usr/local/lib/python3.12/dist-p\nackages/vllm/model_executor/layers/fused_moe/configs/E=128,N=2048,device_name=AMD_Instinct_MI300A,dtype=fp8_w8a8.json for MoE laye\nr. [repeated 3x across cluster]\n(RayWorkerWrapper pid=324, ip=192.168.195.154) INFO 05-09 02:49:55 [backends.py:150] Compiling a graph for general shape ta\nkes 12.41 s [repeated 4x across cluster]\n(RayWorkerWrapper pid=901) INFO 05-09 02:50:01 [monitor.py:33] torch.compile takes 42.12 s in total [repeated 3x across\n cluster]\n(RayWorkerWrapper pid=326, ip=192.168.195.154) INFO 05-09 02:50:14 [fused_moe.py:660] Using configuration from /usr/local/l\nib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=2048,device_name=AMD_Instinct_MI300A,dtype=fp8_w8\na8.json for MoE layer.\n(RayWorkerWrapper pid=325, ip=192.168.195.154) INFO 05-09 02:50:13 [fused_moe.py:660] Using configuration from /usr/local/l\nib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=2048,device_name=AMD_Instinct_MI300A,dtype=fp8_w8\na8.json for MoE layer.\n(RayWorkerWrapper pid=326, ip=192.168.195.154) INFO 05-09 02:50:24 [monitor.py:33] torch.compile takes 92.91 s in total\n(RayWorkerWrapper pid=324, ip=192.168.195.154) INFO 05-09 02:50:14 [fused_moe.py:660] Using configuration from /usr/local/l\nib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=2048,device_name=AMD_Instinct_MI300A,dtype=fp8_w8\na8.json for MoE layer. [repeated 2x across cluster]\n(RayWorkerWrapper pid=325, ip=192.168.195.154) INFO 05-09 02:50:24 [monitor.py:33] torch.compile takes 76.40 s in total\nINFO 05-09 02:50:27 [kv_cache_utils.py:639] GPU KV cache size: 3,331,712 tokens\nINFO 05-09 02:50:27 [kv_cache_utils.py:642] Maximum concurrency for 32,000 tokens per request: 104.12x\nINFO 05-09 02:50:27 [kv_cache_utils.py:639] GPU KV cache size: 3,315,328 tokens\nINFO 05-09 02:50:27 [kv_cache_utils.py:642] Maximum concurrency for 32,000 tokens per request: 103.60x\nINFO 05-09 02:50:27 [kv_cache_utils.py:639] GPU KV cache size: 3,315,328 tokens\nINFO 05-09 02:50:27 [kv_cache_utils.py:642] Maximum concurrency for 32,000 tokens per request: 103.60x\nINFO 05-09 02:50:27 [kv_cache_utils.py:639] GPU KV cache size: 3,331,712 tokens\nINFO 05-09 02:50:27 [kv_cache_utils.py:642] Maximum concurrency for 32,000 tokens per request: 104.12x\nINFO 05-09 02:50:27 [kv_cache_utils.py:639] GPU KV cache size: 3,323,392 tokens\nINFO 05-09 02:50:27 [kv_cache_utils.py:642] Maximum concurrency for 32,000 tokens per request: 103.86x\nINFO 05-09 02:50:27 [kv_cache_utils.py:639] GPU KV cache size: 3,307,008 tokens\nINFO 05-09 02:50:27 [kv_cache_utils.py:642] Maximum concurrency for 32,000 tokens per request: 103.34x\nINFO 05-09 02:50:27 [kv_cache_utils.py:639] GPU KV cache size: 3,307,008 tokens\nINFO 05-09 02:50:27 [kv_cache_utils.py:642] Maximum concurrency for 32,000 tokens per request: 103.34x\nINFO 05-09 02:50:27 [kv_cache_utils.py:639] GPU KV cache size: 3,323,392 tokens\nINFO 05-09 02:50:27 [kv_cache_utils.py:642] Maximum concurrency for 32,000 tokens per request: 103.86x\nINFO 05-09 02:50:27 [kv_cache_utils.py:639] GPU KV cache size: 3,292,896 tokens\nINFO 05-09 02:50:27 [kv_cache_utils.py:642] Maximum concurrency for 32,000 tokens per request: 102.90x\nINFO 05-09 02:50:27 [kv_cache_utils.py:639] GPU KV cache size: 3,276,512 tokens\nINFO 05-09 02:50:27 [kv_cache_utils.py:642] Maximum concurrency for 32,000 tokens per request: 102.39x\nINFO 05-09 02:50:27 [kv_cache_utils.py:639] GPU KV cache size: 3,276,512 tokens\nINFO 05-09 02:50:27 [kv_cache_utils.py:642] Maximum concurrency for 32,000 tokens per request: 102.39x\nINFO 05-09 02:50:27 [kv_cache_utils.py:639] GPU KV cache size: 3,292,896 tokens\nINFO 05-09 02:50:27 [kv_cache_utils.py:642] Maximum concurrency for 32,000 tokens per request: 102.90x\n(RayWorkerWrapper pid=327, ip=192.168.195.152) INFO 05-09 02:52:41 [custom_all_reduce.py:195] Registering 2144 cuda graph a\nddresses\n(RayWorkerWrapper pid=327, ip=192.168.195.154) INFO 05-09 02:50:24 [monitor.py:33] torch.compile takes 66.72 s in total\n [repeated 2x across cluster]\n(RayWorkerWrapper pid=325, ip=192.168.195.154) INFO 05-09 02:53:15 [custom_all_reduce.py:195] Registering 2144 cuda graph a\nddresses\n(RayWorkerWrapper pid=324, ip=192.168.195.152) INFO 05-09 02:53:15 [custom_all_reduce.py:195] Registering 2144 cuda graph a\nddresses\n(RayWorkerWrapper pid=899) INFO 05-09 02:53:31 [custom_all_reduce.py:195] Registering 2144 cuda graph addresses\n(RayWorkerWrapper pid=326, ip=192.168.195.152) INFO 05-09 02:53:35 [custom_all_reduce.py:195] Registering 2144 cuda graph a\nddresses\n(RayWorkerWrapper pid=327, ip=192.168.195.154) INFO 05-09 02:53:53 [custom_all_reduce.py:195] Registering 2144 cuda graph a\nddresses\n(RayWorkerWrapper pid=897) INFO 05-09 02:53:54 [custom_all_reduce.py:195] Registering 2144 cuda graph addresses\n(RayWorkerWrapper pid=914) INFO 05-09 02:54:16 [custom_all_reduce.py:195] Registering 2144 cuda graph addresses\n(RayWorkerWrapper pid=326, ip=192.168.195.154) INFO 05-09 02:54:28 [custom_all_reduce.py:195] Registering 2144 cuda graph a\nddresses\n(RayWorkerWrapper pid=901) INFO 05-09 02:54:33 [gpu_model_runner.py:1697] Graph capturing finished in 201 secs, took 1.49 G\niB\n(RayWorkerWrapper pid=901) INFO 05-09 02:54:33 [custom_all_reduce.py:195] Registering 2144 cuda graph addresses\n(RayWorkerWrapper pid=324, ip=192.168.195.152) INFO 05-09 02:54:46 [gpu_model_runner.py:1697] Graph capturing finished in 2\n14 secs, took 1.49 GiB [repeated 4x across cluster]\n(RayWorkerWrapper pid=325, ip=192.168.195.152) INFO 05-09 02:54:46 [custom_all_reduce.py:195] Registering 2144 cuda graph a\nddresses\n(RayWorkerWrapper pid=326, ip=192.168.195.154) INFO 05-09 02:55:02 [gpu_model_runner.py:1697] Graph capturing finished in 2\n30 secs, took 1.49 GiB [repeated 4x across cluster]\n(RayWorkerWrapper pid=324, ip=192.168.195.154) INFO 05-09 02:55:02 [custom_all_reduce.py:195] Registering 2144 cuda graph a\nddresses\nINFO 05-09 02:55:02 [core.py:161] init engine (profile, create kv cache, warmup model) took 410.21 seconds\nINFO 05-09 02:55:09 [core.py:117] Batch queue is enabled with size 3\nINFO 05-09 02:55:09 [core_client.py:442] Core engine process 0 ready.\nINFO 05-09 02:55:09 [loggers.py:136] vllm cache_config_info with initialization after num_gpu_blocks is: 204782\nWARNING 05-09 02:55:09 [config.py:1276] Default sampling parameters have been overridden by the model's Hugging Face generation co\nnfig recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\nINFO 05-09 02:55:09 [serving_chat.py:116] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\nINFO 05-09 02:55:09 [serving_completion.py:61] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': \n0.9}\nINFO 05-09 02:55:09 [api_server.py:1089] Starting vLLM API server on http://0.0.0.0:8000\nINFO 05-09 02:55:09 [launcher.py:28] Available routes are:\nINFO 05-09 02:55:09 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET\nINFO 05-09 02:55:09 [launcher.py:36] Route: /docs, Methods: HEAD, GET\nINFO 05-09 02:55:09 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET\nINFO 05-09 02:55:09 [launcher.py:36] Route: /redoc, Methods: HEAD, GET\nINFO 05-09 02:55:09 [launcher.py:36] Route: /health, Methods: GET\nINFO 05-09 02:55:09 [launcher.py:36] Route: /load, Methods: GET\nINFO 05-09 02:55:09 [launcher.py:36] Route: /ping, Methods: GET, POST\nINFO 05-09 02:55:09 [launcher.py:36] Route: /tokenize, Methods: POST\nINFO 05-09 02:55:09 [launcher.py:36] Route: /detokenize, Methods: POST\nINFO 05-09 02:55:09 [launcher.py:36] Route: /v1/models, Methods: GET\nINFO 05-09 02:55:09 [launcher.py:36] Route: /version, Methods: GET\nINFO 05-09 02:55:09 [launcher.py:36] Route: /v1/chat/completions, Methods: POST\nINFO 05-09 02:55:09 [launcher.py:36] Route: /v1/completions, Methods: POST\nINFO 05-09 02:55:09 [launcher.py:36] Route: /v1/embeddings, Methods: POST\nINFO 05-09 02:55:09 [launcher.py:36] Route: /pooling, Methods: POST\nINFO 05-09 02:55:09 [launcher.py:36] Route: /score, Methods: POST\nINFO 05-09 02:55:09 [launcher.py:36] Route: /v1/score, Methods: POST\nINFO 05-09 02:55:09 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST\nINFO 05-09 02:55:09 [launcher.py:36] Route: /rerank, Methods: POST\nINFO 05-09 02:55:09 [launcher.py:36] Route: /v1/rerank, Methods: POST\nINFO 05-09 02:55:09 [launcher.py:36] Route: /v2/rerank, Methods: POST\nINFO 05-09 02:55:09 [launcher.py:36] Route: /invocations, Methods: POST\nINFO 05-09 02:55:09 [launcher.py:36] Route: /metrics, Methods: GET\nINFO:     Started server process [625]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO 05-09 03:01:59 [logger.py:39] Received request cmpl-fb9b3c7baac94435a14050383b0c5729-0: prompt: \"Your role as a coding assist\nant is to approach each problem with a rigorous, structured reasoning process that leads to accurate, maintainable, and efficient \ncode. Before writing the final implementation, engage in deep exploration by analyzing requirements, understanding edge cases, eva\nluating possible approaches, debugging step-by-step if needed, and ensuring your solution aligns with best practices. Structure yo\nur response into two main sections: Thought and Solution. In the Thought section, document your reasoning using this format: <|beg\nin_of_thought|> {step-by-step analysis and decision-making with each step separated by '\\n\\n'} <|end_of_thought|>. Your thought pr\nocess should include identifying the problem scope, analyzing inputs/outputs, exploring algorithms or design choices, preemptively\n considering failure cases, optimizing performance, and validating logic with examples or test cases. In the Solution section, wri\nte the final, refined code based on all reasoning, formatted as: <|begin_of_solution|> {final, clean, and correct code implementat\nion} <|end_of_solution|>. This structure ensures the code is well-reasoned, properly scoped, and production-ready. Now, try to sol\nve the following coding task using the above guidelines:\\n  Calculate the Ricci tensor for a two neutron star merger, assume that \none neutron star has twice the mass of the other.  Next, calculate the stress-energy tensor and derive an equation for the geodesi\nc motion.\", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_\np=0.9, top_k=-1, min_p=0.0, ppl_measurement=False, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output\n=False, ignore_eos=False, max_tokens=10000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_be\ntween_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [200000, 15468, \n5811, 486, 262, 22990, 43910, 373, 328, 5191, 1789, 2353, 517, 262, 68572, 24, 37811, 35538, 2100, 511, 16386, 328, 15245, 24, 161\n34, 571, 24, 341, 11592, 2963, 26, 22135, 10388, 290, 1926, 10889, 24, 31970, 310, 11358, 36808, 618, 35627, 11587, 24, 10276, 103\n60, 7140, 24, 36207, 4158, 17205, 24, 53596, 3690, 15382, 21643, 563, 6641, 24, 341, 26870, 913, 3498, 114395, 517, 2698, 19352, 2\n6, 26838, 913, 3495, 1629, 1472, 3081, 17649, 38, 78663, 341, 13855, 26, 608, 290, 78663, 5630, 24, 3286, 913, 35538, 1723, 544, 6\n253, 38, 440, 104, 5286, 10207, 16392, 5587, 159276, 367, 18816, 15382, 21643, 4372, 341, 9126, 40547, 517, 1789, 3690, 24791, 618\n, 481, 368, 20040, 440, 104, 431, 10207, 16392, 5587, 104, 22868, 7107, 7956, 2100, 2036, 4440, 31447, 290, 2353, 12908, 24, 35627\n, 16542, 84685, 95, 24, 38372, 15973, 537, 4174, 17601, 24, 853, 26755, 4848, 16531, 13623, 7140, 24, 74810, 5665, 24, 341, 128342\n, 13894, 517, 9759, 537, 1600, 7140, 26, 608, 290, 13855, 5630, 24, 5210, 290, 1926, 24, 68718, 2963, 3481, 436, 806, 35538, 24, 4\n4429, 486, 38, 440, 104, 5286, 10207, 134132, 159276, 367, 18660, 24, 7485, 24, 341, 4741, 2963, 10889, 105, 440, 104, 431, 10207,\n 134132, 104, 22868, 1394, 5154, 32235, 290, 2963, 373, 2515, 7004, 2979, 303, 24, 17513, 46967, 24, 341, 6790, 137220, 26, 8193, \n24, 2678, 328, 8104, 290, 2182, 22990, 5313, 1723, 290, 4195, 25595, 600, 220, 10379, 290, 108522, 21557, 393, 262, 1472, 39273, 1\n0792, 76044, 24, 12986, 511, 1085, 39273, 10792, 947, 18867, 290, 3708, 323, 290, 1453, 26, 220, 13528, 24, 5900, 290, 9005, 53857\n, 21557, 341, 30506, 315, 3894, 393, 290, 136473, 10228, 26], lora_request: None, prompt_adapter_request: None.\nINFO:     127.0.0.1:34026 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO 05-09 03:01:59 [async_llm.py:255] Added request cmpl-fb9b3c7baac94435a14050383b0c5729-0.\nINFO 05-09 03:01:59 [ray_distributed_executor.py:561] VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE = auto\nINFO 05-09 03:01:59 [ray_distributed_executor.py:563] VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM = False\nINFO 05-09 03:01:59 [ray_distributed_executor.py:578] RAY_CGRAPH_get_timeout is set to 300\nERROR 05-09 03:01:59 [core.py:402] EngineCore encountered a fatal error.\nERROR 05-09 03:01:59 [core.py:402] Traceback (most recent call last):\nERROR 05-09 03:01:59 [core.py:402]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 393, in run_engin\ne_core\nERROR 05-09 03:01:59 [core.py:402]     engine_core.run_busy_loop()\nERROR 05-09 03:01:59 [core.py:402]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 417, in run_busy_\nloop\nERROR 05-09 03:01:59 [core.py:402]     self._process_engine_step()\nERROR 05-09 03:01:59 [core.py:402]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 442, in _process_\nengine_step\nERROR 05-09 03:01:59 [core.py:402]     outputs = self.step_fn()\nERROR 05-09 03:01:59 [core.py:402]               ^^^^^^^^^^^^^^\nERROR 05-09 03:01:59 [core.py:402]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 235, in step_with\n_batch_queue\nERROR 05-09 03:01:59 [core.py:402]     future = self.model_executor.execute_model(scheduler_output)\nERROR 05-09 03:01:59 [core.py:402]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-09 03:01:59 [core.py:402]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/ray_distributed_executor.py\", \nline 51, in execute_model\nERROR 05-09 03:01:59 [core.py:402]     self.forward_dag = self._compiled_ray_dag(enable_asyncio=False)\nERROR 05-09 03:01:59 [core.py:402]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-09 03:01:59 [core.py:402]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py\", lin\ne 629, in _compiled_ray_dag\nERROR 05-09 03:01:59 [core.py:402]     return forward_dag.experimental_compile(\nERROR 05-09 03:01:59 [core.py:402]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-09 03:01:59 [core.py:402]   File \"/usr/local/lib/python3.12/dist-packages/ray/dag/dag_node.py\", line 333, in experimental\n_compile\nERROR 05-09 03:01:59 [core.py:402]     return build_compiled_dag_from_ray_dag(\nERROR 05-09 03:01:59 [core.py:402]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-09 03:01:59 [core.py:402]   File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 3315, in bu\nild_compiled_dag_from_ray_dag\nERROR 05-09 03:01:59 [core.py:402]     compiled_dag._get_or_compile()\nERROR 05-09 03:01:59 [core.py:402]   File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 1564, in _g\net_or_compile\nERROR 05-09 03:01:59 [core.py:402]     self._preprocess()\nERROR 05-09 03:01:59 [core.py:402]   File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 1300, in _p\nreprocess\nERROR 05-09 03:01:59 [core.py:402]     self._init_communicators()\nERROR 05-09 03:01:59 [core.py:402]   File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 1357, in _i\nnit_communicators\nERROR 05-09 03:01:59 [core.py:402]     p2p_communicator_id = _init_communicator(\nERROR 05-09 03:01:59 [core.py:402]                           ^^^^^^^^^^^^^^^^^^^\nERROR 05-09 03:01:59 [core.py:402]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/torch_tensor_nccl_chan\nnel.py\", line 762, in _init_communicator\nERROR 05-09 03:01:59 [core.py:402]     ray.get(actors[0].__ray_call__.remote(_do_get_unique_nccl_id))\nERROR 05-09 03:01:59 [core.py:402]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in au\nto_init_wrapper\nERROR 05-09 03:01:59 [core.py:402]     return fn(*args, **kwargs)\nERROR 05-09 03:01:59 [core.py:402]            ^^^^^^^^^^^^^^^^^^^\nERROR 05-09 03:01:59 [core.py:402]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in\n wrapper\nERROR 05-09 03:01:59 [core.py:402]     return func(*args, **kwargs)\nERROR 05-09 03:01:59 [core.py:402]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 05-09 03:01:59 [core.py:402]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2782, in get\nERROR 05-09 03:01:59 [core.py:402]     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\nERROR 05-09 03:01:59 [core.py:402]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-09 03:01:59 [core.py:402]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 929, in get_objec\nts\nERROR 05-09 03:01:59 [core.py:402]     raise value.as_instanceof_cause()\nERROR 05-09 03:01:59 [core.py:402] ray.exceptions.RayTaskError(ModuleNotFoundError): ray::RayWorkerWrapper.__ray_call__() \n(pid=324, ip=192.168.195.154, actor_id=72876ece9f2cadebbf85ef1301000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at \n0x152e9b5ba900>)\nERROR 05-09 03:01:59 [core.py:402]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-09 03:01:59 [core.py:402]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-09 03:01:59 [core.py:402]   File \"/usr/local/lib/python3.12/dist-packages/ray/actor.py\", line 1732, in __ray_call__\nERROR 05-09 03:01:59 [core.py:402]     return fn(self, *args, **kwargs)\nERROR 05-09 03:01:59 [core.py:402]            ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-09 03:01:59 [core.py:402]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/torch_tensor_nccl_chan\nnel.py\", line 683, in _do_get_unique_nccl_id\nERROR 05-09 03:01:59 [core.py:402]     from cupy.cuda import nccl\nERROR 05-09 03:01:59 [core.py:402] ModuleNotFoundError: No module named 'cupy'\nINFO 05-09 03:01:59 [ray_distributed_executor.py:127] Shutting down Ray distributed executor. If you see error log from logging.cc\n regarding SIGTERM received, please ignore because this is the expected termination process in Ray.\nProcess EngineCore_0:\nERROR 05-09 03:01:59 [async_llm.py:402] AsyncLLM output_handler failed.\nERROR 05-09 03:01:59 [async_llm.py:402] Traceback (most recent call last):\nERROR 05-09 03:01:59 [async_llm.py:402]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 360, in\n output_handler\nERROR 05-09 03:01:59 [async_llm.py:402]     outputs = await engine_core.get_output_async()\nERROR 05-09 03:01:59 [async_llm.py:402]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-09 03:01:59 [async_llm.py:402]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 723, \nin get_output_async\nERROR 05-09 03:01:59 [async_llm.py:402]     raise self._format_exception(outputs) from None\nERROR 05-09 03:01:59 [async_llm.py:402] vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trac\ne (above) for the root cause.\nINFO 05-09 03:01:59 [async_llm.py:327] Request cmpl-fb9b3c7baac94435a14050383b0c5729-0 failed (engine dead).\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 404, in run_engine_core\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 393, in run_engine_core\n    engine_core.run_busy_loop()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 417, in run_busy_loop\n    self._process_engine_step()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 442, in _process_engine_step\n    outputs = self.step_fn()\n              ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 235, in step_with_batch_queue\n    future = self.model_executor.execute_model(scheduler_output)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/ray_distributed_executor.py\", line 51, in execute_model\n    self.forward_dag = self._compiled_ray_dag(enable_asyncio=False)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py\", line 629, in _compiled_ray_dag\n    return forward_dag.experimental_compile(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/ray/dag/dag_node.py\", line 333, in experimental_compile\n    return build_compiled_dag_from_ray_dag(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 3315, in build_compiled_dag_from_ray_dag\n    compiled_dag._get_or_compile()\n  File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 1564, in _get_or_compile\n    self._preprocess()\n  File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 1300, in _preprocess\n    self._init_communicators()\n  File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 1357, in _init_communicators\n    p2p_communicator_id = _init_communicator(\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 762, in _init_communi\ncator\n    ray.get(actors[0].__ray_call__.remote(_do_get_unique_nccl_id))\n  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2782, in get\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 929, in get_objects\n    raise value.as_instanceof_cause()\nray.exceptions.RayTaskError(ModuleNotFoundError): ray::RayWorkerWrapper.__ray_call__() (pid=324, ip=192.168.195.154, actor\n_id=72876ece9f2cadebbf85ef1301000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x152e9b5ba900>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/ray/actor.py\", line 1732, in __ray_call__\n    return fn(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/torch_tensor_nccl_channel.py\", line 683, in _do_get_uniqu\ne_nccl_id\n    from cupy.cuda import nccl\nModuleNotFoundError: No module named 'cupy'\n(RayWorkerWrapper pid=327, ip=192.168.195.154) INFO 05-09 02:55:02 [gpu_model_runner.py:1697] Graph capturing finished in 2\n30 secs, took 1.49 GiB [repeated 3x across cluster]\nINFO:     Shutting down\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\nINFO:     Finished server process [625]\n```\n\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-05-09T03:05:36+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17875/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17875"
  },
  {
    "number": 59,
    "title": "Profile memory usage",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-05-03T06:00:44+00:00",
    "closed_at": "2023-05-19T17:35:45+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/59/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/59"
  },
  {
    "number": 18201,
    "title": "[New Model]: OpenGVLab/VideoChat-Flash-Qwen2_5-7B_InternVideo2-1B",
    "body": "### The model to consider.\n\nHey, I want to make a request for adding a new model [OpenGVLab/VideoChat-Flash-Qwen2_5-7B_InternVideo2-1B](https://huggingface.co/OpenGVLab/VideoChat-Flash-Qwen2_5-7B_InternVideo2-1B). Thank you!\n\n### The closest model vllm already supports.\n\nThe closest model vllm already supports would be https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/llava_onevision.py. \n\n### What's your difficulty of supporting the model you want?\n\nNew processor. 1B InternVL vision processor. \nAlso 16 tokens for a frame that's something I really want to test using vllm. \n\nfrom official Huggingface model card: \"_VideoChat-Flash-Qwen2_5-7B_InternVideo2-1B is constructed upon InternVideo2-1B and Qwen2.5-7B, employing only **16 tokens per frame**. By leveraging Yarn to extend the context window to 128k (Qwen2's native context window is 32k), our model supports input sequences of up to approximately **10,000 frames.**_\"\n\nThank you!\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [],
    "state": "open",
    "created_at": "2025-05-15T12:27:21+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18201/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/18201"
  },
  {
    "number": 21101,
    "title": "[Bug]: Enable cutom_op of rotary_embedding goes error for Qwen3-4B",
    "body": "### Your current environment\n\ntorch   version  2.7.1\nvllm version 0.9.2rc2.dev304+g28a6d5423.cu124 compile myself depends on newset mast.\n\n### \ud83d\udc1b Describe the bug\n\nI modify vllm/config.py to enable rotary_embedding op to enable custom_op.\nmodify:\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -4624,6 +4624,7 @@ class VllmConfig:\n             not self.model_config.enforce_eager:\n             # By default, V1 uses piecewise CUDA graphs. If full_cuda_graph\n             # is set to True, full CUDA graphs will be used.\n+            self.compilation_config.custom_ops=[\"none\",\"+rotary_embedding\u201d]\n\nrun for Qwen3-4B for v1 mode torch.compile. return error as follows:\nERROR 07-17 15:36:30 [core.py:592]   File \"/root/.virtualenvs/torch-env/lib/python3.10/site-packages/torch/fx/graph.py\", line 1172, in erase_node\nERROR 07-17 15:36:30 [core.py:592]     raise RuntimeError(\nERROR 07-17 15:36:30 [core.py:592] torch._inductor.exc.InductorError: RuntimeError: Tried to erase Node getitem_4 but it still had 1 users in the graph: {view_4: None}!\nERROR 07-17 15:36:30 [core.py:592] \nERROR 07-17 15:36:30 [core.py:592] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\nERROR 07-17 15:36:30 [core.py:592] \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-07-17T07:45:23+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/21101/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/21101"
  }
]
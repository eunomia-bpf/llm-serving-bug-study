[
  {
    "number": 4825,
    "title": "[Usage]: How to use tensor-parallel-size argument when deploy Llama3-8b with AsyncLLMEngine",
    "body": "### Your current environment\r\n\r\n```text\r\n My model is Llama3-8B which takes about 14GB GPU-memory.\r\nAnd the machine have 2 * 40GB GPUs. \uff08NVIDIA L40S\uff09\r\n```\r\n\r\n\r\n### How would you like to use vllm\r\n\r\n\r\nHey, \r\nRecently I tried to use AsyncLLMEngine to speed up my LLM inference server. My model is Llama3-8B which takes about 14GB GPU-memory. And the machine have 2 * 40GB GPUs\uff08NVIDIA L40S\uff09. I want to know that:\r\n1. What is the meaning of the \"tensor-parallel-size\" when init the AsyncLLMEngine? if I set it as 2, how is the parallelism been executed when a inference request comes,  It parallel the input tensors to the 2 different GPUs?  or it paralle-distribute the model's weight?\r\n 2. When I test the time-comsuming of the server with \"tensor-parallel-size\" = 1or2, I didn't see an obvious difference on them, so I was wondering maybe I didn't use AsyncLLMEngine  and  \"tensor-parallel-size\" in a correct-way.\r\n\r\n\r\nThanks!\r\n",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-05-15T07:08:55+00:00",
    "closed_at": "2024-06-27T17:19:53+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4825/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4825"
  },
  {
    "number": 9356,
    "title": "[Bug]: llama3.2-11B-Vision-Instruct not working",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.22.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-113-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.5.82\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-SXM2-32GB\r\nGPU 1: Tesla V100-SXM2-32GB\r\nGPU 2: Tesla V100-SXM2-32GB\r\nGPU 3: Tesla V100-SXM2-32GB\r\nGPU 4: Tesla V100-SXM2-32GB\r\nGPU 5: Tesla V100-SXM2-32GB\r\nGPU 6: Tesla V100-SXM2-32GB\r\nGPU 7: Tesla V100-SXM2-32GB\r\n\r\nNvidia driver version: 555.42.06\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             96\r\nOn-line CPU(s) list:                0-95\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Gold 6271C CPU @ 2.60GHz\r\nCPU family:                         6\r\nModel:                              85\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 24\r\nSocket(s):                          2\r\nStepping:                           7\r\nBogoMIPS:                           5200.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp_epp pku ospke avx512_vnni md_clear flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          1.5 MiB (48 instances)\r\nL1i cache:                          1.5 MiB (48 instances)\r\nL2 cache:                           48 MiB (48 instances)\r\nL3 cache:                           66 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-23,48-71\r\nNUMA node1 CPU(s):                  24-47,72-95\r\nVulnerability Gather data sampling: Mitigation; Microcode\r\nVulnerability Itlb multihit:        KVM: Mitigation: VMX disabled\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:             Mitigation; Enhanced IBRS\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI Syscall hardening, KVM SW loop\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Mitigation; TSX disabled\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.77\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.2\r\n[pip3] triton==3.0.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.6.77                  pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.45.2                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.4.dev3+gf0fe4fe8\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV1     NV2     NV1     SYS     SYS     SYS     NV2     NODE    NODE    SYS     SYS     0-23,48-71      0               N/A\r\nGPU1    NV1      X      NV1     NV2     SYS     SYS     NV2     SYS     NODE    NODE    SYS     SYS     0-23,48-71      0               N/A\r\nGPU2    NV2     NV1      X      NV2     SYS     NV1     SYS     SYS     PIX     PIX     SYS     SYS     0-23,48-71      0               N/A\r\nGPU3    NV1     NV2     NV2      X      NV1     SYS     SYS     SYS     PIX     PIX     SYS     SYS     0-23,48-71      0               N/A\r\nGPU4    SYS     SYS     SYS     NV1      X      NV2     NV2     NV1     SYS     SYS     PIX     PIX     24-47,72-95     1               N/A\r\nGPU5    SYS     SYS     NV1     SYS     NV2      X      NV1     NV2     SYS     SYS     PIX     PIX     24-47,72-95     1               N/A\r\nGPU6    SYS     NV2     SYS     SYS     NV2     NV1      X      NV1     SYS     SYS     NODE    NODE    24-47,72-95     1               N/A\r\nGPU7    NV2     SYS     SYS     SYS     NV1     NV2     NV1      X      SYS     SYS     NODE    NODE    24-47,72-95     1               N/A\r\nNIC0    NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS      X      PIX     SYS     SYS\r\nNIC1    NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX      X      SYS     SYS\r\nNIC2    SYS     SYS     SYS     SYS     PIX     PIX     NODE    NODE    SYS     SYS      X      PIX\r\nNIC3    SYS     SYS     SYS     SYS     PIX     PIX     NODE    NODE    SYS     SYS     PIX      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\nINFO 10-15 02:46:25 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/e45c9a02-fb52-4618-b461-49904a0cac09 for IPC Path.\r\nINFO 10-15 02:46:25 api_server.py:179] Started engine process with PID 1016869\r\nWARNING 10-15 02:46:25 config.py:1674] Casting torch.bfloat16 to torch.float16.\r\n/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\r\nNo module named 'vllm._version'\r\n  from vllm.version import __version__ as VLLM_VERSION\r\nWARNING 10-15 02:46:29 config.py:1674] Casting torch.bfloat16 to torch.float16.\r\nINFO 10-15 02:46:30 config.py:887] Defaulting to use mp for distributed inference\r\nWARNING 10-15 02:46:30 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\nINFO 10-15 02:46:34 config.py:887] Defaulting to use mp for distributed inference\r\nWARNING 10-15 02:46:34 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\nINFO 10-15 02:46:34 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/model/models/Llama-3.2-11B-Vision-Instruct', speculative_config=None, tokenizer='/model/models/Llama-3.2-11B-Vision-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=18, served_model_name=Llama-3.2-11B-Vision-Instruct, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\r\nWARNING 10-15 02:46:34 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\nINFO 10-15 02:46:34 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\nINFO 10-15 02:46:35 enc_dec_model_runner.py:141] EncoderDecoderModelRunner requires XFormers backend; overriding backend auto-selection and forcing XFormers.\r\nINFO 10-15 02:46:35 selector.py:115] Using XFormers backend.\r\n/model/anaconda3/envs/llm/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n/model/anaconda3/envs/llm/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\r\nNo module named 'vllm._version'\r\n  from vllm.version import __version__ as VLLM_VERSION\r\n(VllmWorkerProcess pid=1017165) INFO 10-15 02:46:38 enc_dec_model_runner.py:141] EncoderDecoderModelRunner requires XFormers backend; overriding backend auto-selection and forcing XFormers.\r\n(VllmWorkerProcess pid=1017165) INFO 10-15 02:46:38 selector.py:115] Using XFormers backend.\r\n(VllmWorkerProcess pid=1017165) /model/anaconda3/envs/llm/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n(VllmWorkerProcess pid=1017165)   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n(VllmWorkerProcess pid=1017165) /model/anaconda3/envs/llm/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n(VllmWorkerProcess pid=1017165)   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n(VllmWorkerProcess pid=1017165) INFO 10-15 02:46:39 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\r\nINFO 10-15 02:46:39 utils.py:1008] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=1017165) INFO 10-15 02:46:39 utils.py:1008] Found nccl from library libnccl.so.2\r\nINFO 10-15 02:46:39 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(VllmWorkerProcess pid=1017165) INFO 10-15 02:46:39 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(VllmWorkerProcess pid=1017165) INFO 10-15 02:46:39 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_4,5.json\r\nINFO 10-15 02:46:39 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_4,5.json\r\nINFO 10-15 02:46:39 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f54e3b09110>, local_subscribe_port=45621, remote_subscribe_port=None)\r\nINFO 10-15 02:46:39 model_runner.py:1060] Starting to load model /model/models/Llama-3.2-11B-Vision-Instruct...\r\n(VllmWorkerProcess pid=1017165) INFO 10-15 02:46:39 model_runner.py:1060] Starting to load model /model/models/Llama-3.2-11B-Vision-Instruct...\r\nINFO 10-15 02:46:39 selector.py:115] Using XFormers backend.\r\n(VllmWorkerProcess pid=1017165) INFO 10-15 02:46:39 selector.py:115] Using XFormers backend.\r\nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:02<00:11,  2.97s/it]\r\nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:06<00:09,  3.06s/it]\r\nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:07<00:04,  2.18s/it]\r\nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:10<00:02,  2.46s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:13<00:00,  2.66s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:13<00:00,  2.63s/it]\r\n\r\nINFO 10-15 02:46:53 model_runner.py:1071] Loading model weights took 10.0714 GB\r\n(VllmWorkerProcess pid=1017165) INFO 10-15 02:46:53 model_runner.py:1071] Loading model weights took 10.0714 GB\r\nINFO 10-15 02:46:53 enc_dec_model_runner.py:301] Starting profile run for multi-modal models.\r\n(VllmWorkerProcess pid=1017165) INFO 10-15 02:46:53 enc_dec_model_runner.py:301] Starting profile run for multi-modal models.\r\nProcess SpawnProcess-1:\r\nERROR 10-15 02:47:20 multiproc_worker_utils.py:117] Worker VllmWorkerProcess pid 1017165 died, exit code: -15\r\nINFO 10-15 02:47:20 multiproc_worker_utils.py:121] Killing local vLLM worker processes\r\nTraceback (most recent call last):\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/engine/multiprocessing/engine.py\", line 392, in run_mp_engine\r\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/engine/multiprocessing/engine.py\", line 141, in from_engine_args\r\n    return cls(\r\n           ^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/engine/multiprocessing/engine.py\", line 78, in __init__\r\n    self.engine = LLMEngine(*args,\r\n                  ^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 349, in __init__\r\n    self._initialize_kv_caches()\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 484, in _initialize_kv_caches\r\n    self.model_executor.determine_num_available_blocks())\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/executor/distributed_gpu_executor.py\", line 39, in determine_num_available_blocks\r\n    num_blocks = self._run_workers(\"determine_num_available_blocks\", )\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 192, in _run_workers\r\n    driver_worker_output = driver_worker_method(*args, **kwargs)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/worker/worker.py\", line 223, in determine_num_available_blocks\r\n    self.model_runner.profile_run()\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/worker/enc_dec_model_runner.py\", line 359, in profile_run\r\n    self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/worker/enc_dec_model_runner.py\", line 203, in execute_model\r\n    hidden_or_intermediate_states = model_executable(\r\n                                    ^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/model_executor/models/mllama.py\", line 1084, in forward\r\n    cross_attention_states = self.vision_model(pixel_values,\r\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/model_executor/models/mllama.py\", line 508, in forward\r\n    patch_embeds = self.patch_embedding(\r\n                   ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/model_executor/models/mllama.py\", line 227, in forward\r\n    x = self._unfold(x)\r\n        ^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/fold.py\", line 298, in forward\r\n    return F.unfold(input, self.kernel_size, self.dilation,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/functional.py\", line 4853, in unfold\r\n    return torch._C._nn.im2col(input, _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.59 GiB. GPU 0 has a total capacity of 31.73 GiB of which 3.11 GiB is free. Including non-PyTorch memory, this process has 28.62 GiB memory in use. Of the allocated memory 28.12 GiB is allocated by PyTorch, and 80.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\n[rank0]:[W1015 02:47:22.322553616 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\r\nTraceback (most recent call last):\r\n  File \"/model/anaconda3/envs/llm/bin/vllm\", line 8, in <module>\r\n    sys.exit(main())\r\n             ^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/scripts.py\", line 195, in main\r\n    args.dispatch_function(args)\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/scripts.py\", line 41, in serve\r\n    uvloop.run(run_server(args))\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/uvloop/__init__.py\", line 105, in run\r\n    return runner.run(wrapper())\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/asyncio/runners.py\", line 118, in run\r\n    return self._loop.run_until_complete(task)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/uvloop/__init__.py\", line 61, in wrapper\r\n    return await main\r\n           ^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 552, in run_server\r\n    async with build_async_engine_client(args) as engine_client:\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/contextlib.py\", line 210, in __aenter__\r\n    return await anext(self.gen)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 107, in build_async_engine_client\r\n    async with build_async_engine_client_from_engine_args(\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/contextlib.py\", line 210, in __aenter__\r\n    return await anext(self.gen)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/model/anaconda3/envs/llm/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 194, in build_async_engine_client_from_engine_args\r\n    raise RuntimeError(\r\nRuntimeError: Engine process failed to start\r\n(llm) root@ubuntu:/model# /model/anaconda3/envs/llm/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n/model/anaconda3/envs/llm/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\n\n### \ud83d\udc1b Describe the bug\n\nCUDA_VISIBLE_DEVICES=4,5 vllm serve /model/models/Llama-3.2-11B-Vision-Instruct --port xxxxxxx --api-key sk-xxxxxxxxxxxxxxxxxxxxxxxx  -tp 2 --served-model-name Llama-3.2-11B-Vision-Instruct --dtype float16 --enforce-eager  --limit-mm-per-prompt image=2,video=1 --seed 18 --enable-auto-tool-choice  --tool-call-parser llama3_json --chat-template /model/vllm/examples/tool_chat_template_llama3.2_json.jinja  --max-model-len 32768\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-10-15T02:51:07+00:00",
    "closed_at": "2024-10-15T04:25:06+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9356/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9356"
  },
  {
    "number": 4632,
    "title": "[Performance] [Speculative decoding]: Support draft model on different tensor-parallel size than target model",
    "body": "## Overview\r\nSpeculative decoding allows a speedup for memory-bound LLMs by using a fast proposal method to propose tokens that are verified in a single forward pass by the larger LLM. Papers report 2-3x speedup for bs=1, in Anyscale's fork we see up to 2x speedup with a small draft model for bs=8 (30% for bs=16) (we can improve this! see https://github.com/vllm-project/vllm/issues/4630 if you want to help).\r\n\r\nA key optimization for small models (68m/160m domain) is to use tensor-parallel degree 1, even if the target model is using tensor-parallel degree 4 or 8. In our fork, this reduces proposal time from 5ms/tok to 1.5ms/tok. This will allow a well-aligned 68m draft model to get 2x per-user throughput improvement on 70B target model.\r\n\r\nFurthermore, a 1B/7B proposer model may ideally be placed on TP=2 or TP=4, while the larger model is placed on TP=8. vLLM should support these configuration so the community can use the configuration best for their draft model.\r\n\r\n## Design suggestions\r\nI implemented a Worker which patches the tensor parallel group to TP1 in our fork. The [code is dumped here](https://gist.github.com/cadedaniel/f8479bf5fa5543b946d2133b5db38c56). We should use this approach in vLLM, however we can improve it by using @youkaichao 's tensor-parallel group improvements.",
    "labels": [
      "help wanted",
      "performance",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2024-05-06T18:14:40+00:00",
    "closed_at": "2024-06-25T09:56:08+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4632/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4632"
  },
  {
    "number": 15142,
    "title": "missing latest tag in vllm-cpu image",
    "body": "### Your current environment\n\ncurrent CI/CD builds a new vllm-cpu image on new releases which is great. but \u2018latest\u2019 tag is missing which requires having to keep changing the image manually on every release. \n\nwe expect the CI/CD not only to create a versioned tag, but also to tag latest once a new release is out.\n\n### How would you like to use vllm\n\nI want to run inference\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2025-03-19T16:27:37+00:00",
    "closed_at": "2025-03-20T08:19:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15142/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15142"
  },
  {
    "number": 7474,
    "title": "[Bug]: Model serving failed with these arguments --tensor-parallel-size 2 --pipeline-parallel-size 2",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.2\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) [GCC 12.3.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1066-aws-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA L4\r\nGPU 1: NVIDIA L4\r\nGPU 2: NVIDIA L4\r\nGPU 3: NVIDIA L4\r\n\r\nNvidia driver version: 535.183.01\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.2\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.2\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.2\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.2\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.2\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.2\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.2\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nByte Order:                           Little Endian\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nCPU(s):                               48\r\nOn-line CPU(s) list:                  0-47\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   24\r\nSocket(s):                            1\r\nNUMA node(s):                         1\r\nVendor ID:                            AuthenticAMD\r\nCPU family:                           25\r\nModel:                                1\r\nModel name:                           AMD EPYC 7R13 Processor\r\nStepping:                             1\r\nCPU MHz:                              2322.855\r\nBogoMIPS:                             5299.99\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            768 KiB\r\nL1i cache:                            768 KiB\r\nL2 cache:                             12 MiB\r\nL3 cache:                             96 MiB\r\nNUMA node0 CPU(s):                    0-47\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Mitigation; safe RET, no microcode\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] pyzmq==26.1.0\r\n[pip3] torch==2.4.0\r\n[pip3] torch-model-archiver==0.11.0\r\n[pip3] torchaudio==2.3.0+cu121\r\n[pip3] torchserve==0.11.0\r\n[pip3] torchtext==0.18.0+cu121\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.0\r\n[pip3] triton==3.0.0\r\n[conda] mkl                       2024.0.0         ha957f24_49657    conda-forge\r\n[conda] mkl-include               2024.1.0           ha957f24_693    conda-forge\r\n[conda] numpy                     1.26.4          py311h64a7726_0    conda-forge\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] pyzmq                     26.1.0                   pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torch-model-archiver      0.11.0                   pypi_0    pypi\r\n[conda] torchaudio                2.3.0+cu121              pypi_0    pypi\r\n[conda] torchserve                0.11.0                   pypi_0    pypi\r\n[conda] torchtext                 0.18.0+cu121             pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.44.0                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.4@4db5176d9758b720b05460c50ace3c01026eb158\r\nvLLM Build Flags:\r\nCUDA Archs: 5.0 7.0+PTX 7.5+PTX 8.0 8.6 9.0; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tSYS\tSYS\tSYS\t0-47\t0\t\tN/A\r\nGPU1\tSYS\t X \tSYS\tSYS\t0-47\t0\t\tN/A\r\nGPU2\tSYS\tSYS\t X \tSYS\t0-47\t0\t\tN/A\r\nGPU3\tSYS\tSYS\tSYS\t X \t0-47\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nTrying to serve mixtral 7x8 awq model with 4 gpu node and wanted to publish two instances of that same model. in that node. My understanding from the documnets was we can use --tensor-parallel-size 2 --pipeline-parallel-size 2 to distribute these. My expectation was from this was that it will have two instances of this model using two gpu each but its throwing me this error.\r\n\r\n(VllmWorkerProcess pid=138) INFO 08-13 15:22:51 model_runner.py:732] Loading model weights took 11.4953 GB\r\n(VllmWorkerProcess pid=137) INFO 08-13 15:22:51 model_runner.py:732] Loading model weights took 11.4953 GB\r\n(VllmWorkerProcess pid=139) INFO 08-13 15:22:52 model_runner.py:732] Loading model weights took 11.4953 GB\r\nINFO 08-13 15:22:52 model_runner.py:732] Loading model weights took 11.4953 GB\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method determine_num_available_blocks: 'MixtralForCausalLM' object has no attribute 'make_empty_intermediate_tensors', Traceback (most recent call last):\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/vllm/worker/worker.py\", line 179, in determine_num_available_blocks\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]     self.model_runner.profile_run()\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 936, in profile_run\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]     intermediate_tensors = self.model.make_empty_intermediate_tensors(\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1729, in __getattr__\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]     raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226] AttributeError: 'MixtralForCausalLM' object has no attribute 'make_empty_intermediate_tensors'\r\n(VllmWorkerProcess pid=139) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226] \r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method determine_num_available_blocks: 'MixtralForCausalLM' object has no attribute 'make_empty_intermediate_tensors', Traceback (most recent call last):\r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/vllm/worker/worker.py\", line 179, in determine_num_available_blocks\r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]     self.model_runner.profile_run()\r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 936, in profile_run\r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]     intermediate_tensors = self.model.make_empty_intermediate_tensors(\r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1729, in __getattr__\r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226]     raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226] AttributeError: 'MixtralForCausalLM' object has no attribute 'make_empty_intermediate_tensors'\r\n(VllmWorkerProcess pid=138) ERROR 08-13 15:22:52 multiproc_worker_utils.py:226] \r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method determine_num_available_blocks: list index out of range, Traceback (most recent call last):\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/vllm/worker/worker.py\", line 179, in determine_num_available_blocks\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]     self.model_runner.profile_run()\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 940, in profile_run\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]     self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1363, in execute_model\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]     hidden_or_intermediate_states = model_executable(\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]                                     ^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/vllm/model_executor/models/mixtral_quant.py\", line 361, in forward\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]     hidden_states = self.model(input_ids, positions, kv_caches,\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]   File \"/opt/conda/lib/python3.11/site-packages/vllm/model_executor/models/mixtral_quant.py\", line 328, in forward\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]     kv_caches[i], attn_metadata,\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226]     ~~~~~~~~~^^^\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226] IndexError: list index out of range\r\n(VllmWorkerProcess pid=137) ERROR 08-13 15:23:05 multiproc_worker_utils.py:226] \r\n/opt/conda/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\nERROR 08-13 15:23:05 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 138 died, exit code: -15\r\nINFO 08-13 15:23:05 multiproc_worker_utils.py:123] Killing local vLLM worker processes\r\nProcess Process-1:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/opt/conda/lib/python3.11/site-packages/vllm/entrypoints/openai/rpc/server.py\", line 217, in run_rpc_server\r\n    server = AsyncEngineRPCServer(async_engine_args, usage_context, port)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/vllm/entrypoints/openai/rpc/server.py\", line 25, in __init__\r\n    self.engine = AsyncLLMEngine.from_engine_args(async_engine_args,\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 471, in from_engine_args\r\n    engine = cls(\r\n             ^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 381, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 552, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 263, in __init__\r\n    self._initialize_kv_caches()\r\n  File \"/opt/conda/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 362, in _initialize_kv_caches\r\n    self.model_executor.determine_num_available_blocks())\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/vllm/executor/distributed_gpu_executor.py\", line 38, in determine_num_available_blocks\r\n    num_blocks = self._run_workers(\"determine_num_available_blocks\", )\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 192, in _run_workers\r\n    driver_worker_output = driver_worker_method(*args, **kwargs)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/vllm/worker/worker.py\", line 179, in determine_num_available_blocks\r\n    self.model_runner.profile_run()\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 940, in profile_run\r\n    self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1363, in execute_model\r\n    hidden_or_intermediate_states = model_executable(\r\n                                    ^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/vllm/model_executor/models/mixtral_quant.py\", line 361, in forward\r\n    hidden_states = self.model(input_ids, positions, kv_caches,\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/vllm/model_executor/models/mixtral_quant.py\", line 328, in forward\r\n    kv_caches[i], attn_metadata,\r\n    ~~~~~~~~~^^^\r\nIndexError: list index out of range\r\n/opt/conda/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-13T15:47:01+00:00",
    "closed_at": "2024-12-12T02:06:42+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7474/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7474"
  },
  {
    "number": 5704,
    "title": "[Usage]: NVIDIA\u591a\u578b\u53f7\u7684GPU\u5982\u4f55\u5229\u7528\u5230\uff1f",
    "body": "### Your current environment\n\n\r\n![gpu](https://github.com/vllm-project/vllm/assets/47100639/772cf312-d447-472c-918f-b3e495c04a9b)\r\nvllm\u672c\u5730\u90e8\u7f72\u6a21\u578b\u52a0\u8f7d\u7684\u65f6\u5019\u53ea\u7528\u5230\u4e86RTX3060\u663e\u5b58\uff0c\u8dd19B\u7684\u6a21\u578b\u65e0\u6cd5\u8dd1\u8d77\u6765\uff0c\u6709\u6ca1\u6709\u529e\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u5728\u591a\u4e2a\u5361\u4e0a\u9762\u5b9e\u73b0\u5e76\u884c\uff1f\r\n\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\r\n",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-06-20T06:56:23+00:00",
    "closed_at": "2024-06-21T05:11:23+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5704/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5704"
  },
  {
    "number": 17775,
    "title": "[Bug]: qwen2.5vl internal server error when processing videos from split_video_ffmpeg after realease 0.8.3",
    "body": "### Your current environment\n\n<details>\nvllm 0.8.3 and vllm 0.7.3\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI tried to use scenedetect to split video into small slices e.g. :split_video_ffmpeg(video_path, scene_list, output_dir=output_dir, show_progress=True, show_output=True)\n\nAnd I found out vllm can deal with original video but not those slice of videos. \nThe log report says: vllm/multimodal/video.py\", line 174, in load_bytes | assert i == num_frames | | AssertionError\n\nI went to the source code and found out that code was added in realease 0.8.3\n\n![Image](https://github.com/user-attachments/assets/461b57f6-cdeb-4316-897e-70a6d5386d41)\n\nSo I install vllm==0.7.3 instead and it solves that problem, that is the code of 0.7.3\n\n![Image](https://github.com/user-attachments/assets/254a4b6c-82b7-43ea-84dc-709f524c1502)\n\nPlease check that case, thankyou very much!\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-05-07T09:03:18+00:00",
    "closed_at": "2025-05-07T15:36:08+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17775/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17775"
  },
  {
    "number": 13222,
    "title": "[Misc]:  Question about Grouped-query attention (GQA)",
    "body": "### Implementation of Grouped-query attention (GQA)\n\nHello:) I was wondering whether [Grouped-query attention](https://arxiv.org/pdf/2305.13245#:~:text=Multi%2Dquery%20attention%20shares%20single,head%20and%20multi%2Dquery%20attention) (GQA) is implemented in vLLM. I see that Llama3 models come with this feature in their [architecture](https://arxiv.org/pdf/2407.21783), and they are available through vLLM. Are they using GQA in the backend?\n\nThanks a lot and sorry for the inconveniences\n\n### Before submitting a new issue...\n\n- [ ] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2025-02-13T13:14:37+00:00",
    "closed_at": "2025-02-14T16:24:47+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13222/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13222"
  },
  {
    "number": 12619,
    "title": "[Feature]: Only apply Guided/Structured grammar after reasoning steps in Reasoning models",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nOnly apply Guided/Structured grammar only in the answer for reasoning model. i.e. for DeepSeek R1 only enforce grammar inside `<answer></answer>` or after `</think>`\nThis would make Reasoning models more useful in agent workflow expecting structured output.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "structured-output"
    ],
    "state": "closed",
    "created_at": "2025-01-31T16:49:13+00:00",
    "closed_at": "2025-03-05T05:01:40+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12619/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12619"
  },
  {
    "number": 1187,
    "title": "classification with BartForSequenceClassification?",
    "body": "another huge problem requiring speed is BartForSequenceClassification ala \"facebook/bart-large-mnli\"",
    "labels": [],
    "state": "closed",
    "created_at": "2023-09-27T00:53:52+00:00",
    "closed_at": "2024-03-13T11:14:11+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1187/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1187"
  },
  {
    "number": 8158,
    "title": "[Bug]: vllm async engine can not use adag",
    "body": "### Your current environment\n\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.2 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.2.0-37-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.128\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA RTX A6000\r\nGPU 1: NVIDIA RTX A6000\r\nGPU 2: NVIDIA RTX A6000\r\nGPU 3: NVIDIA RTX A6000\r\n\r\nNvidia driver version: 530.30.02\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             64\r\nOn-line CPU(s) list:                0-63\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD Ryzen Threadripper PRO 5975WX 32-Cores\r\nCPU family:                         25\r\nModel:                              8\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 32\r\nSocket(s):                          1\r\nStepping:                           2\r\nFrequency boost:                    enabled\r\nCPU max MHz:                        7006.6401\r\nCPU min MHz:                        1800.0000\r\nBogoMIPS:                           7187.11\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\nVirtualization:                     AMD-V\r\nL1d cache:                          1 MiB (32 instances)\r\nL1i cache:                          1 MiB (32 instances)\r\nL2 cache:                           16 MiB (32 instances)\r\nL3 cache:                           128 MiB (4 instances)\r\nNUMA node(s):                       1\r\nNUMA node0 CPU(s):                  0-63\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; safe RET, no microcode\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.20\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.1\r\n[pip3] triton==3.0.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.6.20                  pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.44.1                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.5@09c7792610ada9f88bbf87d32b472dd44bf23cc2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity\r\nGPU0     X      SYS     SYS     NV4     0-63            N/A\r\nGPU1    SYS      X      NV4     SYS     0-63            N/A\r\nGPU2    SYS     NV4      X      SYS     0-63            N/A\r\nGPU3    NV4     SYS     SYS      X      0-63            N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\n\n### \ud83d\udc1b Describe the bug\n\nI am trying to test PP and PP requires use LLM engine. But when I change the backen to ray + adag, it has problem like this and just stuck at here.\r\n\r\n\r\n```\r\n can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(RayWorkerWrapper pid=3721589) INFO 09-04 12:25:42 model_runner.py:1300] Graph capturing finished in 12 secs.\r\nStarting warm-up...\r\nINFO 09-04 12:25:43 async_llm_engine.py:208] Added request 09f9e54acf7a425fb155949722f1d5ca.\r\nINFO 09-04 12:25:43 ray_gpu_executor.py:444] VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL = True\r\nINFO 09-04 12:25:44 async_llm_engine.py:176] Finished request 09f9e54acf7a425fb155949722f1d5ca.\r\nWarm-up completed.\r\nINFO 09-04 12:25:44 async_llm_engine.py:208] Added request 09f9e54acf7a425fb155949722f1d5ca.\r\nERROR 09-04 12:25:44 async_llm_engine.py:65] Engine background task failed\r\nERROR 09-04 12:25:44 async_llm_engine.py:65] Traceback (most recent call last):\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]   File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 55, in _log_task_completion\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]     return_value = task.result()\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]                    ^^^^^^^^^^^^^\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]   File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 930, in run_engine_loop\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]     result = task.result()\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]              ^^^^^^^^^^^^^\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]   File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 873, in engine_step\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]     request_outputs = await self.engine.step_async(virtual_engine)\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]   File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 337, in step_async\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]     output = await self.model_executor.execute_model_async(\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]   File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 508, in execute_model_async\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]     outputs = await dag_future\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]               ^^^^^^^^^^^^^^^^\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]   File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/ray/experimental/compiled_dag_ref.py\", line 153, in __await__\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]     return _process_return_vals(\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]            ^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]   File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/ray/experimental/compiled_dag_ref.py\", line 22, in _process_return_vals\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]     raise val.as_instanceof_cause()\r\nERROR 09-04 12:25:44 async_llm_engine.py:65] ray.exceptions.RayTaskError(AssertionError): ray::RayWorkerWrapper.__ray_call__() (pid=3721589, ip=10.218.163.87)\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]   File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/executor/ray_utils.py\", line 77, in execute_model_spmd\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]     output = self.worker._execute_model_spmd(execute_model_req,\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]   File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/worker/worker.py\", line 387, in _execute_model_spmd\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]     new_seq_group_metadata_list = self._get_cached_seq_group_metadata(\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]   File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/worker/worker.py\", line 357, in _get_cached_seq_group_metadata\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]     assert isinstance(metadata_or_delta, SequenceGroupMetadata)\r\nERROR 09-04 12:25:44 async_llm_engine.py:65]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-04 12:25:44 async_llm_engine.py:65] AssertionError\r\nERROR:asyncio:Exception in callback _log_task_completion(error_callback=<bound method...7fbd85765250>>)(<Task finishe...rtionError())>) at /home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py:45\r\nhandle: <Handle _log_task_completion(error_callback=<bound method...7fbd85765250>>)(<Task finishe...rtionError())>) at /home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py:45>\r\nTraceback (most recent call last):\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 55, in _log_task_completion\r\n    return_value = task.result()\r\n                   ^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 930, in run_engine_loop\r\n    result = task.result()\r\n             ^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 873, in engine_step\r\n    request_outputs = await self.engine.step_async(virtual_engine)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 337, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 508, in execute_model_async\r\n    outputs = await dag_future\r\n              ^^^^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/ray/experimental/compiled_dag_ref.py\", line 153, in __await__\r\n    return _process_return_vals(\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/ray/experimental/compiled_dag_ref.py\", line 22, in _process_return_vals\r\n    raise val.as_instanceof_cause()\r\nray.exceptions.RayTaskError(AssertionError): ray::RayWorkerWrapper.__ray_call__() (pid=3721589, ip=10.218.163.87)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/executor/ray_utils.py\", line 77, in execute_model_spmd\r\n    output = self.worker._execute_model_spmd(execute_model_req,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/worker/worker.py\", line 387, in _execute_model_spmd\r\n    new_seq_group_metadata_list = self._get_cached_seq_group_metadata(\r\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/worker/worker.py\", line 357, in _get_cached_seq_group_metadata\r\n    assert isinstance(metadata_or_delta, SequenceGroupMetadata)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAssertionError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/asyncio/events.py\", line 84, in _run\r\n    self._context.run(self._callback, *self._args)\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 67, in _log_task_completion\r\n    raise AsyncEngineDeadError(\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for the actual cause.\r\nTraceback (most recent call last):\r\n  File \"/home/zhilong/vllm_pp.py\", line 49, in <module>\r\n    asyncio.run(main())\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/asyncio/runners.py\", line 190, in run\r\n    return runner.run(main)\r\n           ^^^^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/asyncio/runners.py\", line 118, in run\r\n    return self._loop.run_until_complete(task)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\r\n    return future.result()\r\n           ^^^^^^^^^^^^^^^\r\n  File \"/home/zhilong/vllm_pp.py\", line 46, in main\r\n    await benchmark_vllm_engine(engine, prompt, sampling_params)\r\n  File \"/home/zhilong/vllm_pp.py\", line 24, in benchmark_vllm_engine\r\n    async for _ in results_generator:\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 1064, in generate\r\n    async for output in await self.add_request(\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 113, in generator\r\n    raise result\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 55, in _log_task_completion\r\n    return_value = task.result()\r\n                   ^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 930, in run_engine_loop\r\n    result = task.result()\r\n             ^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 873, in engine_step\r\n    request_outputs = await self.engine.step_async(virtual_engine)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 337, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 508, in execute_model_async\r\n    outputs = await dag_future\r\n              ^^^^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/ray/experimental/compiled_dag_ref.py\", line 153, in __await__\r\n    return _process_return_vals(\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/ray/experimental/compiled_dag_ref.py\", line 22, in _process_return_vals\r\n    raise val.as_instanceof_cause()\r\nray.exceptions.RayTaskError(AssertionError): ray::RayWorkerWrapper.__ray_call__() (pid=3721589, ip=10.218.163.87)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/executor/ray_utils.py\", line 77, in execute_model_spmd\r\n    output = self.worker._execute_model_spmd(execute_model_req,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/worker/worker.py\", line 387, in _execute_model_spmd\r\n    new_seq_group_metadata_list = self._get_cached_seq_group_metadata(\r\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/zhilong/miniconda3/envs/vllm/lib/python3.11/site-packages/vllm/worker/worker.py\", line 357, in _get_cached_seq_group_metadata\r\n    assert isinstance(metadata_or_delta, SequenceGroupMetadata)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAssertionError\r\n\r\n```\r\nReproduction codes \r\n\r\n```\r\nexport DISTRIBUTED_EXECUTOR_BACKEND=ray\r\nexport VLLM_USE_RAY_COMPILED_DAG=1\r\nexport VLLM_USE_RAY_SPMD_WORKER=1\r\n```\r\n```\r\nimport asyncio\r\nimport time\r\n\r\nfrom vllm.engine.arg_utils import AsyncEngineArgs\r\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\r\nfrom vllm.sampling_params import SamplingParams\r\nfrom vllm.usage.usage_lib import UsageContext\r\nfrom vllm.utils import random_uuid\r\n\r\nasync def benchmark_vllm_engine(engine: AsyncLLMEngine, prompt: str, sampling_params: SamplingParams, warm_up: bool = True):\r\n    request_id = random_uuid()\r\n\r\n    # Warm-up phase\r\n    if warm_up:\r\n        print(\"Starting warm-up...\")\r\n        async for _ in engine.generate(prompt, sampling_params, request_id):\r\n            pass  # Complete the warm-up without any operation\r\n        print(\"Warm-up completed.\")\r\n\r\n    # Start the benchmark\r\n    start_time = time.time()\r\n    results_generator = engine.generate(prompt, sampling_params, request_id)\r\n    \r\n    async for _ in results_generator:\r\n        pass\r\n\r\n    end_time = time.time()\r\n    total_time = end_time - start_time\r\n    print(f\"Benchmark completed. Total time taken: {total_time:.4f} seconds\")\r\n\r\nasync def main():\r\n    # Set up engine arguments\r\n    args = AsyncEngineArgs(\r\n        model=\"/home/models/Llama-2-7b-hf\",\r\n        distributed_executor_backend =  \"ray\"\r\n    )\r\n\r\n    # Initialize the engine\r\n    engine = AsyncLLMEngine.from_engine_args(args)\r\n\r\n    # Define the prompt and sampling parameters\r\n    prompt = \"Your input text here\"\r\n    sampling_params = SamplingParams(temperature=1.0, max_tokens=50)\r\n\r\n    # Run the benchmark\r\n    await benchmark_vllm_engine(engine, prompt, sampling_params)\r\n\r\nif __name__ == \"__main__\":\r\n    asyncio.run(main())\r\n\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-09-04T16:30:14+00:00",
    "closed_at": "2024-09-24T20:13:10+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8158/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8158"
  },
  {
    "number": 9900,
    "title": "[Bug]: vllm 0.6.3.post1 does not work with `response_format`",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.12.7 (main, Oct  1 2024, 08:52:12) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.8.0-1017-aws-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A10G\r\nNvidia driver version: 550.120\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               48\r\nOn-line CPU(s) list:                  0-47\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD EPYC 7R32\r\nCPU family:                           23\r\nModel:                                49\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   24\r\nSocket(s):                            1\r\nStepping:                             0\r\nBogoMIPS:                             5599.99\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save rdpid\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            768 KiB (24 instances)\r\nL1i cache:                            768 KiB (24 instances)\r\nL2 cache:                             12 MiB (24 instances)\r\nL3 cache:                             96 MiB (6 instances)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-47\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec rstack overflow:   Vulnerable: Safe RET, no microcode\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.6+cu121torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.77\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.2\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.3.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-47    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nEnvironment with lower `vllm` version\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.12.6 (main, Sep 10 2024, 00:05:17) [GCC 9.4.0] (64-bit runtime)\r\nPython platform: Linux-6.8.0-1017-aws-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A10G\r\nNvidia driver version: 550.120\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nByte Order:                           Little Endian\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nCPU(s):                               48\r\nOn-line CPU(s) list:                  0-47\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   24\r\nSocket(s):                            1\r\nNUMA node(s):                         1\r\nVendor ID:                            AuthenticAMD\r\nCPU family:                           23\r\nModel:                                49\r\nModel name:                           AMD EPYC 7R32\r\nStepping:                             0\r\nCPU MHz:                              3300.602\r\nBogoMIPS:                             5599.99\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            768 KiB\r\nL1i cache:                            768 KiB\r\nL2 cache:                             12 MiB\r\nL3 cache:                             96 MiB\r\nNUMA node0 CPU(s):                    0-47\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec rstack overflow:   Vulnerable: Safe RET, no microcode\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save rdpid\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.6+cu121torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.68\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.0\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.1.dev238+ge2c6e0a82\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-47    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n  ```\r\n  \r\n  How to run the server with docker-compose\r\n  ```yaml\r\n  ...\r\n      command: >\r\n      --host 0.0.0.0\r\n      --port 8000\r\n      --api-key \"sk-no-key-required\"\r\n      --max-model-len 16384\r\n      --tensor-parallel-size 1\r\n      --gpu-memory-utilization 0.6\r\n      --served-model-name gpt-4o\r\n      --seed 42\r\n      --disable-log-requests\r\n      --enable-prefix-caching\r\n      --model /models/Llama-3.1-Storm-8B-AWQ\r\n  ```\r\n  \r\n  Python script\r\n  ```python\r\nimport openai\r\n\r\nBASE_URL=\"http://localhost:8000/v1\"\r\nAPI_KEY=\"sk-no-key-required\"\r\n\r\nopenai_client = openai.OpenAI(\r\n    base_url=BASE_URL,\r\n    api_key=API_KEY\r\n)\r\n\r\nfrom pydantic import BaseModel\r\n\r\n\r\nclass Players(BaseModel):\r\n    player_names: list[str]\r\n\r\n\r\ncompletion = openai_client.chat.completions.create(\r\n    model=\"gpt-4o\",\r\n    messages=[\r\n        {\"role\": \"user\", \"content\": \"Give me three baseball players' names and return the result as a JSON object with the key is `player_names`\"}\r\n    ],\r\n    temperature=0.0,\r\n    n=1,\r\n    seed=42,\r\n    max_tokens=512,\r\n    response_format={\r\n        \"type\": \"json_schema\",\r\n        \"json_schema\": {\r\n            \"name\": Players.__name__,\r\n            \"schema\": Players.model_json_schema()\r\n        }\r\n    }\r\n)\r\ncompletion.choices[0].message.content\r\n```\r\n\r\nOutput from `vllm:0.6.1.dev238+ge2c6e0a82`\r\n```\r\n'{\"player_names\": [\"Rickey Henderson\", \"Mike Schmidt\", \"Cal Ripken Jr.\"]}'\r\n```\r\n\r\nOutput from `vllm:0.6.3.post1`\r\n```\r\n'```json\\n{\\n  \"player_names\": [\\n    \"Mike Trout\",\\n    \"Mookie Betts\",\\n    \"Christian Yelich\"\\n  ]\\n}\\n```'\r\n```\r\n\r\n\r\nI observed that the server with latest version did not compile the FSM, while the lower one did.\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-11-01T02:18:13+00:00",
    "closed_at": "2024-11-05T22:48:53+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9900/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9900"
  },
  {
    "number": 10517,
    "title": "[Usage]: What's the relationship between KV cache and MAX_SEQUENCE_LENGTH.",
    "body": "### Your current environment\r\n\r\nGPU : H100 80G *2\r\nModel : Llama 3.1 70B\r\n\r\nModel Params:\r\n~~~\r\n      env:\r\n        - name: MODEL_NAME\r\n          value: /mnt/models/models--meta-llama--llama-3-1-70b-instruct\r\n        - name: DTYPE_STR\r\n          value: float16\r\n        - name: MAX_SEQUENCE_LENGTH\r\n          value: '20000'\r\n        - name: MAX_BATCH_SIZE\r\n          value: '4'\r\n        - name: MAX_NEW_TOKENS\r\n          value: '4096'\r\n        - name: MAX_LOG_LEN\r\n          value: '100'\r\n        - name: DEFAULT_INCLUDE_STOP_SEQS\r\n          value: 'false'\r\n        - name: NUM_GPUS\r\n          value: '2'\r\n        - name: CUDA_VISIBLE_DEVICES\r\n          value: '0,1'\r\n        - name: HUGGINGFACE_HUB_CACHE\r\n          value: /mnt/models/\r\n        - name: HF_MODULES_CACHE\r\n          value: /tmp/huggingface/modules\r\n        - name: PORT\r\n          value: '3000'\r\n~~~\r\n\r\nInitializing an LLM engine (v0.5.4) with config\r\n~~~\r\nmodel='/mnt/models/models--meta-llama--llama-3-1-70b-instruct', speculative_config=None, tokenizer='/mnt/models/models--meta-llama--llama-3-1-70b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/mnt/models/models--meta-llama--llama-3-1-70b-instruct, use_v2_block_manager=False, enable_prefix_caching=False)\r\n~~~\r\n\r\n\r\n\r\n### How would you like to use vllm\r\n\r\nHello, I'm trying to serve `Llama 3.1 70B` on 2*H100 80G. **I know This might cause an error because of lack of GPU memory.**  \r\n\r\nand actually I got this error message:  \r\n~~~\r\nINFO 11-20 08:34:06 distributed_gpu_executor.py:56] # GPU blocks: 1498, # CPU blocks: 1638\r\nProcess SpawnProcess-1:\r\nERROR 11-20 08:34:06 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 118 died, exit code: -15\r\nINFO 11-20 08:34:06 multiproc_worker_utils.py:123] Killing local vLLM worker processes\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib64/python3.11/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/opt/vllm/lib64/python3.11/site-packages/vllm/entrypoints/openai/rpc/server.py\", line 222, in run_rpc_server\r\n    server = AsyncEngineRPCServer(async_engine_args, usage_context, rpc_path)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/vllm/lib64/python3.11/site-packages/vllm/entrypoints/openai/rpc/server.py\", line 26, in __init__\r\n    self.engine = AsyncLLMEngine.from_engine_args(async_engine_args,\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/vllm/lib64/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 735, in from_engine_args\r\n    engine = cls(\r\n             ^^^^\r\n  File \"/opt/vllm/lib64/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 631, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/vllm/lib64/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 830, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/vllm/lib64/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 267, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/opt/vllm/lib64/python3.11/site-packages/vllm/engine/llm_engine.py\", line 282, in __init__\r\n    self._initialize_kv_caches()\r\n  File \"/opt/vllm/lib64/python3.11/site-packages/vllm/engine/llm_engine.py\", line 401, in _initialize_kv_caches\r\n    self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\r\n  File \"/opt/vllm/lib64/python3.11/site-packages/vllm/executor/distributed_gpu_executor.py\", line 62, in initialize_cache\r\n    self._run_workers(\"initialize_cache\",\r\n  File \"/opt/vllm/lib64/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 192, in _run_workers\r\n    driver_worker_output = driver_worker_method(*args, **kwargs)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/vllm/lib64/python3.11/site-packages/vllm/worker/worker.py\", line 226, in initialize_cache\r\n    raise_if_cache_size_invalid(num_gpu_blocks,\r\n  File \"/opt/vllm/lib64/python3.11/site-packages/vllm/worker/worker.py\", line 445, in raise_if_cache_size_invalid\r\n    raise ValueError(\r\nValueError: The model's max seq len (131072) is larger than the maximum number of tokens that can be stored in KV cache (23968). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.\r\n~~~\r\n\r\nError message said KV cache is not enough to store model's max seq length. So I decreased `MAX_SEQUENCE_LENGTH` 131072 to 24000 and got same error message :  \r\n~~~\r\nValueError: The model's max seq len (24000) is larger than the maximum number of tokens that can be stored in KV cache (6592). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.\r\n~~~\r\n\r\nand then I decreased `MAX_SEQUENCE_LENGTH`  again. 24000 -> 20000 \r\n~~~\r\nValueError: The model's max seq len (20000) is larger than the maximum number of tokens that can be stored in KV cache (10944). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.\r\n~~~\r\n\r\nSo I tested few cases: \r\nmax_seq_len|kv_cache||\r\n---|---|---\r\n131072|23968|# GPU blocks: 1498, # CPU blocks: 1638\r\n120000|23968|\r\n24000|6592 | # GPU blocks: 412, # CPU blocks: 1638\r\n20000|10944 |# GPU blocks: 684, # CPU blocks: 1638\r\n16000 |15328|\r\n\r\n### My question is:   \r\nWhy doesn\u2019t the kv_cache consistently increase or decrease as the `MAX_SEQUENCE_LENGTH` decreases?\r\n**What is the relationship between `MAX_SEQUENCE_LENGTH` and KV_Cache?**   \r\nI also dig into source code, and find some calculation logic (it might be wrong... Please correct me)\r\n\r\n![image](https://github.com/user-attachments/assets/8d2dae3f-f1ff-427b-b45d-1876b8273180)\r\n\r\nI can't understand how `MAX_SEQUENCE_LENGTH` effect KV_cache... \r\nPlease share the knowledge and give me an advise! \r\n\r\nThank you!!!\r\n\r\n\r\n\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-21T05:49:40+00:00",
    "closed_at": "2025-04-02T02:06:20+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10517/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10517"
  },
  {
    "number": 6721,
    "title": "[Feature]: GLM4 function call is supported ?",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nGLM4 function call is supported ?\n\n### Alternatives\n\nGLM4 function call is supported ?\n\n### Additional context\n\nGLM4 function call is supported ?",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-24T03:12:49+00:00",
    "closed_at": "2024-11-25T02:04:33+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6721/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6721"
  },
  {
    "number": 5828,
    "title": "[Bug]: unhandled system error with NCCL on v0.5.0.post1",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-1022-azure-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: Tesla T4\r\nNvidia driver version: 535.171.04\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             4\r\nOn-line CPU(s) list:                0-3\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 7V12 64-Core Processor\r\nCPU family:                         23\r\nModel:                              49\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 4\r\nSocket(s):                          1\r\nStepping:                           0\r\nBogoMIPS:                           4890.88\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core ssbd vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru arat umip rdpid\r\nHypervisor vendor:                  Microsoft\r\nVirtualization type:                full\r\nL1d cache:                          128 KiB (4 instances)\r\nL1i cache:                          128 KiB (4 instances)\r\nL2 cache:                           2 MiB (4 instances)\r\nL3 cache:                           16 MiB (1 instance)\r\nNUMA node(s):                       1\r\nNUMA node0 CPU(s):                  0-3\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Mitigation; untrained return thunk; SMT disabled\r\nVulnerability Spec rstack overflow: Vulnerable: Safe RET, no microcode\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] sentence-transformers==2.7.0\r\n[pip3] torch==2.3.0+cu121\r\n[pip3] transformers==4.40.1\r\n[pip3] triton==2.3.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.4.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] sentence-transformers     2.7.0                    pypi_0    pypi\r\n[conda] torch                     2.3.0+cu121              pypi_0    pypi\r\n[conda] transformers              4.40.1                   pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.4.0             pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      SYS     0-3     0               N/A\r\nNIC0    SYS      X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nvllm won't load with v0.5.0 or v0.5.0.post1, when downgrading to 0.4.2 it works fine. \r\n\r\nv0.5.0.post1 throws this error: RuntimeError: NCCL Error 2: unhandled system error. More details below:\r\n\r\nINFO 06-25 15:26:04 api_server.py:177] vLLM API server version 0.5.0.post1\r\nINFO 06-25 15:26:04 api_server.py:178] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='/ai/Meta-Llama-3-8B-Instruct', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='half', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=4096, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=True, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.96, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=True, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, image_processor=None, image_processor_revision=None, disable_image_processor=False, scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, num_speculative_tokens=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, model_loader_extra_config=None, preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, engine_use_ray=False, disable_log_requests=True, max_log_len=None)\r\nWARNING 06-25 15:26:04 config.py:1222] Casting torch.bfloat16 to torch.float16.\r\n2024-06-25 15:26:04,681 INFO worker.py:1568 -- Connecting to existing Ray cluster at address: 10.1.0.10:6379...\r\n2024-06-25 15:26:04,687 INFO worker.py:1753 -- Connected to Ray cluster.\r\nINFO 06-25 15:26:04 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='/ai/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='/ai/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/ai/Meta-Llama-3-8B-Instruct)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nINFO 06-25 15:26:09 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\nINFO 06-25 15:26:09 selector.py:51] Using XFormers backend.\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) INFO 06-25 15:26:09 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) INFO 06-25 15:26:09 selector.py:51] Using XFormers backend.\r\nINFO 06-25 15:26:10 utils.py:637] Found nccl from library libnccl.so.2\r\nINFO 06-25 15:26:10 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) INFO 06-25 15:26:10 utils.py:637] Found nccl from library libnccl.so.2\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) INFO 06-25 15:26:10 pynccl.py:63] vLLM is using nccl==2.20.5\r\nWARNING 06-25 15:26:10 custom_all_reduce.py:117] Custom allreduce is disabled because this process group spans across nodes.\r\nINFO 06-25 15:26:10 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\nINFO 06-25 15:26:10 selector.py:51] Using XFormers backend.\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) WARNING 06-25 15:26:10 custom_all_reduce.py:117] Custom allreduce is disabled because this process group spans across nodes.\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) INFO 06-25 15:26:10 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) INFO 06-25 15:26:10 selector.py:51] Using XFormers backend.\r\nINFO 06-25 15:26:13 model_runner.py:160] Loading model weights took 7.4829 GB\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) INFO 06-25 15:26:16 model_runner.py:160] Loading model weights took 7.4829 GB\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148] Error executing method determine_num_available_blocks. This might cause deadlock in distributed execution.\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148] Traceback (most recent call last):\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]   File \"/home/efleisc222/miniconda3/envs/ai/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 140, in execute_method\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]     return executor(*args, **kwargs)\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]   File \"/home/efleisc222/miniconda3/envs/ai/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]     return func(*args, **kwargs)\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]            ^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]   File \"/home/efleisc222/miniconda3/envs/ai/lib/python3.11/site-packages/vllm/worker/worker.py\", line 162, in determine_num_available_blocks\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]     self.model_runner.profile_run()\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]   File \"/home/efleisc222/miniconda3/envs/ai/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]     return func(*args, **kwargs)\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]            ^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]   File \"/home/efleisc222/miniconda3/envs/ai/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 844, in profile_run\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]     self.execute_model(seqs, kv_caches)\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]   File \"/home/efleisc222/miniconda3/envs/ai/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]     return func(*args, **kwargs)\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]            ^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]   File \"/home/efleisc222/miniconda3/envs/ai/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 758, in execute_model\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]     logits = self.model.compute_logits(hidden_states, sampling_metadata)\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]   File \"/home/efleisc222/miniconda3/envs/ai/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 377, in compute_logits\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]     logits = self.logits_processor(self.lm_head.weight, hidden_states,\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]   File \"/home/efleisc222/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]     return self._call_impl(*args, **kwargs)\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]   File \"/home/efleisc222/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]     return forward_call(*args, **kwargs)\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]   File \"/home/efleisc222/miniconda3/envs/ai/lib/python3.11/site-packages/vllm/model_executor/layers/logits_processor.py\", line 52, in forward\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]     logits = self._get_logits(hidden_states, embedding, embedding_bias)\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]   File \"/home/efleisc222/miniconda3/envs/ai/lib/python3.11/site-packages/vllm/model_executor/layers/logits_processor.py\", line 69, in _get_logits\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]     logits = tensor_model_parallel_gather(logits)\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]   File \"/home/efleisc222/miniconda3/envs/ai/lib/python3.11/site-packages/vllm/distributed/communication_op.py\", line 24, in tensor_model_parallel_gather\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]     return get_tp_group().gather(input_, dst, dim)\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]   File \"/home/efleisc222/miniconda3/envs/ai/lib/python3.11/site-packages/vllm/distributed/parallel_state.py\", line 301, in gather\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]     torch.distributed.gather(input_,\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]   File \"/home/efleisc222/miniconda3/envs/ai/lib/python3.11/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]     return func(*args, **kwargs)\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]            ^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]   File \"/home/efleisc222/miniconda3/envs/ai/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\", line 3133, in gather\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]     work = group.gather(output_tensors, input_tensors, opts)\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=67451, ip=10.1.0.7) ERROR 06-25 15:28:30 worker_base.py:148] RuntimeError: NCCL Error 2: unhandled system error (run with NCCL_DEBUG=INFO for details)",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-06-25T16:20:41+00:00",
    "closed_at": "2024-06-25T20:43:36+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5828/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5828"
  },
  {
    "number": 20360,
    "title": "[Feature]: any plan to support V1 engine fp8 kvcache with flashinfer?",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nvllm already supports fp8 kvcache of v0 engine + flashinfer, and fp8 kvcache of v1 engine + flashattn. Is there any plan to support fp8 kvcache of v1 engine + flashinfer?\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-07-02T07:56:17+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20360/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20360"
  },
  {
    "number": 6567,
    "title": "[Bug]: gptq model fails on pascal gpu with long prompt",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.2 LTS (x86_64)\r\nGCC version: (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0] (64-bit runtime)\r\nPython platform: Linux-4.14.83-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: Tesla P40\r\nGPU 1: Tesla P40\r\n\r\nNvidia driver version: 535.129.03\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.3\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.3\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.3\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.3\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.3\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.3\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.3\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          24\r\nOn-line CPU(s) list:             0-23\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) CPU E5-2678 v3 @ 2.50GHz\r\nCPU family:                      6\r\nModel:                           63\r\nThread(s) per core:              2\r\nCore(s) per socket:              12\r\nSocket(s):                       1\r\nStepping:                        2\r\nCPU max MHz:                     3300.0000\r\nCPU min MHz:                     1200.0000\r\nBogoMIPS:                        5000.19\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp $\r\nm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dc$\r\n sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm cpuid_fault epb invpcid_single pti intel_ppin ibrs ibpb stibp tpr_shadow vnmi flexprior$\r\nty ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts\r\nVirtualization:                  VT-x\r\nL1d cache:                       384 KiB (12 instances)\r\nL1i cache:                       384 KiB (12 instances)\r\nL2 cache:                        3 MiB (12 instances)\r\nL3 cache:                        30 MiB (1 instance)\r\nNUMA node(s):                    1\r\nNUMA node0 CPU(s):               0-23\r\nVulnerability L1tf:              Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable\r\nVulnerability Meltdown:          Mitigation; PTI\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Mitigation; __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Full generic retpoline, IBPB, IBRS_FW\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.23.5\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] torchvision==0.18.0\r\n[pip3] transformers==4.42.4\r\n[pip3] transformers-stream-generator==0.0.4\r\n[pip3] triton==2.3.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PHB     0-23    0               N/A\r\nGPU1    PHB      X      0-23    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nI try to use openai_api_server to serve Qwen2-7B-Instruct on P40. When serving gptq models, vllm comes into errors with long prompt.\r\n\r\nfp16 commond:\r\n```bash\r\npython -m vllm.entrypoints.openai.api_server --served-model-name Qwen2-7B-Instruct --model llm_models/qwen/Qwen2-7B-Instruct/ --gpu-memory-utilization 0.95 --max-model-len 4096 --max-num-seqs 1 --dtype half\r\n```\r\nfp16 works fine with short or long prompt:\r\n```\r\nINFO 07-19 15:44:53 metrics.py:295] Avg prompt throughput: 107.4 tokens/s, Avg generation throughput: 2.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache\r\nusage: 0.4%, CPU KV cache usage: 0.0%.\r\nINFO 07-19 15:44:58 metrics.py:295] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache u\r\nsage: 0.5%, CPU KV cache usage: 0.0%.\r\nINFO 07-19 15:45:03 metrics.py:295] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache u\r\nsage: 0.6%, CPU KV cache usage: 0.0%.\r\n```\r\n\r\n\r\ngptq-int8  command:\r\n```\r\n python -m vllm.entrypoints.openai.api_server --served-model-name Qwen2-7B-Instruct --model llm_models/qwen/Qwen2-7B-Instruct-GPTQ-Int8/ --gpu-memory-utilization 0.95 --max-model-len 4096 --max-num-seqs 1\r\n```\r\nshort prompt works fine:\r\n```\r\nINFO 07-19 16:18:29 metrics.py:295] Avg prompt throughput: 5.2 tokens/s, Avg generation throughput: 3.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 07-19 16:18:34 metrics.py:295] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\r\nINFO 07-19 16:18:39 metrics.py:295] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%\r\n```\r\n\r\nlong prompt go into error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/world/data-gpu-107/github/vllm/vllm/engine/async_llm_engine.py\", line 604, in run_engine_loop\r\n    done, _ = await asyncio.wait(\r\n  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 384, in wait\r\n    return await _wait(fs, timeout, return_when, loop)\r\n  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 491, in _wait\r\n    await waiter\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 399, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 123, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n   await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 756, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 776, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 297, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 77, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 72, in app\r\n    response = await func(request)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 278, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n  File \"/world/data-gpu-107/github/vllm/vllm/entrypoints/openai/api_server.py\", line 132, in create_chat_completion\r\n    generator = await openai_serving_chat.create_chat_completion(\r\n  File \"/world/data-gpu-107/github/vllm/vllm/entrypoints/openai/serving_chat.py\", line 305, in create_chat_completion\r\n    return await self.chat_completion_full_generator(\r\n  File \"/world/data-gpu-107/github/vllm/vllm/entrypoints/openai/serving_chat.py\", line 505, in chat_completion_full_generator\r\n    async for res in result_generator:\r\n  File \"/world/data-gpu-107/github/vllm/vllm/engine/async_llm_engine.py\", line 765, in generate\r\n    async for output in self._process_request(\r\n  File \"/world/data-gpu-107/github/vllm/vllm/engine/async_llm_engine.py\", line 881, in _process_request\r\n    raise e\r\n  File \"/world/data-gpu-107/github/vllm/vllm/engine/async_llm_engine.py\", line 877, in _process_request\r\n    async for request_output in stream:\r\n  File \"/world/data-gpu-107/github/vllm/vllm/engine/async_llm_engine.py\", line 91, in __anext__\r\n    raise result\r\n  File \"/world/data-gpu-107/github/vllm/vllm/engine/async_llm_engine.py\", line 44, in _log_task_completion\r\n    return_value = task.result()\r\n  File \"/world/data-gpu-107/github/vllm/vllm/engine/async_llm_engine.py\", line 603, in run_engine_loop\r\n    async with asyncio_timeout(ENGINE_ITERATION_TIMEOUT_S):\r\n  File \"/world/data-gpu-107/github/vllm/vllm/engine/async_timeout.py\", line 95, in __aexit__\r\n    self._do_exit(exc_type)\r\n  File \"/world/data-gpu-107/github/vllm/vllm/engine/async_timeout.py\", line 178, in _do_exit\r\n    raise asyncio.TimeoutError\r\nasyncio.exceptions.TimeoutError\r\nINFO 07-19 16:09:21 metrics.py:295] Avg prompt throughput: 84.8 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\r\n```\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-07-19T08:19:47+00:00",
    "closed_at": "2024-08-27T03:52:11+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6567/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6567"
  },
  {
    "number": 1830,
    "title": "awq CUDA error: an illegal memory access was encountered",
    "body": "hi, \r\n\r\nI get an \"an illegal memory access was encountered\" error when inference [deepseek-coder-33B-base-AWQ](https://huggingface.co/TheBloke/deepseek-coder-33B-base-AWQ),  which is a Llama2 (GQA) architecture model, but the smaller model is fine([deepseek-coder-6.7B-base-AWQ](https://huggingface.co/TheBloke/deepseek-coder-6.7B-base-AWQ)), the relevant information as follows:\r\n\r\n## Environment\r\npython==3.8\r\ntorch==2.0.1+cu118\r\ntransformers==4.34.1\r\nvllm==0.2.2\r\n\r\n## Code\r\n````\r\nfrom vllm import LLM, SamplingParams\r\nimport torch\r\n\r\nmodel_path = \"deepseek-coder-33b-base-awq\"\r\n\r\nsampling_params = SamplingParams(temperature=0.0, \r\n                                      n=1,\r\n                                      use_beam_search=False,\r\n                                      top_p=1, top_k=-1, max_tokens=200, \r\n                                      skip_special_tokens=False, \r\n                                      stop_token_ids=stop_token_ids)\r\n\r\nllm = LLM(model=model_path, quantization=\"awq\", dtype=\"auto\", gpu_memory_utilization=0.9, swap_space=32)\r\n\r\ntext = \"def quick_sort(\"\r\noutputs = llm.generate([text], sampling_params)\r\nprint(outputs)\r\n````\r\n\r\n## Error\r\n````\r\nINFO 11-29 16:37:53 llm_engine.py:72] Initializing an LLM engine with config: model='deepseek-coder-33b-base-awq', tokenizer='deepseek-coder-33b-base-awq', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=65536, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=awq, seed=0)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nTraceback (most recent call last):\r\n  File \"infer_vllm.py\", line 26, in <module>\r\n    llm = LLM(model=ckpt, quantization=\"awq\", dtype=\"auto\", gpu_memory_utilization=0.9, swap_space=32)\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/entrypoints/llm.py\", line 93, in __init__\r\n    self.llm_engine = LLMEngine.from_engine_args(engine_args)\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/engine/llm_engine.py\", line 243, in from_engine_args\r\n    engine = cls(*engine_configs,\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/engine/llm_engine.py\", line 113, in __init__\r\n    self._init_cache()\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/engine/llm_engine.py\", line 205, in _init_cache\r\n    num_blocks = self._run_workers(\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/engine/llm_engine.py\", line 737, in _run_workers\r\n    self._run_workers_in_batch(workers, method, *args, **kwargs))\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/engine/llm_engine.py\", line 711, in _run_workers_in_batch\r\n    output = executor(*args, **kwargs)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/worker/worker.py\", line 113, in profile_num_available_blocks\r\n    self.model(\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/model_executor/models/llama.py\", line 288, in forward\r\n    hidden_states = self.model(input_ids, positions, kv_caches,\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/model_executor/models/llama.py\", line 254, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/model_executor/models/llama.py\", line 216, in forward\r\n    hidden_states = self.mlp(hidden_states)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/model_executor/models/llama.py\", line 74, in forward\r\n    gate_up, _ = self.gate_up_proj(x)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/model_executor/layers/linear.py\", line 203, in forward\r\n    output_parallel = self.linear_method.apply_weights(\r\n  File \"/home/vllm/build/lib.linux-x86_64-cpython-38/vllm/model_executor/layers/quantization/awq.py\", line 154, in apply_weights\r\n    out = ops.awq_gemm(reshaped_x, qweight, scales, qzeros, pack_factor)\r\nRuntimeError: CUDA error: an illegal memory access was encountered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n````",
    "labels": [],
    "state": "closed",
    "created_at": "2023-11-29T08:57:46+00:00",
    "closed_at": "2023-11-30T09:14:47+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1830/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1830"
  },
  {
    "number": 17306,
    "title": "[Usage]:  vllm infer QWQ32B can\u2018t enable sliding window",
    "body": "### Your current environment\n\n\u5de5\u5177\uff1a Vllm=0.8.4\n\u6a21\u578b\uff1aQwq32B\n\u914d\u7f6e\uff1a\n{\n\"architectures\": [\n\"Qwen2ForCausalLM\"\n],\n\"attention_dropout\": 0.0,\n\"bos_token_id\": 151643,\n\"eos_token_id\": 151645,\n\"hidden_act\": \"silu\",\n\"hidden_size\": 5120,\n\"initializer_range\": 0.02,\n\"intermediate_size\": 27648,\n\"max_position_embeddings\": 40960,\n\"max_window_layers\": 64,\n\"model_type\": \"qwen2\",\n\"num_attention_heads\": 40,\n\"num_hidden_layers\": 64,\n\"num_key_value_heads\": 8,\n\"rms_norm_eps\": 1e-05,\n\"rope_theta\": 1000000.0,\n\"sliding_window\": 40960,\n\"tie_word_embeddings\": false,\n\"torch_dtype\": \"bfloat16\",\n\"transformers_version\": \"4.43.1\",\n\"use_cache\": true,\n\"use_sliding_window\": true,\n\"vocab_size\": 152064\n}\n\n\u542f\u52a8\u547d\u4ee4\uff1a\npython -m vllm.entrypoints.openai.api_server --model /home/user/Models/QwQ-32B --host \"::\" --port 8600 --tensor-parallel-size 8 --gpu-memory-utilization 0.95 --max-model-len 40960 --dtype bfloat16 --max-num-seqs 16 --served-model-name qwq32b --swap-space 10 --enable_prefix_caching --enable-chunked-prefill --use-v2-block-manager --enforce-eager --disable-custom-all-reduce --trust-remote-code\n\n\u9519\u8bef\u4fe1\u606f\uff1a\n\n(VllmWorker rank=1 pid=3457224) raise ValueError(\"Sliding window for some but all layers is not \"\n(VllmWorker rank=1 pid=3457224) ValueError: Sliding window for some but all layers is not supported. This model uses sliding window but max_window_layers = 64 is less than num_hidden_layers = 64. Please open an issue to discuss this feature.\nCRITICAL 04-28 19:54:55 [core_client.py:359] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n(VllmWorker rank=1 pid=3457224) Exception ignored in atexit callback: <function shutdown at 0x7227b809c9d0>\n(VllmWorker rank=1 pid=3457224) Traceback (most recent call last):\n\n### How would you like to use vllm\n\nI want to run inference of a [QWQ32B](https://huggingface.co/Qwen/QwQ-32B). I don't know how to enable sliding window feature with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-04-28T12:11:27+00:00",
    "closed_at": "2025-05-15T05:29:39+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17306/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17306"
  },
  {
    "number": 17711,
    "title": "[Usage]: Offline multi-node inference",
    "body": "### Your current environment\n\nHello everybody\nAccording to the vLLM documentation, it seems that in order to performe multi-node inference, one has to do this in an online setting.\nI am working with access to a GPU cluster, where the compute nodes do not have internet access. My goal is to run inference with llama 3.3 70B Instruct on a file using 4 nodes (4 gpus per node), however, if I try to use the LLM class, I get an error saying that data parallelism isn't possible and I should use AsyncEngine instead.\nHowever, asyncEngine cannot be used with the chat() method, thus I am currently unable to perform inference on this file containing samples.\nI hereby wanted to ask if it's possible to perform offline multi-node inference and if so whether there are guides or further documentation on it, thank you\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "open",
    "created_at": "2025-05-06T10:47:00+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17711/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17711"
  },
  {
    "number": 5663,
    "title": "[Bug]: Qwen2-72B-Instruct-gptq-int4 Repetitive issues",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nMachine A800, VLLM 0.5.0, PROMPT=\u5f00\u59cb, output max tokens = 2048, Temperature sets 0.7\r\n\r\nVLLM loads Qwen2-72b-InStruct-GPTQ-IT4, and uses the Benchmark script of VLLM to do concurrent testing. Whether it is a concurrent limit or 10 concurrency restrictions, the output will be repeated.\r\nhttps://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_serving.py\r\n![image](https://github.com/vllm-project/vllm/assets/57557769/f3440a14-71a1-4b8c-b3e4-6a66aaba4aa8)\r\n",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-06-19T02:29:03+00:00",
    "closed_at": "2024-11-25T02:05:43+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5663/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5663"
  },
  {
    "number": 12410,
    "title": "[Bug]: Performance regression when use PyTorch regional compilation",
    "body": "### Your current environment\n\n<details>\n<summary>Run on hpu backend  on version from  https://github.com/HabanaAI/vllm-fork </summary>\n\n```text\nINFO 01-24 15:38:37 __init__.py:188] Automatically detected platform hpu.\nCollecting environment information...\nPyTorch version: 2.5.1a0+git354fc07\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-113-generic-x86_64-with-glibc2.35\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      52 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             224\nOn-line CPU(s) list:                0-223\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) Platinum 8480+\nCPU family:                         6\nModel:                              143\nThread(s) per core:                 2\nCore(s) per socket:                 56\nSocket(s):                          2\nStepping:                           8\nCPU max MHz:                        3800.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           4000.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                     VT-x\nL1d cache:                          5.3 MiB (112 instances)\nL1i cache:                          3.5 MiB (112 instances)\nL2 cache:                           224 MiB (112 instances)\nL3 cache:                           210 MiB (2 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0-55,112-167\nNUMA node1 CPU(s):                  56-111,168-223\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] habana-torch-dataloader==1.20.0.357\n[pip3] habana-torch-plugin==1.20.0.357\n[pip3] numpy==1.26.4\n[pip3] pynvml==8.0.4\n[pip3] pytorch-lightning==2.5.0.post0\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.5.1a0+git354fc07\n[pip3] torch_tb_profiler==0.4.0\n[pip3] torchaudio==2.5.1a0+1661daf\n[pip3] torchdata==0.9.0+d4bb3e6\n[pip3] torchmetrics==1.6.1\n[pip3] torchtext==0.18.0a0+9bed85d\n[pip3] torchvision==0.20.1a0+3ac97aa\n[pip3] transformers==4.48.1\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.6.3.dev1995+gf78b021c\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nLD_LIBRARY_PATH=/usr/local/lib/python3.10/dist-packages/cv2/../../lib64:/opt/habanalabs/libfabric-1.22.0/lib:/opt/amazon/openmpi/lib:/usr/lib/habanalabs:\n\n```\n\n</details>\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nPerformance regression occurs when use regional compilation after https://github.com/vllm-project/vllm/pull/11967\n\nWhen run :\n```\n python -u benchmarks/benchmark_throughput.py \\\n                                --model /path_to_model/Llama-2-7b-hf \\\n                                --device hpu \\\n                                --seed 2024 \\\n                                --backend vllm \\\n                                --dataset /path_to_dataset/ShareGPT_V3_unfiltered_cleaned_split.json \\\n                                --num-prompts 1000 \\\n                                --dtype bfloat16 \\\n                                --max-model-len 4096 \\\n                                --max-num-batched-tokens 8192 \\\n                                --max-num-seqs 128 \\\n                                --use-padding-aware-scheduling\n```\n\nThe [regional compilation](https://pytorch.org/tutorials/recipes/regional_compilation.html) is used in [the code](https://github.com/HabanaAI/vllm-fork/blob/1b8b69e7b302857549ee95d34c0593bd675a71fd/vllm/worker/hpu_model_runner.py#L227C2-L258C37)  I observe big throughput degradation due to recompilation happened  due to indexing in attention layer:\n\n```\ntorch/_dynamo/guards.py:2813] [1/4] [__recompiles] Recompiling function forward in /software/users/akotlowski/vllm-fork/vllm/model_executor/models/llama.py:267\ntorch/_dynamo/guards.py:2813] [1/4] [__recompiles]     triggered by the following guard failure(s):\ntorch/_dynamo/guards.py:2813] [1/4] [__recompiles]     - 1/3: L['self']._modules['self_attn']._modules['attn'].layer_name == 'model.layers.3.self_attn.attn'  # self = forward_context.attn_layers[layer_name]  # oftware/users/akotlowski/vllm-fork/vllm/attention/layer.py:244 in unified_attention\ntorch/_dynamo/guards.py:2813] [1/4] [__recompiles]     - 1/2: L['self']._modules['self_attn']._modules['attn'].layer_name == 'model.layers.2.self_attn.attn'  # self = forward_context.attn_layers[layer_name]  # oftware/users/akotlowski/vllm-fork/vllm/attention/layer.py:244 in unified_attention\ntorch/_dynamo/guards.py:2813] [1/4] [__recompiles]     - 1/1: L['self']._modules['self_attn']._modules['attn'].layer_name == 'model.layers.1.self_attn.attn'  # self = forward_context.attn_layers[layer_name]  # oftware/users/akotlowski/vllm-fork/vllm/attention/layer.py:244 in unified_attention\ntorch/_dynamo/guards.py:2813] [1/4] [__recompiles]     - 1/0: L['self']._modules['self_attn']._modules['attn'].layer_name == 'model.layers.0.self_attn.attn'  # self = forward_context.attn_layers[layer_name]  # oftware/users/akotlowski/vllm-fork/vllm/attention/layer.py:244 in unified_attention\n```\nhttps://github.com/vllm-project/vllm/blob/e784c6b9984e8f8116f74000b863d941495acb0b/vllm/attention/layer.py#L268\n\nWhen we partially revert  the change https://github.com/vllm-project/vllm/pull/11967, dynamo compilation treats  layers as the same, which originally bring performance boost and reduce time of compilation.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-01-24T15:10:46+00:00",
    "closed_at": "2025-02-05T21:24:27+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12410/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/12410"
  },
  {
    "number": 16668,
    "title": "[Bug]: Wrong lora mapping during prompt logprobs computing",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 04-15 08:06:07 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.31.6\nLibc version: glibc-2.35\n\nPython version: 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.4.203-1-tlinux4-0011.spr.0001.2-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA H20\nGPU 1: NVIDIA H20\nGPU 2: NVIDIA H20\nGPU 3: NVIDIA H20\nGPU 4: NVIDIA H20\nGPU 5: NVIDIA H20\nGPU 6: NVIDIA H20\nGPU 7: NVIDIA H20\n\nNvidia driver version: 535.161.08\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   52 bits physical, 57 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          192\nOn-line CPU(s) list:             0-191\nVendor ID:                       GenuineIntel\nBIOS Vendor ID:                  Intel(R) Corporation\nModel name:                      Intel(R) Xeon(R) Platinum 8476C\nBIOS Model name:                 Intel(R) Xeon(R) Platinum 8476C\nCPU family:                      6\nModel:                           143\nThread(s) per core:              2\nCore(s) per socket:              48\nSocket(s):                       2\nStepping:                        8\nFrequency boost:                 enabled\nCPU max MHz:                     2601.0000\nCPU min MHz:                     800.0000\nBogoMIPS:                        5200.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                  VT-x\nL1d cache:                       4.5 MiB (96 instances)\nL1i cache:                       3 MiB (96 instances)\nL2 cache:                        192 MiB (96 instances)\nL3 cache:                        195 MiB (2 instances)\nNUMA node(s):                    2\nNUMA node0 CPU(s):               0-47,96-143\nNUMA node1 CPU(s):               48-95,144-191\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Vulnerable: eIBRS with unprivileged eBPF\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.1.post2+cu124torch2.6\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.3\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.5.dev22+g1575c1701\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10   NIC11   NIC12   NIC13   NIC14   NIC15   NIC16     NIC17   NIC18   NIC19   NIC20   NIC21   NIC22   NIC23   NIC24   NIC25   CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE      NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     48-95,144-191   1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE      NODE    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    48-95,144-191   1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE      NODE    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    48-95,144-191   1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE      NODE    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    48-95,144-191   1               N/A\nNIC0    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE     X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC1    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC2    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC3    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC4    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC9    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC10   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX       PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC11   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX       PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC12   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX       PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC13   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX       PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC14   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX       PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC15   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX       PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC16   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X        PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC17   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX        X      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC18   PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC19   NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     NODE     X      NODE    NODE    SYS     SYS     SYS     SYS\nNIC20   NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     NODE    NODE     X      NODE    SYS     SYS     SYS     SYS\nNIC21   NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS\nNIC22   SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE      NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE\nNIC23   SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE      NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE\nNIC24   SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE      NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE\nNIC25   SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE      NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n  NIC10: mlx5_10\n  NIC11: mlx5_11\n  NIC12: mlx5_12\n  NIC13: mlx5_13\n  NIC14: mlx5_14\n  NIC15: mlx5_15\n  NIC16: mlx5_16\n  NIC17: mlx5_17\n  NIC18: mlx5_bond_1\n  NIC19: mlx5_bond_2\n  NIC20: mlx5_bond_3\n  NIC21: mlx5_bond_4\n  NIC22: mlx5_bond_5\n  NIC23: mlx5_bond_6\n  NIC24: mlx5_bond_7\n  NIC25: mlx5_bond_8\n\nNVIDIA_VISIBLE_DEVICES=all\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\nNCCL_VERSION=2.20.5-1\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nCUDA_VERSION=12.4.0\nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n\n\n### \ud83d\udc1b Describe the bug\n\nJust run the `multilora_inference` example\n\n```bash\nroot:/vllm-workspace/vllm-master# VLLM_USE_V1=1  python3 examples/offline_inference/multilora_inference.py \nINFO 04-15 08:10:34 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-15 08:10:41 [config.py:697] This model supports multiple tasks: {'reward', 'generate', 'classify', 'score', 'embed'}. Defaulting to 'generate'.\nINFO 04-15 08:10:42 [config.py:1956] Chunked prefill is enabled with max_num_batched_tokens=2048.\nWARNING 04-15 08:10:42 [config.py:2644] LoRA with chunked prefill is still experimental and may be unstable.\nINFO 04-15 08:10:42 [core.py:61] Initializing a V1 LLM engine (v0.8.5.dev22+g1575c1701) with config: model='/root/vllm/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='/root/vllm/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/root/vllm/Llama-2-7b-chat-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\nWARNING 04-15 08:10:45 [utils.py:2491] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f8bd924bce0>\nINFO 04-15 08:10:46 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 04-15 08:10:46 [cuda.py:221] Using Flash Attention backend on V1 engine.\nINFO 04-15 08:10:46 [gpu_model_runner.py:1278] Starting to load model /root/vllm/Llama-2-7b-chat-hf...\nINFO 04-15 08:10:46 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.75s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.25s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.33s/it]\n\nINFO 04-15 08:10:49 [loader.py:458] Loading weights took 2.75 seconds\nINFO 04-15 08:10:49 [punica_selector.py:18] Using PunicaWrapperGPU.\nINFO 04-15 08:10:49 [gpu_model_runner.py:1293] Model loading took 12.5986 GiB and 2.914518 seconds\nINFO 04-15 08:10:58 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/a5f1978ee0/rank_0_0 for vLLM's torch.compile\nINFO 04-15 08:10:58 [backends.py:426] Dynamo bytecode transform time: 9.14 s\nINFO 04-15 08:10:59 [backends.py:115] Directly load the compiled graph for shape None from the cache\nINFO 04-15 08:11:07 [monitor.py:33] torch.compile takes 9.14 s in total\nINFO 04-15 08:11:07 [kv_cache_utils.py:634] GPU KV cache size: 147,888 tokens\nINFO 04-15 08:11:07 [kv_cache_utils.py:637] Maximum concurrency for 4,096 tokens per request: 36.11x\nINFO 04-15 08:11:36 [gpu_model_runner.py:1628] Graph capturing finished in 29 secs, took 0.86 GiB\nINFO 04-15 08:11:37 [core.py:163] init engine (profile, create kv cache, warmup model) took 47.57 seconds\nINFO 04-15 08:11:37 [core_client.py:435] Core engine process 0 ready.\n--------------------------------------------------\nERROR 04-15 08:11:37 [core.py:387] EngineCore hit an exception: Traceback (most recent call last):\nERROR 04-15 08:11:37 [core.py:387]   File \"/vllm-workspace/vllm-master/vllm/v1/engine/core.py\", line 380, in run_engine_core\nERROR 04-15 08:11:37 [core.py:387]     engine_core.run_busy_loop()\nERROR 04-15 08:11:37 [core.py:387]   File \"/vllm-workspace/vllm-master/vllm/v1/engine/core.py\", line 402, in run_busy_loop\nERROR 04-15 08:11:37 [core.py:387]     self._process_engine_step()\nERROR 04-15 08:11:37 [core.py:387]   File \"/vllm-workspace/vllm-master/vllm/v1/engine/core.py\", line 431, in _process_engine_step\nERROR 04-15 08:11:37 [core.py:387]     outputs = self.step_fn()\nERROR 04-15 08:11:37 [core.py:387]               ^^^^^^^^^^^^^^\nERROR 04-15 08:11:37 [core.py:387]   File \"/vllm-workspace/vllm-master/vllm/v1/engine/core.py\", line 207, in step\nERROR 04-15 08:11:37 [core.py:387]     output = self.model_executor.execute_model(scheduler_output)\nERROR 04-15 08:11:37 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 08:11:37 [core.py:387]   File \"/vllm-workspace/vllm-master/vllm/v1/executor/abstract.py\", line 77, in execute_model\nERROR 04-15 08:11:37 [core.py:387]     output = self.collective_rpc(\"execute_model\",\nERROR 04-15 08:11:37 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 08:11:37 [core.py:387]   File \"/vllm-workspace/vllm-master/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\nERROR 04-15 08:11:37 [core.py:387]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 04-15 08:11:37 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 08:11:37 [core.py:387]   File \"/vllm-workspace/vllm-master/vllm/utils.py\", line 2425, in run_method\nERROR 04-15 08:11:37 [core.py:387]     return func(*args, **kwargs)\nERROR 04-15 08:11:37 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 08:11:37 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 04-15 08:11:37 [core.py:387]     return func(*args, **kwargs)\nERROR 04-15 08:11:37 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 08:11:37 [core.py:387]   File \"/vllm-workspace/vllm-master/vllm/v1/worker/gpu_worker.py\", line 242, in execute_model\nERROR 04-15 08:11:37 [core.py:387]     output = self.model_runner.execute_model(scheduler_output)\nERROR 04-15 08:11:37 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 08:11:37 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 04-15 08:11:37 [core.py:387]     return func(*args, **kwargs)\nERROR 04-15 08:11:37 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 08:11:37 [core.py:387]   File \"/vllm-workspace/vllm-master/vllm/v1/worker/gpu_model_runner.py\", line 1141, in execute_model\nERROR 04-15 08:11:37 [core.py:387]     prompt_logprobs_dict = self._get_prompt_logprobs_dict(\nERROR 04-15 08:11:37 [core.py:387]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 08:11:37 [core.py:387]   File \"/vllm-workspace/vllm-master/vllm/v1/worker/gpu_model_runner.py\", line 1359, in _get_prompt_logprobs_dict\nERROR 04-15 08:11:37 [core.py:387]     logits = self.model.compute_logits(prompt_hidden_states, None)\nERROR 04-15 08:11:37 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 08:11:37 [core.py:387]   File \"/vllm-workspace/vllm-master/vllm/model_executor/models/llama.py\", line 550, in compute_logits\nERROR 04-15 08:11:37 [core.py:387]     logits = self.logits_processor(self.lm_head, hidden_states,\nERROR 04-15 08:11:37 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 08:11:37 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nERROR 04-15 08:11:37 [core.py:387]     return self._call_impl(*args, **kwargs)\nERROR 04-15 08:11:37 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 08:11:37 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\nERROR 04-15 08:11:37 [core.py:387]     return forward_call(*args, **kwargs)\nERROR 04-15 08:11:37 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 08:11:37 [core.py:387]   File \"/vllm-workspace/vllm-master/vllm/lora/layers.py\", line 1164, in forward\nERROR 04-15 08:11:37 [core.py:387]     return type(self.base_layer).forward(self, *args, **kwargs)\nERROR 04-15 08:11:37 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 08:11:37 [core.py:387]   File \"/vllm-workspace/vllm-master/vllm/model_executor/layers/logits_processor.py\", line 70, in forward\nERROR 04-15 08:11:37 [core.py:387]     logits = self._get_logits(hidden_states, lm_head, embedding_bias)\nERROR 04-15 08:11:37 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 08:11:37 [core.py:387]   File \"/vllm-workspace/vllm-master/vllm/lora/layers.py\", line 1150, in _get_logits\nERROR 04-15 08:11:37 [core.py:387]     logits[:,\nERROR 04-15 08:11:37 [core.py:387] RuntimeError: The expanded size of the tensor (58) must match the existing size (3) at non-singleton dimension 0.  Target sizes: [58, 256].  Tensor sizes: [3, 256]\nERROR 04-15 08:11:37 [core.py:387] \nCRITICAL 04-15 08:11:37 [core_client.py:359] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\nKilled\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-15T15:13:52+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16668/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16668"
  },
  {
    "number": 1273,
    "title": "`stream` should only accept type Boolean when using OpenAI API Server spec",
    "body": "The current behaviour of vLLM does not match the behaviour of OpenAI and Azure OpenAI when it comes to the `stream` parameter in the request body.\r\n\r\nCurrent behaviour of OpenAI and Azure OpenAI:\r\n- Only `\"stream\": true` or `\"stream\": false` are accepted. Setting `\"stream\": \"true\"` or `\"stream\": \"false\"` (or any other non-Boolean values) will raise the following error:\r\n```\r\n{\r\n  \"error\": {\r\n    \"message\": \"'false' is not of type 'boolean' - 'stream'\",\r\n    \"type\": \"invalid_request_error\",\r\n    \"param\": null,\r\n    \"code\": null\r\n  }\r\n}\r\n```\r\n\r\nCurrent behaviour of vLLM:\r\n- The following values for the `stream` request body parameter are accepted by vLLM: `true`, `\"true\"`, `false`, `\"false\"`\r\n- Any other values will raise the following error:\r\n```\r\n{\r\n    \"object\": \"error\",\r\n    \"message\": \"[{'loc': ('body', 'stream'), 'msg': 'value could not be parsed to a boolean', 'type': 'type_error.bool'}]\",\r\n    \"type\": \"invalid_request_error\",\r\n    \"param\": null,\r\n    \"code\": null\r\n}\r\n```\r\n- It seems like this is caused by the use of Pydantic with `stream` variable set to type `bool` instead of `StrictBool` ([source code](https://github.com/vllm-project/vllm/blob/acbed3ef40f015fcf64460e629813922fab90380/vllm/entrypoints/openai/protocol.py#L63))\r\n\r\nMay I know if there is an agreement that vLLM should also reject the request when `stream` is set to the string values of `\"true\"` and `\"false\"`? I can prepare a PR to make the change if that is the desired behaviour.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-10-06T06:52:39+00:00",
    "closed_at": "2024-03-13T09:44:07+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1273/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1273"
  },
  {
    "number": 11232,
    "title": "[Bug]: With ROCm and certain HF models that require 'trust-remote-code', you get VLLM_RPC_TIMEOUT and failure to finish loading.",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nuser1@dev1:~/vllm$ python collect_env.py\r\nWARNING 12-16 05:24:35 rocm.py:31] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\r\nCollecting environment information...\r\nPyTorch version: 2.6.0.dev20241211+rocm6.1\r\nIs debug build: False\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: 6.1.40091-a8dbc0c19\r\n\r\nOS: Ubuntu 22.04.5 LTS (x86_64)\r\nGCC version: (conda-forge gcc 12.1.0-17) 12.1.0\r\nClang version: Could not collect\r\nCMake version: version 3.31.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:16:10) [GCC 13.3.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-97-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 11.7.64\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: AMD Instinct MI250X/MI250 (gfx90a:sramecc+:xnack-)\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: 6.1.40091\r\nMIOpen runtime version: 3.1.0\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             32\r\nOn-line CPU(s) list:                0-31\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 73F3 16-Core Processor\r\nCPU family:                         25\r\nModel:                              1\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 16\r\nSocket(s):                          2\r\nStepping:                           1\r\nFrequency boost:                    enabled\r\nCPU max MHz:                        4036.6211\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           6986.65\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\nVirtualization:                     AMD-V\r\nL1d cache:                          1 MiB (32 instances)\r\nL1i cache:                          1 MiB (32 instances)\r\nL2 cache:                           16 MiB (32 instances)\r\nL3 cache:                           512 MiB (16 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-15\r\nNUMA node1 CPU(s):                  16-31\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; safe RET\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==11.525.150\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] onnxruntime==1.18.0\r\n[pip3] open-clip-torch==2.24.0\r\n[pip3] pytorch-lightning==2.3.0\r\n[pip3] pytorch-triton-rocm==3.2.0+git35c6c7c6\r\n[pip3] pyzmq==25.1.2\r\n[pip3] torch==2.6.0.dev20241211+rocm6.1\r\n[pip3] torchaudio==2.5.0.dev20241212+rocm6.1\r\n[pip3] torchdiffeq==0.2.4\r\n[pip3] torchmetrics==1.4.0.post0\r\n[pip3] torchsde==0.2.6\r\n[pip3] torchvision==0.22.0.dev20241212+rocm6.1\r\n[pip3] transformers==4.47.0\r\n[pip3] triton==3.1.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\r\n[conda] pytorch-triton-rocm       3.2.0+git35c6c7c6          pypi_0    pypi\r\n[conda] torch                     2.6.0.dev20241211+rocm6.1          pypi_0    pypi\r\n[conda] torchaudio                2.5.0.dev20241212+rocm6.1          pypi_0    pypi\r\n[conda] torchvision               0.22.0.dev20241212+rocm6.1          pypi_0    pypi\r\n[conda] transformers              4.47.0                   pypi_0    pypi\r\n[conda] triton                    3.1.0                    pypi_0    pypi\r\nROCM Version: 6.3.42131-fa1d09cbd\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.4.post2.dev378+g69ba344d\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\n============================ ROCm System Management Interface ============================\r\n================================ Weight between two GPUs =================================\r\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\r\nGPU0   0            15           15           30           30           30           15           30\r\nGPU1   15           0            30           15           30           15           30           45\r\nGPU2   15           30           0            15           15           30           30           30\r\nGPU3   30           15           15           0            30           45           30           15\r\nGPU4   30           30           15           30           0            15           15           30\r\nGPU5   30           15           30           45           15           0            30           15\r\nGPU6   15           30           30           30           15           30           0            15\r\nGPU7   30           45           30           15           30           15           15           0\r\n\r\n================================= Hops between two GPUs ==================================\r\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\r\nGPU0   0            1            1            1            1            1            1            1\r\nGPU1   1            0            1            1            1            1            1            1\r\nGPU2   1            1            0            1            1            1            1            1\r\nGPU3   1            1            1            0            1            1            1            1\r\nGPU4   1            1            1            1            0            1            1            1\r\nGPU5   1            1            1            1            1            0            1            1\r\nGPU6   1            1            1            1            1            1            0            1\r\nGPU7   1            1            1            1            1            1            1            0\r\n\r\n=============================== Link Type between two GPUs ===============================\r\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\r\nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\r\nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\r\nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI\r\nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI\r\nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI\r\nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI\r\nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI\r\nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0\r\n\r\n======================================= Numa Nodes =======================================\r\nGPU[0]          : (Topology) Numa Node: 0\r\nGPU[0]          : (Topology) Numa Affinity: 0\r\nGPU[1]          : (Topology) Numa Node: 0\r\nGPU[1]          : (Topology) Numa Affinity: 0\r\nGPU[2]          : (Topology) Numa Node: 0\r\nGPU[2]          : (Topology) Numa Affinity: 0\r\nGPU[3]          : (Topology) Numa Node: 0\r\nGPU[3]          : (Topology) Numa Affinity: 0\r\nGPU[4]          : (Topology) Numa Node: 1\r\nGPU[4]          : (Topology) Numa Affinity: 1\r\nGPU[5]          : (Topology) Numa Node: 1\r\nGPU[5]          : (Topology) Numa Affinity: 1\r\nGPU[6]          : (Topology) Numa Node: 1\r\nGPU[6]          : (Topology) Numa Affinity: 1\r\nGPU[7]          : (Topology) Numa Node: 1\r\nGPU[7]          : (Topology) Numa Affinity: 1\r\n================================== End of ROCm SMI Log ===================================\r\n\r\nLD_LIBRARY_PATH=/home/user1/.local/lib/python3.10/site-packages/cv2/../../lib64:/opt/rocm/lib:/opt/nvidia/cuda-11.7/lib64\r\nVLLM_WORKER_MULTIPROC_METHOD=spawn\r\nCUDA_MODULE_LOADING=LAZY\r\n\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n```\r\n(VllmWorkerProcess pid=5932) /opt/miniconda3/envs/vllm/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:524: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\r\n(VllmWorkerProcess pid=5932)   warnings.warn(\r\n/opt/miniconda3/envs/vllm/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:524: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\r\n  warnings.warn(\r\n(VllmWorkerProcess pid=5934) /opt/miniconda3/envs/vllm/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:524: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\r\n(VllmWorkerProcess pid=5934)   warnings.warn(\r\n(VllmWorkerProcess pid=5933) /opt/miniconda3/envs/vllm/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:524: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\r\n(VllmWorkerProcess pid=5933)   warnings.warn(\r\nTask exception was never retrieved\r\nfuture: <Task finished name='Task-2' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /opt/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm-0.6.4.post2.dev266+g7be15d93.rocm634-py3.10-linux-x86_64.egg/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\r\nTraceback (most recent call last):\r\n  File \"/opt/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm-0.6.4.post2.dev266+g7be15d93.rocm634-py3.10-linux-x86_64.egg/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\r\n    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\r\n  File \"/home/realai/.local/lib/python3.10/site-packages/zmq/_future.py\", line 382, in poll\r\n    raise _zmq.ZMQError(_zmq.ENOTSUP)\r\nzmq.error.ZMQError: Operation not supported\r\nTask exception was never retrieved\r\nfuture: <Task finished name='Task-3' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /opt/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm-0.6.4.post2.dev266+g7be15d93.rocm634-py3.10-linux-x86_64.egg/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\r\nTraceback (most recent call last):\r\n  File \"/opt/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm-0.6.4.post2.dev266+g7be15d93.rocm634-py3.10-linux-x86_64.egg/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\r\n    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\r\n  File \"/home/realai/.local/lib/python3.10/site-packages/zmq/_future.py\", line 382, in poll\r\n    raise _zmq.ZMQError(_zmq.ENOTSUP)\r\nzmq.error.ZMQError: Operation not supported\r\nTraceback (most recent call last):\r\n  File \"/opt/miniconda3/envs/vllm/bin/vllm\", line 33, in <module>\r\n    sys.exit(load_entry_point('vllm==0.6.4.post2.dev378+g69ba344d.rocm634', 'console_scripts', 'vllm')())\r\n  File \"/opt/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm-0.6.4.post2.dev266+g7be15d93.rocm634-py3.10-linux-x86_64.egg/vllm/scripts.py\", line 201, in main\r\n    args.dispatch_function(args)\r\n  File \"/opt/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm-0.6.4.post2.dev266+g7be15d93.rocm634-py3.10-linux-x86_64.egg/vllm/scripts.py\", line 42, in serve\r\n    uvloop.run(run_server(args))\r\n  File \"/opt/miniconda3/envs/vllm/lib/python3.10/site-packages/uvloop/__init__.py\", line 82, in run\r\n    return loop.run_until_complete(wrapper())\r\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\r\n  File \"/opt/miniconda3/envs/vllm/lib/python3.10/site-packages/uvloop/__init__.py\", line 61, in wrapper\r\n    return await main\r\n  File \"/opt/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm-0.6.4.post2.dev266+g7be15d93.rocm634-py3.10-linux-x86_64.egg/vllm/entrypoints/openai/api_server.py\", line 649, in run_server\r\n    async with build_async_engine_client(args) as engine_client:\r\n  File \"/opt/miniconda3/envs/vllm/lib/python3.10/contextlib.py\", line 199, in __aenter__\r\n    return await anext(self.gen)\r\n  File \"/opt/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm-0.6.4.post2.dev266+g7be15d93.rocm634-py3.10-linux-x86_64.egg/vllm/entrypoints/openai/api_server.py\", line 116, in build_async_engine_client\r\n    async with build_async_engine_client_from_engine_args(\r\n  File \"/opt/miniconda3/envs/vllm/lib/python3.10/contextlib.py\", line 199, in __aenter__\r\n    return await anext(self.gen)\r\n  File \"/opt/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm-0.6.4.post2.dev266+g7be15d93.rocm634-py3.10-linux-x86_64.egg/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\r\n    raise RuntimeError(\r\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\r\n(vllm) realai@c1:~$ /opt/miniconda3/envs/vllm/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 16 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n/opt/miniconda3/envs/vllm/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n\r\n```\n\n### \ud83d\udc1b Describe the bug\n\nAs stated above:\r\nWith ROCm and certain HF models that require 'trust-remote-code', you get VLLM_RPC_TIMEOUT and failure to finish loading.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-16T11:28:31+00:00",
    "closed_at": "2025-04-18T02:06:36+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11232/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11232"
  },
  {
    "number": 1721,
    "title": "Answers generated by vllm 0.2.2+cu121 not align with vllm 0.2.2+cu118, the former is more similar to hf",
    "body": "the latter is same as vllm 0.2.1.post1\r\n\r\npython3.9",
    "labels": [],
    "state": "closed",
    "created_at": "2023-11-20T08:02:24+00:00",
    "closed_at": "2024-03-20T12:50:53+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1721/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1721"
  },
  {
    "number": 15672,
    "title": "[Usage]: torchrun data parallel and tensor parallel at the same time",
    "body": "### Your current environment\n\n```\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] triton==3.2.0\n\nName: vllm\nVersion: 0.8.3.dev77+gb4245a48\n\n[conda] cuda-cudart               12.4.127             h99ab3db_0  \n[conda] cuda-cudart-dev           12.4.127             h99ab3db_0  \n[conda] cuda-cudart-dev_linux-64  12.4.127             hd681fbe_0  \n[conda] cuda-cudart-static        12.4.127             h99ab3db_0  \n[conda] cuda-cudart-static_linux-64 12.4.127             hd681fbe_0  \n[conda] cuda-cudart_linux-64      12.4.127             hd681fbe_0  \n[conda] cuda-cupti                12.4.127             h6a678d5_1  \n[conda] cuda-cupti-dev            12.4.127             h6a678d5_1  \n[conda] cuda-libraries            12.4.1               h06a4308_1  \n[conda] cuda-libraries-dev        12.4.1               h06a4308_1  \n[conda] cuda-nvrtc                12.4.127             h99ab3db_1  \n[conda] cuda-nvrtc-dev            12.4.127             h99ab3db_1  \n[conda] cuda-nvtx                 12.4.127             h6a678d5_1  \n[conda] cuda-opencl               12.4.127             h6a678d5_0  \n[conda] cuda-opencl-dev           12.4.127             h6a678d5_0  \n[conda] libcublas                 12.4.5.8             h99ab3db_1  \n[conda] libcublas-dev             12.4.5.8             h99ab3db_1  \n[conda] libcufft                  11.2.1.3             h99ab3db_1  \n[conda] libcufft-dev              11.2.1.3             h99ab3db_1  \n[conda] libcurand                 10.3.5.147           h99ab3db_1  \n[conda] libcurand-dev             10.3.5.147           h99ab3db_1  \n[conda] libcusolver               11.6.1.9             h99ab3db_1  \n[conda] libcusolver-dev           11.6.1.9             h99ab3db_1  \n[conda] libcusparse               12.3.1.170           h99ab3db_1  \n[conda] libcusparse-dev           12.3.1.170           h99ab3db_1  \n[conda] libnvjitlink              12.4.127             h99ab3db_1  \n[conda] libnvjitlink-dev          12.4.127             h99ab3db_1  \n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\n```\n\nI have 8 cards, and I want to do both data-parallel and tensor-parallel. Specifically, I want to split the model into two cards and run 4 processes. So in my understanding, it will be 4 data parallel, and each process use two cards. I should use torchrun --nproc-per-node=4 and set tensor_parallel_size=2\n\nHowever, following [torchrun_example.py](https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/torchrun_example.py), I should set nproc-per-node match tensor_parallel_size. I am confused. How should I set the parameters? \n\nBesides, when I use torchrun --nproc-per-node=4 and set tensor_parallel_size=2, it only uses the first 4 GPUs. Moreover, it will hang and reach to Watchdog caught collective operation timeout: WorkNCCL(SeqNum=980, OpType=_ALLGATHER_BASE, NumelIn=10237440, NumelOut=20474880, Timeout(ms)=600000) ran for 600045 milliseconds before timing out. Any help will be appreciated.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-28T03:38:33+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15672/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15672"
  },
  {
    "number": 8150,
    "title": "[Feature]: performance optimization by nanoflow",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\noptimize pipeline design\r\nhttps://github.com/efeslab/Nanoflow\r\nhttps://arxiv.org/abs/2408.12757\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-04T09:06:03+00:00",
    "closed_at": "2025-01-04T01:58:21+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8150/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8150"
  },
  {
    "number": 6785,
    "title": "[Feature]: Evaluate multiple ngram speculations in speculative decoding",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\n During the ngram-spec-decode stage, I've always had a question: In RAG, there isn't just one document relevant to the answer; why don't we first let the large model generate 3 tokens, and then take all possible results in the N-gram?\r\n\r\nIn simpler terms, imagine you're looking for an object in several rooms but can only carry three things at once. You might want to pick up some important items now so you won't forget them when carrying more stuff later. This way, you make sure your search is efficient and effective.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-25T12:35:32+00:00",
    "closed_at": "2024-11-24T02:07:16+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6785/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6785"
  },
  {
    "number": 1543,
    "title": "Large `max_tokens` causes vLLM server to throw AsyncEngineDeadError and the server doesn't recover from the error",
    "body": "Hi vLLM team, it appears that a large sequence length (for instance, `max_tokens` being set to 10000) can cause the vLLM server to throw an `AsyncEngineDeadError` error. After the error, the server doesn't recover and becomes unable to handle future requests. Could you help take a look at this issue? Thank you so much.\r\n\r\nSpecifications for reproducing the error are:\r\n- Sampling parameters: `SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=0.01, top_p=0.7, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=True, max_tokens=10000, logprobs=None, skip_special_tokens=True)`\r\n- Model: LLaMA 2 70b\r\n- vLLM version: 0.2.0\r\n- GPU: 8 L4 (24G) GPUs\r\n- Launch parameters (other parameters are unset)\r\n  - --tensor-parallel-size=8\r\n  - --swap-space=16\r\n  - --gpu-memory-utilization=0.9\r\n\r\nError logs are attached below:\r\nP1\r\n<img width=\"1296\" alt=\"Screenshot 2023-11-01 at 3 36 24\u202fPM\" src=\"https://github.com/vllm-project/vllm/assets/143133934/8466289a-5d3c-49bd-a697-7142998782c3\">\r\nP2\r\n<img width=\"1192\" alt=\"Screenshot 2023-11-01 at 3 37 04\u202fPM\" src=\"https://github.com/vllm-project/vllm/assets/143133934/290534b6-8433-43c8-bdf3-69054decbc93\">\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-11-01T22:55:18+00:00",
    "closed_at": "2024-03-13T12:42:02+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1543/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/1543"
  }
]
[
  {
    "number": 13306,
    "title": "[Feature]: Support for RTX 5090 (CUDA 12.8)",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently only nightlies from torch targeting 12.8 support blackwell such as the rtx 5090.\nI tried using VLLM with a rtx 5090 and no dice. Vanilla vllm installation ends in:\n```\nNVIDIA GeForce RTX 5090 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\nThe current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_70 sm_75 sm_80 sm_86 sm_90.\nIf you want to use the NVIDIA GeForce RTX 5090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\nRuntimeError: CUDA error: no kernel image is available for execution on the device\n```\nThanks\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-02-14T20:41:08+00:00",
    "closed_at": "2025-06-26T17:16:55+00:00",
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13306/reactions",
      "total_count": 16,
      "+1": 16,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13306"
  },
  {
    "number": 16901,
    "title": "[Bug]: RuntimeError on RTX 5090: \"no kernel image is available for execution on the device",
    "body": "### Your current environment\n\n### Describe the bug\n\nWhen running \n\n[vLLM.log](https://github.com/user-attachments/files/19829093/vLLM.log)\n\n with a NVIDIA RTX 5090 GPU, I encountered the following error:\n\nRuntimeError: CUDA error: no kernel image is available for execution on the device\n\nFrom the logs, it seems that PyTorch does not support the compute capability of the RTX 5090 (sm_120):\n\n### To Reproduce\n\n1. Use RTX 5090 GPU\n2. Install vLLM with Docker or system Python environment\n3. Launch the vLLM OpenAI API server\n4. Engine fails to start due to CUDA kernel compatibility issue\n\n### Environment\n\n- **GPU**: NVIDIA GeForce RTX 5090\n- **CUDA Driver Version**: 12.8\n- **CUDA Toolkit**: 12.8.93\n- **NVIDIA Driver**: 570.124.06\n- **PyTorch Version**: 2.x (installed via pip)\n- **vLLM Version**: Latest (from PyPI)\n- **Python Version**: 3.10\n- **OS**: Ubuntu 22.04\n\n### Additional Context\n\nIt seems that the RTX 5090 uses a new compute capability (`sm_120`), which is currently not supported in the stable PyTorch build I'm using.\n\nIs there a recommended way to run vLLM with this GPU? Should I:\n\n- Switch to a nightly PyTorch build that supports sm_120?\n- Build PyTorch from source with `TORCH_CUDA_ARCH_LIST=\"12.0\"`?\n- Wait for official support from PyTorch?\n\nAny guidance or workaround would be greatly appreciated. Thanks!\n\n### How you are installing vllm\n\n```sh\npip install -vvv vllm\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2025-04-21T04:26:48+00:00",
    "closed_at": "2025-06-26T17:16:10+00:00",
    "comments": 28,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16901/reactions",
      "total_count": 8,
      "+1": 8,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16901"
  },
  {
    "number": 19854,
    "title": "[RFC]: KV cache offloading",
    "body": "### Motivation.\n\nCurrently, in vLLM v1 there is no in-house solution for offloading KV cache data from the GPU memory to other medium (in particular, CPU memory).\nThere is a proposed RFC (#16144) and respective PRs (#13377 and #17653) that try to address that.\nThe approach they take is somewhat similar to the way offloading was implemented in V0:\n1. On the scheduler side, extend the core GPU allocator (KVCacheManager) to support CPU offloading\n2. On the worker side, add a synchronous call to handle the actual CPU<->GPU transfer in the `execute_model` function.\n\nIn this RFC I propose an alternative approach which supports the following requirements:\n* **Async saving** of new KV data from GPU to cache. GPU memory will not be freed until save is completed (similar to NixlConnector)\n* **Async loading** of KV data from the cache to GPU. Requests waiting for cache load won't be scheduled until load is completed (similar to NixlConnector)\n* Support **pluggable backends** for cache (CPU backend for start)\n* Allow pulling of **cache events** (cache insert, evict, access), in the same way as the GPU cache. This will allow a unified KVCacheEvent stream.\n* Enable **LRU eviction** of offloaded data\n\n\n### Proposed Change.\n\n![Image](https://github.com/user-attachments/assets/00cc58a5-d18a-4a9a-958f-6f595c1748f0)\n\nI suggest we will enable offloading using a new OffloadingConnector, with minimal changes to vLLM's core.\n\nOn the scheduler side, this connector will delegate to an abstract `OffloadingManager`.\nThe `OffloadingManager` will be responsible for bookkeeping allocation and evictions of offloaded data. Its output will be encoded over `KVConnectorMetadata` and sent to workers.\n\nOn the worker side, we will also have an OffloadingConnector which parses load/store requests sent by the `OffloadingManager` and executed asynchronously using a set of worker threads, one per transfer type (e.g. 1 thread doing GPU->CPU, 1 for CPU->GPU, and in the future also 1 CPU->Disk, etc.).\n\nEach transfer request submitted by the `OffloadingManager` will be responded by a unique `job_id`, which will be used to track completions.\n\nTo enable this design, we need 3 changes in vLLM's core:\n1. PR #19555 Introduce connector-metadata also in the direction of worker->scheduler (currently, only the scheduler connector can pass-on metadata to workers, but not the other way around).\n2. PR #19728 Introduce `Request.block_hashes` to allow the `OffloadingConnector` to re-use the block-hashes computed by the KVCacheManager.\n3. PR #19737 Add a connector API for collecting KV cache events (cache insertion, deletion).\n\nAside from the above PRs with changes to vLLM's core, I already opened PR #19848 including the basis of pluggable offloading implementation.\nOn-top of this PR, there will be PRs for a concrete CPU offloading implementation (probably one PR for the scheduler side, and one PR for the worker side).\nThe last step will be to introduce the actual `OffloadingConnector` that will enable the e2e use of offloading in v1.\n\n### Feedback Period.\n\nOne week.\n\n### CC List.\n\n@WoosukKwon @simon-mo  @robertgshaw2-redhat @njhill\n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "open",
    "created_at": "2025-06-19T10:52:43+00:00",
    "closed_at": null,
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19854/reactions",
      "total_count": 7,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 4,
      "rocket": 3,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19854"
  },
  {
    "number": 8219,
    "title": "[Bug]: vLLM v0.6.1 Instability issue under load.",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-25-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 1: NVIDIA H100 80GB HBM3\r\nGPU 2: NVIDIA H100 80GB HBM3\r\nGPU 3: NVIDIA H100 80GB HBM3\r\n\r\nNvidia driver version: 535.86.10\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   52 bits physical, 57 bits virtual\r\nCPU(s):                          96\r\nOn-line CPU(s) list:             0-95\r\nThread(s) per core:              1\r\nCore(s) per socket:              48\r\nSocket(s):                       2\r\nNUMA node(s):                    8\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           143\r\nModel name:                      Intel(R) Xeon(R) Platinum 8468\r\nStepping:                        8\r\nCPU MHz:                         2100.000\r\nCPU max MHz:                     2100.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        4200.00\r\nL1d cache:                       4.5 MiB\r\nL1i cache:                       3 MiB\r\nL2 cache:                        192 MiB\r\nL3 cache:                        210 MiB\r\nNUMA node0 CPU(s):               0-11\r\nNUMA node1 CPU(s):               12-23\r\nNUMA node2 CPU(s):               24-35\r\nNUMA node3 CPU(s):               36-47\r\nNUMA node4 CPU(s):               48-59\r\nNUMA node5 CPU(s):               60-71\r\nNUMA node6 CPU(s):               72-83\r\nNUMA node7 CPU(s):               84-95\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr avx512_fp16 flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.6+cu121torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.68\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.2\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.0@\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV18    NV18    NV18    PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-11    0               N/A\r\nGPU1    NV18     X      NV18    NV18    SYS     PIX     PIX     PXB     SYS     SYS     SYS     SYS     24-35   2               N/A\r\nGPU2    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB     72-83   6               N/A\r\nGPU3    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     72-83   6               N/A\r\nNIC0    PXB     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC1    SYS     PIX     SYS     SYS     SYS      X      PIX     PXB     SYS     SYS     SYS     SYS\r\nNIC2    SYS     PIX     SYS     SYS     SYS     PIX      X      PXB     SYS     SYS     SYS     SYS\r\nNIC3    SYS     PXB     SYS     SYS     SYS     PXB     PXB      X      SYS     SYS     SYS     SYS\r\nNIC4    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     PXB     SYS\r\nNIC5    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      PXB     SYS\r\nNIC6    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB     PXB      X      SYS\r\nNIC7    SYS     SYS     PXB     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n\r\n```\r\n\r\n</details>\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI did a load test on vLLM v0.6.0 based on the conversation history. \r\nI've run the test about 3 times, and I've always encountered the issue. I'm not sure if this issue appears after GPU cache usage reaches 100%, but so far it's been reproduced during load after GPU cache usage reaches 100%.\r\n\r\n```\r\nERROR 09-05 17:22:27 async_llm_engine.py:63] Engine background task failed\r\nERROR 09-05 17:22:27 async_llm_engine.py:63] Traceback (most recent call last):\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 53, in _log_task_completion\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]     return_value = task.result()\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 939, in run_engine_loop\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]     result = task.result()\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 868, in engine_step\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]     request_outputs = await self.engine.step_async(virtual_engine)\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 345, in step_async\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]     output = await self.model_executor.execute_model_async(\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 177, in execute_model_async\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]     return await self._driver_execute_model_async(execute_model_req)\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 231, in _driver_execute_model_async\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]     return await self.driver_exec_model(execute_model_req)\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]     result = self.fn(*self.args, **self.kwargs)\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 303, in execute_model\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]     inputs = self.prepare_input(execute_model_req)\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 291, in prepare_input\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]     return self._get_driver_input_and_broadcast(execute_model_req)\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 253, in _get_driver_input_and_broadcast\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]     self.model_runner.prepare_model_input(\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1380, in prepare_model_input\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]     model_input = self._prepare_model_input_tensors(\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1042, in _prepare_model_input_tensors\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]     return builder.build()  # type: ignore\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 698, in build\r\nERROR 09-05 17:22:27 async_llm_engine.py:63]     max(inter_data.seq_lens))\r\nERROR 09-05 17:22:27 async_llm_engine.py:63] ValueError: max() arg is an empty sequence\r\nERROR:asyncio:Exception in callback functools.partial(<function _log_task_completion at 0x7f204842b640>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7f20447a8970>>)\r\nhandle: <Handle functools.partial(<function _log_task_completion at 0x7f204842b640>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7f20447a8970>>)>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 53, in _log_task_completion\r\n    return_value = task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 939, in run_engine_loop\r\n    result = task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 868, in engine_step\r\n    request_outputs = await self.engine.step_async(virtual_engine)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 345, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 177, in execute_model_async\r\n    return await self._driver_execute_model_async(execute_model_req)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 231, in _driver_execute_model_async\r\n    return await self.driver_exec_model(execute_model_req)\r\n  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 303, in execute_model\r\n    inputs = self.prepare_input(execute_model_req)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 291, in prepare_input\r\n    return self._get_driver_input_and_broadcast(execute_model_req)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 253, in _get_driver_input_and_broadcast\r\n    self.model_runner.prepare_model_input(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1380, in prepare_model_input\r\n    model_input = self._prepare_model_input_tensors(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1042, in _prepare_model_input_tensors\r\n    return builder.build()  # type: ignore\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 698, in build\r\n    max(inter_data.seq_lens))\r\nValueError: max() arg is an empty sequence\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 65, in _log_task_completion\r\n    raise AsyncEngineDeadError(\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for the actual cause.\r\n\r\n(...)\r\n\r\nERROR 09-05 17:22:27 client.py:412] Traceback (most recent call last):\r\nERROR 09-05 17:22:27 client.py:412]   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/client.py\", line 409, in generate\r\nERROR 09-05 17:22:27 client.py:412]     await self.check_health(socket=socket)\r\nERROR 09-05 17:22:27 client.py:412]   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/client.py\", line 429, in check_health\r\nERROR 09-05 17:22:27 client.py:412]     await self._send_one_way_rpc_request(\r\nERROR 09-05 17:22:27 client.py:412]   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/client.py\", line 267, in _send_one_way_rpc_request\r\nERROR 09-05 17:22:27 client.py:412]     raise response\r\n\r\n(...)\r\n\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 257, in __call__\r\n    await wrap(partial(self.listen_for_disconnect, receive))\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 253, in wrap\r\n    await func()\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 230, in listen_for_disconnect\r\n    message = await receive()\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 555, in receive\r\n    await self.message_event.wait()\r\n  File \"/usr/lib/python3.10/asyncio/locks.py\", line 214, in wait\r\n    await fut\r\nasyncio.exceptions.CancelledError: Cancelled by cancel scope 7fb1397c3c10\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 401, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 113, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 715, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 735, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 288, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 76, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 74, in app\r\n    await response(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 250, in __call__\r\n    async with anyio.create_task_group() as task_group:\r\n  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 680, in __aexit__\r\n    raise BaseExceptionGroup(\r\nexceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 257, in __call__\r\n    await wrap(partial(self.listen_for_disconnect, receive))\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 253, in wrap\r\n    await func()\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 230, in listen_for_disconnect\r\n    message = await receive()\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 555, in receive\r\n    await self.message_event.wait()\r\n  File \"/usr/lib/python3.10/asyncio/locks.py\", line 214, in wait\r\n    await fut\r\nasyncio.exceptions.CancelledError: Cancelled by cancel scope 7fb13b480a00\r\n\r\n\r\n```\r\n\r\nThe average number of input prompt tokens in the conversation history we used for testing was 1,600 tokens, and the average number of answer tokens from the model was 150 tokens.\r\n\r\nThe following are the vLLM startup arguments.\r\n```\r\n    - args:\r\n      - --model\r\n      - /data/models/llama-65b-instruct/base\r\n      - --tensor-parallel-size\r\n      - \"4\"\r\n      - --load-format\r\n      - \"auto\"\r\n      - --block-size\r\n      - \"32\"\r\n      - --max-seq-len-to-capture\r\n      - \"8192\"\r\n      - --max-model-len\r\n      - \"8192\"\r\n      - --disable-log-requests\r\n      - --uvicorn-log-level\r\n      - \"warning\"\r\n      - --gpu-memory-utilization\r\n      - \"0.95\"\r\n```\r\n\r\nThis error did not occur in versions prior to v0.5.5. (I did load tests 3 times with exactly same arguments.)\r\n\r\nDuring a load, I also got this warning message 5 minutes before the system died. Could this be related to the issue?\r\n\r\nWARNING 09-05 17:10:02 scheduler.py:1355] Sequence group cmpl-0c49e59124fe4f9c8b8e6e0f4bae49d7-0 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\r\n\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-09-06T00:36:46+00:00",
    "closed_at": "2024-09-13T14:58:53+00:00",
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8219/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8219"
  },
  {
    "number": 1237,
    "title": "Data parallel inference",
    "body": "Is there a recommended way to run data parallel inference (i.e. a copy of the model on each GPU)? It's possible by hacking CUDA_VISIBLE_DEVICES, but I was wondering if there's a cleaner method.\r\n```python\r\ndef worker(worker_idx):\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(worker_idx)\r\n    prompts = [\r\n        \"Hello, my name is\",\r\n        \"The president of the United States is\",\r\n        \"The capital of France is\",\r\n        \"The future of AI is\",\r\n    ]\r\n    sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\r\n    llm = LLM(model=\"facebook/opt-125m\")\r\n    outputs = llm.generate(prompts, sampling_params)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    \r\n    with multiprocessing.Pool(4) as pool:\r\n        pool.map(worker, range(4))\r\n```",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2023-09-30T23:24:38+00:00",
    "closed_at": "2024-09-13T17:00:25+00:00",
    "comments": 29,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1237/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1237"
  },
  {
    "number": 180,
    "title": "Whisper support",
    "body": "Is support for Whisper on the roadmap? Something like https://github.com/ggerganov/whisper.cpp would be great.",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2023-06-21T07:06:07+00:00",
    "closed_at": "2025-01-03T08:39:21+00:00",
    "comments": 40,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/180/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/180"
  },
  {
    "number": 16626,
    "title": "[Bug]: Multi-modal inference too slow",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 04-15 02:56:47 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.6 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: version 3.16.3\nLibc version: glibc-2.31\n\nPython version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-1086-azure-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: 12.6.85\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA A100-SXM4-80GB\nGPU 1: NVIDIA A100-SXM4-80GB\nGPU 2: NVIDIA A100-SXM4-80GB\nGPU 3: NVIDIA A100-SXM4-80GB\nGPU 4: NVIDIA A100-SXM4-80GB\nGPU 5: NVIDIA A100-SXM4-80GB\nGPU 6: NVIDIA A100-SXM4-80GB\nGPU 7: NVIDIA A100-SXM4-80GB\n\nNvidia driver version: 550.90.07\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nByte Order:                           Little Endian\nAddress sizes:                        48 bits physical, 48 bits virtual\nCPU(s):                               96\nOn-line CPU(s) list:                  0-95\nThread(s) per core:                   1\nCore(s) per socket:                   48\nSocket(s):                            2\nNUMA node(s):                         4\nVendor ID:                            AuthenticAMD\nCPU family:                           23\nModel:                                49\nModel name:                           AMD EPYC 7V12 64-Core Processor\nStepping:                             0\nCPU MHz:                              3108.517\nBogoMIPS:                             4890.88\nHypervisor vendor:                    Microsoft\nVirtualization type:                  full\nL1d cache:                            3 MiB\nL1i cache:                            3 MiB\nL2 cache:                             48 MiB\nL3 cache:                             384 MiB\nNUMA node0 CPU(s):                    0-23\nNUMA node1 CPU(s):                    24-47\nNUMA node2 CPU(s):                    48-71\nNUMA node3 CPU(s):                    72-95\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT disabled\nVulnerability Spec rstack overflow:   Mitigation; safe RET, no microcode\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core ssbd vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr rdpru arat umip rdpid\n\nVersions of relevant libraries:\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.50.0.dev0\n[pip3] triton==3.2.0\n[pip3] zmq==0.0.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] transformers              4.50.0.dev0              pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\n[conda] zmq                       0.0.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n\u001b[4mGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tNIC4\tNIC5\tNIC6\tNIC7\tNIC8\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\nGPU0\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t24-47\t1\t\tN/A\nGPU1\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t24-47\t1\t\tN/A\nGPU2\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tSYS\tSYS\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\t0-23\t0\t\tN/A\nGPU3\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tSYS\tSYS\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\t0-23\t0\t\tN/A\nGPU4\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tSYS\tSYS\tSYS\t72-95\t3\t\tN/A\nGPU5\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tSYS\tSYS\tSYS\t72-95\t3\t\tN/A\nGPU6\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tSYS\t48-71\t2\t\tN/A\nGPU7\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tSYS\t48-71\t2\t\tN/A\nNIC0\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t\t\t\t\nNIC1\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\t X \tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t\t\t\t\nNIC2\tSYS\tSYS\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\t\t\t\t\nNIC3\tSYS\tSYS\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\t X \tSYS\tSYS\tSYS\tSYS\tNODE\t\t\t\t\nNIC4\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tNODE\tSYS\tSYS\tSYS\t\t\t\t\nNIC5\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\t X \tSYS\tSYS\tSYS\t\t\t\t\nNIC6\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tNODE\tSYS\t\t\t\t\nNIC7\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\t X \tSYS\t\t\t\t\nNIC8\tSYS\tSYS\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t X \t\t\t\t\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n\nLD_LIBRARY_PATH=:/usr/local/cuda/lib64\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n### \ud83d\udc1b Describe the bug\n\nI am noticing an issue when using Gemma-3-27B (and other multimodal models like NVLM-D-72B as well). I am running with TP=4 with 4 A100 GPUs in a single node using NVLink.\n\nI have an inference request that has around 30 images.\n\nI notice that it takes a long time (about 1.5 mins) to complete. However during this time, mostly the GPUs are idle. The main thing that is happening is that the host DRAM usage is slowly increasing. I enabled vLLM tracing to figure out what was going on and there is a large window of time (about 30s) where there is no function call and the code is waiting on acquire_read in shm_broadcast.py. I am attaching the relevant code snippets below.\n\n\n![Image](https://github.com/user-attachments/assets/8bfaabca-ee0f-428d-ac8f-47f788bb3d25)\n![Image](https://github.com/user-attachments/assets/ee83ab79-6b55-4ebc-a51d-85d1c407971d)\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-04-15T02:58:23+00:00",
    "closed_at": null,
    "comments": 25,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16626/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16626"
  },
  {
    "number": 6473,
    "title": "[Bug]: [vllm-openvino]: ValueError: `use_cache` was set to `True` but the loaded model only supports `use_cache=False`. ",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n\r\n(vllm-openvino) yongshuai_wang@cpu-10-48-1-249:~/models$ python collect_env.py \r\nCollecting environment information...\r\nWARNING 07-16 19:50:52 _custom_ops.py:14] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\r\n/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/vllm/usage/usage_lib.py:19: RuntimeWarning: Failed to read commit hash:\r\nNo module named 'vllm.commit_id'\r\n  from vllm.version import __version__ as VLLM_VERSION\r\nPyTorch version: 2.3.1+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-94-generic-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             128\r\nOn-line CPU(s) list:                0-127\r\nVendor ID:                          GenuineIntel\r\nModel name:                         INTEL(R) XEON(R) GOLD 6530\r\nCPU family:                         6\r\nModel:                              207\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 32\r\nSocket(s):                          2\r\nStepping:                           2\r\nFrequency boost:                    enabled\r\nCPU max MHz:                        2101.0000\r\nCPU min MHz:                        800.0000\r\nBogoMIPS:                           4200.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          3 MiB (64 instances)\r\nL1i cache:                          2 MiB (64 instances)\r\nL2 cache:                           128 MiB (64 instances)\r\nL3 cache:                           320 MiB (2 instances)\r\nNUMA node(s):                       4\r\nNUMA node0 CPU(s):                  0-15,64-79\r\nNUMA node1 CPU(s):                  16-31,80-95\r\nNUMA node2 CPU(s):                  32-47,96-111\r\nNUMA node3 CPU(s):                  48-63,112-127\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] onnx==1.16.1\r\n[pip3] torch==2.3.1+cpu\r\n[pip3] transformers==4.42.4\r\n[pip3] triton==3.0.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] torch                     2.3.1+cpu                pypi_0    pypi\r\n[conda] transformers              4.42.4                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\n## 1\u3001bug description\r\nIt is normal to let vllm-openvino convert openvino IR at runtime;\r\nHowever, manually converting the model to openvino IR will result in an error `use_cache` XXXX\r\n\r\n## 2\u3001manualy convert module to OpenVINO IR\uff0cand run ,get error:\r\n### convert commad\r\n```\r\noptimum-cli export openvino -m Qwen1.5-4B-Chat --task text-generation --weight-format int4   Qwen1.5-4B-Chat-optimum-int4\r\n```\r\n\r\n#### convert OpenVION IR logs\r\n```\r\n(vllm-openvino) yongshuai_wang@cpu-10-48-1-249:~/models$ \r\noptimum-cli export openvino \\\r\n    -m Qwen1.5-4B-Chat \\\r\n    --task text-generation \\\r\n    --weight-format int4   \\\r\n    Qwen1.5-4B-Chat-optimum-int4\r\nFramework not specified. Using pt to export the model.\r\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.33it/s]\r\nThe task `text-generation` was manually specified, and past key values will not be reused in the decoding. if needed, please pass `--task text-generation-with-past` to export using the past key values.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nUsing framework PyTorch: 2.3.1+cpu\r\nOverriding 1 configuration item(s)\r\n        - use_cache -> False\r\n/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:1116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  if sequence_length != 1:\r\n/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:128: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  if seq_len > self.max_seq_len_cached:\r\n['input_ids', 'attention_mask', 'position_ids']\r\nMixed-Precision assignment \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501   9% 2\r\nMixed-Precision assignment \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 280/280 \u2022 0:00:46 \u2022 0:00:00\r\nINFO:nncf:Statistics of the bitwidth distribution:\r\n\u250d\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2511\r\n\u2502   Num bits (N) \u2502 % all parameters (layers)   \u2502 % ratio-defining parameters (layers)   \u2502\r\n\u251d\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2525\r\n\u2502              8 \u2502 36% (77 / 282)              \u2502 20% (75 / 280)                         \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502              4 \u2502 64% (205 / 282)             \u2502 80% (205 / 280)                        \u2502\r\n\u2515\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2519\r\nApplying Weight Compression \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 282/282 \u2022 0:01:36 \u2022 0:00:00\r\nReplacing `(?!\\S)` pattern to `(?:$|[^\\S])` in RegexSplit operation\r\n```\r\n\r\n### run command\r\nuse manuly convert model : /home/yongshuai_wang/models/Qwen1.5-4B-Chat-optimum-int4 \r\n```\r\nVLLM_OPENVINO_KVCACHE_SPACE=30 \\\r\nLLM_OPENVINO_CPU_KV_CACHE_PRECISION=u8 \\\r\nVLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS=ON \\\r\n        python3 -m vllm.entrypoints.openai.api_server \\\r\n                --model /home/yongshuai_wang/models/Qwen1.5-4B-Chat-optimum-int4 \\\r\n                --port 10003\r\n```\r\n\r\n#### get use_cache error\r\n```\r\n(vllm-openvino) yongshuai_wang@cpu-10-48-1-249:~/models$\r\nVLLM_OPENVINO_KVCACHE_SPACE=30 \\\r\nLLM_OPENVINO_CPU_KV_CACHE_PRECISION=u8 \\\r\nVLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS=ON \\\r\n        python3 -m vllm.entrypoints.openai.api_server \\\r\n                --model /home/yongshuai_wang/models/Qwen1.5-4B-Chat-optimum-int4 \\\r\n                --port 10003\r\nWARNING 07-16 19:48:08 _custom_ops.py:14] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\r\n/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/vllm/usage/usage_lib.py:19: RuntimeWarning: Failed to read commit hash:\r\nNo module named 'vllm.commit_id'\r\n  from vllm.version import __version__ as VLLM_VERSION\r\nINFO 07-16 19:48:11 api_server.py:212] vLLM API server version 0.5.2\r\nINFO 07-16 19:48:11 api_server.py:213] args: Namespace(host=None, port=10003, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='/home/yongshuai_wang/models/Qwen1.5-4B-Chat-optimum-int4', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, model_loader_extra_config=None, preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\nINFO 07-16 19:48:11 config.py:1374] Downcasting torch.float32 to torch.float16.\r\nINFO 07-16 19:48:11 llm_engine.py:174] Initializing an LLM engine (v0.5.2) with config: model='/home/yongshuai_wang/models/Qwen1.5-4B-Chat-optimum-int4', speculative_config=None, tokenizer='/home/yongshuai_wang/models/Qwen1.5-4B-Chat-optimum-int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/home/yongshuai_wang/models/Qwen1.5-4B-Chat-optimum-int4, use_v2_block_manager=False, enable_prefix_caching=False)\r\nWARNING 07-16 19:48:11 openvino_executor.py:132] Only float32 dtype is supported on OpenVINO, casting from torch.float16.\r\nWARNING 07-16 19:48:11 openvino_executor.py:137] CUDA graph is not supported on OpenVINO backend, fallback to the eager mode.\r\nINFO 07-16 19:48:11 openvino_executor.py:159] OpenVINO optimal block size is 32, overriding currently set 16\r\nINFO 07-16 19:48:14 selector.py:121] Cannot use _Backend.FLASH_ATTN backend on OpenVINO.\r\nINFO 07-16 19:48:14 selector.py:69] Using OpenVINO Attention backend.\r\nWARNING 07-16 19:48:14 openvino.py:130] OpenVINO IR is available for provided model id /home/yongshuai_wang/models/Qwen1.5-4B-Chat-optimum-int4. This IR will be used for inference as-is, all possible options that may affect model conversion are ignored.\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n[rank0]:     return _run_code(code, main_globals, None,\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/runpy.py\", line 86, in _run_code\r\n[rank0]:     exec(code, run_globals)\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 282, in <module>\r\n[rank0]:     run_server(args)\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 224, in run_server\r\n[rank0]:     if llm_engine is not None else AsyncLLMEngine.from_engine_args(\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 444, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 373, in __init__\r\n[rank0]:     self.engine = self._init_engine(*args, **kwargs)\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 520, in _init_engine\r\n[rank0]:     return engine_class(*args, **kwargs)\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 249, in __init__\r\n[rank0]:     self.model_executor = executor_class(\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 150, in __init__\r\n[rank0]:     super().__init__(model_config, cache_config, parallel_config,\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 46, in __init__\r\n[rank0]:     self._init_executor()\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/vllm/executor/openvino_executor.py\", line 28, in _init_executor\r\n[rank0]:     self._init_worker()\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/vllm/executor/openvino_executor.py\", line 55, in _init_worker\r\n[rank0]:     self.driver_worker.load_model()\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/vllm/worker/openvino_worker.py\", line 199, in load_model\r\n[rank0]:     self.model_runner.load_model()\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/vllm/worker/openvino_model_runner.py\", line 91, in load_model\r\n[rank0]:     self.model = get_model(\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/vllm/model_executor/model_loader/openvino.py\", line 210, in get_model\r\n[rank0]:     return OpenVINOCasualLM(model_config, device_config, kv_cache_dtype)\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/vllm/model_executor/model_loader/openvino.py\", line 137, in __init__\r\n[rank0]:     pt_model = OVModelForCausalLM.from_pretrained(\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/optimum/modeling_base.py\", line 427, in from_pretrained\r\n[rank0]:     return from_pretrained_method(\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/optimum/intel/openvino/modeling_decoder.py\", line 796, in _from_pretrained\r\n[rank0]:     causal_model = init_cls(\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/optimum/intel/openvino/modeling_decoder.py\", line 171, in __init__\r\n[rank0]:     raise_error(self.use_cache, use_cache, \"use_cache\")\r\n[rank0]:   File \"/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/optimum/intel/openvino/modeling_decoder.py\", line 159, in raise_error\r\n[rank0]:     raise ValueError(\r\n[rank0]: ValueError: `use_cache` was set to `True` but the loaded model only supports `use_cache=False`. Please load your current model with `use_cache=False` or export the original model once again with `use_cache=True` when calling the `from_pretrained` method. To export your model, simply set `export=True`.\r\n```\r\n\r\n## 3\u3001however, directly run vllm openvion with original modle Qwen1.5-4B-Chat \uff0c is OK:\r\n\r\n### run log\r\n```\r\n(vllm-openvino) yongshuai_wang@cpu-10-48-1-249:~/models/Qwen1.5-4B-Chat$ \r\nVLLM_OPENVINO_KVCACHE_SPACE=30 \\\r\nLLM_OPENVINO_CPU_KV_CACHE_PRECISION=u8 \\\r\nVLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS=ON \\\r\n        python3 -m vllm.entrypoints.openai.api_server \\\r\n                --model /home/yongshuai_wang/models/Qwen1.5-4B-Chat \\\r\n                --port 10003\r\nWARNING 07-16 19:33:14 _custom_ops.py:14] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\r\n/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/vllm/usage/usage_lib.py:19: RuntimeWarning: Failed to read commit hash:\r\nNo module named 'vllm.commit_id'\r\n  from vllm.version import __version__ as VLLM_VERSION\r\nINFO 07-16 19:33:17 api_server.py:212] vLLM API server version 0.5.2\r\nINFO 07-16 19:33:17 api_server.py:213] args: Namespace(host=None, port=10003, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='/home/yongshuai_wang/models/Qwen1.5-4B-Chat', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, model_loader_extra_config=None, preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\nINFO 07-16 19:33:17 llm_engine.py:174] Initializing an LLM engine (v0.5.2) with config: model='/home/yongshuai_wang/models/Qwen1.5-4B-Chat', speculative_config=None, tokenizer='/home/yongshuai_wang/models/Qwen1.5-4B-Chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/home/yongshuai_wang/models/Qwen1.5-4B-Chat, use_v2_block_manager=False, enable_prefix_caching=False)\r\nWARNING 07-16 19:33:17 openvino_executor.py:132] Only float32 dtype is supported on OpenVINO, casting from torch.bfloat16.\r\nWARNING 07-16 19:33:17 openvino_executor.py:137] CUDA graph is not supported on OpenVINO backend, fallback to the eager mode.\r\nINFO 07-16 19:33:17 openvino_executor.py:159] OpenVINO optimal block size is 32, overriding currently set 16\r\nINFO 07-16 19:33:19 selector.py:121] Cannot use _Backend.FLASH_ATTN backend on OpenVINO.\r\nINFO 07-16 19:33:19 selector.py:69] Using OpenVINO Attention backend.\r\nWARNING 07-16 19:33:20 openvino.py:123] Provided model id /home/yongshuai_wang/models/Qwen1.5-4B-Chat does not contain OpenVINO IR, the model will be converted to IR with default options. If you need to use specific options for model conversion, use optimum-cli export openvino with desired options.\r\nFramework not specified. Using pt to export the model.\r\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.56it/s]\r\nUsing framework PyTorch: 2.3.1+cpu\r\nOverriding 1 configuration item(s)\r\n\t- use_cache -> True\r\nWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\r\n/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:1116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  if sequence_length != 1:\r\n/home/yongshuai_wang/miniconda3/envs/vllm-openvino/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:128: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  if seq_len > self.max_seq_len_cached:\r\n['input_ids', 'attention_mask', 'position_ids', 'past_key_values']\r\nINFO:nncf:Statistics of the bitwidth distribution:\r\n\u250d\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2511\r\n\u2502   Num bits (N) \u2502 % all parameters (layers)   \u2502 % ratio-defining parameters (layers)   \u2502\r\n\u251d\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2525\r\n\u2502              8 \u2502 100% (282 / 282)            \u2502 100% (282 / 282)                       \u2502\r\n\u2515\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2519\r\nApplying Weight Compression \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 282/282 \u2022 0:00:26 \u2022 0:00:00\r\nINFO 07-16 19:34:36 openvino_executor.py:72] # CPU blocks: 2457\r\nINFO 07-16 19:34:47 serving_chat.py:94] Using default chat template:\r\nINFO 07-16 19:34:47 serving_chat.py:94] {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\r\nINFO 07-16 19:34:47 serving_chat.py:94] You are a helpful assistant.<|im_end|>\r\nINFO 07-16 19:34:47 serving_chat.py:94] ' }}{% endif %}{{'<|im_start|>' + message['role'] + '\r\nINFO 07-16 19:34:47 serving_chat.py:94] ' + message['content'] + '<|im_end|>' + '\r\nINFO 07-16 19:34:47 serving_chat.py:94] '}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\r\nINFO 07-16 19:34:47 serving_chat.py:94] ' }}{% endif %}\r\nWARNING 07-16 19:34:48 serving_embedding.py:141] embedding_mode is False. Embedding API will not work.\r\nINFO 07-16 19:34:48 api_server.py:257] Available routes are:\r\nINFO 07-16 19:34:48 api_server.py:262] Route: /openapi.json, Methods: HEAD, GET\r\nINFO 07-16 19:34:48 api_server.py:262] Route: /docs, Methods: HEAD, GET\r\nINFO 07-16 19:34:48 api_server.py:262] Route: /docs/oauth2-redirect, Methods: HEAD, GET\r\nINFO 07-16 19:34:48 api_server.py:262] Route: /redoc, Methods: HEAD, GET\r\nINFO 07-16 19:34:48 api_server.py:262] Route: /health, Methods: GET\r\nINFO 07-16 19:34:48 api_server.py:262] Route: /tokenize, Methods: POST\r\nINFO 07-16 19:34:48 api_server.py:262] Route: /detokenize, Methods: POST\r\nINFO 07-16 19:34:48 api_server.py:262] Route: /v1/models, Methods: GET\r\nINFO 07-16 19:34:48 api_server.py:262] Route: /version, Methods: GET\r\nINFO 07-16 19:34:48 api_server.py:262] Route: /v1/chat/completions, Methods: POST\r\nINFO 07-16 19:34:48 api_server.py:262] Route: /v1/completions, Methods: POST\r\nINFO 07-16 19:34:48 api_server.py:262] Route: /v1/embeddings, Methods: POST\r\nINFO:     Started server process [42639]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://0.0.0.0:10003 (Press CTRL+C to quit)\r\nINFO 07-16 19:34:58 metrics.py:295] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 07-16 19:35:08 metrics.py:295] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\n```\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-07-16T12:05:32+00:00",
    "closed_at": "2024-07-19T02:04:07+00:00",
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6473/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6473"
  },
  {
    "number": 16715,
    "title": "[Installation]: Kimi-VL-A3B failed to be deployed using vllm mirroring",
    "body": "### Your current environment\n\nmodel\uff1aKimi-VL-A3B-Thinking\nimage\uff1avllm-openai\uff1alatest    \nvllm version:0.8.4\n\n1.docker pull vllm/vllm-openai\n\u91cc\u9762\u7684vllm version:0.8.4\n2.docker run --gpus all -v /mnt/data1/LargeLanguageModels/qwen:/model --ipc=host --network=host  --name kimi-vl -it --entrypoint vllm/vllm-openai \uff1alatest  bash\n3. \u5728\u5bb9\u5668\u4e2d\u5982\u4e0b\u547d\u4ee4\u542f\u52a8\u5927\u6a21\u578b\n\n> CUDA_VISIBLE_DEVICES=0 python -m vllm.entrypoints.openai.api_server \\\n\n    --port 3000 \\\n    --served-model-name kimi-vl \\\n    --trust-remote-code \\\n    --model /models/Kimi-VL-A3B-Thinking/Kimi-VL-A3B-Thinking \\\n    --tensor-parallel-size 1 \\\n    --max-num-batched-tokens 131072 \\\n    --max-model-len 131072 \\\n    --max-num-seqs 512 \\\n    --limit-mm-per-prompt image=256 \\\n    --disable-mm-preprocessor-cache\n\n\u51fa\u73b0\u62a5\u9519\n\n> Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/gwm-tmp/kimi_vl/vllm/vllm/entrypoints/openai/api_server.py\", line 1121, in <module>\n    uvloop.run(run_server(args))\n  File \"/gwm-tmp/kimi_vl/venv/lib/python3.10/site-packages/uvloop/__init__.py\", line 82, in run\n    return loop.run_until_complete(wrapper())\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/gwm-tmp/kimi_vl/venv/lib/python3.10/site-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n  File \"/gwm-tmp/kimi_vl/vllm/vllm/entrypoints/openai/api_server.py\", line 1069, in run_server\n    async with build_async_engine_client(args) as engine_client:\n  File \"/usr/local/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/gwm-tmp/kimi_vl/vllm/vllm/entrypoints/openai/api_server.py\", line 146, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/usr/local/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/gwm-tmp/kimi_vl/vllm/vllm/entrypoints/openai/api_server.py\", line 166, in build_async_engine_client_from_engine_args\n    vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n  File \"/gwm-tmp/kimi_vl/vllm/vllm/engine/arg_utils.py\", line 1169, in create_engine_config\n    model_config = self.create_model_config()\n  File \"/gwm-tmp/kimi_vl/vllm/vllm/engine/arg_utils.py\", line 1057, in create_model_config\n    return ModelConfig(\n  File \"/gwm-tmp/kimi_vl/vllm/vllm/config.py\", line 413, in __init__\n    self.multimodal_config = self._init_multimodal_config(\n  File \"/gwm-tmp/kimi_vl/vllm/vllm/config.py\", line 486, in _init_multimodal_config\n    raise ValueError(\"`limit_mm_per_prompt` is only supported for \"\nValueError: `limit_mm_per_prompt` is only supported for multimodal models.\n\n\n4.\u6211\u53bb\u6389\u4e86`limit_mm_per_prompt`\u53c2\u6570\u540e\u7528\u5982\u4e0b\u547d\u4ee4\u591a\u51fa\u5c1d\u8bd5\u5168\u662f\u5982\u4e0b\u62a5\u9519\n\u9996\u5148\u662fvllm\u955c\u50cf\u7f3a\u5c11blobfile\uff0c\u624b\u52a8\u5bfc\u5165\u5b89\u88c5\u89e3\u51b3\n\n> CUDA_VISIBLE_DEVICES=3  python3 -m vllm.entrypoints.openai.api_server --port 8888 --served-model-name kimi-vl --trust-remote-code --model moonshotai/Kimi-VL-A3B-Instruct --tensor-parallel-size 1 --max-num-batched-tokens 131072 --max-model-len 131072 --max-num-seqs 512  \n\n> VLLM_USE_V1_0 CUDA_VISIBLE_DEVICES=3  python3 -m vllm.entrypoints.openai.api_server --port 8888 --served-model-name kimi-vl --trust-remote-code --model moonshotai/Kimi-VL-A3B-Instruct --tensor-parallel-size 1 --max-num-batched-tokens 131072 --max-model-len 131072 --max-num-seqs 512\n\n> VLLM_USE_V1_0 CUDA_VISIBLE_DEVICES=3  python3 -m vllm.entrypoints.openai.api_server --port 8888 --served-model-name kimi-vl --trust-remote-code --model moonshotai/Kimi-VL-A3B-Instruct --tensor-parallel-size 1 \n\n\n> CUDA_VISIBLE_DEVICES=3  python3 -m vllm.entrypoints.openai.api_server --port 8888 --served-model-name kimi-vl --trust-remote-code --model moonshotai/Kimi-VL-A3B-Instruct --tensor-parallel-size 1 \n\n\u62a5\u9519\u5982\u4e0b\n> ERROR 04-16 02:31:40[engine.py:448] Traceback (most recent call last):\nERROR 04-16 02:31:40[engine.py:448]File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/mult iprocess ing/engine .py\", line 436, in run_mp_engine\nERROR 04-16 02:31:40[engine.py:448]engine = MQLLMEngine,from_yllm_config(\nERROR 04-16 02:31:40[engine.py:448]^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-16 02:31:40[engine.py:448]File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/mult iprocess ing/engine .py\", line 128, in from_vllm_config\nERROR 04-16 02:31:40[engine.py:448] return cls(^^^ ERROR 04-16 02:31:40[engine.py:448]\nERROR 04-16 02:31:40[engine.py:448]3File\"/usr/local/lib/python3.12/dist-packages/vllm/engine/mult iprocess ing/engine.py\",line 82, in __init_ ERROR 04-16 02:31:40 ERROR 04-16 02:31:40[engine.py:448[engine.py:448] self,engine = LLMEngine(*args, **kwargs)\n2025-04-16 17:33215000-0813815712-0014101E8791\nERROR 04-16 02:31:40[engine.py:448]File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/lln_engine .py\", line 282, in _init_\nERROR 04-16 02:31:40 ERROR 04-16 02:31:40[engine.py:448][engine.py:448] self.model_executor = executor_class(vllm config=viim canfia.)\nERROR 04-16 02:31:40 ERROR 04-16 02:31:40[engine.py:448][engine.py:448] self._init_executor( ) File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 52, in _init_\nERROR 04-16 02:31:40[engine.py:448]File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\nERROR 04-16 02:31:40[engine.py:448]answerself.collective_rpc(\"load_model\" )File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor .py\", line 56, in collective_rpc\nERROR 04-16 02:31:40[engine.py:448]\nERROR 04-16 02:31:40[engine.py:448]run method(self.driver_worker, method, args,kwargs )\nERROR 04-16 02:31:40[engine.py:448]\nERROR 04-16 02:31:40[engine.py:448]File \"/usr/local/lib/python3.12/dist-packages/vllm/utiis.py . line 2378, in run_method\nERROR 04-16 02:31:40[engine.py:448]return func(*args,**kwargs)\nERROR 04-16 02:31:40[engine.py:448\nERROR 04-16 02:31:40[engine.py:448]File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 183, in load_model\nERROR 04-16 02:31:40[engine.py:448self.model_runner.load model()\nERROR 04-16 02:31:40[engine.py:448]File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/mddel_runner.py\", line 1113, in load_model\nERROR 04-16 02:31:40[engine.py:448]self.model = get_model(vllm_config=solf.vllm_config)\nERROR 04-16 02:31:40ERROR 04-16 02:31:40ERROR 04-16 02:31:40[engine,py:448][engine.py:448[engine.py:448return loader.load model(vilm config=vllm config)File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/_init_.py\". line 14, in get_model\n843315712-001410188791\n-04-16 17:33\nERROR 04-16 02:31:40ERROR 04-16 02:31:40[engine.py:448[engine.py:448\u300cengine.py:448File \"/usr/local/lib/python3. 12/dist-packages/vllm/model_executor/model_loader/loader.py\". line 452. in load_model\nERROR 04-16 02:31:40model=initiallize model(yllm config=vlim config)\nERROR 04-16 02:31:40[engine,py:448\nERROR 04-16 02:31:40[engine.py:448File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 123, in _initialize_model\nERROR 04-16 02:31:40[engine.py:448model_class,_ = get_model_architecture(modol_config)\nERROR 04-16 02:31:40ERROR 04-16 02:31:40ERROR 04-16 02:31:40ERROR 04-16 02:31:40[eng ine.py:448[engine.py:448[engine.py:448[engine.py:448n^^^^^^~^^^^^^^^^^^^^^^^^^^^^^^^^architectures = resolve_transformers_arch(model_config, architectures)File */usr/local/lib/python3.12/dist-packages/vllm/model_executor/mode?_loader/utils.py\", line 104, in get_model_architecture\nERROR 04-16 02:31:40[engine .py:448File */usr/local/lib/python3. 12/dist-packages/vllm/model_executor/model_loader/utils.py\", line 72, in resolve_transformers_arch\nERROR 04-16 02:31:40ERROR 04-16 02:31:40[engine,py:448][engine.py:448]raise ValueError(valueError: KimiyLForConditionalGeneration has no vLLM implementation and the Transformers implementation is not compatible with vLLM. Try settin\n9 VLLM USE_V1=0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### How you are installing vllm\n\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2025-04-16T10:11:58+00:00",
    "closed_at": "2025-04-17T12:53:32+00:00",
    "comments": 28,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16715/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16715"
  },
  {
    "number": 7382,
    "title": "[Bug]: LLaMa 3.1 8B/70B/405B all behave poorly and differently using completions API as compared to good chat API",
    "body": "### Your current environment\r\n\r\nDocker latest 0.5.4\r\n\r\n```\r\ndocker pull vllm/vllm-openai:latest\r\ndocker run -d --restart=always \\\r\n    --runtime=nvidia \\\r\n    --gpus '\"device=0\"' \\\r\n    --shm-size=10.24gb \\\r\n    -p 5000:5000 \\\r\n        -e NCCL_IGNORE_DISABLED_P2P=1 \\\r\n    -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\\r\n    -e VLLM_NCCL_SO_PATH=/usr/local/lib/python3.10/dist-packages/nvidia/nccl/lib/libnccl.so.2 \\\r\n    -v /etc/passwd:/etc/passwd:ro \\\r\n    -v /etc/group:/etc/group:ro \\\r\n    -u `id -u`:`id -g` \\\r\n    -v \"${HOME}\"/.cache:$HOME/.cache/ \\\r\n    -v \"${HOME}\"/.cache/huggingface:$HOME/.cache/huggingface \\\r\n    -v \"${HOME}\"/.cache/huggingface/hub:$HOME/.cache/huggingface/hub \\\r\n    -v \"${HOME}\"/.config:$HOME/.config/   -v \"${HOME}\"/.triton:$HOME/.triton/  \\\r\n    --network host \\\r\n    --name llama31_8b \\\r\n    vllm/vllm-openai:latest \\\r\n        --port=5000 \\\r\n        --host=0.0.0.0 \\\r\n        --model=meta-llama/Meta-Llama-3.1-8B-Instruct \\\r\n        --seed 1234 \\\r\n        --tensor-parallel-size=1 \\\r\n        --max-model-len=131072 \\\r\n        --max-num-batched-tokens=131072 --max-log-len=100 \\\r\n        --served-model-name meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3-8B-Instruct \\\r\n        --download-dir=$HOME/.cache/huggingface/hub &>> logs.vllm_server.llama3-8b.txt\r\n```\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI cannot reproduce HuggingFace transformers (works) or Chat API with vLLM (works) against Completions API (fails).\r\n\r\nBoth streaming and non-streaming completions API behave poorly.\r\n\r\ncode:\r\n```\r\nbase_url = '' # FILL\r\n\r\nfrom transformers import AutoTokenizer\r\n\r\nmodel = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\r\nsystem_prompt = \"\"\"Follow these steps in solving any problem:\r\n1) Know: This will help students find the important information.\r\n2) Need to Know: This will force students to reread the question and write down what they are trying to solve for.\r\n3) Organize:  I think this would be a great place for teachers to emphasize drawing a model or picture.\r\n4) Work: Students show their calculations here.\r\n5) Solution: This is where students will ask themselves if the answer is reasonable and whether it answered the question.\"\"\"\r\nprompt = \"What is bigger, 9.9 or 9.11?\"\r\n\r\nmessages = [\r\n    {\r\n        \"role\": \"system\",\r\n        \"content\": system_prompt,\r\n    },\r\n    {\r\n        \"role\": \"user\",\r\n        \"content\": prompt,\r\n    }\r\n]\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\nadd_generation_prompt = True\r\nprompt_llm = tokenizer.apply_chat_template(conversation=messages, tokenize=False,\r\n                                           add_generation_prompt=add_generation_prompt)\r\nprint(\"prompt_llm: %s\\n\\n\" % prompt_llm)\r\n\r\nkwargs = dict(max_tokens=1024, temperature=0)\r\n\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(base_url=base_url)\r\nkwargs['model'] = model\r\n\r\nfor i in range(1, 2):\r\n    kwargs['seed'] = i\r\n\r\n    responses = client.completions.create(**kwargs, stream=False, prompt=prompt_llm)\r\n    text = responses.choices[0].text\r\n    print(\"seed: %s: stream nochat:\\n\\n %s\" % (i, text))\r\n\r\n    responses = client.completions.create(**kwargs, stream=True, prompt=prompt_llm)\r\n    text = ''\r\n    for event in responses:\r\n        delta = event.choices[0].text if event.choices else None  # extract the text\r\n        if delta:\r\n            text += delta  # append the text\r\n    print(\"seed: %s: stream nochat:\\n\\n %s\" % (i, text))\r\n\r\n    response = client.chat.completions.create(messages=messages, **kwargs, stream=False).choices[0].message.content\r\n    print(\"seed: %s: nostream chat:\\n\\n %s\" % (i, response))\r\n\r\n    responses = client.chat.completions.create(stream=True, messages=messages, **kwargs)\r\n    text = ''\r\n    for chunk in responses:\r\n        delta = chunk.choices[0].delta.content if chunk.choices else None\r\n        if delta:\r\n            text += delta\r\n    print(\"seed: %s: stream chat:\\n\\n %s\" % (i, text))\r\n```\r\n\r\nThis always gives something like:\r\n\r\n```\r\nprompt_llm: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\r\n\r\nCutting Knowledge Date: December 2023\r\nToday Date: 26 Jul 2024\r\n\r\nFollow these steps in solving any problem:\r\n1) Know: This will help students find the important information.\r\n2) Need to Know: This will force students to reread the question and write down what they are trying to solve for.\r\n3) Organize:  I think this would be a great place for teachers to emphasize drawing a model or picture.\r\n4) Work: Students show their calculations here.\r\n5) Solution: This is where students will ask themselves if the answer is reasonable and whether it answered the question.<|eot_id|><|start_header_id|>user<|end_header_id|>\r\n\r\nWhat is bigger, 9.9 or 9.11?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\r\n\r\n\r\n\r\n\r\nseed: 1: stream nochat:\r\n\r\n To solve this problem, we will follow the steps:\r\n\r\n1. Know: We know that we are comparing two decimal numbers, 9.9 and 9.11.\r\n\r\n2. Need to Know: We need to know which number is bigger. \r\n\r\n3. Organize: We can organize this by comparing the numbers. Since they are decimals, we can compare them by looking at the tenths place and the hundredths place.\r\n\r\n4. Work: \r\n- The tenths place is the same in both numbers (9).\r\n- The hundredths place in 9.9 is 0, and in 9.11, it is 1.\r\n- Since 1 is greater than 0, 9.11 is greater than 9.9.\r\n\r\n5. Solution: Yes, 9.11 is indeed bigger than 9.9.\r\nseed: 1: stream nochat:\r\n\r\n To solve this problem, we will follow the steps:\r\n\r\n1. Know: We know that we are comparing two decimal numbers, 9.9 and 9.11.\r\n\r\n2. Need to Know: We need to know which number is bigger. \r\n\r\n3. Organize: We can organize this by comparing the numbers. Since they are decimals, we can compare them by looking at the tenths place and the hundredths place.\r\n\r\n4. Work: \r\n- The tenths place is the same in both numbers (9).\r\n- The hundredths place in 9.9 is 0, and in 9.11, it is 1.\r\n- Since 1 is greater than 0, 9.11 is greater than 9.9.\r\n\r\n5. Solution: Yes, 9.11 is indeed bigger than 9.9.\r\nseed: 1: nostream chat:\r\n\r\n Let's follow the steps to solve this problem:\r\n\r\n**Know**: We need to compare two decimal numbers: 9.9 and 9.11.\r\n\r\n**Need to Know**: We are trying to determine which number is bigger.\r\n\r\n**Organize**: Let's think about the numbers. We can compare them by looking at the decimal part. If the decimal part is greater, the number is bigger.\r\n\r\n**Work**: We can compare the decimal parts: 0.9 is less than 0.11. Since 0.11 is greater, 9.11 is bigger than 9.9.\r\n\r\n**Solution**: Yes, 9.11 is indeed bigger than 9.9. This makes sense because 0.11 is greater than 0.9.\r\nseed: 1: stream chat:\r\n\r\n Let's follow the steps to solve this problem:\r\n\r\n**Know**: We need to compare two decimal numbers: 9.9 and 9.11.\r\n\r\n**Need to Know**: We are trying to determine which number is bigger.\r\n\r\n**Organize**: Let's think about the numbers. We can compare them by looking at the decimal part. If the decimal part is greater, the number is bigger.\r\n\r\n**Work**: We can compare the decimal parts: 0.9 is less than 0.11. Since 0.11 is greater, 9.11 is bigger than 9.9.\r\n\r\n**Solution**: Yes, 9.11 is indeed bigger than 9.9. This makes sense because 0.11 is greater than 0.9.\r\n```\r\n\r\nThat is, the Chat API output disagrees with the completions output, and it's always (varying seed, penalty, etc.) is always worse than the Chat API.\r\n\r\nI checked the code in vLLM and can't see what I'm doing differently in using the chat template.  With temperature=0, should be deterministic to good extent.  One can add repeats and see that it's always this way.",
    "labels": [
      "bug",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-08-10T01:47:36+00:00",
    "closed_at": null,
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7382/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7382"
  },
  {
    "number": 8581,
    "title": "[Feature]: DRY Sampling",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nDRY is a sampler that completely mitigates repetitions. This is especially important for small models which tend to slop in large contexts. Here's an explanation of DRY from the author himself https://github.com/oobabooga/text-generation-webui/pull/5677\r\nAlong with oobabooga, koboldcpp also has an implementation of DRY which according to author IIRC is better than oobabooga's\r\nDRY has been a completely game changer for me and from what I have seen several others. It completely removes need for other samplers like top_p, top_k, repetition_penalty. It is recommended to be used with min_p and produces great coherent results.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-09-18T23:05:42+00:00",
    "closed_at": null,
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8581/reactions",
      "total_count": 20,
      "+1": 20,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8581"
  },
  {
    "number": 13216,
    "title": "ValueError: Model architectures ['Qwen2ForCausalLM'] failed to be inspected. Please check the logs for more details.",
    "body": "### Your current environment\n\nValueError: Model architectures ['Qwen2ForCausalLM'] failed to be inspected. Please check the logs for more details. \u5728\u4f7f\u7528\ndeepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\u6a21\u578b\u65f6\uff0c\u62a5\u4e86\u8fd9\u4e2a\u9519\u8bef\u3002\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "open",
    "created_at": "2025-02-13T09:42:39+00:00",
    "closed_at": null,
    "comments": 24,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13216/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13216"
  },
  {
    "number": 5901,
    "title": "[Bug]: TRACKING ISSUE: `AsyncEngineDeadError`",
    "body": "### Your current environment\r\n\r\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nRecently, we have seen reports of `AsyncEngineDeadError`, including:\r\n\r\n- [ ] #5060\r\n- [x] #2000\r\n- [x] #3310\r\n- [x] #3839\r\n- [x] #4000\r\n- [x] #4135\r\n- [x] #4293\r\n- [x] #5443\r\n- [x] #5732\r\n- [x] #5822\r\n- [x] #6190 \r\n- [x] #6208\r\n- [x] #6361\r\n- [x] #6421\r\n- [ ] #6614\r\n- [x] #6790\r\n- [x] #6969\r\n- [x] #7356\r\n\r\nIf you see something like the following, please report here:\r\n\r\n```bash\r\n2024-06-25 12:27:29.905   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 84, in health\r\n2024-06-25 12:27:29.905     await openai_serving_chat.engine.check_health()\r\n2024-06-25 12:27:29.905   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 839, in check_health\r\n2024-06-25 12:27:29.905     raise AsyncEngineDeadError(\"Background loop is stopped.\")\r\n2024-06-25 12:27:29.905 vllm.engine.async_llm_engine.AsyncEngineDeadError: Background loop is stopped.\r\n```\r\n\r\nKey areas we are looking into include:\r\n- logprob usage\r\n- guided regex usage\r\n\r\nWhen reporting an issue, please include a sample request that causes the issue so we can reproduce on our side.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-06-27T11:49:38+00:00",
    "closed_at": null,
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5901/reactions",
      "total_count": 14,
      "+1": 14,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/5901"
  },
  {
    "number": 322,
    "title": "ray OOM in tensor parallel",
    "body": "In my case , I can deploy the vllm service on single GPU. but when I use multi gpu, I meet the ray OOM error. Could you please help solve this problem?\r\nmy model is yahma/llama-7b-hf\r\nmy transformers version is 4.28.0\r\nmy cuda version is 11.4\r\n\r\n\r\n--------\r\n2023-06-30 09:24:53,455 WARNING utils.py:593 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.\r\n2023-06-30 09:24:53,459 WARNING services.py:1826 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=6.12gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\r\n2023-06-30 09:24:53,584 INFO worker.py:1636 -- Started a local Ray instance.\r\nINFO 06-30 09:24:54 llm_engine.py:59] Initializing an LLM engine with config: model='/opt/app/yahma-llama-lora', dtype=torch.float16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=4, seed=0)\r\nWARNING 06-30 09:24:54 config.py:131] Possibly too large swap space. 16.00 GiB out of the 32.00 GiB total CPU memory is allocated for the swap space.\r\n/opt/app/yahma-llama-lora\r\nException in thread ray_print_logs:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python3.8/threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/ray/_private/worker.py\", line 900, in print_logs\r\n    global_worker_stdstream_dispatcher.emit(data)\r\n  File \"/usr/local/lib/python3.8/dist-packages/ray/_private/ray_logging.py\", line 264, in emit\r\n    handle(data)\r\n  File \"/usr/local/lib/python3.8/dist-packages/ray/_private/worker.py\", line 1788, in print_to_stdstream\r\n    print_worker_logs(batch, sink)\r\n  File \"/usr/local/lib/python3.8/dist-packages/ray/_private/worker.py\", line 1950, in print_worker_logs\r\n    restore_tqdm()\r\n  File \"/usr/local/lib/python3.8/dist-packages/ray/_private/worker.py\", line 1973, in restore_tqdm\r\n    tqdm_ray.instance().unhide_bars()\r\n  File \"/usr/local/lib/python3.8/dist-packages/ray/experimental/tqdm_ray.py\", line 344, in instance\r\n    _manager = _BarManager()\r\n  File \"/usr/local/lib/python3.8/dist-packages/ray/experimental/tqdm_ray.py\", line 256, in __init__\r\n    self.should_colorize = not ray.widgets.util.in_notebook()\r\n  File \"/usr/local/lib/python3.8/dist-packages/ray/widgets/util.py\", line 205, in in_notebook\r\n    shell = _get_ipython_shell_name()\r\n  File \"/usr/local/lib/python3.8/dist-packages/ray/widgets/util.py\", line 194, in _get_ipython_shell_name\r\n    import IPython\r\n  File \"/usr/local/lib/python3.8/dist-packages/IPython/__init__.py\", line 30, in <module>\r\n    raise ImportError(\r\nImportError: \r\nIPython 8.13+ supports Python 3.9 and above, following NEP 29.\r\nIPython 8.0-8.12 supports Python 3.8 and above, following NEP 29.\r\nWhen using Python 2.7, please install IPython 5.x LTS Long Term Support version.\r\nPython 3.3 and 3.4 were supported up to IPython 6.x.\r\nPython 3.5 was supported with IPython 7.0 to 7.9.\r\nPython 3.6 was supported with IPython up to 7.16.\r\nPython 3.7 was still supported with the 7.x branch.\r\n\r\nSee IPython `README.rst` file for more information:\r\n\r\n    https://github.com/ipython/ipython/blob/main/README.rst\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/app/vllm-0.1.1/vllm/entrypoints/llm.py\", line 55, in __init__\r\n    self.llm_engine = LLMEngine.from_engine_args(engine_args)\r\n  File \"/opt/app/vllm-0.1.1/vllm/engine/llm_engine.py\", line 151, in from_engine_args\r\n    engine = cls(*engine_configs, distributed_init_method, devices,\r\n  File \"/opt/app/vllm-0.1.1/vllm/engine/llm_engine.py\", line 102, in __init__\r\n    self._init_cache()\r\n  File \"/opt/app/vllm-0.1.1/vllm/engine/llm_engine.py\", line 114, in _init_cache\r\n    num_blocks = self._run_workers(\r\n  File \"/opt/app/vllm-0.1.1/vllm/engine/llm_engine.py\", line 317, in _run_workers\r\n    all_outputs = ray.get(all_outputs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/ray/_private/worker.py\", line 2542, in get\r\n    raise value\r\nray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\r\nMemory on the node (IP: 10.30.192.36, ID: 17400c6c9eee3bc1384c172eecd4e1ecf2992cbc7f50cb27d2dc60d7) where the task (task ID: ffffffffffffffff283e91f20257d747969124a201000000, name=Worker.__init__, pid=26332, memory used=4.54GB) was running was 31.27GB / 32.00GB (0.977298), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cb6154315a0e1a33d85683935ae20cf76eecd48230c3c4b3a5563fe4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.30.192.36`. To see the logs of the worker, use `ray logs worker-cb6154315a0e1a33d85683935ae20cf76eecd48230c3c4b3a5563fe4*out -ip 10.30.192.36. Top 10 memory users:\r\nPID     MEM(GB) COMMAND\r\n26333   4.60    ray::Worker.__init__\r\n26332   4.54    ray::Worker.__init__\r\n26331   4.51    ray::Worker.__init__\r\n26330   4.47    ray::Worker.__init__\r\n25044   0.23    python\r\n25099   0.19    /usr/local/lib/python3.8/dist-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_20...\r\n25340   0.06    ray::IDLE\r\n25174   0.06    /usr/bin/python /usr/local/lib/python3.8/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1 -...\r\n25310   0.06    /usr/bin/python -u /usr/local/lib/python3.8/dist-packages/ray/dashboard/agent.py --node-ip-address=1...\r\n25349   0.05    ray::IDLE\r\nRefer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-06-30T09:27:50+00:00",
    "closed_at": "2024-03-20T12:35:08+00:00",
    "comments": 27,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/322/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/322"
  },
  {
    "number": 10034,
    "title": "[Bug]: serve Llama-3.2-11B-Vision-Instruct with 2 A10 oom",
    "body": "### Your current environment\n\ndocker image vllm/vllm-openai:v0.6.2 and vllm/vllm-openai:v0.6.3\r\ncommand\uff1adocker run --runtime nvidia --gpus '\"device=0,1\"' -d -v /data/model/llama:/data/model/llama -p 8001:8000 vllm/vllm-openai:v0.6.2 --model /data/model/llama --max-model-len 1024 --served_model_name Llama-3.2-11B-Vision-Instruct --tensor-parallel-size 2 --gpu_memory_utilization 0.7\r\n\r\nI tried v0.6.2 and v0.6.3\uff0cboth not work\uff0conly half of the gpu memory is occupied\r\n\r\nnvidia-smi output\uff1a\r\n+-----------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 555.42.06              Driver Version: 555.42.06      CUDA Version: 12.5     |\r\n|-----------------------------------------+------------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                        |               MIG M. |\r\n|=========================================+========================+======================|\r\n|   0  NVIDIA A10                     On  |   00000000:00:04.0 Off |                    0 |\r\n|  0%   47C    P0             60W /  150W |   10797MiB /  23028MiB |      0%      Default |\r\n|                                         |                        |                  N/A |\r\n+-----------------------------------------+------------------------+----------------------+\r\n|   1  NVIDIA A10                     On  |   00000000:00:08.0 Off |                    0 |\r\n|  0%   47C    P0             61W /  150W |   10797MiB /  23028MiB |      0%      Default |\r\n|                                         |                        |                  N/A |\r\n+-----------------------------------------+------------------------+----------------------+\r\n|   2  NVIDIA A10                     On  |   00000000:00:09.0 Off |                    0 |\r\n|  0%   55C    P0             63W /  150W |   18493MiB /  23028MiB |      0%      Default |\r\n|                                         |                        |                  N/A |\r\n+-----------------------------------------+------------------------+----------------------+\r\n                                                                                         \r\n+-----------------------------------------------------------------------------------------+\r\n| Processes:                                                                              |\r\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n|        ID   ID                                                               Usage      |\r\n|=========================================================================================|\r\n|    0   N/A  N/A   2750406      C   /usr/bin/python3                            10788MiB |\r\n|    1   N/A  N/A   2750455      C   /usr/bin/python3                            10788MiB |\r\n|    2   N/A  N/A   2738538      C   ...nda/miniconda3/envs/vllm/bin/python      18470MiB |\r\n+-----------------------------------------------------------------------------------------+\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nINFO 11-05 03:21:18 api_server.py:526] vLLM API server version 0.6.1.dev238+ge2c6e0a82\r\nINFO 11-05 03:21:18 api_server.py:527] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, model='/data/model/llama', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=1024, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.7, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=False, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['Llama-3.2-11B-Vision-Instruct'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False)\r\nINFO 11-05 03:21:18 api_server.py:164] Multiprocessing frontend to use ipc:///tmp/056a940a-9835-44c7-95c2-01e0c84f4ed4 for IPC Path.\r\nINFO 11-05 03:21:18 api_server.py:177] Started engine process with PID 29\r\nINFO 11-05 03:21:18 config.py:899] Defaulting to use mp for distributed inference\r\njunjie_wang@ubuntuT-CCLLM-4804084:~$ sudo docker logs c123d8180a936a7bda26a6b3ce14dbee4964ac79b89f1ae74bb29c3467e0b37f\r\nINFO 11-05 03:21:18 api_server.py:526] vLLM API server version 0.6.1.dev238+ge2c6e0a82\r\nINFO 11-05 03:21:18 api_server.py:527] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, model='/data/model/llama', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=1024, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.7, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=False, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['Llama-3.2-11B-Vision-Instruct'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False)\r\nINFO 11-05 03:21:18 api_server.py:164] Multiprocessing frontend to use ipc:///tmp/056a940a-9835-44c7-95c2-01e0c84f4ed4 for IPC Path.\r\nINFO 11-05 03:21:18 api_server.py:177] Started engine process with PID 29\r\nINFO 11-05 03:21:18 config.py:899] Defaulting to use mp for distributed inference\r\nINFO 11-05 03:21:22 config.py:899] Defaulting to use mp for distributed inference\r\nINFO 11-05 03:21:22 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='/data/model/llama', speculative_config=None, tokenizer='/data/model/llama', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Llama-3.2-11B-Vision-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, mm_processor_kwargs=None)\r\nWARNING 11-05 03:21:23 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\nINFO 11-05 03:21:23 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\nINFO 11-05 03:21:23 enc_dec_model_runner.py:140] EncoderDecoderModelRunner requires XFormers backend; overriding backend auto-selection and forcing XFormers.\r\n(VllmWorkerProcess pid=67) INFO 11-05 03:21:23 enc_dec_model_runner.py:140] EncoderDecoderModelRunner requires XFormers backend; overriding backend auto-selection and forcing XFormers.\r\n(VllmWorkerProcess pid=67) INFO 11-05 03:21:23 selector.py:116] Using XFormers backend.\r\nINFO 11-05 03:21:23 selector.py:116] Using XFormers backend.\r\n/usr/local/lib/python3.12/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n(VllmWorkerProcess pid=67) /usr/local/lib/python3.12/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n(VllmWorkerProcess pid=67)   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n/usr/local/lib/python3.12/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n(VllmWorkerProcess pid=67) /usr/local/lib/python3.12/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n(VllmWorkerProcess pid=67)   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n(VllmWorkerProcess pid=67) INFO 11-05 03:21:23 multiproc_worker_utils.py:218] Worker ready; awaiting tasks\r\nINFO 11-05 03:21:24 utils.py:992] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=67) INFO 11-05 03:21:24 utils.py:992] Found nccl from library libnccl.so.2\r\nINFO 11-05 03:21:24 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(VllmWorkerProcess pid=67) INFO 11-05 03:21:24 pynccl.py:63] vLLM is using nccl==2.20.5\r\nINFO 11-05 03:21:24 custom_all_reduce_utils.py:204] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\nINFO 11-05 03:21:35 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n(VllmWorkerProcess pid=67) INFO 11-05 03:21:35 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\nINFO 11-05 03:21:35 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f3afe2f6570>, local_subscribe_port=33245, remote_subscribe_port=None)\r\nINFO 11-05 03:21:35 model_runner.py:1014] Starting to load model /data/model/llama...\r\n(VllmWorkerProcess pid=67) INFO 11-05 03:21:35 model_runner.py:1014] Starting to load model /data/model/llama...\r\nINFO 11-05 03:21:35 selector.py:116] Using XFormers backend.\r\n(VllmWorkerProcess pid=67) INFO 11-05 03:21:35 selector.py:116] Using XFormers backend.\r\nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:01<00:04,  1.08s/it]\r\nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:02<00:03,  1.22s/it]\r\nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:03<00:02,  1.23s/it]\r\nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:05<00:01,  1.28s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:05<00:00,  1.05it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:05<00:00,  1.07s/it]\r\n\r\nINFO 11-05 03:21:41 model_runner.py:1025] Loading model weights took 10.0714 GB\r\n(VllmWorkerProcess pid=67) INFO 11-05 03:21:41 model_runner.py:1025] Loading model weights took 10.0714 GB\r\nINFO 11-05 03:21:41 enc_dec_model_runner.py:297] Starting profile run for multi-modal models.\r\n(VllmWorkerProcess pid=67) INFO 11-05 03:21:41 enc_dec_model_runner.py:297] Starting profile run for multi-modal models.\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233] Exception in worker VllmWorkerProcess while processing method determine_num_available_blocks: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 1 has a total capacity of 22.09 GiB of which 758.44 MiB is free. Process 2751070 has 21.35 GiB memory in use. Of the allocated memory 20.90 GiB is allocated by PyTorch, and 85.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables), Traceback (most recent call last):\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 226, in _run_worker_process\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]              ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 223, in determine_num_available_blocks\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     self.model_runner.profile_run()\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/enc_dec_model_runner.py\", line 348, in profile_run\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/enc_dec_model_runner.py\", line 201, in execute_model\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     hidden_or_intermediate_states = model_executable(\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]                                     ^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/mllama.py\", line 1084, in forward\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     cross_attention_states = self.vision_model(pixel_values,\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/mllama.py\", line 508, in forward\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     patch_embeds = self.patch_embedding(\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]                    ^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/mllama.py\", line 229, in forward\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     x, _ = self._linear(x)\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py\", line 367, in forward\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     output_parallel = self.quant_method.apply(self, input_, bias)\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py\", line 135, in apply\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]     return F.linear(x, layer.weight, bias)\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 1 has a total capacity of 22.09 GiB of which 758.44 MiB is free. Process 2751070 has 21.35 GiB memory in use. Of the allocated memory 20.90 GiB is allocated by PyTorch, and 85.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\n(VllmWorkerProcess pid=67) ERROR 11-05 03:21:52 multiproc_worker_utils.py:233] \r\n(VllmWorkerProcess pid=67) INFO 11-05 03:21:53 multiproc_worker_utils.py:244] Worker exiting\r\nINFO 11-05 03:21:53 multiproc_worker_utils.py:124] Killing local vLLM worker processes\r\nProcess SpawnProcess-1:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 388, in run_mp_engine\r\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 138, in from_engine_args\r\n    return cls(\r\n           ^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 78, in __init__\r\n    self.engine = LLMEngine(*args,\r\n                  ^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 339, in __init__\r\n    self._initialize_kv_caches()\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 474, in _initialize_kv_caches\r\n    self.model_executor.determine_num_available_blocks())\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 39, in determine_num_available_blocks\r\n    num_blocks = self._run_workers(\"determine_num_available_blocks\", )\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 185, in _run_workers\r\n    driver_worker_output = driver_worker_method(*args, **kwargs)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 223, in determine_num_available_blocks\r\n    self.model_runner.profile_run()\r\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/enc_dec_model_runner.py\", line 348, in profile_run\r\n    self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/enc_dec_model_runner.py\", line 201, in execute_model\r\n    hidden_or_intermediate_states = model_executable(\r\n                                    ^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/mllama.py\", line 1084, in forward\r\n    cross_attention_states = self.vision_model(pixel_values,\r\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/mllama.py\", line 508, in forward\r\n    patch_embeds = self.patch_embedding(\r\n                   ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/mllama.py\", line 229, in forward\r\n    x, _ = self._linear(x)\r\n           ^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py\", line 367, in forward\r\n    output_parallel = self.quant_method.apply(self, input_, bias)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py\", line 135, in apply\r\n    return F.linear(x, layer.weight, bias)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 22.09 GiB of which 758.44 MiB is free. Process 2751020 has 21.35 GiB memory in use. Of the allocated memory 20.90 GiB is allocated by PyTorch, and 85.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\n[rank0]:[W1105 03:21:54.083552440 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-11-05T11:34:17+00:00",
    "closed_at": "2024-11-06T03:45:11+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10034/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10034"
  },
  {
    "number": 10300,
    "title": "[Bug]: undefined symbol: __nvJitLinkComplete_12_4, version libnvJitLink.so.12",
    "body": "### Your current environment\r\n\r\ni cannot execute collect_env.py  because of this error.\r\n\r\nin my another environment: torch is 2.4.0 and the version of vllm is `0.6.3.post1` which works fine.\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nfollowing installation guide: https://docs.vllm.ai/en/stable/getting_started/installation.html#install-the-latest-code\r\n\r\n`vllm version: 0.6.3.post2.dev386+g0b8bb86b`\r\n\r\nhowever, it forces the installation of torch to be `2.5.1`\r\n\r\nwhich causes the error :\r\n\r\n> Traceback (most recent call last):\r\n>   File \"/home/ubuntu/vllm/collect_env.py\", line 15, in <module>\r\n>     from vllm.envs import environment_variables\r\n>   File \"/home/ubuntu/vllm/vllm/__init__.py\", line 3, in <module>\r\n>     from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\r\n>   File \"/home/ubuntu/vllm/vllm/engine/arg_utils.py\", line 8, in <module>\r\n>     import torch\r\n>   File \"/opt/conda/envs/vllmsource/lib/python3.11/site-packages/torch/__init__.py\", line 367, in <module>\r\n>     from torch._C import *  # noqa: F403\r\n>     ^^^^^^^^^^^^^^^^^^^^^^\r\n> ImportError: /opt/conda/envs/vllmsource/lib/python3.11/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkComplete_12_4, version libnvJitLink.so.12\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2024-11-13T16:27:59+00:00",
    "closed_at": null,
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10300/reactions",
      "total_count": 7,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10300"
  },
  {
    "number": 10913,
    "title": "[Bug]: Speculative decoding inconsistency for Qwen-Coder-32B",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nOutput of `python collect_env.py` was not collected because I do not have wget within my docker container. I can attempt a docker cp command to put the collect_env.py within my container later, but haven't yet.\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\nRunning VLLM with docker. Speculative decoding for the Qwen-coder-32B using the 0.5B model does not work. Note that all the Qwen models described are from the official Qwen AWQ repos on Huggingface.\r\n\r\nHere is the relevant section of docker-compose.yml:\r\n\r\ncommand: >\r\n      --model /app/models/Qwen2.5-Coder-32B-Instruct-AWQ\r\n      --tensor_parallel_size 2\r\n      --max-model-len 23568\r\n      --enable-auto-tool-choice\r\n      --tool-call-parser hermes\r\n      --speculative_model=\"/app/models/Qwen2.5-Coder-0.5B-Instruct-AWQ\"\r\n      --num_speculative_tokens=5\r\n\r\nHowever, curiously, the 7B model DOES work.\r\n\r\ncommand: >\r\n      --model /app/models/Qwen2.5-Coder-32B-Instruct-AWQ\r\n      --tensor_parallel_size 2\r\n      --max-model-len 23568\r\n      --enable-auto-tool-choice\r\n      --tool-call-parser hermes\r\n      --speculative_model=\"/app/models/Qwen2.5-Coder-**7B**-Instruct-AWQ\"\r\n      --num_speculative_tokens=5\r\n\r\nA write-up on HuggingFace says that the error _could_ be due to a difference in the vocabulary size between 0.5/3B and 7/32B.\r\nThe 7B model works well enough on my dual-3090 setup, but I would love to save the VRAM and get even faster performance from using the 0.5B model for speculative decoding.\n\n### \ud83d\udc1b Describe the bug\n\nSpeculative decoding for the Qwen-coder-32B using the **0.5B** model does not work. However, using the **7B** model for speculative decoding DOES work. Possibly due to differing vocab-sizes.\r\n\r\n* Note that all the Qwen models described are from the official Qwen AWQ repos on Huggingface:\r\nhttps://huggingface.co/collections/Qwen/qwen25-coder-66eaa22e6f99801bf65b0c2f\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-12-05T03:49:40+00:00",
    "closed_at": null,
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10913/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10913"
  },
  {
    "number": 15779,
    "title": "[Bug]: Disagreement and misalignment between supported models in documentation and actual testing",
    "body": "### Your current environment\n\nTested under VLLM ==0.7.3 and 0.8.2\n\n\n### \ud83d\udc1b Describe the bug\n\nI am using this model (after quantizing it to 4 bits):\n**nvidia/Llama-3_3-Nemotron-Super-49B-v1**\nhttps://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1\nand according to the documentation here:\nhttps://docs.vllm.ai/en/latest/models/supported_models.html\n(This is the summary of the related section in the above website:\n`To determine whether a given model is natively supported, you can check the config.json file inside the HF repository. If the \"architectures\" field contains a model architecture listed below, then it should be natively supported.`\n)\nThe **nvidia/Llama-3_3-Nemotron-Super-49B-v1** model architecture according to the HF is **DeciLMForCausalLM**\nand according to the documentation above, **DeciLMForCausalLM** is listed as one of the supported architectures hence it should be natively supported. However loading the above model create these issues:\n\n![Image](https://github.com/user-attachments/assets/2cb8e815-2169-462d-a7a6-6903f81a81e8)\n\n1- Under VLLM ==0.7.3 I get this error:\nDeciLMConfig object has no attribute 'num_key_value_heads_per_layer' \n2- Under VLLM==0.8.2 I get OOM and this error together.\nCan someone explain if I am doing anything wrong, or if I am interpreting anything wrong.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-30T21:10:24+00:00",
    "closed_at": "2025-04-07T06:09:23+00:00",
    "comments": 26,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15779/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15779"
  },
  {
    "number": 19483,
    "title": "[Bug]: Docker vLLM 0.9.1 CUDA error: an illegal memory access, sampled_token_ids.tolist()",
    "body": "### Your current environment\n\nDocker on 4 x A100 SMX.\nBTW: vLLM 0.8.4 worked stable with same setup.\n0.9.01 was already unstable (restarted few time a day), now even more.\n\n```\nservices:\n  vllm-qwen25-72b:\n    image: vllm/vllm-openai:v0.9.1\n    container_name: vllm-qwen25-72b\n    environment:\n     ...\n      - HF_TOKEN=$HF_TOKEN\n      - VLLM_NO_USAGE_STATS=1\n    ipc: host\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              device_ids: ['0', '1', '2', '3']\n              capabilities: [ gpu ]\n    network_mode: host\n    volumes:\n      - /mnt/sda/huggingface:/root/.cache/huggingface\n      - .:/opt/vllm\n    command:\n      - --port=8000\n      - --disable-log-requests\n      - --model=Qwen/Qwen2.5-72B-Instruct\n      # - --served-model-name=Qwen/Qwen2.5-72B-Instruct\n      # - --max-model-len=32768\n      - --tensor-parallel-size=4\n      - --gpu-memory-utilization=0.90\n      - --swap-space=5\n    restart: unless-stopped\n\n```\n\n### \ud83d\udc1b Describe the bug\n\nSee log file below\n\nvLLM 0.9.1 crashes frequently with Qwen 2.5 on 4xA100 SMX.\n\n(0.9.0.1 also crashed with \"CUDA error: an illegal memory access was encountered\", but much less frequently and not with a clear hint what went wrong. 0.8.4 was running stable.)\n\nI have no example request - we use a mix of normal and guided JSON sampling.\n\n\nThis might be the main problem?\n\n```\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()\nException raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fa563f785e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527] RuntimeError: CUDA error: an illegal memory access was encountered\n```\n\nFull log:\n\n```\n[rank0]:[E611 01:51:09.940883637 ProcessGroupNCCL.cpp:1896] [PG ID 2 PG GUID 3 Rank 0] Process group watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nException raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fa563f785e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x7fa563f0d4a2 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x7fa564365422 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7fa4f3c8b456 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x7fa4f3c9b6f0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x7fa4f3c9d282 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fa4f3c9ee8d in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0xdc253 (0x7fa4e3fb3253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #8: <unknown function> + 0x94ac3 (0x7fa564c42ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #9: clone + 0x44 (0x7fa564cd3a04 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n\nterminate called after throwing an instance of 'c10::DistBackendError'\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527] WorkerProc hit an exception.\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527] Traceback (most recent call last):\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 522, in worker_busy_loop\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]     output = func(*args, **kwargs)\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]     return func(*args, **kwargs)\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 293, in execute_model\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]     return func(*args, **kwargs)\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1374, in execute_model\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527] RuntimeError: CUDA error: an illegal memory access was encountered\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527] \n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527] Traceback (most recent call last):\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 522, in worker_busy_loop\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]     output = func(*args, **kwargs)\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]     return func(*args, **kwargs)\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 293, in execute_model\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,\n  what():  [PG ID 2 PG GUID 3 Rank 0] Process group watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]     return func(*args, **kwargs)\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1374, in execute_model\n\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]     valid_sampled_token_ids = sampled_token_ids.tolist()\nException raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fa563f785e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527] RuntimeError: CUDA error: an illegal memory access was encountered\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x7fa563f0d4a2 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x7fa564365422 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7fa4f3c8b456 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x7fa4f3c9b6f0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x7fa4f3c9d282 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fa4f3c9ee8d in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0xdc253 (0x7fa4e3fb3253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #8: <unknown function> + 0x94ac3 (0x7fa564c42ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #9: clone + 0x44 (0x7fa564cd3a04 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n\nException raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1902 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fa563f785e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0xcc7a4e (0x7fa4f3c6da4e in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #2: <unknown function> + 0x9165ed (0x7fa4f38bc5ed in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #3: <unknown function> + 0xdc253 (0x7fa4e3fb3253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #4: <unknown function> + 0x94ac3 (0x7fa564c42ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #5: clone + 0x44 (0x7fa564cd3a04 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527] \n(VllmWorker rank=0 pid=226) ERROR 06-11 01:51:09 [multiproc_executor.py:527] \nERROR 06-11 01:51:09 [dump_input.py:69] Dumping input data\nERROR 06-11 01:51:09 [dump_input.py:71] V1 LLM engine (v0.9.1) with config: model='/root/.cache/huggingface/hub/models--Qwen--Qwen2.5-72B-Instruct/snapshots/d3d951150c1e5848237cd6a7ad11df4836aee842/', speculative_config=None, tokenizer='/root/.cache/huggingface/hub/models--Qwen--Qwen2.5-72B-Instruct/snapshots/d3d951150c1e5848237cd6a7ad11df4836aee842/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-72B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}, \nERROR 06-11 01:51:09 [dump_input.py:79] Dumping scheduler output for model execution:\nERROR 06-11 01:51:09 [dump_input.py:80] SchedulerOutput(scheduled_new_reqs=[], scheduled_cached_reqs=[CachedRequestData(req_id='chatcmpl-f8adb07fdf2e41e69e9be99f4f9cc7eb', resumed_from_preemption=false, new_token_ids=[374], new_block_ids=[[]], num_computed_tokens=187), CachedRequestData(req_id='chatcmpl-a0b407784af14747b4a9af20d4d69829', resumed_from_preemption=false, new_token_ids=[330], new_block_ids=[[]], num_computed_tokens=2184), CachedRequestData(req_id='chatcmpl-d52cc47002544eaa97785872789929c8', resumed_from_preemption=false, new_token_ids=[330], new_block_ids=[[]], num_computed_tokens=9828), CachedRequestData(req_id='chatcmpl-6505bbb4a369474fb64b00f9e8e36de7', resumed_from_preemption=false, new_token_ids=[1008], new_block_ids=[[]], num_computed_tokens=66), CachedRequestData(req_id='chatcmpl-835ba60f60fe4171b7cc74141ca68a31', resumed_from_preemption=false, new_token_ids=[1008], new_block_ids=[[]], num_computed_tokens=66)], num_scheduled_tokens={chatcmpl-835ba60f60fe4171b7cc74141ca68a31: 1, chatcmpl-6505bbb4a369474fb64b00f9e8e36de7: 1, chatcmpl-a0b407784af14747b4a9af20d4d69829: 1, chatcmpl-f8adb07fdf2e41e69e9be99f4f9cc7eb: 1, chatcmpl-d52cc47002544eaa97785872789929c8: 1}, total_num_scheduled_tokens=5, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[0], finished_req_ids=[], free_encoder_input_ids=[], structured_output_request_ids={chatcmpl-a0b407784af14747b4a9af20d4d69829: 1, chatcmpl-d52cc47002544eaa97785872789929c8: 2}, grammar_bitmask=array([[      0,       0,       2, ...,       0,       0,       0],\nERROR 06-11 01:51:09 [dump_input.py:80]        [      0, 1507336,       0, ...,       0,       0,       0]],\nERROR 06-11 01:51:09 [dump_input.py:80]       shape=(2, 4752), dtype=int32), kv_connector_metadata=null)\nERROR 06-11 01:51:09 [dump_input.py:82] SchedulerStats(num_running_reqs=5, num_waiting_reqs=0, gpu_cache_usage=0.026913812964708295, prefix_cache_stats=PrefixCacheStats(reset=False, requests=0, queries=0, hits=0), spec_decoding_stats=None)\nERROR 06-11 01:51:09 [core.py:517] EngineCore encountered a fatal error.\nERROR 06-11 01:51:09 [core.py:517] Traceback (most recent call last):\nERROR 06-11 01:51:09 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 508, in run_engine_core\nERROR 06-11 01:51:09 [core.py:517]     engine_core.run_busy_loop()\nERROR 06-11 01:51:09 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 535, in run_busy_loop\nERROR 06-11 01:51:09 [core.py:517]     self._process_engine_step()\nERROR 06-11 01:51:09 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 560, in _process_engine_step\nERROR 06-11 01:51:09 [core.py:517]     outputs, model_executed = self.step_fn()\nERROR 06-11 01:51:09 [core.py:517]                               ^^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 231, in step\nERROR 06-11 01:51:09 [core.py:517]     model_output = self.execute_model(scheduler_output)\nERROR 06-11 01:51:09 [core.py:517]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 217, in execute_model\nERROR 06-11 01:51:09 [core.py:517]     raise err\nERROR 06-11 01:51:09 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 211, in execute_model\nERROR 06-11 01:51:09 [core.py:517]     return self.model_executor.execute_model(scheduler_output)\nERROR 06-11 01:51:09 [core.py:517]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 163, in execute_model\nERROR 06-11 01:51:09 [core.py:517]     (output, ) = self.collective_rpc(\"execute_model\",\nERROR 06-11 01:51:09 [core.py:517]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 220, in collective_rpc\nERROR 06-11 01:51:09 [core.py:517]     result = get_response(w, dequeue_timeout)\nERROR 06-11 01:51:09 [core.py:517]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 207, in get_response\nERROR 06-11 01:51:09 [core.py:517]     raise RuntimeError(\nERROR 06-11 01:51:09 [core.py:517] RuntimeError: Worker failed with error 'CUDA error: an illegal memory access was encountered\nERROR 06-11 01:51:09 [core.py:517] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nERROR 06-11 01:51:09 [core.py:517] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\nERROR 06-11 01:51:09 [core.py:517] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\nERROR 06-11 01:51:09 [core.py:517] ', please check the stack trace above for the root cause\nERROR 06-11 01:51:09 [async_llm.py:420] AsyncLLM output_handler failed.\nERROR 06-11 01:51:09 [async_llm.py:420] Traceback (most recent call last):\nERROR 06-11 01:51:09 [async_llm.py:420]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 379, in output_handler\nERROR 06-11 01:51:09 [async_llm.py:420]     outputs = await engine_core.get_output_async()\nERROR 06-11 01:51:09 [async_llm.py:420]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [async_llm.py:420]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 790, in get_output_async\nERROR 06-11 01:51:09 [async_llm.py:420]     raise self._format_exception(outputs) from None\nERROR 06-11 01:51:09 [async_llm.py:420] vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\nERROR 06-11 01:51:09 [serving_chat.py:911] Error in chat completion stream generator.\nERROR 06-11 01:51:09 [serving_chat.py:911] Traceback (most recent call last):\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 481, in chat_completion_stream_generator\nERROR 06-11 01:51:09 [serving_chat.py:911]     async for res in result_generator:\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 327, in generate\nERROR 06-11 01:51:09 [serving_chat.py:911]     out = q.get_nowait() or await q.get()\nERROR 06-11 01:51:09 [serving_chat.py:911]                             ^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 52, in get\nERROR 06-11 01:51:09 [serving_chat.py:911]     raise output\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/utils.py\", line 100, in wrapper\nERROR 06-11 01:51:09 [serving_chat.py:911]     return await func(*args, **kwargs)\nERROR 06-11 01:51:09 [serving_chat.py:911]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 554, in create_chat_completion\nERROR 06-11 01:51:09 [serving_chat.py:911]     generator = await handler.create_chat_completion(request, raw_request)\nERROR 06-11 01:51:09 [serving_chat.py:911]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 268, in create_chat_completion\nERROR 06-11 01:51:09 [serving_chat.py:911]     return await self.chat_completion_full_generator(\nERROR 06-11 01:51:09 [serving_chat.py:911]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 932, in chat_completion_full_generator\nERROR 06-11 01:51:09 [serving_chat.py:911]     async for res in result_generator:\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 327, in generate\nERROR 06-11 01:51:09 [serving_chat.py:911]     out = q.get_nowait() or await q.get()\nERROR 06-11 01:51:09 [serving_chat.py:911]                             ^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 52, in get\nERROR 06-11 01:51:09 [serving_chat.py:911]     raise output\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 379, in output_handler\nERROR 06-11 01:51:09 [serving_chat.py:911]     outputs = await engine_core.get_output_async()\nERROR 06-11 01:51:09 [serving_chat.py:911]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 790, in get_output_async\nERROR 06-11 01:51:09 [serving_chat.py:911]     raise self._format_exception(outputs) from None\nERROR 06-11 01:51:09 [serving_chat.py:911] vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\nERROR 06-11 01:51:09 [serving_chat.py:911] Error in chat completion stream generator.\nERROR 06-11 01:51:09 [serving_chat.py:911] Traceback (most recent call last):\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 481, in chat_completion_stream_generator\nERROR 06-11 01:51:09 [serving_chat.py:911]     async for res in result_generator:\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 327, in generate\nERROR 06-11 01:51:09 [serving_chat.py:911]     out = q.get_nowait() or await q.get()\nERROR 06-11 01:51:09 [serving_chat.py:911]                             ^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 52, in get\nERROR 06-11 01:51:09 [serving_chat.py:911]     raise output\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 481, in chat_completion_stream_generator\nERROR 06-11 01:51:09 [serving_chat.py:911]     async for res in result_generator:\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 327, in generate\nERROR 06-11 01:51:09 [serving_chat.py:911]     out = q.get_nowait() or await q.get()\nERROR 06-11 01:51:09 [serving_chat.py:911]                             ^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 52, in get\nERROR 06-11 01:51:09 [serving_chat.py:911]     raise output\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/utils.py\", line 100, in wrapper\nERROR 06-11 01:51:09 [serving_chat.py:911]     return await func(*args, **kwargs)\nERROR 06-11 01:51:09 [serving_chat.py:911]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 554, in create_chat_completion\nERROR 06-11 01:51:09 [serving_chat.py:911]     generator = await handler.create_chat_completion(request, raw_request)\nERROR 06-11 01:51:09 [serving_chat.py:911]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 268, in create_chat_completion\nERROR 06-11 01:51:09 [serving_chat.py:911]     return await self.chat_completion_full_generator(\nERROR 06-11 01:51:09 [serving_chat.py:911]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 932, in chat_completion_full_generator\nERROR 06-11 01:51:09 [serving_chat.py:911]     async for res in result_generator:\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 327, in generate\nERROR 06-11 01:51:09 [serving_chat.py:911]     out = q.get_nowait() or await q.get()\nERROR 06-11 01:51:09 [serving_chat.py:911]                             ^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 52, in get\nERROR 06-11 01:51:09 [serving_chat.py:911]     raise output\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 379, in output_handler\nERROR 06-11 01:51:09 [serving_chat.py:911]     outputs = await engine_core.get_output_async()\nERROR 06-11 01:51:09 [serving_chat.py:911]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-11 01:51:09 [serving_chat.py:911]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 790, in get_output_async\nERROR 06-11 01:51:09 [serving_chat.py:911]     raise self._format_exception(outputs) from None\nERROR 06-11 01:51:09 [serving_chat.py:911] vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\nINFO:     127.0.0.1:46320 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\nINFO:     172.19.103.111:36678 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\nINFO:     172.19.103.111:57278 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\nINFO:     Shutting down\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\nINFO:     Finished server process [1]\n[rank2]:[W611 01:51:09.369133081 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=86, addr=[localhost]:37972, remote=[localhost]:59835): failed to recv, got 0 bytes\nException raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f86bc1785e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x5ba8afe (0x7f86a023cafe in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\nframe #2: <unknown function> + 0x5baae40 (0x7f86a023ee40 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\nframe #3: <unknown function> + 0x5bab74a (0x7f86a023f74a in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\nframe #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7f86a02391a9 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\nframe #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7f864be99989 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0xdc253 (0x7f863c1b3253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #7: <unknown function> + 0x94ac3 (0x7f86bce6cac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #8: clone + 0x44 (0x7f86bcefda04 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n\n[rank2]:[W611 01:51:09.374069347 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0 Rank 2] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n[rank3]:[W611 01:51:09.424776342 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=86, addr=[localhost]:37988, remote=[localhost]:59835): failed to recv, got 0 bytes\nException raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fc675f785e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x5ba8afe (0x7fc65a03cafe in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\nframe #2: <unknown function> + 0x5baae40 (0x7fc65a03ee40 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\nframe #3: <unknown function> + 0x5bab74a (0x7fc65a03f74a in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\nframe #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7fc65a0391a9 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\nframe #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7fc605c99989 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0xdc253 (0x7fc5f5fb3253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #7: <unknown function> + 0x94ac3 (0x7fc676ce9ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #8: clone + 0x44 (0x7fc676d7aa04 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n\n[rank3]:[W611 01:51:09.429163290 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0 Rank 3] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n[rank1]:[W611 01:51:09.436189340 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=86, addr=[localhost]:38002, remote=[localhost]:59835): failed to recv, got 0 bytes\nException raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7efce171e5e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x5ba8afe (0x7efd3683cafe in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\nframe #2: <unknown function> + 0x5baae40 (0x7efd3683ee40 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\nframe #3: <unknown function> + 0x5bab74a (0x7efd3683f74a in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\nframe #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7efd368391a9 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\nframe #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7efce2499989 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0xdc253 (0x7efcd27b3253 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #7: <unknown function> + 0x94ac3 (0x7efd53341ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #8: clone + 0x44 (0x7efd533d2a04 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n\n[rank1]:[W611 01:51:09.440752408 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0 Rank 1] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\nnanobind: leaked 4 instances!\n - leaked instance 0x7efc44398258 of type \"xgrammar.xgrammar_bindings.GrammarMatcher\"\n - leaked instance 0x7efc442df2e8 of type \"xgrammar.xgrammar_bindings.CompiledGrammar\"\n - leaked instance 0x7efc4438b798 of type \"xgrammar.xgrammar_bindings.CompiledGrammar\"\n - leaked instance 0x7efc44396718 of type \"xgrammar.xgrammar_bindings.GrammarMatcher\"\nnanobind: leaked 2 types!\n - leaked type \"xgrammar.xgrammar_bindings.GrammarMatcher\"\n - leaked type \"xgrammar.xgrammar_bindings.CompiledGrammar\"\nnanobind: leaked 13 functions!\n - leaked function \"fill_next_token_bitmask\"\n - leaked function \"rollback\"\n - leaked function \"__init__\"\n - leaked function \"\"\n - leaked function \"\"\n - leaked function \"\"\n - leaked function \"\"\n - leaked function \"\"\n - leaked function \"find_jump_forward_string\"\n - leaked function \"reset\"\n - leaked function \"_debug_accept_string\"\n - leaked function \"is_terminated\"\n - leaked function \"accept_token\"\nnanobind: this is likely caused by a reference counting issue in the binding code.\n/usr/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n/usr/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 2 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-06-11T09:17:05+00:00",
    "closed_at": null,
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19483/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19483"
  },
  {
    "number": 15078,
    "title": "[Bug]: new bug after loosening type check on `llava_onevision.py`",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nCollecting environment information...\nPyTorch version: 2.5.1+cu118\nIs debug build: False\nCUDA used to build PyTorch: 11.8\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.6 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\nClang version: Could not collect\nCMake version: version 3.16.3\nLibc version: glibc-2.31\n\nPython version: 3.12.9 | packaged by conda-forge | (main, Mar  4 2025, 22:48:41) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-5.4.0-146-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: 12.0.140\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA GeForce RTX 4090\nGPU 1: NVIDIA GeForce RTX 4090\nGPU 2: NVIDIA GeForce RTX 4090\nGPU 3: NVIDIA GeForce RTX 4090\nGPU 4: NVIDIA GeForce RTX 4090\nGPU 5: NVIDIA GeForce RTX 4090\nGPU 6: NVIDIA GeForce RTX 4090\nGPU 7: NVIDIA GeForce RTX 4090\n\nNvidia driver version: 525.105.17\ncuDNN version: Probably one of the following:\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn.so.8.0.5\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.0.5\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.0.5\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.0.5\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.0.5\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.0.5\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.0.5\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn.so.8.4.0\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.4.0\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.4.0\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.4.0\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.4.0\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.4.0\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.4.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn.so.8.9.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.9.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.9.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.9.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.9.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.9.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.9.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nByte Order:                      Little Endian\nAddress sizes:                   46 bits physical, 57 bits virtual\nCPU(s):                          128\nOn-line CPU(s) list:             0-127\nThread(s) per core:              2\nCore(s) per socket:              32\nSocket(s):                       2\nNUMA node(s):                    2\nVendor ID:                       GenuineIntel\nCPU family:                      6\nModel:                           106\nModel name:                      Intel(R) Xeon(R) Gold 6338 CPU @ 2.00GHz\nStepping:                        6\nFrequency boost:                 enabled\nCPU MHz:                         1285.322\nCPU max MHz:                     3200.0000\nCPU min MHz:                     800.0000\nBogoMIPS:                        4000.00\nVirtualization:                  VT-x\nL1d cache:                       3 MiB\nL1i cache:                       2 MiB\nL2 cache:                        80 MiB\nL3 cache:                        96 MiB\nNUMA node0 CPU(s):               0-31,64-95\nNUMA node1 CPU(s):               32-63,96-127\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:          Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu11==11.11.3.6\n[pip3] nvidia-cuda-cupti-cu11==11.8.87\n[pip3] nvidia-cuda-nvrtc-cu11==11.8.89\n[pip3] nvidia-cuda-runtime-cu11==11.8.89\n[pip3] nvidia-cudnn-cu11==9.1.0.70\n[pip3] nvidia-cufft-cu11==10.9.0.58\n[pip3] nvidia-curand-cu11==10.3.0.86\n[pip3] nvidia-cusolver-cu11==11.4.1.48\n[pip3] nvidia-cusparse-cu11==11.7.5.86\n[pip3] nvidia-nccl-cu11==2.21.5\n[pip3] nvidia-nvtx-cu11==11.8.86\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1+cu118\n[pip3] torchaudio==2.5.1+cu118\n[pip3] torchvision==0.20.1+cu118\n[pip3] transformers==4.49.0\n[pip3] triton==3.1.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu11        11.11.3.6                pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu11    11.8.87                  pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu11    11.8.89                  pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu11  11.8.89                  pypi_0    pypi\n[conda] nvidia-cudnn-cu11         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu11         10.9.0.58                pypi_0    pypi\n[conda] nvidia-curand-cu11        10.3.0.86                pypi_0    pypi\n[conda] nvidia-cusolver-cu11      11.4.1.48                pypi_0    pypi\n[conda] nvidia-cusparse-cu11      11.7.5.86                pypi_0    pypi\n[conda] nvidia-nccl-cu11          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvtx-cu11          11.8.86                  pypi_0    pypi\n[conda] pyzmq                     26.2.1                   pypi_0    pypi\n[conda] torch                     2.5.1+cu118              pypi_0    pypi\n[conda] torchaudio                2.5.1+cu118              pypi_0    pypi\n[conda] torchvision               0.20.1+cu118             pypi_0    pypi\n[conda] transformers              4.49.0                   pypi_0    pypi\n[conda] triton                    3.1.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.3\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n\ufffd[4mGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tCPU Affinity\tNUMA Affinity\ufffd[0m\nGPU0\t X \tPXB\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\t0-31,64-95\t0\nGPU1\tPXB\t X \tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tPIX\tPIX\t0-31,64-95\t0\nGPU2\tPXB\tPXB\t X \tPIX\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\t0-31,64-95\t0\nGPU3\tPXB\tPXB\tPIX\t X \tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\t0-31,64-95\t0\nGPU4\tSYS\tSYS\tSYS\tSYS\t X \tPXB\tPXB\tPXB\tSYS\tSYS\t32-63,96-127\t1\nGPU5\tSYS\tSYS\tSYS\tSYS\tPXB\t X \tPXB\tPXB\tSYS\tSYS\t32-63,96-127\t1\nGPU6\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\t X \tPIX\tSYS\tSYS\t32-63,96-127\t1\nGPU7\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tPIX\t X \tSYS\tSYS\t32-63,96-127\t1\nNIC0\tPXB\tPIX\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\t X \tPIX\t\t\nNIC1\tPXB\tPIX\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tPIX\t X \t\t\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n\nLD_LIBRARY_PATH=/home/phd-chen.yirong2/anaconda3/envs/vllm/lib/python3.12/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:\nCUDA_HOME=/usr/local/cuda/\nCUDA_HOME=/usr/local/cuda/\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nHi, @DarkLight1337 \nThis is a bug report when I'm trying #15021 to fix #15017.\n\nAfter I tried the specific change to `llava_onevision.py` following #15021, I came across a new bug like this:\n```text\nERROR 03-18 13:40:00 engine.py:140]   File \".../anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/llava_onevision.py\", line 952, in forward                                                    \nERROR 03-18 13:40:00 engine.py:140]     inputs_embeds = self.get_input_embeddings_v0(                                                                                                                                                       \nERROR 03-18 13:40:00 engine.py:140]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                       \nERROR 03-18 13:40:00 engine.py:140]   File \".../anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/llava_onevision.py\", line 913, in get_input_embeddings_v0                                    \nERROR 03-18 13:40:00 engine.py:140]     video_embeds = self._process_video_pixels(video_input)                                                                                                                                              \nERROR 03-18 13:40:00 engine.py:140]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                              \nERROR 03-18 13:40:00 engine.py:140]   File \".../anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/llava_onevision.py\", line 828, in _process_video_pixels                                      \nERROR 03-18 13:40:00 engine.py:140]     num_videos, frames, c, h, w = video_pixel.shape                                                                                                                                                     \nERROR 03-18 13:40:00 engine.py:140]                                   ^^^^^^^^^^^^^^^^^                                                                                                                                                     \nERROR 03-18 13:40:00 engine.py:140] AttributeError: 'list' object has no attribute 'shape'   \n```\n\nI checked the var `pixel_values_videos` (in my local code it's `pixel_values`) in function `_parse_and_validate_video_input()` with different number of videos (1, 2 and 4). And I got this output:\n```shell\nlen(pixel_values)=1 # 1 video\nlen(pixel_values)=1, len(pixel_values[0])=2, pixel_values[0]=[tensor(...), tensor(...)] # 2 videos\nlen(pixel_values)=1, len(pixel_values[0])=4, pixel_values[0]=[tensor(...), tensor(...), tensor(...), tensor(...)] # 4 videos\n```\nIt ran successfully when I input only 1 video, but went wrong with 2 or more videos. Note that I didn't check `pixel_values[0]` with 1 video input because it's a list of `torch.Tensor` since it ran successfully.\n\nIt seems the problem is with `pixel_values_videos`. It's supposed to be a list of at least two `torch.Tensor` objects, but it turns out to be a list of a list of `torch.Tensor` objects.\n\nSo, I kind of feel that the change we made to the `_parse_and_validate_video_input()` function in #15021 might not be the right way to go. And I think we should try to find out what's wrong with `pixel_values_videos` before it's passed into the function. What do u think?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-19T03:19:23+00:00",
    "closed_at": "2025-03-20T11:24:46+00:00",
    "comments": 24,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15078/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15078"
  },
  {
    "number": 4989,
    "title": "[Bug]: Loading mistral-7B-instruct-v03 KeyError: 'layers.0.attention.wk.weight'",
    "body": "### Your current environment\n\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Rocky Linux 8.8 (Green Obsidian) (x86_64)\r\nGCC version: (GCC) 8.5.0 20210514 (Red Hat 8.5.0-20)\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.28\r\n\r\nPython version: 3.9.13 (main, Oct 13 2022, 21:15:33)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-4.18.0-513.9.1.el8_9.x86_64-x86_64-with-glibc2.28\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA H100 PCIe\r\nNvidia driver version: 535.129.03\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              256\r\nOn-line CPU(s) list: 0-255\r\nThread(s) per core:  2\r\nCore(s) per socket:  64\r\nSocket(s):           2\r\nNUMA node(s):        2\r\nVendor ID:           AuthenticAMD\r\nCPU family:          25\r\nModel:               17\r\nModel name:          AMD EPYC 9534 64-Core Processor\r\nStepping:            1\r\nCPU MHz:             2450.000\r\nCPU max MHz:         3718.0659\r\nCPU min MHz:         1500.0000\r\nBogoMIPS:            4900.22\r\nVirtualization:      AMD-V\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            1024K\r\nL3 cache:            32768K\r\nNUMA node0 CPU(s):   0-63,128-191\r\nNUMA node1 CPU(s):   64-127,192-255\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov succor smca fsrm flush_l1d\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] triton==2.3.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.4.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.4.0             pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tSYS\t5-6,133-134\t0\t\tN/A\r\nNIC0\tSYS\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_bond_0\r\n\n\n### \ud83d\udc1b Describe the bug\n\nI am trying to load the new Mistral 7B instruct v03 model. However, it gives `KeyError: 'layers.0.attention.wk.weight'`. Curiously it seems to use the llama model loader (see stack trace). I am not sure if that is intended.\r\n\r\n```text\r\nKeyError                                  Traceback (most recent call last)\r\nCell In[13], line 43\r\n     40 else:\r\n     41     raise ValueError(model)\r\n---> 43 llm = LLM(\r\n     44     model=model_path, \r\n     45     dtype=\"float16\",\r\n     46     max_model_len=max_model_len,\r\n     47     gpu_memory_utilization=gpu_memory_utilization,\r\n     48     **kwargs\r\n     49 )\r\n\r\nFile ~/miniconda/envs/project-experiments-py39-vllm3/lib/python3.9/site-packages/vllm/entrypoints/llm.py:123, in LLM.__init__(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)\r\n    102     kwargs[\"disable_log_stats\"] = True\r\n    103 engine_args = EngineArgs(\r\n    104     model=model,\r\n    105     tokenizer=tokenizer,\r\n   (...)\r\n    121     **kwargs,\r\n    122 )\r\n--> 123 self.llm_engine = LLMEngine.from_engine_args(\r\n    124     engine_args, usage_context=UsageContext.LLM_CLASS)\r\n    125 self.request_counter = Counter()\r\n\r\nFile ~/miniconda/envs/project-experiments-py39-vllm3/lib/python3.9/site-packages/vllm/engine/llm_engine.py:292, in LLMEngine.from_engine_args(cls, engine_args, usage_context)\r\n    289     executor_class = GPUExecutor\r\n    291 # Create the LLM engine.\r\n--> 292 engine = cls(\r\n    293     **engine_config.to_dict(),\r\n    294     executor_class=executor_class,\r\n    295     log_stats=not engine_args.disable_log_stats,\r\n    296     usage_context=usage_context,\r\n    297 )\r\n    298 return engine\r\n\r\nFile ~/miniconda/envs/project-experiments-py39-vllm3/lib/python3.9/site-packages/vllm/engine/llm_engine.py:160, in LLMEngine.__init__(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, vision_language_config, speculative_config, decoding_config, executor_class, log_stats, usage_context)\r\n    156 self.seq_counter = Counter()\r\n    157 self.generation_config_fields = _load_generation_config_dict(\r\n    158     model_config)\r\n--> 160 self.model_executor = executor_class(\r\n    161     model_config=model_config,\r\n    162     cache_config=cache_config,\r\n    163     parallel_config=parallel_config,\r\n    164     scheduler_config=scheduler_config,\r\n    165     device_config=device_config,\r\n    166     lora_config=lora_config,\r\n    167     vision_language_config=vision_language_config,\r\n    168     speculative_config=speculative_config,\r\n    169     load_config=load_config,\r\n    170 )\r\n    172 self._initialize_kv_caches()\r\n    174 # If usage stat is enabled, collect relevant info.\r\n\r\nFile ~/miniconda/envs/project-experiments-py39-vllm3/lib/python3.9/site-packages/vllm/executor/executor_base.py:41, in ExecutorBase.__init__(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, vision_language_config, speculative_config)\r\n     38 self.vision_language_config = vision_language_config\r\n     39 self.speculative_config = speculative_config\r\n---> 41 self._init_executor()\r\n\r\nFile ~/miniconda/envs/project-experiments-py39-vllm3/lib/python3.9/site-packages/vllm/executor/gpu_executor.py:23, in GPUExecutor._init_executor(self)\r\n     17 \"\"\"Initialize the worker and load the model.\r\n     18 \r\n     19 If speculative decoding is enabled, we instead create the speculative\r\n     20 worker.\r\n     21 \"\"\"\r\n     22 if self.speculative_config is None:\r\n---> 23     self._init_non_spec_worker()\r\n     24 else:\r\n     25     self._init_spec_worker()\r\n\r\nFile ~/miniconda/envs/project-experiments-py39-vllm3/lib/python3.9/site-packages/vllm/executor/gpu_executor.py:69, in GPUExecutor._init_non_spec_worker(self)\r\n     67 self.driver_worker = self._create_worker()\r\n     68 self.driver_worker.init_device()\r\n---> 69 self.driver_worker.load_model()\r\n\r\nFile ~/miniconda/envs/project-experiments-py39-vllm3/lib/python3.9/site-packages/vllm/worker/worker.py:118, in Worker.load_model(self)\r\n    117 def load_model(self):\r\n--> 118     self.model_runner.load_model()\r\n\r\nFile ~/miniconda/envs/project-experiments-py39-vllm3/lib/python3.9/site-packages/vllm/worker/model_runner.py:164, in ModelRunner.load_model(self)\r\n    162 def load_model(self) -> None:\r\n    163     with CudaMemoryProfiler() as m:\r\n--> 164         self.model = get_model(\r\n    165             model_config=self.model_config,\r\n    166             device_config=self.device_config,\r\n    167             load_config=self.load_config,\r\n    168             lora_config=self.lora_config,\r\n    169             vision_language_config=self.vision_language_config,\r\n    170             parallel_config=self.parallel_config,\r\n    171             scheduler_config=self.scheduler_config,\r\n    172         )\r\n    174     self.model_memory_usage = m.consumed_memory\r\n    175     logger.info(\"Loading model weights took %.4f GB\",\r\n    176                 self.model_memory_usage / float(2**30))\r\n\r\nFile ~/miniconda/envs/project-experiments-py39-vllm3/lib/python3.9/site-packages/vllm/model_executor/model_loader/__init__.py:19, in get_model(model_config, load_config, device_config, parallel_config, scheduler_config, lora_config, vision_language_config)\r\n     13 def get_model(\r\n     14         *, model_config: ModelConfig, load_config: LoadConfig,\r\n     15         device_config: DeviceConfig, parallel_config: ParallelConfig,\r\n     16         scheduler_config: SchedulerConfig, lora_config: Optional[LoRAConfig],\r\n     17         vision_language_config: Optional[VisionLanguageConfig]) -> nn.Module:\r\n     18     loader = get_model_loader(load_config)\r\n---> 19     return loader.load_model(model_config=model_config,\r\n     20                              device_config=device_config,\r\n     21                              lora_config=lora_config,\r\n     22                              vision_language_config=vision_language_config,\r\n     23                              parallel_config=parallel_config,\r\n     24                              scheduler_config=scheduler_config)\r\n\r\nFile ~/miniconda/envs/project-experiments-py39-vllm3/lib/python3.9/site-packages/vllm/model_executor/model_loader/loader.py:224, in DefaultModelLoader.load_model(self, model_config, device_config, lora_config, vision_language_config, parallel_config, scheduler_config)\r\n    221 with torch.device(device_config.device):\r\n    222     model = _initialize_model(model_config, self.load_config,\r\n    223                               lora_config, vision_language_config)\r\n--> 224 model.load_weights(\r\n    225     self._get_weights_iterator(model_config.model,\r\n    226                                model_config.revision,\r\n    227                                fall_back_to_pt=getattr(\r\n    228                                    model,\r\n    229                                    \"fall_back_to_pt_during_load\",\r\n    230                                    True)), )\r\n    231 for _, module in model.named_modules():\r\n    232     quant_method = getattr(module, \"quant_method\", None)\r\n\r\nFile ~/miniconda/envs/project-experiments-py39-vllm3/lib/python3.9/site-packages/vllm/model_executor/models/llama.py:415, in LlamaForCausalLM.load_weights(self, weights)\r\n    413 if name.endswith(\".bias\") and name not in params_dict:\r\n    414     continue\r\n--> 415 param = params_dict[name]\r\n    416 weight_loader = getattr(param, \"weight_loader\",\r\n    417                         default_weight_loader)\r\n    418 weight_loader(param, loaded_weight)\r\n\r\nKeyError: 'layers.0.attention.wk.weight'\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-05-22T18:52:58+00:00",
    "closed_at": "2024-05-24T13:38:02+00:00",
    "comments": 25,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4989/reactions",
      "total_count": 5,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 5
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4989"
  },
  {
    "number": 15597,
    "title": "[Bug]:vllm\u4ece0.7.0\u5f00\u59cb\u7248\u672c\u90e8\u7f72Qwen2_vl\u670d\u52a1\u5b58\u5728\u5185\u5b58(\u4e0d\u662fGPU\u663e\u5b58)\u6cc4\u6f0f\u95ee\u9898",
    "body": "### Your current environment\n\n<details>\n<summary>vllm\u4ece0.7.0\u5f00\u59cb\u7248\u672c\u90e8\u7f72Qwen2_vl\u670d\u52a1\u5b58\u5728\u5185\u5b58(\u4e0d\u662fGPU\u663e\u5b58)\u6cc4\u6f0f\u95ee\u9898</summary>\n\n```text\n\n\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n\u4f7f\u75280.7.0\u7248\u672c\u7684vllm\u90e8\u7f72Qwen2_vl\u6a21\u578b\u670d\u52a1\u65f6\uff0c\u5bf9\u670d\u52a1\u8fdb\u884c\u8bf7\u6c42\u540e\uff0c\u670d\u52a1\u76f8\u5173\u8fdb\u7a0b\u5185\u5b58\u4e0d\u4f1a\u91ca\u653e\uff0c\u6700\u7ec8\u6253\u7206\u670d\u52a1\u5668\u5185\u5b58\uff0c\u5bfc\u81f4\u670d\u52a1\u505c\u6b62\uff0c\u6d4b\u8bd5\u53d1\u73b00.6.6\u7248\u672c\u65e0\u6b64\u95ee\u9898\uff0c0.7.0\u53ca\u4ee5\u4e0a\u7248\u672c\u5747\u6709\u6b64\u95ee\u9898\u3002\n\n<img width=\"625\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e588c75e-efb7-4e49-872b-4dadba95e06c\" />\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-27T04:45:50+00:00",
    "closed_at": "2025-03-27T07:13:46+00:00",
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15597/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15597"
  },
  {
    "number": 5569,
    "title": "[Bug]: BitsandBytes quantization is not working as expected",
    "body": "### Your current environment\n\n```text\r\n$ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.5\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: Tesla T4\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             72\r\nOn-line CPU(s) list:                0-71\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Gold 6154 CPU @ 3.00GHz\r\nCPU family:                         6\r\nModel:                              85\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 18\r\nSocket(s):                          2\r\nStepping:                           4\r\nBogoMIPS:                           6000.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          1.1 MiB (36 instances)\r\nL1i cache:                          1.1 MiB (36 instances)\r\nL2 cache:                           36 MiB (36 instances)\r\nL3 cache:                           49.5 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70\r\nNUMA node1 CPU(s):                  1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71\r\nVulnerability Gather data sampling: Mitigation; Microcode\r\nVulnerability Itlb multihit:        KVM: Mitigation: VMX disabled\r\nVulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable\r\nVulnerability Mds:                  Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Meltdown:             Mitigation; PTI\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:             Mitigation; IBRS\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; IBRS, IBPB conditional, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Mitigation; Clear CPU buffers; SMT vulnerable\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] transformers==4.41.2\r\n[pip3] triton==2.3.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] transformers              4.41.2                   pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      1,3,5,7,9,11    1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nWith the latest bitsandbytes quantization feature, the official [Llama3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) produces garbage.\r\n\r\nStart the server:\r\n\r\n```shell\r\n$ python -m vllm.entrypoints.openai.api_server --dtype half --served-model-name llama3-8b --model /models/Meta-Llama-3-8B-Instruct --load-format bitsandbytes --quantization bitsandbytes\r\nINFO 06-15 14:33:24 api_server.py:177] vLLM API server version 0.5.0.post1\r\nINFO 06-15 14:33:24 api_server.py:178] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, chat_template=None, response_role='assistant',\r\nssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='/models/Meta-Llama-3-8B-Instruct', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust\r\n_remote_code=False, download_dir=None, load_format='bitsandbytes', dtype='half', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tens\r\nor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.9, num_gpu_blocks_ov\r\nerride=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization='bitsandbytes', rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=F\r\nalse, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, device='auto\r\n', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, image_processor=None, image_processor_revision=None, disable_image_processor=False, scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, num_speculative\r\n_tokens=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, model_loader_extra_config=None, preemption_mode=None, served_model_name=['llama3-8b'], qlora_adapter_name_or_path=None, engine_use_\r\nray=False, disable_log_requests=False, max_log_len=None)\r\nWARNING 06-15 14:33:24 config.py:1222] Casting torch.bfloat16 to torch.float16.\r\nWARNING 06-15 14:33:24 config.py:217] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\nINFO 06-15 14:33:24 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='/models/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='/models/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_sc\r\naling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False\r\n, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=llama3-8b)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nINFO 06-15 14:33:25 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\nINFO 06-15 14:33:25 selector.py:51] Using XFormers backend.\r\nINFO 06-15 14:33:26 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\nINFO 06-15 14:33:26 selector.py:51] Using XFormers backend.\r\nINFO 06-15 14:33:26 loader.py:744] Loading weights with BitsAndBytes quantization.  May take a while ...\r\nINFO 06-15 14:33:32 model_runner.py:160] Loading model weights took 5.3128 GB\r\nINFO 06-15 14:34:17 gpu_executor.py:83] # GPU blocks: 2595, # CPU blocks: 2048\r\nINFO 06-15 14:34:19 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 06-15 14:34:19 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\nINFO 06-15 14:35:47 model_runner.py:965] Graph capturing finished in 88 secs.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nINFO 06-15 14:35:48 serving_chat.py:92] Using default chat template:\r\nINFO 06-15 14:35:48 serving_chat.py:92] {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\r\nINFO 06-15 14:35:48 serving_chat.py:92]\r\nINFO 06-15 14:35:48 serving_chat.py:92] '+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\r\nINFO 06-15 14:35:48 serving_chat.py:92]\r\nINFO 06-15 14:35:48 serving_chat.py:92] ' }}{% endif %}\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nWARNING 06-15 14:35:48 serving_embedding.py:141] embedding_mode is False. Embedding API will not work.\r\nINFO:     Started server process [2622103]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\r\n```\r\n\r\nTest the service:\r\n\r\n```shell\r\n$ curl localhost:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{\"model\": \"llama3-8b\", \"messages\": [{\"role\": \"user\", \"content\": \"Hi!\"}], \"max_tokens\": 128}'\r\n{\"id\":\"cmpl-5a7d1b331b8345f88433fbaf0da9c7e2\",\"object\":\"chat.completion\",\"created\":1718460912,\"model\":\"llama3-8b\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\" the!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\",\"tool_calls\":[]},\"logprobs\":null,\"finish_reason\":\"length\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":12,\"total_tokens\":140,\"completion_tokens\":128}}\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-06-15T14:20:10+00:00",
    "closed_at": "2024-07-27T02:08:24+00:00",
    "comments": 32,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5569/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/5569"
  },
  {
    "number": 9991,
    "title": "[Bug]: Llama3.2 tool calling OpenAI API not working",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...                                                                                                                                                                                                                                                           \r\nWARNING 11-04 11:59:16 cuda.py:81] Detected different devices in the system:                                                                                                                                                                                                                    \r\nWARNING 11-04 11:59:16 cuda.py:81] NVIDIA A100 80GB PCIe                                                                                                                                                                                                                                        \r\nWARNING 11-04 11:59:16 cuda.py:81] NVIDIA GeForce RTX 4090                                                                                                                                                                                                                                      \r\nWARNING 11-04 11:59:16 cuda.py:81] NVIDIA GeForce RTX 4090                                                                                                                                                                                                                                      \r\nWARNING 11-04 11:59:16 cuda.py:81] NVIDIA GeForce RTX 4090                                                                                                                                                                                                                                      \r\nWARNING 11-04 11:59:16 cuda.py:81] Please make sure to set `CUDA_DEVICE_ORDER=PCI_BUS_ID` to avoid unexpected behavior.                                                                                                                                                                         \r\nPyTorch version: 2.5.0+cu124                                                                                                                                                                                                                                                                    \r\nIs debug build: False                                                                                                                                                                                                                                                                           \r\nCUDA used to build PyTorch: 12.4                                                                                                                                                                                                                                                                \r\nROCM used to build PyTorch: N/A                                                                                                                                                                                                                                                                 \r\n                                                                                                                                                                                                                                                                                                \r\nOS: Ubuntu 22.04.4 LTS (x86_64)                                                                                                                                                                                                                                                                 \r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0                                                                                                                                                                                                                                              \r\nClang version: Could not collect                                                                                                                                                                                                                                                                \r\nCMake version: version 3.30.0                                                                                                                                                                                                                                                                   \r\nLibc version: glibc-2.35                                                                                                                                                                                                                                                                        \r\n                                                                                                                                                                                                                                                                                                \r\nPython version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64-bit runtime)                                                                                                                                                                                                             \r\nPython platform: Linux-5.15.0-122-generic-x86_64-with-glibc2.35                                                                                                                                                                                                                                 \r\nIs CUDA available: True                                                                                                                                                                                                                                                                         \r\nCUDA runtime version: Could not collect                                                                                                                                                                                                                                                         \r\nCUDA_MODULE_LOADING set to: LAZY                                                                                                                                                                                                                                                                \r\nGPU models and configuration:                                                                                                                                                                                                                                                                   \r\nGPU 0: NVIDIA A100 80GB PCIe                                                                                                                                                                                                                                                                    \r\nGPU 1: NVIDIA GeForce RTX 4090                                                                                                                                                                                                                                                                  \r\nGPU 2: NVIDIA GeForce RTX 4090                                                                                                                                                                                                                                                                  \r\nGPU 3: NVIDIA GeForce RTX 4090                                                                                                                                                                                                                                                                  \r\n                                                                                                                                                                                                                                                                                                \r\nNvidia driver version: 535.183.01                                                                                                                                                                                                                                                               \r\ncuDNN version: Could not collect                                                                                                                                                                                                                                                                \r\nHIP runtime version: N/A                                                                                                                                                                                                                                                                        \r\nMIOpen runtime version: N/A                                                                                                                                                                                                                                                                     \r\nIs XNNPACK available: True                                                                                                                                                                                                                                                                      \r\n                                                                                                                                                                                                                                                                                                \r\nCPU:                                                                                                                                                                                                                                                                                            \r\nArchitecture:                         x86_64                                                                                                                                                                                                                                                    \r\nCPU op-mode(s):                       32-bit, 64-bit                                                                                                                                                                                                                                            \r\nAddress sizes:                        46 bits physical, 48 bits virtual                                                                                                                                                                                                                         \r\nByte Order:                           Little Endian                                                                                                                                                                                                                                             \r\nCPU(s):                               32                                                                                                                                                                                                                                                        \r\nOn-line CPU(s) list:                  0-31                                                                                                                                                                                                                                                      \r\nVendor ID:                            GenuineIntel                                                                                                                                                                                                                                              \r\nModel name:                           13th Gen Intel(R) Core(TM) i9-13900KS                                                                                                                                                                                                                     \r\nCPU family:                           6                                                                                                                                                                                                                                                         \r\nModel:                                183                                                                                                                                                                                                                                                       \r\nThread(s) per core:                   2                                                                                                                                                                                                                                                         \r\nCore(s) per socket:                   24                                                                                                                                                                                                                                                        \r\nSocket(s):                            1                                                                                                                                                                                                                                                         \r\nStepping:                             1                                                                                                                                                                                                                                                         \r\nCPU max MHz:                          6000.0000                                                                                                                                                                                                                                                 \r\nCPU min MHz:                          800.0000                                                                                                                                                                                                                                                  \r\nBogoMIPS:                             6374.40                                                                                                                                                                                                                                                   \r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known\r\n_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority\r\n ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req umip pku ospke waitpkg gfni vaes vpclm\r\nulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr flush_l1d arch_capabilities                                                                                                                                                                                          \r\nVirtualization:                       VT-x\r\nL1d cache:                            896 KiB (24 instances)\r\nL1i cache:                            1.3 MiB (24 instances)\r\nL2 cache:                             32 MiB (12 instances)\r\nL3 cache:                             36 MiB (1 instance)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-31\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Mitigation; Clear Register File\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.2\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.0.3\r\n[pip3] torch==2.5.0\r\n[pip3] torchvision==0.20.0\r\n[pip3] transformers==4.46.0\r\n[pip3] triton==3.1.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\r\n[conda] pyzmq                     26.0.3                   pypi_0    pypi\r\n[conda] torch                     2.5.0                    pypi_0    pypi\r\n[conda] torchvision               0.20.0                   pypi_0    pypi\r\n[conda] transformers              4.46.0                   pypi_0    pypi\r\n[conda] triton                    3.1.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.3.post2.dev139+g622b7ab9\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PHB     PHB     PHB     0-31    0               N/A\r\nGPU1    PHB      X      PHB     PHB     0-31    0               N/A\r\nGPU2    PHB     PHB      X      PHB     0-31    0               N/A\r\nGPU3    PHB     PHB     PHB      X      0-31    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nWhen trying to run Llama3.2 tool calling via `python -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-3.2-1B-Instruct --enable-auto-tool-choice --tool-call-parser llama3_json` I do not get the OpenAI API function calling functionality but rather just get the tool call string:\r\n```\r\n\u276f current time?\r\n<|python_tag|>{\"type\": \"function\", \"function\": \"get_time\", \"parameters\": {\"timezone\":\r\n\"America/New_York\"}}\r\n```\r\nUsing the official OpenAI API with 4o and Ollama works with my code\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-11-04T11:04:00+00:00",
    "closed_at": "2024-11-14T04:14:36+00:00",
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9991/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9991"
  },
  {
    "number": 10459,
    "title": "[Installation]: VLLM on ARM machine with GH200",
    "body": "### Your current environment\n\n(I can not run collect_env since it requires VLLM installed)\r\n\r\n```text\r\n$ pip freeze\r\ncertifi==2022.12.7\r\ncharset-normalizer==2.1.1\r\nfilelock==3.16.1\r\nfsspec==2024.10.0\r\nidna==3.4\r\nJinja2==3.1.4\r\nMarkupSafe==3.0.2\r\nmpmath==1.3.0\r\nnetworkx==3.4.2\r\nnumpy==2.1.3\r\npillow==10.2.0\r\npynvml==11.5.3\r\nrequests==2.28.1\r\nsympy==1.13.1\r\ntorch==2.5.1\r\ntyping_extensions==4.12.2\r\nurllib3==1.26.13\r\n\r\n$ lsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 22.04.4 LTS\r\nRelease:        22.04\r\nCodename:       jammy\r\n```\r\n\r\nI have an ARM CPU and a NVIDIA GH200 Driver Version: 550.90.07 CUDA Version: 12.4.\n\n### How you are installing vllm\n\n```sh\r\npip install torch numpy\r\npip install vllm\r\n```\r\n\r\nI get this error:\r\n```sh\r\npip install vllm\r\nCollecting vllm\r\n  Using cached vllm-0.6.4.post1.tar.gz (3.1 MB)\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Getting requirements to build wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [18 lines of output]\r\n      /tmp/pip-build-env-8t3z_6ag/overlay/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\r\n        cpu = _conversion_method_template(device=torch.device(\"cpu\"))\r\n      Traceback (most recent call last):\r\n        File \"/hpi/fs00/home/philipp.hildebrandt/armpython/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\r\n          main()\r\n        File \"/hpi/fs00/home/philipp.hildebrandt/armpython/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\r\n          json_out['return_val'] = hook(**hook_input['kwargs'])\r\n        File \"/hpi/fs00/home/philipp.hildebrandt/armpython/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\r\n          return hook(config_settings)\r\n        File \"/tmp/pip-build-env-8t3z_6ag/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 334, in get_requires_for_build_wheel\r\n          return self._get_build_requires(config_settings, requirements=[])\r\n        File \"/tmp/pip-build-env-8t3z_6ag/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 304, in _get_build_requires\r\n          self.run_setup()\r\n        File \"/tmp/pip-build-env-8t3z_6ag/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 320, in run_setup\r\n          exec(code, locals())\r\n        File \"<string>\", line 526, in <module>\r\n        File \"<string>\", line 433, in get_vllm_version\r\n      RuntimeError: Unknown runtime environment\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: subprocess-exited-with-error\r\n\r\n\u00d7 Getting requirements to build wheel did not run successfully.\r\n\u2502 exit code: 1\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This error originates from a subprocess, and is likely not a problem with pip.\r\n```\r\n\r\nI thought numpy was missing or there was some problem with torch, which is why I manually installed numpy and torch in a fresh venv before trying this again. Torch has cuda available, but the error looks like VLLM might be trying to use a CPU backend. I tried manually installing pynvml, but it did not change anything.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "open",
    "created_at": "2024-11-19T16:57:34+00:00",
    "closed_at": null,
    "comments": 28,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10459/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10459"
  },
  {
    "number": 1234,
    "title": "Running out of memory loading 7B AWQ quantized models with 12GB vram",
    "body": "Hi, \r\n\r\ni am trying to make use of the AWQ quantization to try to load 7B LLama based models onto my RTX 3060 with 12 GB.\r\nThis fails OOM for models like https://huggingface.co/TheBloke/leo-hessianai-7B-AWQ .\r\nI was able to load https://huggingface.co/TheBloke/tulu-7B-AWQ with its 2k seq length taking up 11.2GB of my ram.\r\n\r\nMy expectation was that these 7B models with AWQ quantization with GEMM would need for inference around ~ 3.5 gB to load.\r\n\r\nI tried to load the models from within my app using vLLM as a lib and following Brokes instructions with\r\n```\r\npython -m vllm.entrypoints.api_server --model TheBloke/tulu-7B-AWQ --quantization awq\r\n```\r\n\r\nDo I miss something here?\r\n\r\nThx,\r\nManuel",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-09-30T18:09:45+00:00",
    "closed_at": "2024-12-01T02:16:11+00:00",
    "comments": 25,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1234/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1234"
  },
  {
    "number": 15901,
    "title": "[SpecDecode] Support EAGLE in V1",
    "body": "- [x] 1. Correctly initializing and loading the EAGLE draft model\n- [x] 2. Consider the lookahead slots in the KV cache manager\n- [x] 3. Cache `draft_probs` inside the model runner and correctly feed it to the rejection sampler in the next step (temporarily workaround: #16899)\n- [x] 4. Handle the edge cases like when the draft model generates beyond `max_pos_embeddings`\n- [ ] 5. Handle the seeds correctly\n- [ ] 6. Do E2E correctness and performance tests\n- [x] 7. Support prefix caching. Eagle requires special handling because Eagle's i-th KV cache is coupled with the i+1-th token ID. (@LiuXiaoxuanPKU)\n- [ ] 8. Properly handle the sampling parameters that are not (currently) compatible with spec decoding (e.g., min_p).\n- [x] 9. Use CUDA graphs for draft model. (@luyuzhe111)\n- [x] 10. Support Eagle 3 (https://github.com/vllm-project/vllm/pull/16937)\n\n_Originally posted by @WoosukKwon in https://github.com/vllm-project/vllm/issues/15729#issuecomment-2765192455_\n            ",
    "labels": [
      "speculative-decoding",
      "v1"
    ],
    "state": "open",
    "created_at": "2025-04-01T19:45:13+00:00",
    "closed_at": null,
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15901/reactions",
      "total_count": 11,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 10,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15901"
  },
  {
    "number": 16570,
    "title": "[Bug]: qwen2.5-vl-72b oom in 4 A100 in 0.8.3",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\n- Platform: Linux-5.15.0-56-generic-x86_64-with-glibc2.35\n- Python version: 3.12.3\n- PyTorch version: 2.6.0+cu124 (GPU)\n- Transformers version: 4.50.3\n- Datasets version: 3.2.0\n- Accelerate version: 1.2.1\n- PEFT version: 0.15.0\n- TRL version: 0.9.6\n- GPU type: NVIDIA A100-PCIE-40GB\n- GPU number: 8\n- GPU memory: 39.38GB\n- vLLM version: 0.8.3\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nVLLM_WORKER_MULTIPROC_METHOD=spawn python -m vllm.entrypoints.openai.api_server \\\n--dtype auto \\\n--port 8009 \\\n--trust-remote-code \\\n--served-model-name qwen2vl \\\n--model /home/ps/data/pretrained_model/Qwen/Qwen2.5-VL-72B-Instruct/ \\\n--tensor-parallel-size 4 \\\n--gpu_memory_utilization 0.95 \\\n--max_num_seqs 2 \\\n--max_model_len 8192 \\\n--mm_processor_kwargs '{\"max_pixels\":1280, \"min_pixels\":256}'\n\nabove is my running code. I find that it caused 36GB memory each card, but i don't know the later process and vllm caused oom error. Can anyone know why? Thank you very much\n\n```text\n(VllmWorker rank=1 pid=52940) INFO 04-14 14:23:05 [loader.py:447] Loading weights took 20.16 seconds\n(VllmWorker rank=2 pid=52983) INFO 04-14 14:23:05 [loader.py:447] Loading weights took 20.27 seconds\n(VllmWorker rank=1 pid=52940) INFO 04-14 14:23:05 [gpu_model_runner.py:1273] Model loading took 34.4340 GiB and 20.495027 seconds\n(VllmWorker rank=3 pid=53006) INFO 04-14 14:23:05 [loader.py:447] Loading weights took 20.42 seconds\nLoading safetensors checkpoint shards:  97% Completed | 37/38 [00:20<00:00,  1.98it/s]\n(VllmWorker rank=2 pid=52983) INFO 04-14 14:23:05 [gpu_model_runner.py:1273] Model loading took 34.4340 GiB and 20.602629 seconds\n(VllmWorker rank=3 pid=53006) INFO 04-14 14:23:05 [gpu_model_runner.py:1273] Model loading took 34.4340 GiB and 20.795247 seconds\nLoading safetensors checkpoint shards: 100% Completed | 38/38 [00:20<00:00,  2.02it/s]\nLoading safetensors checkpoint shards: 100% Completed | 38/38 [00:20<00:00,  1.81it/s]\n(VllmWorker rank=0 pid=52924)\n(VllmWorker rank=0 pid=52924) INFO 04-14 14:23:06 [loader.py:447] Loading weights took 21.01 seconds\n(VllmWorker rank=0 pid=52924) INFO 04-14 14:23:06 [gpu_model_runner.py:1273] Model loading took 34.4340 GiB and 21.353455 seconds\n(VllmWorker rank=3 pid=53006) INFO 04-14 14:23:06 [gpu_model_runner.py:1542] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 256 video items o\nf the maximum feature size.\n(VllmWorker rank=1 pid=52940) INFO 04-14 14:23:06 [gpu_model_runner.py:1542] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 256 video items o\nf the maximum feature size.\n(VllmWorker rank=2 pid=52983) INFO 04-14 14:23:06 [gpu_model_runner.py:1542] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 256 video items o\nf the maximum feature size.\n(VllmWorker rank=0 pid=52924) INFO 04-14 14:23:06 [gpu_model_runner.py:1542] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 256 video items o\nf the maximum feature size.\n(VllmWorker rank=2 pid=52983) INFO 04-14 14:23:29 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/9cfb59d003/rank_2_0 for vLLM's torch.compile\n(VllmWorker rank=2 pid=52983) INFO 04-14 14:23:29 [backends.py:426] Dynamo bytecode transform time: 18.25 s\n(VllmWorker rank=1 pid=52940) INFO 04-14 14:23:29 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/9cfb59d003/rank_1_0 for vLLM's torch.compile\n(VllmWorker rank=1 pid=52940) INFO 04-14 14:23:29 [backends.py:426] Dynamo bytecode transform time: 18.55 s\n(VllmWorker rank=0 pid=52924) INFO 04-14 14:23:29 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/9cfb59d003/rank_0_0 for vLLM's torch.compile\n(VllmWorker rank=0 pid=52924) INFO 04-14 14:23:29 [backends.py:426] Dynamo bytecode transform time: 18.66 s\n(VllmWorker rank=3 pid=53006) INFO 04-14 14:23:29 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/9cfb59d003/rank_3_0 for vLLM's torch.compile\n(VllmWorker rank=3 pid=53006) INFO 04-14 14:23:29 [backends.py:426] Dynamo bytecode transform time: 18.80 s\n(VllmWorker rank=2 pid=52983) INFO 04-14 14:23:30 [backends.py:115] Directly load the compiled graph for shape None from the cache\n(VllmWorker rank=1 pid=52940) INFO 04-14 14:23:30 [backends.py:115] Directly load the compiled graph for shape None from the cache\n(VllmWorker rank=3 pid=53006) INFO 04-14 14:23:30 [backends.py:115] Directly load the compiled graph for shape None from the cache\n(VllmWorker rank=0 pid=52924) INFO 04-14 14:23:31 [backends.py:115] Directly load the compiled graph for shape None from the cache\n(VllmWorker rank=1 pid=52940) INFO 04-14 14:23:48 [monitor.py:33] torch.compile takes 18.55 s in total\n(VllmWorker rank=2 pid=52983) INFO 04-14 14:23:48 [monitor.py:33] torch.compile takes 18.25 s in total\n(VllmWorker rank=0 pid=52924) INFO 04-14 14:23:48 [monitor.py:33] torch.compile takes 18.66 s in total\n(VllmWorker rank=3 pid=53006) INFO 04-14 14:23:48 [monitor.py:33] torch.compile takes 18.80 s in total\nINFO 04-14 14:23:49 [kv_cache_utils.py:578] GPU KV cache size: 9,168 tokens\nINFO 04-14 14:23:49 [kv_cache_utils.py:581] Maximum concurrency for 8,192 tokens per request: 1.12x\nINFO 04-14 14:23:49 [kv_cache_utils.py:578] GPU KV cache size: 9,072 tokens\nINFO 04-14 14:23:49 [kv_cache_utils.py:581] Maximum concurrency for 8,192 tokens per request: 1.11x\nINFO 04-14 14:23:49 [kv_cache_utils.py:578] GPU KV cache size: 9,072 tokens\nINFO 04-14 14:23:49 [kv_cache_utils.py:581] Maximum concurrency for 8,192 tokens per request: 1.11x\nINFO 04-14 14:23:49 [kv_cache_utils.py:578] GPU KV cache size: 9,584 tokens\nINFO 04-14 14:23:49 [kv_cache_utils.py:581] Maximum concurrency for 8,192 tokens per request: 1.17x\n(VllmWorker rank=2 pid=52983) INFO 04-14 14:24:52 [gpu_model_runner.py:1608] Graph capturing finished in 62 secs, took 2.65 GiB\n(VllmWorker rank=0 pid=52924) INFO 04-14 14:24:52 [gpu_model_runner.py:1608] Graph capturing finished in 62 secs, took 2.65 GiB\n(VllmWorker rank=1 pid=52940) INFO 04-14 14:24:52 [gpu_model_runner.py:1608] Graph capturing finished in 62 secs, took 2.65 GiB\n(VllmWorker rank=3 pid=53006) INFO 04-14 14:24:52 [gpu_model_runner.py:1608] Graph capturing finished in 62 secs, took 2.65 GiB\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383] WorkerProc hit an exception: %s\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383] Traceback (most recent call last):\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]   File \"/data/ps/vllm/venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1466, in _dummy_sampler_run\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]     sampler_output = self.model.sample(\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]                      ^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]   File \"/data/ps/vllm/venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2_5_vl.py\", line 1107, in sample\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]     return self.language_model.sample(logits, sampling_metadata)\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]   File \"/data/ps/vllm/venv/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py\", line 480, in sample\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]     next_tokens = self.sampler(logits, sampling_metadata)\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]   File \"/data/ps/vllm/venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]     return self._call_impl(*args, **kwargs)\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]   File \"/data/ps/vllm/venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]     return forward_call(*args, **kwargs)\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]   File \"/data/ps/vllm/venv/lib/python3.12/site-packages/vllm/v1/sample/sampler.py\", line 49, in forward\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]     sampled = self.sample(logits, sampling_metadata)\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]   File \"/data/ps/vllm/venv/lib/python3.12/site-packages/vllm/v1/sample/sampler.py\", line 115, in sample\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]     random_sampled = self.topk_topp_sampler(\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]                      ^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]   File \"/data/ps/vllm/venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]     return self._call_impl(*args, **kwargs)\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]   File \"/data/ps/vllm/venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]     return forward_call(*args, **kwargs)\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]   File \"/data/ps/vllm/venv/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 98, in forward_native\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]     logits = apply_top_k_top_p(logits, k, p)\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]   File \"/data/ps/vllm/venv/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 186, in apply_top_k_top_p\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=52940) ERROR 04-14 14:24:52 [multiproc_executor.py:383] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 1 has a total capacity of 39.38 GiB of which 65.38 MiB is free. Including non-PyTorch memory, this process has 39.31 GiB memory in use. Of the allocated memory 35.91 GiB is allocated by PyTorch, with 73.88 MiB allocated in private pools (e.g., CUDA Graphs), and 120.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-04-14T06:25:47+00:00",
    "closed_at": "2025-04-16T10:29:50+00:00",
    "comments": 37,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16570/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16570"
  },
  {
    "number": 13655,
    "title": "[Bug]: Qwen2.5 VL Internal Server Error",
    "body": "### Your current environment\n\nI used the official docker image v0.7.2 and reinstalled vllm with commit d0a7a2769d92619afdcdc3b91c78098eaa9e38c0 and trainsformers 4.49.0.\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\n/usr/local/lib/python3.12/dist-packages/vllm/__init__.py:5: RuntimeWarning: Failed to read commit hash:\nNo module named 'vllm._version'\n  from .version import __version__, __version_tuple__  # isort:skip\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.31.4\nLibc version: glibc-2.35\n\nPython version: 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.10.0-32-amd64-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA A40\nGPU 1: NVIDIA A40\nGPU 2: NVIDIA A40\nGPU 3: NVIDIA A40\n\nNvidia driver version: 560.35.05\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        43 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               128\nOn-line CPU(s) list:                  0-127\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 7742 64-Core Processor\nCPU family:                           23\nModel:                                49\nThread(s) per core:                   1\nCore(s) per socket:                   64\nSocket(s):                            2\nStepping:                             0\nFrequency boost:                      enabled\nCPU max MHz:                          3414.5500\nCPU min MHz:                          1500.0000\nBogoMIPS:                             4491.63\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip rdpid overflow_recov succor smca sme sev sev_es\nVirtualization:                       AMD-V\nL1d cache:                            4 MiB (128 instances)\nL1i cache:                            4 MiB (128 instances)\nL2 cache:                             64 MiB (128 instances)\nL3 cache:                             512 MiB (32 instances)\nNUMA node(s):                         8\nNUMA node0 CPU(s):                    0-15\nNUMA node1 CPU(s):                    16-31\nNUMA node2 CPU(s):                    32-47\nNUMA node3 CPU(s):                    48-63\nNUMA node4 CPU(s):                    64-79\nNUMA node5 CPU(s):                    80-95\nNUMA node6 CPU(s):                    96-111\nNUMA node7 CPU(s):                    112-127\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT disabled\nVulnerability Spec rstack overflow:   Mitigation; SMT disabled\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines, IBPB conditional, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.49.0\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: N/A (dev)\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tGPU1\tGPU2\tGPU3\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tSYS\tSYS\tSYS\tSYS\t48-63\t3\t\tN/A\nGPU1\tSYS\t X \tSYS\tSYS\tSYS\t16-31\t1\t\tN/A\nGPU2\tSYS\tSYS\t X \tSYS\tSYS\t112-127\t7\t\tN/A\nGPU3\tSYS\tSYS\tSYS\t X \tPHB\t80-95\t5\t\tN/A\nNIC0\tSYS\tSYS\tSYS\tPHB\t X \t\t\t\t\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_bond_0\n\nNVIDIA_VISIBLE_DEVICES=all\nNVIDIA_REQUIRE_CUDA=cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\nVLLM_ALLOW_LONG_MAX_MODEL_LEN=1\nNCCL_VERSION=2.17.1-1\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nNVIDIA_CUDA_END_OF_LIFE=1\nCUDA_VERSION=12.1.0\nVLLM_TRACE_FUNCTION=1\nLD_LIBRARY_PATH=/usr/local/lib/python3.12/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nVLLM_USE_V1=0\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nvllm startup command:\n\n```bash\npython3 -m vllm.entrypoints.openai.api_server \\\n --model /model/Qwen2.5-VL-72B-Instruct \\\n --port 23333 \\\n --trust-remote-code \\\n --tensor-parallel-size 4 \\\n --max-model-len 20000 \\\n --max-num-batched-tokens 20000 \\\n --max-num-seqs 1 \\\n --max-logprobs 0 \\\n --enforce-eager \\\n --gpu-memory-utilization 0.95\n\n```\n\nThe test input image is a 500*19 png.\n<details>\n<summary>The test script</summary>\n\n```bash\ncurl  http://10.44.128.25:23333/v1/chat/completions \\\n -H \"Content-Type: application/json\" \\\n -d '{\"model\":\"/model/Qwen2.5-VL-72B-Instruct\",\"messages\":[{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/jpeg;base64,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\"}}]}]}'\n```\n\n</details>\n\n\n\nThe server log:\n```text\nINFO 02-20 23:56:14 logger.py:39] Received request chatcmpl-c10b409ab01b4abbaca5fa25c3d3d808: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\nHello<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19976, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\nINFO:     10.44.128.16:52852 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\nINFO 02-20 23:56:14 engine.py:293] Aborted request chatcmpl-c10b409ab01b4abbaca5fa25c3d3d808.\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-02-21T08:07:01+00:00",
    "closed_at": "2025-07-01T02:58:45+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13655/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13655"
  },
  {
    "number": 7494,
    "title": "[Bug]: DeepSeek-Coder-V2-Instruct-AWQ    assert self.quant_method is not None",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\n```text\r\nollecting environment information...\r\nPyTorch version: 2.3.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\r\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\r\nClang version: Could not collect\r\nCMake version: version 3.30.2\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.2 (default, Feb 28 2021, 17:03:44)  [GCC 10.2.1 20210110] (64-bit runtime)\r\nPython platform: Linux-5.4.143.bsk.8-amd64-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA L40\r\nGPU 1: NVIDIA L40\r\nGPU 2: NVIDIA L40\r\nGPU 3: NVIDIA L40\r\nGPU 4: NVIDIA L40\r\nGPU 5: NVIDIA L40\r\nGPU 6: NVIDIA L40\r\nGPU 7: NVIDIA L40\r\n\r\nNvidia driver version: Could not collect\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   52 bits physical, 57 bits virtual\r\nCPU(s):                          180\r\nOn-line CPU(s) list:             0-179\r\nThread(s) per core:              2\r\nCore(s) per socket:              45\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           143\r\nModel name:                      Intel(R) Xeon(R) Platinum 8457C\r\nStepping:                        8\r\nCPU MHz:                         2599.044\r\nBogoMIPS:                        5198.08\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       4.2 MiB\r\nL1i cache:                       2.8 MiB\r\nL2 cache:                        180 MiB\r\nL3 cache:                        195 MiB\r\nNUMA node0 CPU(s):               0-89\r\nNUMA node1 CPU(s):               90-179\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Mitigation; TSX disabled\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 wbnoinvd arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid cldemote movdiri movdir64b md_clear arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] byted-torch==2.1.0.post2\r\n[pip3] flashinfer==0.0.8+cu121torch2.3\r\n[pip3] numpy==1.26.2\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] pyzmq==26.1.0\r\n[pip3] torch==2.3.1\r\n[pip3] torchaudio==2.1.0+cu121\r\n[pip3] torchvision==0.18.1\r\n[pip3] transformers==4.44.0\r\n[pip3] triton==2.3.1\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.3.post1@38c4b7e863570a045308af814c72f4504297222e\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     2-89    0               N/A\r\nGPU1    NODE     X      NODE    NODE    SYS     SYS     SYS     SYS     SYS     2-89    0               N/A\r\nGPU2    NODE    NODE     X      NODE    SYS     SYS     SYS     SYS     SYS     2-89    0               N/A\r\nGPU3    NODE    NODE    NODE     X      SYS     SYS     SYS     SYS     SYS     2-89    0               N/A\r\nGPU4    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     92-177  1               N/A\r\nGPU5    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     92-177  1               N/A\r\nGPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS     92-177  1               N/A\r\nGPU7    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     92-177  1               N/A\r\nNIC0    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n```\r\n\r\n```\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nUsing the ds-coder-v2-awq [,](https://huggingface.co/casperhansen/deepseek-coder-v2-instruct-awq) the following error is reported.\r\n\r\nTraceback (most recent call last):\r\n[rank0]:   File \"/opt/tiger/deepseek_http/vllm_server.py\", line 134, in <module>\r\n[rank0]:     engine = AsyncLLMEngine.from_engine_args(engine_args)\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/engine/async_llm_engine.py\", line 466, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/engine/async_llm_engine.py\", line 380, in __init__\r\n[rank0]:     self.engine = self._init_engine(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/engine/async_llm_engine.py\", line 547, in _init_engine\r\n[rank0]:     return engine_class(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/engine/llm_engine.py\", line 251, in __init__\r\n[rank0]:     self.model_executor = executor_class(\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 201, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 25, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/executor/executor_base.py\", line 47, in __init__\r\n[rank0]:     self._init_executor()\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 124, in _init_executor\r\n[rank0]:     self._run_workers(\"load_model\",\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 178, in _run_workers\r\n[rank0]:     driver_worker_output = driver_worker_method(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/worker/worker.py\", line 139, in load_model\r\n[rank0]:     self.model_runner.load_model()\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/worker/model_runner.py\", line 682, in load_model\r\n[rank0]:     self.model = get_model(model_config=self.model_config,\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model\r\n[rank0]:     return loader.load_model(model_config=model_config,\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/model_executor/model_loader/loader.py\", line 280, in load_model\r\n[rank0]:     model = _initialize_model(model_config, self.load_config,\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/model_executor/model_loader/loader.py\", line 111, in _initialize_model\r\n[rank0]:     return model_class(config=model_config.hf_config,\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/model_executor/models/deepseek_v2.py\", line 439, in __init__\r\n[rank0]:     self.model = DeepseekV2Model(config, cache_config, quant_config)\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/model_executor/models/deepseek_v2.py\", line 401, in __init__\r\n[rank0]:     self.layers = nn.ModuleList([\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/model_executor/models/deepseek_v2.py\", line 402, in <listcomp>\r\n[rank0]:     DeepseekV2DecoderLayer(config,\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/model_executor/models/deepseek_v2.py\", line 341, in __init__\r\n[rank0]:     self.mlp = DeepseekV2MoE(config=config, quant_config=quant_config)\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/model_executor/models/deepseek_v2.py\", line 106, in __init__\r\n[rank0]:     self.experts = FusedMoE(num_experts=config.n_routed_experts,\r\n[rank0]:   File \"/usr/local/lib/python3.9/dist-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 186, in __init__\r\n[rank0]:     assert self.quant_method is not None\r\n[rank0]: AssertionError",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-14T01:25:56+00:00",
    "closed_at": "2025-05-07T02:10:19+00:00",
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7494/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7494"
  }
]
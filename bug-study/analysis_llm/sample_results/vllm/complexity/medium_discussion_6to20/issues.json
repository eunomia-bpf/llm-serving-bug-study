[
  {
    "number": 3796,
    "title": "[Bug]: Can't run vllm entrypoint after a fresh install because of a bug in huggingface-hub ",
    "body": "### Your current environment\n\n```text\r\n2024-04-02 13:00:28 (5.94 MB/s) - \u2018collect_env.py\u2019 saved [24853/24853]\r\n\r\nCollecting environment information...\r\n/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\r\n  warnings.warn(\r\nPyTorch version: 2.1.2+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-97-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 11.8.89\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090\r\nNvidia driver version: 550.54.14\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      43 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             256\r\nOn-line CPU(s) list:                0-255\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 7H12 64-Core Processor\r\nCPU family:                         23\r\nModel:                              49\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 64\r\nSocket(s):                          2\r\nStepping:                           0\r\nFrequency boost:                    enabled\r\nCPU max MHz:                        2600.0000\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           5200.17\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\r\nVirtualization:                     AMD-V\r\nL1d cache:                          4 MiB (128 instances)\r\nL1i cache:                          4 MiB (128 instances)\r\nL2 cache:                           64 MiB (128 instances)\r\nL3 cache:                           512 MiB (32 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-63,128-191\r\nNUMA node1 CPU(s):                  64-127,192-255\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec rstack overflow: Mitigation; safe RET\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.24.1\r\n[pip3] torch==2.1.2\r\n[pip3] torchaudio==2.1.0+cu118\r\n[pip3] torchvision==0.16.0+cu118\r\n[pip3] triton==2.1.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PHB     64-127,192-255  1               N/A\r\nNIC0    PHB      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_bond_0\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\n```bash\r\npip install vllm autoawq\r\n\r\npython -m vllm.entrypoints.openai.api_server \\\r\n       --host 0.0.0.0 --port 8000 \\\r\n       --model TheBloke/Mistral-7B-Instruct-v0.2-AWQ \\\r\n       --dtype auto --quantization awq \r\n```\r\n will resutlt in the following error:\r\n\r\n```text\r\npython -m vllm.entrypoints.openai.api_server \\\r\n       --host 0.0.0.0 --port 8000 \\\r\n       --model TheBloke/Mistral-7B-Instruct-v0.2-AWQ \\\r\n       --dtype auto --quantization awq\r\n/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\r\n  warnings.warn(\r\nINFO 04-02 13:06:48 api_server.py:148] vLLM API server version 0.4.0\r\nINFO 04-02 13:06:48 api_server.py:149] args: Namespace(host='0.0.0.0', port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='TheBloke/Mistral-7B-Instruct-v0.2-AWQ', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, seed=0, swap_space=4, gpu_memory_utilization=0.9, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization='awq', enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\nWARNING 04-02 13:06:48 config.py:208] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\nINFO 04-02 13:06:48 llm_engine.py:75] Initializing an LLM engine (v0.4.0) with config: model='TheBloke/Mistral-7B-Instruct-v0.2-AWQ', tokenizer='TheBloke/Mistral-7B-Instruct-v0.2-AWQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\r\nINFO 04-02 13:06:48 selector.py:45] Cannot use FlashAttention because the package is not found. Please install it for better performance.\r\nINFO 04-02 13:06:48 selector.py:21] Using XFormers backend.\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 156, in <module>\r\n    engine = AsyncLLMEngine.from_engine_args(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 348, in from_engine_args\r\n    engine = cls(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 311, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 422, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 111, in __init__\r\n    self.model_executor = executor_class(model_config, cache_config,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 37, in __init__\r\n    self._init_worker()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 66, in _init_worker\r\n    self.driver_worker.load_model()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 107, in load_model\r\n    self.model_runner.load_model()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 95, in load_model\r\n    self.model = get_model(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader.py\", line 101, in get_model\r\n    model.load_weights(model_config.model, model_config.download_dir,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py\", line 377, in load_weights\r\n    for name, loaded_weight in hf_model_weights_iterator(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/weight_utils.py\", line 224, in hf_model_weights_iterator\r\n    hf_folder, hf_weights_files, use_safetensors = prepare_hf_model_weights(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/weight_utils.py\", line 168, in prepare_hf_model_weights\r\n    file_list = fs.ls(model_name_or_path, detail=False, revision=revision)\r\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_file_system.py\", line 283, in ls\r\n    resolved_path = self.resolve_path(path, revision=revision)\r\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_file_system.py\", line 189, in resolve_path\r\n    repo_and_revision_exist, err = self._repo_and_revision_exist(repo_type, repo_id, revision)\r\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_file_system.py\", line 126, in _repo_and_revision_exist\r\n    self._api.repo_info(repo_id, revision=revision, repo_type=repo_type)\r\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 119, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\", line 2418, in repo_info\r\n    return method(\r\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 119, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\", line 2230, in model_info\r\n    return ModelInfo(**data)\r\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\", line 710, in __init__\r\n    self.safetensors = SafeTensorsInfo(**safetensors) if safetensors else None\r\nTypeError: SafeTensorsInfo.__init__() got an unexpected keyword argument 'sharded'\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-04-02T13:19:07+00:00",
    "closed_at": "2024-04-02T21:47:34+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3796/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3796"
  },
  {
    "number": 9183,
    "title": "[Bug]: Qwen2.5-Math-7B-Instruct vllm output garbled code, but the probability of huggingface outputing garbled code is lower.",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\n2024-10-09 14:34:40 (641 KB/s) - \u2018collect_env.py\u2019 saved [25599/25599]\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.4\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-155-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 11.8.89\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A800-SXM4-80GB\r\nGPU 1: NVIDIA A800-SXM4-80GB\r\nGPU 2: NVIDIA A800-SXM4-80GB\r\nGPU 3: NVIDIA A800-SXM4-80GB\r\n\r\nNvidia driver version: 535.54.03\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          64\r\nOn-line CPU(s) list:             0-63\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Platinum 8358P CPU @ 2.60GHz\r\nCPU family:                      6\r\nModel:                           106\r\nThread(s) per core:              1\r\nCore(s) per socket:              32\r\nSocket(s):                       2\r\nStepping:                        6\r\nCPU max MHz:                     3400.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        5200.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig flush_l1d arch_capabilities\r\nVirtualization:                  VT-x\r\nL1d cache:                       3 MiB (64 instances)\r\nL1i cache:                       2 MiB (64 instances)\r\nL2 cache:                        80 MiB (64 instances)\r\nL3 cache:                        96 MiB (2 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-31\r\nNUMA node1 CPU(s):               32-63\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT disabled\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==8.9.2.26\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.535.161\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.68\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==25.1.2\r\n[pip3] torch==2.3.0\r\n[pip3] torchtyping==0.1.5\r\n[pip3] torchvision==0.18.0\r\n[pip3] transformers==4.45.0\r\n[pip3] triton==2.3.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         8.9.2.26                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.6.68                  pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] torchtyping               0.1.5                    pypi_0    pypi\r\n[conda] torchvision               0.18.0                   pypi_0    pypi\r\n[conda] transformers              4.45.0                   pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV8     NV8     NV8     NODE    NODE    0-31    0               N/A\r\nGPU1    NV8      X      NV8     NV8     NODE    NODE    0-31    0               N/A\r\nGPU2    NV8     NV8      X      NV8     SYS     SYS     32-63   1               N/A\r\nGPU3    NV8     NV8     NV8      X      SYS     SYS     32-63   1               N/A\r\nNIC0    NODE    NODE    SYS     SYS      X      PIX\r\nNIC1    NODE    NODE    SYS     SYS     PIX      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_bond_0\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nHi!\r\n\r\nI'm now using Qwen2.5-Math-7B-Instruct to solve problems in the MATH dataset. And I found that the vLLM engine sometimes has weird outputs(garbled code). Here is the code \r\n\r\n\r\n```python\r\nfrom vllm import LLM, SamplingParams\r\n\r\n## VLLM output\r\nmodel_name = \"Qwen2.5-Math-7B-Instruct\"\r\nllm = LLM(model_name)\r\n\r\ntext='<|im_start|>system\\nPlease reason step by step, and put your final answer within \\\\boxed{{}}.<|im_end|>\\n<|im_start|>user\\nSimplify $\\\\tan 100^\\\\circ + 4 \\\\sin 100^\\\\circ.$<|im_end|>\\n<|im_start|>assistant\\nTo simplify the expression \\\\(\\\\tan 100^\\\\circ + 4 \\\\sin 100^\\\\circ\\\\), we start by using the identity \\\\(\\\\tan 100^\\\\circ = \\\\tan (180^\\\\circ - 80^\\\\circ) = -\\\\tan 80^\\\\circ\\\\). Therefore, the expression becomes:\\n\\n\\\\[\\n\\\\tan 100^\\\\circ + 4 \\\\sin 100^\\\\circ = -\\\\tan 80^\\\\circ + 4 \\\\sin 100^\\\\circ\\n\\\\]\\n\\nNext, we use the identity \\\\(\\\\sin 100^\\\\circ = \\\\sin (180^\\\\circ - 80^\\\\circ) = \\\\sin 80^\\\\circ\\\\). So the expression further simplifies to:\\n\\n\\\\[\\n-\\\\tan 80^\\\\circ + 4 \\\\sin 80^\\\\circ\\n\\\\]\\n\\nWe can express \\\\(\\\\tan 80^\\\\circ\\\\) as \\\\(\\\\frac{\\\\sin 80^\\\\circ}{\\\\cos 80^\\\\circ}\\\\). Substituting this into the expression, we get:\\n\\n\\\\[\\n-\\\\frac{\\\\sin 80^\\\\circ}{\\\\cos 80^\\\\circ} + 4 \\\\sin 80^\\\\circ\\n\\\\]\\n\\nTo combine these terms, we need a common denominator. The common denominator is \\\\(\\\\cos 80^\\\\circ\\\\), so we rewrite the expression as:\\n\\n\\\\[\\n-\\\\frac{\\\\sin 80^\\\\circ}{\\\\cos 80^\\\\circ} + \\\\frac{4 \\\\sin 80^\\\\circ \\\\cos 80^\\\\circ}{\\\\cos 80^\\\\circ} = \\\\frac{-\\\\sin 80^\\\\circ + 4 \\\\sin 80^\\\\circ \\\\cos 80^\\\\circ}{\\\\cos 80^\\\\circ}\\n\\\\]\\n\\nWe can factor out \\\\(\\\\sin 80^\\\\circ\\\\) from the numerator:\\n\\n\\\\[\\n\\\\frac{\\\\sin 80^\\\\circ (-1 + 4 \\\\cos 80^\\\\circ)}{\\\\cos 80^\\\\circ}\\n\\\\]\\n\\nThis simplifies to:\\n\\n\\\\[\\n\\\\sin 80^\\\\circ \\\\cdot \\\\frac{-1 + 4 \\\\cos 80^\\\\circ}{\\\\cos 80^\\\\circ} = \\\\sin 80^\\\\circ \\\\left( \\\\frac{-1}{\\\\cos 80^\\\\circ} + 4 \\\\right) = \\\\sin 80^\\\\circ \\\\left( -\\\\sec 80^\\\\circ + 4 \\\\right)\\n\\\\]\\n\\nWe know from trigonometric identities that \\\\(\\\\sec 80^\\\\circ = \\\\frac{1}{\\\\cos 80^\\\\circ} = \\\\csc 10^\\\\circ\\\\) (since \\\\(\\\\sec (90^\\\\circ - \\\\theta) = \\\\csc \\\\theta\\\\)). Therefore, \\\\(-\\\\sec 80^\\\\circ = -\\\\csc 10^\\\\circ\\\\). So the expression becomes:\\n\\n\\\\[\\n\\\\sin 80^\\\\circ \\\\left( -\\\\csc 10^\\\\circ + 4 \\\\right) = \\\\sin 80^\\\\circ \\\\left( -\\\\frac{1}{\\\\sin 10^\\\\circ} + 4 \\\\right)\\n\\\\]\\n\\nWe can rewrite \\\\(\\\\sin 80^\\\\circ\\\\) as \\\\(\\\\cos 10^\\\\circ\\\\) (since \\\\(\\\\sin (90^\\\\circ - \\\\theta) = \\\\cos \\\\theta\\\\)). So the expression is:\\n\\n\\\\[\\n\\\\cos 10^\\\\circ \\\\left( -\\\\frac{1}{\\\\sin 10^\\\\circ} + 4 \\\\right) = \\\\cos 10^\\\\circ \\\\left( -\\\\frac{1}{\\\\sin 10^\\\\circ} \\\\right) + \\\\cos 10^\\\\circ \\\\cdot 4 = -\\\\frac{\\\\cos 10^\\\\circ}{\\\\sin 10^\\\\circ} + 4 \\\\cos 10^\\\\circ = -\\\\cot 10^\\\\circ + 4 \\\\cos 10^\\\\circ\\n\\\\]\\n\\nWe know from trigonometric identities that \\\\(\\\\cot 10^\\\\circ = \\\\frac{\\\\cos 10^\\\\circ}{\\\\sin 10^\\\\circ}\\\\). So we have:\\n\\n\\\\[\\n-\\\\cot 10^\\\\circ + 4 \\\\cos 10^\\\\circ = -\\\\frac{\\\\cos 10^\\\\circ}{\\\\sin 10^\\\\circ} + 4 \\\\cos 10^\\\\circ = -\\\\frac{\\\\cos 10^\\\\circ}{\\\\sin 10^\\\\circ} + \\\\frac{4 \\\\cos 10^\\\\circ \\\\sin 10^\\\\circ}{\\\\sin 10^\\\\circ} = \\\\frac{-\\\\cos 10^\\\\circ + 4 \\\\cos 10^\\\\circ \\\\sin 10^\\\\circ}{\\\\sin 10^\\\\circ}\\n\\\\]\\n\\nWe can factor out \\\\(\\\\cos 10^\\\\circ\\\\) from the numerator:\\n\\n'\r\noutput = llm.generate(\r\n    text, \r\n    sampling_params=SamplingParams(\r\n        n=1, temperature=0.7, \r\n        top_k=50, \r\n        max_tokens=2048,\r\n        stop=[\"</s>\", \"<|im_end|>\", \"<|endoftext|>\"],\r\n        stop_token_ids=[151645, 151643])\r\n    )\r\nprint(output[0].outputs[0].text)\r\n```\r\nand vLLM's output is\r\n[output.txt](https://github.com/user-attachments/files/17303386/output.txt)\r\n<img width=\"1115\" alt=\"image\" src=\"https://github.com/user-attachments/assets/a4213d6d-55b8-406b-82e4-5a0f8e4c19de\">\r\n\r\n\r\nand here is the code for text generation with `transformers`\r\n```python\r\ntext='<|im_start|>system\\nPlease reason step by step, and put your final answer within \\\\boxed{{}}.<|im_end|>\\n<|im_start|>user\\nSimplify $\\\\tan 100^\\\\circ + 4 \\\\sin 100^\\\\circ.$<|im_end|>\\n<|im_start|>assistant\\nTo simplify the expression \\\\(\\\\tan 100^\\\\circ + 4 \\\\sin 100^\\\\circ\\\\), we start by using the identity \\\\(\\\\tan 100^\\\\circ = \\\\tan (180^\\\\circ - 80^\\\\circ) = -\\\\tan 80^\\\\circ\\\\). Therefore, the expression becomes:\\n\\n\\\\[\\n\\\\tan 100^\\\\circ + 4 \\\\sin 100^\\\\circ = -\\\\tan 80^\\\\circ + 4 \\\\sin 100^\\\\circ\\n\\\\]\\n\\nNext, we use the identity \\\\(\\\\sin 100^\\\\circ = \\\\sin (180^\\\\circ - 80^\\\\circ) = \\\\sin 80^\\\\circ\\\\). So the expression further simplifies to:\\n\\n\\\\[\\n-\\\\tan 80^\\\\circ + 4 \\\\sin 80^\\\\circ\\n\\\\]\\n\\nWe can express \\\\(\\\\tan 80^\\\\circ\\\\) as \\\\(\\\\frac{\\\\sin 80^\\\\circ}{\\\\cos 80^\\\\circ}\\\\). Substituting this into the expression, we get:\\n\\n\\\\[\\n-\\\\frac{\\\\sin 80^\\\\circ}{\\\\cos 80^\\\\circ} + 4 \\\\sin 80^\\\\circ\\n\\\\]\\n\\nTo combine these terms, we need a common denominator. The common denominator is \\\\(\\\\cos 80^\\\\circ\\\\), so we rewrite the expression as:\\n\\n\\\\[\\n-\\\\frac{\\\\sin 80^\\\\circ}{\\\\cos 80^\\\\circ} + \\\\frac{4 \\\\sin 80^\\\\circ \\\\cos 80^\\\\circ}{\\\\cos 80^\\\\circ} = \\\\frac{-\\\\sin 80^\\\\circ + 4 \\\\sin 80^\\\\circ \\\\cos 80^\\\\circ}{\\\\cos 80^\\\\circ}\\n\\\\]\\n\\nWe can factor out \\\\(\\\\sin 80^\\\\circ\\\\) from the numerator:\\n\\n\\\\[\\n\\\\frac{\\\\sin 80^\\\\circ (-1 + 4 \\\\cos 80^\\\\circ)}{\\\\cos 80^\\\\circ}\\n\\\\]\\n\\nThis simplifies to:\\n\\n\\\\[\\n\\\\sin 80^\\\\circ \\\\cdot \\\\frac{-1 + 4 \\\\cos 80^\\\\circ}{\\\\cos 80^\\\\circ} = \\\\sin 80^\\\\circ \\\\left( \\\\frac{-1}{\\\\cos 80^\\\\circ} + 4 \\\\right) = \\\\sin 80^\\\\circ \\\\left( -\\\\sec 80^\\\\circ + 4 \\\\right)\\n\\\\]\\n\\nWe know from trigonometric identities that \\\\(\\\\sec 80^\\\\circ = \\\\frac{1}{\\\\cos 80^\\\\circ} = \\\\csc 10^\\\\circ\\\\) (since \\\\(\\\\sec (90^\\\\circ - \\\\theta) = \\\\csc \\\\theta\\\\)). Therefore, \\\\(-\\\\sec 80^\\\\circ = -\\\\csc 10^\\\\circ\\\\). So the expression becomes:\\n\\n\\\\[\\n\\\\sin 80^\\\\circ \\\\left( -\\\\csc 10^\\\\circ + 4 \\\\right) = \\\\sin 80^\\\\circ \\\\left( -\\\\frac{1}{\\\\sin 10^\\\\circ} + 4 \\\\right)\\n\\\\]\\n\\nWe can rewrite \\\\(\\\\sin 80^\\\\circ\\\\) as \\\\(\\\\cos 10^\\\\circ\\\\) (since \\\\(\\\\sin (90^\\\\circ - \\\\theta) = \\\\cos \\\\theta\\\\)). So the expression is:\\n\\n\\\\[\\n\\\\cos 10^\\\\circ \\\\left( -\\\\frac{1}{\\\\sin 10^\\\\circ} + 4 \\\\right) = \\\\cos 10^\\\\circ \\\\left( -\\\\frac{1}{\\\\sin 10^\\\\circ} \\\\right) + \\\\cos 10^\\\\circ \\\\cdot 4 = -\\\\frac{\\\\cos 10^\\\\circ}{\\\\sin 10^\\\\circ} + 4 \\\\cos 10^\\\\circ = -\\\\cot 10^\\\\circ + 4 \\\\cos 10^\\\\circ\\n\\\\]\\n\\nWe know from trigonometric identities that \\\\(\\\\cot 10^\\\\circ = \\\\frac{\\\\cos 10^\\\\circ}{\\\\sin 10^\\\\circ}\\\\). So we have:\\n\\n\\\\[\\n-\\\\cot 10^\\\\circ + 4 \\\\cos 10^\\\\circ = -\\\\frac{\\\\cos 10^\\\\circ}{\\\\sin 10^\\\\circ} + 4 \\\\cos 10^\\\\circ = -\\\\frac{\\\\cos 10^\\\\circ}{\\\\sin 10^\\\\circ} + \\\\frac{4 \\\\cos 10^\\\\circ \\\\sin 10^\\\\circ}{\\\\sin 10^\\\\circ} = \\\\frac{-\\\\cos 10^\\\\circ + 4 \\\\cos 10^\\\\circ \\\\sin 10^\\\\circ}{\\\\sin 10^\\\\circ}\\n\\\\]\\n\\nWe can factor out \\\\(\\\\cos 10^\\\\circ\\\\) from the numerator:\\n\\n'\r\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\r\n\r\ngenerated_ids = model.generate(\r\n    **model_inputs,\r\n    max_new_tokens=2048,\r\n    temperature=0.7\r\n)\r\ngenerated_ids = [\r\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\r\n]\r\n\r\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\r\nprint(response)\r\n```\r\nThe output with `transformers` looks good.\r\n\r\nI know that `t > 0` may cause some randomness, but for both of the two codes here, I've tested over 5 times. And vLLM output strange texts in >60% cases. However, the `transformers`  implementation outputs garbled code **with a much lower probability**. \r\n\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-10-09T07:07:21+00:00",
    "closed_at": "2024-12-27T03:50:22+00:00",
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9183/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9183"
  },
  {
    "number": 18572,
    "title": "[Bug]: ValueError: Attempted to assign 25 + 25 + 25 = 75 multimodal tokens to 147 placeholders",
    "body": "### Your current environment\n\nvllm==0.7.3   transformers==4.49.0\n\n\n\n### \ud83d\udc1b Describe the bug\n\n```import os \nos.environ[\"HF_HOME\"] = \"/gz-data/.cache/huggingface\"\nimport torch\nfrom datasets import Dataset\nfrom qwen_vl_utils import process_vision_info\nfrom peft import LoraConfig, TaskType, get_peft_model, PeftModel\nfrom transformers import (\n    TrainingArguments,\n    Trainer,\n    DataCollatorForSeq2Seq,\n    Qwen2_5_VLForConditionalGeneration,\n    AutoProcessor,\n    AutoTokenizer,\n    TrainerCallback, \n    TrainerControl, \n    TrainerState,\n    BitsAndBytesConfig\n)\nimport json\nimport mlflow\nimport os\nfrom transformers.integrations import HfDeepSpeedConfig\nimport shutil\nfrom Levenshtein import ratio as levenshtein_ratio\nfrom vllm_grpo_trainer import Qwen2VLGRPOVLLMTrainer\nfrom trl import GRPOConfig\nfrom PIL import Image\n\ndef process_func_for_grpo(example):\n    conversation = example[\"conversations\"]\n    input_msg = conversation[0][\"value\"]\n    output_msg = conversation[1][\"value\"]\n\n    file_path = input_msg.split(\"<|vision_start|>\")[1].split(\"<|vision_end|>\")[0]\n    img = Image.open(file_path).convert(\"RGB\").resize((128, 128)) \n\n    prompt = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\"},\n                {\"type\": \"text\", \"text\": \"Is this image manipulated or synthesized?\"}\n            ]\n        }\n    ]\n\n    return {\n        \"prompt\": prompt,\n        \"solution\": output_msg,\n        \"image\": img,\n    }\n\ndef dataset_generator():\n    dataset = Dataset.from_json(\"./train.json\").select(range(10))\n    for example in dataset:\n        yield process_func_for_grpo(example)\n\n\nclass SaveBestTrainLossCallback(TrainerCallback):\n    def __init__(self, output_dir):\n        self.best_loss = float(\"inf\")\n        self.output_dir = output_dir\n\n    def on_log(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n        logs = logs or {}\n        if \"loss\" in logs:\n            current_loss = logs[\"loss\"]\n            if current_loss < self.best_loss:\n                self.best_loss = current_loss\n                kwargs[\"model\"].save_pretrained(self.output_dir)\n                tokenizer.save_pretrained(self.output_dir)\n                processor.save_pretrained(self.output_dir)\n                print(f\"Saved new best model with train loss: {current_loss:.4f}\")\n\ndef levenshtein_reward_func(completions, solution, **kwargs):\n    \"\"\"Reward function that compares the entire completion to the solution.\"\"\"\n    res = []\n    for completion, sol in zip(completions, solution):\n        text = completion[0]['content']\n        res.append(levenshtein_ratio(text, sol))\n    return res\n\n\nif __name__ == \"__main__\":\n    \n    # bnb_config = BitsAndBytesConfig(\n    #     load_in_4bit=True,\n    #     bnb_4bit_use_double_quant=False,\n    #     bnb_4bit_quant_type=\"nf4\",\n    #     bnb_4bit_compute_dtype=getattr(torch, \"float16\"),\n    # )\n\n    # mlflow.set_experiment(\"Qwen2.5-VL-3B-Instruct-Fintune-Multi-GPUs-Full\")\n\n    ds_config = {\n        \"train_micro_batch_size_per_gpu\": 1,\n        \"gradient_accumulation_steps\": 4,\n        \"bf16\": {\n            \"enabled\": True\n        },\n        \"zero_optimization\": {\n            \"stage\": 2,  \n            \"offload_optimizer\": {\n                \"device\": \"cpu\",\n                \"pin_memory\": True\n            },\n            \"allgather_partitions\": True,\n            \"allgather_bucket_size\": 2e8,\n            \"overlap_comm\": True,\n            \"reduce_scatter\": True,\n            \"reduce_bucket_size\": 2e8,\n            \"contiguous_gradients\": True\n        },\n        \"gradient_clipping\": 1.0,\n        \"steps_per_print\": 10,\n    }\n    \n    # Initialize DeepSpeed\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29500\"\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n    \n    \n    model_id = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n    # os.system(\"huggingface-cli download Qwen/Qwen2.5-VL-3B-Instruct\")\n\n    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False, trust_remote_code=True)\n    processor = AutoProcessor.from_pretrained(model_id)\n\n\n    # dschf = HfDeepSpeedConfig(ds_config)\n    \n\n    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n        model_id,\n        torch_dtype=torch.float32,\n    )\n    \n    model.enable_input_require_grads() \n\n    raw_ds = Dataset.from_json(\"./train.json\").select(range(100))\n    train_dataset = raw_ds.map(process_func_for_grpo)\n\n    config = LoraConfig(\n        task_type=TaskType.CAUSAL_LM,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        inference_mode=False,  \n        r=8, \n        lora_alpha=16,  \n        lora_dropout=0.05,  \n        bias=\"none\",\n    )\n\n    # peft_model = get_peft_model(model, config)\n\n    args = GRPOConfig(\n        output_dir=\"./output/Qwen2.5-VL-3B-Instruct\",\n        per_device_train_batch_size=3,  \n        gradient_accumulation_steps=4,  \n        logging_steps=10,\n        logging_first_step=5,\n        num_train_epochs=1,\n        num_generations=3,\n        save_steps=100,\n        learning_rate=1e-4,\n        save_on_each_node=True,\n        gradient_checkpointing=False,\n        report_to=\"mlflow\",\n        # bf16=False,\n        # fp16=True,\n        # deepspeed=ds_config, \n        use_vllm=True,\n        vllm_device=\"cuda:0\",\n        vllm_gpu_memory_utilization=0.50,\n        max_completion_length=512\n    )\n    \n    trainer = Qwen2VLGRPOVLLMTrainer(\n        model=model,\n        args=args,\n        train_dataset=train_dataset,\n        # data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n        reward_funcs=[levenshtein_reward_func],\n        processing_class=processor,\n        # callbacks=[SaveBestTrainLossCallback(output_dir=\"./output/Qwen2.5-VL-3B-Instruct/best_step\")],\n        peft_config = config,\n    )\n\n    \n    trainer.train()\n    trainer.save_model(\"./output/Qwen2.5-VL-3B-Instruct/final_step\")```\n\n\nvllm_grpo_trainer.py\n\n```# Copyright 2025 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport textwrap\nfrom collections import defaultdict\nfrom typing import Any, Callable, Optional, Union\nfrom accelerate.utils.other import is_compiled_module\nfrom accelerate.utils import broadcast_object_list, gather, gather_object\nimport torch\nimport torch.utils.data\nimport transformers\nimport warnings\nfrom unittest.mock import patch\nfrom datasets import Dataset, IterableDataset\nfrom packaging import version\nfrom transformers import (\n    AriaForConditionalGeneration,\n    AriaProcessor,\n    AutoModelForCausalLM,\n    AutoModelForSequenceClassification,\n    AutoProcessor,\n    AutoTokenizer,\n    GenerationConfig,\n    PreTrainedModel,\n    PreTrainedTokenizerBase,\n    Qwen2VLForConditionalGeneration,\n    Trainer,\n    TrainerCallback,\n    is_wandb_available,\n)\nfrom transformers.integrations.deepspeed import is_deepspeed_zero3_enabled\nfrom transformers.utils import is_peft_available\n\nfrom trl.data_utils import (\n    apply_chat_template,\n    is_conversational,\n    maybe_apply_chat_template,\n)\nfrom trl.import_utils import is_vllm_available\n\nfrom trl.models import (\n    create_reference_model,\n    prepare_deepspeed,\n    unwrap_model_for_generation,\n)\nfrom trl.trainer.grpo_config import GRPOConfig\nfrom trl.trainer.utils import generate_model_card, get_comet_experiment_url, pad\nfrom trl import GRPOTrainer\n\nimport copy\n\nif is_peft_available():\n    from peft import PeftConfig, get_peft_model\n\nif is_vllm_available():\n    from vllm import LLM, SamplingParams\n\n\nif is_wandb_available():\n    import wandb\nimport torch.nn as nn\nfrom torch.utils.data import Sampler\n\n# What we call a reward function is a callable that takes a list of prompts and completions and returns a list of\n# rewards. When it's a string, it's a model ID, so it's loaded as a pretrained model.\nRewardFunc = Union[str, PreTrainedModel, Callable[[list, list], list[float]]]\n\n\nclass RepeatRandomSampler(Sampler):\n    \"\"\"\n    Sampler that repeats the indices of a dataset N times.\n\n    Args:\n        data_source (`Sized`):\n            Dataset to sample from.\n        repeat_count (`int`):\n            Number of times to repeat each index.\n\n    Example:\n    ```python\n    >>> sampler = RepeatRandomSampler([\"a\", \"b\", \"c\", \"d\"], repeat_count=2)\n    >>> list(sampler)\n    [2, 2, 0, 0, 3, 3, 1, 1]\n    ```\n    \"\"\"\n\n    def __init__(self, data_source, repeat_count: int):\n        self.data_source = data_source\n        self.repeat_count = repeat_count\n        self.num_samples = len(data_source)\n\n    def __iter__(self):\n        indexes = [\n            idx\n            for idx in torch.randperm(self.num_samples).tolist()\n            for _ in range(self.repeat_count)\n        ]\n        return iter(indexes)\n\n    def __len__(self):\n        return self.num_samples * self.repeat_count\n\n\nclass Qwen2VLGRPOVLLMTrainer(Trainer):\n    def __init__(\n        self,\n        model: Union[str, PreTrainedModel],\n        reward_funcs: Union[RewardFunc, list[RewardFunc]],\n        args: GRPOConfig = None,\n        train_dataset: Optional[Union[Dataset, IterableDataset]] = None,\n        eval_dataset: Optional[\n            Union[Dataset, IterableDataset, dict[str, Union[Dataset, IterableDataset]]]\n        ] = None,\n        processing_class: Optional[PreTrainedTokenizerBase] = None,\n        reward_processing_classes: Optional[\n            Union[PreTrainedTokenizerBase, list[PreTrainedTokenizerBase]]\n        ] = None,\n        callbacks: Optional[list[TrainerCallback]] = None,\n        optimizers: tuple[\n            Optional[torch.optim.Optimizer], Optional[torch.optim.lr_scheduler.LambdaLR]\n        ] = (None, None),\n        peft_config: Optional[\"PeftConfig\"] = None,\n        # qwen2-vl related params\n        max_pixels: Optional[int] = 12845056,\n        min_pixels: Optional[int] = 3136,\n        attn_implementation: str = \"flash_attention_2\",\n    ):\n\n        # Args\n        if args is None:\n            model_name = model if isinstance(model, str) else model.config._name_or_path\n            model_name = model_name.split(\"/\")[-1]\n            args = GRPOConfig(f\"{model_name}-GRPO\")\n\n        # Models\n        # Trained model\n        model_init_kwargs = args.model_init_kwargs or {}\n        model_init_kwargs[\"attn_implementation\"] = attn_implementation\n        if isinstance(model, str):\n            model_id = model\n            torch_dtype = model_init_kwargs.get(\"torch_dtype\")\n            if (\n                isinstance(torch_dtype, torch.dtype)\n                or torch_dtype == \"auto\"\n                or torch_dtype is None\n            ):\n                pass  # torch_dtype is already a torch.dtype or \"auto\" or None\n            elif isinstance(torch_dtype, str):  # it's a str, but not \"auto\"\n                torch_dtype = getattr(torch, torch_dtype)\n                model_init_kwargs[\"torch_dtype\"] = torch_dtype\n            else:\n                raise ValueError(\n                    \"Invalid `torch_dtype` passed to `GRPOConfig`. Expected either 'auto' or a string representing \"\n                    f\"a `torch.dtype` (e.g., 'float32'), but got {torch_dtype}.\"\n                )\n            # Disable caching if gradient checkpointing is enabled (not supported)\n            model_init_kwargs[\"use_cache\"] = (\n                False\n                if args.gradient_checkpointing\n                else model_init_kwargs.get(\"use_cache\")\n            )\n            if \"Qwen2-VL\" in model_id:\n                model = Qwen2VLForConditionalGeneration.from_pretrained(\n                    model, **model_init_kwargs\n                )\n            elif \"Aria\" in model_id:\n                model_init_kwargs.pop(\"use_cache\")\n                model = AriaForConditionalGeneration.from_pretrained(\n                    model, **model_init_kwargs\n                )\n            else:\n                model = AutoModelForCausalLM.from_pretrained(model, **model_init_kwargs)\n        else:\n            model_id = model.config._name_or_path\n            if args.model_init_kwargs is not None:\n                raise ValueError(\n                    \"You passed `model_init_kwargs` to the `GRPOConfig`, but your model is already instantiated. \"\n                    \"This argument can only be used when the `model` argument is a string.\"\n                )\n\n        if peft_config is not None:\n            model = get_peft_model(model, peft_config)\n\n        # Reference model\n        if is_deepspeed_zero3_enabled():\n            if \"Qwen2-VL\" in model_id:\n                self.ref_model = Qwen2VLForConditionalGeneration.from_pretrained(\n                    model_id, **model_init_kwargs\n                )\n            elif \"Aria\" in model_id:\n                self.ref_model = AriaForConditionalGeneration.from_pretrained(\n                    model_id, **model_init_kwargs\n                )\n            else:\n                self.ref_model = AutoModelForCausalLM.from_pretrained(\n                    model_id, **model_init_kwargs\n                )\n        elif peft_config is None:\n            # If PEFT configuration is not provided, create a reference model based on the initial model.\n            self.ref_model = create_reference_model(model)\n        else:\n            # If PEFT is used, the reference model is not needed since the adapter can be disabled\n            # to revert to the initial model.\n            self.ref_model = None\n\n        # Processing class\n        pad_token_id = processing_class.tokenizer.pad_token_id # in case the error of local variable 'pad_token_id' referenced before assignment\n        if processing_class is None:\n            if \"Qwen2-VL\" in model_id or \"Aria\" in model_id:\n                processing_class = AutoProcessor.from_pretrained(model_id)\n                pad_token_id = processing_class.tokenizer.pad_token_id\n                processing_class.pad_token_id = pad_token_id\n                processing_class.eos_token_id = processing_class.tokenizer.eos_token_id\n                if \"Qwen2-VL\" in model_id:\n                    processing_class.image_processor.max_pixels = max_pixels\n                    processing_class.image_processor.min_pixels = min_pixels\n            else:\n                processing_class = AutoTokenizer.from_pretrained(\n                    model.config._name_or_path, padding_side=\"left\"\n                )\n                pad_token_id = processing_class.pad_token_id\n\n        # Reward functions\n        if not isinstance(reward_funcs, list):\n            reward_funcs = [reward_funcs]\n        for i, reward_func in enumerate(reward_funcs):\n            if isinstance(reward_func, str):\n                reward_funcs[i] = AutoModelForSequenceClassification.from_pretrained(\n                    reward_func, num_labels=1, **model_init_kwargs\n                )\n        self.reward_funcs = reward_funcs\n\n        # Reward processing class\n        if reward_processing_classes is None:\n            reward_processing_classes = [None] * len(reward_funcs)\n        elif not isinstance(reward_processing_classes, list):\n            reward_processing_classes = [reward_processing_classes]\n        else:\n            if len(reward_processing_classes) != len(reward_funcs):\n                raise ValueError(\n                    \"The number of reward processing classes must match the number of reward functions.\"\n                )\n\n        for i, (reward_processing_class, reward_func) in enumerate(\n            zip(reward_processing_classes, reward_funcs)\n        ):\n            if isinstance(reward_func, PreTrainedModel):\n                if reward_processing_class is None:\n                    reward_processing_class = AutoTokenizer.from_pretrained(\n                        reward_func.config._name_or_path\n                    )\n                if reward_processing_class.pad_token_id is None:\n                    reward_processing_class.pad_token = (\n                        reward_processing_class.eos_token\n                    )\n                # The reward model computes the reward for the latest non-padded token in the input sequence.\n                # So it's important to set the pad token ID to the padding token ID of the processing class.\n                reward_func.config.pad_token_id = reward_processing_class.pad_token_id\n                reward_processing_classes[i] = reward_processing_class\n        self.reward_processing_classes = reward_processing_classes\n\n        # Data collator\n        def data_collator(features):  # No data collation is needed in GRPO\n            return features\n\n        # Training arguments\n        self.max_prompt_length = args.max_prompt_length\n        self.max_completion_length = (\n            args.max_completion_length\n        )  # = |o_i| in the GRPO paper\n        self.num_generations = args.num_generations  # = G in the GRPO paper\n        self.generation_config = GenerationConfig(\n            max_new_tokens=self.max_completion_length,\n            do_sample=True,\n            temperature=1,  # HACK\n            num_return_sequences=self.num_generations,\n            pad_token_id=pad_token_id,\n        )\n        self.beta = args.beta\n\n        # The trainer estimates the number of FLOPs (floating-point operations) using the number of elements in the\n        # input tensor associated with the key \"input_ids\". However, in GRPO, the sampled data does not include the\n        # \"input_ids\" key. Instead, the available keys is \"prompt\". As a result, the trainer issues the warning:\n        # \"Could not estimate the number of tokens of the input, floating-point operations will not be computed.\" To\n        # suppress this warning, we set the \"estimate_tokens\" key in the model's \"warnings_issued\" dictionary to True.\n        # This acts as a flag to indicate that the warning has already been issued.\n        model.warnings_issued[\"estimate_tokens\"] = True\n\n        # Initialize the metrics\n        self._metrics = defaultdict(list)\n        self.use_vllm = args.use_vllm\n\n        # rewrite the processing AutoTokenizer -> AutoProcessor\n        model_id = model if isinstance(model, str) else model.config._name_or_path\n        if processing_class is None:\n            if \"Qwen2-VL\" in model_id or \"Aria\" in model_id:\n                processing_class = AutoProcessor.from_pretrained(model_id)\n                pad_token_id = processing_class.tokenizer.pad_token_id\n                processing_class.pad_token_id = pad_token_id\n                processing_class.eos_token_id = processing_class.tokenizer.eos_token_id\n                if \"Qwen2-VL\" in model_id:\n                    processing_class.image_processor.max_pixels = max_pixels\n                    processing_class.image_processor.min_pixels = min_pixels\n            else:\n                processing_class = AutoTokenizer.from_pretrained(\n                    model.config._name_or_path, padding_side=\"left\"\n                )\n                pad_token_id = processing_class.pad_token_id\n\n        super().__init__(\n            model=model,\n            args=args,\n            data_collator=data_collator,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            processing_class=processing_class,\n            callbacks=callbacks,\n            optimizers=optimizers,\n        )\n        # Gradient accumulation requires scaled loss. Normally, loss scaling in the parent class depends on whether the\n        # model accepts loss-related kwargs. Since we compute our own loss, this check is irrelevant. We set\n        # self.model_accepts_loss_kwargs to False to enable scaling.\n        self.model_accepts_loss_kwargs = False\n        # Check if the per_device_train/eval_batch_size * num processes can be divided by the number of generations\n        num_processes = self.accelerator.num_processes\n        global_batch_size = args.per_device_train_batch_size * num_processes\n        possible_values = [\n            n_gen\n            for n_gen in range(2, global_batch_size + 1)\n            if (global_batch_size) % n_gen == 0\n        ]\n\n        if self.num_generations not in possible_values:\n            raise ValueError(\n                f\"The global train batch size ({num_processes} x {args.per_device_train_batch_size}) must be evenly \"\n                f\"divisible by the number of generations per prompt ({self.num_generations}). Given the current train \"\n                f\"batch size, the valid values for the number of generations are: {possible_values}.\"\n            )\n        if self.args.eval_strategy != \"no\":\n            global_batch_size = args.per_device_eval_batch_size * num_processes\n            possible_values = [\n                n_gen\n                for n_gen in range(2, global_batch_size + 1)\n                if (global_batch_size) % n_gen == 0\n            ]\n            if self.num_generations not in possible_values:\n                raise ValueError(\n                    f\"The global eval batch size ({num_processes} x {args.per_device_eval_batch_size}) must be evenly \"\n                    f\"divisible by the number of generations per prompt ({self.num_generations}). Given the current \"\n                    f\"eval batch size, the valid values for the number of generations are: {possible_values}.\"\n                )\n\n        if self.use_vllm:\n            if not is_vllm_available():\n                raise ImportError(\n                    \"vLLM is not available and `use_vllm` is set to True. Please install vLLM with \"\n                    \"`pip install vllm` to use it.\"\n                )\n\n            if self.accelerator.is_main_process:\n                vllm_device = self.args.vllm_device\n                if vllm_device == \"auto\":\n                    vllm_device = f\"cuda:{self.accelerator.num_processes}\"  # take the next GPU idx\n                # Check that the requested device is available\n                if (\n                    vllm_device.split(\":\")[0] == \"cuda\"\n                    and int(vllm_device.split(\":\")[1]) >= torch.cuda.device_count()\n                ):\n                    raise ValueError(\n                        f\"The requested device for vllm ({vllm_device}) is not available. You are likely using vLLM \"\n                        \"without restricting the number of GPUs for training. Set the `--num_processes` argument to a \"\n                        \"value lower than the number of GPUs available on your machine\u2014typically, reducing it by one \"\n                        f\"is sufficient. In your case: `--num_processes {torch.cuda.device_count() - 1}`.\"\n                    )\n                # Check that the requested device is not also used for training\n                if vllm_device in {\n                    f\"cuda:{idx}\" for idx in range(self.accelerator.num_processes)\n                }:\n                    warnings.warn(\n                        f\"The requested device {vllm_device} is also used for training. This may lead to unexpected \"\n                        \"behavior. It is recommended to use a dedicated device for vLLM.\"\n                    )\n                # vLLM is not compatible with accelerate. So we need to patch it to make sure we can (1) place the vLLM\n                # model on the desired device (world_size_patch) and (2) avoid a test that is not designed for our\n                # setting (profiling_patch).\n                world_size_patch = patch(\n                    \"torch.distributed.get_world_size\", return_value=1\n                )\n                profiling_patch = patch(\n                    \"vllm.worker.worker.Worker._assert_memory_footprint_increased_during_profiling\",\n                    return_value=None,\n                )\n                with world_size_patch, profiling_patch:\n                    print(\"vllm is running on: \", vllm_device)\n                    self.llm = LLM(\n                        model=model.name_or_path,\n                        device=vllm_device,\n                        gpu_memory_utilization=self.args.vllm_gpu_memory_utilization,\n                        dtype=torch.bfloat16,\n                        # Automatic Prefix Caching caches the KV cache of existing queries, so that a new query can\n                        # directly reuse the KV cache if it shares the same prefix with one of the existing queries.\n                        # This is particularly useful here because we generate completions from the same prompts.\n                        enable_prefix_caching=True,\n                        enforce_eager=True,\n                        max_model_len=args.max_completion_length,\n                    )\n                self.sampling_params = SamplingParams(\n                    temperature=args.temperature,\n                    max_tokens=self.max_completion_length,\n                )\n\n            self._last_loaded_step = (\n                0  # tag to avoid useless loading during grad accumulation\n            )\n\n            # When using vLLM, the main process is responsible for loading the model weights. This can cause process\n            # desynchronization and seems to lead to DeepSpeed hanging during initialization. To prevent this, we\n            # synchronize all processes after vLLM has been fully initialized.\n            self.accelerator.wait_for_everyone()\n        else:\n            raise ValueError(\n                \"Qwen2VLGRPOVLLMTrainer only supports vllm generation, please set --use_vllm True\"\n            )\n\n        if self.ref_model is not None:\n            if self.is_deepspeed_enabled:\n                self.ref_model = prepare_deepspeed(self.ref_model, self.accelerator)\n            else:\n                self.ref_model = self.accelerator.prepare_model(\n                    self.ref_model, evaluation_mode=True\n                )\n\n        for i, reward_func in enumerate(self.reward_funcs):\n            if isinstance(reward_func, PreTrainedModel):\n                self.reward_funcs[i] = self.accelerator.prepare_model(\n                    reward_func, evaluation_mode=True\n                )\n\n    def _set_signature_columns_if_needed(self):\n        # If `self.args.remove_unused_columns` is True, non-signature columns are removed.\n        # By default, this method sets `self._signature_columns` to the model's expected inputs.\n        # In GRPOTrainer, we preprocess data, so using the model's signature columns doesn't work.\n        # Instead, we set them to the columns expected by the `training_step` method, hence the override.\n        if self._signature_columns is None:\n            self._signature_columns = [\"prompt\"]\n\n    # We need a custom sampler that samples the same prompt multiple times\n    def _get_train_sampler(self):\n        return RepeatRandomSampler(self.train_dataset, self.num_generations)\n\n    # Get the per-token log probabilities for the completions for the model and the reference model\n    def _get_per_token_logps(\n        self,\n        model,\n        input_ids,\n        attention_mask,\n        pixel_values,\n        image_grid_thw,\n        logits_to_keep,\n    ):\n        pixel_values = pixel_values.to(model.device)\n        image_grid_thw = image_grid_thw.to(device=model.device)\n        logits = model(\n            input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values,\n            image_grid_thw=image_grid_thw,\n        ).logits  # (B, L, V)\n        logits = logits[\n            :, :-1, :\n        ]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred\n        input_ids = input_ids[\n            :, -logits_to_keep:\n        ]  # (B, L-1), exclude the first input ID since we don't have logits for it\n        # Compute the log probabilities for the input tokens. Use a loop to reduce memory peak.\n        logits = logits[:, -logits_to_keep:]\n        per_token_logps = []\n        for logits_row, input_ids_row in zip(logits, input_ids):\n            log_probs = logits_row.log_softmax(dim=-1)\n            token_log_prob = torch.gather(\n                log_probs, dim=1, index=input_ids_row.unsqueeze(1)\n            ).squeeze(1)\n            per_token_logps.append(token_log_prob)\n        return torch.stack(per_token_logps)\n\n    # Trainer \"prepares\" the inputs before calling `compute_loss`. It converts to tensor and move to device.\n    # Since we preprocess the data in `compute_loss`, we need to override this method to skip this step.\n    def _prepare_inputs(\n        self, inputs: dict[str, Union[torch.Tensor, Any]]\n    ) -> dict[str, Union[torch.Tensor, Any]]:\n        device = self.accelerator.device\n        prompts = [x[\"prompt\"] for x in inputs]\n        images = [x[\"image\"] for x in inputs]\n        prompts_text = [\n            maybe_apply_chat_template(example, self.processing_class)[\"prompt\"]\n            for example in inputs\n        ]\n        prompt_inputs = self.processing_class(\n            # prompts_text, return_tensors=\"pt\", padding=True, padding_side=\"left\", add_special_tokens=False\n            text=prompts_text,\n            images=images,\n            return_tensors=\"pt\",\n            padding=True,\n            padding_side=\"left\",\n            add_special_tokens=False,\n        )\n        prompt_ids, prompt_mask = (\n            prompt_inputs[\"input_ids\"].to(device),\n            prompt_inputs[\"attention_mask\"].to(device),\n        )\n        if self.max_prompt_length is not None:\n            prompt_ids = prompt_ids[:, -self.max_prompt_length :]\n            prompt_mask = prompt_mask[:, -self.max_prompt_length :]\n\n        if self.args.use_vllm:\n            # First, have main process load weights if needed\n            if self.state.global_step != self._last_loaded_step:\n                with unwrap_model_for_generation(\n                    self.model,\n                    self.accelerator,\n                    gather_deepspeed3_params=False,  # TODO: fix this, self.args.ds3_gather_for_generation,\n                ) as unwrapped_model:\n                    if is_compiled_module(unwrapped_model):\n                        state_dict = unwrapped_model._orig_mod.state_dict()\n                    else:\n                        state_dict = unwrapped_model.state_dict()\n                if self.accelerator.is_main_process:\n                    llm_model = (\n                        self.llm.llm_engine.model_executor.driver_worker.model_runner.model\n                    )\n                    llm_model.load_weights(state_dict.items())\n                self._last_loaded_step = self.state.global_step\n\n            # Generate completions using vLLM: gather all prompts and use them in a single call in the main process\n            all_prompts_text = gather_object(prompts_text)\n            all_images = gather_object(images)\n            # group into pairs\n            all_multimodal_inputs = [\n                {\"prompt\": p, \"multi_modal_data\": {\"image\": i}}\n                for p, i in zip(all_prompts_text, all_images)\n            ]\n\n            if self.accelerator.is_main_process:\n                outputs = self.llm.generate(\n                    all_multimodal_inputs,\n                    sampling_params=self.sampling_params,\n                    use_tqdm=False,\n                )\n                completion_ids = [\n                    out.token_ids\n                    for completions in outputs\n                    for out in completions.outputs\n                ]\n            else:\n                completion_ids = [None] * len(all_prompts_text)\n            completion_ids = broadcast_object_list(completion_ids, from_process=0)\n            process_slice = slice(\n                self.accelerator.process_index * len(prompts),\n                (self.accelerator.process_index + 1) * len(prompts),\n            )\n            completion_ids = completion_ids[process_slice]\n\n            # Pad the completions, and concatenate them with the prompts\n            completion_ids = [\n                torch.tensor(ids, device=device) for ids in completion_ids\n            ]\n            completion_ids = pad(\n                completion_ids, padding_value=self.processing_class.pad_token_id\n            )\n            prompt_completion_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n        else:\n            raise ValueError(\"Only vLLM generation is supported in this version \")\n\n        # below are the same with yifan's code\n        # Mask everything after the first EOS token\n        is_eos = completion_ids == self.processing_class.eos_token_id\n        device = self.accelerator.device\n        eos_idx = torch.full(\n            (is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device\n        )\n        eos_idx[is_eos.any(dim=1)] = is_eos.int().argmax(dim=1)[is_eos.any(dim=1)]\n        sequence_indices = torch.arange(is_eos.size(1), device=device).expand(\n            is_eos.size(0), -1\n        )\n        completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()\n\n        # Concatenate prompt_mask with completion_mask for logit computation\n        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)  # (B*G, P+C)\n        # pixel_values = prompt_inputs[\"pixel_values\"].repeat_interleave(\n        #     self.num_generations, dim=0\n        # )\n\n        pixel_values = prompt_inputs[\"pixel_values\"]\n        # [None].repeat_interleave(self.num_generations, dim=0)\n        # pixel_values = pixel_values.view(-1, pixel_values.shape[-1])\n\n        image_grid_thw = prompt_inputs[\"image_grid_thw\"]\n        # .repeat_interleave(\n        #     self.num_generations, dim=0\n        # )\n        logits_to_keep = completion_ids.size(1)\n\n        with torch.inference_mode():\n            if self.ref_model is not None:\n                ref_per_token_logps = self._get_per_token_logps(\n                    self.ref_model,\n                    prompt_completion_ids,\n                    attention_mask,\n                    pixel_values,\n                    image_grid_thw,\n                    logits_to_keep,\n                )\n            else:\n                with self.accelerator.unwrap_model(self.model).disable_adapter():\n                    ref_per_token_logps = self._get_per_token_logps(\n                        self.model,\n                        prompt_completion_ids,\n                        attention_mask,\n                        pixel_values,\n                        image_grid_thw,\n                        logits_to_keep,\n                    )\n\n        # Decode the generated completions\n        completions = self.processing_class.batch_decode(\n            completion_ids, skip_special_tokens=True\n        )\n        if is_conversational(inputs[0]):\n            completions = [\n                [{\"role\": \"assistant\", \"content\": completion}]\n                for completion in completions\n            ]\n\n        # Compute the rewards\n        rewards_per_func = torch.zeros(\n            len(prompts), len(self.reward_funcs), device=device\n        )\n        for i, (reward_func, reward_processing_class) in enumerate(\n            zip(self.reward_funcs, self.reward_processing_classes)\n        ):\n            if isinstance(reward_func, PreTrainedModel):\n                if is_conversational(inputs[0]):\n                    messages = [\n                        {\"messages\": p + c} for p, c in zip(prompts, completions)\n                    ]\n                    texts = [\n                        apply_chat_template(x, reward_processing_class)[\"text\"]\n                        for x in messages\n                    ]\n                else:\n                    texts = [p + c for p, c in zip(prompts, completions)]\n                reward_inputs = reward_processing_class(\n                    texts,\n                    return_tensors=\"pt\",\n                    padding=True,\n                    padding_side=\"right\",\n                    add_special_tokens=False,\n                )\n                reward_inputs = super()._prepare_inputs(reward_inputs)\n                with torch.inference_mode():\n                    rewards_per_func[:, i] = reward_func(**reward_inputs).logits[\n                        :, 0\n                    ]  # Shape (B*G,)\n            else:\n                # Repeat all input columns (but \"prompt\" and \"completion\") to match the number of generations\n                reward_kwargs = {\n                    key: []\n                    for key in inputs[0].keys()\n                    if key not in [\"prompt\", \"completion\"]\n                }\n                for key in reward_kwargs:\n                    for example in inputs:\n                        # Repeat each value in the column for `num_generations` times\n                        reward_kwargs[key].extend([example[key]] * self.num_generations)\n                output_reward_func = reward_func(\n                    prompts=prompts, completions=completions, **reward_kwargs\n                )\n                rewards_per_func[:, i] = torch.tensor(\n                    output_reward_func, dtype=torch.float32, device=device\n                )\n        rewards_per_func = gather(rewards_per_func)\n        # Sum the rewards from all reward functions\n        rewards = rewards_per_func.sum(dim=1)\n\n        # Compute grouped-wise rewards\n        mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)\n        std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)\n\n        # Normalize the rewards to compute the advantages\n        mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(\n            self.num_generations, dim=0\n        )\n        std_grouped_rewards = std_grouped_rewards.repeat_interleave(\n            self.num_generations, dim=0\n        )\n        advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)\n\n        # Slice to keep only the local part of the data\n        process_slice = slice(\n            self.accelerator.process_index * len(prompts),\n            (self.accelerator.process_index + 1) * len(prompts),\n        )\n        advantages = advantages[process_slice]\n\n        # Log the metrics\n        reward_per_func = rewards_per_func.mean(0)\n        for i, reward_func in enumerate(self.reward_funcs):\n            if isinstance(\n                reward_func, nn.Module\n            ):  # Module instead of PretrainedModel for compat with compiled models\n                reward_func_name = reward_func.config._name_or_path.split(\"/\")[-1]\n            else:\n                reward_func_name = reward_func.__name__\n            self._metrics[f\"rewards/{reward_func_name}\"].append(\n                reward_per_func[i].item()\n            )\n\n        self._metrics[\"reward\"].append(rewards.mean().item())\n        self._metrics[\"reward_std\"].append(std_grouped_rewards.mean().item())\n\n        return {\n            \"prompt_ids\": prompt_ids,\n            \"prompt_mask\": prompt_mask,\n            \"completion_ids\": completion_ids,\n            \"completion_mask\": completion_mask,\n            \"ref_per_token_logps\": ref_per_token_logps,\n            \"advantages\": advantages,\n            \"pixel_values\": pixel_values,\n            \"image_grid_thw\": image_grid_thw,\n        }\n\n    def compute_loss(\n        self, model, inputs, return_outputs=False, num_items_in_batch=None\n    ):\n        if return_outputs:\n            raise ValueError(\"The GRPOTrainer does not support returning outputs\")\n        # Compute the per-token log probabilities for the model\n\n        prompt_ids, prompt_mask = inputs[\"prompt_ids\"], inputs[\"prompt_mask\"]\n        completion_ids, completion_mask = (\n            inputs[\"completion_ids\"],\n            inputs[\"completion_mask\"],\n        )\n        input_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)\n        pixel_values = inputs[\"pixel_values\"]\n        image_grid_thw = inputs[\"image_grid_thw\"]\n        logits_to_keep = completion_ids.size(\n            1\n        )  # we only need to compute the logits for the completion tokens\n\n        per_token_logps = self._get_per_token_logps(\n            model,\n            input_ids,\n            attention_mask,\n            pixel_values,\n            image_grid_thw,\n            logits_to_keep,\n        )\n\n        # Compute the KL divergence between the model and the reference model\n        ref_per_token_logps = inputs[\"ref_per_token_logps\"]\n        per_token_kl = (\n            torch.exp(ref_per_token_logps - per_token_logps)\n            - (ref_per_token_logps - per_token_logps)\n            - 1\n        )\n\n        # x - x.detach() allows for preserving gradients from x\n        advantages = inputs[\"advantages\"]\n        per_token_loss = torch.exp(\n            per_token_logps - per_token_logps.detach()\n        ) * advantages.unsqueeze(1)\n        per_token_loss = -(per_token_loss - self.beta * per_token_kl)\n        loss = (\n            (per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)\n        ).mean()\n\n        # Log the metrics\n        completion_length = (\n            self.accelerator.gather_for_metrics(completion_mask.sum(1))\n            .float()\n            .mean()\n            .item()\n        )\n        self._metrics[\"completion_length\"].append(completion_length)\n\n        mean_kl = (\n            (per_token_kl * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)\n        ).mean()\n        self._metrics[\"kl\"].append(\n            self.accelerator.gather_for_metrics(mean_kl).mean().item()\n        )\n\n        return loss\n\n        \n    def log(self, logs: dict[str, float], start_time: Optional[float] = None) -> None:\n        metrics = {key: sum(val) / len(val) for key, val in self._metrics.items()}  # average the metrics\n\n        # This method can be called both in training and evaluation. When called in evaluation, the keys in `logs`\n        # start with \"eval_\". We need to add the prefix \"eval_\" to the keys in `metrics` to match the format.\n        if next(iter(logs.keys())).startswith(\"eval_\"):\n            metrics = {f\"eval_{key}\": val for key, val in metrics.items()}\n\n        logs = {**logs, **metrics}\n        if version.parse(transformers.__version__) >= version.parse(\"4.47.0.dev0\"):\n            super().log(logs, start_time)\n        else:  # transformers<=4.46\n            super().log(logs)\n        self._metrics.clear()```\n\n\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/gz-data/train_single_GPU_Full_GPRO_LoRA_vLLM.py\", line 190, in <module>\n[rank0]:     trainer.train()\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/transformers/trainer.py\", line 2241, in train\n[rank0]:     return inner_training_loop(\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/transformers/trainer.py\", line 2548, in _inner_training_loop\n[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/transformers/trainer.py\", line 3692, in training_step\n[rank0]:     inputs = self._prepare_inputs(inputs)\n[rank0]:   File \"/gz-data/vllm_grpo_trainer.py\", line 556, in _prepare_inputs\n[rank0]:     outputs = self.llm.generate(\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/vllm/utils.py\", line 1057, in inner\n[rank0]:     return fn(*args, **kwargs)\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 469, in generate\n[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 1397, in _run_engine\n[rank0]:     step_outputs = self.llm_engine.step()\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 1391, in step\n[rank0]:     outputs = self.model_executor.execute_model(\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 139, in execute_model\n[rank0]:     output = self.collective_rpc(\"execute_model\",\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n[rank0]:     answer = run_method(self.driver_worker, method, args, kwargs)\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/vllm/utils.py\", line 2196, in run_method\n[rank0]:     return func(*args, **kwargs)\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 420, in execute_model\n[rank0]:     output = self.model_runner.execute_model(\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[rank0]:     return func(*args, **kwargs)\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1724, in execute_model\n[rank0]:     hidden_or_intermediate_states = model_executable(\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2_5_vl.py\", line 1062, in forward\n[rank0]:     inputs_embeds = self.get_input_embeddings_v0(\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/qwen2_5_vl.py\", line 995, in get_input_embeddings_v0\n[rank0]:     inputs_embeds = merge_multimodal_embeddings(\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 455, in merge_multimodal_embeddings\n[rank0]:     return _merge_multimodal_embeddings(\n[rank0]:   File \"/usr/local/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 371, in _merge_multimodal_embeddings\n[rank0]:     raise ValueError(\n[rank0]: ValueError: Attempted to assign 25 + 25 + 25 = 75 multimodal tokens to 147 placeholders\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-05-22T22:16:11+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18572/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/18572"
  },
  {
    "number": 8176,
    "title": "[Performance]: reproducing vLLM performance benchmark",
    "body": "### Proposal to improve performance\r\n\r\n_No response_\r\n\r\n### Report of performance regression\r\n\r\n_No response_\r\n\r\n### Misc discussion on performance\r\n\r\nTo reproduce vLLM's performance benchmark, please launch a shell in the following docker images:\r\n\r\n- SGlang: `lmsysorg/sglang:v0.3.0-cu124`\r\n- lmdeploy: `openmmlab/lmdeploy:v0.6.0a0-cu12`\r\n- TensorRT-LLM: `nvcr.io/nvidia/tritonserver:24.07-trtllm-python-py3`\r\n- vLLM: `vllm/vllm-openai:v0.6.0`\r\n\r\nAnd then run the following bash script (don't forget to replace <your HF TOKEN> with your huggingface token that has Llama-3 model access):\r\n```bash\r\nexport HF_TOKEN=<your HF TOKEN>\r\napt update\r\napt install -y wget unzip \r\n# download benchmarking code\r\nwget -O benchmarking_code.zip https://buildkite.com/organizations/vllm/pipelines/performance-benchmark/builds/8532/jobs/0191bbbf-c603-4c15-9f5d-e0b2933ba097/artifacts/0191bd2a-d6cd-4f6d-b618-a7aa2c39456c\r\nunzip benchmarking_code.zip\r\n# remove previous results\r\nrm -r ./benchmarks/results\r\nVLLM_SOURCE_CODE_LOC=$(pwd) bash .buildkite/nightly-benchmarks/scripts/run-nightly-benchmarks.sh\r\n```\r\n\r\nYour benchmarking results will be in `./benchmarks/results`, with the name format of `xxx_nightly_results.json `and can be loaded and converted to pandas dataframe by `pandas.DataFrame.from_dict()`. Each benchmark run takes roughly 1 hour 10 minutes assuming that the model weights are already downloaded (and 1 hour 30 minutes for TensorRT-LLM as it needs to convert the model to triton inference engine).\r\n\r\nWhen you run the H100 benchmark inside TensorRT-LLM docker container, you may experience a memory leaking issue ([issue link](https://github.com/triton-inference-server/tensorrtllm_backend/issues/587)). In this case, please add the following code\r\n```bash\r\n      # temporary fix for trt\r\n      kill_gpu_processes\r\n      bash -c \"python3 /tensorrtllm_backend/scripts/launch_triton_server.py \\\r\n              --world_size=${tp} \\\r\n              --model_repo=/tensorrtllm_backend/triton_model_repo & \" </dev/null >/dev/null 2>&1 &\r\n      wait_for_server\r\n```\r\nto Line 211 (right after the for loop) in `./.buildkite/nightly-benchmarks/scripts/run-nightly-benchmarks.sh` to force TensorRT-LLM to restart the serve more often.\r\n\r\n\r\nKnown issue: \r\n\r\n- In different serving engines, the # of output tokens do not strictly align (even after setting `ignore_eos` or `max_length` due to imperfect implementation of these two flags in different engines). That said, the number of tokens generated by vLLM is roughly aligned with other engines as all engines are performing greedy sampling using the same model.\r\n\r\n### Your current environment (if you think it is necessary)\r\n\r\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-05T04:47:18+00:00",
    "closed_at": "2024-12-23T18:48:16+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8176/reactions",
      "total_count": 5,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 2,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8176"
  },
  {
    "number": 214,
    "title": "`8-bit quantization` support",
    "body": "As far as I know `vllm` and `ray` doesn't support `8-bit quantization` as of now. I think it's the most viable quantization technique out there and should be implemented for faster inference and reduced memory usage. ",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2023-06-22T23:02:03+00:00",
    "closed_at": "2024-04-18T13:52:08+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/214/reactions",
      "total_count": 68,
      "+1": 50,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 18,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/214"
  },
  {
    "number": 12559,
    "title": "[Bug]: vllm container does not set LD_LIBRARY_PATH correctly",
    "body": "### Your current environment\n\nvllm container 0.7.0\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nmycontainer# env|grep LD_ \nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n\nBut...\n/usr/local/nvidia does not exist in the container\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.\n",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-29T17:30:27+00:00",
    "closed_at": "2025-06-01T02:14:31+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12559/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12559"
  },
  {
    "number": 4277,
    "title": "[Bug]: vllm stall on llama3-70b warmup with 0.4.1",
    "body": "### Your current environment\r\n\r\nI'm running a minimally modified image of `nvcr.io/nvidia/pytorch:23.10-py3` with Python 3.11.\r\n\r\n```\r\nPyTorch version: 2.2.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.5 (main, Aug 26 2023, 07:22:50) [Clang 16.0.3 ] (64-bit runtime)\r\nPython platform: Linux-4.4.0-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nGPU 1: NVIDIA A100-SXM4-80GB\r\n\r\nNvidia driver version: 535.129.03\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.5\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nAddress sizes:       46 bits physical, 48 bits virtual\r\nByte Order:          Little Endian\r\nCPU(s):              5\r\nOn-line CPU(s) list: 0-4\r\nVendor ID:           GenuineIntel\r\nModel name:          unknown\r\nCPU family:          6\r\nModel:               85\r\nThread(s) per core:  0\r\nCore(s) per socket:  0\r\nSocket(s):           0\r\nStepping:            unknown\r\nBogoMIPS:            2200.19\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_vnni md_clear arch_capabilities\r\nHypervisor vendor:   KVM\r\nVirtualization type: full\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.19.3\r\n[pip3] torch==2.2.1\r\n[pip3] triton==2.2.0\r\n[pip3] vllm_nccl_cu11==2.18.1.0.3.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.1\r\nvLLM Build Flags:\r\nCUDA Archs: 5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI am trying to boot a llama-3 70b model with llvm, but it times out while trying to instantiate. So far I've let it rest for upwards of 10min with minimal observed GPU activity. It allocates a maximum of 7GB despite having access to the full 160GB (A100 80x2).\r\n\r\n```python\r\nMODEL_DIR = \"/model\"\r\nMODEL_NAME = \"NousResearch/Meta-Llama-3-70B-Instruct\"\r\n```\r\n\r\nBefore this run, I'm downloading the safetensor files through huggingface-cli and placing them in the local /model volume. No downloading seems to be happening when the entrypoint boots.\r\n\r\n```python\r\nengine_args = AsyncEngineArgs(\r\n  model=MODEL_DIR,\r\n  tensor_parallel_size=2,\r\n  gpu_memory_utilization=0.90,\r\n  enforce_eager=False,\r\n  disable_log_requests=True,\r\n)\r\nengine = AsyncLLMEngine.from_engine_args(engine_args)\r\n```\r\n\r\n```\r\nStarting cold inference...\r\n2024-04-22 22:29:35,056\tINFO worker.py:1749 -- Started a local Ray instance.\r\nINFO 04-22 22:29:40 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='/model', speculative_config=None, tokenizer='/model', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nINFO 04-22 22:29:56 utils.py:570] Found nccl from environment variable VLLM_NCCL_SO_PATH=/usr/lib/x86_64-linux-gnu/libnccl.so\r\n(RayWorkerWrapper pid=419) INFO 04-22 22:29:56 utils.py:570] Found nccl from environment variable VLLM_NCCL_SO_PATH=/usr/lib/x86_64-linux-gnu/libnccl.so\r\nINFO 04-22 22:29:58 selector.py:28] Using FlashAttention backend.\r\n(RayWorkerWrapper pid=419) INFO 04-22 22:29:58 selector.py:28] Using FlashAttention backend.\r\nINFO 04-22 22:29:59 pynccl_utils.py:45] vLLM is using nccl==2.19.3\r\n(RayWorkerWrapper pid=419) INFO 04-22 22:29:59 pynccl_utils.py:45] vLLM is using nccl==2.19.3\r\n```\r\n\r\nNote that before getting here, my env was throwing an exception related to https://github.com/vllm-project/vllm/issues/4257, so I'm providing the NCCL path manually:\r\n\r\n```\r\nexport VLLM_NCCL_SO_PATH=\"/usr/lib/x86_64-linux-gnu/libnccl.so\"\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-04-22T22:41:13+00:00",
    "closed_at": "2024-06-13T09:02:35+00:00",
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4277/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4277"
  },
  {
    "number": 16450,
    "title": "[Bug]: DeepSeek-R1 KeyError: 'layers.61.mlp.experts.w2_weight'",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n- cmd\n\n```bash\nexport VLLM_USE_V1=0 && export VLLM_PP_LAYER_PARTITION=\"22,20,19\"\nnohup python3 -m vllm.entrypoints.openai.api_server \\\n        --model=/workspace/dev/hf_models/DeepSeek-R1 \\\n        --dtype=auto \\\n        --block-size 32 \\\n        --tokenizer-mode=slow \\\n        --max-model-len 32768 \\\n        --max-num-batched-tokens 2048 \\\n        --tensor-parallel-size 8 \\\n        --pipeline-parallel-size 3 \\\n        --gpu-memory-utilization 0.90 \\\n        --max-num-seqs 48 \\\n        --trust-remote-code \\\n        --no-enable-prefix-caching \\\n        --enable-chunked-prefill=True \\\n        --disable-custom-all-reduce \\\n        --max-log-len 0 \\\n        --port 8862 > vllm.R1.log.3 2>&1 &\n```\n\n- error\n\n```bash\n quantized or ignored modules [repeated 22x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620] Error executing method 'load_model'. This might cause deadlock in distributed execution. [repeated 7x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620] Traceback (most recent call last): [repeated 7x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 612, in execute_method [repeated 7x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620]     return run_method(self, method, args, kwargs) [repeated 7x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620]   File \"/usr/local/lib/python3.10/dist-packages/vllm/utils.py\", line 2363, in run_method [repeated 7x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620]     return func(*args, **kwargs) [repeated 7x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 455, in load_model [repeated 21x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620]     self.model_runner.load_model() [repeated 7x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620]     self.model = get_model(vllm_config=self.vllm_config) [repeated 7x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model [repeated 7x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620]     return loader.load_model(vllm_config=vllm_config) [repeated 7x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620]     loaded_weights = model.load_weights( [repeated 7x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/deepseek_v2.py\", line 728, in load_weights [repeated 21x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620]     return loader.load_weights(weights) [repeated 7x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620]     autoloaded_weights = set(self._load_module(\"\", self.module, weights)) [repeated 7x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 195, in _load_module [repeated 14x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620]     yield from self._load_module(prefix, [repeated 7x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620]     loaded_params = module_load_weights(weights) [repeated 7x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620]     param = params_dict[name] [repeated 7x across cluster]\n(RayWorkerWrapper pid=269885, ip=10.189.109.87) ERROR 04-11 11:26:12 [worker_base.py:620] KeyError: 'layers.61.mlp.experts.w2_weight' [repeated 7x across cluster]\n```\n\nmore log:\n\n```bash\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n[rank0]:     return _run_code(code, main_globals, None,\n[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n[rank0]:     exec(code, run_globals)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1121, in <module>\n[rank0]:     uvloop.run(run_server(args))\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/uvloop/__init__.py\", line 82, in run\n[rank0]:     return loop.run_until_complete(wrapper())\n[rank0]:   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n[rank0]:     return await main\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1069, in run_server\n[rank0]:     async with build_async_engine_client(args) as engine_client:\n[rank0]:   File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n[rank0]:     return await anext(self.gen)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 146, in build_async_engine_client\n[rank0]:     async with build_async_engine_client_from_engine_args(\n[rank0]:   File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n[rank0]:     return await anext(self.gen)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 194, in build_async_engine_client_from_engine_args\n[rank0]:     engine_client = AsyncLLMEngine.from_vllm_config(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 653, in from_vllm_config\n[rank0]:     return cls(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 608, in __init__\n[rank0]:     self.engine = self._engine_class(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 267, in __init__\n[rank0]:     super().__init__(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 282, in __init__\n[rank0]:     self.model_executor = executor_class(vllm_config=vllm_config, )\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\", line 286, in __init__\n[rank0]:     super().__init__(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\", line 52, in __init__\n[rank0]:     self._init_executor()\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/ray_distributed_executor.py\", line 114, in _init_executor\n[rank0]:     self._init_workers_ray(placement_group)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/ray_distributed_executor.py\", line 396, in _init_workers_ray\n[rank0]:     self._run_workers(\"load_model\",\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/ray_distributed_executor.py\", line 516, in _run_workers\n[rank0]:     self.driver_worker.execute_method(sent_method, *args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 621, in execute_method\n[rank0]:     raise e\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 612, in execute_method\n[rank0]:     return run_method(self, method, args, kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/utils.py\", line 2363, in run_method\n[rank0]:     return func(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 183, in load_model\n[rank0]:     self.model_runner.load_model()\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1113, in load_model\n[rank0]:     self.model = get_model(vllm_config=self.vllm_config)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n[rank0]:     return loader.load_model(vllm_config=vllm_config)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 455, in load_model\n[rank0]:     loaded_weights = model.load_weights(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/deepseek_v2.py\", line 826, in load_weights\n[rank0]:     return loader.load_weights(weights)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 261, in load_weights\n[rank0]:     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 222, in _load_module\n[rank0]:     yield from self._load_module(prefix,\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 195, in _load_module\n[rank0]:     loaded_params = module_load_weights(weights)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/deepseek_v2.py\", line 728, in load_weights\n[rank0]:     param = params_dict[name]\n[rank0]: KeyError: 'layers.61.mlp.experts.w2_weight'\n```\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-04-11T03:33:48+00:00",
    "closed_at": "2025-04-11T06:44:24+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16450/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16450"
  },
  {
    "number": 6276,
    "title": "[Bug]:  deepseek-coder-v2-lite-instruct;  Exception in worker VllmWorkerProcess while processing method initialize_cache: [Errno 2] No such file or directory: '/root/.triton/cache/de758c429c9ff1f18930bbd9c3004506/fused_moe_kernel.json.tmp.pid_1528_587007', Traceback (most recent call last):",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\r\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\r\nClang version: Could not collect\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.2 (default, Feb 28 2021, 17:03:44)  [GCC 10.2.1 20210110] (64-bit runtime)\r\nPython platform: Linux-5.4.143.bsk.8-amd64-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA L40\r\nGPU 1: NVIDIA L40\r\nGPU 2: NVIDIA L40\r\nGPU 3: NVIDIA L40\r\n\r\nNvidia driver version: Could not collect\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   52 bits physical, 57 bits virtual\r\nCPU(s):                          180\r\nOn-line CPU(s) list:             0-179\r\nThread(s) per core:              2\r\nCore(s) per socket:              45\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           143\r\nModel name:                      Intel(R) Xeon(R) Platinum 8457C\r\nStepping:                        8\r\nCPU MHz:                         2599.520\r\nBogoMIPS:                        5199.04\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       4.2 MiB\r\nL1i cache:                       2.8 MiB\r\nL2 cache:                        180 MiB\r\nL3 cache:                        195 MiB\r\nNUMA node0 CPU(s):               0-89\r\nNUMA node1 CPU(s):               90-179\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Mitigation; TSX disabled\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 wbnoinvd arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid cldemote movdiri movdir64b md_clear arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] byted-torch==2.1.0.post2\r\n[pip3] numpy==1.26.2\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] torchaudio==2.1.0+cu121\r\n[pip3] torchvision==0.18.0\r\n[pip3] transformers==4.42.3\r\n[pip3] triton==2.3.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NODE    NODE    SYS     SYS     1,4-89  0               N/A\r\nGPU1    NODE     X      NODE    SYS     SYS     1,4-89  0               N/A\r\nGPU2    NODE    NODE     X      SYS     SYS     1,4-89  0               N/A\r\nGPU3    SYS     SYS     SYS      X      SYS     91,94-179       1               N/A\r\nNIC0    SYS     SYS     SYS     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\n When using L40 4-card inference, the following error occurs probabilistically\uff1a\r\n```bash\r\n ERROR 07-10 09:23:56 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method initialize_cache: [Errno 2] No such file or directory: '/root/.triton/cache/de758c429c9ff1f18930bbd9c3004506/fused_moe_kernel.json.tmp.pid_1528_587007', Traceback (most recent call last):\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/vllm/worker/worker.py\", line 214, in initialize_cache\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     self._warm_up_model()\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/vllm/worker/worker.py\", line 230, in _warm_up_model\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     self.model_runner.capture_model(self.gpu_cache)\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/vllm/worker/model_runner.py\", line 1109, in capture_model\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     graph_runner.capture(**capture_inputs)\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/vllm/worker/model_runner.py\", line 1327, in capture\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     self.model(\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/vllm/model_executor/models/deepseek_v2.py\", line 482, in forward\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     hidden_states = self.model(input_ids, positions, kv_caches,\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/vllm/model_executor/models/deepseek_v2.py\", line 449, in forward\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     hidden_states, residual = layer(positions, hidden_states,\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/vllm/model_executor/models/deepseek_v2.py\", line 407, in forward\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     hidden_states = self.mlp(hidden_states)\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/vllm/model_executor/models/deepseek_v2.py\", line 164, in forward\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     final_hidden_states = fused_experts(\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 506, in fused_experts\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     invoke_fused_moe_kernel(intermediate_cache2,\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 246, in invoke_fused_moe_kernel\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     fused_moe_kernel[grid](\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/triton/runtime/jit.py\", line 167, in <lambda>\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/triton/runtime/jit.py\", line 416, in run\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     self.cache[device][key] = compile(\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/triton/compiler/compiler.py\", line 202, in compile\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     return CompiledKernel(so_path, metadata_group.get(metadata_filename))\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/triton/compiler/compiler.py\", line 230, in __init__\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     self.asm = {\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.9/dist-packages/triton/compiler/compiler.py\", line 231, in <dictcomp>\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     file.suffix[1:]: file.read_bytes() if file.suffix[1:] == driver.binary_ext else file.read_text()\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/lib/python3.9/pathlib.py\", line 1255, in read_text\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     with self.open(mode='r', encoding=encoding, errors=errors) as f:\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/lib/python3.9/pathlib.py\", line 1241, in open\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     return io.open(self, mode, buffering, encoding, errors, newline,\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]   File \"/usr/lib/python3.9/pathlib.py\", line 1109, in _opener\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226]     return self._accessor.open(self, flags, mode)\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226] FileNotFoundError: [Errno 2] No such file or directory: '/root/.triton/cache/de758c429c9ff1f18930bbd9c3004506/fused_moe_kernel.json.tmp.pid_1528_587007'\r\n(VllmWorkerProcess pid=1527) ERROR 07-10 09:23:56 multiproc_worker_utils.py:226] \r\n```",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-10T01:32:01+00:00",
    "closed_at": "2024-11-25T02:04:50+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6276/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6276"
  },
  {
    "number": 3392,
    "title": "AWQ + Marlin Error",
    "body": "I convert model follow AutoAWQ library as follow script.\r\n\r\n1. Quantize with Marlin\r\n```python\r\nfrom awq import AutoAWQForCausalLM\r\nfrom transformers import AutoTokenizer\r\n\r\nmodel_path = 'mistralai/Mistral-7B-Instruct-v0.2'\r\nquant_path = 'mistral-instruct-v0.2-awq-marlin'\r\nquant_config = { \"zero_point\": False, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"Marlin\" }\r\n\r\n# Load model\r\nmodel = AutoAWQForCausalLM.from_pretrained(\r\n    model_path, **{\"low_cpu_mem_usage\": True, \"use_cache\": False}\r\n)\r\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\n\r\n# Quantize\r\nmodel.quantize(tokenizer, quant_config=quant_config)\r\n\r\n# Save quantized model\r\nmodel.save_quantized(quant_path)\r\ntokenizer.save_pretrained(quant_path)\r\n\r\nprint(f'Model is quantized and saved at \"{quant_path}\"')\r\n```\r\n\r\n2. Generate\r\n\r\n```python\r\nfrom awq import AutoAWQForCausalLM\r\nfrom transformers import AutoTokenizer, TextStreamer\r\n\r\n\r\nquant_path = \"./mistral-instruct-v0.2-awq-marlin\"\r\n\r\n# Load model\r\nmodel = AutoAWQForCausalLM.from_quantized(quant_path, fuse_layers=False)\r\ntokenizer = AutoTokenizer.from_pretrained(quant_path, trust_remote_code=True)\r\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\r\n\r\n# Convert prompt to tokens\r\nprompt_template = \"[INST] {prompt} [/INST]\"\r\n\r\nprompt = \"You're standing on the surface of the Earth. \"\\\r\n        \"You walk one mile south, one mile west and one mile north. \"\\\r\n        \"You end up exactly where you started. Where are you?\"\r\n\r\ntokens = tokenizer(\r\n    prompt_template.format(prompt=prompt), \r\n    return_tensors='pt'\r\n).input_ids.cuda()\r\n\r\n# Generate output\r\ngeneration_output = model.generate(\r\n    tokens, \r\n    streamer=streamer,\r\n    max_new_tokens=512\r\n)\r\n```\r\n\r\n2 steps above work perfectly.\r\n\r\nBut when I using that quantization folder to start serving in vllm with this script.\r\n\r\n```bash\r\npython services/vllm/api_server.py \\\r\n    --host 0.0.0.0 \\\r\n    --port 8000 \\\r\n    --disable-log-requests \\\r\n    --model /root/AutoAWQ/mistral-instruct-v0.2-awq-marlin \\\r\n    --quantization awq \\\r\n    --trust-remote-code \\\r\n    --dtype float16 \\\r\n```\r\n\r\nThe error show that:\r\n```bash\r\n  File \"/root/vllm/vllm/engine/llm_engine.py\", line 101, in __init__\r\n    self.model_executor = executor_class(model_config, cache_config,\r\n  File \"/root/vllm/vllm/executor/gpu_executor.py\", line 42, in __init__\r\n    self._init_worker()\r\n  File \"/root/vllm/vllm/executor/gpu_executor.py\", line 77, in _init_worker\r\n    self.driver_worker.load_model()\r\n  File \"/root/vllm/vllm/worker/worker.py\", line 99, in load_model\r\n    self.model_runner.load_model()\r\n  File \"/root/vllm/vllm/worker/model_runner.py\", line 89, in load_model\r\n    self.model = get_model(self.model_config,\r\n  File \"/root/vllm/vllm/model_executor/utils.py\", line 52, in get_model\r\n    return get_model_fn(model_config, device_config, **kwargs)\r\n  File \"/root/vllm/vllm/model_executor/model_loader.py\", line 86, in get_model\r\n    model.load_weights(model_config.model, model_config.download_dir,\r\n  File \"/root/vllm/vllm/model_executor/models/llama.py\", line 391, in load_weights\r\n    weight_loader(param, loaded_weight)\r\n  File \"/root/vllm/vllm/model_executor/layers/linear.py\", line 556, in weight_loader\r\n    loaded_weight = loaded_weight.narrow(input_dim, start_idx,\r\nRuntimeError: start (0) + length (14336) exceeds dimension size (896).\r\n```\r\n\r\nIt look like the weight can not load and vllm did not support AWQ with Marlin kernel properly. How can we fix this error.\r\nThank you.",
    "labels": [],
    "state": "closed",
    "created_at": "2024-03-14T04:52:22+00:00",
    "closed_at": "2024-07-25T19:53:41+00:00",
    "comments": 16,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3392/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3392"
  },
  {
    "number": 17117,
    "title": "[Bug]: Why does torch.cuda.memory_allocated() remain unchanged after calling sleep()?",
    "body": "### Your current environment\n\nHi! Although the free bytes increase (as shown by torch.cuda.mem_get_info()[0]), why does torch.cuda.memory_allocated() remain unchanged after calling sleep()?\n\n<details>\n<summary>See myexperience</summary>\n\n```text\n(Pdb) print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6:.2f} MB\")\nAllocated: 28294.25 MB\n(Pdb) print(f\"Free bytes: {torch.cuda.mem_get_info()[0]/ 1e6:.2f} MB\")\nFree bytes: 79872.92 MB\n(Pdb) print(f\"Reserved : {torch.cuda.memory_reserved() / 1e6:.2f} MB\")\nReserved : 29047.65 MB\n(Pdb) self.llm.wake_up()\nINFO 04-24 14:05:33 executor_base.py:219] It took 0.298841 seconds to wake up.\n(Pdb) print(f\"Reserved : {torch.cuda.memory_reserved() / 1e6:.2f} MB\")\nReserved : 29047.65 MB\n(Pdb) print(f\"Free bytes: {torch.cuda.mem_get_info()[0]/ 1e6:.2f} MB\")\nFree bytes: 54379.94 MB\n(Pdb) print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6:.2f} MB\")\nAllocated: 28294.25 MB\n(Pdb) self.llm.sleep(level=2)\nINFO 04-24 14:05:59 worker.py:133] Sleep mode freed 23.74 GiB memory, 4.75 GiB memory is still in use.\nINFO 04-24 14:05:59 executor_base.py:208] It took 0.023361 seconds to fall asleep.\n(Pdb) print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6:.2f} MB\")\nAllocated: 28294.25 MB\n(Pdb) print(f\"Free bytes: {torch.cuda.mem_get_info()[0]/ 1e6:.2f} MB\")\nFree bytes: 79872.92 MB\n(Pdb) print(f\"Reserved : {torch.cuda.memory_reserved() / 1e6:.2f} MB\")\nReserved : 29047.65 MB\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nHi! Although the free bytes increase (as shown by torch.cuda.mem_get_info()[0]), why does torch.cuda.memory_allocated() remain unchanged after calling sleep()?\n\n<details>\n<summary>See myexperience</summary>\n\n```text\n(Pdb) print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6:.2f} MB\")\nAllocated: 28294.25 MB\n(Pdb) print(f\"Free bytes: {torch.cuda.mem_get_info()[0]/ 1e6:.2f} MB\")\nFree bytes: 79872.92 MB\n(Pdb) print(f\"Reserved : {torch.cuda.memory_reserved() / 1e6:.2f} MB\")\nReserved : 29047.65 MB\n(Pdb) self.llm.wake_up()\nINFO 04-24 14:05:33 executor_base.py:219] It took 0.298841 seconds to wake up.\n(Pdb) print(f\"Reserved : {torch.cuda.memory_reserved() / 1e6:.2f} MB\")\nReserved : 29047.65 MB\n(Pdb) print(f\"Free bytes: {torch.cuda.mem_get_info()[0]/ 1e6:.2f} MB\")\nFree bytes: 54379.94 MB\n(Pdb) print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6:.2f} MB\")\nAllocated: 28294.25 MB\n(Pdb) self.llm.sleep(level=2)\nINFO 04-24 14:05:59 worker.py:133] Sleep mode freed 23.74 GiB memory, 4.75 GiB memory is still in use.\nINFO 04-24 14:05:59 executor_base.py:208] It took 0.023361 seconds to fall asleep.\n(Pdb) print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6:.2f} MB\")\nAllocated: 28294.25 MB\n(Pdb) print(f\"Free bytes: {torch.cuda.mem_get_info()[0]/ 1e6:.2f} MB\")\nFree bytes: 79872.92 MB\n(Pdb) print(f\"Reserved : {torch.cuda.memory_reserved() / 1e6:.2f} MB\")\nReserved : 29047.65 MB\n```\n\n</details>\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.\n\nCC @youkaichao @fingertap @fabianlim ",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-04-24T15:38:59+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17117/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17117"
  },
  {
    "number": 6308,
    "title": "[Bug]: Gloo Connection reset by peer",
    "body": "### Your current environment\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.1 LTS (x86_64)\r\nGCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-58-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA L4\r\nGPU 1: NVIDIA L4\r\nGPU 2: NVIDIA L4\r\nGPU 3: NVIDIA L4\r\nGPU 4: NVIDIA L4\r\nGPU 5: NVIDIA L4\r\nGPU 6: NVIDIA L4\r\nGPU 7: NVIDIA L4\r\n\r\nNvidia driver version: 535.86.10\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] torchvision==0.18.0\r\n[pip3] transformers==4.42.3\r\n[pip3] triton==2.3.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\n```\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI'm running Llama3-70B on two nodes with 8 GPUs each using TP=16. I tried adding the options eager-mode and disable-custom-all-reduce without any success. First ~100 queries are always running fine, but after a while I get this Runtime Error:\r\n```\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348] Error executing method start_worker_execution_loop. This might cause deadlock in distributed execution.\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348] Traceback (most recent call last):\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]   File \"/secondary/thies/.virtualenvs/vllm/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 340, in execute_method\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]     return executor(*args, **kwargs)\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]   File \"/secondary/thies/.virtualenvs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]     return func(*args, **kwargs)\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]   File \"/secondary/thies/.virtualenvs/vllm/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 64, in start_worker_execution_loop\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]     output = self.execute_model(execute_model_req=None)\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]   File \"/secondary/thies/.virtualenvs/vllm/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 249, in execute_model\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]     broadcast_data = broadcast_tensor_dict(src=0)\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]   File \"/secondary/thies/.virtualenvs/vllm/lib/python3.10/site-packages/vllm/distributed/communication_op.py\", line 32, in broadcast_tensor_dict\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]     return get_tp_group().broadcast_tensor_dict(tensor_dict, src)\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]   File \"/secondary/thies/.virtualenvs/vllm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 528, in broadcast_tensor_dict\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]     metadata_list = self.broadcast_object(None, src=src)\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]   File \"/secondary/thies/.virtualenvs/vllm/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 390, in broadcast_object\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]     torch.distributed.broadcast_object_list(recv,\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]   File \"/secondary/thies/.virtualenvs/vllm/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]     return func(*args, **kwargs)\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]   File \"/secondary/thies/.virtualenvs/vllm/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2649, in broadcast_object_list\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]     broadcast(object_sizes_tensor, src=src, group=group)\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]   File \"/secondary/thies/.virtualenvs/vllm/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 75, in wrapper\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]     return func(*args, **kwargs)\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]   File \"/secondary/thies/.virtualenvs/vllm/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 2144, in broadcast\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348]     work.wait()\r\n(RayWorkerWrapper pid=191565) ERROR 07-10 17:40:09 worker_base.py:348] RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:525] Read error [172.26.161.177]:50407: Connection reset by peer\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-07-10T14:28:48+00:00",
    "closed_at": "2024-07-12T14:10:41+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6308/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6308"
  },
  {
    "number": 286,
    "title": "Long context will cause the vLLM stop",
    "body": "If I exceed the token limit of 4096, the vLLM abruptly stops. It would be helpful if you could incorporate some logging functionality into the stopping code. This way, users can easily modify the code to resume the vLLM from where it left off.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-06-28T07:07:46+00:00",
    "closed_at": "2023-11-05T01:30:47+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/286/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/286"
  },
  {
    "number": 20894,
    "title": "[Feature]: Make SequenceClassification(Qwen3ForSequenceClassification) models support auto prefix cache",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nAs more and more LLMs are used for classification tasks, can we consider supporting this feature?\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-07-14T02:01:16+00:00",
    "closed_at": null,
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20894/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20894"
  },
  {
    "number": 1755,
    "title": "vllm always tries to download model from huggingface/modelscope even if I specify --download-dir  with already downloaded models",
    "body": "The situation: I've downloaded the huge models on my server. And hope vllm could load the model.\r\n\r\nthe structure of the model dir: \r\n``` $ ls /data/vllm.model/01ai/Yi-34B-200K/ ```\r\n\r\n```\r\nLICENSE              generation_config.json            pytorch_model-00004-of-00007.bin  tokenizer.json\r\nREADME.md            md5                               pytorch_model-00005-of-00007.bin  tokenizer.model\r\nYi.svg               modeling_yi.py                    pytorch_model-00006-of-00007.bin  tokenizer_config.json\r\nconfig.json          pytorch_model-00001-of-00007.bin  pytorch_model-00007-of-00007.bin\r\nconfiguration.json   pytorch_model-00002-of-00007.bin  pytorch_model.bin.index.json\r\nconfiguration_yi.py  pytorch_model-00003-of-00007.bin  tokenization_yi.py\r\n```\r\n\r\nTry to load the mode as:\r\n```\r\nVLLM_USE_MODELSCOPE=True python -m vllm.entrypoints.openai.api_server --model 01ai/Yi-34B-200K  --download-dir /data/vllm.model/ --host 0.0.0.0   --trust-remote-code\r\n```\r\n\r\nBut it always tries to download the huge model files:\r\n```\r\n$VLLM_USE_MODELSCOPE=True python -m vllm.entrypoints.openai.api_server --model 01ai/Yi-34B-200K  --download-dir /data/vllm.model/ --host 0.0.0.0   --trust-remote-code\r\n```\r\n\r\n```\r\nINFO 11-22 22:04:27 api_server.py:638] args: Namespace(host='0.0.0.0', port=8000, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], served_model_name=None, model='01ai/Yi-34B-200K', tokenizer=None, revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, download_dir='/data/vllm.model/', load_format='auto', dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, block_size=16, seed=0, swap_space=4, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_seqs=256, max_paddings=256, disable_log_stats=False, quantization=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\n2023-11-22 22:04:27,237 - modelscope - INFO - PyTorch version 2.1.0 Found.\r\n2023-11-22 22:04:27,238 - modelscope - INFO - Loading ast index from /home/yi/.cache/modelscope/ast_indexer\r\n2023-11-22 22:04:27,347 - modelscope - INFO - Loading done! Current index file version is 1.9.5, with md5 47320a1734a7b8e8c52138f4be051026 and a total number of 945 components indexed\r\nDownloading:   0%|                                                                                    | 0.00/9.29G [00:00<?, ?B/s]\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-11-22T14:06:46+00:00",
    "closed_at": "2024-04-03T15:27:58+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1755/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1755"
  },
  {
    "number": 16150,
    "title": "[Bug]: Error When Launching Llama-4-Scout-17B-16E-Instruct Without `--kv-cache-dtype fp8`",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 04-07 11:13:31 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\n/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.28.1\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-94-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.3.107\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA H100 80GB HBM3\nGPU 1: NVIDIA H100 80GB HBM3\nGPU 2: NVIDIA H100 80GB HBM3\nGPU 3: NVIDIA H100 80GB HBM3\nGPU 4: NVIDIA H100 80GB HBM3\nGPU 5: NVIDIA H100 80GB HBM3\nGPU 6: NVIDIA H100 80GB HBM3\nGPU 7: NVIDIA H100 80GB HBM3\n\nNvidia driver version: 535.129.03\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      52 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             192\nOn-line CPU(s) list:                0-191\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) Platinum 8468V\nCPU family:                         6\nModel:                              143\nThread(s) per core:                 2\nCore(s) per socket:                 48\nSocket(s):                          2\nStepping:                           8\nCPU max MHz:                        3800.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           4800.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                     VT-x\nL1d cache:                          4.5 MiB (96 instances)\nL1i cache:                          3 MiB (96 instances)\nL2 cache:                           192 MiB (96 instances)\nL3 cache:                           195 MiB (2 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0-47,96-143\nNUMA node1 CPU(s):                  48-95,144-191\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-dali-cuda120==1.34.0\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] nvidia-pyindex==1.0.9\n[pip3] onnx==1.15.0rc2\n[pip3] optree==0.15.0\n[pip3] pynvml==11.4.1\n[pip3] pytorch-quantization==2.1.2\n[pip3] pyzmq==25.1.2\n[pip3] torch==2.6.0\n[pip3] torch-tensorrt==2.3.0a0\n[pip3] torchaudio==2.6.0\n[pip3] torchdata==0.7.1a0\n[pip3] torchtext==0.17.0a0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.0\n[pip3] triton==3.2.0\n[conda] No relevant packages\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3\nvLLM Build Flags:\nCUDA Archs: 5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     PIX     SYS     SYS     0-47,96-143     0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     PIX     SYS     48-95,144-191   1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     48-95,144-191   1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     PIX     48-95,144-191   1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     48-95,144-191   1               N/A\nNIC0    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS\nNIC1    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS\nNIC2    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS      X      SYS\nNIC3    SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n\nNVIDIA_VISIBLE_DEVICES=GPU-3a6d9eec-13d8-bfca-3668-204ab09379ef,GPU-ab44c55b-99b1-5821-d382-150b233fb39f,GPU-080dc61b-a2d8-4a88-e73f-c9d1f7d30a95,GPU-06cb9a7c-4cc5-3d1f-66a8-7e6c621ae217,GPU-5071dd6b-acd5-ed2a-009b-141e51d820eb,GPU-4cd47a10-6313-7393-37e7-7367f08bc018,GPU-6a1fb276-f409-db79-f944-53dfb0f7ddc6,GPU-e4ed4b90-5883-a84c-e913-8f4bf558b85a\nCUBLAS_VERSION=12.3.4.1\nNVIDIA_REQUIRE_CUDA=cuda>=9.0\nCUDA_CACHE_DISABLE=1\nTORCH_CUDA_ARCH_LIST=5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX\nNCCL_VERSION=2.19.stable.20231214+cuda12.3\nNVIDIA_DRIVER_CAPABILITIES=compute,utility,video\nVLLM_WORKER_MULTIPROC_METHOD=spawn\nNVIDIA_PRODUCT_NAME=PyTorch\nCUDA_VERSION=\nPYTORCH_VERSION=2.3.0a0+ebedce2\nPYTORCH_BUILD_NUMBER=0\nMAX_JOBS=64\nCUDNN_VERSION=9.0.0.306\nPYTORCH_HOME=/opt/pytorch/pytorch\nLD_LIBRARY_PATH=/usr/local/cuda/compat/lib.real:/usr/local/lib/python3.10/dist-packages/torch/lib:/usr/local/lib/python3.10/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNVIDIA_BUILD_ID=82611821\nCUDA_DRIVER_VERSION=545.23.08\nPYTORCH_BUILD_VERSION=2.3.0a0+ebedce2\nCUDA_HOME=/usr/local/cuda\nCUDA_HOME=/usr/local/cuda\nCUDA_MODULE_LOADING=LAZY\nNVIDIA_REQUIRE_JETPACK_HOST_MOUNTS=\nNVIDIA_PYTORCH_VERSION=24.02\nTORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen attempting to launch the vLLM server using the following command from the documentation, I encountered an error:\n```bash\nvllm serve meta-llama/Llama-4-Scout-17B-16E-Instruct -tp 8 --max-model-len 128000 --override-generation-config='{\"attn_temperature_tuning\": true}' --load-format runai_streamer\n```\n\nError Logs:\n```\nERROR 04-07 10:56:15 [core.py:390] EngineCore hit an exception: Traceback (most recent call last):\nERROR 04-07 10:56:15 [core.py:390]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py\", line 378, in run_engine_core\nERROR 04-07 10:56:15 [core.py:390]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 04-07 10:56:15 [core.py:390]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py\", line 319, in __init__\nERROR 04-07 10:56:15 [core.py:390]     super().__init__(vllm_config, executor_class, log_stats)\nERROR 04-07 10:56:15 [core.py:390]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py\", line 71, in __init__\nERROR 04-07 10:56:15 [core.py:390]     self._initialize_kv_caches(vllm_config)\nERROR 04-07 10:56:15 [core.py:390]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py\", line 132, in _initialize_kv_caches\nERROR 04-07 10:56:15 [core.py:390]     available_gpu_memory = self.model_executor.determine_available_memory()\nERROR 04-07 10:56:15 [core.py:390]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/executor/abstract.py\", line 66, in determine_available_memory\nERROR 04-07 10:56:15 [core.py:390]     output = self.collective_rpc(\"determine_available_memory\")\nERROR 04-07 10:56:15 [core.py:390]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 134, in collective_rpc\nERROR 04-07 10:56:15 [core.py:390]     raise e\nERROR 04-07 10:56:15 [core.py:390]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 123, in collective_rpc\nERROR 04-07 10:56:15 [core.py:390]     raise result\nERROR 04-07 10:56:15 [core.py:390] ValueError: too many values to unpack (expected 12)\nERROR 04-07 10:56:15 [core.py:390] \n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py\", line 387, in __call__\n(VllmWorker rank=1 pid=9198) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     return self.current_callable(inputs)\n(VllmWorker rank=3 pid=9324) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/output_code.py\", line 466, in __call__\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n(VllmWorker rank=1 pid=9198) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/root/.cache/vllm/torch_compile_cache/ea2d23402a/rank_1_0/inductor_cache/7a/c7anr47kmmdiy3qutmy34yk4g6ninrz4eu4dl2k46yjetwpxxcyh.py\", line 370, in call\n(VllmWorker rank=3 pid=9324) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     return self.current_callable(inputs)\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n(VllmWorker rank=1 pid=9198) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1 = args\n(VllmWorker rank=3 pid=9324) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/root/.cache/vllm/torch_compile_cache/ea2d23402a/rank_3_0/inductor_cache/js/cjs2yfrbs7n6dixeibsukc7xrpinqgfbc76l4ckdkz77iy4glynb.py\", line 370, in call\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     return self._call_impl(*args, **kwargs)\n(VllmWorker rank=1 pid=9198) ERROR 04-07 10:56:15 [multiproc_executor.py:383] ValueError: too many values to unpack (expected 12)\n(VllmWorker rank=3 pid=9324) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1 = args\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n(VllmWorker rank=3 pid=9324) ERROR 04-07 10:56:15 [multiproc_executor.py:383] ValueError: too many values to unpack (expected 12)\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     submod_6 = self.submod_6(getitem_12, s0, \n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     return forward_call(*args, **kwargs)\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"<eval_with_key>.98\", line 461, in forward\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     submod_6 = self.submod_6(getitem_12, s0, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_, getitem_13, l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_feed_forward_modules_router_parameters_weight_, l_self_modules_layers_modules_2_modules_feed_forward_modules_shared_expert_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_feed_forward_modules_shared_expert_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_feed_forward_modules_experts_parameters_w13_weight_, l_self_modules_layers_modules_2_modules_feed_forward_modules_experts_parameters_w2_weight_, l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_, l_positions_);  getitem_12 = l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_ = getitem_13 = l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_feed_forward_modules_router_parameters_weight_ = l_self_modules_layers_modules_2_modules_feed_forward_modules_shared_expert_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_feed_forward_modules_shared_expert_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_feed_forward_modules_experts_parameters_w13_weight_ = l_self_modules_layers_modules_2_modules_feed_forward_modules_experts_parameters_w2_weight_ = l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_ = None\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/usr/local/lib/python3.10/dist-packages/vllm/compilation/backends.py\", line 608, in __call__\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     return self.compiled_graph_for_general_shape(*args)\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/usr/local/lib/python3.10/dist-packages/vllm/compilation/compiler_interface.py\", line 357, in compiled_graph\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     graph_output = inductor_compiled_graph(list_args)\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/output_code.py\", line 466, in __call__\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     return self.current_callable(inputs)\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/root/.cache/vllm/torch_compile_cache/ea2d23402a/rank_5_0/inductor_cache/bx/cbxnsxbxmoiuzf32l5mxinvqc2wu2ex6u5o76dkfslsyn6wabunx.py\", line 370, in call\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1 = args\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383] ValueError: too many values to unpack (expected 12)\n(VllmWorker rank=2 pid=9252) ERROR 04-07 10:56:15 [multiproc_executor.py:383] WorkerProc hit an exception: %s\nCRITICAL 04-07 10:56:15 [core_client.py:361] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\nKilled\n```\n\nBut after I add `--kv-cache-dtype fp8`, it seems to return to normal.\n```bash\nvllm serve meta-llama/Llama-4-Scout-17B-16E-Instruct -tp 8 --max-model-len 128000 --override-generation-config='{\"attn_temperature_tuning\": true}' --kv-cache-dtype fp8 --load-format runai_streamer\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "closed",
    "created_at": "2025-04-07T03:33:28+00:00",
    "closed_at": "2025-04-15T06:11:13+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16150/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16150"
  },
  {
    "number": 1824,
    "title": "Local models cannot be used when the network is not accessible.",
    "body": "2023-11-29 11:34:19 Traceback (most recent call last):\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 467, in _make_request\r\n2023-11-29 11:34:19     self._validate_conn(conn)\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 1096, in _validate_conn\r\n2023-11-29 11:34:19     conn.connect()\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 642, in connect\r\n2023-11-29 11:34:19     sock_and_verified = _ssl_wrap_socket_and_match_hostname(\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 782, in _ssl_wrap_socket_and_match_hostname\r\n2023-11-29 11:34:19     ssl_sock = ssl_wrap_socket(\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/ssl_.py\", line 470, in ssl_wrap_socket\r\n2023-11-29 11:34:19     ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/ssl_.py\", line 514, in _ssl_wrap_socket_impl\r\n2023-11-29 11:34:19     return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\r\n2023-11-29 11:34:19   File \"/usr/lib/python3.10/ssl.py\", line 513, in wrap_socket\r\n2023-11-29 11:34:19     return self.sslsocket_class._create(\r\n2023-11-29 11:34:19   File \"/usr/lib/python3.10/ssl.py\", line 1071, in _create\r\n2023-11-29 11:34:19     self.do_handshake()\r\n2023-11-29 11:34:19   File \"/usr/lib/python3.10/ssl.py\", line 1342, in do_handshake\r\n2023-11-29 11:34:19     self._sslobj.do_handshake()\r\n2023-11-29 11:34:19 ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)\r\n2023-11-29 11:34:19 \r\n2023-11-29 11:34:19 During handling of the above exception, another exception occurred:\r\n2023-11-29 11:34:19 \r\n2023-11-29 11:34:19 Traceback (most recent call last):\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 790, in urlopen\r\n2023-11-29 11:34:19     response = self._make_request(\r\n2023-11-29 11:34:19 INFO 11-29 03:34:19 api_server.py:638] args: Namespace(host=None, port=8000, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], served_model_name=None, model='TheBloke/Yi-34B-Chat-AWQ', tokenizer=None, revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, block_size=16, seed=0, swap_space=4, gpu_memory_utilization=0.95, max_num_batched_tokens=None, max_num_seqs=256, max_paddings=256, disable_log_stats=False, quantization=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 491, in _make_request\r\n2023-11-29 11:34:19     raise new_e\r\n2023-11-29 11:34:19 urllib3.exceptions.SSLError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)\r\n2023-11-29 11:34:19 \r\n2023-11-29 11:34:19 The above exception was the direct cause of the following exception:\r\n2023-11-29 11:34:19 \r\n2023-11-29 11:34:19 Traceback (most recent call last):\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 486, in send\r\n2023-11-29 11:34:19     resp = conn.urlopen(\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 844, in urlopen\r\n2023-11-29 11:34:19     retries = retries.increment(\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\", line 515, in increment\r\n2023-11-29 11:34:19     raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\r\n2023-11-29 11:34:19 urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /TheBloke/Yi-34B-Chat-AWQ/resolve/main/config.json (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\r\n2023-11-29 11:34:19 \r\n2023-11-29 11:34:19 During handling of the above exception, another exception occurred:\r\n2023-11-29 11:34:19 \r\n2023-11-29 11:34:19 Traceback (most recent call last):\r\n2023-11-29 11:34:19   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n2023-11-29 11:34:19     return _run_code(code, main_globals, None,\r\n2023-11-29 11:34:19   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n2023-11-29 11:34:19     exec(code, run_globals)\r\n2023-11-29 11:34:19   File \"/workspace/vllm/entrypoints/openai/api_server.py\", line 646, in <module>\r\n2023-11-29 11:34:19     engine = AsyncLLMEngine.from_engine_args(engine_args)\r\n2023-11-29 11:34:19   File \"/workspace/vllm/engine/async_llm_engine.py\", line 480, in from_engine_args\r\n2023-11-29 11:34:19     engine_configs = engine_args.create_engine_configs()\r\n2023-11-29 11:34:19   File \"/workspace/vllm/engine/arg_utils.py\", line 187, in create_engine_configs\r\n2023-11-29 11:34:19     model_config = ModelConfig(self.model, self.tokenizer,\r\n2023-11-29 11:34:19   File \"/workspace/vllm/config.py\", line 91, in __init__\r\n2023-11-29 11:34:19     self.hf_config = get_config(self.model, trust_remote_code, revision)\r\n2023-11-29 11:34:19   File \"/workspace/vllm/transformers_utils/config.py\", line 23, in get_config\r\n2023-11-29 11:34:19     config = AutoConfig.from_pretrained(\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 1048, in from_pretrained\r\n2023-11-29 11:34:19     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 622, in get_config_dict\r\n2023-11-29 11:34:19     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 677, in _get_config_dict\r\n2023-11-29 11:34:19     resolved_config_file = cached_file(\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 430, in cached_file\r\n2023-11-29 11:34:19     resolved_file = hf_hub_download(\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n2023-11-29 11:34:19     return fn(*args, **kwargs)\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1247, in hf_hub_download\r\n2023-11-29 11:34:19     metadata = get_hf_file_metadata(\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n2023-11-29 11:34:19     return fn(*args, **kwargs)\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1624, in get_hf_file_metadata\r\n2023-11-29 11:34:19     r = _request_wrapper(\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 402, in _request_wrapper\r\n2023-11-29 11:34:19     response = _request_wrapper(\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 425, in _request_wrapper\r\n2023-11-29 11:34:19     response = get_session().request(method=method, url=url, **params)\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 589, in request\r\n2023-11-29 11:34:19     resp = self.send(prep, **send_kwargs)\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 703, in send\r\n2023-11-29 11:34:19     r = adapter.send(request, **kwargs)\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 63, in send\r\n2023-11-29 11:34:19     return super().send(request, *args, **kwargs)\r\n2023-11-29 11:34:19   File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 517, in send\r\n2023-11-29 11:34:19     raise SSLError(e, request=request)\r\n2023-11-29 11:34:19 requests.exceptions.SSLError: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /TheBloke/Yi-34B-Chat-AWQ/resolve/main/config.json (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\"), '(Request ID: 91b68885-d0ae-4e19-bb51-63855c72ce75)')",
    "labels": [],
    "state": "closed",
    "created_at": "2023-11-29T03:40:15+00:00",
    "closed_at": "2024-04-03T15:26:51+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1824/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1824"
  },
  {
    "number": 18120,
    "title": "[Feature]: Qwen 3 MoE Lora adapter support.",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\n#### Feature Proposal:\nSupport for **Qwen 3 MoE LoRA (Low-Rank Adaptation)** adapter in vLLM to enable efficient fine-tuning and inference.\n\n#### Motivation:\nThe Qwen 3 MoE model offer very good capabilities and performance. However, current vLLM do not support integration with LoRA adapters for fine-tuning and serving multiple finetuned models.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-05-14T06:50:30+00:00",
    "closed_at": "2025-07-17T18:32:53+00:00",
    "comments": 16,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18120/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/18120"
  },
  {
    "number": 14687,
    "title": "[Usage]: Gemma3 vllm serve AttributeError: 'Gemma3Config' object has no attribute 'vocab_size'",
    "body": "### Your current environment\n\n```text\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 24.04.1 LTS (x86_64)\nGCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.39\n\nPython version: 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-6.8.0-52-generic-x86_64-with-glibc2.39\nIs CUDA available: True\nCUDA runtime version: 12.0.140\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA GeForce RTX 4090\nGPU 1: NVIDIA GeForce RTX 4090\n\nNvidia driver version: 550.120\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               32\nOn-line CPU(s) list:                  0-31\nVendor ID:                            AuthenticAMD\nModel name:                           AMD Ryzen 9 7950X 16-Core Processor\nCPU family:                           25\nModel:                                97\nThread(s) per core:                   2\nCore(s) per socket:                   16\nSocket(s):                            1\nStepping:                             2\nCPU(s) scaling MHz:                   41%\nCPU max MHz:                          5881.0000\nCPU min MHz:                          545.0000\nBogoMIPS:                             8982.91\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid overflow_recov succor smca fsrm flush_l1d\nVirtualization:                       AMD-V\nL1d cache:                            512 KiB (16 instances)\nL1i cache:                            512 KiB (16 instances)\nL2 cache:                             16 MiB (16 instances)\nL3 cache:                             64 MiB (2 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-31\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.50.0.dev0\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.3\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tGPU1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tPHB\t0-31\t0\t\tN/A\nGPU1\tPHB\t X \t0-31\t0\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n\n\n\n\n### How would you like to use vllm\n\nI try to use vllm to host gemma3 12b -it but it showed me this error\n\nvllm serve \"google/gemma-3-12b-it\" --tensor-parallel-size 2\n\n\n(venv) developer1@g4090-4:~/vllm$ vllm serve \"google/gemma-3-12b-it\" --tensor-parallel-size 2\nINFO 03-12 16:53:17 __init__.py:207] Automatically detected platform cuda.\nINFO 03-12 16:53:17 api_server.py:912] vLLM API server version 0.7.3\nINFO 03-12 16:53:17 api_server.py:913] args: Namespace(subparser='serve', model_tag='google/gemma-3-12b-it', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='google/gemma-3-12b-it', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function ServeSubcommand.cmd at 0x79515ef2c360>)\nINFO 03-12 16:53:17 api_server.py:209] Started engine process with PID 3469310\nINFO 03-12 16:53:17 config.py:2444] Downcasting torch.float32 to torch.float16.\nINFO 03-12 16:53:19 __init__.py:207] Automatically detected platform cuda.\nINFO 03-12 16:53:19 config.py:2444] Downcasting torch.float32 to torch.float16.\nINFO 03-12 16:53:20 config.py:549] This model supports multiple tasks: {'classify', 'reward', 'score', 'embed', 'generate'}. Defaulting to 'generate'.\nINFO 03-12 16:53:20 config.py:1382] Defaulting to use mp for distributed inference\nWARNING 03-12 16:53:20 arg_utils.py:1197] The model has a long context length (1048576). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\nINFO 03-12 16:53:23 config.py:549] This model supports multiple tasks: {'generate', 'score', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.\nINFO 03-12 16:53:24 config.py:1382] Defaulting to use mp for distributed inference\nWARNING 03-12 16:53:24 arg_utils.py:1197] The model has a long context length (1048576). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\nINFO 03-12 16:53:24 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='google/gemma-3-12b-it', speculative_config=None, tokenizer='google/gemma-3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1048576, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-3-12b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \nWARNING 03-12 16:53:25 multiproc_worker_utils.py:300] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 03-12 16:53:25 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\nINFO 03-12 16:53:26 cuda.py:229] Using Flash Attention backend.\nINFO 03-12 16:53:27 __init__.py:207] Automatically detected platform cuda.\n(VllmWorkerProcess pid=3469513) INFO 03-12 16:53:27 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=3469513) INFO 03-12 16:53:28 cuda.py:229] Using Flash Attention backend.\nINFO 03-12 16:53:28 utils.py:916] Found nccl from library libnccl.so.2\nINFO 03-12 16:53:28 pynccl.py:69] vLLM is using nccl==2.21.5\n(VllmWorkerProcess pid=3469513) INFO 03-12 16:53:28 utils.py:916] Found nccl from library libnccl.so.2\n(VllmWorkerProcess pid=3469513) INFO 03-12 16:53:28 pynccl.py:69] vLLM is using nccl==2.21.5\nINFO 03-12 16:53:28 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/developer1/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n(VllmWorkerProcess pid=3469513) INFO 03-12 16:53:28 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/developer1/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\nWARNING 03-12 16:53:28 custom_all_reduce.py:145] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n(VllmWorkerProcess pid=3469513) WARNING 03-12 16:53:28 custom_all_reduce.py:145] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nINFO 03-12 16:53:28 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_efe1e1cb'), local_subscribe_port=37439, remote_subscribe_port=None)\nINFO 03-12 16:53:28 model_runner.py:1110] Starting to load model google/gemma-3-12b-it...\n(VllmWorkerProcess pid=3469513) INFO 03-12 16:53:28 model_runner.py:1110] Starting to load model google/gemma-3-12b-it...\nWARNING 03-12 16:53:28 utils.py:78] Gemma3ForConditionalGeneration has no vLLM implementation, falling back to Transformers implementation. Some features may not be supported and performance may not be optimal.\nINFO 03-12 16:53:28 transformers.py:129] Using Transformers backend.\n(VllmWorkerProcess pid=3469513) WARNING 03-12 16:53:28 utils.py:78] Gemma3ForConditionalGeneration has no vLLM implementation, falling back to Transformers implementation. Some features may not be supported and performance may not be optimal.\n(VllmWorkerProcess pid=3469513) INFO 03-12 16:53:28 transformers.py:129] Using Transformers backend.\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method load_model.\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242] Traceback (most recent call last):\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]   File \"/home/developer1/vllm/venv/lib/python3.12/site-packages/vllm/executor/multiproc_worker_utils.py\", line 236, in _run_worker_process\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]   File \"/home/developer1/vllm/venv/lib/python3.12/site-packages/vllm/utils.py\", line 2196, in run_method\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]     return func(*args, **kwargs)\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]   File \"/home/developer1/vllm/venv/lib/python3.12/site-packages/vllm/worker/worker.py\", line 183, in load_model\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]     self.model_runner.load_model()\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]   File \"/home/developer1/vllm/venv/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 1112, in load_model\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]     self.model = get_model(vllm_config=self.vllm_config)\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]   File \"/home/developer1/vllm/venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]     return loader.load_model(vllm_config=vllm_config)\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]   File \"/home/developer1/vllm/venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py\", line 406, in load_model\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]     model = _initialize_model(vllm_config=vllm_config)\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]   File \"/home/developer1/vllm/venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py\", line 125, in _initialize_model\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]     return model_class(vllm_config=vllm_config, prefix=prefix)\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]   File \"/home/developer1/vllm/venv/lib/python3.12/site-packages/vllm/model_executor/models/transformers.py\", line 135, in __init__\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]     self.vocab_size = config.vocab_size\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]                       ^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]   File \"/home/developer1/vllm/venv/lib/python3.12/site-packages/transformers/configuration_utils.py\", line 214, in __getattribute__\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]     return super().__getattribute__(key)\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=3469513) ERROR 03-12 16:53:28 multiproc_worker_utils.py:242] AttributeError: 'Gemma3Config' object has no attribute 'vocab_size'\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-03-12T16:53:52+00:00",
    "closed_at": "2025-03-12T17:45:28+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14687/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14687"
  },
  {
    "number": 9495,
    "title": "[Feature]: LoRA support for InternVLChatModel",
    "body": "### Your current environment\r\n\r\nvllm version = 0.6.1\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\n<details>\r\n<summary>The output of `command:`</summary>\r\n\r\n\r\nvllm version = 0.6.1. InternVLChat is in list of supported models.\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=0 python3 -m vllm.entrypoints.openai.api_server --model OpenGVLab/InternVL2-8B --vllm_enable_lora=True --vllm_max_lora_rank=32 --lora-modules line_items=checkpoint-786/ --api-key=abcd  --host=0.0.0.0 --port=8817 --gpu_memory_utilization 0.95 --max_model_len=8192 --trust_remote_code --limit-mm-per-prompt 'image=16' \r\n```\r\n\r\n```\r\nrank0]:   File \"/root/anaconda3/envs/msswift_latest/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 636, in __init__\r\n[rank0]:     self.engine = self._init_engine(*args, **kwargs)\r\n[rank0]:   File \"/root/anaconda3/envs/msswift_latest/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 840, in _init_engine\r\n[rank0]:     return engine_class(*args, **kwargs)\r\n[rank0]:   File \"/root/anaconda3/envs/msswift_latest/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 272, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/root/anaconda3/envs/msswift_latest/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 270, in __init__\r\n[rank0]:     self.model_executor = executor_class(\r\n[rank0]:   File \"/root/anaconda3/envs/msswift_latest/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 46, in __init__\r\n[rank0]:     self._init_executor()\r\n[rank0]:   File \"/root/anaconda3/envs/msswift_latest/lib/python3.10/site-packages/vllm/executor/gpu_executor.py\", line 39, in _init_executor\r\n[rank0]:     self.driver_worker.load_model()\r\n[rank0]:   File \"/root/anaconda3/envs/msswift_latest/lib/python3.10/site-packages/vllm/worker/worker.py\", line 182, in load_model\r\n[rank0]:     self.model_runner.load_model()\r\n[rank0]:   File \"/root/anaconda3/envs/msswift_latest/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 881, in load_model\r\n[rank0]:     self.model = get_model(model_config=self.model_config,\r\n[rank0]:   File \"/root/anaconda3/envs/msswift_latest/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 19, in get_model\r\n[rank0]:     return loader.load_model(model_config=model_config,\r\n[rank0]:   File \"/root/anaconda3/envs/msswift_latest/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 341, in load_model\r\n[rank0]:     model = _initialize_model(model_config, self.load_config,\r\n[rank0]:   File \"/root/anaconda3/envs/msswift_latest/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 170, in _initialize_model\r\n[rank0]:     return build_model(\r\n[rank0]:   File \"/root/anaconda3/envs/msswift_latest/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 151, in build_model\r\n[rank0]:     extra_kwargs = _get_model_initialization_kwargs(model_class, lora_config,\r\n[rank0]:   File \"/root/anaconda3/envs/msswift_latest/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 128, in _get_model_initialization_kwargs\r\n[rank0]:     raise ValueError(\r\n[rank0]: ValueError: Model InternVLChatModel does not support LoRA, but LoRA is enabled. Support for this model may be added in the future. If this is important to you, please open an issue on github\r\n```\r\n\r\n\r\n</details>\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-18T08:03:53+00:00",
    "closed_at": "2025-03-05T02:02:37+00:00",
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9495/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9495"
  },
  {
    "number": 566,
    "title": "Unable to run baichuan13b on 2 GPUs",
    "body": "Hi, I have 2 4090, each one of them can not fully load a 13b model, but vllm unable to automatically locate model into 2 GPUs, what else need I specific? (I have set a 0.8 GPU frac due to one GPU have a tiny process running consumes about  1GB mem)",
    "labels": [],
    "state": "closed",
    "created_at": "2023-07-25T04:50:32+00:00",
    "closed_at": "2023-08-07T21:38:33+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/566/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/566"
  },
  {
    "number": 16348,
    "title": "[Bug]: Missing metrics  in V1",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\nClang version: Could not collect\nCMake version: version 3.18.4\nLibc version: glibc-2.31\n\nPython version: 3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:16:10) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-5.10.0-34-cloud-amd64-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA L4\nGPU 1: NVIDIA L4\n\nNvidia driver version: 550.90.07\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nByte Order:                           Little Endian\nAddress sizes:                        46 bits physical, 48 bits virtual\nCPU(s):                               24\nOn-line CPU(s) list:                  0-23\nThread(s) per core:                   2\nCore(s) per socket:                   12\nSocket(s):                            1\nNUMA node(s):                         1\nVendor ID:                            GenuineIntel\nCPU family:                           6\nModel:                                85\nModel name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\nStepping:                             7\nCPU MHz:                              2200.194\nBogoMIPS:                             4400.38\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            384 KiB\nL1i cache:                            384 KiB\nL2 cache:                             12 MiB\nL3 cache:                             38.5 MiB\nNUMA node0 CPU(s):                    0-23\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; Enhanced IBRS\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n\nVersions of relevant libraries:\n[pip3] mypy==1.15.0\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.3.0\n[pip3] sentence-transformers==3.4.1\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.1\n[pip3] triton==3.2.0\n[conda] numpy                     1.25.2                   pypi_0    pypi\n[conda] nvidia-ml-py              11.495.46                pypi_0    pypi\n[conda] pyzmq                     26.3.0                   pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PHB     0-23    0               N/A\nGPU1    PHB      X      0-23    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n\nThis is my sample `script.py`\n\n```python\nfrom vllm import SamplingParams, AsyncEngineArgs, AsyncLLMEngine\nimport uuid\nimport asyncio\nimport time\n\nasync def main():\n    \"\"\"\n    Simulates sending 100 concurrent requests to a vLLM inference engine.\n    \n    :returns: None\n    \"\"\"\n    # Configure sampling parameters\n    sampling_params = SamplingParams(\n        max_tokens=100,\n    )\n    \n    # Configure engine\n    engine_args = AsyncEngineArgs(\n        model=\"facebook/opt-125m\",\n        enforce_eager=True,\n        gpu_memory_utilization=0.90,\n        disable_log_requests=True,\n        disable_log_stats=False\n    )\n    \n    # Initialize engine\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\n    \n    # Define request processing coroutine\n    async def process_request(request_id):\n        \"\"\"\n        Process a single request using the vLLM engine.\n        \n        :param request_id: Unique identifier for the request\n        :returns: Generated text output\n        \"\"\"\n        prompt = f\"Hello, how are you? This is request {request_id}\"\n        \n        # Send request to the engine\n        result_generator = engine.generate(\n            prompt=prompt,\n            sampling_params=sampling_params,\n            request_id=str(request_id),\n            \n        )\n        \n        # Process the streaming results\n        final_output = None\n        async for request_output in result_generator:\n            final_output = request_output\n            \n        return final_output\n    \n    # Create 10000 concurrent tasks\n    print(f\"Starting 10000 concurrent requests\")\n    \n    tasks = []\n    for i in range(10000):\n        request_id = uuid.uuid4()\n        tasks.append(process_request(request_id))\n    \n    # Wait for all tasks to complete\n    await asyncio.gather(*tasks)\n    \nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\nWhen I run `VLLM_USE_V1=0 python script.py`\n\nI do see the aggregate throughput metrics being regularly shown to me:\n\n```bash\n...\nINFO 04-09 14:25:03 [worker.py:267] model weights take 0.24GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 19.00GiB.\nINFO 04-09 14:25:03 [executor_base.py:112] # cuda blocks: 34595, # CPU blocks: 7281\nINFO 04-09 14:25:03 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.27x\nINFO 04-09 14:25:06 [llm_engine.py:448] init engine (profile, create kv cache, warmup model) took 4.49 seconds\nStarting 10000 concurrent requests\nINFO 04-09 14:25:12 [metrics.py:488] Avg prompt throughput: 1899.6 tokens/s, Avg generation throughput: 975.1 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 9725 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.\nINFO 04-09 14:25:17 [metrics.py:488] Avg prompt throughput: 2006.7 tokens/s, Avg generation throughput: 5044.2 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 9435 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\nINFO 04-09 14:25:22 [metrics.py:488] Avg prompt throughput: 1859.2 tokens/s, Avg generation throughput: 4441.8 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 9166 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%\n```\n\nhowever when `VLLM_USE_V1=1python script.py`\n\nI only see:\n\n```bash\nINFO 04-09 14:27:52 [loader.py:447] Loading weights took 0.21 seconds\nINFO 04-09 14:27:52 [gpu_model_runner.py:1273] Model loading took 0.2393 GiB and 0.666895 seconds\nINFO 04-09 14:27:53 [kv_cache_utils.py:578] GPU KV cache size: 502,304 tokens\nINFO 04-09 14:27:53 [kv_cache_utils.py:581] Maximum concurrency for 2,048 tokens per request: 245.27x\nINFO 04-09 14:27:53 [core.py:162] init engine (profile, create kv cache, warmup model) took 0.97 seconds\nStarting 10000 concurrent requests\n```\n\nand then nothing until completion.\n\nI research the documentation together with vLLM chatbot and came across this option:\n\n`VLLM_USE_V1=1 VLLM_LOGGING_CONFIG_PATH=\"logging.json\" script.py`\n\nwhich outputs:\n\n```bash\n{\"message\": \"Loading weights took 0.21 seconds\"}\n{\"message\": \"Model loading took 0.2393 GiB and 0.746670 seconds\"}\n{\"message\": \"GPU KV cache size: 502,304 tokens\"}\n{\"message\": \"Maximum concurrency for 2,048 tokens per request: 245.27x\"}\n{\"message\": \"init engine (profile, create kv cache, warmup model) took 0.67 seconds\"}\nStarting 10000 concurrent requests\n```\nand then nothing.\n\nI know that not too long ago the metrics for V1 were still in WiP state: https://github.com/vllm-project/vllm/issues/10582\n\nBut according to the changelog, `0.8.3` seems to be already feature-complete in that regard.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "v1"
    ],
    "state": "open",
    "created_at": "2025-04-09T14:38:15+00:00",
    "closed_at": null,
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16348/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16348"
  },
  {
    "number": 12808,
    "title": "[Bug]: Embedding model with Lora doesn't work.",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 02-06 16:43:14 __init__.py:183] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: SUSE Linux Enterprise Server 15 SP4 (x86_64)\nGCC version: (GCC) 12.3.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:16:10) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-5.14.21-150400.24.28-default-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: 12.2.140\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA H100\nGPU 1: NVIDIA H100\nGPU 2: NVIDIA H100\nGPU 3: NVIDIA H100\n\nNvidia driver version: 550.54.15\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   46 bits physical, 57 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          72\nOn-line CPU(s) list:             0-71\nVendor ID:                       GenuineIntel\nModel name:                      Intel(R) Xeon(R) Platinum 8452Y\nCPU family:                      6\nModel:                           143\nThread(s) per core:              1\nCore(s) per socket:              36\nSocket(s):                       2\nStepping:                        8\nCPU max MHz:                     3200.0000\nCPU min MHz:                     800.0000\nBogoMIPS:                        4000.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr avx512_fp16 amx_tile flush_l1d arch_capabilities\nL1d cache:                       3.4 MiB (72 instances)\nL1i cache:                       2.3 MiB (72 instances)\nL2 cache:                        144 MiB (72 instances)\nL3 cache:                        135 MiB (2 instances)\nNUMA node(s):                    2\nNUMA node0 CPU(s):               0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70\nNUMA node1 CPU(s):               1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Not affected\nVulnerability Retbleed:          Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] onnxruntime==1.20.1\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.48.2\n[pip3] triton==3.2.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-ml-py              12.570.86                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.2.1                   pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] transformers              4.48.2                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV6     NV6     NV6     SYS     SYS     0,2,4,6,8,10    0               N/A\nGPU1    NV6      X      NV6     NV6     PIX     SYS     0,2,4,6,8,10    0               N/A\nGPU2    NV6     NV6      X      NV6     SYS     PIX     1,3,5,7,9,11    1               N/A\nGPU3    NV6     NV6     NV6      X      SYS     SYS     1,3,5,7,9,11    1               N/A\nNIC0    SYS     PIX     SYS     SYS      X      SYS                             \nNIC1    SYS     SYS     PIX     SYS     SYS      X                              \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n\nLD_LIBRARY_PATH=/xxx/conda_envs/tevatron/lib/python3.10/site-packages/cv2/../../lib64:/apps/cuda/12.2.2/lib64:/apps/cuda/12.2.2/extras/CUPTI/lib64\nCUDA_DRIVER_HOME=/cm/local/apps/cuda-driver/libs/current\nOMP_NUM_THREADS=1\nCUDA_HOME=/apps/cuda/12.2.2\nCUDA_HOME=/apps/cuda/12.2.2\nCUDA_ROOT=/apps/cuda/12.2.2\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nEmbedding model API with Lora weights doesn't work as expected.\n\nFor example, the transformer version is:\n\n```python\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\nfrom peft import PeftModel, PeftConfig\n\ndef get_model(peft_model_name):\n    config = PeftConfig.from_pretrained(peft_model_name)\n    base_model = AutoModel.from_pretrained(config.base_model_name_or_path)\n    model = PeftModel.from_pretrained(base_model, peft_model_name)\n    model = model.merge_and_unload()\n    model.eval()\n    return model\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\nmodel = get_model('castorini/repllama-v1-7b-lora-passage')\n\n# Define query and passage inputs\nquery = \"What is llama?\"\ntitle = \"Llama\"\npassage = \"The llama is a domesticated South American camelid, widely used as a meat and pack animal by Andean cultures since the pre-Columbian era.\"\nquery_input = tokenizer(f'query: {query}</s>', return_tensors='pt')\npassage_input = tokenizer(f'passage: {title} {passage}</s>', return_tensors='pt')\n\n# Run the model forward to compute embeddings and query-passage similarity score\nwith torch.no_grad():\n    # compute query embedding\n    query_outputs = model(**query_input)\n    query_embedding = query_outputs.last_hidden_state[0][-1]\n    query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=0)\n\n    # compute passage embedding\n    passage_outputs = model(**passage_input)\n    passage_embeddings = passage_outputs.last_hidden_state[0][-1]\n    passage_embeddings = torch.nn.functional.normalize(passage_embeddings, p=2, dim=0)\n\n    # compute similarity score\n    score = torch.dot(query_embedding, passage_embeddings)\n    print(score)\n\n# printed score: tensor(0.8932)\n```\n\nMy vllm code is:\n\n```python\nfrom vllm import LLM\nfrom huggingface_hub import snapshot_download\nfrom vllm.lora.request import LoRARequest\nfrom vllm.config import PoolerConfig\n\nlora_path = snapshot_download(repo_id=\"castorini/repllama-v1-7b-lora-passage\")\n\n\n# Sample prompts.\nprompts = [\n    f'query: {query}</s>',\n    f'passage: {title} {passage}</s>',\n]\n\npooler_config = PoolerConfig(pooling_type='LAST',\n                             normalize=True)\n\nmodel = LLM(\n    model=\"meta-llama/Llama-2-7b-hf\",\n    task=\"embed\",\n    enforce_eager=True,\n    enable_lora=True,\n    max_lora_rank=32,\n    override_pooler_config=pooler_config,\n)\n\n# Generate embedding. The output is a list of EmbeddingRequestOutputs.\noutputs = model.embed(prompts,\n                      lora_request=LoRARequest(\"repllama_adapter\",\n                                               1,\n                                               lora_path))\n\n\nquery_embedding = torch.Tensor(outputs[0].outputs.embedding)\npassage_embeddings = torch.Tensor(outputs[1].outputs.embedding)\nprint(torch.dot(query_embedding, passage_embeddings))\n\n# printed score: tensor(0.7878)\n```\n\nThe similarity score is very different, and so are the embeddings.\n\nWe identified that this is due to Lora weights are not loaded, the vllm code with or without lora gives the same results. and the vllm code with Lora weights merged model will give the same score and embeddings as the transformers code.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-02-06T05:55:48+00:00",
    "closed_at": "2025-03-18T12:07:01+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12808/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12808"
  },
  {
    "number": 763,
    "title": "pip install error - CUDA version mismatch",
    "body": "I have the torch version 2.0.1+cu117\r\n\r\n```\r\nimport torch\r\nprint(torch.__version__)\r\nprint(torch.version.cuda)\r\n2.0.1+cu117\r\n11.7\r\n```\r\nHowever pip install vllm fails with\r\n```\r\n\r\nFile \"/tmp/pip-build-env-jw9yras7/overlay/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 387, in _check_cuda_version\r\n          raise RuntimeError(CUDA_MISMATCH_MESSAGE.format(cuda_str_version, torch.version.cuda))\r\n      RuntimeError:\r\n      The detected CUDA version (12.2) mismatches the version that was used to compile\r\n      PyTorch (11.7). Please make sure to use the same CUDA versions.\r\n      \r\n      [end of output]\r\n```\r\n      \r\nnvidia-smi\r\n```\r\nTue Aug 15 11:43:53 2023       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA A40          On   | 00000000:35:00.0 Off |                    0 |\r\n|  0%   39C    P8    61W / 300W |      0MiB / 46068MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  NVIDIA A40          On   | 00000000:36:00.0 Off |                    0 |\r\n|  0%   39C    P0    41W / 300W |      0MiB / 46068MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  NVIDIA A40          On   | 00000000:39:00.0 Off |                    0 |\r\n|  0%   39C    P0    42W / 300W |      0MiB / 46068MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  NVIDIA A40          On   | 00000000:3D:00.0 Off |                    0 |\r\n|  0%   40C    P0    61W / 300W |      0MiB / 46068MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  NVIDIA A40          On   | 00000000:CE:00.0 Off |                    0 |\r\n|  0%   38C    P0    58W / 300W |      0MiB / 46068MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  NVIDIA A40          On   | 00000000:CF:00.0 Off |                    0 |\r\n|  0%   40C    P0    25W / 300W |      0MiB / 46068MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  NVIDIA A40          On   | 00000000:D2:00.0 Off |                    0 |\r\n|  0%   38C    P0    21W / 300W |      0MiB / 46068MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  NVIDIA A40          On   | 00000000:D6:00.0 Off |                    0 |\r\n|  0%   39C    P0    25W / 300W |      0MiB / 46068MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nls /usr/local | grep cuda\r\n```\r\ncuda\r\ncuda-12\r\ncuda-12.2\r\n```\r\n",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2023-08-15T11:46:42+00:00",
    "closed_at": "2023-08-25T08:58:30+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/763/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/763"
  },
  {
    "number": 7418,
    "title": "[Misc]: How to use intel-gpu in openvino",
    "body": "### Anything you want to discuss about vllm.\n\nHi, I successfully create the openvino env. I am wondering how to use the intel-gpu? ",
    "labels": [
      "misc",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-12T13:43:15+00:00",
    "closed_at": "2025-01-11T01:59:57+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7418/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7418"
  },
  {
    "number": 2149,
    "title": "GPTQ does not support bfloat16",
    "body": "Currently, our GPTQ kernels only support the float16 precision.",
    "labels": [
      "help wanted",
      "feature request",
      "quantization",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-17T06:06:30+00:00",
    "closed_at": "2024-11-30T02:03:24+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2149/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2149"
  },
  {
    "number": 12693,
    "title": "[Usage]: V0 Does Qwen2-VL Support torch.compile in vllm?",
    "body": "### Your current environment\n\nHi there,\n\nFirst of all, thank you for all the hard work on vllm\u2014it\u2019s an excellent project!\n\nI am currently exploring the use of torch.compile within vllm to optimize inference performance. I have seen that many decoder-only models (such as GPT-series and LLaMA) work well with torch.compile. However, I am particularly interested in using the Qwen2-VL model and could not find any documentation or discussion regarding torch.compile support for it.\n\nCould you please clarify the following:\n\nIs Qwen2-VL currently supported with torch.compile in the latest version of vllm?\nIf not, are there any plans to add support for Qwen2-VL with torch.compile in the near future?\nAre there any known workarounds or tips for using torch.compile with multi-modal models like Qwen2-VL?\nAny guidance or insights would be greatly appreciated!\n\nThank you for your time and assistance.\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-03T14:31:02+00:00",
    "closed_at": "2025-06-20T02:13:44+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12693/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12693"
  },
  {
    "number": 15844,
    "title": "[Bug]: ImportError: _flash_supports_window_size missing for baichuan-inc/Baichuan-M1-14B-Instruct (with trust_remote_code=True) in vLLM v0.8.2",
    "body": "### Your current environment\n\n\ud658\uacbd (Environment):\n\nvLLM \ubc84\uc804 (vLLM Version): 0.8.2 (vllm/vllm-openai:v0.8.2 \ub3c4\ucee4 \uc774\ubbf8\uc9c0 \uc0ac\uc6a9)\nTransformers \ubc84\uc804 (Transformers Version): vllm/vllm-openai:v0.8.2 \uc774\ubbf8\uc9c0\uc5d0 \ud3ec\ud568\ub41c \ubc84\uc804 (\uc815\ud655\ud55c \ubc84\uc804 \ud655\uc778 \ud544\uc694 \uc2dc \uba85\uc2dc, \ud604\uc7ac\ub294 \uc624\ub958\ub85c \ubbf8\ub8e8\uc5b4 \ud638\ud658\ub418\uc9c0 \uc54a\ub294 \ubc84\uc804\uc73c\ub85c \ucd94\uc815)\nPython \ubc84\uc804 (Python Version): (\ub85c\uadf8\uc5d0\uc11c \ud655\uc778\ub41c \ubc84\uc804, \uc608: 3.12)\nCUDA \ubc84\uc804 (CUDA Version): (vllm/vllm-openai:v0.8.2 \uc774\ubbf8\uc9c0 \uae30\ubc18 \ubc84\uc804, \uc608: 11.8 \ub610\ub294 12.x)\n\uc0ac\uc6a9 \ubaa8\ub378 (Model Used): baichuan-inc/Baichuan-M1-14B-Instruct\n\uc2e4\ud589 \ud658\uacbd (Runtime Environment): Kubernetes Deployment (NVIDIA GPU \uc0ac\uc6a9)\n\uc7ac\ud604 \ub2e8\uacc4 (Steps to Reproduce):\n\nvllm/vllm-openai:v0.8.2 \ub3c4\ucee4 \uc774\ubbf8\uc9c0\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\ub2e4\uc74c\uacfc \uc720\uc0ac\ud55c \uba85\ub839\uc5b4\ub85c vLLM \uc11c\ubc84\ub97c \uc2dc\uc791\ud569\ub2c8\ub2e4 (\ucfe0\ubc84\ub124\ud2f0\uc2a4 Deployment args \uae30\uc900):\nBash\n\nvllm serve baichuan-inc/Baichuan-M1-14B-Instruct \\\n    --trust-remote-code \\\n    --enable-chunked-prefill \\\n    --max_num_batched_tokens 1024\n\n\n### \ud83d\udc1b Describe the bug\n\n\uad00\ucc30\ub41c \ub3d9\uc791 (Observed Behavior):\n\nvLLM \uc11c\ubc84\uac00 \ubaa8\ub378 \ub85c\ub529 \ub2e8\uacc4\uc5d0\uc11c \uc2e4\ud328\ud558\uba70 \ub2e4\uc74c\uacfc \uac19\uc740 ImportError \ub85c\uadf8\ub97c \ub0a8\uae30\uace0 \ucee8\ud14c\uc774\ub108\uac00 \ube44\uc815\uc0c1 \uc885\ub8cc\ub429\ub2c8\ub2e4. \ucfe0\ubc84\ub124\ud2f0\uc2a4 \ud658\uacbd\uc5d0\uc11c\ub294 Pod\uac00 READY: 0/1 \uc0c1\ud0dc\uac00 \ub418\uace0 \ubc18\ubcf5\uc801\uc73c\ub85c \uc7ac\uc2dc\uc791\ud569\ub2c8\ub2e4 (CrashLoopBackOff).\n\n\nERROR 03-31 18:56:05 [core.py:343] EngineCore hit an exception: Traceback (most recent call last):\n# ... (\uc911\uac04 \ud2b8\ub808\uc774\uc2a4\ubc31 \uc0dd\ub7b5) ...\nERROR 03-31 18:56:05 [core.py:343]   File \"/usr/local/lib/python3.12/dist-packages/transformers/dynamic_module_utils.py\", line 250, in get_class_in_module\nERROR 03-31 18:56:05 [core.py:343]     module_spec.loader.exec_module(module)\nERROR 03-31 18:56:05 [core.py:343]   File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\nERROR 03-31 18:56:05 [core.py:343]   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\nERROR 03-31 18:56:05 [core.py:343]   File \"/root/.cache/huggingface/modules/transformers_modules/baichuan-inc/Baichuan-M1-14B-Instruct/c1798dfc9dab0270eb10180a0611cb607d5276d2/modeling_baichuan.py\", line 15, in <module>\nERROR 03-31 18:56:05 [core.py:343]     from transformers.modeling_flash_attention_utils import _flash_supports_window_size, \\\nERROR 03-31 18:56:05 [core.py:343] ImportError: cannot import name '_flash_supports_window_size' from 'transformers.modeling_flash_attention_utils' (/usr/local/lib/python3.12/dist-packages/transformers/modeling_flash_attention_utils.py)\nERROR 03-31 18:56:05 [core.py:343]\nCRITICAL 03-31 18:56:05 [core_client.py:269] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\nKilled\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-01T02:30:43+00:00",
    "closed_at": null,
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15844/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15844"
  },
  {
    "number": 2422,
    "title": "Phi-2 Broken",
    "body": "I've tried pip's version of `vllm`, and github `master`.\r\nI've tried pip's version of `transformers` and github `master`.\r\nI've tried an older `git clone` of HF's `microsoft/phi-2`, and then `git cloned` it anew.\r\n\r\nWhen I try running it in python code, or in server mode, I get this error:\r\n\r\n```sh\r\nFile \"/home/mobius/_/lib/vllm/vllm/model_executor/models/phi_1_5.py\", line 219, in __init__\r\n    self.h = nn.ModuleList([\r\n  File \"/home/mobius/_/lib/vllm/vllm/model_executor/models/phi_1_5.py\", line 220, in <listcomp>\r\n    PhiLayer(config, linear_method)\r\n  File \"/home/mobius/_/lib/vllm/vllm/model_executor/models/phi_1_5.py\", line 186, in __init__\r\n    eps=config.layer_norm_epsilon)\r\n  File \"/home/mobius/_/lib/transformers/src/transformers/configuration_utils.py\", line 265, in __getattribute__\r\n    return super().__getattribute__(key)\r\nAttributeError: 'PhiConfig' object has no attribute 'layer_norm_epsilon'. Did you mean: 'layer_norm_eps'?\r\n```\r\n\r\nIf I go into the `vllm` code and fix that, I now get:\r\n\r\n```sh\r\n  File \"/home/user/_/lib/vllm/vllm/model_executor/models/phi_1_5.py\", line 192, in __init__\r\n    self.mixer = PhiAttention(config, linear_method)\r\n  File \"/home/user/_/lib/vllm/vllm/model_executor/models/phi_1_5.py\", line 116, in __init__\r\n    rotary_dim = config.rotary_dim\r\n  File \"/home/user/_/lib/transformers/src/transformers/configuration_utils.py\", line 265, in __getattribute__\r\n    return super().__getattribute__(key)\r\nAttributeError: 'PhiConfig' object has no attribute 'rotary_dim'\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-11T21:35:22+00:00",
    "closed_at": "2024-02-12T20:00:09+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2422/reactions",
      "total_count": 29,
      "+1": 29,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2422"
  },
  {
    "number": 10931,
    "title": "[Bug]: vLLM (running HuggingFaceTB/SmolVLM-Instruct) crashes with a 500 when making concurrent requests through the OpenAI compatible HTTP server",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 23.10 (x86_64)\r\nGCC version: (Ubuntu 13.2.0-4ubuntu3) 13.2.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.38\r\n\r\nPython version: 3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-44-generic-x86_64-with-glibc2.38\r\nIs CUDA available: True\r\nCUDA runtime version: 12.0.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA GeForce RTX 4090\r\nGPU 1: NVIDIA GeForce RTX 4090\r\nGPU 2: NVIDIA GeForce RTX 4090\r\nGPU 3: NVIDIA GeForce RTX 4090\r\n\r\nNvidia driver version: 535.171.04\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             32\r\nOn-line CPU(s) list:                0-31\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD Ryzen Threadripper PRO 5955WX 16-Cores\r\nCPU family:                         25\r\nModel:                              8\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 16\r\nSocket(s):                          1\r\nStepping:                           2\r\nFrequency boost:                    enabled\r\nCPU(s) scaling MHz:                 47%\r\nCPU max MHz:                        7031.2500\r\nCPU min MHz:                        1800.0000\r\nBogoMIPS:                           7985.24\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\nVirtualization:                     AMD-V\r\nL1d cache:                          512 KiB (16 instances)\r\nL1i cache:                          512 KiB (16 instances)\r\nL2 cache:                           8 MiB (16 instances)\r\nL3 cache:                           64 MiB (2 instances)\r\nNUMA node(s):                       1\r\nNUMA node0 CPU(s):                  0-31\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Vulnerable: Safe RET, no microcode\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.46.3\r\n[pip3] triton==3.1.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.20.1                   pypi_0    pypi\r\n[conda] transformers              4.46.3                   pypi_0    pypi\r\n[conda] triton                    3.1.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.4.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tSYS\tSYS\tSYS\t0-31\t0\t\tN/A\r\nGPU1\tSYS\t X \tSYS\tSYS\t0-31\t0\t\tN/A\r\nGPU2\tSYS\tSYS\t X \tSYS\t0-31\t0\t\tN/A\r\nGPU3\tSYS\tSYS\tSYS\t X \t0-31\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nLD_LIBRARY_PATH=/home/rendernet/.conda/envs/vllm/lib/python3.10/site-packages/cv2/../../lib64:\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nStarting the vllm sever with the following command: \r\n```\r\nvllm serve HuggingFaceTB/SmolVLM-Instruct --dtype bfloat16\r\n```\r\nSnippet used to fire requests to the  OpenAI compatible server\r\n\r\n```\r\ncurl -X POST \\\r\n  'http://localhost:8000/v1/chat/completions' \\\r\n  -H 'Content-Type: application/json' \\\r\n  -H 'Authorization: Bearer abc' \\\r\n  -d '{\r\n    \"model\": \"HuggingFaceTB/SmolVLM-Instruct\",\r\n    \"max_tokens\": 512,\r\n    \"temperature\": 0.4,\r\n    \"top_p\": 0.8,\r\n    \"frequency_penalty\": 0.2,\r\n    \"messages\": [\r\n      {\r\n        \"role\": \"user\",\r\n        \"content\": [\r\n          {\r\n            \"type\": \"image_url\",\r\n            \"image_url\": {\r\n              \"url\": <image_url_goes_here>\r\n            }\r\n          },\r\n          {\r\n            \"type\": \"text\",\r\n            \"text\": \"Please provide a detailed, flowing description that could have been used as a prompt to generate this image.\\nYour description should naturally incorporate:\\n- The main subjects, capturing their specific features, expressions, poses, and distinguishing characteristics\\n- The complete setting and background elements, from foreground to distance\\n- The color palette, quality of light, and atmospheric conditions\\n- Any striking or unusual details that catch the eye\\n- How different elements are positioned and interact spatially with each other\\nWrite in the style of a generative art prompt while maintaining natural narrative flow. Frame the description as if it were the original input used to create this image. The description should be at least 100 words long, written in continuous prose that weaves together all these aspects without separate sections or summaries. Focus solely on describing what is visible in a way that could serve as a detailed image generation prompt.\"\r\n          }\r\n        ]\r\n      }\r\n    ]\r\n  }'\r\n  ```\r\n  \r\nError faced for concurrent requests (>1):\r\n\r\n```\r\n\"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/aiohttp/streams.py\", line 344, in _wait\r\n    await waiter\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\r\n    return await self.app(scope, receive, send)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/applications.py\", line 113, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 187, in __call__\r\n    raise exc\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/middleware/base.py\", line 185, in __call__\r\n    with collapse_excgroups():\r\n  File \"/usr/lib/python3.11/contextlib.py\", line 155, in __exit__\r\n    self.gen.throw(typ, value, traceback)\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\r\n    raise exc\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/middleware/base.py\", line 187, in __call__\r\n    response = await self.dispatch_func(request, call_next)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 517, in add_request_id\r\n    response = await call_next(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/middleware/base.py\", line 163, in call_next\r\n    raise app_exc\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/middleware/base.py\", line 149, in coro\r\n    await self.app(scope, receive_or_disconnect, send_no_error)\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n raise exc\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/routing.py\", line 715, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/routing.py\", line 735, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/routing.py\", line 288, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/routing.py\", line 76, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    raise exc\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/starlette/routing.py\", line 73, in app\r\n    response = await f(request)\r\n               ^^^^^^^^^^^^^^^^\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/fastapi/routing.py\", line 301, in app\r\n    raw_response = await run_endpoint_function(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 354, in create_chat_completion\r\n    generator = await handler.create_chat_completion(request, raw_request)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py\", line 160, in create_chat_completion\r\n    ) = await self._preprocess_chat(\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_engine.py\", line 484, in _preprocess_chat\r\n    mm_data = await mm_data_future\r\n              ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/vllm/entrypoints/chat_utils.py\", line 490, in all_mm_data\r\n    items = await asyncio.gather(*self._items)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/vllm/multimodal/utils.py\", line 288, in async_get_and_parse_image\r\n    image = await async_fetch_image(\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/vllm/multimodal/utils.py\", line 111, in async_fetch_image\r\n    image_raw = await global_http_connection.async_get_bytes(\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/vllm/connections.py\", line 95, in async_get_bytes\r\n    return await r.read()\r\n           ^^^^^^^^^^^^^^\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/aiohttp/client_reqrep.py\", line 1214, in read\r\n    self._body = await self.content.read()\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/aiohttp/streams.py\", line 415, in read\r\n    block = await self.readany()\r\n            ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/aiohttp/streams.py\", line 437, in readany\r\n    await self._wait(\"readany\")\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/aiohttp/streams.py\", line 343, in _wait\r\n    with self._timer:\r\n  File \"/home/rendernet/horsey/vllm-updated/venv/lib/python3.11/site-packages/aiohttp/helpers.py\", line 671, in __exit__\r\n    raise asyncio.TimeoutError from exc_val\r\nTimeoutError      \r\n```\r\n\r\nTried out the offline inferencing as well. The issue is that after trying a concurrent request, the server hangs and no more inference is possible.\r\n\r\n```\r\nfrom fastapi import FastAPI, HTTPException\r\nfrom vllm import LLM, SamplingParams\r\nfrom PIL import Image\r\nimport requests\r\nfrom io import BytesIO\r\nfrom pydantic import BaseModel\r\nimport uvicorn\r\n\r\n# Define request model\r\nclass ImageRequest(BaseModel):\r\n    image_url: str\r\n    system_prompt: str = \"You are a helpful AI assistant skilled at describing images accurately and concisely.\"\r\n\r\n# Initialize FastAPI app\r\napp = FastAPI(title=\"Image Captioning Service\")\r\n\r\n# Initialize the LLM\r\nllm = LLM(model=\"HuggingFaceTB/SmolVLM-Instruct\")\r\nsampling_params = SamplingParams(max_tokens=512, temperature = 0.4, top_p=0.8, frequency_penalty=0.2,)\r\n\r\n@app.post(\"/caption\")\r\nasync def generate_caption(request: ImageRequest):\r\n    \"\"\"Generate caption for the provided image URL using chat format\"\"\"\r\n    try:\r\n        act_prompt = \"\"\"Please provide a detailed, flowing description that could have been used as a prompt to generate this image.\r\n\r\nYour description should naturally incorporate:\r\n- The main subjects, capturing their specific features, expressions, poses, and distinguishing characteristics\r\n- The complete setting and background elements, from foreground to distance\r\n- The color palette, quality of light, and atmospheric conditions\r\n- Any striking or unusual details that catch the eye\r\n- How different elements are positioned and interact spatially with each other\r\n\r\nWrite in the style of a generative art prompt while maintaining natural narrative flow. Frame the description as if it were the original input used to create this image. The description should be at least 100 words long, written in continuous prose that weaves together all these aspects without separate sections or summaries. Focus solely on describing what is visible in a way that could serve as a detailed image generation prompt.\"\"\"\r\n\r\n        # Prepare chat messages\r\n        messages=[{\r\n                \"role\": \"user\",\r\n                \"content\": [\r\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": request.image_url }},\r\n                    {\"type\": \"text\", \"text\": act_prompt },\r\n                ],\r\n        }]\r\n\r\n        # Generate caption using chat\r\n        outputs = llm.chat(\r\n            messages=messages,\r\n            sampling_params=sampling_params\r\n        )\r\n\r\n        # Extract generated text\r\n        generated_text = outputs[0].outputs[0].text\r\n\r\n        return {\r\n            \"caption\": generated_text,\r\n            \"system_prompt_used\": request.system_prompt\r\n        }\r\n\r\n    except Exception as e:\r\n        raise HTTPException(status_code=500, detail=f\"Error generating caption: {str(e)}\")\r\n\r\n@app.get(\"/health\")\r\nasync def health_check():\r\n    \"\"\"Health check endpoint\"\"\"\r\n    return {\"status\": \"healthy\", \"model\": \"SmolVLM-Instruct\"}\r\n\r\nif __name__ == \"__main__\":\r\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-12-05T16:44:15+00:00",
    "closed_at": "2024-12-06T12:18:36+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10931/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10931"
  }
]
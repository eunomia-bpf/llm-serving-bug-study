[
  {
    "number": 14034,
    "title": "[Installation]: Can't find OpenMP headers on macOS",
    "body": "Seems that clang can't find the OpenMP headers.\n\n### Your current environment\n\n```text\n(vllm) \u279c  vllm git:(v0.7.2) python collect_env.py \nINFO 02-28 18:13:24 __init__.py:190] Automatically detected platform cpu.\nCollecting environment information...\nPyTorch version: 2.5.1\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: macOS 15.3.1 (arm64)\nGCC version: Could not collect\nClang version: 16.0.0 (clang-1600.0.26.6)\nCMake version: version 3.31.5\nLibc version: N/A\n\nPython version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 12:55:12) [Clang 14.0.6 ] (64-bit runtime)\nPython platform: macOS-15.3.1-arm64-arm-64bit\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nApple M1 Max\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.49.0\n[conda] numpy                     1.26.4          py312h7f4fdc5_0  \n[conda] numpy-base                1.26.4          py312he047099_0  \n[conda] pyzmq                     26.2.1                   pypi_0    pypi\n[conda] torch                     2.5.1                    pypi_0    pypi\n[conda] torchaudio                2.5.1                    pypi_0    pypi\n[conda] torchvision               0.20.1                   pypi_0    pypi\n[conda] transformers              4.49.0                   pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nLD_LIBRARY_PATH=/Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages/cv2/../../lib:\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\n\n\n```\n\n\n### How you are installing vllm\n\n```sh\n(vllm) \u279c  vllm git:(v0.7.2) pip install -e .            \nObtaining file:///Users/shengyao/vllm\n  Installing build dependencies ... done\n  Checking if build backend supports build_editable ... done\n  Getting requirements to build editable ... done\n  Preparing editable metadata (pyproject.toml) ... done\nRequirement already satisfied: psutil in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (7.0.0)\nRequirement already satisfied: sentencepiece in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (0.2.0)\nRequirement already satisfied: numpy<2.0.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (1.26.4)\nRequirement already satisfied: requests>=2.26.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (2.32.3)\nRequirement already satisfied: tqdm in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (4.67.1)\nRequirement already satisfied: blake3 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (1.0.4)\nRequirement already satisfied: py-cpuinfo in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (9.0.0)\nRequirement already satisfied: transformers>=4.48.2 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (4.49.0)\nRequirement already satisfied: tokenizers>=0.19.1 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (0.21.0)\nRequirement already satisfied: protobuf in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (5.29.3)\nRequirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (0.115.9)\nRequirement already satisfied: aiohttp in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (3.11.13)\nRequirement already satisfied: openai>=1.52.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (1.65.1)\nRequirement already satisfied: uvicorn[standard] in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (0.34.0)\nRequirement already satisfied: pydantic>=2.9 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (2.10.6)\nRequirement already satisfied: prometheus_client>=0.18.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (0.21.1)\nRequirement already satisfied: pillow in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (11.1.0)\nRequirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (7.0.2)\nRequirement already satisfied: tiktoken>=0.6.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (0.9.0)\nRequirement already satisfied: lm-format-enforcer<0.11,>=0.10.9 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (0.10.11)\nRequirement already satisfied: outlines==0.1.11 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (0.1.11)\nRequirement already satisfied: lark==1.2.2 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (1.2.2)\nRequirement already satisfied: typing_extensions>=4.10 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (4.12.2)\nRequirement already satisfied: filelock>=3.16.1 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (3.17.0)\nRequirement already satisfied: partial-json-parser in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (0.2.1.1.post5)\nRequirement already satisfied: pyzmq in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (26.2.1)\nRequirement already satisfied: msgspec in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (0.19.0)\nRequirement already satisfied: gguf==0.10.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (0.10.0)\nRequirement already satisfied: importlib_metadata in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (8.6.1)\nRequirement already satisfied: mistral_common>=1.5.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from mistral_common[opencv]>=1.5.0->vllm==0.7.2+cpu) (1.5.3)\nRequirement already satisfied: pyyaml in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (6.0.2)\nRequirement already satisfied: six>=1.16.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (1.17.0)\nRequirement already satisfied: setuptools>=74.1.1 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (75.8.0)\nRequirement already satisfied: einops in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (0.8.1)\nRequirement already satisfied: compressed-tensors==0.9.1 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (0.9.1)\nRequirement already satisfied: depyf==0.18.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (0.18.0)\nRequirement already satisfied: cloudpickle in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (3.1.1)\nRequirement already satisfied: torch==2.5.1 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (2.5.1)\nRequirement already satisfied: torchaudio in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (2.5.1)\nRequirement already satisfied: torchvision in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (0.20.1)\nRequirement already satisfied: datasets in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from vllm==0.7.2+cpu) (3.3.2)\nRequirement already satisfied: astor in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from depyf==0.18.0->vllm==0.7.2+cpu) (0.8.1)\nRequirement already satisfied: dill in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from depyf==0.18.0->vllm==0.7.2+cpu) (0.3.8)\nRequirement already satisfied: interegular in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from outlines==0.1.11->vllm==0.7.2+cpu) (0.3.3)\nRequirement already satisfied: jinja2 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from outlines==0.1.11->vllm==0.7.2+cpu) (3.1.5)\nRequirement already satisfied: nest_asyncio in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from outlines==0.1.11->vllm==0.7.2+cpu) (1.6.0)\nRequirement already satisfied: diskcache in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from outlines==0.1.11->vllm==0.7.2+cpu) (5.6.3)\nRequirement already satisfied: referencing in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from outlines==0.1.11->vllm==0.7.2+cpu) (0.36.2)\nRequirement already satisfied: jsonschema in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from outlines==0.1.11->vllm==0.7.2+cpu) (4.23.0)\nRequirement already satisfied: pycountry in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from outlines==0.1.11->vllm==0.7.2+cpu) (24.6.1)\nRequirement already satisfied: airportsdata in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from outlines==0.1.11->vllm==0.7.2+cpu) (20250224)\nRequirement already satisfied: outlines_core==0.1.26 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from outlines==0.1.11->vllm==0.7.2+cpu) (0.1.26)\nRequirement already satisfied: networkx in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from torch==2.5.1->vllm==0.7.2+cpu) (3.4.2)\nRequirement already satisfied: fsspec in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from torch==2.5.1->vllm==0.7.2+cpu) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from torch==2.5.1->vllm==0.7.2+cpu) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.5.1->vllm==0.7.2+cpu) (1.3.0)\nRequirement already satisfied: starlette<0.46.0,>=0.40.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm==0.7.2+cpu) (0.45.3)\nRequirement already satisfied: packaging in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from lm-format-enforcer<0.11,>=0.10.9->vllm==0.7.2+cpu) (24.2)\nRequirement already satisfied: opencv-python-headless>=4.0.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from mistral_common[opencv]>=1.5.0->vllm==0.7.2+cpu) (4.11.0.86)\nRequirement already satisfied: anyio<5,>=3.5.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from openai>=1.52.0->vllm==0.7.2+cpu) (4.8.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from openai>=1.52.0->vllm==0.7.2+cpu) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from openai>=1.52.0->vllm==0.7.2+cpu) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from openai>=1.52.0->vllm==0.7.2+cpu) (0.8.2)\nRequirement already satisfied: sniffio in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from openai>=1.52.0->vllm==0.7.2+cpu) (1.3.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from pydantic>=2.9->vllm==0.7.2+cpu) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from pydantic>=2.9->vllm==0.7.2+cpu) (2.27.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from requests>=2.26.0->vllm==0.7.2+cpu) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from requests>=2.26.0->vllm==0.7.2+cpu) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from requests>=2.26.0->vllm==0.7.2+cpu) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from requests>=2.26.0->vllm==0.7.2+cpu) (2025.1.31)\nRequirement already satisfied: regex>=2022.1.18 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from tiktoken>=0.6.0->vllm==0.7.2+cpu) (2024.11.6)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from tokenizers>=0.19.1->vllm==0.7.2+cpu) (0.29.1)\nRequirement already satisfied: safetensors>=0.4.1 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from transformers>=4.48.2->vllm==0.7.2+cpu) (0.5.3)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from aiohttp->vllm==0.7.2+cpu) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from aiohttp->vllm==0.7.2+cpu) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from aiohttp->vllm==0.7.2+cpu) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from aiohttp->vllm==0.7.2+cpu) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from aiohttp->vllm==0.7.2+cpu) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from aiohttp->vllm==0.7.2+cpu) (0.3.0)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from aiohttp->vllm==0.7.2+cpu) (1.18.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from datasets->vllm==0.7.2+cpu) (19.0.1)\nRequirement already satisfied: pandas in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from datasets->vllm==0.7.2+cpu) (2.2.3)\nRequirement already satisfied: xxhash in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from datasets->vllm==0.7.2+cpu) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from datasets->vllm==0.7.2+cpu) (0.70.16)\nRequirement already satisfied: zipp>=3.20 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from importlib_metadata->vllm==0.7.2+cpu) (3.21.0)\nRequirement already satisfied: click>=7.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from uvicorn[standard]->vllm==0.7.2+cpu) (8.1.8)\nRequirement already satisfied: h11>=0.8 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from uvicorn[standard]->vllm==0.7.2+cpu) (0.14.0)\nRequirement already satisfied: httptools>=0.6.3 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from uvicorn[standard]->vllm==0.7.2+cpu) (0.6.4)\nRequirement already satisfied: python-dotenv>=0.13 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from uvicorn[standard]->vllm==0.7.2+cpu) (1.0.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from uvicorn[standard]->vllm==0.7.2+cpu) (0.21.0)\nRequirement already satisfied: watchfiles>=0.13 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from uvicorn[standard]->vllm==0.7.2+cpu) (1.0.4)\nRequirement already satisfied: websockets>=10.4 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from uvicorn[standard]->vllm==0.7.2+cpu) (15.0)\nRequirement already satisfied: httpcore==1.* in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.52.0->vllm==0.7.2+cpu) (1.0.7)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->vllm==0.7.2+cpu) (2024.10.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->vllm==0.7.2+cpu) (0.23.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from jinja2->outlines==0.1.11->vllm==0.7.2+cpu) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from pandas->datasets->vllm==0.7.2+cpu) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from pandas->datasets->vllm==0.7.2+cpu) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages (from pandas->datasets->vllm==0.7.2+cpu) (2025.1)\nBuilding wheels for collected packages: vllm\n  Building editable for vllm (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n  \n  \u00d7 Building editable for vllm (pyproject.toml) did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [173 lines of output]\n      /private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n        cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n      VLLM_TARGET_DEVICE automatically set to `cpu` due to macOS\n      running editable_wheel\n      creating /private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-wheel-0gh_7xri/.tmp-k7cx31nn/vllm.egg-info\n      writing /private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-wheel-0gh_7xri/.tmp-k7cx31nn/vllm.egg-info/PKG-INFO\n      writing dependency_links to /private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-wheel-0gh_7xri/.tmp-k7cx31nn/vllm.egg-info/dependency_links.txt\n      writing entry points to /private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-wheel-0gh_7xri/.tmp-k7cx31nn/vllm.egg-info/entry_points.txt\n      writing requirements to /private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-wheel-0gh_7xri/.tmp-k7cx31nn/vllm.egg-info/requires.txt\n      writing top-level names to /private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-wheel-0gh_7xri/.tmp-k7cx31nn/vllm.egg-info/top_level.txt\n      writing manifest file '/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-wheel-0gh_7xri/.tmp-k7cx31nn/vllm.egg-info/SOURCES.txt'\n      reading manifest template 'MANIFEST.in'\n      adding license file 'LICENSE'\n      writing manifest file '/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-wheel-0gh_7xri/.tmp-k7cx31nn/vllm.egg-info/SOURCES.txt'\n      creating '/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-wheel-0gh_7xri/.tmp-k7cx31nn/vllm-0.7.2+cpu.dist-info'\n      creating /private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-wheel-0gh_7xri/.tmp-k7cx31nn/vllm-0.7.2+cpu.dist-info/WHEEL\n      running build_py\n      running build_ext\n      -- The CXX compiler identification is AppleClang 16.0.0.16000026\n      -- Detecting CXX compiler ABI info\n      -- Detecting CXX compiler ABI info - done\n      -- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ - skipped\n      -- Detecting CXX compile features\n      -- Detecting CXX compile features - done\n      -- Build type: RelWithDebInfo\n      -- Target device: cpu\n      -- Found Python: /Users/shengyao/anaconda3/envs/vllm/bin/python (found version \"3.12.9\") found components: Interpreter Development.Module Development.SABIModule\n      -- Found python matching: /Users/shengyao/anaconda3/envs/vllm/bin/python.\n      CMake Warning at /private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\n        static library kineto_LIBRARY-NOTFOUND not found.\n      Call Stack (most recent call first):\n        /private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:120 (append_torchlib_if_found)\n        CMakeLists.txt:81 (find_package)\n      \n      \n      -- Found Torch: /private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/torch/lib/libtorch.dylib\n      -- Apple Silicon Detected\n      -- CPU extension compile flags: -Xpreprocessor;-fopenmp;-DVLLM_CPU_EXTENSION\n      -- NUMA is disabled\n      -- Enabling C extension.\n      -- Configuring done (1.2s)\n      -- Generating done (0.0s)\n      -- Build files have been written to: /private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/tmplepo0g_7.build-temp\n      [1/8] Building CXX object CMakeFiles/_C.dir/csrc/cpu/attention.cpp.o\n      FAILED: CMakeFiles/_C.dir/csrc/cpu/attention.cpp.o\n      ccache /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ -DPy_LIMITED_API=3 -DTORCH_EXTENSION_NAME=_C -DUSE_C10D_GLOO -DUSE_DISTRIBUTED -DUSE_RPC -DUSE_TENSORPIPE -DVLLM_NUMA_DISABLED -D_C_EXPORTS -I/Users/shengyao/vllm/csrc -isystem /Users/shengyao/anaconda3/envs/vllm/include/python3.12 -isystem /private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/torch/include -isystem /private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -O2 -g -DNDEBUG -std=gnu++17 -arch arm64 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.2.sdk -fPIC -Xpreprocessor -fopenmp -DVLLM_CPU_EXTENSION -MD -MT CMakeFiles/_C.dir/csrc/cpu/attention.cpp.o -MF CMakeFiles/_C.dir/csrc/cpu/attention.cpp.o.d -o CMakeFiles/_C.dir/csrc/cpu/attention.cpp.o -c /Users/shengyao/vllm/csrc/cpu/attention.cpp\n      /Users/shengyao/vllm/csrc/cpu/attention.cpp:281:40: error: use of undeclared identifier 'omp_get_max_threads'\n        281 |     const int parallel_work_item_num = omp_get_max_threads();\n            |                                        ^\n      /Users/shengyao/vllm/csrc/cpu/attention.cpp:301:22: error: use of undeclared identifier 'omp_get_thread_num'; did you mean 'at::get_thread_num'?\n        301 |             logits + omp_get_thread_num() * max_seq_len_padded;\n            |                      ^~~~~~~~~~~~~~~~~~\n            |                      at::get_thread_num\n      /private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/torch/include/ATen/Parallel.h:24:15: note: 'at::get_thread_num' declared here\n         24 | TORCH_API int get_thread_num();\n            |               ^\n      2 errors generated.\n      [2/8] Building CXX object CMakeFiles/_C.dir/csrc/cpu/utils.cpp.o\n      [3/8] Building CXX object CMakeFiles/_C.dir/csrc/cpu/layernorm.cpp.o\n      [4/8] Building CXX object CMakeFiles/_C.dir/csrc/cpu/cache.cpp.o\n      [5/8] Building CXX object CMakeFiles/_C.dir/csrc/cpu/pos_encoding.cpp.o\n      [6/8] Building CXX object CMakeFiles/_C.dir/csrc/cpu/activation.cpp.o\n      [7/8] Building CXX object CMakeFiles/_C.dir/csrc/cpu/torch_bindings.cpp.o\n      ninja: build stopped: subcommand failed.\n      Traceback (most recent call last):\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 139, in run\n          self._create_wheel_file(bdist_wheel)\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 340, in _create_wheel_file\n          files, mapping = self._run_build_commands(dist_name, unpacked, lib, tmp)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 263, in _run_build_commands\n          self._run_build_subcommands()\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 290, in _run_build_subcommands\n          self.run_command(name)\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 339, in run_command\n          self.distribution.run_command(command)\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/dist.py\", line 999, in run_command\n          super().run_command(command)\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1002, in run_command\n          cmd_obj.run()\n        File \"<string>\", line 249, in run\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/command/build_ext.py\", line 99, in run\n          _build_ext.run(self)\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 365, in run\n          self.build_extensions()\n        File \"<string>\", line 220, in build_extensions\n        File \"/Users/shengyao/anaconda3/envs/vllm/lib/python3.12/subprocess.py\", line 415, in check_call\n          raise CalledProcessError(retcode, cmd)\n      subprocess.CalledProcessError: Command '['cmake', '--build', '.', '-j=10', '--target=_C']' returned non-zero exit status 1.\n      /private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py:1002: _DebuggingTips: Problem in editable installation.\n      !!\n      \n              ********************************************************************************\n              An error happened while installing `vllm` in editable mode.\n      \n              The following steps are recommended to help debug this problem:\n      \n              - Try to install the project normally, without using the editable mode.\n                Does the error still persist?\n                (If it does, try fixing the problem before attempting the editable mode).\n              - If you are using binary extensions, make sure you have all OS-level\n                dependencies installed (e.g. compilers, toolchains, binary libraries, ...).\n              - Try the latest version of setuptools (maybe the error was already fixed).\n              - If you (or your project dependencies) are using any setuptools extension\n                or customization, make sure they support the editable mode.\n      \n              After following the steps above, if the problem still persists and\n              you think this is related to how setuptools handles editable installations,\n              please submit a reproducible example\n              (see https://stackoverflow.com/help/minimal-reproducible-example) to:\n      \n                  https://github.com/pypa/setuptools/issues\n      \n              See https://setuptools.pypa.io/en/latest/userguide/development_mode.html for details.\n              ********************************************************************************\n      \n      !!\n        cmd_obj.run()\n      Traceback (most recent call last):\n        File \"/Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 389, in <module>\n          main()\n        File \"/Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 373, in main\n          json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"/Users/shengyao/anaconda3/envs/vllm/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 303, in build_editable\n          return hook(wheel_directory, config_settings, metadata_directory)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 476, in build_editable\n          return self._build_with_temp_dir(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 407, in _build_with_temp_dir\n          self.run_setup()\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 320, in run_setup\n          exec(code, locals())\n        File \"<string>\", line 631, in <module>\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/__init__.py\", line 117, in setup\n          return distutils.core.setup(**attrs)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 186, in setup\n          return run_commands(dist)\n                 ^^^^^^^^^^^^^^^^^^\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 202, in run_commands\n          dist.run_commands()\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 983, in run_commands\n          self.run_command(cmd)\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/dist.py\", line 999, in run_command\n          super().run_command(command)\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1002, in run_command\n          cmd_obj.run()\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 139, in run\n          self._create_wheel_file(bdist_wheel)\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 340, in _create_wheel_file\n          files, mapping = self._run_build_commands(dist_name, unpacked, lib, tmp)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 263, in _run_build_commands\n          self._run_build_subcommands()\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 290, in _run_build_subcommands\n          self.run_command(name)\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 339, in run_command\n          self.distribution.run_command(command)\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/dist.py\", line 999, in run_command\n          super().run_command(command)\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1002, in run_command\n          cmd_obj.run()\n        File \"<string>\", line 249, in run\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/command/build_ext.py\", line 99, in run\n          _build_ext.run(self)\n        File \"/private/var/folders/y3/3jtypt212bb1qgh729dpkklr0000gn/T/pip-build-env-wdxq_9mv/overlay/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 365, in run\n          self.build_extensions()\n        File \"<string>\", line 220, in build_extensions\n        File \"/Users/shengyao/anaconda3/envs/vllm/lib/python3.12/subprocess.py\", line 415, in check_call\n          raise CalledProcessError(retcode, cmd)\n      subprocess.CalledProcessError: Command '['cmake', '--build', '.', '-j=10', '--target=_C']' returned non-zero exit status 1.\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building editable for vllm\nFailed to build vllm\nERROR: Failed to build installable wheels for some pyproject.toml based projects (vllm)\n\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2025-02-28T10:20:31+00:00",
    "closed_at": "2025-03-03T01:35:02+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14034/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14034"
  },
  {
    "number": 311,
    "title": "Support `ChatCompletion` Endpoint in OpenAI demo server",
    "body": "@infwinston Feel free to use FastChat's completion template to implement a chat completion endpoint in our demo server. You can use the completion API as a reference:\r\n\r\nhttps://github.com/vllm-project/vllm/blob/9d27b09d12767de775a92d765e177a61f8477189/vllm/entrypoints/openai/api_server.py#L88-L101",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2023-06-29T14:25:32+00:00",
    "closed_at": "2023-07-03T17:47:45+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/311/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/311"
  },
  {
    "number": 3779,
    "title": "[Bug]: vllm-0.4.0 is much slower than vllm-0.3.3",
    "body": "### Your current environment\r\n\r\nTraceback (most recent call last):\r\n  File \"/zhanghongbo/collect_env.py\", line 719, in <module>\r\n    main()\r\n  File \"/zhanghongbo/collect_env.py\", line 698, in main\r\n    output = get_pretty_env_info()\r\n  File \"/zhanghongbo/collect_env.py\", line 693, in get_pretty_env_info\r\n    return pretty_str(get_env_info())\r\n  File \"/zhanghongbo/collect_env.py\", line 530, in get_env_info\r\n    vllm_version = get_vllm_version()\r\n  File \"/zhanghongbo/collect_env.py\", line 262, in get_vllm_version\r\n    return vllm.__version__\r\nAttributeError: module 'vllm' has no attribute '__version__'\r\n\r\nI encountered the above issue, so I commented out a line of code.\r\n\r\nPyTorch version: 2.1.2+cu118\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.8\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\nClang version: Could not collect\r\nCMake version: version 3.26.1\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.17 (main, Jul  5 2023, 20:41:20)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-3.10.0-1160.el7.x86_64-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 11.8.89\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nGPU 1: NVIDIA A100-SXM4-80GB\r\nGPU 2: NVIDIA A100-SXM4-80GB\r\n\r\nNvidia driver version: 525.105.17\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.4\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.4\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.4\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.4\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.4\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.4\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.4\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nThread(s) per core:              2\r\nCore(s) per socket:              32\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           106\r\nModel name:                      Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz\r\nStepping:                        6\r\nFrequency boost:                 enabled\r\nCPU MHz:                         800.000\r\nCPU max MHz:                     3400.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        5200.00\r\nVirtualization:                  VT-x\r\nL1d cache:                       3 MiB\r\nL1i cache:                       2 MiB\r\nL2 cache:                        80 MiB\r\nL3 cache:                        96 MiB\r\nNUMA node0 CPU(s):               0-31,64-95\r\nNUMA node1 CPU(s):               32-63,96-127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; Load fences, usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 invpcid_single intel_pt ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq md_clear pconfig spec_ctrl intel_stibp flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.23.5\r\n[pip3] onnxruntime==1.16.0\r\n[pip3] torch==2.1.2+cu118\r\n[pip3] torch-tb-profiler==0.4.1\r\n[pip3] torchaudio==2.1.2+cu118\r\n[pip3] torchvision==0.16.2+cu118\r\n[pip3] triton==2.1.0\r\n[conda] numpy                     1.23.5                   pypi_0    pypi\r\n[conda] torch                     2.1.2+cu118              pypi_0    pypi\r\n[conda] torch-tb-profiler         0.4.1                    pypi_0    pypi\r\n[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\r\n[conda] torchvision               0.16.2+cu118             pypi_0    pypi\r\n[conda] triton                    2.1.0                    pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0+cuda11.8\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    NIC0    CPU Affinity    NUMA Affinity\r\nGPU0     X      NV12    NV12    NODE    0-31,64-95      0\r\nGPU1    NV12     X      NV12    PXB     0-31,64-95      0\r\nGPU2    NV12    NV12     X      SYS     32-63,96-127    1\r\nNIC0    NODE    PXB     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nHello, I found that when running the same code to generate the same number of responses, the speed of vllm 0.3.3 is four times faster than vllm-0.4.0.\r\n\r\nAt the same time, I also discovered that after generating responses for a period of time, the speed of vllm-0.3.3 starts to slow down. I'm not sure what is causing this issue.\r\n\r\nvllm-0.4.0:\r\n<img width=\"735\" alt=\"image\" src=\"https://github.com/vllm-project/vllm/assets/34183206/61b03dd9-e54b-4552-8c73-936cfa380204\">\r\nvllm-0.3.3:\r\n<img width=\"732\" alt=\"image\" src=\"https://github.com/vllm-project/vllm/assets/34183206/cbf8a474-be0b-40d7-bfbb-4cffa7ce2004\">\r\nMy code is as below:\r\n```\r\nmodel_path = '/pretrained_models/Yi-34B-Chat'\r\nsampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=1,logprobs=tokenizer.vocab_size)\r\nllm = LLM(model=model_path,enforce_eager=True,\r\n          max_logprobs = tokenizer.vocab_size\r\n          )\r\noutput_list = llm.generate(input_list,sampling_params,use_tqdm=True)\r\n```\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-04-01T17:33:48+00:00",
    "closed_at": "2024-04-02T22:32:22+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3779/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3779"
  },
  {
    "number": 18928,
    "title": "[Feature]: Return token strings in addition to token ids for /tokenize",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently /tokenize return array of token ids\n```\n\"tokens\": [\n    50258,\n    50363,\n    16216\n  ],\n```\nCan it also return the token strings, e.g.\n```\n\"token_strs\": [\n    \"this\",\n    \" is\",\n    \" a\"\n  ],\n```\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-05-29T21:21:52+00:00",
    "closed_at": "2025-05-31T18:00:13+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18928/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/18928"
  },
  {
    "number": 3309,
    "title": "Fail to build vllm from source for H100",
    "body": "It worked if I change `NVIDIA_SUPPORTED_ARCHS` in `setup.py` to `NVIDIA_SUPPORTED_ARCHS = {\"8.0\", \"8.6\"}`\n\n```\n#10 812.2       [1/12] /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-o232h5cp/overlay/local/lib/python3.10/dist-packages/torch/include -I/tmp/pip-build-env-o232h5cp/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o232h5cp/overlay/local/lib/python3.10/dist-packages/torch/include/TH -I/tmp/pip-build-env-o232h5cp/overlay/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /home/corvo/vllm/csrc/cuda_utils_kernels.cu -o /tmp/tmp6jq49t5c.build-temp/csrc/cuda_utils_kernels.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_80,code=sm_80 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n#10 812.2       [2/12] c++ -MMD -MF /tmp/tmp6jq49t5c.build-temp/csrc/pybind.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-build-env-o232h5cp/overlay/local/lib/python3.10/dist-packages/torch/include -I/tmp/pip-build-env-o232h5cp/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o232h5cp/overlay/local/lib/python3.10/dist-packages/torch/include/TH -I/tmp/pip-build-env-o232h5cp/overlay/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /home/corvo/vllm/csrc/pybind.cpp -o /tmp/tmp6jq49t5c.build-temp/csrc/pybind.o -g -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n#10 812.2       [3/12] /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-o232h5cp/overlay/local/lib/python3.10/dist-packages/torch/include -I/tmp/pip-build-env-o232h5cp/overlay/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o232h5cp/overlay/local/lib/python3.10/dist-packages/torch/include/TH -I/tmp/pip-build-env-o232h5cp/overlay/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /home/corvo/vllm/csrc/quantization/awq/gemm_kernels.cu -o /tmp/tmp6jq49t5c.build-temp/csrc/quantization/awq/gemm_kernels.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_80,code=sm_80 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n#10 812.2       FAILED: /tmp/tmp6jq49t5c.build-temp/csrc/quantization/awq/gemm_kernels.o\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-03-11T05:39:40+00:00",
    "closed_at": "2024-03-12T20:35:55+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3309/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3309"
  },
  {
    "number": 11122,
    "title": "[Bug]: InternVL2-Llama3-76B-AWQ RUN ERROR KeyError: 'layers.39.mlp.gate_up_proj.qweight'",
    "body": "### Your current environment\n\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.4.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PIX     PXB     PXB     SYS     SYS     SYS     SYS     0-95            N/A             N/A\r\nGPU1    PIX      X      PXB     PXB     SYS     SYS     SYS     SYS     0-95            N/A             N/A\r\nGPU2    PXB     PXB      X      PXB     SYS     SYS     SYS     SYS     0-95            N/A             N/A\r\nGPU3    PXB     PXB     PXB      X      SYS     SYS     SYS     SYS     0-95            N/A             N/A\r\nGPU4    SYS     SYS     SYS     SYS      X      PIX     PXB     PXB     0-95            N/A             N/A\r\nGPU5    SYS     SYS     SYS     SYS     PIX      X      PXB     PXB     0-95            N/A             N/A\r\nGPU6    SYS     SYS     SYS     SYS     PXB     PXB      X      PXB     0-95            N/A             N/A\r\nGPU7    SYS     SYS     SYS     SYS     PXB     PXB     PXB      X      0-95            N/A             N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNVIDIA_VISIBLE_DEVICES=all\r\nCUBLAS_VERSION=12.2.5.6\r\nNVIDIA_REQUIRE_CUDA=cuda>=9.0\r\nCUDA_CACHE_DISABLE=1\r\nNCCL_VERSION=2.19.3\r\nNVIDIA_DRIVER_CAPABILITIES=compute,utility,video\r\nNVIDIA_PRODUCT_NAME=Triton Server\r\nCUDA_VERSION=12.2.2.009\r\nCUDA_VISIBLE_DEVICES=0,1,2,3\r\nCUDA_VISIBLE_DEVICES=0,1,2,3\r\nCUDNN_VERSION=8.9.5.29\r\nNVIDIA_TRITON_SERVER_VERSION=23.10\n\n### Model Input Dumps\n\nNone\n\n### \ud83d\udc1b Describe the bug\n\nDoes the **InternVL2-Llama3-76B-AWQ** model support this configuration?  \r\nI am running **vllm version 0.6.4.post1+cu122** with the following command:\r\n\r\n```bash\r\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 OMP_NUM_THREADS=64 vllm serve /InternVL2-Llama3-76B-AWQ \\\r\n  --limit-mm-per-prompt image=6 \\\r\n  --tensor-parallel-size 8 \\\r\n  --gpu-memory-utilization 0.9 \\\r\n  --enforce_eager \\\r\n  --max_model_len 8192\r\n```\r\n\r\nHowever, I encountered an error. Could you confirm if this model is currently supported or if there are known bugs?\r\n\r\n```\r\nERROR 12-12 03:24:49 engine.py:366] 'layers.39.mlp.gate_up_proj.qweight'\r\nERROR 12-12 03:24:49 engine.py:366] Traceback (most recent call last):\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/engine/multiprocessing/engine.py\", line 357, in run_mp_engine\r\nERROR 12-12 03:24:49 engine.py:366]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/engine/multiprocessing/engine.py\", line 119, in from_engine_args\r\nERROR 12-12 03:24:49 engine.py:366]     return cls(ipc_path=ipc_path,\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/engine/multiprocessing/engine.py\", line 71, in __init__\r\nERROR 12-12 03:24:49 engine.py:366]     self.engine = LLMEngine(*args, **kwargs)\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/engine/llm_engine.py\", line 347, in __init__\r\nERROR 12-12 03:24:49 engine.py:366]     self.model_executor = executor_class(vllm_config=vllm_config, )\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/executor/distributed_gpu_executor.py\", line 26, in __init__\r\nERROR 12-12 03:24:49 engine.py:366]     super().__init__(*args, **kwargs)\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/executor/executor_base.py\", line 36, in __init__\r\nERROR 12-12 03:24:49 engine.py:366]     self._init_executor()\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/executor/multiproc_gpu_executor.py\", line 114, in _init_executor\r\nERROR 12-12 03:24:49 engine.py:366]     self._run_workers(\"load_model\",\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/executor/multiproc_gpu_executor.py\", line 195, in _run_workers\r\nERROR 12-12 03:24:49 engine.py:366]     driver_worker_output = driver_worker_method(*args, **kwargs)\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/worker/worker.py\", line 152, in load_model\r\nERROR 12-12 03:24:49 engine.py:366]     self.model_runner.load_model()\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/worker/model_runner.py\", line 1074, in load_model\r\nERROR 12-12 03:24:49 engine.py:366]     self.model = get_model(vllm_config=self.vllm_config)\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/model_executor/model_loader/__init__.py\", line 12, in get_model\r\nERROR 12-12 03:24:49 engine.py:366]     return loader.load_model(vllm_config=vllm_config)\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/model_executor/model_loader/loader.py\", line 334, in load_model\r\nERROR 12-12 03:24:49 engine.py:366]     model.load_weights(self._get_all_weights(model_config, model))\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/model_executor/models/internvl.py\", line 668, in load_weights\r\nERROR 12-12 03:24:49 engine.py:366]     loader.load_weights(weights)\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/model_executor/models/utils.py\", line 229, in load_weights\r\nERROR 12-12 03:24:49 engine.py:366]     autoloaded_weights = list(self._load_module(\"\", self.module, weights))\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/model_executor/models/utils.py\", line 190, in _load_module\r\nERROR 12-12 03:24:49 engine.py:366]     yield from self._load_module(prefix,\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/model_executor/models/utils.py\", line 175, in _load_module\r\nERROR 12-12 03:24:49 engine.py:366]     module_load_weights(weights)\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/model_executor/models/llama.py\", line 586, in load_weights\r\nERROR 12-12 03:24:49 engine.py:366]     loader.load_weights(\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/model_executor/models/utils.py\", line 229, in load_weights\r\nERROR 12-12 03:24:49 engine.py:366]     autoloaded_weights = list(self._load_module(\"\", self.module, weights))\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/model_executor/models/utils.py\", line 190, in _load_module\r\nERROR 12-12 03:24:49 engine.py:366]     yield from self._load_module(prefix,\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/model_executor/models/utils.py\", line 175, in _load_module\r\nERROR 12-12 03:24:49 engine.py:366]     module_load_weights(weights)\r\nERROR 12-12 03:24:49 engine.py:366]   File \"/code/lab/projects/vllm/vllm/model_executor/models/llama.py\", line 390, in load_weights\r\nERROR 12-12 03:24:49 engine.py:366]     param = params_dict[name]\r\nERROR 12-12 03:24:49 engine.py:366] KeyError: 'layers.39.mlp.gate_up_proj.qweight'\r\nINFO 12-12 03:24:50 multiproc_worker_utils.py:120] Killing local vLLM worker processes\r\n```\r\n\r\n Thanks!\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-12-12T03:59:26+00:00",
    "closed_at": "2024-12-17T02:44:39+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11122/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11122"
  },
  {
    "number": 1563,
    "title": "Issue when trying to run inference on this model EleutherAI/gpt-j-6b",
    "body": "EleutherAI/gpt-j-6b is mentioned as supported in the docs. Trying to run inference on Google Colab with free tier GPU. Getting this error. AssertionError: tensor model parallel group is already initialized.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-11-04T18:29:30+00:00",
    "closed_at": "2023-11-08T07:12:38+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1563/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1563"
  },
  {
    "number": 7325,
    "title": "[Usage]: How to config the parameters to support higher concurrency for deploying the qwen2-7b model as an API at 8-GPU A800 (80G) server?",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-78-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA A800-SXM4-80GB\r\nGPU 1: NVIDIA A800-SXM4-80GB\r\nGPU 2: NVIDIA A800-SXM4-80GB\r\nGPU 3: NVIDIA A800-SXM4-80GB\r\nGPU 4: NVIDIA A800-SXM4-80GB\r\nGPU 5: NVIDIA A800-SXM4-80GB\r\nGPU 6: NVIDIA A800-SXM4-80GB\r\nGPU 7: NVIDIA A800-SXM4-80GB\r\n\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n```\r\n\n\n### How would you like to use vllm\n\nI have one GPU server with 8-GPU A800 (80G). \r\nAnd I want to serve the model qwen2-7b as OpenAI API. The qwen2-7b can be run one GPU.\r\nI want to run 8 models paralleling as 8 instances to support multiple API requests concurrently.  \r\nThe basic requirement is as below pic shows.\r\n\r\n![image](https://github.com/user-attachments/assets/99555837-e1cd-4134-aa64-a88928760797)\r\n\r\nBelow is my cmd (8-gpu-serve) :\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -m vllm.entrypoints.openai.api_server \\\r\n    --model /data/models/Qwen2-7B-Instruct/ \\\r\n    --served-model-name aaa \\\r\n    --trust-remote-code \\\r\n    --tensor-parallel-size 1 \\\r\n    --pipeline-parallel-size 8 \\\r\n    --port 8000\r\n```\r\n\r\nBut my multiprocessing test shows the request average response time of 50 processes is almost the same as below (1-gpu-serve):\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=0 python -m vllm.entrypoints.openai.api_server \\\r\n    --model /data/models/Qwen2-7B-Instruct/ \\\r\n    --served-model-name aaa \\\r\n    --trust-remote-code \\\r\n    --tensor-parallel-size 1 \\\r\n    --pipeline-parallel-size 1 \\\r\n    --port 8000\r\n```\r\n\r\nMy question is how to set the parameters to get better concurrency to use 8 GPUs ?\r\n\r\n\r\n\r\n",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-08-09T02:39:50+00:00",
    "closed_at": "2024-08-13T03:08:08+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7325/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7325"
  },
  {
    "number": 13885,
    "title": "[Feature]: Any plan run deepseek-r1 fp8 on Ampere gpu",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nI notice the core reason of vllm can not deployment deepseek-r1 fp8 model is `Marlin doesn't support block-wise fp8`. So, What are the possible approaches to run the DeepSeek R1 FP8 on Ampere architecture GPU\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-02-26T09:30:08+00:00",
    "closed_at": "2025-03-03T01:56:34+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13885/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13885"
  },
  {
    "number": 13120,
    "title": "[Usage]: How to use breakpoints with VLLM to debug",
    "body": "### Your current environment\n\nNone\n\n### How would you like to use vllm\n\nIt seems like the execution of vllm is managed through a compiled engine, which makes it challenging to directly debug within an IDE using breakpoints. When debugging some custom funcs to the vllm, is there any way that we could disable compilation and just set breakpoints like normal python scripts? Thank you for your help!\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-02-11T23:48:52+00:00",
    "closed_at": "2025-02-17T09:55:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13120/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13120"
  },
  {
    "number": 15999,
    "title": "[Bug]: The reasoning_parser doesn't dynamically apply to parser.add_argument after I added a custom Reasoning Parser",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\npip install vllm==0.8.2\n\n```text\n\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nHello\n\nI have declared a custom reasoning parser class and am preparing for online serving. However, the functions within this package alone cannot initialize the custom reasoning parser class class. \nAdditionally, during xgrammars initialization, only deepseek_r1 is being used as the reasoner. \nWhen a custom reasoning parser class is created, I would like to be able to dynamically initialize the reasoner as well during the ReasoningParserManager in vllm engine initialization.\n\nThank you for your time and consideration.\nRegard\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-04-03T07:42:16+00:00",
    "closed_at": "2025-04-10T06:55:42+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15999/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15999"
  },
  {
    "number": 7246,
    "title": "[Bug]: ngc24.05 \"RuntimeError: Cannot re-initialize CUDA in forked subprocess.\"",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-3.10.0-1062.9.1.el7.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA A30\r\nGPU 1: NVIDIA A30\r\nGPU 2: NVIDIA A30\r\nGPU 3: NVIDIA A30\r\n\r\nNvidia driver version: 525.85.12\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.1.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Platinum 8350C CPU @ 2.60GHz\r\nCPU family:                      6\r\nModel:                           106\r\nThread(s) per core:              2\r\nCore(s) per socket:              32\r\nSocket(s):                       2\r\nStepping:                        6\r\nFrequency boost:                 enabled\r\nCPU max MHz:                     3500.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        5200.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 invpcid_single intel_pt ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq md_clear pconfig spec_ctrl intel_stibp flush_l1d arch_capabilities\r\nVirtualization:                  VT-x\r\nL1d cache:                       3 MiB (64 instances)\r\nL1i cache:                       2 MiB (64 instances)\r\nL2 cache:                        80 MiB (64 instances)\r\nL3 cache:                        96 MiB (2 instances)\r\nNUMA node(s):                    1\r\nNUMA node0 CPU(s):               0-127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Vulnerable: Load fences, __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:        Vulnerable, IBPB\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.24.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] onnx==1.16.0\r\n[pip3] optree==0.11.0\r\n[pip3] pytorch-quantization==2.1.2\r\n[pip3] pytorch-triton==3.0.0+989adb9a2\r\n[pip3] torch==2.4.0\r\n[pip3] torch-tensorrt==2.4.0a0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.43.4\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.4\r\nvLLM Build Flags:\r\nCUDA Archs: 5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\r\nGPU0\t X \tSYS\tSYS\tSYS\t0-127\t\tN/A\r\nGPU1\tSYS\t X \tSYS\tSYS\t0-127\t\tN/A\r\nGPU2\tSYS\tSYS\t X \tSYS\t0-127\t\tN/A\r\nGPU3\tSYS\tSYS\tSYS\t X \t0-127\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\n```bash\r\npip install vllm==0.5.4\r\nexport VLLM_WORKER_MULTIPROC_METHOD=spawn\r\nCUDA_VISIBLE_DEVICES=0 vllm serve openbmb/MiniCPM-Llama3-V-2_5 --trust-remote-code --tensor-parallel-size 1 --max-model-len 2048\r\n```\r\nThe following error happens in ngc24.05 no matter set `VLLM_WORKER_MULTIPROC_METHOD=spawn` or not.\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/server.py\", line 217, in run_rpc_server\r\n    server = AsyncEngineRPCServer(async_engine_args, usage_context, port)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/server.py\", line 25, in __init__\r\n    self.engine = AsyncLLMEngine.from_engine_args(async_engine_args,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 471, in from_engine_args\r\n    engine = cls(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 381, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 552, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 249, in __init__\r\n    self.model_executor = executor_class(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\", line 47, in __init__\r\n    self._init_executor()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 35, in _init_executor\r\n    self.driver_worker.init_device()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 123, in init_device\r\n    torch.cuda.set_device(self.device)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 420, in set_device\r\n    torch._C._cuda_setDevice(device)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\r\n    raise RuntimeError(\r\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\r\n```\r\nHowever, everything went fine using ngc24.02/ngc24.04 (tested) or any ngc image version lower than 24.05 (I guess).",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-08-07T04:45:15+00:00",
    "closed_at": "2024-08-13T22:40:18+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7246/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7246"
  },
  {
    "number": 3001,
    "title": "Offline Batched Inference with lora?",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-23T02:15:48+00:00",
    "closed_at": "2024-02-27T21:57:14+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3001/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3001"
  },
  {
    "number": 6893,
    "title": "[Bug]: Stuck at \"generating GPU P2P access cache\"",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: 2.3.1\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-113-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\n\r\nCUDA runtime version: 12.1.66\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA H100 PCIe\r\nGPU 1: NVIDIA H100 PCIe\r\n\r\nNvidia driver version: 535.161.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             128\r\nOn-line CPU(s) list:                0-127\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Gold 6430\r\nCPU family:                         6\r\nModel:                              143\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 32\r\nSocket(s):                          2\r\nStepping:                           8\r\nFrequency boost:                    enabled\r\nCPU max MHz:                        2101.0000\r\nCPU min MHz:                        800.0000\r\nBogoMIPS:                           4200.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe sy\r\nscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 mon\r\nitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm\r\n3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad f\r\nsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw av\r\nx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pt\r\ns avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdi\r\nri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\n\r\nVirtualization:                     VT-x\r\nL1d cache:                          3 MiB (64 instances)\r\nL1i cache:                          2 MiB (64 instances)\r\nL2 cache:                           128 MiB (64 instances)\r\nL3 cache:                           120 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-31,64-95\r\nNUMA node1 CPU(s):                  32-63,96-127\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.3.1\r\n[pip3] torchaudio==2.3.1\r\n[pip3] torchvision==0.18.1\r\n[pip3] transformers==4.43.3\r\n[pip3] triton==2.3.1\r\n[conda] blas                      1.0                         mkl    defaults\r\n[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\r\n[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\r\n[conda] mkl                       2023.1.0         h213fc3f_46344    defaults\r\n[conda] mkl-service               2.4.0           py311h5eee18b_1    defaults\r\n[conda] mkl_fft                   1.3.8           py311h5eee18b_0    defaults\r\n[conda] mkl_random                1.2.4           py311hdb19cb5_0    defaults\r\n[conda] nccl                      2.22.3.1             hbc370b7_0    conda-forge\r\n[conda] numpy                     1.26.4          py311h08b1b3b_0    defaults\r\n[conda] numpy-base                1.26.4          py311hf175353_0    defaults\r\n[conda] pytorch                   2.3.1           py3.11_cuda12.1_cudnn8.9.2_0    pytorch\r\n[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch\r\n[conda] pytorch-mutex             1.0                        cuda    pytorch\r\n[conda] torchaudio                2.3.1               py311_cu121    pytorch\r\n[conda] torchtriton               2.3.1                     py311    pytorch\r\n[conda] torchvision               0.18.1              py311_cu121    pytorch\r\n[conda] transformers              4.43.3                   pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.3.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NODE    SYS     32-63,96-127    1               N/A\r\nGPU1    NODE     X      SYS     32-63,96-127    1               N/A\r\nNIC0    SYS     SYS      X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n```\n\n### \ud83d\udc1b Describe the bug\n\n\r\n```bash \r\nCUDA_VISIBLE_DEVICES=0,1 vllm serve /data1/chenweize/checkpoints/Meta-Llama-3.1-70B-Instruct/ --dtype auto -tp 2 --enable-chunked-prefill=false --max-model-len 8192\r\n```\r\n\r\n**Output**:\r\n```\r\nINFO 07-29 08:28:17 api_server.py:219] vLLM API server version 0.5.3.post1\r\nINFO 07-29 08:28:17 api_server.py:220] args: Namespace(model_tag='/data1/chenweize/checkpoints/Meta-Llama-3.1-70B-Instruct/', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='/data1/chenweize/checkpoints/Meta-Llama-3.1-70B-Instruct/', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=8192, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None, dispatch_function=<function serve at 0x7fa0f244ea20>)\r\nINFO 07-29 08:28:17 config.py:715] Defaulting to use mp for distributed inference\r\nINFO 07-29 08:28:17 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/data1/chenweize/checkpoints/Meta-Llama-3.1-70B-Instruct/', speculative_config=None, tokenizer='/data1/chenweize/checkpoints/Meta-Llama-3.1-70B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/data1/chenweize/checkpoints/Meta-Llama-3.1-70B-Instruct/, use_v2_block_manager=False, enable_prefix_caching=False)\r\nINFO 07-29 08:28:18 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n(VllmWorkerProcess pid=730991) INFO 07-29 08:28:18 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\nINFO 07-29 08:28:18 utils.py:784] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=730991) INFO 07-29 08:28:18 utils.py:784] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=730991) INFO 07-29 08:28:18 pynccl.py:63] vLLM is using nccl==2.22.3\r\nINFO 07-29 08:28:18 pynccl.py:63] vLLM is using nccl==2.22.3\r\nINFO 07-29 08:28:19 custom_all_reduce_utils.py:212] generating GPU P2P access cache in /home/chenweize/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n```\r\n\r\nAfter some debugging, I locate the potential bug in https://github.com/vllm-project/vllm/blob/3eeb148f467e3619e8890b1a5ebe86a173f91bc9/vllm/distributed/device_communicators/cuda_wrapper.py#L39-L56\r\nwhere the cudart library is assumed to locate in \".../nvidia/cuda_runtime/lib\", but it is not the case in my environment. Is the \"hardcoded\" path reasonable? In my environment, the library seems to locate in \"path_to_miniconda/envs/myenv/lib/libcudart.so\".",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-07-29T08:35:14+00:00",
    "closed_at": "2024-08-04T18:31:53+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6893/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6893"
  },
  {
    "number": 15453,
    "title": "[Bug]: ValueError: not enough values to unpack (expected 22, got 21) when deploying DeepSeekV3",
    "body": "### Your current environment\n\nFor the environment, i simply use the vllm v0.8.0 docker image.\n\n### \ud83d\udc1b Describe the bug\n\nWhen deploying DeepSeekV3 with TP=16 on two nodes, I encounter the `ValueError: not enough values to unpack (expected 22, got 21) ` error.\n\nTo create containers and ray, I use the  following commands, in which the `run_cluster.sh` refers to [vllm sample](https://github.com/vllm-project/vllm/blob/main/examples/online_serving/run_cluster.sh).\n```bash\n# head node\nDOCKER_IMAGE=\"xxx\"\nHEAD_NODE_ADDRESS=\"xxx\"\nNODE_TYPE=\"--head\"  # Should be --head or --worker\nPATH_TO_HF_HOME=\"xxx\"\n\nCRT_NODE_ADDRESS=${HEAD_NODE_ADDRESS}\n\nbash run_cluster.sh \\\n        ${DOCKER_IMAGE} \\\n        ${HEAD_NODE_ADDRESS} \\\n        ${NODE_TYPE} \\\n        ${PATH_TO_HF_HOME} \\\n        -e VLLM_HOST_IP=${CRT_NODE_ADDRESS}\n\n# other node\nDOCKER_IMAGE=\"xxx\"\nHEAD_NODE_ADDRESS=\"xxx\"\nNODE_TYPE=\"--worker\"  # Should be --head or --worker\nPATH_TO_HF_HOME=\"xxx\"\n\nCRT_NODE_ADDRESS=\"xxx\"\n\nbash run_cluster.sh \\\n        ${DOCKER_IMAGE} \\\n        ${HEAD_NODE_ADDRESS} \\\n        ${NODE_TYPE} \\\n        ${PATH_TO_HF_HOME} \\\n        -e VLLM_HOST_IP=${CRT_NODE_ADDRESS}\n```\n\nTo start API service, I use\n```bash\nNCCL_DEBUG=INFO \\\nNCCL_SOCKET_IFNAME=eth0 \\\nGLOO_SOCKET_IFNAME=eth0 \\\nHF_EVALUATE_OFFLINE=1 \\\nHF_DATASETS_OFFLINE=1 \\\nTRANSFORMERS_OFFLINE=1 \\\nvllm serve xxx \\\n     --trust-remote-code \\\n     --tensor-parallel-size 16 2>&1 | tee api_serve.log\n```\n\nThe following errors occur when starting API service:\n\n```\nINFO 03-25 08:30:56 [__init__.py:256] Automatically detected platform cuda.\nINFO 03-25 08:30:58 [api_server.py:977] vLLM API server version 0.8.0\nINFO 03-25 08:30:58 [api_server.py:978] args: Namespace(subparser='serve', model_tag='/mnt/huggingface_hub_137_llm/hub/models--deepseek-ai--DeepSeek-V3/snapshots/86518964eaef84e3fdd98e9861759a1384f9c29d', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/mnt/huggingface_hub_137_llm/hub/models--deepseek-ai--DeepSeek-V3/snapshots/86518964eaef84e3fdd98e9861759a1384f9c29d', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=16, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x7f09effcc2c0>)\nINFO 03-25 08:30:58 [config.py:208] Replacing legacy 'type' key with 'rope_type'\nINFO 03-25 08:31:20 [config.py:583] This model supports multiple tasks: {'generate', 'reward', 'score', 'embed', 'classify'}. Defaulting to 'generate'.\nINFO 03-25 08:32:09 [config.py:1515] Defaulting to use ray for distributed inference\nINFO 03-25 08:32:09 [config.py:1693] Chunked prefill is enabled with max_num_batched_tokens=2048.\nWARNING 03-25 08:32:10 [fp8.py:54] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\nINFO 03-25 08:32:10 [cuda.py:159] Forcing kv cache block size to 64 for FlashMLA backend.\nINFO 03-25 08:32:10 [config.py:208] Replacing legacy 'type' key with 'rope_type'\nINFO 03-25 08:32:13 [__init__.py:256] Automatically detected platform cuda.\nINFO 03-25 08:32:16 [core.py:53] Initializing a V1 LLM engine (v0.8.0) with config: model='/mnt/huggingface_hub_137_llm/hub/models--deepseek-ai--DeepSeek-V3/snapshots/86518964eaef84e3fdd98e9861759a1384f9c29d', speculative_config=None, tokenizer='/mnt/huggingface_hub_137_llm/hub/models--deepseek-ai--DeepSeek-V3/snapshots/86518964eaef84e3fdd98e9861759a1384f9c29d', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=163840, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=16, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/mnt/huggingface_hub_137_llm/hub/models--deepseek-ai--DeepSeek-V3/snapshots/86518964eaef84e3fdd98e9861759a1384f9c29d, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n2025-03-25 08:32:16,128 INFO worker.py:1654 -- Connecting to existing Ray cluster at address: xxx:6379...\n2025-03-25 08:32:16,137 INFO worker.py:1841 -- Connected to Ray cluster.\nWARNING 03-25 08:32:16 [ray_utils.py:199] tensor_parallel_size=16 is bigger than a reserved number of GPUs (8 GPUs) in a node 44f85237c2ccc6f3044757686b8b20f050cd28746ad6a254b6977c95. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 16 GPUs available at each node.\nWARNING 03-25 08:32:16 [ray_utils.py:199] tensor_parallel_size=16 is bigger than a reserved number of GPUs (8 GPUs) in a node da63bb6d76fe02da166a0dd739e32a8ea2a18599f095f4418a043139. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 16 GPUs a[[36m(RayWorkerWrapper pid=2300)^[[0m ^MLoading safetensors checkpoint shards: 100% Completed | 163/163 [00:34<00:00,  4.72it/s]\n^[[36m(RayWorkerWrapper pid=2300)^[[0m\n^[[36m(RayWorkerWrapper pid=10522, ip=xxx)^[[0m WARNING 03-25 08:33:08 [utils.py:574] Overwriting environment variable LD_LIBRARY_PATH from '/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64' to '/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64'\n^[[36m(pid=10529, ip=xxx)^[[0m INFO 03-25 08:32:27 [__init__.py:256] Automatically detected platform cuda.^[[32m [repeated 7x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=3178)^[[0m WARNING 03-25 08:33:13 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f79ddcb7d10>\n^[[36m(RayWorkerWrapper pid=3178)^[[0m WARNING 03-25 08:33:08 [utils.py:574] Overwriting environment variable LD_LIBRARY_PATH from '/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64' to '/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64'^[[32m [repeated 15x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=10526, ip=xxx)^[[0m WARNING 03-25 08:33:34 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fb2c5f5f470>^[[32m [repeated 8x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=2300)^[[0m INFO 03-25 08:33:36 [utils.py:925] Found nccl from library libnccl.so.2\n^[[36m(RayWorkerWrapper pid=2300)^[[0m INFO 03-25 08:33:36 [pynccl.py:69] vLLM is using nccl==2.21.5\n^[[36m(RayWorkerWrapper pid=10522, ip=xxx)^[[0m WARNING 03-25 08:33:40 [custom_all_reduce.py:84] Custom allreduce is disabled because this process group spans across nodes.\n^[[36m(RayWorkerWrapper pid=10527, ip=xxx)^[[0m WARNING 03-25 08:33:34 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f6f76ce2030>^[[32m [repeated 7x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=2300)^[[0m INFO 03-25 08:33:40 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_3068bba3'), local_subscribe_addr='ipc:///tmp/6d3dc210-f056-431c-b370-1f520ed62f33', remote_subscribe_addr='tcp://xxx:33595', remote_addr_ipv6=False)\n^[[36m(RayWorkerWrapper pid=2300)^[[0m INFO 03-25 08:33:40 [parallel_state.py:967] rank 0 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 0\n^[[36m(RayWorkerWrapper pid=2300)^[[0m INFO 03-25 08:33:40 [cuda.py:206] Using FlashMLA backend on V1 engine.vailable at each node.\nINFO 03-25 08:32:16 [ray_distributed_executor.py:176] use_ray_spmd_worker: True\n\n...\n\n[[36m(RayWorkerWrapper pid=2300)^[[0m ^MLoading safetensors checkpoint shards: 100% Completed | 163/163 [00:34<00:00,  4.72it/s]\n^[[36m(RayWorkerWrapper pid=2300)^[[0m\n^[[36m(RayWorkerWrapper pid=10522, ip=xxx)^[[0m WARNING 03-25 08:33:08 [utils.py:574] Overwriting environment variable LD_LIBRARY_PATH from '/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64' to '/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64'\n^[[36m(pid=10529, ip=xxx)^[[0m INFO 03-25 08:32:27 [__init__.py:256] Automatically detected platform cuda.^[[32m [repeated 7x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=3178)^[[0m WARNING 03-25 08:33:13 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f79ddcb7d10>\n^[[36m(RayWorkerWrapper pid=3178)^[[0m WARNING 03-25 08:33:08 [utils.py:574] Overwriting environment variable LD_LIBRARY_PATH from '/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64' to '/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64'^[[32m [repeated 15x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=10526, ip=xxx)^[[0m WARNING 03-25 08:33:34 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fb2c5f5f470>^[[32m [repeated 8x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=2300)^[[0m INFO 03-25 08:33:36 [utils.py:925] Found nccl from library libnccl.so.2\n^[[36m(RayWorkerWrapper pid=2300)^[[0m INFO 03-25 08:33:36 [pynccl.py:69] vLLM is using nccl==2.21.5\n^[[36m(RayWorkerWrapper pid=10522, ip=xxx)^[[0m WARNING 03-25 08:33:40 [custom_all_reduce.py:84] Custom allreduce is disabled because this process group spans across nodes.\n^[[36m(RayWorkerWrapper pid=10527, ip=xxx)^[[0m WARNING 03-25 08:33:34 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f6f76ce2030>^[[32m [repeated 7x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=2300)^[[0m INFO 03-25 08:33:40 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_3068bba3'), local_subscribe_addr='ipc:///tmp/6d3dc210-f056-431c-b370-1f520ed62f33', remote_subscribe_addr='tcp://xxx:33595', remote_addr_ipv6=False)\n^[[36m(RayWorkerWrapper pid=2300)^[[0m INFO 03-25 08:33:40 [parallel_state.py:967] rank 0 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 0\n^[[36m(RayWorkerWrapper pid=2300)^[[0m INFO 03-25 08:33:40 [cuda.py:206] Using FlashMLA backend on V1 engine.\n[[36m(RayWorkerWrapper pid=2300)^[[0m INFO 03-25 08:33:40 [gpu_model_runner.py:1128] Starting to load model /mnt/huggingface_hub_137_llm/hub/models--deepseek-ai--DeepSeek-V3/snapshots/86518964eaef84e3fdd98e9861759a1384f9c29d...\n^[[36m(RayWorkerWrapper pid=2300)^[[0m WARNING 03-25 08:33:40 [utils.py:169] The model class DeepseekV3ForCausalLM has not defined `packed_modules_mapping`, this may lead to incorrect mapping of quantized or ignored modules\n^[[36m(RayWorkerWrapper pid=2300)^[[0m INFO 03-25 08:33:41 [topk_topp_sampler.py:53] Using FlashInfer for top-p & top-k sampling.\n^[[36m(RayWorkerWrapper pid=10529, ip=xxx)^[[0m INFO 03-25 08:33:36 [utils.py:925] Found nccl from library libnccl.so.2^[[32m [repeated 15x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=10529, ip=xxx)^[[0m INFO 03-25 08:33:36 [pynccl.py:69] vLLM is using nccl==2.21.5^[[32m [repeated 15x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=2300)^[[0m INFO 03-25 08:34:15 [loader.py:429] Loading weights took 34.55 seconds\n^[[36m(RayWorkerWrapper pid=3178)^[[0m WARNING 03-25 08:33:40 [custom_all_reduce.py:84] Custom allreduce is disabled because this process group spans across nodes.^[[32m [repeated 15x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=10529, ip=xxx)^[[0m INFO 03-25 08:33:40 [parallel_state.py:967] rank 15 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 15^[[32m [repeated 15x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=10529, ip=xxx)^[[0m INFO 03-25 08:33:40 [cuda.py:206] Using FlashMLA backend on V1 engine.^[[32m [repeated 15x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=10529, ip=xxx)^[[0m INFO 03-25 08:33:40 [gpu_model_runner.py:1128] Starting to load model /mnt/huggingface_hub_137_llm/hub/models--deepseek-ai--DeepSeek-V3/snapshots/86518964eaef84e3fdd98e9861759a1384f9c29d...^[[32m [repeated 15x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=10529, ip=xxx)^[[0m WARNING 03-25 08:33:40 [utils.py:169] The model class DeepseekV3ForCausalLM has not defined `packed_modules_mapping`, this may lead to incorrect mapping of quantized or ignored modules^[[32m [repeated 15x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=10529, ip=xxx)^[[0m INFO 03-25 08:33:41 [topk_topp_sampler.py:53] Using FlashInfer for top-p & top-k sampling.^[[32m [repeated 15x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=2300)^[[0m INFO 03-25 08:34:16 [gpu_model_runner.py:1140] Model loading took 40.3172 GB and 35.573925 seconds\n^[[36m(RayWorkerWrapper pid=1023)^[[0m INFO 03-25 08:34:23 [loader.py:429] Loading weights took 41.61 seconds^[[32m [repeated 12x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=10527, ip=xxx)^[[0m INFO 03-25 08:34:20 [gpu_model_runner.py:1140] Model loading took 40.3172 GB and 38.866216 seconds^[[32m [repeated 11x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=10524, ip=xxx)^[[0m INFO 03-25 08:34:36 [backends.py:409] Using cache directory: /root/.cache/vllm/torch_compile_cache/58e26f075c/rank_10_0 for vLLM's torch.compile\n^[[36m(RayWorkerWrapper pid=10524, ip=xxx)^[[0m INFO 03-25 08:34:36 [backends.py:419] Dynamo bytecode transform time: 12.69 s\n^[[36m(RayWorkerWrapper pid=3178)^[[0m INFO 03-25 08:34:23 [loader.py:429] Loading weights took 41.61 seconds^[[32m [repeated 3x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=3178)^[[0m INFO 03-25 08:34:23 [gpu_model_runner.^[[36m(RayWorkerWrapper pid=3178)^[[0m INFO 03-25 08:34:23 [loader.py:429] Loading weights took 41.61 seconds^[[32m [repeated 3x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=3178)^[[0m INFO 03-25 08:34:23 [gpu_model_runner.py:1140] Model loading took 40.3172 GB and 42.642502 seconds^[[32m [repeated 4x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=10524, ip=xxx)^[[0m INFO 03-25 08:34:37 [backends.py:115] Directly load the compiled graph for shape None from the cache\n^[[36m(RayWorkerWrapper pid=10522, ip=xxx)^[[0m INFO 03-25 08:34:49 [fp8_utils.py:422] Using configuration from /opt/venv/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json for W8A8 Block FP8 kernel.\n^[[36m(RayWorkerWrapper pid=2715)^[[0m INFO 03-25 08:34:36 [backends.py:409] Using cache directory: /root/.cache/vllm/torch_compile_cache/58e26f075c/rank_4_0 for vLLM's torch.compile^[[32m [repeated 15x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=2715)^[[0m INFO 03-25 08:34:36 [backends.py:419] Dynamo bytecode transform time: 13.13 s^[[32m [repeated 15x across cluster]^[[0m\n^[[36m(RayWorkerWrapper pid=2715)^[[0m INFO 03-25 08:34:37 [backends.py:115] Directly load the compiled graph for shape None from the cache^[[32m [repeated 15x across cluster]^[[0m\nERROR 03-25 08:34:50 [core.py:340] EngineCore hit an exception: Traceback (most recent call last):^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 332, in run_engine_core^M\nERROR 03-25 08:34:50 [core.py:340]     engine_core = EngineCoreProc(*args, **kwargs)^M\nERROR 03-25 08:34:50 [core.py:340]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 287, in __init__^M\nERROR 03-25 08:34:50 [core.py:340]     super().__init__(vllm_config, executor_class, log_stats)^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 62, in __init__^M\nERROR 03-25 08:34:50 [core.py:340]     num_gpu_blocks, num_cpu_blocks = self._initialize_kv_caches(^M\nERROR 03-25 08:34:50 [core.py:340]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 121, in _initialize_kv_caches^M\nERROR 03-25 08:34:50 [core.py:340]     available_gpu_memory = self.model_executor.determine_available_memory()^M\nERROR 03-25 08:34:50 [core.py:340]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 66, in determine_available_memory^M\nERROR 03-25 08:34:50 [core.py:340]     output = self.collective_rpc(\"determine_available_memory\")^M\nERROR 03-25 08:34:50 [core.py:340]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 316, in collective_rpc^M\nERROR 03-25 08:34:50 [core.py:340]     return self._run_workers(method, *args, **(kwargs or {}))^M\nERROR 03-25 08:34:50 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/executor/ray_distributed_executor.py\", line 519, in _run_workers^M\nERROR 03-25 08:34:50 [core.py:340]     ray_worker_outputs = ray.get(ray_worker_outputs)^M\nERROR 03-25 08:34:50 [core.py:340]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper^M\nERROR 03-25 08:34:50 [core.py:340]     return fn(*args, **kwargs)^M\nERROR 03-25 08:34:50 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper^M\nERROR 03-25 08:34:50 [core.py:340]     return func(*args, **kwargs)^M\nERROR 03-25 08:34:50 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2771, in get^M\nERROR 03-25 08:34:50 [core.py:340]     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)^M\nERROR 03-25 08:34:50 [core.py:340]py:1140] Model loading took 40.3172 GB and 42.642502 seconds^[[32m [repeated 4x across cluster]^[[0m\n\nobject_refs, timeout=timeout)^M\nERROR 03-25 08:34:50 [core.py:340]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 919, in get_objects^M\nERROR 03-25 08:34:50 [core.py:340]     raise value.as_instanceof_cause()^M\nERROR 03-25 08:34:50 [core.py:340] ray.exceptions.RayTaskError(ValueError): ^[[36mray::RayWorkerWrapper.execute_method()^[[39m (pid=10529, ip=xxx, actor_id=81ed90595c322810740eb92907000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x7f93b176e990>)^M\nERROR 03-25 08:34:50 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/worker/worker_base.py\", line 621, in execute_method^M\nERROR 03-25 08:34:50 [core.py:340]     raise e^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/worker/worker_base.py\", line 612, in execute_method^M\nERROR 03-25 08:34:50 [core.py:340]     return run_method(self, method, args, kwargs)^M\nERROR 03-25 08:34:50 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/utils.py\", line 2216, in run_method^M\nERROR 03-25 08:34:50 [core.py:340]     return func(*args, **kwargs)^M\nERROR 03-25 08:34:50 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context^M\nERROR 03-25 08:34:50 [core.py:340]     return func(*args, **kwargs)^M\nERROR 03-25 08:34:50 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 157, in determine_available_memory^M\nERROR 03-25 08:34:50 [core.py:340]     self.model_runner.profile_run()^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1401, in profile_run^M\nERROR 03-25 08:34:50 [core.py:340]     hidden_states = self._dummy_run(self.max_num_tokens)^M\nERROR 03-25 08:34:50 [core.py:340]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context^M\nERROR 03-25 08:34:50 [core.py:340]     return func(*args, **kwargs)^M\nERROR 03-25 08:34:50 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1265, in _dummy_run^M\nERROR 03-25 08:34:50 [core.py:340]     hidden_states = model(^M\nERROR 03-25 08:34:50 [core.py:340]                     ^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl^M\nERROR 03-25 08:34:50 [core.py:340]     return self._call_impl(*args, **kwargs)^M\nERROR 03-25 08:34:50 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl^M\nERROR 03-25 08:34:50 [core.py:340]     return forward_call(*args, **kwargs)^M\nERROR 03-25 08:34:50 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 688, in forward^M\nERROR 03-25 08:34:50 [core.py:340]     hidden_states = self.model(input_ids, positions, intermediate_tensors,^M\nERROR 03-25 08:34:50 [core.py:340]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 238, in __call__^M\nERROR 03-25 08:34:50 [core.py:340]     raise e^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 387, in __call__^M\nERROR 03-25 08:34:50 [core.py:340]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]^M\nERROR 03-25 08:34:50 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl^M\nERROR 03-25 08:34:50 [core.py:340]     return self._call_impl(*args, **kwargs)^M\nERROR 03-25 08:34:50 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl^M\nERROR 03-25 08:34:50 [core.py:340]     return forward_call(*args, **kwargs)^M\nERROR 03-25 08:34:50 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"<eval_with_key>.124\", line 1116, in forward^M\nERROR 03-25 08:34:50 [core.py:340]     submod_8 = self.submod_8(getitem_18, s0, getitem_19, l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_3_modules_mlp_modules_shared_experts_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_mlp_modules_shared_experts_modules_gate_up_proj_parameters_weight_scale_inv_, l_self_modules_layers_modules_3_modules_mlp_modules_shared_experts_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_mlp_modules_shared_experts_modules_down_proj_parameters_weight_scale_inv_, l_self_modules_layers_modules_3_modules_mlp_modules_gate_parameters_weight_, l_self_modules_layers_modules_3_modules_mlp_modules_experts_parameters_e_score_correction_bias_, l_self_modules_layers_modules_3_modules_mlp_modules_experts_parameters_w13_weight_, l_self_modules_layers_modules_3_modules_mlp_modules_experts_parameters_w2_weight_, l_self_modules_layers_modules_3_modules_mlp_modules_experts_parameters_w13_weight_scale_inv_, l_self_modules_layers_modules_3_modules_mlp_modules_experts_parameters_w2_weight_scale_inv_, l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_4_modules_self_attn_modules_q_a_proj_parameters_weight_, l_self_modules_layers_modules_4_modules_self_attn_modules_q_a_proj_parameters_weight_scale_inv_, l_self_modules_layers_modules_4_modules_self_attn_modules_q_a_layernorm_parameters_weight_, l_self_modules_layers_modules_4_modules_self_attn_modules_kv_a_proj_with_mqa_parameters_weight_, l_self_modules_layers_modules_4_modules_self_attn_modules_kv_a_proj_with_mqa_parameters_weight_scale_inv_, l_self_modules_layers_modules_4_modules_self_attn_modules_kv_a_layernorm_parameters_weight_);  getitem_18 = getitem_19 = l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_3_modules_mlp_modules_shared_experts_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_mlp_modules_shared_experts_modules_gate_up_proj_parameters_weight_scale_inv_ = l_self_modules_layers_modules_3_modules_mlp_modules_shared_experts_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_mlp_modules_shared_experts_modules_down_proj_parameters_weight_scale_inv_ = l_self_modules_layers_modules_3_modules_mlp_modules_gate_parameters_weight_ = l_self_modules_layers_modules_3_modules_mlp_modules_experts_parameters_e_score_correction_bias_ = l_self_modules_layers_modules_3_modules_mlp_modules_experts_parameters_w13_weight_ = l_self_modules_layers_modules_3_modules_mlp_modules_experts_parameters_w2_weight_ = l_self_modules_layers_modules_3_modules_mlp_modules_experts_parameters_w13_weight_scale_inv_ = l_self_modules_layers_modules_3_modules_mlp_modules_experts_parameters_w2_weight_scale_inv_ = l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_4_modules_self_attn_modules_q_a_proj_parameters_weight_ = l_self_modules_layers_modules_4_modules_self_attn_modules_q_a_proj_parameters_weight_scale_inv_ = l_self_modules_layers_modules_4_modules_self_attn_modules_q_a_layernorm_parameters_weight_ = l_self_modules_layers_modules_4_modules_self_attn_modules_kv_a_proj_with_mqa_parameters_weight_ = l_self_modules_layers_modules_4_modules_self_attn_modules_kv_a_proj_with_mqa_parameters_weight_scale_inv_ = l_self_modules_layers_modules_4_modules_self_attn_modules_kv_a_layernorm_parameters_weight_ = None^M\nERROR 03-25 08:34:50 [core.py:340]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/compilation/backends.py\", line 601, in __call__^M\nERROR 03-25 08:34:50 [core.py:340]     return self.compiled_graph_for_general_shape(*args)^M\nERROR 03-25 08:34:50 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py\", line 331, in compiled_graph^M\nERROR 03-25 08:34:50 [core.py:340]     graph_output = inductor_compiled_graph(list_args)^M\nERROR 03-25 08:34:50 [core.py:340]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/torch/_inductor/output_code.py\", line 466, in __call__^M\nERROR 03-25 08:34:50 [core.py:340]     return self.current_callable(inputs)^M\nERROR 03-25 08:34:50 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340]   File \"/root/.cache/vllm/torch_compile_cache/58e26f075c/rank_15_0/inductor_cache/ep/cepq2zs3ie4utwixtq2lbat3fqtszdwjyplv4q7emc3uv7geew7r.py\", line 735, in call^M\nERROR 03-25 08:34:50 [core.py:340]     arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1, arg12_1, arg13_1, arg14_1, arg15_1, arg16_1, arg17_1, arg18_1, arg19_1, arg20_1, arg21_1 = args^M\nERROR 03-25 08:34:50 [core.py:340]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^M\nERROR 03-25 08:34:50 [core.py:340] ValueError: not enough values to unpack (expected 22, got 21)^M\nERROR 03-25 08:34:50 [core.py:340]\nCRITICAL 03-25 08:34:50 [core_client.py:269] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-25T09:12:15+00:00",
    "closed_at": "2025-03-26T09:33:45+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15453/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15453"
  },
  {
    "number": 17127,
    "title": "[Feature]: Support for image linebreak tokens for vision model",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently when GPUModelRunner computes start and end indices https://github.com/vllm-project/vllm/blob/main/vllm/v1/worker/gpu_model_runner.py#L931-L934\n\nit assumes the relative position of placeholder tokens is the same as `encoder_output`, however if we use image linebreak tokens this assumption no longer holds, which will lead to errors when chunked prefill is enabled.\n\nFor example, say an input image is split into 3 crops and each crop corresponds to 4 placeholder tokens, in our use case we add an <|im_linbreak|> token to separate each crop, the full image tokens would be\n```\n<|im_start|>\n<|im_placeholder|><|im_placeholder|><|im_placeholder|><|im_placeholder|><|im_linebreak|>\n<|im_placeholder|><|im_placeholder|><|im_placeholder|><|im_placeholder|><|im_linebreak|>\n<|im_placeholder|><|im_placeholder|><|im_placeholder|><|im_placeholder|><|im_linebreak|>\n<|im_end|>\n```\nthe number of tokens between <|im_start|> and <|im_end|> is 15, however the length of `encoder_output` would still be 12, so in this case the start and end idx calculation needs to be readjusted to take into account the <|im_linebreak|> tokens.\n\n\nMy current workaround is\n1. include number of linebreak tokens when calculating mm_positions.length from the model\n2. adjust start and end indices in `_gather_mm_embeddings` to align the index with `encoder_output`\n\nwould like to know your thoughts on this, thanks.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-04-24T17:19:10+00:00",
    "closed_at": "2025-04-28T20:37:05+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17127/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17127"
  },
  {
    "number": 15041,
    "title": "[Bug]: V1 Engine crashes when sending requests with same request id",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.31.6\nLibc version: glibc-2.35\n\nPython version: 3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 17:29:18) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-133-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.3.107\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090\nNvidia driver version: 550.54.14\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               128\nOn-line CPU(s) list:                  0-127\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Gold 6430\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   32\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          3400.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4200.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            3 MiB (64 instances)\nL1i cache:                            2 MiB (64 instances)\nL2 cache:                             128 MiB (64 instances)\nL3 cache:                             120 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126\nNUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.3.0\n[pip3] sentence-transformers==3.2.1\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.48.2\n[pip3] transformers-stream-generator==0.0.5\n[pip3] triton==3.2.0\n[pip3] tritonclient==2.51.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-ml-py              12.570.86                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.3.0                   pypi_0    pypi\n[conda] sentence-transformers     3.2.1                    pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] transformers              4.48.2                   pypi_0    pypi\n[conda] transformers-stream-generator 0.0.5                    pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\n[conda] tritonclient              2.51.0                   pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.1.dev5239+g3b45714.d20250318 (git sha: 3b45714.d20250318\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tNIC0\tNIC1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tSYS\tSYS\t0,2,4,6,8,10\t0\t\tN/A\nNIC0\tSYS\t X \tPIX\nNIC1\tSYS\tPIX\t X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n\nCUDA_HOME=/usr/local/cuda\nCUDA_HOME=/usr/local/cuda\nLD_LIBRARY_PATH=/usr/local/cuda/lib64:\nVLLM_USE_MODELSCOPE=true\nVLLM_USE_V1=1\nVLLM_WORKER_MULTIPROC_METHOD=spawn\nTORCHINDUCTOR_CACHE_DIR=/tmp/torchinductor_sunjh\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nCaller to the OpenAI Server can set request_id arbitarily. If sending requests with duplicate request_id to OpenAI Server, the V1 Engine will crash immediately.\n\nBelow is an example how to trigger this bug.\n```python\nfrom openai import OpenAI\nopenai_api_key = \"EMPTY\"\nopenai_base_url = \"http://localhost:8800/v1\"\n\nif __name__ == \"__main__\":\n    client = OpenAI(api_key=openai_api_key, base_url=openai_base_url)\n\n    prompt = \"Write a one-sentence bedtime story about a unicorn.\"\n    request_id = \"test\"\n\n    model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n\n    completion = client.completions.create(\n        model=model,\n        prompt=prompt,\n        max_tokens=10,\n        extra_headers={\n            \"X-Request-Id\": request_id\n        },\n    )\n\n    completion = client.completions.create(\n        model=model,\n        prompt=prompt,\n        max_tokens=10,\n        extra_headers={\n            \"X-Request-Id\": request_id\n        },\n    )\n\n```\n\nvllm reports the following error.\n```text\nINFO:     Started server process [2095823]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO 03-18 14:46:40 [logger.py:39] Received request cmpl-test-0: prompt: 'Write a one-sentence bedtime story about a unicorn.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=0.95, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [151646, 7985, 264, 825, 1331, 18380, 88507, 3364, 911, 264, 81830, 13], lora_request: None, prompt_adapter_request: None.\nINFO 03-18 14:46:40 [async_llm.py:204] Added request cmpl-test-0.\nINFO:     127.0.0.1:56084 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO 03-18 14:46:40 [logger.py:39] Received request cmpl-test-0: prompt: 'Write a one-sentence bedtime story about a unicorn.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=0.95, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [151646, 7985, 264, 825, 1331, 18380, 88507, 3364, 911, 264, 81830, 13], lora_request: None, prompt_adapter_request: None.\nINFO 03-18 14:46:40 [async_llm.py:204] Added request cmpl-test-0.\nERROR 03-18 14:46:40 [core.py:341] EngineCore hit an exception: Traceback (most recent call last):\nERROR 03-18 14:46:40 [core.py:341]   File \"/home/sunjh/vllm/vllm/v1/engine/core.py\", line 334, in run_engine_core\nERROR 03-18 14:46:40 [core.py:341]     engine_core.run_busy_loop()\nERROR 03-18 14:46:40 [core.py:341]   File \"/home/sunjh/vllm/vllm/v1/engine/core.py\", line 368, in run_busy_loop\nERROR 03-18 14:46:40 [core.py:341]     outputs = step_fn()\nERROR 03-18 14:46:40 [core.py:341]               ^^^^^^^^^\nERROR 03-18 14:46:40 [core.py:341]   File \"/home/sunjh/vllm/vllm/v1/engine/core.py\", line 193, in step\nERROR 03-18 14:46:40 [core.py:341]     output = self.model_executor.execute_model(scheduler_output)\nERROR 03-18 14:46:40 [core.py:341]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-18 14:46:40 [core.py:341]   File \"/home/sunjh/vllm/vllm/v1/executor/abstract.py\", line 80, in execute_model\nERROR 03-18 14:46:40 [core.py:341]     output = self.collective_rpc(\"execute_model\",\nERROR 03-18 14:46:40 [core.py:341]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-18 14:46:40 [core.py:341]   File \"/home/sunjh/vllm/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\nERROR 03-18 14:46:40 [core.py:341]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 03-18 14:46:40 [core.py:341]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-18 14:46:40 [core.py:341]   File \"/home/sunjh/vllm/vllm/utils.py\", line 2216, in run_method\nERROR 03-18 14:46:40 [core.py:341]     return func(*args, **kwargs)\nERROR 03-18 14:46:40 [core.py:341]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 03-18 14:46:40 [core.py:341]   File \"/home/sunjh/miniconda3/envs/vllm-dev/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 03-18 14:46:40 [core.py:341]     return func(*args, **kwargs)\nERROR 03-18 14:46:40 [core.py:341]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 03-18 14:46:40 [core.py:341]   File \"/home/sunjh/vllm/vllm/v1/worker/gpu_worker.py\", line 242, in execute_model\nERROR 03-18 14:46:40 [core.py:341]     output = self.model_runner.execute_model(scheduler_output)\nERROR 03-18 14:46:40 [core.py:341]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-18 14:46:40 [core.py:341]   File \"/home/sunjh/miniconda3/envs/vllm-dev/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 03-18 14:46:40 [core.py:341]     return func(*args, **kwargs)\nERROR 03-18 14:46:40 [core.py:341]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 03-18 14:46:40 [core.py:341]   File \"/home/sunjh/vllm/vllm/v1/worker/gpu_model_runner.py\", line 934, in execute_model\nERROR 03-18 14:46:40 [core.py:341]     attn_metadata, logits_indices = self._prepare_inputs(scheduler_output)\nERROR 03-18 14:46:40 [core.py:341]                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-18 14:46:40 [core.py:341]   File \"/home/sunjh/vllm/vllm/v1/worker/gpu_model_runner.py\", line 479, in _prepare_inputs\nERROR 03-18 14:46:40 [core.py:341]     num_scheduled_tokens[i] = num_tokens\nERROR 03-18 14:46:40 [core.py:341]     ~~~~~~~~~~~~~~~~~~~~^^^\nERROR 03-18 14:46:40 [core.py:341] IndexError: index 1 is out of bounds for axis 0 with size 1\nERROR 03-18 14:46:40 [core.py:341]\nCRITICAL 03-18 14:46:40 [core_client.py:269] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n[1]    2095823 killed     vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --port 8800\n```\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-18T14:49:35+00:00",
    "closed_at": "2025-03-20T17:01:03+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15041/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15041"
  },
  {
    "number": 7339,
    "title": "[Feature]: Small Model Large Latency Compared to SGLang and TensorRT-LLM",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nIn this post, https://lmsys.org/blog/2024-07-25-sglang-llama3/, it looks like vllm is not efficient in small model size in both online and offline benchmark. What is the bottleneck for vllm for small model inference and whether this will be addressed to catch SGLang and TensorRT performance.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-08-09T05:53:18+00:00",
    "closed_at": "2024-08-11T22:53:24+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7339/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7339"
  },
  {
    "number": 4497,
    "title": "[Usage]: How to modify the default system prompt\uff1f",
    "body": "### Your current environment\r\n\r\n```text\r\nvllm-0.4.0\r\n```\r\n\r\n\r\n\r\n### How would you like to use vllm\r\n\r\nWhen I deploy an OpenAI-Compatible Server using `python -m vllm.entrypoints.openai.api_server`, how do I modify the default system prompt(You are a helpful assistant.)?\r\n\r\n",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-04-30T09:23:20+00:00",
    "closed_at": "2024-05-07T05:13:08+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4497/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4497"
  },
  {
    "number": 7375,
    "title": "[Usage]: Getting empty text using llm.generate of mixtral-8X7b-Instruct AWQ model",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### How would you like to use vllm\n\nI want to run inference of a Mixtral-8x7B-Instruct-v0.1-AWQ . Its giving me empty text as generation.\r\nI am using the sample code from the model card:?\r\nfrom vllm import LLM, SamplingParams\r\nprompts = [\r\n   \"Tell me about AI\",\r\n   \"Write a story about llamas\",\r\n   \"What is 291 - 150?\",\r\n   \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\",\r\n]\r\nprompt_template=f'''[INST] {prompt} [/INST]'''\r\n\r\nprompts = [prompt_template.format(prompt=prompt) for prompt in prompts]\r\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\r\nllm = LLM(model=\"TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ\", quantization=\"awq\", tensor_parallel_size=4)\r\noutputs = llm.generate(prompts, sampling_params)\r\n# Print the outputs.\r\nfor output in outputs:\r\n    prompt = output.prompt\r\n    generated_text = output.outputs[0].text\r\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\r\n\r\n\r\nThe output is always empty.\r\nPrompt: '[INST] Tell me about AI[/INST]', Generated text: ''\r\n\r\nIts working as expected for other models. Is there something i am missing while using this?",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-08-09T18:28:26+00:00",
    "closed_at": "2024-08-11T23:38:29+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7375/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7375"
  },
  {
    "number": 2353,
    "title": "snapshot download from HF not from modelscope",
    "body": "since VLLM uses snapshot download from modelscope will not allow us to download private models , if it is implemented with snapshot download from HF it would be highly appreciable ",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-05T13:09:56+00:00",
    "closed_at": "2024-01-06T18:24:12+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2353/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2353"
  },
  {
    "number": 15199,
    "title": "[Bug]: Can't deserialize object reported by ray, H800*16 DeepSeek R1",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.31.6\nLibc version: glibc-2.35\n\nPython version: 3.12.9 (main, Mar 17 2025, 21:01:58) [Clang 20.1.0 ] (64-bit runtime)\nPython platform: Linux-5.15.0-47-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: NVIDIA H800\nGPU 1: NVIDIA H800\nGPU 2: NVIDIA H800\nGPU 3: NVIDIA H800\nGPU 4: NVIDIA H800\nGPU 5: NVIDIA H800\nGPU 6: NVIDIA H800\nGPU 7: NVIDIA H800\n\nNvidia driver version: 535.161.08\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   52 bits physical, 57 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          192\nOn-line CPU(s) list:             0-191\nVendor ID:                       GenuineIntel\nModel name:                      Intel(R) Xeon(R) Platinum 8463B\nCPU family:                      6\nModel:                           143\nThread(s) per core:              2\nCore(s) per socket:              48\nSocket(s):                       2\nStepping:                        8\nFrequency boost:                 enabled\nCPU max MHz:                     2601.0000\nCPU min MHz:                     800.0000\nBogoMIPS:                        5200.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                  VT-x\nL1d cache:                       4.5 MiB (96 instances)\nL1i cache:                       3 MiB (96 instances)\nL2 cache:                        192 MiB (96 instances)\nL3 cache:                        210 MiB (2 instances)\nNUMA node(s):                    2\nNUMA node0 CPU(s):               0-47,96-143\nNUMA node1 CPU(s):               48-95,144-191\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Not affected\nVulnerability Retbleed:          Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.1.post2+cu124torch2.6\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.49.0\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tNIC4\tNIC5\tNIC6\tNIC7\tNIC8\tNIC9\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tNV8\tNV8\tNV8\tNV8\tNV8\tNV8\tNV8\tNODE\tNODE\tPIX\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\nGPU1\tNV8\t X \tNV8\tNV8\tNV8\tNV8\tNV8\tNV8\tNODE\tNODE\tNODE\tPIX\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\nGPU2\tNV8\tNV8\t X \tNV8\tNV8\tNV8\tNV8\tNV8\tNODE\tNODE\tNODE\tNODE\tPIX\tNODE\tSYS\tSYS\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\nGPU3\tNV8\tNV8\tNV8\t X \tNV8\tNV8\tNV8\tNV8\tNODE\tNODE\tNODE\tNODE\tNODE\tPIX\tSYS\tSYS\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\nGPU4\tNV8\tNV8\tNV8\tNV8\t X \tNV8\tNV8\tNV8\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPIX\tNODE\tNODE\tNODE\t48-95,144-191\t1\t\tN/A\nGPU5\tNV8\tNV8\tNV8\tNV8\tNV8\t X \tNV8\tNV8\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tPIX\tNODE\tNODE\t48-95,144-191\t1\t\tN/A\nGPU6\tNV8\tNV8\tNV8\tNV8\tNV8\tNV8\t X \tNV8\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPIX\tNODE\t48-95,144-191\t1\t\tN/A\nGPU7\tNV8\tNV8\tNV8\tNV8\tNV8\tNV8\tNV8\t X \tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tPIX\t48-95,144-191\t1\t\tN/A\nNIC0\tNODE\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t X \tPIX\tNODE\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\nNIC1\tNODE\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tPIX\t X \tNODE\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\nNIC2\tPIX\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\t X \tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\nNIC3\tNODE\tPIX\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\t X \tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\nNIC4\tNODE\tNODE\tPIX\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t X \tNODE\tSYS\tSYS\tSYS\tSYS\nNIC5\tNODE\tNODE\tNODE\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tNODE\t X \tSYS\tSYS\tSYS\tSYS\nNIC6\tSYS\tSYS\tSYS\tSYS\tPIX\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tNODE\tNODE\tNODE\nNIC7\tSYS\tSYS\tSYS\tSYS\tNODE\tPIX\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\t X \tNODE\tNODE\nNIC8\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPIX\tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\t X \tNODE\nNIC9\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tPIX\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\t X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n\nNVIDIA_VISIBLE_DEVICES=all\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\nNCCL_VERSION=2.20.5-1\nNCCL_SOCKET_IFNAME=eth0\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNCCL_DEBUG=INFO\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nCUDA_VERSION=12.4.0\nLD_LIBRARY_PATH=/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nRay status is ready, start serving...\n+ vllm serve /model --tensor-parallel-size 8 --pipeline-parallel-size 2 --trust-remote-code --gpu-memory-utilization 0.92 --max-model-len 98304 --host 0.0.0.0 --port 8102 --served-model-name DeepSeek-R1 --uvicorn-log-level info\nINFO 03-20 08:10:14 [__init__.py:256] Automatically detected platform cuda.\nINFO 03-20 08:10:18 [api_server.py:977] vLLM API server version 0.8.1\nINFO 03-20 08:10:18 [api_server.py:978] args: Namespace(subparser='serve', model_tag='/model', config='', host='0.0.0.0', port=8102, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/model', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=98304, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=2, tensor_parallel_size=8, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.92, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['DeepSeek-R1'], qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x7f4d7621f7e0>)\nINFO 03-20 08:10:18 [config.py:208] Replacing legacy 'type' key with 'rope_type'\nINFO 03-20 08:10:25 [config.py:583] This model supports multiple tasks: {'reward', 'classify', 'generate', 'embed', 'score'}. Defaulting to 'generate'.\nWARNING 03-20 08:10:27 [arg_utils.py:1770] Detected VLLM_USE_V1=1 with PP. Usage should be considered experimental. Please report any issues on Github.\nINFO 03-20 08:10:28 [config.py:1515] Defaulting to use ray for distributed inference\nINFO 03-20 08:10:28 [config.py:1693] Chunked prefill is enabled with max_num_batched_tokens=2048.\nWARNING 03-20 08:10:28 [fp8.py:54] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\nINFO 03-20 08:10:28 [cuda.py:159] Forcing kv cache block size to 64 for FlashMLA backend.\nINFO 03-20 08:10:32 [__init__.py:256] Automatically detected platform cuda.\nINFO 03-20 08:10:35 [core.py:53] Initializing a V1 LLM engine (v0.8.1) with config: model='/model', speculative_config=None, tokenizer='/model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=98304, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=DeepSeek-R1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n2025-03-20 08:10:35,648\tINFO worker.py:1654 -- Connecting to existing Ray cluster at address: 10.78.154.175:8959...\n2025-03-20 08:10:35,661\tINFO worker.py:1841 -- Connected to Ray cluster.\nINFO 03-20 08:10:37 [ray_distributed_executor.py:176] use_ray_spmd_worker: True\n(pid=1169) INFO 03-20 08:10:41 [__init__.py:256] Automatically detected platform cuda.\n(pid=295, ip=10.78.154.177) INFO 03-20 08:10:46 [__init__.py:256] Automatically detected platform cuda. [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\nINFO 03-20 08:10:54 [ray_distributed_executor.py:350] non_carry_over_env_vars from config: set()\nINFO 03-20 08:10:54 [ray_distributed_executor.py:352] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_USAGE_SOURCE', 'VLLM_USE_RAY_SPMD_WORKER', 'VLLM_USE_RAY_COMPILED_DAG', 'VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USE_V1']\nINFO 03-20 08:10:54 [ray_distributed_executor.py:355] If certain env vars should NOT be copied to workers, add them to /root/.config/vllm/ray_non_carry_over_env_vars.json file\n(RayWorkerWrapper pid=295, ip=10.78.154.177) WARNING 03-20 08:10:54 [utils.py:574] Overwriting environment variable LD_LIBRARY_PATH from '/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64' to '/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64'\n(pid=297, ip=10.78.154.177) INFO 03-20 08:10:47 [__init__.py:256] Automatically detected platform cuda. [repeated 7x across cluster]\n\nINFO:     10.158.103.151:24634 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nINFO 03-20 08:20:03 [async_llm.py:204] Added request chatcmpl-0fddc187baab4611a157d86734a96460.\n2025-03-20 08:20:05,953\tERROR serialization.py:462 -- Can't deserialize object: ObjectRef(001e4aab298197f90e1a7caaae1e0127f42510750100000002e1f505), metadata: b'\\x01'\nTraceback (most recent call last):\n  File \"/opt/venv/lib/python3.12/site-packages/ray/_private/serialization.py\", line 331, in _deserialize_object\n    error_type = int(metadata_fields[0])\n                 ^^^^^^^^^^^^^^^^^^^^^^^\nValueError: invalid literal for int() with base 10: b'\\x01'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/venv/lib/python3.12/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n    obj = self._deserialize_object(data, metadata, object_ref)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/venv/lib/python3.12/site-packages/ray/_private/serialization.py\", line 333, in _deserialize_object\n    raise Exception(\nException: Can't deserialize object: ObjectRef(001e4aab298197f90e1a7caaae1e0127f42510750100000002e1f505), metadata: b'\\x01'\nINFO 03-20 08:20:11 [loggers.py:80] Avg prompt throughput: 94.6 tokens/s, Avg generation throughput: 252.7 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 37.2%\nERROR 03-20 08:20:15 [core.py:340] EngineCore hit an exception: Traceback (most recent call last):\nERROR 03-20 08:20:15 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/ray/dag/compiled_dag_node.py\", line 2344, in _execute_until\nERROR 03-20 08:20:15 [core.py:340]     result = self._dag_output_fetcher.read(timeout)\nERROR 03-20 08:20:15 [core.py:340]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-20 08:20:15 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/ray/experimental/channel/common.py\", line 318, in read\nERROR 03-20 08:20:15 [core.py:340]     outputs = self._read_list(timeout)\nERROR 03-20 08:20:15 [core.py:340]               ^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-20 08:20:15 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/ray/experimental/channel/common.py\", line 409, in _read_list\nERROR 03-20 08:20:15 [core.py:340]     raise e\nERROR 03-20 08:20:15 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/ray/experimental/channel/common.py\", line 391, in _read_list\nERROR 03-20 08:20:15 [core.py:340]     result = c.read(min(remaining_timeout, iteration_timeout))\nERROR 03-20 08:20:15 [core.py:340]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-20 08:20:15 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/ray/experimental/channel/shared_memory_channel.py\", line 776, in read\nERROR 03-20 08:20:15 [core.py:340]     return self._channel_dict[self._resolve_actor_id()].read(timeout)\nERROR 03-20 08:20:15 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-20 08:20:15 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/ray/experimental/channel/shared_memory_channel.py\", line 480, in read\nERROR 03-20 08:20:15 [core.py:340]     ret = self._worker.get_objects(\nERROR 03-20 08:20:15 [core.py:340]           ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-20 08:20:15 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 893, in get_objects\nERROR 03-20 08:20:15 [core.py:340]     ] = self.core_worker.get_objects(\nERROR 03-20 08:20:15 [core.py:340]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-20 08:20:15 [core.py:340]   File \"python/ray/_raylet.pyx\", line 3189, in ray._raylet.CoreWorker.get_objects\nERROR 03-20 08:20:15 [core.py:340]   File \"python/ray/includes/common.pxi\", line 106, in ray._raylet.check_status\nERROR 03-20 08:20:15 [core.py:340] ray.exceptions.RayChannelTimeoutError: System error: Timed out waiting for object available to read. ObjectID: 001c7c55bebc69d00e1a7caaae1e0127f42510750100000002e1f505\nERROR 03-20 08:20:15 [core.py:340]\nERROR 03-20 08:20:15 [core.py:340] The above exception was the direct cause of the following exception:\nERROR 03-20 08:20:15 [core.py:340]\nERROR 03-20 08:20:15 [core.py:340] Traceback (most recent call last):\nERROR 03-20 08:20:15 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 333, in run_engine_core\nERROR 03-20 08:20:15 [core.py:340]     engine_core.run_busy_loop()\nERROR 03-20 08:20:15 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 367, in run_busy_loop\nERROR 03-20 08:20:15 [core.py:340]     outputs = step_fn()\nERROR 03-20 08:20:15 [core.py:340]               ^^^^^^^^^\nERROR 03-20 08:20:15 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 234, in step_with_batch_queue\nERROR 03-20 08:20:15 [core.py:340]     model_output = future.result()\nERROR 03-20 08:20:15 [core.py:340]                    ^^^^^^^^^^^^^^^\nERROR 03-20 08:20:15 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/ray_distributed_executor.py\", line 24, in result\nERROR 03-20 08:20:15 [core.py:340]     return self.ref.get()\nERROR 03-20 08:20:15 [core.py:340]            ^^^^^^^^^^^^^^\nERROR 03-20 08:20:15 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/ray/experimental/compiled_dag_ref.py\", line 124, in get\nERROR 03-20 08:20:15 [core.py:340]     self._dag._execute_until(\nERROR 03-20 08:20:15 [core.py:340]   File \"/opt/venv/lib/python3.12/site-packages/ray/dag/compiled_dag_node.py\", line 2350, in _execute_until\nERROR 03-20 08:20:15 [core.py:340]     raise RayChannelTimeoutError(\nERROR 03-20 08:20:15 [core.py:340] ray.exceptions.RayChannelTimeoutError: System error: If the execution is expected to take a long time, increase RAY_CGRAPH_get_timeout which is currently 10 seconds. Otherwise, this may indicate that the execution is hanging.\nERROR 03-20 08:20:15 [core.py:340]\nINFO 03-20 08:20:15 [ray_distributed_executor.py:127] Shutting down Ray distributed executor. If you see error log from logging.cc regarding SIGTERM received, please ignore because this is the expected termination process in Ray.\n2025-03-20 08:20:15,556\tINFO compiled_dag_node.py:2109 -- Tearing down compiled DAG\n2025-03-20 08:20:15,556\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 9c5d60dd967d3cc4db5708b501000000)\n2025-03-20 08:20:15,556\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, e71b777d5ccfd4b02c825a2c01000000)\n2025-03-20 08:20:15,556\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 5842ef64b2f7e33de64a7b1101000000)\n2025-03-20 08:20:15,556\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 32d9d5de80a4d6f72ad832c501000000)\n2025-03-20 08:20:15,556\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 6eb32feaef6c8ac136b474c101000000)\n2025-03-20 08:20:15,556\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 70283d217b917ef6d8c66bfb01000000)\n2025-03-20 08:20:15,556\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, ebd6b07c9723ec1fe02ec76401000000)\n2025-03-20 08:20:15,556\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 66b0b337d32ff8aa6672d39701000000)\n2025-03-20 08:20:15,556\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, a33f3ba4293fd17a8f56f7a201000000)\nCRITICAL 03-20 08:20:15 [core_client.py:269] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n2025-03-20 08:20:15,556\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 28ca4d3de50c04b44543ed7101000000)\n2025-03-20 08:20:15,556\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, e87f20a55340e9113cd6acc901000000)\n2025-03-20 08:20:15,556\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, fdbffa98ed2f82d46c35746401000000)\n2025-03-20 08:20:15,556\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, b40963929d7dc9fca54d774701000000)\n2025-03-20 08:20:15,556\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, a6629dc1201bdb9dd27f112001000000)\n2025-03-20 08:20:15,556\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 328ef8a91f9eb15cf9bbdc2801000000)\n2025-03-20 08:20:15,556\tINFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 5ab9f8901c0c36a4790a50ab01000000)\n(RayWorkerWrapper pid=1165) Destructing NCCL group on actor: Actor(RayWorkerWrapper, e87f20a55340e9113cd6acc901000000)\n/home/work/easyedge/llm/llm_bootstrap.sh: line 104:   599 Killed                  vllm serve ${LLMKIT_MODEL_DIR} --tensor-parallel-size ${gpu_count} --pipeline-parallel-size ${LLMKIT_NODE_NUM} --trust-remote-code --gpu-memory-utilization 0.92 ${docker_args} --host 0.0.0.0 --port ${LLMKIT_PARAMS_PORT} --served-model-name ${LLMKIT_MODEL_NAME} --uvicorn-log-level info\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "closed",
    "created_at": "2025-03-20T08:50:09+00:00",
    "closed_at": "2025-03-22T05:25:45+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15199/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15199"
  },
  {
    "number": 4588,
    "title": "[Usage]: Difference in language model usage post updating versions form 0.2 to 0.4 ",
    "body": "### Your current environment\r\n\r\n\r\n```text\r\nThe output of `python collect_env.py`\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.2.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Amazon Linux 2 (x86_64)\r\nGCC version: (GCC) 7.3.1 20180712 (Red Hat 7.3.1-17)\r\nClang version: Could not collect\r\nCMake version: version 3.27.7\r\nLibc version: glibc-2.26\r\n\r\nPython version: 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:20:04) [GCC 11.3.0] (64-bit runtime)\r\nPython platform: Linux-5.10.213-201.855.amzn2.x86_64-x86_64-with-glibc2.26\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A10G\r\nNvidia driver version: 550.54.14\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              4\r\nOn-line CPU(s) list: 0-3\r\nThread(s) per core:  2\r\nCore(s) per socket:  2\r\nSocket(s):           1\r\nNUMA node(s):        1\r\nVendor ID:           AuthenticAMD\r\nCPU family:          23\r\nModel:               49\r\nModel name:          AMD EPYC 7R32\r\nStepping:            0\r\nCPU MHz:             3293.001\r\nBogoMIPS:            5600.00\r\nHypervisor vendor:   KVM\r\nVirtualization type: full\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            512K\r\nL3 cache:            8192K\r\nNUMA node0 CPU(s):   0-3\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save rdpid\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.9.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.2\r\n[pip3] nvidia-nccl-cu11==2.14.3\r\n[pip3] nvidia-nccl-cu12==2.19.3\r\n[pip3] torch==2.2.1\r\n[pip3] triton==2.2.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.4.0\r\n[conda] numpy                     1.26.2                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu11          2.14.3                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.19.3                   pypi_0    pypi\r\n[conda] torch                     2.2.1                    pypi_0    pypi\r\n[conda] triton                    2.2.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.4.0             pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \t0-3\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n\r\n\r\n\r\n### How would you like to use vllm\r\n\r\n\r\n\r\nI am observing a change in the behavior of vLLM since updating the vLLm library from v ~2.0+ to the latest v 0.4.1 build.\r\n\r\nWhat are the changes?\r\n\r\n1. The same Mistral 7B that ran on both OpenAI API and \"traditional\" API doesn't function anymore `as is` in the new version that I upgraded to. For example for an A10 GPU the API now requires the `max-model-len` param to be set for every model because it can't load any 7B models.\r\nAPI requires a `max_model_len` param to be set which I assume is a part of the engine arguments and from the description is the context length for the model and is made compulsory so that solves it, but what I am unable to figure out is why doesn't it load the model with it's full length after updating?\r\n\r\nNow since I admit this jump from 0.2 to 0.4 is far from an ideal update when there are so many versions that were released in between them and I tried looking into the change logs but, couldn't discern what was going on.\r\nsince I get this as an error output.\r\n\r\n```\r\nValueError: The model's max seq len (32768) is larger than the maximum number of tokens that can be stored in KV cache (16864). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.\r\n``` \r\n\r\nCan I get help in understanding what changed?\r\n\r\nAlso I keep seeing a burst of these messages and not sure what the model is trying to do here\r\n```\r\nINFO:    xx:xx - \"GET / HTTP/1.1\" 404 Not Found\r\nINFO 05-03 18:07:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\r\nINFO 05-03 18:07:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\r\n```\r\n",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-05-03T18:18:16+00:00",
    "closed_at": "2024-05-10T01:07:50+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4588/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4588"
  },
  {
    "number": 16543,
    "title": "[Usage]: Free GPU memory upon destructing the Python client",
    "body": "### Your current environment\n\n```text\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 24.04.2 LTS (x86_64)\nGCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.39\n\nPython version: 3.12.7 (main, Apr  9 2025, 11:35:32) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-6.11.0-19-generic-x86_64-with-glibc2.39\nIs CUDA available: True\nCUDA runtime version: 12.8.93\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA L4\nGPU 1: NVIDIA L4\nGPU 2: NVIDIA L4\nGPU 3: NVIDIA L4\nGPU 4: NVIDIA L4\nGPU 5: NVIDIA L4\nGPU 6: NVIDIA L4\nGPU 7: NVIDIA L4\n\nNvidia driver version: 570.124.06\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               64\nOn-line CPU(s) list:                  0-63\nVendor ID:                            GenuineIntel\nModel name:                           INTEL(R) XEON(R) SILVER 4514Y\nCPU family:                           6\nModel:                                207\nThread(s) per core:                   2\nCore(s) per socket:                   16\nSocket(s):                            2\nStepping:                             2\nCPU(s) scaling MHz:                   27%\nCPU max MHz:                          3400.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req hfi vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualisation:                       VT-x\nL1d cache:                            1.5 MiB (32 instances)\nL1i cache:                            1 MiB (32 instances)\nL2 cache:                             64 MiB (32 instances)\nL3 cache:                             60 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62\nNUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==2.1.3\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] onnxruntime==1.21.0\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.2\n[pip3] triton==3.2.0\n[conda] No relevant packages\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    0,2,4,6,8,10    0               N/A\nGPU1    NODE     X      NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    0,2,4,6,8,10    0               N/A\nGPU2    NODE    NODE     X      NODE    SYS     SYS     SYS     SYS     NODE    NODE    0,2,4,6,8,10    0               N/A\nGPU3    NODE    NODE    NODE     X      SYS     SYS     SYS     SYS     NODE    NODE    0,2,4,6,8,10    0               N/A\nGPU4    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     1,3,5,7,9,11    1               N/A\nGPU5    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     SYS     1,3,5,7,9,11    1               N/A\nGPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS     SYS     1,3,5,7,9,11    1               N/A\nGPU7    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     1,3,5,7,9,11    1               N/A\nNIC0    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      PIX                             \nNIC1    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX      X                              \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n\n### How would you like to use vllm\n\nI run vLLM inference in my main process, i.e. with `VLLM_ENABLE_V1_MULTIPROCESSING=0`.\n\nAt some point of my code, I need to clear cuda memory to make room for another part of my script. Interestingly, if I simply destruct the `LLM` object of vLLM, the memory is not freed. This persists with an explicit gc.collect() call. I tried to get into the details of vLLM and simply `del` the correct object, but sadly I loose the stackframe once it switches to cuda code.\n\nAny hints to what objects I have to manually clear would be greatly appreciated. More generally though, I find it quite alarming that it is so difficult to clear the memory. I reckon this can't be such a niche use-case, so it might be worth it including some code for this in the destructor.\n\nA simple example, where I added some sleeps just to account for any funky GPU sync issues.\n```python3\nfrom time import sleep\nmodel = LLM(model=\"kfdong/STP_model_Lean_0320\", dtype='bfloat16', max_model_len=1024,\n                    gpu_memory_utilization=0.85)\nprint(\"Before delete\")\nsleep(30)\nmodel = None\n# TODO: Somehow delete the correct thing, the below does not suffice\ndel model.llm_engine.engine_core.engine_core.model_executor.driver_worker.model_runner.model\ngc.collect()\ntorch.cuda.empty_cache()\nprint(\"After delete\")\nsleep(30)\n# Memory still used\n```\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-04-12T22:45:50+00:00",
    "closed_at": "2025-04-16T09:12:35+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16543/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16543"
  },
  {
    "number": 3359,
    "title": "FlashAttentionBackend only supports head sizes supported by xformers",
    "body": "`FlashAttentionBackend` currently only supports head sizes supported by `XFormersBackend`, specifically `[64, 80, 96, 112, 128, 256]`. Is there any reason to only support these head sizes with flash attention? If not, I can open a PR to remove this constraint (flash should support all dimensions up to 256) so that smaller models or those with unsupported head sizes can be used with vLLM w/flash attention.\r\n\r\n```python\r\nsuppored_head_sizes = PagedAttentionImpl.get_supported_head_sizes()\r\nif head_size not in suppored_head_sizes:\r\n    raise ValueError(\r\n        f\"Head size {head_size} is not supported by PagedAttention. \"\r\n        f\"Supported head sizes are: {suppored_head_sizes}.\")\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-03-12T18:08:27+00:00",
    "closed_at": "2024-03-13T22:33:37+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3359/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3359"
  },
  {
    "number": 14596,
    "title": "[Bug]: Error while deserializing header: InvalidHeaderDeserialization",
    "body": "### Your current environment\n\nroot@node37:/disk1/qwen-2.5-vl-72b-in-awq-0226# docker compose -f docker-compose.yml down\n[+] Running 2/2\n \u2714 Container qwen-2.5-vl-72b-in-awq            Removed                                                                                                                                     2.0s \n \u2714 Network qwen-25-vl-72b-in-awq-0226_default  Removed                                                                                                                                     0.2s \nroot@node37:/disk1/qwen-2.5-vl-72b-in-awq-0226# docker compose -f docker-compose.yml up -d\n[+] Running 2/2\n \u2714 Network qwen-25-vl-72b-in-awq-0226_default  Created                                                                                                                                     0.1s \n \u2714 Container qwen-2.5-vl-72b-in-awq            Started                                                                                                                                     0.5s \nroot@node37:/disk1/qwen-2.5-vl-72b-in-awq-0226# docker logs -f qwen-2.5-vl-72b-in-awq \nINFO 03-10 21:30:35 __init__.py:207] Automatically detected platform cuda.\nINFO 03-10 21:30:35 api_server.py:912] vLLM API server version 0.7.3\nINFO 03-10 21:30:35 api_server.py:913] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='/models/qwen-2.5-vl-72b-in-awq-0226', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='float16', kv_cache_dtype='auto', max_model_len=32768, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=16, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['Qwen2.5-VL-72B-Instruct-AWQ'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)\nINFO 03-10 21:30:35 api_server.py:209] Started engine process with PID 45\nWARNING 03-10 21:30:35 config.py:2448] Casting torch.bfloat16 to torch.float16.\nINFO 03-10 21:30:42 __init__.py:207] Automatically detected platform cuda.\nWARNING 03-10 21:30:42 config.py:2448] Casting torch.bfloat16 to torch.float16.\nINFO 03-10 21:30:44 config.py:549] This model supports multiple tasks: {'generate', 'classify', 'score', 'reward', 'embed'}. Defaulting to 'generate'.\nINFO 03-10 21:30:46 awq_marlin.py:114] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\nINFO 03-10 21:30:49 config.py:549] This model supports multiple tasks: {'score', 'classify', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.\nINFO 03-10 21:30:51 awq_marlin.py:114] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\nINFO 03-10 21:30:51 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/models/qwen-2.5-vl-72b-in-awq-0226', speculative_config=None, tokenizer='/models/qwen-2.5-vl-72b-in-awq-0226', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen2.5-VL-72B-Instruct-AWQ, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[16,8,4,2,1],\"max_capture_size\":16}, use_cached_outputs=True, \nINFO 03-10 21:30:52 cuda.py:229] Using Flash Attention backend.\nINFO 03-10 21:30:52 model_runner.py:1110] Starting to load model /models/qwen-2.5-vl-72b-in-awq-0226...\nWARNING 03-10 21:30:52 vision.py:94] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\nINFO 03-10 21:30:52 config.py:3054] cudagraph sizes specified by model runner [1, 2, 4, 8, 16] is overridden by config [1, 2, 4, 8, 16]\nLoading safetensors checkpoint shards:   0% Completed | 0/11 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:   9% Completed | 1/11 [00:00<00:08,  1.23it/s]\nLoading safetensors checkpoint shards:  18% Completed | 2/11 [00:01<00:07,  1.17it/s]\nLoading safetensors checkpoint shards:  27% Completed | 3/11 [00:02<00:06,  1.18it/s]\nLoading safetensors checkpoint shards:  36% Completed | 4/11 [00:03<00:06,  1.16it/s]\nLoading safetensors checkpoint shards:  45% Completed | 5/11 [00:04<00:05,  1.15it/s]\nLoading safetensors checkpoint shards:  55% Completed | 6/11 [00:05<00:04,  1.14it/s]\nLoading safetensors checkpoint shards:  64% Completed | 7/11 [00:06<00:03,  1.05it/s]\nLoading safetensors checkpoint shards:  64% Completed | 7/11 [00:06<00:03,  1.11it/s]\n\nERROR 03-10 21:30:59 engine.py:400] Error while deserializing header: InvalidHeaderDeserialization\nERROR 03-10 21:30:59 engine.py:400] Traceback (most recent call last):\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 391, in run_mp_engine\nERROR 03-10 21:30:59 engine.py:400]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\nERROR 03-10 21:30:59 engine.py:400]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 124, in from_engine_args\nERROR 03-10 21:30:59 engine.py:400]     return cls(ipc_path=ipc_path,\nERROR 03-10 21:30:59 engine.py:400]            ^^^^^^^^^^^^^^^^^^^^^^\nProcess SpawnProcess-1:\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 76, in __init__\nERROR 03-10 21:30:59 engine.py:400]     self.engine = LLMEngine(*args, **kwargs)\nERROR 03-10 21:30:59 engine.py:400]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 273, in __init__\nERROR 03-10 21:30:59 engine.py:400]     self.model_executor = executor_class(vllm_config=vllm_config, )\nERROR 03-10 21:30:59 engine.py:400]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 52, in __init__\nERROR 03-10 21:30:59 engine.py:400]     self._init_executor()\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\nERROR 03-10 21:30:59 engine.py:400]     self.collective_rpc(\"load_model\")\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\nERROR 03-10 21:30:59 engine.py:400]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 03-10 21:30:59 engine.py:400]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2196, in run_method\nERROR 03-10 21:30:59 engine.py:400]     return func(*args, **kwargs)\nERROR 03-10 21:30:59 engine.py:400]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 183, in load_model\nERROR 03-10 21:30:59 engine.py:400]     self.model_runner.load_model()\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1112, in load_model\nERROR 03-10 21:30:59 engine.py:400]     self.model = get_model(vllm_config=self.vllm_config)\nERROR 03-10 21:30:59 engine.py:400]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\nERROR 03-10 21:30:59 engine.py:400]     return loader.load_model(vllm_config=vllm_config)\nERROR 03-10 21:30:59 engine.py:400]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 409, in load_model\nERROR 03-10 21:30:59 engine.py:400]     loaded_weights = model.load_weights(\nERROR 03-10 21:30:59 engine.py:400]                      ^^^^^^^^^^^^^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2_5_vl.py\", line 1097, in load_weights\nERROR 03-10 21:30:59 engine.py:400]     return loader.load_weights(weights, mapper=self.hf_to_vllm_mapper)\nERROR 03-10 21:30:59 engine.py:400]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 235, in load_weights\nERROR 03-10 21:30:59 engine.py:400]     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\nERROR 03-10 21:30:59 engine.py:400]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 196, in _load_module\nERROR 03-10 21:30:59 engine.py:400]     yield from self._load_module(prefix,\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 173, in _load_module\nERROR 03-10 21:30:59 engine.py:400]     loaded_params = module_load_weights(weights)\nERROR 03-10 21:30:59 engine.py:400]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2.py\", line 515, in load_weights\nERROR 03-10 21:30:59 engine.py:400]     return loader.load_weights(weights)\nERROR 03-10 21:30:59 engine.py:400]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 235, in load_weights\nERROR 03-10 21:30:59 engine.py:400]     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\nERROR 03-10 21:30:59 engine.py:400]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 196, in _load_module\nERROR 03-10 21:30:59 engine.py:400]     yield from self._load_module(prefix,\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 173, in _load_module\nERROR 03-10 21:30:59 engine.py:400]     loaded_params = module_load_weights(weights)\nERROR 03-10 21:30:59 engine.py:400]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2.py\", line 375, in load_weights\nERROR 03-10 21:30:59 engine.py:400]     for name, loaded_weight in weights:\nERROR 03-10 21:30:59 engine.py:400]                                ^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 108, in <genexpr>\nERROR 03-10 21:30:59 engine.py:400]     for parts, weights_data in group),\nERROR 03-10 21:30:59 engine.py:400]                                ^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 99, in <genexpr>\nERROR 03-10 21:30:59 engine.py:400]     for weight_name, weight_data in weights)\nERROR 03-10 21:30:59 engine.py:400]                                     ^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 108, in <genexpr>\nERROR 03-10 21:30:59 engine.py:400]     for parts, weights_data in group),\nERROR 03-10 21:30:59 engine.py:400]                                ^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 99, in <genexpr>\nERROR 03-10 21:30:59 engine.py:400]     for weight_name, weight_data in weights)\nERROR 03-10 21:30:59 engine.py:400]                                     ^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 61, in <genexpr>\nERROR 03-10 21:30:59 engine.py:400]     return ((out_name, data) for name, data in weights\nERROR 03-10 21:30:59 engine.py:400]                                                ^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 385, in _get_all_weights\nERROR 03-10 21:30:59 engine.py:400]     yield from self._get_weights_iterator(primary_weights)\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 369, in <genexpr>\nERROR 03-10 21:30:59 engine.py:400]     for (name, tensor) in weights_iterator)\nERROR 03-10 21:30:59 engine.py:400]                           ^^^^^^^^^^^^^^^^\nERROR 03-10 21:30:59 engine.py:400]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/weight_utils.py\", line 425, in safetensors_weights_iterator\nERROR 03-10 21:30:59 engine.py:400]     with safe_open(st_file, framework=\"pt\") as f:\nERROR 03-10 21:30:59 engine.py:400]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-10 21:30:59 engine.py:400] safetensors_rust.SafetensorError: Error while deserializing header: InvalidHeaderDeserialization\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 402, in run_mp_engine\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 391, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 124, in from_engine_args\n    return cls(ipc_path=ipc_path,\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 76, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 273, in __init__\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 52, in __init__\n    self._init_executor()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n    self.collective_rpc(\"load_model\")\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2196, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 183, in load_model\n    self.model_runner.load_model()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1112, in load_model\n    self.model = get_model(vllm_config=self.vllm_config)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n    return loader.load_model(vllm_config=vllm_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 409, in load_model\n    loaded_weights = model.load_weights(\n                     ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2_5_vl.py\", line 1097, in load_weights\n    return loader.load_weights(weights, mapper=self.hf_to_vllm_mapper)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 235, in load_weights\n    autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 196, in _load_module\n    yield from self._load_module(prefix,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 173, in _load_module\n    loaded_params = module_load_weights(weights)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2.py\", line 515, in load_weights\n    return loader.load_weights(weights)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 235, in load_weights\n    autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 196, in _load_module\n    yield from self._load_module(prefix,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 173, in _load_module\n    loaded_params = module_load_weights(weights)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2.py\", line 375, in load_weights\n    for name, loaded_weight in weights:\n                               ^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 108, in <genexpr>\n    for parts, weights_data in group),\n                               ^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 99, in <genexpr>\n    for weight_name, weight_data in weights)\n                                    ^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 108, in <genexpr>\n    for parts, weights_data in group),\n                               ^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 99, in <genexpr>\n    for weight_name, weight_data in weights)\n                                    ^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 61, in <genexpr>\n    return ((out_name, data) for name, data in weights\n                                               ^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 385, in _get_all_weights\n    yield from self._get_weights_iterator(primary_weights)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 369, in <genexpr>\n    for (name, tensor) in weights_iterator)\n                          ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/weight_utils.py\", line 425, in safetensors_weights_iterator\n    with safe_open(st_file, framework=\"pt\") as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nsafetensors_rust.SafetensorError: Error while deserializing header: InvalidHeaderDeserialization\n[rank0]:[W310 21:31:00.286836771 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 991, in <module>\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 947, in run_server\n    async with build_async_engine_client(args) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 139, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 233, in build_async_engine_client_from_engine_args\n    raise RuntimeError(\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\nroot@node37:/disk1/qwen-2.5-vl-72b-in-awq-0226# \n\n### \ud83d\udc1b Describe the bug\n\n\u4f7f\u7528\u7684\u6a21\u578b\u6743\u91cd\uff1a\nhttps://modelscope.cn/models/Qwen/Qwen2.5-VL-72B-Instruct-AWQ\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-11T04:34:08+00:00",
    "closed_at": "2025-03-13T04:05:37+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14596/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14596"
  },
  {
    "number": 13034,
    "title": "[Usage]: Multi lora inference support for llava v1.6",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\n```\n\n\n### How would you like to use vllm\n\nI tried to run multi-lora inference on vllm for llava-hf/llava-v1.6-mistral-7b-hf. But i found it not supported according to the doc here https://docs.vllm.ai/en/latest/models/supported_models.html#list-of-multimodal-language-models. \n\nIs there any plan to support multi lora for more mllm in near future or any practical way to obtain it on llava-next?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-02-10T11:27:11+00:00",
    "closed_at": "2025-02-17T10:09:54+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13034/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13034"
  },
  {
    "number": 7368,
    "title": "[Installation]: git clone cutlass fails",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Red Hat Enterprise Linux release 8.10 (Ootpa) (x86_64)\r\nGCC version: (GCC) 8.5.0 20210514 (Red Hat 8.5.0-22)\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.28\r\n\r\nPython version: 3.11.9 (main, Jun 19 2024, 10:02:06) [GCC 8.5.0 20210514 (Red Hat 8.5.0-22)] (64-bit runtime)\r\nPython platform: Linux-4.18.0-553.8.1.el8_10.x86_64-x86_64-with-glibc2.28\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA L40S-48C\r\nGPU 1: NVIDIA L40S-48C\r\nGPU 2: NVIDIA L40S-48C\r\n\r\nNvidia driver version: 535.129.03\r\ncuDNN version: Probably one of the following:\r\n/usr/lib64/libcudnn.so.9.3.0\r\n/usr/lib64/libcudnn_adv.so.9.3.0\r\n/usr/lib64/libcudnn_cnn.so.9.3.0\r\n/usr/lib64/libcudnn_engines_precompiled.so.9.3.0\r\n/usr/lib64/libcudnn_engines_runtime_compiled.so.9.3.0\r\n/usr/lib64/libcudnn_graph.so.9.3.0\r\n/usr/lib64/libcudnn_heuristic.so.9.3.0\r\n/usr/lib64/libcudnn_ops.so.9.3.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              12\r\nOn-line CPU(s) list: 0-11\r\nThread(s) per core:  1\r\nCore(s) per socket:  12\r\nSocket(s):           1\r\nNUMA node(s):        1\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               143\r\nModel name:          Intel(R) Xeon(R) Platinum 8462Y+\r\nStepping:            8\r\nCPU MHz:             2799.999\r\nBogoMIPS:            5599.99\r\nHypervisor vendor:   VMware\r\nVirtualization type: full\r\nL1d cache:           48K\r\nL1i cache:           32K\r\nL2 cache:            2048K\r\nL3 cache:            61440K\r\nNUMA node0 CPU(s):   0-11\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 wbnoinvd arat avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid cldemote movdiri movdir64b fsrm md_clear flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==2.0.1\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.4.0\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PIX     PIX     0-11    0               N/A\r\nGPU1    PIX      X      PIX     0-11    0               N/A\r\nGPU2    PIX     PIX      X      0-11    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\n```\r\n\n\n### How you are installing vllm\n\n```sh\r\npip install -e .\r\n```\r\n\r\nThe following error occurs\r\n\r\n```sh\r\nBuilding wheels for collected packages: vllm\r\n  Building editable for vllm (pyproject.toml) ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  \u00d7 Building editable for vllm (pyproject.toml) did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [200 lines of output]\r\n      /u01/tmp/pip-build-env-rr9ekh9u/overlay/lib64/python3.11/site-packages/torch/_subclasses/functional_tensor.py:258: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\r\n        cpu = _conversion_method_template(device=torch.device(\"cpu\"))\r\n      running editable_wheel\r\n      creating /u01/tmp/pip-wheel-n4hc__5j/.tmp-39puxztq/vllm.egg-info\r\n      writing /u01/tmp/pip-wheel-n4hc__5j/.tmp-39puxztq/vllm.egg-info/PKG-INFO\r\n      writing dependency_links to /u01/tmp/pip-wheel-n4hc__5j/.tmp-39puxztq/vllm.egg-info/dependency_links.txt\r\n      writing entry points to /u01/tmp/pip-wheel-n4hc__5j/.tmp-39puxztq/vllm.egg-info/entry_points.txt\r\n      writing requirements to /u01/tmp/pip-wheel-n4hc__5j/.tmp-39puxztq/vllm.egg-info/requires.txt\r\n      writing top-level names to /u01/tmp/pip-wheel-n4hc__5j/.tmp-39puxztq/vllm.egg-info/top_level.txt\r\n      writing manifest file '/u01/tmp/pip-wheel-n4hc__5j/.tmp-39puxztq/vllm.egg-info/SOURCES.txt'\r\n      reading manifest file '/u01/tmp/pip-wheel-n4hc__5j/.tmp-39puxztq/vllm.egg-info/SOURCES.txt'\r\n      reading manifest template 'MANIFEST.in'\r\n      adding license file 'LICENSE'\r\n      writing manifest file '/u01/tmp/pip-wheel-n4hc__5j/.tmp-39puxztq/vllm.egg-info/SOURCES.txt'\r\n      creating '/u01/tmp/pip-wheel-n4hc__5j/.tmp-39puxztq/vllm-0.5.4+cu122.dist-info'\r\n      creating /u01/tmp/pip-wheel-n4hc__5j/.tmp-39puxztq/vllm-0.5.4+cu122.dist-info/WHEEL\r\n      running build_py\r\n      running build_ext\r\n      -- The CXX compiler identification is GNU 8.5.0\r\n      -- Detecting CXX compiler ABI info\r\n      -- Detecting CXX compiler ABI info - done\r\n      -- Check for working CXX compiler: /usr/bin/c++ - skipped\r\n      -- Detecting CXX compile features\r\n      -- Detecting CXX compile features - done\r\n      -- Build type: RelWithDebInfo\r\n      -- Target device: cuda\r\n      -- Found Python: /u01/data/analytics/environments/ppl_env/bin/python3.11 (found version \"3.11.9\") found components: Interpreter Development.Module Development.SABIModule\r\n      -- Found python matching: /u01/data/analytics/environments/ppl_env/bin/python3.11.\r\n      -- Found CUDA: /usr/local/cuda-12.2 (found version \"12.2\")\r\n      -- The CUDA compiler identification is NVIDIA 12.2.140\r\n      -- Detecting CUDA compiler ABI info\r\n      -- Detecting CUDA compiler ABI info - done\r\n      -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\r\n      -- Detecting CUDA compile features\r\n      -- Detecting CUDA compile features - done\r\n      -- Found CUDAToolkit: /usr/local/cuda/include (found version \"12.2.140\")\r\n      -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n      -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\r\n      -- Looking for pthread_create in pthreads\r\n      -- Looking for pthread_create in pthreads - not found\r\n      -- Looking for pthread_create in pthread\r\n      -- Looking for pthread_create in pthread - found\r\n      -- Found Threads: TRUE\r\n      -- Caffe2: CUDA detected: 12.2\r\n      -- Caffe2: CUDA nvcc is: /usr/local/cuda-12.2/bin/nvcc\r\n      -- Caffe2: CUDA toolkit directory: /usr/local/cuda-12.2\r\n      -- Caffe2: Header version is: 12.2\r\n      -- /usr/local/cuda-12.2/lib64/libnvrtc.so shorthash is 000ca627\r\n      -- USE_CUDNN is set to 0. Compiling without cuDNN support\r\n      -- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support\r\n      -- Autodetected CUDA architecture(s):  8.9 8.9 8.9\r\n      -- Added CUDA NVCC flags for: -gencode;arch=compute_89,code=sm_89\r\n      CMake Warning at /u01/tmp/pip-build-env-rr9ekh9u/overlay/lib64/python3.11/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\r\n        static library kineto_LIBRARY-NOTFOUND not found.\r\n      Call Stack (most recent call first):\r\n        /u01/tmp/pip-build-env-rr9ekh9u/overlay/lib64/python3.11/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:120 (append_torchlib_if_found)\r\n        CMakeLists.txt:67 (find_package)\r\n\r\n\r\n      -- Found Torch: /u01/tmp/pip-build-env-rr9ekh9u/overlay/lib64/python3.11/site-packages/torch/lib/libtorch.so\r\n      -- Enabling core extension.\r\n      -- CUDA supported arches: 7.0;7.5;8.0;8.6;8.9;9.0\r\n      -- CUDA target arches: 89-real\r\n      [1/9] Creating directories for 'cutlass-populate'\r\n      [1/9] Performing download step (git clone) for 'cutlass-populate'\r\n      Cloning into 'cutlass-src'...\r\n      fatal: unable to access 'https://github.com/nvidia/cutlass.git/': Could not resolve host: github.com\r\n      Cloning into 'cutlass-src'...\r\n      fatal: unable to access 'https://github.com/nvidia/cutlass.git/': Could not resolve host: github.com\r\n      Cloning into 'cutlass-src'...\r\n      fatal: unable to access 'https://github.com/nvidia/cutlass.git/': Could not resolve host: github.com\r\n      Had to git clone more than once: 3 times.\r\n      CMake Error at cutlass-subbuild/cutlass-populate-prefix/tmp/cutlass-populate-gitclone.cmake:50 (message):\r\n        Failed to clone repository: 'https://github.com/nvidia/cutlass.git'\r\n\r\n\r\n      FAILED: cutlass-populate-prefix/src/cutlass-populate-stamp/cutlass-populate-download /u01/tmp/tmpc3wgp9dv.build-temp/_deps/cutlass-subbuild/cutlass-populate-prefix/src/cutlass-populate-stamp/cutlass-populate-download\r\n      cd /u01/tmp/tmpc3wgp9dv.build-temp/_deps && /u01/tmp/pip-build-env-rr9ekh9u/overlay/lib64/python3.11/site-packages/cmake/data/bin/cmake -DCMAKE_MESSAGE_LOG_LEVEL=VERBOSE -P /u01/tmp/tmpc3wgp9dv.build-temp/_deps/cutlass-subbuild/cutlass-populate-prefix/tmp/cutlass-populate-gitclone.cmake && /u01/tmp/pip-build-env-rr9ekh9u/overlay/lib64/python3.11/site-packages/cmake/data/bin/cmake -E touch /u01/tmp/tmpc3wgp9dv.build-temp/_deps/cutlass-subbuild/cutlass-populate-prefix/src/cutlass-populate-stamp/cutlass-populate-download\r\n      ninja: build stopped: subcommand failed.\r\n\r\n      CMake Error at /u01/tmp/pip-build-env-rr9ekh9u/overlay/lib64/python3.11/site-packages/cmake/data/share/cmake-3.30/Modules/FetchContent.cmake:1918 (message):\r\n        Build step for cutlass failed: 1\r\n      Call Stack (most recent call first):\r\n        /u01/tmp/pip-build-env-rr9ekh9u/overlay/lib64/python3.11/site-packages/cmake/data/share/cmake-3.30/Modules/FetchContent.cmake:1609 (__FetchContent_populateSubbuild)\r\n        /u01/tmp/pip-build-env-rr9ekh9u/overlay/lib64/python3.11/site-packages/cmake/data/share/cmake-3.30/Modules/FetchContent.cmake:2145:EVAL:2 (__FetchContent_doPopulation)\r\n        /u01/tmp/pip-build-env-rr9ekh9u/overlay/lib64/python3.11/site-packages/cmake/data/share/cmake-3.30/Modules/FetchContent.cmake:2145 (cmake_language)\r\n        /u01/tmp/pip-build-env-rr9ekh9u/overlay/lib64/python3.11/site-packages/cmake/data/share/cmake-3.30/Modules/FetchContent.cmake:2384 (__FetchContent_Populate)\r\n        CMakeLists.txt:200 (FetchContent_MakeAvailable)\r\n\r\n\r\n      -- Configuring incomplete, errors occurred!\r\n      Traceback (most recent call last):\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/command/editable_wheel.py\", line 153, in run\r\n          self._create_wheel_file(bdist_wheel)\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/command/editable_wheel.py\", line 355, in _create_wheel_file\r\n          files, mapping = self._run_build_commands(dist_name, unpacked, lib, tmp)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/command/editable_wheel.py\", line 278, in _run_build_commands\r\n          self._run_build_subcommands()\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/command/editable_wheel.py\", line 305, in _run_build_subcommands\r\n          self.run_command(name)\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\r\n          self.distribution.run_command(command)\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 945, in run_command\r\n          super().run_command(command)\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 989, in run_command\r\n          cmd_obj.run()\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 93, in run\r\n          _build_ext.run(self)\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\r\n          self.build_extensions()\r\n        File \"<string>\", line 219, in build_extensions\r\n        File \"<string>\", line 201, in configure\r\n        File \"/usr/lib64/python3.11/subprocess.py\", line 413, in check_call\r\n          raise CalledProcessError(retcode, cmd)\r\n      subprocess.CalledProcessError: Command '['cmake', '/u01/data/analytics/personal2/Paul/vllm', '-G', 'Ninja', '-DCMAKE_BUILD_TYPE=RelWithDebInfo', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=/u01/tmp/tmpwpuabv8f.build-lib/vllm', '-DCMAKE_ARCHIVE_OUTPUT_DIRECTORY=/u01/tmp/tmpc3wgp9dv.build-temp', '-DVLLM_TARGET_DEVICE=cuda', '-DVLLM_PYTHON_EXECUTABLE=/u01/data/analytics/environments/ppl_env/bin/python3.11', '-DNVCC_THREADS=1', '-DCMAKE_JOB_POOL_COMPILE:STRING=compile', '-DCMAKE_JOB_POOLS:STRING=compile=12']' returned non-zero exit status 1.\r\n      /u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py:989: _DebuggingTips: Problem in editable installation.\r\n      !!\r\n\r\n              ********************************************************************************\r\n              An error happened while installing `vllm` in editable mode.\r\n\r\n              The following steps are recommended to help debug this problem:\r\n\r\n              - Try to install the project normally, without using the editable mode.\r\n                Does the error still persist?\r\n                (If it does, try fixing the problem before attempting the editable mode).\r\n              - If you are using binary extensions, make sure you have all OS-level\r\n                dependencies installed (e.g. compilers, toolchains, binary libraries, ...).\r\n              - Try the latest version of setuptools (maybe the error was already fixed).\r\n              - If you (or your project dependencies) are using any setuptools extension\r\n                or customization, make sure they support the editable mode.\r\n\r\n              After following the steps above, if the problem still persists and\r\n              you think this is related to how setuptools handles editable installations,\r\n              please submit a reproducible example\r\n              (see https://stackoverflow.com/help/minimal-reproducible-example) to:\r\n\r\n                  https://github.com/pypa/setuptools/issues\r\n\r\n              See https://setuptools.pypa.io/en/latest/userguide/development_mode.html for details.\r\n              ********************************************************************************\r\n\r\n      !!\r\n        cmd_obj.run()\r\n      Traceback (most recent call last):\r\n        File \"/u01/data/analytics/environments/ppl_env/lib64/python3.11/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 351, in <module>\r\n          main()\r\n        File \"/u01/data/analytics/environments/ppl_env/lib64/python3.11/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 333, in main\r\n          json_out['return_val'] = hook(**hook_input['kwargs'])\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/u01/data/analytics/environments/ppl_env/lib64/python3.11/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 271, in build_editable\r\n          return hook(wheel_directory, config_settings, metadata_directory)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 453, in build_editable\r\n          return self._build_with_temp_dir(\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 397, in _build_with_temp_dir\r\n          self.run_setup()\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 313, in run_setup\r\n          exec(code, locals())\r\n        File \"<string>\", line 456, in <module>\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/__init__.py\", line 108, in setup\r\n          return distutils.core.setup(**attrs)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 184, in setup\r\n          return run_commands(dist)\r\n                 ^^^^^^^^^^^^^^^^^^\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 200, in run_commands\r\n          dist.run_commands()\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 970, in run_commands\r\n          self.run_command(cmd)\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 945, in run_command\r\n          super().run_command(command)\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 989, in run_command\r\n          cmd_obj.run()\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/command/editable_wheel.py\", line 153, in run\r\n          self._create_wheel_file(bdist_wheel)\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/command/editable_wheel.py\", line 355, in _create_wheel_file\r\n          files, mapping = self._run_build_commands(dist_name, unpacked, lib, tmp)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/command/editable_wheel.py\", line 278, in _run_build_commands\r\n          self._run_build_subcommands()\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/command/editable_wheel.py\", line 305, in _run_build_subcommands\r\n          self.run_command(name)\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\r\n          self.distribution.run_command(command)\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 945, in run_command\r\n          super().run_command(command)\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 989, in run_command\r\n          cmd_obj.run()\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 93, in run\r\n          _build_ext.run(self)\r\n        File \"/u01/tmp/pip-build-env-rr9ekh9u/overlay/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\r\n          self.build_extensions()\r\n        File \"<string>\", line 219, in build_extensions\r\n        File \"<string>\", line 201, in configure\r\n        File \"/usr/lib64/python3.11/subprocess.py\", line 413, in check_call\r\n          raise CalledProcessError(retcode, cmd)\r\n      subprocess.CalledProcessError: Command '['cmake', '/u01/data/analytics/personal2/Paul/vllm', '-G', 'Ninja', '-DCMAKE_BUILD_TYPE=RelWithDebInfo', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=/u01/tmp/tmpwpuabv8f.build-lib/vllm', '-DCMAKE_ARCHIVE_OUTPUT_DIRECTORY=/u01/tmp/tmpc3wgp9dv.build-temp', '-DVLLM_TARGET_DEVICE=cuda', '-DVLLM_PYTHON_EXECUTABLE=/u01/data/analytics/environments/ppl_env/bin/python3.11', '-DNVCC_THREADS=1', '-DCMAKE_JOB_POOL_COMPILE:STRING=compile', '-DCMAKE_JOB_POOLS:STRING=compile=12']' returned non-zero exit status 1.\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building editable for vllm\r\nFailed to build vllm\r\nERROR: Could not build wheels for vllm, which is required to install pyproject.toml-based projects\r\n\r\n```\r\n\r\nBut numpy is installed:\r\n\r\n```\r\n(xyz) [adm@xxx vllm]$ pip show numpy\r\nName: numpy\r\nVersion: 2.0.1\r\nSummary: Fundamental package for array computing in Python\r\nHome-page: https://numpy.org\r\nAuthor: Travis E. Oliphant et al.\r\nAuthor-email:\r\nLicense: Copyright (c) 2005-2024, NumPy Developers.\r\nAll rights reserved.\r\n\r\n```\r\n\r\ncutlass is also installed. how can I prevent the install process from trying to download cutlass? (I am in a restricted environment)\r\n```\r\n(xyz) [adm@xxx vllm]$ vllm]$ pip show nvidia-cutlass\r\nName: nvidia-cutlass\r\nVersion: 3.5.1.0\r\nSummary: CUTLASS\r\nHome-page:\r\nAuthor:\r\nAuthor-email:\r\nLicense: BSD-3-Clause\r\nLocation: /u01/data/analytics/environments/ppl_env/lib64/python3.11/site-packages\r\nRequires: cuda-python, networkx, numpy, pydot, scipy, treelib\r\nRequired-by:\r\n\r\n```\r\n\r\n\r\nCan you help me, please?",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2024-08-09T15:29:52+00:00",
    "closed_at": "2024-08-11T22:54:41+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7368/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7368"
  },
  {
    "number": 17170,
    "title": "[Bug]: HIP error: invalid device function",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\nINFO 04-25 08:07:32 [__init__.py:239] Automatically detected platform rocm.\nCollecting environment information...\nPyTorch version: 2.7.0a0+git1341794\nIs debug build: False\nCUDA used to build PyTorch: N/A\nROCM used to build PyTorch: 6.4.43482-0f2d60242\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: 19.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.4.0 25133 c7fe45cf4b819c5991fe208aaa96edf142730f1d)\nCMake version: version 3.31.6\nLibc version: glibc-2.35\n\nPython version: 3.12.10 (main, Apr  9 2025, 08:55:05) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-6.8.0-57-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: AMD Instinct MI100 (gfx908:sramecc+:xnack-)\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\nHIP runtime version: 6.4.43482\nMIOpen runtime version: 3.4.0\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        43 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               32\nOn-line CPU(s) list:                  0-31\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 7282 16-Core Processor\nCPU family:                           23\nModel:                                49\nThread(s) per core:                   1\nCore(s) per socket:                   16\nSocket(s):                            2\nStepping:                             0\nFrequency boost:                      disabled\nCPU max MHz:                          2800.0000\nCPU min MHz:                          1500.0000\nBogoMIPS:                             5599.93\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sev sev_es\nVirtualization:                       AMD-V\nL1d cache:                            1 MiB (32 instances)\nL1i cache:                            1 MiB (32 instances)\nL2 cache:                             16 MiB (32 instances)\nL3 cache:                             128 MiB (8 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-15\nNUMA node1 CPU(s):                    16-31\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT disabled\nVulnerability Spec rstack overflow:   Mitigation; SMT disabled\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==2.2.5\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.0a0+git1341794\n[pip3] torchvision==0.22.0+2ddb698\n[pip3] transformers==4.51.3\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: 6.4.43482-0f2d60242\nNeuron SDK Version: N/A\nvLLM Version: 0.8.5.dev137+gacba33a0f.d20250422 (git sha: acba33a0f, date: 20250422)\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n============================ ROCm System Management Interface ============================\n================================ Weight between two GPUs =================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \nGPU0   0            40           15           15           72           72           72           15           \nGPU1   40           0            40           40           15           15           15           72           \nGPU2   15           40           0            15           72           72           72           15           \nGPU3   15           40           15           0            72           72           72           15           \nGPU4   72           15           72           72           0            15           15           40           \nGPU5   72           15           72           72           15           0            15           40           \nGPU6   72           15           72           72           15           15           0            40           \nGPU7   15           72           15           15           40           40           40           0            \n\n================================= Hops between two GPUs ==================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \nGPU0   0            2            1            1            3            3            3            1            \nGPU1   2            0            2            2            1            1            1            3            \nGPU2   1            2            0            1            3            3            3            1            \nGPU3   1            2            1            0            3            3            3            1            \nGPU4   3            1            3            3            0            1            1            2            \nGPU5   3            1            3            3            1            0            1            2            \nGPU6   3            1            3            3            1            1            0            2            \nGPU7   1            3            1            1            2            2            2            0            \n\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \nGPU0   0            PCIE         XGMI         XGMI         PCIE         PCIE         PCIE         XGMI         \nGPU1   PCIE         0            PCIE         PCIE         XGMI         XGMI         XGMI         PCIE         \nGPU2   XGMI         PCIE         0            XGMI         PCIE         PCIE         PCIE         XGMI         \nGPU3   XGMI         PCIE         XGMI         0            PCIE         PCIE         PCIE         XGMI         \nGPU4   PCIE         XGMI         PCIE         PCIE         0            XGMI         XGMI         PCIE         \nGPU5   PCIE         XGMI         PCIE         PCIE         XGMI         0            XGMI         PCIE         \nGPU6   PCIE         XGMI         PCIE         PCIE         XGMI         XGMI         0            PCIE         \nGPU7   XGMI         PCIE         XGMI         XGMI         PCIE         PCIE         PCIE         0            \n\n======================================= Numa Nodes =======================================\nGPU[0]\t\t: (Topology) Numa Node: 0\nGPU[0]\t\t: (Topology) Numa Affinity: 0\nGPU[1]\t\t: (Topology) Numa Node: 0\nGPU[1]\t\t: (Topology) Numa Affinity: 0\nGPU[2]\t\t: (Topology) Numa Node: 0\nGPU[2]\t\t: (Topology) Numa Affinity: 0\nGPU[3]\t\t: (Topology) Numa Node: 0\nGPU[3]\t\t: (Topology) Numa Affinity: 0\nGPU[4]\t\t: (Topology) Numa Node: 1\nGPU[4]\t\t: (Topology) Numa Affinity: 1\nGPU[5]\t\t: (Topology) Numa Node: 1\nGPU[5]\t\t: (Topology) Numa Affinity: 1\nGPU[6]\t\t: (Topology) Numa Node: 1\nGPU[6]\t\t: (Topology) Numa Affinity: 1\nGPU[7]\t\t: (Topology) Numa Node: 1\nGPU[7]\t\t: (Topology) Numa Affinity: 1\n================================== End of ROCm SMI Log ===================================\n\nPYTORCH_ROCM_ARCH=gfx908\nLD_LIBRARY_PATH=/opt/rocm/lib:/usr/local/lib:\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nfrom vllm import LLM, SamplingParams\n\n# Sample prompts.\nprompts = [\n    \"Hello, my name is\",\n    \"The president of the United States is\",\n    \"The capital of France is\",\n    \"The future of AI is\",\n]\nsampling_params = SamplingParams(max_tokens=8192, temperature=0.8, top_p=0.95)\n\t\nif __name__ == \"__main__\":\n\n\tmodel_name = \"/home/zz/mistralai/mistralhf-gptq/\"\n\t# Create an LLM.\n\tllm = LLM(model=model_name) #quantization=\"fp8\"  tensor_parallel_size=4\n\n\t# Generate texts from the prompts. The output is a list of RequestOutput objects\n\t# that contain the prompt, generated text, and other information.\n\toutputs = llm.generate(prompts, sampling_params)\n\t# Print the outputs.\n\tfor output in outputs:\n\t\tprompt = output.prompt\n\t\tgenerated_text = output.outputs[0].text\n\t\tprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\n\n\nWhen running the simple quick start code, the following error was reported:\n\nINFO 04-25 08:04:59 [__init__.py:239] Automatically detected platform rocm.\nINFO 04-25 08:05:18 [config.py:713] This model supports multiple tasks: {'embed', 'classify', 'score', 'generate', 'reward'}. Defaulting to 'generate'.\nWARNING 04-25 08:05:27 [config.py:792] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nINFO 04-25 08:05:27 [arg_utils.py:1697] rocm is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.\nWARNING 04-25 08:05:27 [arg_utils.py:1564] The model has a long context length (131072). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.\nINFO 04-25 08:05:27 [config.py:1796] Disabled the custom all-reduce kernel because it is not supported on current platform.\nINFO 04-25 08:05:27 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.5.dev137+gacba33a0f.d20250422) with config: model='/home/zz/mistralai/mistralhf-gptq/', speculative_config=None, tokenizer='/home/zz/mistralai/mistralhf-gptq/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=gptq, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/zz/mistralai/mistralhf-gptq/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \nINFO 04-25 08:05:28 [rocm.py:156] None is not supported in AMD GPUs.\nINFO 04-25 08:05:28 [rocm.py:157] Using ROCmFlashAttention backend.\nINFO 04-25 08:05:28 [parallel_state.py:946] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 04-25 08:05:28 [model_runner.py:1119] Starting to load model /home/zz/mistralai/mistralhf-gptq/...\nWARNING 04-25 08:05:28 [rocm.py:258] Model architecture 'MistralForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/zz/zz_vllm.py\", line 16, in <module>\n[rank0]:     llm = LLM(model=model_name) #quantization=\"fp8\"  tensor_parallel_size=4\n[rank0]:           ^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 1149, in inner\n[rank0]:     return fn(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py\", line 248, in __init__\n[rank0]:     self.llm_engine = LLMEngine.from_engine_args(\n[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 522, in from_engine_args\n[rank0]:     return engine_cls.from_vllm_config(\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 498, in from_vllm_config\n[rank0]:     return cls(\n[rank0]:            ^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 282, in __init__\n[rank0]:     self.model_executor = executor_class(vllm_config=vllm_config, )\n[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 52, in __init__\n[rank0]:     self._init_executor()\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n[rank0]:     self.collective_rpc(\"load_model\")\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n[rank0]:     answer = run_method(self.driver_worker, method, args, kwargs)\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2428, in run_method\n[rank0]:     return func(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 203, in load_model\n[rank0]:     self.model_runner.load_model()\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1122, in load_model\n[rank0]:     self.model = get_model(vllm_config=self.vllm_config)\n[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n[rank0]:     return loader.load_model(vllm_config=vllm_config)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 452, in load_model\n[rank0]:     model = _initialize_model(vllm_config=vllm_config)\n[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\n[rank0]:     return model_class(vllm_config=vllm_config, prefix=prefix)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 486, in __init__\n[rank0]:     self.model = self._init_model(vllm_config=vllm_config,\n[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 527, in _init_model\n[rank0]:     return LlamaModel(vllm_config=vllm_config,\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 151, in __init__\n[rank0]:     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 321, in __init__\n[rank0]:     self.start_layer, self.end_layer, self.layers = make_layers(\n[rank0]:                                                     ^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 610, in make_layers\n[rank0]:     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n[rank0]:                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 323, in <lambda>\n[rank0]:     lambda prefix: layer_type(config=config,\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 239, in __init__\n[rank0]:     self.self_attn = LlamaAttention(\n[rank0]:                      ^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 165, in __init__\n[rank0]:     self.rotary_emb = get_rope(\n[rank0]:                       ^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding.py\", line 1490, in get_rope\n[rank0]:     rotary_emb = RotaryEmbedding(head_size, rotary_dim, max_position, base,\n[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding.py\", line 99, in __init__\n[rank0]:     cache = self._compute_cos_sin_cache()\n[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding.py\", line 116, in _compute_cos_sin_cache\n[rank0]:     inv_freq = self._compute_inv_freq(self.base)\n[rank0]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding.py\", line 110, in _compute_inv_freq\n[rank0]:     inv_freq = 1.0 / (base**(torch.arange(\n[rank0]:                              ^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py\", line 104, in __torch_function__\n[rank0]:     return func(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n[rank0]: RuntimeError: HIP error: invalid device function\n[rank0]: HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank0]: For debugging consider passing AMD_SERIALIZE_KERNEL=3\n[rank0]: Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\n\n[rank0]:[W425 08:05:29.808958148 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-04-25T08:13:42+00:00",
    "closed_at": "2025-04-30T02:03:34+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17170/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17170"
  },
  {
    "number": 13258,
    "title": "[Bug]: error when set  VLLM_ATTENTION_BACKEND=FLASHINFER",
    "body": "### Your current environment\n\n<details>\n\n```text\nPyTorch version: 2.4.0+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) [GCC 12.3.0] (64-bit runtime)\nPython platform: Linux-5.15.0-131-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.5.82\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: NVIDIA H100 80GB HBM3\nGPU 1: NVIDIA H100 80GB HBM3\nGPU 2: NVIDIA H100 80GB HBM3\nGPU 3: NVIDIA H100 80GB HBM3\nGPU 4: NVIDIA H100 80GB HBM3\nGPU 5: NVIDIA H100 80GB HBM3\nGPU 6: NVIDIA H100 80GB HBM3\nGPU 7: NVIDIA H100 80GB HBM3\n\nNvidia driver version: 550.127.08\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               96\nOn-line CPU(s) list:                  0-95\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8470\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   48\nSocket(s):                            1\nStepping:                             8\nBogoMIPS:                             4000.06\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch topoext cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk avx512_fp16 arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            3 MiB (96 instances)\nL1i cache:                            3 MiB (96 instances)\nL2 cache:                             192 MiB (48 instances)\nL3 cache:                             16 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-95\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Unknown: No mitigations\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Mitigation; TSX disabled\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.1.3.1\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.0.2.54\n[pip3] nvidia-curand-cu12==10.3.2.106\n[pip3] nvidia-cusolver-cu12==11.4.5.107\n[pip3] nvidia-cusparse-cu12==12.1.0.106\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.20.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.1.105\n[pip3] pyzmq==26.0.3\n[pip3] torch==2.4.0\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.19.0\n[pip3] transformers==4.48.3\n[pip3] triton==3.0.0\n[conda] blas                      2.116                       mkl    conda-forge\n[conda] blas-devel                3.9.0            16_linux64_mkl    conda-forge\n[conda] cuda-cudart               12.1.105                      0    nvidia\n[conda] cuda-cupti                12.1.105                      0    nvidia\n[conda] cuda-libraries            12.1.0                        0    nvidia\n[conda] cuda-nvrtc                12.1.105                      0    nvidia\n[conda] cuda-nvtx                 12.1.105                      0    nvidia\n[conda] cuda-opencl               12.5.39                       0    nvidia\n[conda] cuda-runtime              12.1.0                        0    nvidia\n[conda] cuda-version              12.5                          3    nvidia\n[conda] libblas                   3.9.0            16_linux64_mkl    conda-forge\n[conda] libcblas                  3.9.0            16_linux64_mkl    conda-forge\n[conda] libcublas                 12.1.0.26                     0    nvidia\n[conda] libcufft                  11.0.2.4                      0    nvidia\n[conda] libcufile                 1.10.0.4                      0    nvidia\n[conda] libcurand                 10.3.6.39                     0    nvidia\n[conda] libcusolver               11.4.4.55                     0    nvidia\n[conda] libcusparse               12.0.2.55                     0    nvidia\n[conda] liblapack                 3.9.0            16_linux64_mkl    conda-forge\n[conda] liblapacke                3.9.0            16_linux64_mkl    conda-forge\n[conda] libnpp                    12.0.2.50                     0    nvidia\n[conda] libnvjitlink              12.1.105                      0    nvidia\n[conda] libnvjpeg                 12.1.1.14                     0    nvidia\n[conda] libopenvino-pytorch-frontend 2024.2.0             he02047a_1    conda-forge\n[conda] mkl                       2022.1.0           h84fe81f_915    conda-forge\n[conda] mkl-devel                 2022.1.0           ha770c72_916    conda-forge\n[conda] mkl-include               2022.1.0           h84fe81f_915    conda-forge\n[conda] nomkl                     2.0                           0    anaconda\n[conda] numpy                     1.26.4          py311h64a7726_0    conda-forge\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\n[conda] nvidia-ml-py              12.570.86                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\n[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch\n[conda] pytorch-mutex             1.0                        cuda    pytorch\n[conda] pyzmq                     26.0.3          py311h08a0b41_0    conda-forge\n[conda] torch                     2.4.0                    pypi_0    pypi\n[conda] torchaudio                2.5.1                    pypi_0    pypi\n[conda] torchvision               0.19.0                   pypi_0    pypi\n[conda] transformers              4.48.3                   pypi_0    pypi\n[conda] triton                    3.0.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.6.3.post1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    0-95    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    0-95    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    0-95    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    0-95    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    0-95    0               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    0-95    0               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    0-95    0               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      0-95    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nVLLM_ATTENTION_BACKEND=FLASHINFER\nLD_LIBRARY_PATH=/home/user/mambaforge/envs/env/lib/python3.11/site-packages/cv2/../../lib64:/usr/local/cuda-12.5/lib64:\nMKL_INTERFACE_LAYER=LP64,GNU\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI set  \n`export VLLM_ATTENTION_BACKEND=FLASHINFER`  \nand my start commend here \n`vllm serve sophosympatheia/Midnight-Miqu-70B-v1.0 --tensor-parallel-size 8 --gpu-memory-utilization .94 --max-model-len 16384 --enable-prefix-caching --kv-cache-dtype fp8 --dtype bfloat16 --max-num-seqs 128 --block-size 32 --enable-chunked-prefill --max-num-batched-tokens 8192`\n\nerror Appeared\n`INFO 02-14 03:50:41 api_server.py:528] vLLM API server version 0.6.3.post1\nINFO 02-14 03:50:41 api_server.py:529] args: Namespace(subparser='serve', model_tag='sophosympatheia/Midnight-Miqu-70B-v1.0', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='sophosympatheia/Midnight-Miqu-70B-v1.0', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='bfloat16', kv_cache_dtype='fp8', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=8, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.94, num_gpu_blocks_override=None, max_num_batched_tokens=8192, max_num_seqs=128, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=True, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7f49fa346480>)\nINFO 02-14 03:50:41 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/25e8600a-2258-4d2e-9ade-9661ace9b9d5 for IPC Path.\nINFO 02-14 03:50:41 api_server.py:179] Started engine process with PID 25403\nWARNING 02-14 03:50:42 config.py:1668] Casting torch.float16 to torch.bfloat16.\nINFO 02-14 03:50:54 config.py:653] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor\nWARNING 02-14 03:50:59 config.py:1668] Casting torch.float16 to torch.bfloat16.\nINFO 02-14 03:51:01 config.py:905] Defaulting to use mp for distributed inference\nWARNING 02-14 03:51:01 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\nINFO 02-14 03:51:01 config.py:1021] Chunked prefill is enabled with max_num_batched_tokens=8192.\nINFO 02-14 03:51:03 config.py:653] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor\nINFO 02-14 03:51:03 config.py:905] Defaulting to use mp for distributed inference\nWARNING 02-14 03:51:03 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\nINFO 02-14 03:51:03 config.py:1021] Chunked prefill is enabled with max_num_batched_tokens=8192.\nINFO 02-14 03:51:03 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='sophosympatheia/Midnight-Miqu-70B-v1.0', speculative_config=None, tokenizer='sophosympatheia/Midnight-Miqu-70B-v1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=fp8, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=sophosympatheia/Midnight-Miqu-70B-v1.0, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, mm_processor_kwargs=None)\nWARNING 02-14 03:51:03 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 02-14 03:51:03 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\nINFO 02-14 03:51:04 selector.py:141] Using Flashinfer backend.\n(VllmWorkerProcess pid=25667) INFO 02-14 03:51:08 selector.py:141] Using Flashinfer backend.\n(VllmWorkerProcess pid=25667) INFO 02-14 03:51:08 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=25671) INFO 02-14 03:51:08 selector.py:141] Using Flashinfer backend.\n(VllmWorkerProcess pid=25671) INFO 02-14 03:51:08 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=25668) INFO 02-14 03:51:08 selector.py:141] Using Flashinfer backend.\n(VllmWorkerProcess pid=25669) INFO 02-14 03:51:08 selector.py:141] Using Flashinfer backend.\n(VllmWorkerProcess pid=25670) INFO 02-14 03:51:08 selector.py:141] Using Flashinfer backend.\n(VllmWorkerProcess pid=25665) INFO 02-14 03:51:08 selector.py:141] Using Flashinfer backend.\n(VllmWorkerProcess pid=25666) INFO 02-14 03:51:08 selector.py:141] Using Flashinfer backend.\n(VllmWorkerProcess pid=25668) INFO 02-14 03:51:08 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=25669) INFO 02-14 03:51:08 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=25670) INFO 02-14 03:51:08 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=25665) INFO 02-14 03:51:08 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=25666) INFO 02-14 03:51:08 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\nINFO 02-14 03:51:11 utils.py:1008] Found nccl from library libnccl.so.2\nINFO 02-14 03:51:11 pynccl.py:63] vLLM is using nccl==2.20.5\n(VllmWorkerProcess pid=25667) INFO 02-14 03:51:11 utils.py:1008] Found nccl from library libnccl.so.2\n(VllmWorkerProcess pid=25665) INFO 02-14 03:51:11 utils.py:1008] Found nccl from library libnccl.so.2\n(VllmWorkerProcess pid=25667) INFO 02-14 03:51:11 pynccl.py:63] vLLM is using nccl==2.20.5\n(VllmWorkerProcess pid=25665) INFO 02-14 03:51:11 pynccl.py:63] vLLM is using nccl==2.20.5\n(VllmWorkerProcess pid=25666) INFO 02-14 03:51:11 utils.py:1008] Found nccl from library libnccl.so.2\n(VllmWorkerProcess pid=25668) INFO 02-14 03:51:11 utils.py:1008] Found nccl from library libnccl.so.2\n(VllmWorkerProcess pid=25666) INFO 02-14 03:51:11 pynccl.py:63] vLLM is using nccl==2.20.5\n(VllmWorkerProcess pid=25671) INFO 02-14 03:51:11 utils.py:1008] Found nccl from library libnccl.so.2\n(VllmWorkerProcess pid=25668) INFO 02-14 03:51:11 pynccl.py:63] vLLM is using nccl==2.20.5\n(VllmWorkerProcess pid=25669) INFO 02-14 03:51:11 utils.py:1008] Found nccl from library libnccl.so.2\n(VllmWorkerProcess pid=25671) INFO 02-14 03:51:11 pynccl.py:63] vLLM is using nccl==2.20.5\n(VllmWorkerProcess pid=25670) INFO 02-14 03:51:11 utils.py:1008] Found nccl from library libnccl.so.2\n(VllmWorkerProcess pid=25669) INFO 02-14 03:51:11 pynccl.py:63] vLLM is using nccl==2.20.5\n(VllmWorkerProcess pid=25670) INFO 02-14 03:51:11 pynccl.py:63] vLLM is using nccl==2.20.5\n(VllmWorkerProcess pid=25667) INFO 02-14 03:51:18 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/user/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n(VllmWorkerProcess pid=25666) INFO 02-14 03:51:18 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/user/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n(VllmWorkerProcess pid=25670) INFO 02-14 03:51:18 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/user/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n(VllmWorkerProcess pid=25671) INFO 02-14 03:51:18 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/user/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\nINFO 02-14 03:51:18 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/user/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n(VllmWorkerProcess pid=25669) INFO 02-14 03:51:18 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/user/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n(VllmWorkerProcess pid=25668) INFO 02-14 03:51:18 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/user/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n(VllmWorkerProcess pid=25665) INFO 02-14 03:51:18 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/user/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\nINFO 02-14 03:51:18 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f5608267990>, local_subscribe_port=50629, remote_subscribe_port=None)\nINFO 02-14 03:51:18 model_runner.py:1056] Starting to load model sophosympatheia/Midnight-Miqu-70B-v1.0...\n(VllmWorkerProcess pid=25666) INFO 02-14 03:51:18 model_runner.py:1056] Starting to load model sophosympatheia/Midnight-Miqu-70B-v1.0...\n(VllmWorkerProcess pid=25669) INFO 02-14 03:51:18 model_runner.py:1056] Starting to load model sophosympatheia/Midnight-Miqu-70B-v1.0...\n(VllmWorkerProcess pid=25665) INFO 02-14 03:51:18 model_runner.py:1056] Starting to load model sophosympatheia/Midnight-Miqu-70B-v1.0...\n(VllmWorkerProcess pid=25670) INFO 02-14 03:51:18 model_runner.py:1056] Starting to load model sophosympatheia/Midnight-Miqu-70B-v1.0...\n(VllmWorkerProcess pid=25668) INFO 02-14 03:51:18 model_runner.py:1056] Starting to load model sophosympatheia/Midnight-Miqu-70B-v1.0...\n(VllmWorkerProcess pid=25671) INFO 02-14 03:51:18 model_runner.py:1056] Starting to load model sophosympatheia/Midnight-Miqu-70B-v1.0...\n(VllmWorkerProcess pid=25667) INFO 02-14 03:51:18 model_runner.py:1056] Starting to load model sophosympatheia/Midnight-Miqu-70B-v1.0...\n(VllmWorkerProcess pid=25667) INFO 02-14 03:51:18 selector.py:141] Using Flashinfer backend.\n(VllmWorkerProcess pid=25666) INFO 02-14 03:51:18 selector.py:141] Using Flashinfer backend.\n(VllmWorkerProcess pid=25671) INFO 02-14 03:51:18 selector.py:141] Using Flashinfer backend.\n(VllmWorkerProcess pid=25670) INFO 02-14 03:51:18 selector.py:141] Using Flashinfer backend.\n(VllmWorkerProcess pid=25668) INFO 02-14 03:51:18 selector.py:141] Using Flashinfer backend.\n(VllmWorkerProcess pid=25665) INFO 02-14 03:51:18 selector.py:141] Using Flashinfer backend.\nINFO 02-14 03:51:18 selector.py:141] Using Flashinfer backend.\n(VllmWorkerProcess pid=25669) INFO 02-14 03:51:18 selector.py:141] Using Flashinfer backend.\n(VllmWorkerProcess pid=25671) INFO 02-14 03:51:18 weight_utils.py:243] Using model weights format ['*.safetensors']\n(VllmWorkerProcess pid=25668) INFO 02-14 03:51:18 weight_utils.py:243] Using model weights format ['*.safetensors']\n(VllmWorkerProcess pid=25665) INFO 02-14 03:51:18 weight_utils.py:243] Using model weights format ['*.safetensors']\nINFO 02-14 03:51:18 weight_utils.py:243] Using model weights format ['*.safetensors']\n(VllmWorkerProcess pid=25670) INFO 02-14 03:51:18 weight_utils.py:243] Using model weights format ['*.safetensors']\n(VllmWorkerProcess pid=25666) INFO 02-14 03:51:18 weight_utils.py:243] Using model weights format ['*.safetensors']\n(VllmWorkerProcess pid=25669) INFO 02-14 03:51:18 weight_utils.py:243] Using model weights format ['*.safetensors']\n(VllmWorkerProcess pid=25667) INFO 02-14 03:51:18 weight_utils.py:243] Using model weights format ['*.safetensors']\nLoading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:   7% Completed | 1/15 [00:01<00:22,  1.61s/it]\nLoading safetensors checkpoint shards:  13% Completed | 2/15 [00:03<00:20,  1.58s/it]\nLoading safetensors checkpoint shards:  20% Completed | 3/15 [00:04<00:19,  1.59s/it]\nLoading safetensors checkpoint shards:  27% Completed | 4/15 [00:06<00:17,  1.59s/it]\nLoading safetensors checkpoint shards:  33% Completed | 5/15 [00:06<00:10,  1.10s/it]\nLoading safetensors checkpoint shards:  40% Completed | 6/15 [00:08<00:11,  1.23s/it]\nLoading safetensors checkpoint shards:  47% Completed | 7/15 [00:09<00:10,  1.34s/it]\nLoading safetensors checkpoint shards:  53% Completed | 8/15 [00:11<00:09,  1.43s/it]\nLoading safetensors checkpoint shards:  60% Completed | 9/15 [00:12<00:08,  1.48s/it]\nLoading safetensors checkpoint shards:  67% Completed | 10/15 [00:14<00:07,  1.50s/it]\nLoading safetensors checkpoint shards:  73% Completed | 11/15 [00:16<00:06,  1.54s/it]\nLoading safetensors checkpoint shards:  80% Completed | 12/15 [00:17<00:04,  1.53s/it]\nLoading safetensors checkpoint shards:  87% Completed | 13/15 [00:19<00:03,  1.55s/it]\nLoading safetensors checkpoint shards:  93% Completed | 14/15 [00:20<00:01,  1.57s/it]\nLoading safetensors checkpoint shards: 100% Completed | 15/15 [00:22<00:00,  1.57s/it]\nLoading safetensors checkpoint shards: 100% Completed | 15/15 [00:22<00:00,  1.49s/it]\n\nINFO 02-14 03:51:41 model_runner.py:1067] Loading model weights took 16.0698 GB\n(VllmWorkerProcess pid=25667) INFO 02-14 03:51:42 model_runner.py:1067] Loading model weights took 16.0698 GB\n(VllmWorkerProcess pid=25671) INFO 02-14 03:51:43 model_runner.py:1067] Loading model weights took 16.0698 GB\n(VllmWorkerProcess pid=25670) INFO 02-14 03:51:43 model_runner.py:1067] Loading model weights took 16.0698 GB\n(VllmWorkerProcess pid=25668) INFO 02-14 03:51:43 model_runner.py:1067] Loading model weights took 16.0698 GB\n(VllmWorkerProcess pid=25665) INFO 02-14 03:51:43 model_runner.py:1067] Loading model weights took 16.0698 GB\n(VllmWorkerProcess pid=25669) INFO 02-14 03:51:43 model_runner.py:1067] Loading model weights took 16.0698 GB\n(VllmWorkerProcess pid=25666) INFO 02-14 03:51:43 model_runner.py:1067] Loading model weights took 16.0698 GB\n(VllmWorkerProcess pid=25665) INFO 02-14 03:51:43 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20250214-035143.pkl...\nINFO 02-14 03:51:43 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20250214-035143.pkl...\n(VllmWorkerProcess pid=25669) INFO 02-14 03:51:43 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20250214-035143.pkl...\n(VllmWorkerProcess pid=25667) INFO 02-14 03:51:43 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20250214-035143.pkl...\n(VllmWorkerProcess pid=25668) INFO 02-14 03:51:43 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20250214-035143.pkl...\n(VllmWorkerProcess pid=25666) INFO 02-14 03:51:43 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20250214-035143.pkl...\n(VllmWorkerProcess pid=25671) INFO 02-14 03:51:43 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20250214-035143.pkl...\nINFO 02-14 03:51:43 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20250214-035143.pkl.\n(VllmWorkerProcess pid=25665) INFO 02-14 03:51:43 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20250214-035143.pkl.\n(VllmWorkerProcess pid=25669) INFO 02-14 03:51:43 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20250214-035143.pkl.\n(VllmWorkerProcess pid=25668) INFO 02-14 03:51:43 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20250214-035143.pkl.\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229] Exception in worker VllmWorkerProcess while processing method determine_num_available_blocks.\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229] Traceback (most recent call last):\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]   File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/worker/model_runner_base.py\", line 116, in _wrapper\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]     return func(*args, **kwargs)\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]   File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1629, in execute_model\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]     self.attn_state.begin_forward(model_input)\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]   File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/attention/backends/flashinfer.py\", line 257, in begin_forward\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]     model_input.attn_metadata.prefill_wrapper = state._get_prefill_wrapper(\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]   File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/attention/backends/flashinfer.py\", line 117, in _get_prefill_wrapper\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]     self._prefill_wrapper = BatchPrefillWithPagedKVCacheWrapper(\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229] TypeError: 'NoneType' object is not callable\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229] The above exception was the direct cause of the following exception:\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229] Traceback (most recent call last):\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]   File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]     output = executor(*args, **kwargs)\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]   File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]     return func(*args, **kwargs)\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]   File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/worker/worker.py\", line 223, in determine_num_available_blocks\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]     self.model_runner.profile_run()\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]   File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]     return func(*args, **kwargs)\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]   File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1305, in profile_run\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]     self.execute_model(model_input, kv_caches, intermediate_tensors)\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]   File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]     return func(*args, **kwargs)\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]   File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/worker/model_runner_base.py\", line 152, in _wrapper\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]     raise type(err)(\n(VllmWorkerProcess pid=25665) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229] TypeError: Error in model execution (input dumped to /tmp/err_execute_model_input_20250214-035143.pkl): 'NoneType' object is not callable\n(VllmWorkerProcess pid=25669) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229] Exception in worker VllmWorkerProcess while processing method determine_num_available_blocks.\n(VllmWorkerProcess pid=25669) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229] Traceback (most recent call last):\n(VllmWorkerProcess pid=25669) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]   File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/worker/model_runner_base.py\", line 116, in _wrapper\n(VllmWorkerProcess pid=25669) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]     return func(*args, **kwargs)\n(VllmWorkerProcess pid=25669) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=25669) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]   File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1629, in execute_model\n(VllmWorkerProcess pid=25669) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]     self.attn_state.begin_forward(model_input)\n(VllmWorkerProcess pid=25669) ERROR 02-14 03:51:43 multiproc_worker_utils.py:229]   File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/attention/backends/flashinfer.py\", line 257, in begin_forward\nERROR 02-14 03:51:45 multiproc_worker_utils.py:116] Worker VllmWorkerProcess pid 25666 died, exit code: -15\nERROR 02-14 03:51:45 multiproc_worker_utils.py:116] Worker VllmWorkerProcess pid 25671 died, exit code: -15\nINFO 02-14 03:51:45 multiproc_worker_utils.py:120] Killing local vLLM worker processes\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/worker/model_runner_base.py\", line 116, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1629, in execute_model\n    self.attn_state.begin_forward(model_input)\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/attention/backends/flashinfer.py\", line 257, in begin_forward\n    model_input.attn_metadata.prefill_wrapper = state._get_prefill_wrapper(\n                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/attention/backends/flashinfer.py\", line 117, in _get_prefill_wrapper\n    self._prefill_wrapper = BatchPrefillWithPagedKVCacheWrapper(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: 'NoneType' object is not callable\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/engine/multiprocessing/engine.py\", line 390, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/engine/multiprocessing/engine.py\", line 139, in from_engine_args\n    return cls(\n           ^^^^\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/engine/multiprocessing/engine.py\", line 78, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 348, in __init__\n    self._initialize_kv_caches()\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 483, in _initialize_kv_caches\n    self.model_executor.determine_num_available_blocks())\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/executor/distributed_gpu_executor.py\", line 39, in determine_num_available_blocks\n    num_blocks = self._run_workers(\"determine_num_available_blocks\", )\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 192, in _run_workers\n    driver_worker_output = driver_worker_method(*args, **kwargs)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/worker/worker.py\", line 223, in determine_num_available_blocks\n    self.model_runner.profile_run()\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1305, in profile_run\n    self.execute_model(model_input, kv_caches, intermediate_tensors)\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/worker/model_runner_base.py\", line 152, in _wrapper\n    raise type(err)(\nTypeError: Error in model execution (input dumped to /tmp/err_execute_model_input_20250214-035143.pkl): 'NoneType' object is not callable\n[rank0]:[W214 03:51:48.050615578 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\nTraceback (most recent call last):\n  File \"/home/user/mambaforge/envs/env/bin/vllm\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/scripts.py\", line 195, in main\n    args.dispatch_function(args)\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/scripts.py\", line 41, in serve\n    uvloop.run(run_server(args))\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/uvloop/__init__.py\", line 105, in run\n    return runner.run(wrapper())\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 552, in run_server\n    async with build_async_engine_client(args) as engine_client:\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 107, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/mambaforge/envs/env/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 194, in build_async_engine_client_from_engine_args\n    raise RuntimeError(\nRuntimeError: Engine process failed to start`\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-02-14T03:52:30+00:00",
    "closed_at": "2025-02-17T00:31:59+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13258/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13258"
  }
]
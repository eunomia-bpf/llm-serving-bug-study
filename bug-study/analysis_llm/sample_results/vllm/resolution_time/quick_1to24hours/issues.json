[
  {
    "number": 12522,
    "title": "[Bug]: Error Using V1 Engine with DeepSeek Llama 70B",
    "body": "### Your current environment\n\nVllm 0.7.0\nCUDA 12.6\nDriver Version 560.94\ntorch 2.5.1\ntransformers 4.46.0\n\n\n\n### Model Input Dumps\n\nTraceback (most recent call last):\n  File \"/home/nd600/miniconda3/envs/vllm/bin/vllm\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/nd600/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/scripts.py\", line 201, in main\n    args.dispatch_function(args)\n  File \"/home/nd600/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/scripts.py\", line 42, in serve\n    uvloop.run(run_server(args))\n  File \"/home/nd600/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/home/nd600/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/home/nd600/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/home/nd600/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/home/nd600/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 863, in run_server\n    async with build_async_engine_client(args) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd600/miniconda3/envs/vllm/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd600/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 133, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd600/miniconda3/envs/vllm/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd600/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 157, in build_async_engine_client_from_engine_args\n    engine_client = AsyncLLMEngine.from_engine_args(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd600/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py\", line 100, in from_engine_args\n    vllm_config = engine_args.create_engine_config(usage_context)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd600/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/arg_utils.py\", line 1248, in create_engine_config\n    config = VllmConfig(\n             ^^^^^^^^^^^\n  File \"<string>\", line 19, in __init__\n  File \"/home/nd600/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/config.py\", line 3203, in __post_init__\n    current_platform.check_and_update_config(self)\n  File \"/home/nd600/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/platforms/cuda.py\", line 123, in check_and_update_config\n    raise NotImplementedError\nNotImplementedError\n\n### \ud83d\udc1b Describe the bug\n\nI use: vllm serve deepseek-ai/DeepSeek-R1-Distill-Llama-70B  --host 0.0.0.0   --port 8000   --tensor-parallel-size 8  --seed 1234  --num-scheduler-steps 8  --max-model-len 16000\nWhen I set the env to use V1 engine i got the above error. When I don't set to V1, it runs fine. \nI would appreciate some help from anyone. Thanks\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-01-28T19:31:10+00:00",
    "closed_at": "2025-01-28T22:39:19+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12522/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12522"
  },
  {
    "number": 18757,
    "title": "[Bug]: model_executor/test_model_load_with_params.py  fails with AttributeError",
    "body": "### Your current environment\n\nIssue encountered on main branch tests.\n\n### \ud83d\udc1b Describe the bug\n\nTest failing with below traceback:\n\n```\nvllm_runner = <class 'tests.conftest.VllmRunner'>\n\n    @pytest.mark.skipif(current_platform.is_rocm(),\n                        reason=\"Xformers backend is not supported on ROCm.\")\n    def test_model_loading_with_params(vllm_runner):\n        \"\"\"\n        Test parameter weight loading with tp>1.\n        \"\"\"\n        with vllm_runner(model_name=MODEL_NAME,\n                         revision=REVISION,\n                         dtype=\"float16\",\n                         max_model_len=MAX_MODEL_LEN) as vllm_model:\n            output = vllm_model.encode(\"Write a short story about a robot that\"\n                                       \" dreams for the first time.\\n\")\n    \n            model_config = vllm_model.model.llm_engine.model_config\n            model_tokenizer = vllm_model.model.llm_engine.tokenizer\n    \n            # asserts on the bert model config file\n            assert model_config.encoder_config[\"max_seq_length\"] == 512\n            assert model_config.encoder_config[\"do_lower_case\"]\n    \n            # asserts on the pooling config files\n            assert model_config.pooler_config.pooling_type == PoolingType.CLS.name\n>           assert model_config.pooler_config.pooling_norm\nE           AttributeError: 'PoolerConfig' object has no attribute 'pooling_norm'. Did you mean: 'pooling_type'?\n\ntests/model_executor/test_model_load_with_params.py:43: AttributeError\n------------------------------------------------- Captured stdout call -------------------------------------------------\nINFO 05-27 12:31:33 [__init__.py:31] Available plugins for group vllm.general_plugins:\nINFO 05-27 12:31:33 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\nINFO 05-27 12:31:33 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\nINFO 05-27 12:32:10 [config.py:577] Found sentence-transformers tokenize configuration.\nINFO 05-27 12:32:10 [config.py:3131] Downcasting torch.float32 to torch.float16.\nINFO 05-27 12:32:18 [config.py:473] Found sentence-transformers modules configuration.\nINFO 05-27 12:32:18 [config.py:493] Found pooling configuration.\nINFO 05-27 12:32:18 [config.py:793] This model supports multiple tasks: {'reward', 'embed', 'classify', 'score'}. Defaulting to 'embed'.\nWARNING 05-27 12:32:18 [arg_utils.py:1583] --task embed is not supported by the V1 Engine. Falling back to V0. \nINFO 05-27 12:32:18 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.1.dev148+gfba064270.d20250526) with config: model='BAAI/bge-base-en-v1.5', speculative_config=None, tokenizer='BAAI/bge-base-en-v1.5', skip_tokenizer_init=False, tokenizer_mode=auto, revision=main, override_neuron_config={}, tokenizer_revision=main, trust_remote_code=True, dtype=torch.float16, max_seq_len=128, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=BAAI/bge-base-en-v1.5, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type='CLS', normalize=True, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"cudagraph_capture_sizes\": [256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 256}, use_cached_outputs=False, \nINFO 05-27 12:32:18 [cuda.py:292] Using Flash Attention backend.\nINFO 05-27 12:32:19 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nINFO 05-27 12:32:19 [model_runner.py:1170] Starting to load model BAAI/bge-base-en-v1.5...\nINFO 05-27 12:32:19 [weight_utils.py:291] Using model weights format ['*.safetensors']\nINFO 05-27 12:32:20 [weight_utils.py:344] No model.safetensors.index.json found in remote.\nINFO 05-27 12:32:20 [default_loader.py:280] Loading weights took 0.08 seconds\nINFO 05-27 12:32:20 [model_runner.py:1202] Model loading took 0.2091 GiB and 0.738950 seconds\n------------------------------------------------- Captured stderr call -------------------------------------------------\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 13.57it/s]\n\nAdding requests: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 66/66 [00:00<00:00, 3031.89it/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 66/66 [00:00<00:00, 176.21it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n________________________________________ test_roberta_model_loading_with_params ________________________________________\n\nvllm_runner = <class 'tests.conftest.VllmRunner'>\n\n    @pytest.mark.skipif(current_platform.is_rocm(),\n                        reason=\"Xformers backend is not supported on ROCm.\")\n    def test_roberta_model_loading_with_params(vllm_runner):\n        \"\"\"\n        Test parameter weight loading with tp>1.\n        \"\"\"\n        with vllm_runner(model_name=MODEL_NAME_ROBERTA,\n                         revision=REVISION_ROBERTA,\n                         dtype=\"float16\",\n                         max_model_len=MAX_MODEL_LEN) as vllm_model:\n            output = vllm_model.encode(\"Write a short story about a robot that\"\n                                       \" dreams for the first time.\\n\")\n    \n            model_config = vllm_model.model.llm_engine.model_config\n            model_tokenizer = vllm_model.model.llm_engine.tokenizer\n    \n            # asserts on the bert model config file\n            assert model_config.encoder_config[\"max_seq_length\"] == 512\n            assert not model_config.encoder_config[\"do_lower_case\"]\n    \n            # asserts on the pooling config files\n            assert model_config.pooler_config.pooling_type == PoolingType.MEAN.name\n>           assert model_config.pooler_config.pooling_norm\nE           AttributeError: 'PoolerConfig' object has no attribute 'pooling_norm'. Did you mean: 'pooling_type'?\n\ntests/model_executor/test_model_load_with_params.py:83: AttributeError\n------------------------------------------------- Captured stdout call -------------------------------------------------\nINFO 05-27 12:32:21 [config.py:577] Found sentence-transformers tokenize configuration.\nINFO 05-27 12:32:21 [config.py:3131] Downcasting torch.float32 to torch.float16.\nINFO 05-27 12:32:21 [config.py:473] Found sentence-transformers modules configuration.\nINFO 05-27 12:32:21 [config.py:493] Found pooling configuration.\nINFO 05-27 12:32:21 [config.py:793] This model supports multiple tasks: {'reward', 'embed', 'classify', 'score'}. Defaulting to 'embed'.\nWARNING 05-27 12:32:21 [arg_utils.py:1583] --task embed is not supported by the V1 Engine. Falling back to V0. \nINFO 05-27 12:32:21 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.1.dev148+gfba064270.d20250526) with config: model='intfloat/multilingual-e5-small', speculative_config=None, tokenizer='intfloat/multilingual-e5-small', skip_tokenizer_init=False, tokenizer_mode=auto, revision=main, override_neuron_config={}, tokenizer_revision=main, trust_remote_code=True, dtype=torch.float16, max_seq_len=128, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=intfloat/multilingual-e5-small, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type='MEAN', normalize=True, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"cudagraph_capture_sizes\": [256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 256}, use_cached_outputs=False, \nINFO 05-27 12:32:23 [cuda.py:292] Using Flash Attention backend.\nINFO 05-27 12:32:23 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nINFO 05-27 12:32:23 [model_runner.py:1170] Starting to load model intfloat/multilingual-e5-small...\nINFO 05-27 12:32:24 [weight_utils.py:291] Using model weights format ['*.safetensors']\nINFO 05-27 12:32:24 [weight_utils.py:344] No model.safetensors.index.json found in remote.\nINFO 05-27 12:32:24 [default_loader.py:280] Loading weights took 0.06 seconds\nINFO 05-27 12:32:24 [model_runner.py:1202] Model loading took 0.2204 GiB and 1.087685 seconds\n------------------------------------------------- Captured stderr call -------------------------------------------------\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 18.67it/s]\n\nAdding requests: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 66/66 [00:00<00:00, 11180.74it/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 66/66 [00:00<00:00, 2941.53it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n=============================================== short test summary info ================================================\nFAILED tests/model_executor/test_model_load_with_params.py::test_model_loading_with_params - AttributeError: 'PoolerConfig' object has no attribute 'pooling_norm'. Did you mean: 'pooling_type'?\nFAILED tests/model_executor/test_model_load_with_params.py::test_roberta_model_loading_with_params - AttributeError: 'PoolerConfig' object has no attribute 'pooling_norm'. Did you mean: 'pooling_type'?\n======================================== 2 failed, 1 passed in 64.00s (0:01:04) ========================================\n\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-05-27T10:01:18+00:00",
    "closed_at": "2025-05-28T05:42:56+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18757/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/18757"
  },
  {
    "number": 19859,
    "title": "[Doc]: Add list of commands for `vllm serve`",
    "body": "### \ud83d\udcda The doc issue\n\nIn previous versions of documentation there was list of available arguments for `vllm serve` (e.g.  which was very conviniet, but I can't find them for `v0.9.0`.\n\nIn previous versions of the documentation, there was a list of available arguments for `vllm serve`, such as in the version [`v0.8.5`](https://docs.vllm.ai/en/v0.8.5.post1/serving/openai_compatible_server.html#cli-reference)). This was very convenient, but I am unable to find this information for version `v0.9.0` and above\n\n### Suggest a potential alternative/fix\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2025-06-19T12:45:31+00:00",
    "closed_at": "2025-06-20T05:06:18+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19859/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19859"
  },
  {
    "number": 19574,
    "title": "[CI Failure]: lint-and-deploy: unexpected HTTP status 500",
    "body": "### Name of failing test\n\nlint-and-deploy\n\n### Basic information\n\n- [ ] Flaky test\n- [ ] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\nhttps://github.com/vllm-project/vllm/actions/runs/15618574945/job/43997475275?pr=19573\n\nhttps://github.com/vllm-project/vllm/actions/runs/15618287178/job/43996526081?pr=19572\n\n```bash\nRun helm/chart-testing-action@0d28d3144d3a25ea2cc349d6e59901c4ff469b3b\nRun sigstore/cosign-installer@dc72c7d5c4d10cd6bcb8cf6e3fd625a9e5e537da\nRun #!/bin/bash\nINFO: Downloading bootstrap version 'v2.4.1' of cosign to verify version to be installed...\n      https://github.com/sigstore/cosign/releases/download/v2.4.1/cosign-linux-amd64\nINFO: bootstrap version successfully verified and matches requested version so nothing else to do\nRun echo \"$HOME/.cosign\" >> $GITHUB_PATH\nRun cd $GITHUB_ACTION_PATH \\\nInstalling chart-testing v3.10.1...\nError: getting Rekor public keys: updating local metadata and targets: error updating to TUF remote mirror: tuf: failed to download timestamp.json: GET \"https://tuf-repo-cdn.sigstore.dev/timestamp.json\": unexpected HTTP status 500\nmain.go:74: error during command execution: getting Rekor public keys: updating local metadata and targets: error updating to TUF remote mirror: tuf: failed to download timestamp.json: GET \"https://tuf-repo-cdn.sigstore.dev/timestamp.json\": unexpected HTTP status 500\nError: Process completed with exit code 1.\n```\n\n### \ud83d\udcdd History of failing test\n\nJust one hour ago\n\n### CC List.\n\n_No response_",
    "labels": [
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-06-12T18:52:35+00:00",
    "closed_at": "2025-06-13T14:37:47+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19574/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19574"
  },
  {
    "number": 2413,
    "title": "out of memory running mixtral gptq model in vllm 0.2.7",
    "body": "\r\nhttps://github.com/vllm-project/vllm/assets/39525455/c6787334-8a22-4dd4-838a-9fff1a1e0a38\r\n\r\nThe model downloaded from https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ and it works well in vllm 0.2.6 but run out of memory using 0.2.7.",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-11T03:57:46+00:00",
    "closed_at": "2024-01-11T13:34:33+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2413/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2413"
  },
  {
    "number": 12810,
    "title": "[V1]: Stuck at \"Automatically detected platform cuda\" when using V1 serving llava-next",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Debian GNU/Linux 12 (bookworm) (x86_64)\nGCC version: (Debian 12.2.0-14) 12.2.0\nClang version: Could not collect\nCMake version: version 3.25.1\nLibc version: glibc-2.36\n\nPython version: 3.11.2 (main, May  2 2024, 11:59:08) [GCC 12.2.0] (64-bit runtime)\nPython platform: Linux-5.4.210.bsk.6-amd64-x86_64-with-glibc2.36\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-80GB\nNvidia driver version: 535.161.08\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.4.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   46 bits physical, 57 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          128\nOn-line CPU(s) list:             0-127\nVendor ID:                       GenuineIntel\nModel name:                      Intel(R) Xeon(R) Platinum 8336C CPU @ 2.30GHz\nCPU family:                      6\nModel:                           106\nThread(s) per core:              2\nCore(s) per socket:              32\nSocket(s):                       2\nStepping:                        6\nCPU(s) scaling MHz:              86%\nCPU max MHz:                     3500.0000\nCPU min MHz:                     800.0000\nBogoMIPS:                        4600.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig flush_l1d arch_capabilities\nVirtualization:                  VT-x\nL1d cache:                       3 MiB (64 instances)\nL1i cache:                       2 MiB (64 instances)\nL2 cache:                        80 MiB (64 instances)\nL3 cache:                        108 MiB (2 instances)\nNUMA node(s):                    2\nNUMA node0 CPU(s):               0-31,64-95\nNUMA node1 CPU(s):               32-63,96-127\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\n\nVersions of relevant libraries:\n[pip3] byted-torch==2.4.1.post4\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.2\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     SYS     NODE    PXB     32-63,96-127    1               N/A\nNIC0    SYS      X      NODE    SYS     SYS\nNIC1    SYS     NODE     X      SYS     SYS\nNIC2    NODE    SYS     SYS      X      NODE\nNIC3    PXB     SYS     SYS     NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n\nNVIDIA_VISIBLE_DEVICES=GPU-c92d2497-8c4f-b7e1-71c8-fec251663632\nNVIDIA_REQUIRE_CUDA=cuda>=12.2 brand=tesla,driver>=450,driver<451 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=510,driver<511 brand=unknown,driver>=510,driver<511 brand=nvidia,driver>=510,driver<511 brand=nvidiartx,driver>=510,driver<511 brand=geforce,driver>=510,driver<511 brand=geforcertx,driver>=510,driver<511 brand=quadro,driver>=510,driver<511 brand=quadrortx,driver>=510,driver<511 brand=titan,driver>=510,driver<511 brand=titanrtx,driver>=510,driver<511 brand=tesla,driver>=515,driver<516 brand=unknown,driver>=515,driver<516 brand=nvidia,driver>=515,driver<516 brand=nvidiartx,driver>=515,driver<516 brand=geforce,driver>=515,driver<516 brand=geforcertx,driver>=515,driver<516 brand=quadro,driver>=515,driver<516 brand=quadrortx,driver>=515,driver<516 brand=titan,driver>=515,driver<516 brand=titanrtx,driver>=515,driver<516 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\nNCCL_NET_PLUGIN=none\nNCCL_SOCKET_IFNAME==eth0\nNCCL_DEBUG_SUBSYS=INIT,ENV,GRAPH\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNCCL_DEBUG=INFO\nNCCL_IB_HCA=mlx5\nNCCL_IB_GID_INDEX=3\nNCCL_SOCKET_FAMILY=AF_INET6\nNCCL_IB_TIMEOUT=23\nLD_LIBRARY_PATH=/home/tiger/.local/lib/python3.11/site-packages/cv2/../../lib64:/opt/tiger/native_libhdfs/lib/native:/opt/tiger/jdk/jdk8u265-b01/jre/lib/amd64/server:/opt/tiger/yarn_deploy/hadoop_current/lib/native:/opt/tiger/yarn_deploy/hadoop_current/lib/native/ufs:/opt/tiger/yarn_deploy/hadoop/lib/native:/opt/tiger/yarn_deploy/hadoop_current/lib/native:/usr/lib/x86_64-linux-gnu/openmpi:/usr/lib:/lib64:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64\nNCCL_IB_DISABLE=0\nOMP_NUM_THREADS=15\nCUDA_HOME=/usr/local/cuda\nCUDA_HOME=/usr/local/cuda\nNCCL_IB_RETRY_CNT=7\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n```python\nmodel_path=\"./llava-v1.6-mistral-7b-hf\"\nmodel = LLM(\n    model=model_path,\n    trust_remote_code=True, \n    max_model_len=8192,  # Otherwise, it may not fit in smaller GPUs\n    limit_mm_per_prompt={\"image\": 3},\n    gpu_memory_utilization=0.8,\n    enable_prefix_caching=True\n)\n```\n\ncmd:\nVLLM_ENABLE_V1_MULTIPROCESSING=1 VLLM_USE_V1=1 bash bootstrap.sh\n\nget stuck at:\n```\n[2025-02-06 06:30:46 +0000] [59332] [INFO] Starting gunicorn 20.1.0\n[2025-02-06 06:30:46 +0000] [59332] [INFO] Listening at: http://[::]:10849 (59332)\n[2025-02-06 06:30:46 +0000] [59332] [INFO] Using worker: euler.worker.SyncWorker\n[2025-02-06 06:30:46 +0000] [59333] [INFO] Booting worker with pid: 59333\nINFO 02-06 06:30:51 __init__.py:183] Automatically detected platform cuda.\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-02-06T06:40:50+00:00",
    "closed_at": "2025-02-07T03:46:08+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12810/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12810"
  },
  {
    "number": 3170,
    "title": "why vllm==0.3.3 need to access google",
    "body": "![\u5fae\u4fe1\u56fe\u7247_20240304112403](https://github.com/vllm-project/vllm/assets/38678334/f21e1ec0-bd6f-4b26-aeee-d6e4e5822fc2)\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-03-04T03:24:43+00:00",
    "closed_at": "2024-03-04T19:17:14+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3170/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3170"
  },
  {
    "number": 9828,
    "title": "> **Bug**:Interesting finding: The official pip package v0.6.3 is broken. However, installing `https://vllm-wheels.s3.us-west-2.amazonaws.com/nightly/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl` fixes this issue. (`vLLM API server version 0.6.3.post2.dev139+g622b7ab9`)",
    "body": "              > Interesting finding: The official pip package v0.6.3 is broken. However, installing `https://vllm-wheels.s3.us-west-2.amazonaws.com/nightly/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl` fixes this issue. (`vLLM API server version 0.6.3.post2.dev139+g622b7ab9`)\r\n\r\n@SinanAkkoyun What does python 3.10.15 should install, seemly I meet the same issue, thanks a lot!!\r\n\r\n_Originally posted by @Wiselnn570 in https://github.com/vllm-project/vllm/issues/9732#issuecomment-2445843188_\r\n            ",
    "labels": [],
    "state": "closed",
    "created_at": "2024-10-30T04:58:44+00:00",
    "closed_at": "2024-10-31T04:47:05+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9828/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9828"
  },
  {
    "number": 442,
    "title": "docker env cannot launch openai.server",
    "body": "docker: `nvcr.io/nvidia/pytorch:22.12-py3`\r\ncommand: `python -m vllm.entrypoints.openai.api_server --host [0.0.0.0](http://0.0.0.0/) --port 8080`\r\nError:\r\n```\r\nroot@ip-172-31-1-200:/workspace# python -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 8080\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/entrypoints/openai/api_server.py\", line 17, in <module>\r\n    from fastchat.model.model_adapter import get_conversation_template\r\n  File \"/usr/local/lib/python3.8/dist-packages/fastchat/model/__init__.py\", line 1, in <module>\r\n    from fastchat.model.model_adapter import (\r\n  File \"/usr/local/lib/python3.8/dist-packages/fastchat/model/model_adapter.py\", line 13, in <module>\r\n    import accelerate\r\n  File \"/usr/local/lib/python3.8/dist-packages/accelerate/__init__.py\", line 3, in <module>\r\n    from .accelerator import Accelerator\r\n  File \"/usr/local/lib/python3.8/dist-packages/accelerate/accelerator.py\", line 34, in <module>\r\n    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\r\n  File \"/usr/local/lib/python3.8/dist-packages/accelerate/checkpointing.py\", line 24, in <module>\r\n    from .utils import (\r\n  File \"/usr/local/lib/python3.8/dist-packages/accelerate/utils/__init__.py\", line 112, in <module>\r\n    from .launch import (\r\n  File \"/usr/local/lib/python3.8/dist-packages/accelerate/utils/launch.py\", line 27, in <module>\r\n    from ..utils.other import merge_dicts\r\n  File \"/usr/local/lib/python3.8/dist-packages/accelerate/utils/other.py\", line 24, in <module>\r\n    from .transformer_engine import convert_model\r\n  File \"/usr/local/lib/python3.8/dist-packages/accelerate/utils/transformer_engine.py\", line 21, in <module>\r\n    import transformer_engine.pytorch as te\r\n  File \"/usr/local/lib/python3.8/dist-packages/transformer_engine/__init__.py\", line 7, in <module>\r\n    from . import pytorch\r\n  File \"/usr/local/lib/python3.8/dist-packages/transformer_engine/pytorch/__init__.py\", line 6, in <module>\r\n    from .module import LayerNormLinear\r\n  File \"/usr/local/lib/python3.8/dist-packages/transformer_engine/pytorch/module.py\", line 16, in <module>\r\n    import transformer_engine_extensions as tex\r\nImportError: /usr/local/lib/python3.8/dist-packages/transformer_engine_extensions.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail23torchInternalAssertFailEPKcS2_jS2_RKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-07-12T09:11:14+00:00",
    "closed_at": "2023-07-12T12:00:52+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/442/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/442"
  },
  {
    "number": 3165,
    "title": "Model support: starcoder2",
    "body": "[bigcode/starcoder2-15b](https://huggingface.co/bigcode/starcoder2-15b)",
    "labels": [],
    "state": "closed",
    "created_at": "2024-03-03T09:25:18+00:00",
    "closed_at": "2024-03-03T22:37:55+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3165/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3165"
  },
  {
    "number": 12834,
    "title": "[Feature]: Make `tool_choice` parameter optional when `--enable-auto-tool-choice` is passed",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nWhen using vLLM with `--enable-auto-tool-choice`, the `tool_choice` parameter is currently required in the request. If not specified, a ValueError is raised stating \"tool_choice must either be a named tool, 'auto', or 'none'\".\n\nThis differs from OpenAI's API behavior where `tool_choice` is optional and defaults to \"auto\" when tools are provided ([source](https://platform.openai.com/docs/guides/function-calling#tool-choice)). For better API compatibility and developer experience, I propose making the `tool_choice` parameter optional in vLLM with a default value of \"auto\" when `--enable-auto-tool-choice` is passed.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-02-06T14:22:03+00:00",
    "closed_at": "2025-02-06T23:51:15+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12834/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12834"
  },
  {
    "number": 2935,
    "title": "How can i get metrics like {gpu_cache_usage, cpu_cache_usage, time_to_first_tokens, time_per_output_tokens, time_per_output_tokens} when using offline inference ",
    "body": "Is there any metris api to call.\r\n\r\nFor example : (Wrong )\r\n\r\n    start = time.perf_counter()\r\n    # FIXME(woosuk): Do not use internal method.\r\n    llm._run_engine(use_tqdm=True)\r\n    end = time.perf_counter()\r\n    Stats = llm.llm_engine._get_stats()\r\n    llm.llm_engine.stat_logger._log_prometheus(Stats)\r\n    return end - start\r\n\r\nThanks a lot ",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-20T13:41:10+00:00",
    "closed_at": "2024-02-21T03:47:14+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2935/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2935"
  },
  {
    "number": 16813,
    "title": "[Bug]: ImportError: cannot import name 'get_scheduler_metadata' from 'vllm.vllm_flash_attn'",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\n\nERROR 04-18 10:16:33 [core.py:390] Traceback (most recent call last):\nERROR 04-18 10:16:33 [core.py:390]   File \"/home/user/vllm/vllm/v1/engine/core.py\", line 381, in run_engine_core\nERROR 04-18 10:16:33 [core.py:390]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 04-18 10:16:33 [core.py:390]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-18 10:16:33 [core.py:390]   File \"/home/user/vllm/vllm/v1/engine/core.py\", line 323, in __init__\nERROR 04-18 10:16:33 [core.py:390]     super().__init__(vllm_config, executor_class, log_stats,\nERROR 04-18 10:16:33 [core.py:390]   File \"/home/user/vllm/vllm/v1/engine/core.py\", line 63, in __init__\nERROR 04-18 10:16:33 [core.py:390]     self.model_executor = executor_class(vllm_config)\nERROR 04-18 10:16:33 [core.py:390]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-18 10:16:33 [core.py:390]   File \"/home/user/vllm/vllm/executor/executor_base.py\", line 52, in __init__\nERROR 04-18 10:16:33 [core.py:390]     self._init_executor()\nERROR 04-18 10:16:33 [core.py:390]   File \"/home/user/vllm/vllm/executor/uniproc_executor.py\", line 45, in _init_executor\nERROR 04-18 10:16:33 [core.py:390]     self.collective_rpc(\"init_worker\", args=([kwargs], ))\nERROR 04-18 10:16:33 [core.py:390]   File \"/home/user/vllm/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\nERROR 04-18 10:16:33 [core.py:390]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 04-18 10:16:33 [core.py:390]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-18 10:16:33 [core.py:390]   File \"/home/user/vllm/vllm/utils.py\", line 2428, in run_method\nERROR 04-18 10:16:33 [core.py:390]     return func(*args, **kwargs)\nERROR 04-18 10:16:33 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 04-18 10:16:33 [core.py:390]   File \"/home/user/vllm/vllm/worker/worker_base.py\", line 558, in init_worker\nERROR 04-18 10:16:33 [core.py:390]     worker_class = resolve_obj_by_qualname(\nERROR 04-18 10:16:33 [core.py:390]                    ^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-18 10:16:33 [core.py:390]   File \"/home/user/vllm/vllm/utils.py\", line 2059, in resolve_obj_by_qualname\nERROR 04-18 10:16:33 [core.py:390]     module = importlib.import_module(module_name)\nERROR 04-18 10:16:33 [core.py:390]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-18 10:16:33 [core.py:390]   File \"/home/reid/miniconda3/envs/vllm/lib/python3.12/importlib/__init__.py\", line 90, in import_module\nERROR 04-18 10:16:33 [core.py:390]     return _bootstrap._gcd_import(name[level:], package, level)\nERROR 04-18 10:16:33 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-18 10:16:33 [core.py:390]   File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\nERROR 04-18 10:16:33 [core.py:390]   File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\nERROR 04-18 10:16:33 [core.py:390]   File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\nERROR 04-18 10:16:33 [core.py:390]   File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\nERROR 04-18 10:16:33 [core.py:390]   File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\nERROR 04-18 10:16:33 [core.py:390]   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\nERROR 04-18 10:16:33 [core.py:390]   File \"/home/user/vllm/vllm/v1/worker/gpu_worker.py\", line 26, in <module>\nERROR 04-18 10:16:33 [core.py:390]     from vllm.v1.worker.gpu_model_runner import GPUModelRunner\nERROR 04-18 10:16:33 [core.py:390]   File \"/home/user/vllm/vllm/v1/worker/gpu_model_runner.py\", line 32, in <module>\nERROR 04-18 10:16:33 [core.py:390]     from vllm.v1.attention.backends.flash_attn import FlashAttentionMetadata\nERROR 04-18 10:16:33 [core.py:390]   File \"/home/user/vllm/vllm/v1/attention/backends/flash_attn.py\", line 26, in <module>\nERROR 04-18 10:16:33 [core.py:390]     from vllm.vllm_flash_attn import (flash_attn_varlen_func,\nERROR 04-18 10:16:33 [core.py:390] ImportError: cannot import name 'get_scheduler_metadata' from 'vllm.vllm_flash_attn' (/home/user/vllm/vllm/vllm_flash_attn/__init__.py)\nProcess EngineCore_0:\nTraceback (most recent call last):\n  File \"/home/reid/miniconda3/envs/vllm/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/home/reid/miniconda3/envs/vllm/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/user/vllm/vllm/v1/engine/core.py\", line 394, in run_engine_core\n    raise e\n  File \"/home/user/vllm/vllm/v1/engine/core.py\", line 381, in run_engine_core\n    engine_core = EngineCoreProc(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/vllm/vllm/v1/engine/core.py\", line 323, in __init__\n    super().__init__(vllm_config, executor_class, log_stats,\n  File \"/home/user/vllm/vllm/v1/engine/core.py\", line 63, in __init__\n    self.model_executor = executor_class(vllm_config)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/vllm/vllm/executor/executor_base.py\", line 52, in __init__\n    self._init_executor()\n  File \"/home/user/vllm/vllm/executor/uniproc_executor.py\", line 45, in _init_executor\n    self.collective_rpc(\"init_worker\", args=([kwargs], ))\n  File \"/home/user/vllm/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/vllm/vllm/utils.py\", line 2428, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/vllm/vllm/worker/worker_base.py\", line 558, in init_worker\n    worker_class = resolve_obj_by_qualname(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/vllm/vllm/utils.py\", line 2059, in resolve_obj_by_qualname\n    module = importlib.import_module(module_name)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/reid/miniconda3/envs/vllm/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \"/home/user/vllm/vllm/v1/worker/gpu_worker.py\", line 26, in <module>\n    from vllm.v1.worker.gpu_model_runner import GPUModelRunner\n  File \"/home/user/vllm/vllm/v1/worker/gpu_model_runner.py\", line 32, in <module>\n    from vllm.v1.attention.backends.flash_attn import FlashAttentionMetadata\n  File \"/home/user/vllm/vllm/v1/attention/backends/flash_attn.py\", line 26, in <module>\n    from vllm.vllm_flash_attn import (flash_attn_varlen_func,\nImportError: cannot import name 'get_scheduler_metadata' from 'vllm.vllm_flash_attn' (/home/user/vllm/vllm/vllm_flash_attn/__init__.py)\nTraceback (most recent call last):\n  File \"/home/reid/miniconda3/envs/vllm/bin/vllm\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/user/vllm/vllm/entrypoints/cli/main.py\", line 51, in main\n    args.dispatch_function(args)\n  File \"/home/user/vllm/vllm/entrypoints/cli/serve.py\", line 27, in cmd\n    uvloop.run(run_server(args))\n  File \"/home/reid/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/home/reid/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/home/reid/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/home/reid/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/home/user/vllm/vllm/entrypoints/openai/api_server.py\", line 1078, in run_server\n    async with build_async_engine_client(args) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/reid/miniconda3/envs/vllm/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/vllm/vllm/entrypoints/openai/api_server.py\", line 146, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/reid/miniconda3/envs/vllm/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/vllm/vllm/entrypoints/openai/api_server.py\", line 178, in build_async_engine_client_from_engine_args\n    async_llm = AsyncLLM.from_vllm_config(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/vllm/vllm/v1/engine/async_llm.py\", line 141, in from_vllm_config\n    return cls(\n           ^^^^\n  File \"/home/user/vllm/vllm/v1/engine/async_llm.py\", line 103, in __init__\n    self.engine_core = core_client_class(\n                       ^^^^^^^^^^^^^^^^^^\n  File \"/home/user/vllm/vllm/v1/engine/core_client.py\", line 620, in __init__\n    super().__init__(\n  File \"/home/user/vllm/vllm/v1/engine/core_client.py\", line 395, in __init__\n    self._wait_for_engine_startup()\n  File \"/home/user/vllm/vllm/v1/engine/core_client.py\", line 421, in _wait_for_engine_startup\n    raise RuntimeError(\"Engine core initialization failed. \"\nRuntimeError: Engine core initialization failed. See root cause above.\n\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nImportError: cannot import name 'get_scheduler_metadata' from 'vllm.vllm_flash_attn' \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-04-18T04:20:40+00:00",
    "closed_at": "2025-04-18T13:44:25+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16813/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16813"
  },
  {
    "number": 2016,
    "title": "'MistralConfig' object has no attribute 'num_local_experts'",
    "body": "I have the Transformers from git and also installed vllm from git. I am trying to get the model up and running: https://huggingface.co/DiscoResearch/DiscoLM-mixtral-8x7b-v2\r\n\r\n\r\n2023-12-11 10:12:18 | ERROR | stderr |   File \"/project/vllm_git/vllm/worker/model_runner.py\", line 36, in load_model\r\n2023-12-11 10:12:18 | ERROR | stderr |     self.model = get_model(self.model_config)\r\n2023-12-11 10:12:18 | ERROR | stderr |   File \"/project/vllm_git/vllm/model_executor/model_loader.py\", line 117, in get_model\r\n2023-12-11 10:12:18 | ERROR | stderr |     model = model_class(model_config.hf_config, linear_method)\r\n2023-12-11 10:12:18 | ERROR | stderr |   File \"/project/vllm_git/vllm/model_executor/models/mixtral.py\", line 469, in __init__\r\n2023-12-11 10:12:18 | ERROR | stderr |     self.layers = nn.ModuleList([\r\n2023-12-11 10:12:18 | ERROR | stderr |   File \"/project/vllm_git/vllm/model_executor/models/mixtral.py\", line 470, in <listcomp>\r\n2023-12-11 10:12:18 | ERROR | stderr |     MixtralDecoderLayer(config)\r\n2023-12-11 10:12:18 | ERROR | stderr |   File \"/project/vllm_git/vllm/model_executor/models/mixtral.py\", line 416, in __init__\r\n2023-12-11 10:12:18 | ERROR | stderr |     num_experts=config.num_local_experts,\r\n2023-12-11 10:12:18 | ERROR | stderr |   File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 265, in __getattribute__\r\n2023-12-11 10:12:18 | ERROR | stderr |     return super().__getattribute__(key)\r\n2023-12-11 10:12:18 | ERROR | stderr | AttributeError: 'MistralConfig' object has no attribute 'num_local_experts'",
    "labels": [],
    "state": "closed",
    "created_at": "2023-12-11T10:16:33+00:00",
    "closed_at": "2023-12-11T18:50:34+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2016/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2016"
  },
  {
    "number": 2640,
    "title": "Error in newest dev ver: Worker.__init__() got an unexpected keyword argument 'cache_config'",
    "body": "I build the newest master branch with #2279 commit.\r\nAnd I run the following command\r\n`python -m vllm.entrypoints.openai.api_server --model ./Mistral-7B-Instruct-v0.2-AWQ --quantization awq --dtype auto --host 0.0.0.0 --port 8081 --tensor-parallel-size 2`\r\nI meet the error:\r\n```\r\n\r\nINFO 01-29 09:41:47 api_server.py:209] args: Namespace(host='0.0.0.0', port=8081, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, root_path=None, middleware=[], model='./Mistral-7B-Instruct-v0.2-AWQ', tokenizer=None, revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, block_size=16, seed=0, swap_space=4, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_seqs=256, max_paddings=256, disable_log_stats=False, quantization='awq', enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\nWARNING 01-29 09:41:47 config.py:177] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n2024-01-29 09:41:49,090 INFO worker.py:1724 -- Started a local Ray instance.\r\nINFO 01-29 09:41:50 llm_engine.py:72] Initializing an LLM engine with config: model='./Mistral-7B-Instruct-v0.2-AWQ', tokenizer='./Mistral-7B-Instruct-v0.2-AWQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, seed=0)\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/home/my/vllm/vllm/entrypoints/openai/api_server.py\", line 217, in <module>\r\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/my/vllm/vllm/engine/async_llm_engine.py\", line 615, in from_engine_args\r\n    engine = cls(parallel_config.worker_use_ray,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/my/vllm/vllm/engine/async_llm_engine.py\", line 319, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/my/vllm/vllm/engine/async_llm_engine.py\", line 364, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/my/vllm/vllm/engine/llm_engine.py\", line 109, in __init__\r\n    self._init_workers_ray(placement_group)\r\n  File \"/home/my/vllm/vllm/engine/llm_engine.py\", line 260, in _init_workers_ray\r\n    self.driver_worker = Worker(\r\n                         ^^^^^^^\r\nTypeError: Worker.__init__() got an unexpected keyword argument 'cache_config'\r\n2024-01-29 09:41:54,676 ERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::RayWorkerVllm.init_worker() (pid=3160378, ip=10.20.4.57, actor_id=ca7bf2aa56e3f1a0c1a7678201000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7f043133e7d0>)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/my/vllm/vllm/engine/ray_utils.py\", line 23, in init_worker\r\n    self.worker = worker_init_fn()\r\n                  ^^^^^^^^^^^^^^^^\r\n  File \"/home/my/vllm/vllm/engine/llm_engine.py\", line 247, in <lambda>\r\n    lambda rank=rank, local_rank=local_rank: Worker(\r\n                                             ^^^^^^^\r\nTypeError: Worker.__init__() got an unexpected keyword argument 'cache_config'\r\n```\r\n\r\nI am running with `python=3.11`, `CUDA 12.1`, `driver 530` with 2x RTX 3090 NVLink.\r\nI notice there is a discussion (https://github.com/vllm-project/vllm/pull/2279#discussion_r1468772680) about `cache_config`, I am not sure whether it is related ",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-01-29T01:57:34+00:00",
    "closed_at": "2024-01-29T20:30:10+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2640/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2640"
  },
  {
    "number": 13938,
    "title": "[Feature]: Add a vllm help CLI command",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nIt would be helpful to have a command like vllm --help or vllm <subcommand> --help which provides a list of the available CLI commands and provides a brief explanation of how to use each command.\n\nFrom #13840, it is possible that vllm may have many new commands in the future, so having this functionality would greatly help discern each specific subcommand.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-02-27T02:52:39+00:00",
    "closed_at": "2025-02-27T04:43:44+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13938/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13938"
  },
  {
    "number": 3918,
    "title": "[Bug]: The performance for `Prefix Caching` is very un-stable for different requests !!!!",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: 2.1.2+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.27.6\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-4.18.0-240.el8.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA L20\r\nGPU 1: NVIDIA L20\r\nGPU 2: NVIDIA L20\r\nGPU 3: NVIDIA L20\r\nGPU 4: NVIDIA L20\r\nGPU 5: NVIDIA L20\r\nGPU 6: NVIDIA L20\r\nGPU 7: NVIDIA L20\r\n\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.5\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   52 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nVendor ID:                       GenuineIntel\r\nBIOS Vendor ID:                  Intel(R) Corporation\r\nModel name:                      Intel(R) Xeon(R) Gold 6430\r\nBIOS Model name:                 Intel(R) Xeon(R) Gold 6430\r\nCPU family:                      6\r\nModel:                           143\r\nThread(s) per core:              2\r\nCore(s) per socket:              32\r\nSocket(s):                       2\r\nStepping:                        8\r\nFrequency boost:                 enabled\r\nCPU max MHz:                     2101.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        4200.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid cldemote movdiri movdir64b md_clear pconfig flush_l1d arch_capabilities\r\nVirtualization:                  VT-x\r\nL1d cache:                       3 MiB (64 instances)\r\nL1i cache:                       2 MiB (64 instances)\r\nL2 cache:                        128 MiB (64 instances)\r\nL3 cache:                        120 MiB (2 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-31,64-95\r\nNUMA node1 CPU(s):               32-63,96-127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==0.991\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.22.2\r\n[pip3] onnx==1.14.0\r\n[pip3] pytorch-quantization==2.1.2\r\n[pip3] torch==2.1.2\r\n[pip3] torch-tensorrt==0.0.0\r\n[pip3] triton==2.1.0+e621604\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: 5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU1\tPIX\t X \tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU2\tPIX\tPIX\t X \tPIX\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU3\tPIX\tPIX\tPIX\t X \tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU4\tSYS\tSYS\tSYS\tSYS\t X \tPIX\tPIX\tPIX\t32-63,96-127\t1\t\tN/A\r\nGPU5\tSYS\tSYS\tSYS\tSYS\tPIX\t X \tPIX\tPIX\t32-63,96-127\t1\t\tN/A\r\nGPU6\tSYS\tSYS\tSYS\tSYS\tPIX\tPIX\t X \tPIX\t32-63,96-127\t1\t\tN/A\r\nGPU7\tSYS\tSYS\tSYS\tSYS\tPIX\tPIX\tPIX\t X \t32-63,96-127\t1\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nThe first token performance for `Prefix Caching` is very un-stable for different random requests, in my case, range from 50ms -> 500ms, and the mean time of first token vs `NO Prefix Caching` is: 337ms vs 390ms, only 60ms boost.\r\n```bash\r\n[I] [1/20] 5.00% output_len: 484,  first_token: 58.25 ms,  request_latency: 25.75 s\r\n[I] [2/20] 10.00% output_len: 493,  first_token: 88.23 ms,  request_latency: 26.22 s\r\n[I] [3/20] 15.00% output_len: 489,  first_token: 468.82 ms,  request_latency: 26.49 s\r\n[I] [4/20] 20.00% output_len: 480,  first_token: 348.85 ms,  request_latency: 26.23 s\r\n[I] [5/20] 25.00% output_len: 499,  first_token: 470.80 ms,  request_latency: 26.09 s\r\n[I] [6/20] 30.00% output_len: 499,  first_token: 430.00 ms,  request_latency: 25.74 s\r\n[I] [7/20] 35.00% output_len: 488,  first_token: 432.71 ms,  request_latency: 25.43 s\r\n[I] [8/20] 40.00% output_len: 495,  first_token: 481.83 ms,  request_latency: 25.06 s\r\n[I] [9/20] 45.00% output_len: 475,  first_token: 495.65 ms,  request_latency: 24.66 s\r\n[I] [10/20] 50.00% output_len: 494,  first_token: 494.50 ms,  request_latency: 24.21 s\r\n[I] [11/20] 55.00% output_len: 500,  first_token: 70.87 ms,  request_latency: 23.76 s\r\n[I] [12/20] 60.00% output_len: 497,  first_token: 255.73 ms,  request_latency: 23.71 s\r\n[I] [13/20] 65.00% output_len: 500,  first_token: 455.26 ms,  request_latency: 23.44 s\r\n[I] [14/20] 70.00% output_len: 500,  first_token: 294.82 ms,  request_latency: 22.94 s\r\n[I] [15/20] 75.00% output_len: 500,  first_token: 285.17 ms,  request_latency: 22.59 s\r\n[I] [16/20] 80.00% output_len: 482,  first_token: 450.05 ms,  request_latency: 22.26 s\r\n[I] [17/20] 85.00% output_len: 489,  first_token: 59.68 ms,  request_latency: 21.71 s\r\n[I] [18/20] 90.00% output_len: 466,  first_token: 686.31 ms,  request_latency: 21.44 s\r\n[I] [19/20] 95.00% output_len: 498,  first_token: 76.72 ms,  request_latency: 20.70 s\r\n[I] [20/20] 100.00% output_len: 491,  first_token: 344.26 ms,  request_latency: 20.34 s\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-04-08T12:17:19+00:00",
    "closed_at": "2024-04-09T06:08:21+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3918/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3918"
  },
  {
    "number": 2704,
    "title": "the syntax in serving_completion.py is not compatible in python3.8",
    "body": "in #2529 @simon-mo introduce some python typing syntax, which is not compatible in python3.8\r\n\r\nlike  TypeTokenIDs = list[int] in https://github.com/vllm-project/vllm/blob/93b38bea5dd03e1b140ca997dfaadef86f8f1855/vllm/entrypoints/openai/serving_completion.py#L22C1-L22C25\r\n\r\nwhich should be TypeTokenIDs=List[int] in python3.8.\r\n\r\ncould you please fix it?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-01T06:14:36+00:00",
    "closed_at": "2024-02-01T22:00:59+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2704/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2704"
  },
  {
    "number": 13099,
    "title": "[Bug]: Is vllm still support passing max_pixels and min_pixels for Qwen2VL?",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\nINFO 02-12 00:04:31 __init__.py:190] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-43-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: NVIDIA A800 80GB PCIe\nGPU 1: NVIDIA A800 80GB PCIe\nGPU 2: NVIDIA A800 80GB PCIe\nGPU 3: NVIDIA A800 80GB PCIe\n\nNvidia driver version: 535.86.05\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   52 bits physical, 57 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          160\nOn-line CPU(s) list:             0-159\nVendor ID:                       GenuineIntel\nModel name:                      Intel(R) Xeon(R) Platinum 8383C CPU @ 2.70GHz\nCPU family:                      6\nModel:                           106\nThread(s) per core:              2\nCore(s) per socket:              40\nSocket(s):                       2\nStepping:                        6\nCPU max MHz:                     3600.0000\nCPU min MHz:                     800.0000\nBogoMIPS:                        5400.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\nVirtualization:                  VT-x\nL1d cache:                       3.8 MiB (80 instances)\nL1i cache:                       2.5 MiB (80 instances)\nL2 cache:                        100 MiB (80 instances)\nL3 cache:                        120 MiB (2 instances)\nNUMA node(s):                    2\nNUMA node0 CPU(s):               0-39,80-119\nNUMA node1 CPU(s):               40-79,120-159\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\n\nVersions of relevant libraries:\n[pip3] flake8==7.1.1\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.3\n[pip3] triton==3.1.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-ml-py              12.570.86                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.2.1                   pypi_0    pypi\n[conda] torch                     2.5.1                    pypi_0    pypi\n[conda] torchaudio                2.5.1                    pypi_0    pypi\n[conda] torchvision               0.20.1                   pypi_0    pypi\n[conda] transformers              4.48.3                   pypi_0    pypi\n[conda] triton                    3.1.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PXB     SYS     SYS     0-39,80-119     0               N/A\nGPU1    PXB      X      SYS     SYS     0-39,80-119     0               N/A\nGPU2    SYS     SYS      X      PXB     40-79,120-159   1               N/A\nGPU3    SYS     SYS     PXB      X      40-79,120-159   1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nCUDA_VISIBLE_DEVICES=0,1\nCUDA_VISIBLE_DEVICES=0,1\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n### \ud83d\udc1b Describe the bug\n\nSummary: Setting Qwen2VL image_processor parameters like max_pixels and min_pixels is not available for vllm==0.7.2.\n\nDetails: \nI find a bug when run my code to generate candidates for GRPO algorithms with vllm. \nThe model is init as:\n```\nself.llm = LLM(\n    model=model.name_or_path,\n    device=vllm_device,\n    gpu_memory_utilization=self.args.vllm_gpu_memory_utilization,\n    dtype=torch.bfloat16,\n    # Automatic Prefix Caching caches the KV cache of existing queries, so that a new query can\n    # directly reuse the KV cache if it shares the same prefix with one of the existing queries.\n    # This is particularly useful here because we generate completions from the same prompts.\n    enable_prefix_caching=True,\n    enforce_eager=True,\n    max_model_len=2048,\n)\n```\nand generate with\uff1a\n```\nprompts_text = [\n    maybe_apply_chat_template(example, self.processing_class)[\"prompt\"]\n    for example in inputs\n]\nall_prompts_text = gather_object(prompts_text)  # something like <visual_start> <image_pad>....<image_pad> <visual_end> <text here>\nall_images = gather_object(images)   # Image\n\nall_multimodal_inputs = [\n    {\"prompt\": p, \"multi_modal_data\": {\"image\": i} }\n    for p, i in zip(all_prompts_text, all_images)\n]\n\noutputs = self.llm.generate(\n    all_multimodal_inputs,\n    sampling_params=self.sampling_params,\n    use_tqdm=False,\n)\ncompletion_ids = [\n    out.token_ids\n    for completions in outputs\n    for out in completions.outputs\n]\n```\nThe prompts_text is applied chat templates and the number of <image_pad> is under the setting of max_pixel = 768 * 768. However, the self.llm use the default setting and make the errors:\n```\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"LLM_RL_works/open-r1-multimodal/src/open_r1/grpo.py\", line 207, in <module>\n[rank0]:     main(script_args, training_args, model_args)\n[rank0]:   File \"LLM_RL_works/open-r1-multimodal/src/open_r1/grpo.py\", line 196, in main\n[rank0]:     trainer.train()\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/transformers/trainer.py\", line 2171, in train\n[rank0]:     return inner_training_loop(\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/transformers/trainer.py\", line 2531, in _inner_training_loop\n[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/transformers/trainer.py\", line 3669, in training_step\n[rank0]:     inputs = self._prepare_inputs(inputs)\n[rank0]:   File \"LLM_RL_works/open-r1-multimodal/src/open_r1/trainer/vllm_grpo_trainer.py\", line 553, in _prepare_inputs\n[rank0]:     outputs = self.llm.generate(\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/vllm/utils.py\", line 1086, in inner\n[rank0]:     return fn(*args, **kwargs)\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 469, in generate\n[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 1390, in _run_engine\n[rank0]:     step_outputs = self.llm_engine.step()\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 1386, in step\n[rank0]:     outputs = self.model_executor.execute_model(\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 138, in execute_model\n[rank0]:     output = self.collective_rpc(\"execute_model\",\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 51, in collective_rpc\n[rank0]:     answer = run_method(self.driver_worker, method, args, kwargs)\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/vllm/utils.py\", line 2220, in run_method\n[rank0]:     return func(*args, **kwargs)\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 413, in execute_model\n[rank0]:     output = self.model_runner.execute_model(\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[rank0]:     return func(*args, **kwargs)\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1719, in execute_model\n[rank0]:     hidden_or_intermediate_states = model_executable(\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/vllm/model_executor/models/qwen2_vl.py\", line 1347, in forward\n[rank0]:     inputs_embeds = self.get_input_embeddings_v0(\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/vllm/model_executor/models/qwen2_vl.py\", line 1283, in get_input_embeddings_v0\n[rank0]:     inputs_embeds = merge_multimodal_embeddings(\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 455, in merge_multimodal_embeddings\n[rank0]:     return _merge_multimodal_embeddings(\n[rank0]:   File \"env/openr1_multimodal/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 371, in _merge_multimodal_embeddings\n[rank0]:     raise ValueError(\n[rank0]: ValueError: Attempted to assign 1369 + 1369 = 2738 multimodal tokens to 3704 placeholders\n```\nSo I want to align the vllm version Qwen2VL as in https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/vision_language.py.\nBut I failed: WARNING 02-11 23:25:25 utils.py:1474] The following intended overrides are not keyword-only args and and will be dropped.\n\nAnd I further set in inference time as in https://github.com/vllm-project/vllm/pull/9612:\n```\nall_multimodal_inputs = [\n    {\"prompt\": p, \"multi_modal_data\": {\"image\": i}, \n    ############ I change here.  ##############\n    \"mm_processor_kwargs\": { \"max_pixels\": self._max_pixels, \"min_pixels\": self._min_pixels} }\n    for p, i in zip(all_prompts_text, all_images)\n]\n\noutputs = self.llm.generate(\n    all_multimodal_inputs,\n    sampling_params=self.sampling_params,\n    use_tqdm=False,\n)\ncompletion_ids = [\n    out.token_ids\n    for completions in outputs\n    for out in completions.outputs\n]\n```\nIt still does not work: Keyword argument max_pixels is not a valid argument for this processor and will be ignored.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-02-11T16:21:43+00:00",
    "closed_at": "2025-02-12T11:55:25+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13099/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13099"
  },
  {
    "number": 4964,
    "title": "[Usage]: vllm inferring gemma7B is still very slow",
    "body": "### Your current environment\n\nwhy  gemma7b using vllm inference is still slow? any params to set to improve? hope your help ~~\r\n\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\r\n",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-05-22T02:07:37+00:00",
    "closed_at": "2024-05-22T07:13:44+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4964/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4964"
  },
  {
    "number": 3929,
    "title": "[Misc]: page attention v2",
    "body": "### Anything you want to discuss about vllm.\n\nCan VLLM's page attention v2 be understood as incorporating the implementation of flash decoding",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-04-09T07:40:04+00:00",
    "closed_at": "2024-04-10T06:27:48+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3929/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3929"
  },
  {
    "number": 7463,
    "title": "[Usage]: openai.APIStatusError: Error code: 405 - {'detail': 'Method Not Allowed'}",
    "body": "### Your current environment\r\n\r\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\r\n\r\n### How would you like to use vllm\r\n\r\nI run `vllm serve /mnt/datastore/shared/model-fp8 --max-model-len 16384 --tensor-parallel-size 8 --gpu-memory-utilization 0.95 --served-model-name model-v2-405b-e4`\r\n\r\nBut then I get `openai.APIStatusError: Error code: 405 - {'detail': 'Method Not Allowed'}`\r\n\r\nI get this only with the chat.completions api from oai lib. Text completion api works fine..",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-08-13T07:06:53+00:00",
    "closed_at": "2024-08-13T15:07:20+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7463/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7463"
  },
  {
    "number": 16407,
    "title": "[Feature]: vLLM support for Granite Rapids (GNR - Intel Xeon 6th Gen)",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nIntel Xeon Granite Rapids (GNR) is the 6th Generation Intel Xeon System. I was trying to run vLLM on bare metal which is not working giving an error \"No platform detected, vLLM is running on UnspecifiedPlatform\". This is a new hardware platform (x86) from Intel. Not sure when the support will be provided.\n\n### Alternatives\n\nWe can host the model through some custom code and Flask as a temporary solution. This might not get the feature of adaptive batching and scalability required. Added to that multiple Flask instances will have to be locally load balanced using NGINX.\n\n### Additional context\n\nvllm serve mistralai/Mistral-7B-Instruct-v0.3\nINFO 04-10 10:50:47 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform\nINFO 04-10 10:50:48 [api_server.py:981] vLLM API server version 0.8.2\nINFO 04-10 10:50:48 [api_server.py:982] args: Namespace(subparser='serve', model_tag='mistralai/Mistral-7B-Instruct-v0.3', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='mistralai/Mistral-7B-Instruct-v0.3', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x70248478cea0>)\nTraceback (most recent call last):\n  File \"/home/devcloud/mistral/vllm_env/bin/vllm\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/devcloud/mistral/vllm_env/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py\", line 75, in main\n    args.dispatch_function(args)\n  File \"/home/devcloud/mistral/vllm_env/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py\", line 33, in cmd\n    uvloop.run(run_server(args))\n  File \"/home/devcloud/mistral/vllm_env/lib/python3.12/site-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/home/devcloud/mistral/vllm_env/lib/python3.12/site-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/home/devcloud/mistral/vllm_env/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 1016, in run_server\n    async with build_async_engine_client(args) as engine_client:\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devcloud/mistral/vllm_env/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 141, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devcloud/mistral/vllm_env/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 161, in build_async_engine_client_from_engine_args\n    vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devcloud/mistral/vllm_env/lib/python3.12/site-packages/vllm/engine/arg_utils.py\", line 1295, in create_engine_config\n    device_config = DeviceConfig(device=self.device)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/devcloud/mistral/vllm_env/lib/python3.12/site-packages/vllm/config.py\", line 1802, in __init__\n    raise RuntimeError(\"Failed to infer device type\")\nRuntimeError: Failed to infer device type\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-04-10T11:04:06+00:00",
    "closed_at": "2025-04-10T14:54:51+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16407/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16407"
  },
  {
    "number": 9100,
    "title": "[Bug]: Try-catch conditions are incorrect to import correct  ROCm Flash Attention Backend in Draft Model",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nI found an issue running draft model speculative decoding on AMD platform, the issue arised from  `vllm/spec_decode/draft_model_runner.py`\r\n```\r\ntry:\r\n    from vllm.attention.backends.flash_attn import FlashAttentionMetadata # this is throwing ImportError rather than ModuleNotFoundError\r\nexcept ModuleNotFoundError:\r\n    # vllm_flash_attn is not installed, use the identical ROCm FA metadata\r\n    from vllm.attention.backends.rocm_flash_attn import (\r\n        ROCmFlashAttentionMetadata as FlashAttentionMetadata)\r\n```\r\n\r\nWithin the try-catch block `ImportError` is thrown rather than `ModuleNotFoundError`\r\n\r\n```\r\n  File \"/home/aac/apps/rocm611-0929/vllm-fix-spec-amd/vllm/engine/multiprocessing/engine.py\", line 78, in __init__                          \r\n    self.engine = LLMEngine(*args,                                                                                                          \r\n  File \"/home/aac/apps/rocm611-0929/vllm-fix-spec-amd/vllm/engine/llm_engine.py\", line 335, in __init__                                     \r\n    self.model_executor = executor_class(                                                                                                   \r\n  File \"/home/aac/apps/rocm611-0929/vllm-fix-spec-amd/vllm/executor/distributed_gpu_executor.py\", line 26, in __init__                      \r\n    super().__init__(*args, **kwargs)                                                                                                       \r\n  File \"/home/aac/apps/rocm611-0929/vllm-fix-spec-amd/vllm/executor/executor_base.py\", line 47, in __init__                                 \r\n    self._init_executor()                                                                                                                   \r\n  File \"/home/aac/apps/rocm611-0929/vllm-fix-spec-amd/vllm/executor/multiproc_gpu_executor.py\", line 108, in _init_executor                 \r\n    self.driver_worker = self._create_worker(                                                                                               \r\n  File \"/home/aac/apps/rocm611-0929/vllm-fix-spec-amd/vllm/executor/gpu_executor.py\", line 105, in _create_worker                           \r\n    return create_worker(**self._get_create_worker_kwargs(                                                                                  \r\n  File \"/home/aac/apps/rocm611-0929/vllm-fix-spec-amd/vllm/executor/gpu_executor.py\", line 24, in create_worker                             \r\n    wrapper.init_worker(**kwargs)                                                                                                           \r\n  File \"/home/aac/apps/rocm611-0929/vllm-fix-spec-amd/vllm/worker/worker_base.py\", line 446, in init_worker                                 \r\n    mod = importlib.import_module(self.worker_module_name)                                                                                  \r\n  File \"/home/aac/anaconda3/envs/rocm611-0929/lib/python3.9/importlib/__init__.py\", line 127, in import_module                              \r\n    return _bootstrap._gcd_import(name[level:], package, level)                                                                             \r\n  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import                                                                           \r\n  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load                                                                        \r\n  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked                                                                \r\n  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked                                                                         \r\n  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module                                                                   \r\n  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed                                                              \r\n  File \"/home/aac/apps/rocm611-0929/vllm-fix-spec-amd/vllm/spec_decode/spec_decode_worker.py\", line 21, in <module>                         \r\n    from vllm.spec_decode.draft_model_runner import TP1DraftModelRunner\r\n  File \"/home/aac/apps/rocm611-0929/vllm-fix-spec-amd/vllm/spec_decode/draft_model_runner.py\", line 9, in <module>\r\n    from vllm.attention.backends.flash_attn import FlashAttentionMetadata\r\n  File \"/home/aac/apps/rocm611-0929/vllm-fix-spec-amd/vllm/attention/backends/flash_attn.py\", line 23, in <module>\r\n    from vllm.vllm_flash_attn import (flash_attn_varlen_func,                                                                               \r\nImportError: cannot import name 'flash_attn_varlen_func' from 'vllm.vllm_flash_attn' (unknown location)\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-10-06T01:19:09+00:00",
    "closed_at": "2024-10-06T05:00:05+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9100/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/9100"
  },
  {
    "number": 1291,
    "title": "ImportError: /opt/vllm/vllm/attention_ops.cpython-39-x86_64-linux-gnu.so: undefined symbol: _ZNK3c1010TensorImpl27throw_data_ptr_access_errorEv",
    "body": "Excuse me, why is this mistake?\r\n\r\n![image](https://github.com/vllm-project/vllm/assets/22927505/d676ff76-2745-4a74-8cbd-b3ac248e9a38)\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-10-08T03:15:41+00:00",
    "closed_at": "2023-10-08T06:36:01+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1291/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1291"
  },
  {
    "number": 2115,
    "title": "pytest tests/models fail",
    "body": "Running the following command on an H100 computer\r\n\r\n```\r\npytest tests/models/\r\n```\r\n\r\ngave me the following errors:\r\n\r\n```\r\nFAILED tests/models/test_models.py::test_models[128-half-mistralai/Mistral-7B-v0.1] - AssertionError: tensor model parallel group is already initialized\r\nFAILED tests/models/test_models.py::test_models[128-half-tiiuae/falcon-7b] - AssertionError: tensor model parallel group is already initialized\r\nFAILED tests/models/test_models.py::test_models[128-half-gpt2] - AssertionError: tensor model parallel group is already initialized\r\nFAILED tests/models/test_models.py::test_models[128-half-bigcode/tiny_starcoder_py] - AssertionError: tensor model parallel group is already initialized\r\nFAILED tests/models/test_models.py::test_models[128-half-EleutherAI/gpt-j-6b] - AssertionError: tensor model parallel group is already initialized\r\nFAILED tests/models/test_models.py::test_models[128-half-EleutherAI/pythia-70m] - AssertionError: tensor model parallel group is already initialized\r\nFAILED tests/models/test_models.py::test_models[128-half-bigscience/bloom-560m] - AssertionError: tensor model parallel group is already initialized\r\nFAILED tests/models/test_models.py::test_models[128-half-mosaicml/mpt-7b] - AssertionError: tensor model parallel group is already initialized\r\nFAILED tests/models/test_models.py::test_models[128-half-microsoft/phi-1_5] - AssertionError: tensor model parallel group is already initialized\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-12-14T22:39:49+00:00",
    "closed_at": "2023-12-15T05:00:35+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2115/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2115"
  },
  {
    "number": 1519,
    "title": "Error:  When using OpenAI-Compatible Server, the server is available but cannot be accessed from the same terminal.",
    "body": "I'm using this perfect framework to build up an on-air api server for my local LLM. Specifically, I had run this command in my linux terminal:\r\n`python -m vllm.entrypoints.openai.api_server    --model /home/XXX/baichuan2    --trust-remote-code    --tensor-parallel-size 1    --host 10.201.1.181    --port 8000`\r\nNote that /baichuan2 is a directory containing the fine-tuned model derived from baichuan-inc/Baichuan2-13B-Base.\r\n\r\nAnd I get:\r\n`INFO:     Started server process [448902]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://10.201.1.181:8000 (Press CTRL+C to quit)`\r\n\r\nTo test whether the server is ready or not, I used `telnet 10.201.1.181 8000` in another shell, which returned \r\n`Trying 10.201.1.181...\r\nConnected to 10.201.1.181.\r\nEscape character is '^]'.`\r\nSo I suppose the server is running properly.\r\n\r\nBut when I attempt to call this server via python scripts as documented :\r\n`import openai\r\n\r\nopenai.api_key = \"0.0.0.0\"\r\n\r\nopenai.api_base = 'http://10.201.1.181:8000/v1'\r\n\r\nresponse = openai.Completion.create(model='/home/XXX/baichuan2', prompt=prompt, temperature = 0.8, do_sample = True, max_tokens = 512)\r\n`\r\n\r\nThe program will report errors after a long-time pausing with a html code in the end indicating 502 error: \r\n\r\n\r\n`Traceback (most recent call last):\r\n  File \"/home/XXX/anaconda3/lib/python3.11/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\r\n    data = json.loads(rbody)\r\n           ^^^^^^^^^^^^^^^^^\r\n  File \"/home/XXX/anaconda3/lib/python3.11/json/__init__.py\", line 346, in loads\r\n    return _default_decoder.decode(s)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/XXX/anaconda3/lib/python3.11/json/decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/XXX/anaconda3/lib/python3.11/json/decoder.py\", line 355, in raw_decode\r\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/XXX/ChatTU/main.py\", line 31, in <module>\r\n    main(test_mode = test_mode, fixed_label = args.fixed_label, query = query, ip = ip)\r\n  File \"/home/XXX/ChatTU/main.py\", line 17, in main\r\n    Conversation_Chain(test_mode = test_mode, fixed_label = fixed_label,  query = query, ip = ip)\r\n  File \"/home/XXX/ChatTU/SourceCode/Customer_Service.py\", line 164, in Conversation_Chain\r\n    response = local_llm.chat(args = args)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/XXX/ChatTU/LLM/Local_LLM.py\", line 172, in chat\r\n    response = openai.Completion.create(model='/home/XXX/baichuan2', prompt=prompt, temperature = 0.8, do_sample = True, max_tokens = 512)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/XXX/anaconda3/lib/python3.11/site-packages/openai/api_resources/completion.py\", line 25, in create\r\n    return super().create(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/XXX/anaconda3/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\r\n    response, _, api_key = requestor.request(\r\n                           ^^^^^^^^^^^^^^^^^^\r\n  File \"/home/XXX/anaconda3/lib/python3.11/site-packages/openai/api_requestor.py\", line 299, in request\r\n    resp, got_stream = self._interpret_response(result, stream)\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/XXX/anaconda3/lib/python3.11/site-packages/openai/api_requestor.py\", line 710, in _interpret_response\r\n    self._interpret_response_line(\r\n  File \"/home/XXX/anaconda3/lib/python3.11/site-packages/openai/api_requestor.py\", line 767, in _interpret_response_line\r\n    raise error.APIError(\r\nopenai.error.APIError: HTTP code 502 from API (<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01//EN\" \"http://www.w3.org/TR/html4/strict.dtd\"`\r\n\r\n\r\n(I didn't post that html code since it's way too long.)\r\n\r\nBesides, my LLM is supported by this framework since \r\n`from vllm import LLM\r\n\r\nllm = LLM(model='/home/XXX/baichuan2', trust_remote_code = True)  # Name or path of your model\r\noutput = llm.generate(\"Hello, my name is\")\r\nprint(output)`\r\nworks fine.\r\n\r\nI'm quiet confused about this problem. But several days earlier I had successfully depolyed an api server for the same LLM on a different machine, using the exact same method like this one, with the only difference being the host. I'm wondering what's wrong with my code and will be extremely grateful if anyone would like to offer your invaluable assistance.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-10-31T09:58:04+00:00",
    "closed_at": "2023-10-31T19:55:42+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1519/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1519"
  },
  {
    "number": 15610,
    "title": "[Bug]: The content is empty after gemma3 is deployed on the T4 graphics card to send request inference",
    "body": "The environment is two T4 cards and the operating system is centos7\n### \ud83d\udc1b Describe the bug\n\nstart command\n\nnohup vllm serve /data1/model/LLM-Research/gemma-3-4b-it --tensor-parallel-size 2 --pipeline-parallel-size 1 --max-model-len 4096 --host 0.0.0.0 --dtype float16 --port 8000 --trust-remote-code --served-model-name gemma-3-4b-it --max-num-batched-tokens 4096 --gpu-memory-utilization 0.7 > vllm.log 2>&1 &\n\nHere is the calling procedure\n```python\nfrom openai import OpenAI\nimport base64\n\n# \u914d\u7f6e API\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://127.0.0.1:8000/v1\"\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\n# \u6253\u5f00\u672c\u5730\u56fe\u7247\u6587\u4ef6\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3aBase64\u7f16\u7801\u7684\u5b57\u7b26\u4e32\nwith open('1.jpg', 'rb') as file:\n    image = \"data:image/jpeg;base64,\" + base64.b64encode(file.read()).decode('utf-8')\n\n# \u4f7f\u7528\u5ba2\u6237\u7aef\u4e0e\u6a21\u578b\u8fdb\u884c\u4ea4\u4e92\uff0c\u53d1\u9001\u5305\u542b\u56fe\u7247\u548c\u6587\u672c\u7684\u8bf7\u6c42\nchat_response = client.chat.completions.create(\n    model=\"gemma-3-4b-it\",  # \u672c\u5730\u6a21\u578b\u8def\u5f84\u6216Hugging Face ID\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"\u8bf7\u63cf\u8ff0\u8fd9\u5f20\u56fe\u7247\"},\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": image,  # \u4f7f\u7528Base64\u7f16\u7801\u7684\u56fe\u7247\u6570\u636e\n                },\n            },\n        ],\n    }]\n)\n\n# \u6253\u5370\u6a21\u578b\u7684\u54cd\u5e94\u5185\u5bb9\nprint(\"Chat response:\", chat_response)\nprint(\"Chat response content:\", chat_response.choices[0].message.content)\n```\nThe response is as follows\n\nChat response: ChatCompletion(id='chatcmpl-7534453d31b54b16a1e1d16dd143d9f2', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None)], created=1743065205, model='gemma-3-4b-it', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=3824, prompt_tokens=273, total_tokens=4097, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n\nContent is empty",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-27T08:56:58+00:00",
    "closed_at": "2025-03-28T00:44:44+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15610/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15610"
  },
  {
    "number": 238,
    "title": "model parallelism",
    "body": "I would like to ask when the model can support parallelism inference?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-06-25T12:53:25+00:00",
    "closed_at": "2023-06-25T17:07:08+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/238/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/238"
  },
  {
    "number": 20073,
    "title": "[Usage]: Whether to support quantized LoRA",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\n```\n\n\n### How would you like to use vllm\n\nI want to know whether vllm supports quantized LoRA? \nBecause I don't see any entry for providing `scale` in the lora expand/shrink kernel.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-06-25T11:53:42+00:00",
    "closed_at": "2025-06-25T14:28:16+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20073/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20073"
  }
]
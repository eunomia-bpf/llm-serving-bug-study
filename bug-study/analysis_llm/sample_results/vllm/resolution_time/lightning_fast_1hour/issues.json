[
  {
    "number": 6699,
    "title": "[Bug]: OpenAI API Completions and Chat API inconsistency",
    "body": "### Your current environment\n\nCollecting environment information...\r\nPyTorch version: 2.3.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Mar 22 2024, 16:50:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-113-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             8\r\nOn-line CPU(s) list:                0-7\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Core(TM) i7-14700K\r\nCPU family:                         6\r\nModel:                              183\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 1\r\nSocket(s):                          8\r\nStepping:                           1\r\nBogoMIPS:                           6835.55\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush acpi mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip pku ospke gfni vaes vpclmulqdq rdpid md_clear flush_l1d arch_capabilities\r\nHypervisor vendor:                  Xen\r\nVirtualization type:                full\r\nL1d cache:                          384 KiB (8 instances)\r\nL1i cache:                          256 KiB (8 instances)\r\nL2 cache:                           16 MiB (8 instances)\r\nL3 cache:                           264 MiB (8 instances)\r\nNUMA node(s):                       1\r\nNUMA node0 CPU(s):                  0-7\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.1\r\n[pip3] torchvision==0.18.1\r\n[pip3] transformers==4.43.1\r\n[pip3] triton==2.3.1\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-7     0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n### \ud83d\udc1b Describe the bug\n\nwhen using `python -m vllm.entrypoints.openai.api_server --model` OpenAI server\r\nAnd calling it with `openai.completions.create` or `openai.chat.completions.create`\r\n\r\nIn case of `chat` `BOS` token isn't automaticaly added\r\nIn case of `completions` it's automaticaly added\r\n\r\nWhich is unexpected behaviour difference that changes model responce",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-07-23T18:18:16+00:00",
    "closed_at": "2024-07-23T18:35:41+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6699/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6699"
  },
  {
    "number": 3503,
    "title": "[Usage]: Set dtype for VLLM using YAML",
    "body": "### Your current environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.2.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.19.0-1010-nvidia-lowlatency-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA GeForce RTX 2080 Ti\r\nGPU 1: NVIDIA GeForce RTX 2080 Ti\r\nGPU 2: NVIDIA GeForce RTX 2080 Ti\r\nGPU 3: NVIDIA GeForce RTX 2080 Ti\r\nGPU 4: NVIDIA GeForce RTX 2080 Ti\r\nGPU 5: NVIDIA GeForce RTX 2080 Ti\r\nGPU 6: NVIDIA GeForce RTX 2080 Ti\r\nGPU 7: NVIDIA GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 550.54.14\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          32\r\nOn-line CPU(s) list:             0-31\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz\r\nCPU family:                      6\r\nModel:                           79\r\nThread(s) per core:              2\r\nCore(s) per socket:              8\r\nSocket(s):                       2\r\nStepping:                        1\r\nCPU max MHz:                     3000.0000\r\nCPU min MHz:                     1200.0000\r\nBogoMIPS:                        4190.30\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d\r\nVirtualization:                  VT-x\r\nL1d cache:                       512 KiB (16 instances)\r\nL1i cache:                       512 KiB (16 instances)\r\nL2 cache:                        4 MiB (16 instances)\r\nL3 cache:                        40 MiB (2 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-7,16-23\r\nNUMA node1 CPU(s):               8-15,24-31\r\nVulnerability Itlb multihit:     KVM: Mitigation: VMX disabled\r\nVulnerability L1tf:              Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable\r\nVulnerability Mds:               Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Meltdown:          Mitigation; PTI\r\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT vulnerable\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.2.1\r\n[pip3] triton==2.2.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tPIX\tPHB\tPHB\tSYS\tSYS\tSYS\tSYS\t0-7,16-23\t0\t\tN/A\r\nGPU1\tPIX\t X \tPHB\tPHB\tSYS\tSYS\tSYS\tSYS\t0-7,16-23\t0\t\tN/A\r\nGPU2\tPHB\tPHB\t X \tPIX\tSYS\tSYS\tSYS\tSYS\t0-7,16-23\t0\t\tN/A\r\nGPU3\tPHB\tPHB\tPIX\t X \tSYS\tSYS\tSYS\tSYS\t0-7,16-23\t0\t\tN/A\r\nGPU4\tSYS\tSYS\tSYS\tSYS\t X \tPIX\tPHB\tPHB\t8-15,24-31\t1\t\tN/A\r\nGPU5\tSYS\tSYS\tSYS\tSYS\tPIX\t X \tPHB\tPHB\t8-15,24-31\t1\t\tN/A\r\nGPU6\tSYS\tSYS\tSYS\tSYS\tPHB\tPHB\t X \tPIX\t8-15,24-31\t1\t\tN/A\r\nGPU7\tSYS\tSYS\tSYS\tSYS\tPHB\tPHB\tPIX\t X \t8-15,24-31\t1\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\n\r\n### How would you like to use vllm\r\n\r\nI want to run inference using VLLM with TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ.\r\nI get the following error:\r\n\r\n`{\"error\":{\"code\":500,\"message\":\"could not load model (no success): Unexpected err=ValueError('Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your NVIDIA GeForce RTX 2080 Ti GPU has compute capability 7.5. You can use float16 instead by explicitly setting the`dtype` flag in CLI, for example: --dtype=half.'), type(err)=\\u003cclass 'ValueError'\\u003e\",`\r\n\r\nBut i cant find a way to set dtype = 'half' inside my vllm.yaml:\r\n```\r\n\r\nname: vllm\r\nbackend: vllm\r\nparameters:\r\n  model: \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\r\n\r\n# Uncomment to specify a quantization method (optional)\r\nquantization: \"gptq\"\r\n# Uncomment to limit the GPU memory utilization (vLLM default is 0.9 for 90%)\r\ngpu_memory_utilization: 0.7\r\n# Uncomment to trust remote code from huggingface\r\ntrust_remote_code: true\r\n# Uncomment to enable eager execution\r\n# enforce_eager: true\r\n# Uncomment to specify the size of the CPU swap space per GPU (in GiB)\r\n# swap_space: 2\r\n# Uncomment to specify the maximum length of a sequence (including prompt and output)\r\nmax_model_len: 32000\r\ntensor-parallel-size: 8\r\ncuda: true\r\n```\r\n\r\n\r\n\r\n\r\nI'm using docker.\r\n\r\n`sudo docker run --rm -ti --gpus all -p 8080:8080 -e DEBUG=true -e MODELS_PATH=/models -e THREADS=1 -v /opt/localai/models:/models --name localai quay.io/go-skynet/local-ai:master-cublas-cuda12-ffmpeg`\r\n",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-03-19T17:55:06+00:00",
    "closed_at": "2024-03-19T18:04:06+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3503/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3503"
  },
  {
    "number": 14535,
    "title": "`RuntimeError: Cannot re-initialize CUDA in forked subprocess` when initializing vLLM with tensor parallelism",
    "body": "### Description\nWhen attempting to initialize a vLLM instance with a custom model and tensor parallelism, I encounter a `RuntimeError: Cannot re-initialize CUDA in forked subprocess` error across multiple worker processes. The error suggests that CUDA cannot be re-initialized in a forked subprocess and recommends using the `'spawn'` start method for multiprocessing. This issue prevents the model from loading successfully.\n\nThe error occurs consistently when running the provided code snippet on a system with CUDA-enabled GPUs and tensor parallelism set to 8.\n\n### Steps to Reproduce\n1. Install the dependencies as listed in the [environment](#environment) section.\n2. Run the following Python code:\n   ```python\n   import vllm\n   model_name = '/QwQ-32B/'\n   llm = vllm.LLM(model_name, tensor_parallel_size=8)\n   ```\n3. Observe the error output in the logs.\n\n### Expected Behavior\nThe vLLM instance should initialize successfully, with all worker processes starting and the model loading without errors.\n\n### Actual Behavior\nThe initialization fails with the following error across multiple worker processes:\n```\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n```\nThe full stack trace and logs are provided in the [logs](#logs) section below.\n\n### Environment\n- **vLLM Version**: 0.7.3\n- **Python Version**: 3.10\n- **CUDA Version**: Not explicitly listed (assumed compatible with PyTorch 2.5.1 and `nvidia-cuda-*` packages)\n- **PyTorch Version**: 2.5.1\n\n### Logs\n```\nINFO 03-10 11:30:21 __init__.py:207] Automatically detected platform cuda.\nINFO 03-10 11:30:28 config.py:549] This model supports multiple tasks: {'score', 'classify', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.\nINFO 03-10 11:30:28 config.py:1382] Defaulting to use mp for distributed inference\nWARNING 03-10 11:30:28 arg_utils.py:1187] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\nINFO 03-10 11:30:28 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=2048.\nINFO 03-10 11:30:28 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/public/shared/ma4agi/model/QwQ-32B/', speculative_config=None, tokenizer='/public/shared/ma4agi/model/QwQ-32B/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/public/shared/ma4agi/model/QwQ-32B/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \nWARNING 03-10 11:30:28 multiproc_worker_utils.py:300] Reducing Torch parallelism from 96 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 03-10 11:30:28 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n(VllmWorkerProcess pid=4820) INFO 03-10 11:30:28 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=4821) INFO 03-10 11:30:28 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=4822) INFO 03-10 11:30:28 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=4823) INFO 03-10 11:30:28 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=4824) INFO 03-10 11:30:28 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=4825) INFO 03-10 11:30:28 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=4826) INFO 03-10 11:30:28 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\nINFO 03-10 11:30:29 cuda.py:229] Using Flash Attention backend.\n(VllmWorkerProcess pid=4820) INFO 03-10 11:30:29 cuda.py:229] Using Flash Attention backend.\n(VllmWorkerProcess pid=4822) INFO 03-10 11:30:29 cuda.py:229] Using Flash Attention backend.\n(VllmWorkerProcess pid=4821) INFO 03-10 11:30:29 cuda.py:229] Using Flash Attention backend.\n(VllmWorkerProcess pid=4823) INFO 03-10 11:30:29 cuda.py:229] Using Flash Attention backend.\n(VllmWorkerProcess pid=4826) INFO 03-10 11:30:29 cuda.py:229] Using Flash Attention backend.\n(VllmWorkerProcess pid=4824) INFO 03-10 11:30:29 cuda.py:229] Using Flash Attention backend.\n(VllmWorkerProcess pid=4825) INFO 03-10 11:30:29 cuda.py:229] Using Flash Attention backend.\n(VllmWorkerProcess pid=4821) ERROR 03-10 11:30:29 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method init_device.\n(VllmWorkerProcess pid=4821) ERROR 03-10 11:30:29 multiproc_worker_utils.py:242] Traceback (most recent call last):\n(VllmWorkerProcess pid=4821) ERROR 03-10 11:30:29 multiproc_worker_utils.py:242]   File \"/usr/local/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 236, in _run_worker_process\n(VllmWorkerProcess pid=4821) ERROR 03-10 11:30:29 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)\n(VllmWorkerProcess pid=4821) ERROR 03-10 11:30:29 multiproc_worker_utils.py:242]   File \"/usr/local/lib/python3.10/site-packages/vllm/utils.py\", line 2196, in run_method\n(VllmWorkerProcess pid=4821) ERROR 03-10 11:30:29 multiproc_worker_utils.py:242]     return func(*args, **kwargs)\n(VllmWorkerProcess pid=4821) ERROR 03-10 11:30:29 multiproc_worker_utils.py:242]   File \"/usr/local/lib/python3.10/site-packages/vllm/worker/worker.py\", line 155, in init_device\n(VllmWorkerProcess pid=4821) ERROR 03-10 11:30:29 multiproc_worker_utils.py:242]     torch.cuda.set_device(self.device)\n(VllmWorkerProcess pid=4821) ERROR 03-10 11:30:29 multiproc_worker_utils.py:242]   File \"/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n(VllmWorkerProcess pid=4821) ERROR 03-10 11:30:29 multiproc_worker_utils.py:242]     torch._C._cuda_setDevice(device)\n(VllmWorkerProcess pid=4821) ERROR 03-10 11:30:29 multiproc_worker_utils.py:242]   File \"/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n(VllmWorkerProcess pid=4821) ERROR 03-10 11:30:29 multiproc_worker_utils.py:242]     raise RuntimeError(\n(VllmWorkerProcess pid=4821) ERROR 03-10 11:30:29 multiproc_worker_utils.py:242] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n[... Additional similar errors for other worker processes omitted for brevity ...]\n```\n### Request\nCould the vLLM team:\n1. Confirm if this is a known issue with vLLM 0.7.3/0.7.2/0.7.1?\n2. Provide guidance on the recommended approach to resolve this\n\nThank you!",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-10T03:36:50+00:00",
    "closed_at": "2025-03-10T03:43:14+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14535/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14535"
  },
  {
    "number": 946,
    "title": "ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your Tesla V100-SXM2-32GB GPU has compute capability 7.0.",
    "body": "Does any one know about this issue?\r\n\r\n\r\n<img width=\"1206\" alt=\"Screenshot 2023-09-05 at 11 21 03 am\" src=\"https://github.com/vllm-project/vllm/assets/29119972/a59c401a-399e-4c44-9a4c-72cb7fa24ded\">\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-09-05T01:22:12+00:00",
    "closed_at": "2023-09-05T01:39:42+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/946/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/946"
  },
  {
    "number": 16586,
    "title": "[Usage]: How to replace vllm model's weights after engine is started up?",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\n```\n\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-04-14T10:45:48+00:00",
    "closed_at": "2025-04-14T11:39:39+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16586/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16586"
  },
  {
    "number": 19408,
    "title": "[Bug]: load Qwen2.5-14B-1M error",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\ntorch                                    2.6.0\ntorchaudio                               2.6.0\ntorchvision                              0.21.0\ntqdm                                     4.67.1\ntransformers                             4.51.3\ntriton                                   3.2.0\ntyper                                    0.16.0\ntyping_extensions                        4.14.0\ntyping-inspection                        0.4.1\nurllib3                                  2.4.0\nuvicorn                                  0.34.3\nuvloop                                   0.21.0\nvirtualenv                               20.31.2\nvllm                                     0.8.5.post1\nwatchfiles                               1.0.5\nwebsockets                               15.0.1\nwheel                                    0.45.1\nwrapt                                    1.17.2\nxformers                                 0.0.29.post2\nxgrammar                                 0.1.18\nyarl                                     1.20.0\nzipp                                     3.23.0\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n\n\ni have an error when i load qwen2.5-14B-Instruct-1M\n\nINFO 06-10 16:32:52 [core.py:159] init engine (profile, create kv cache, warmup model) took 7.68 seconds\nINFO 06-10 16:32:52 [core_client.py:439] Core engine process 0 ready.\nProcessed prompts: 0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]ERROR 06-10 16:33:51 [core.py:398] EngineCore encountered a fatal error.\nERROR 06-10 16:33:51 [core.py:398] Traceback (most recent call last):\nERROR 06-10 16:33:51 [core.py:398] File \"/home/brain/anaconda3/envs/tk_longcontext/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 181, in collective_rpc\nERROR 06-10 16:33:51 [core.py:398] status, result = w.worker_response_mq.dequeue(\nERROR 06-10 16:33:51 [core.py:398] File \"/home/brain/anaconda3/envs/tk_longcontext/lib/python3.10/site-packages/vllm/distributed/device_communicators/shm_broadcast.py\", line 479, in dequeue\nERROR 06-10 16:33:51 [core.py:398] with self.acquire_read(timeout, cancel) as buf:\nERROR 06-10 16:33:51 [core.py:398] File \"/home/brain/anaconda3/envs/tk_longcontext/lib/python3.10/contextlib.py\", line 135, in enter\nERROR 06-10 16:33:51 [core.py:398] return next(self.gen)\nERROR 06-10 16:33:51 [core.py:398] File \"/home/brain/anaconda3/envs/tk_longcontext/lib/python3.10/site-packages/vllm/distributed/device_communicators/shm_broadcast.py\", line 443, in acquire_read\nERROR 06-10 16:33:51 [core.py:398] raise TimeoutError\nERROR 06-10 16:33:51 [core.py:398] TimeoutError\nERROR 06-10 16:33:51 [core.py:398]\nERROR 06-10 16:33:51 [core.py:398] The above exception was the direct cause of the following exception:\nERROR 06-10 16:33:51 [core.py:398]\nERROR 06-10 16:33:51 [core.py:398] Traceback (most recent call last):\nERROR 06-10 16:33:51 [core.py:398] File \"/home/brain/anaconda3/envs/tk_longcontext/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 389, in run_engine_core\nERROR 06-10 16:33:51 [core.py:398] engine_core.run_busy_loop()\nERROR 06-10 16:33:51 [core.py:398] File \"/home/brain/anaconda3/envs/tk_longcontext/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 413, in run_busy_loop\nERROR 06-10 16:33:51 [core.py:398] self._process_engine_step()\nERROR 06-10 16:33:51 [core.py:398] File \"/home/brain/anaconda3/envs/tk_longcontext/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 438, in _process_engine_step\nERROR 06-10 16:33:51 [core.py:398] outputs = self.step_fn()\nERROR 06-10 16:33:51 [core.py:398] File \"/home/brain/anaconda3/envs/tk_longcontext/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 203, in step\nERROR 06-10 16:33:51 [core.py:398] output = self.model_executor.execute_model(scheduler_output)\nERROR 06-10 16:33:51 [core.py:398] File \"/home/brain/anaconda3/envs/tk_longcontext/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 146, in execute_model\nERROR 06-10 16:33:51 [core.py:398] (output, ) = self.collective_rpc(\"execute_model\",\nERROR 06-10 16:33:51 [core.py:398] File \"/home/brain/anaconda3/envs/tk_longcontext/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 193, in collective_rpc\nERROR 06-10 16:33:51 [core.py:398] raise TimeoutError(f\"RPC call to {method} timed out.\") from e\nERROR 06-10 16:33:51 [core.py:398] TimeoutError: RPC call to execute_model timed out.\nTraceback (most recent call last):\nFile \"/data/tangkai/projects/long_text_rag/long_context.py\", line 142, in\noutputs = llm.generate([text], sampling_params)\nFile \"/home/brain/anaconda3/envs/tk_longcontext/lib/python3.10/site-packages/vllm/utils.py\", line 1196, in inner\nreturn fn(*args, **kwargs)\nFile \"/home/brain/anaconda3/envs/tk_longcontext/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 473, in generate\noutputs = self._run_engine(use_tqdm=use_tqdm)\nFile \"/home/brain/anaconda3/envs/tk_longcontext/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 1423, in _run_engine\nstep_outputs = self.llm_engine.step()\nFile \"/home/brain/anaconda3/envs/tk_longcontext/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\", line 218, in step\noutputs = self.engine_core.get_output()\nFile \"/home/brain/anaconda3/envs/tk_longcontext/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\", line 558, in get_output\nraise self._format_exception(outputs) from None\nvllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n/home/brain/anaconda3/envs/tk_longcontext/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\nwarnings.warn('resource_tracker: There appear to be %d '\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-06-10T08:46:23+00:00",
    "closed_at": "2025-06-10T08:58:08+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19408/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19408"
  },
  {
    "number": 11297,
    "title": "[Usage]: What are the correct parameters for offline beam search inference in vllm ?",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### How would you like to use vllm\n\nAs beam search api was changed recently and `use_beam_search` was removed from SamplingParams, I'm not sure which is the way to trigger beam search (without sampling) in vllm offline inference. Currently, I'm adopting the following codes:\r\n```\r\nfrom vllm import LLM, SamplingParams\r\nbeam_size=4\r\nsampling_params = SamplingParams(temperature=1.0,n=1,top_k=-1,top_p=1,seed=42,max_tokens=128,best_of=beam_size)\r\noutputs = vllm_model.chat(messages=prompts, sampling_params=sampling_params, use_tqdm=True)\r\n```\r\n\r\nIs this the correct way to trigger beam search ?\r\n\r\nvllm version: v0.6.4.post1\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-12-18T10:16:51+00:00",
    "closed_at": "2024-12-18T10:43:59+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11297/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11297"
  },
  {
    "number": 16301,
    "title": "[Usage]: When performing inference with vLLM, it keeps getting stuck at 0%.",
    "body": "### Your current environment\n\nNFO 04-09 02:05:52 [loader.py:447] Loading weights took 33.15 seconds\nINFO 04-09 02:05:53 [gpu_model_runner.py:1273] Model loading took 61.0374 GiB and 33.525153 seconds\nINFO 04-09 02:06:09 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/e8c79b34a0/rank_0_0 for vLLM's torch.compile\nINFO 04-09 02:06:09 [backends.py:426] Dynamo bytecode transform time: 16.61 s\nINFO 04-09 02:06:10 [backends.py:115] Directly load the compiled graph for shape None from the cache\nINFO 04-09 02:06:25 [monitor.py:33] torch.compile takes 16.61 s in total\nINFO 04-09 02:06:28 [kv_cache_utils.py:578] GPU KV cache size: 16,064 tokens\nINFO 04-09 02:06:28 [kv_cache_utils.py:581] Maximum concurrency for 10,000 tokens per request: 1.61x\nINFO 04-09 02:07:01 [gpu_model_runner.py:1608] Graph capturing finished in 33 secs, took 3.17 GiB\nINFO 04-09 02:07:01 [core.py:162] init engine (profile, create kv cache, warmup model) took 68.69 seconds\nProcessed prompts:   0%|                   | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n\n\n### How would you like to use vllm\n\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-04-09T02:27:49+00:00",
    "closed_at": "2025-04-09T02:28:51+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16301/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16301"
  },
  {
    "number": 3485,
    "title": "[Bug]: ",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\npython -m vllm.entrypoints.openai.api_server --model /root/autodl-tmp/models/Qwen1.5-14B-Chat-GPTQ-Int8 --max-model-len 2000  --trust-remote-code\r\n\r\nwhen request api, some errors return:\r\n\r\n{\"object\":\"error\",\"message\":\"The model `Qwen1.5-14B-Chat-GPTQ-Int8` does not exist.\",\"type\":\"NotFoundError\",\"param\":null,\"code\":404}",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-03-19T01:44:56+00:00",
    "closed_at": "2024-03-19T01:48:44+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3485/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3485"
  },
  {
    "number": 4031,
    "title": "[Feature]: Support Int8 dtype for storing weights - currently uses FP16 wasting 50% of VRAM",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCould you please add Int8 as a supported dtype? Currently when using Int8 models such as https://huggingface.co/Qwen/Qwen1.5-7B-Chat-GPTQ-Int8 with xformers instead of FlashAttention, the weights are stored as FP16 taking double the VRAM.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-04-12T07:56:27+00:00",
    "closed_at": "2024-04-12T08:15:30+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4031/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4031"
  },
  {
    "number": 17718,
    "title": "[Bug]: Gemma model is giving empty responses with new version of docker image vllm-openai:v.8.5",
    "body": "### Current environment\n\nKubernetes Cluster on Azure with A100 GPUs\n\n### Bug\n\nHello team,\n\nAfter upgrading the Docker image from vllm-openai:v0.8.4 to v0.8.5, I observed one issue when running the google/gemma-3-27b-it model ([Hugging Face Model Link](https://huggingface.co/google/gemma-3-27b-it)).\n\nThe model successfully returns metadata (e.g., finish reason, token usage), but the content field in the response is consistently an empty string. No changes were made to the Kubernetes deployment manifest apart from the image version bump.\n\nWhen reverting to v0.8.4, the model responds correctly with expected text completions, confirming that the issue is specific to the new image version.\n\nSteps to Reproduce:\n\n1. Deploy vllm-openai:v0.8.5 with the gemma-3-27b-it model.\n\n2. Send a chat completion request.\n\n3. Observe that the content field is empty in the response.\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-05-06T13:12:44+00:00",
    "closed_at": "2025-05-06T13:58:35+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17718/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17718"
  },
  {
    "number": 13451,
    "title": "[Bug]: With docker \"ValueError: No supported config format found in\"",
    "body": "### Your current environment\n\nOfficial latest docker image\n\n### \ud83d\udc1b Describe the bug\n\n\nI run a docker container with the commande line:\n\n\"docker run --runtime nvidia --gpus all \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --env \"HUGGING_FACE_HUB_TOKEN=<secret>\" \\\n    -p 8000:8000 \\\n    --ipc=host \\\n    vllm/vllm-openai:latest \\\n    --model mistralai/Mistral-7B-v0.1\"\n\nA few hours ago, everything was working fine. However, when I try to run a model using the Docker image now, I encounter the following error:\n\n\"ValueError: No supported config format found in\"\n\nBelow, you\u2019ll find the full error message that appears when I run the command provided on the vLLM website.\n\n\ndocker run --runtime nvidia --gpus all \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --env \"HUGGING_FACE_HUB_TOKEN=<secret>\" \\\n    -p 8000:8000 \\\n    --ipc=host \\\n    vllm/vllm-openai:latest \\\n    --model mistralai/Mistral-7B-v0.1\nINFO 02-17 19:14:31 __init__.py:190] Automatically detected platform cuda.\nINFO 02-17 19:14:31 api_server.py:840] vLLM API server version 0.7.2\nINFO 02-17 19:14:31 api_server.py:841] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='mistralai/Mistral-7B-v0.1', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)\nINFO 02-17 19:14:31 api_server.py:206] Started engine process with PID 20\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 911, in <module>\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 875, in run_server\n    async with build_async_engine_client(args) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 136, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 217, in build_async_engine_client_from_engine_args\n    engine_config = engine_args.create_engine_config()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\", line 1075, in create_engine_config\n    model_config = self.create_model_config()\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\", line 998, in create_model_config\n    return ModelConfig(\n           ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/config.py\", line 302, in __init__\n    hf_config = get_config(self.model, trust_remote_code, revision,\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/transformers_utils/config.py\", line 201, in get_config\n    raise ValueError(f\"No supported config format found in {model}\")\nValueError: No supported config format found in mistralai/Mistral-7B-v0.1\nINFO 02-17 19:14:35 __init__.py:190] Automatically detected platform cuda.\nProcess SpawnProcess-1:\nERROR 02-17 19:14:36 engine.py:389] No supported config format found in mistralai/Mistral-7B-v0.1\nERROR 02-17 19:14:36 engine.py:389] Traceback (most recent call last):\nERROR 02-17 19:14:36 engine.py:389]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 380, in run_mp_engine\nERROR 02-17 19:14:36 engine.py:389]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\nERROR 02-17 19:14:36 engine.py:389]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-17 19:14:36 engine.py:389]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 118, in from_engine_args\nERROR 02-17 19:14:36 engine.py:389]     engine_config = engine_args.create_engine_config(usage_context)\nERROR 02-17 19:14:36 engine.py:389]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-17 19:14:36 engine.py:389]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\", line 1075, in create_engine_config\nERROR 02-17 19:14:36 engine.py:389]     model_config = self.create_model_config()\nERROR 02-17 19:14:36 engine.py:389]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-17 19:14:36 engine.py:389]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\", line 998, in create_model_config\nERROR 02-17 19:14:36 engine.py:389]     return ModelConfig(\nERROR 02-17 19:14:36 engine.py:389]            ^^^^^^^^^^^^\nERROR 02-17 19:14:36 engine.py:389]   File \"/usr/local/lib/python3.12/dist-packages/vllm/config.py\", line 302, in __init__\nERROR 02-17 19:14:36 engine.py:389]     hf_config = get_config(self.model, trust_remote_code, revision,\nERROR 02-17 19:14:36 engine.py:389]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-17 19:14:36 engine.py:389]   File \"/usr/local/lib/python3.12/dist-packages/vllm/transformers_utils/config.py\", line 201, in get_config\nERROR 02-17 19:14:36 engine.py:389]     raise ValueError(f\"No supported config format found in {model}\")\nERROR 02-17 19:14:36 engine.py:389] ValueError: No supported config format found in mistralai/Mistral-7B-v0.1\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 391, in run_mp_engine\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 380, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 118, in from_engine_args\n    engine_config = engine_args.create_engine_config(usage_context)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\", line 1075, in create_engine_config\n    model_config = self.create_model_config()\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\", line 998, in create_model_config\n    return ModelConfig(\n           ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/config.py\", line 302, in __init__\n    hf_config = get_config(self.model, trust_remote_code, revision,\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/transformers_utils/config.py\", line 201, in get_config\n    raise ValueError(f\"No supported config format found in {model}\")\nValueError: No supported config format found in mistralai/Mistral-7B-v0.1\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-02-18T03:23:50+00:00",
    "closed_at": "2025-02-18T04:16:53+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13451/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13451"
  },
  {
    "number": 3306,
    "title": "flash_attn is installed, but \"flash_attn is not found. Using xformers backend.\"",
    "body": "```bash  \r\n >>> flash_attn is not found. Using xformers backend.\r\n```\r\nbut flash_attn has been added into the vllm wheel \r\n```bash \r\nadding 'vllm/thirdparty_files/flash_attn/ops/triton/rotary.py'\r\nadding 'vllm/thirdparty_files/flash_attn/ops/triton/__pycache__/__init__.cpython-310.pyc'\r\nadding 'vllm/thirdparty_files/flash_attn/ops/triton/__pycache__/cross_entropy.cpython-310.pyc'\r\nadding 'vllm/thirdparty_files/flash_attn/ops/triton/__pycache__/k_activations.cpython-310.pyc'\r\nadding 'vllm/thirdparty_files/flash_attn/ops/triton/__pycache__/layer_norm.cpython-310.pyc'\r\nadding 'vllm/thirdparty_files/flash_attn/ops/triton/__pycache__/linear.cpython-310.pyc'\r\nadding 'vllm/thirdparty_files/flash_attn/ops/triton/__pycache__/mlp.cpython-310.pyc'\r\nadding 'vllm/thirdparty_files/flash_attn/ops/triton/__pycache__/rotary.cpython-310.pyc'\r\nadding 'vllm/thirdparty_files/flash_attn/utils/__init__.py'\r\nadding 'vllm/thirdparty_files/flash_attn/utils/benchmark.py'\r\nadding 'vllm/thirdparty_files/flash_attn/utils/distributed.py'\r\nadding 'vllm/thirdparty_files/flash_attn/utils/generation.py'\r\nadding 'vllm/thirdparty_files/flash_attn/utils/pretrained.py'\r\nadding 'vllm/thirdparty_files/flash_attn/utils/__pycache__/__init__.cpython-310.pyc'\r\nadding 'vllm/thirdparty_files/flash_attn/utils/__pycache__/benchmark.cpython-310.pyc'\r\nadding 'vllm/thirdparty_files/flash_attn/utils/__pycache__/distributed.cpython-310.pyc'\r\nadding 'vllm/thirdparty_files/flash_attn/utils/__pycache__/generation.cpython-310.pyc'\r\nadding 'vllm/thirdparty_files/flash_attn/utils/__pycache__/pretrained.cpython-310.pyc'\r\n```\r\nand the import of flash_attn also failed \r\n```bash \r\n>>> import flash_attn\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'flash_attn'\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-03-11T02:46:04+00:00",
    "closed_at": "2024-03-11T03:42:36+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3306/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3306"
  },
  {
    "number": 5198,
    "title": "[Bug]: Issues with Applying LoRA in vllm on a T4 GPU",
    "body": "### Your current environment\n\nI am currently using a T4 instance on Google Colaboratory.\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: 14.0.0-1ubuntu1.1\r\nCMake version: version 3.27.9\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.1.85+-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: Tesla T4\r\nNvidia driver version: 535.104.05\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        46 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               2\r\nOn-line CPU(s) list:                  0,1\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\r\nCPU family:                           6\r\nModel:                                79\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   1\r\nSocket(s):                            1\r\nStepping:                             0\r\nBogoMIPS:                             4399.99\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            32 KiB (1 instance)\r\nL1i cache:                            32 KiB (1 instance)\r\nL2 cache:                             256 KiB (1 instance)\r\nL3 cache:                             55 MiB (1 instance)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0,1\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Mitigation; PTE Inversion\r\nVulnerability Mds:                    Vulnerable; SMT Host state unknown\r\nVulnerability Meltdown:               Vulnerable\r\nVulnerability Mmio stale data:        Vulnerable\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Vulnerable\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Vulnerable\r\nVulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Vulnerable\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.25.2\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0+cu121\r\n[pip3] torchaudio==2.3.0+cu121\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchtext==0.18.0\r\n[pip3] torchvision==0.18.0+cu121\r\n[pip3] triton==2.3.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.3\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \t0-1\t\tN/A\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\n\n### \ud83d\udc1b Describe the bug\n\nI encounter an error when attempting to apply LoRA in vllm. Here are the details of the problem:\r\n- The error occurs when enable_lora=True is added.\r\n- The error only occurs when using a T4 GPU. It does not occur with L4 or A100 GPUs.\r\n\r\nBelow is a sample code snippet that reproduces the issue:\r\n```python\r\n!pip install torch==2.3.0+cu121 vllm==0.4.3\r\n\r\nimport torch\r\nimport vllm\r\nfrom vllm import LLM, SamplingParams\r\nfrom vllm.lora.request import LoRARequest\r\n\r\nmodel_path = \"microsoft/Phi-3-mini-4k-instruct\"\r\nVLLM_TENSOR_PARALLEL_SIZE = 1\r\nVLLM_GPU_MEMORY_UTILIZATION = 0.85\r\n\r\nllm = vllm.LLM(\r\n    model=model_path,\r\n    tensor_parallel_size=VLLM_TENSOR_PARALLEL_SIZE,\r\n    gpu_memory_utilization=VLLM_GPU_MEMORY_UTILIZATION,\r\n    trust_remote_code=True,\r\n    dtype=torch.float16,\r\n    enforce_eager=True,\r\n    enable_lora=True\r\n  )\r\n```\r\n\r\nwhich yields the following output:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n[<ipython-input-5-d4df0971412c>](https://localhost:8080/#) in <cell line: 1>()\r\n----> 1 llm = vllm.LLM(\r\n      2     model=model_path,\r\n      3     tensor_parallel_size=VLLM_TENSOR_PARALLEL_SIZE,\r\n      4     gpu_memory_utilization=VLLM_GPU_MEMORY_UTILIZATION,\r\n      5     trust_remote_code=True,\r\n\r\n22 frames\r\n[/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py](https://localhost:8080/#) in __init__(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)\r\n    142             **kwargs,\r\n    143         )\r\n--> 144         self.llm_engine = LLMEngine.from_engine_args(\r\n    145             engine_args, usage_context=UsageContext.LLM_CLASS)\r\n    146         self.request_counter = Counter()\r\n\r\n[/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py](https://localhost:8080/#) in from_engine_args(cls, engine_args, usage_context)\r\n    357 \r\n    358         # Create the LLM engine.\r\n--> 359         engine = cls(\r\n    360             **engine_config.to_dict(),\r\n    361             executor_class=executor_class,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py](https://localhost:8080/#) in __init__(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, vision_language_config, speculative_config, decoding_config, executor_class, log_stats, usage_context)\r\n    233 \r\n    234         if not self.model_config.embedding_mode:\r\n--> 235             self._initialize_kv_caches()\r\n    236 \r\n    237         # If usage stat is enabled, collect relevant info.\r\n\r\n[/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py](https://localhost:8080/#) in _initialize_kv_caches(self)\r\n    310         \"\"\"\r\n    311         num_gpu_blocks, num_cpu_blocks = (\r\n--> 312             self.model_executor.determine_num_available_blocks())\r\n    313 \r\n    314         if self.cache_config.num_gpu_blocks_override is not None:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py](https://localhost:8080/#) in determine_num_available_blocks(self)\r\n     73         underlying worker.\r\n     74         \"\"\"\r\n---> 75         return self.driver_worker.determine_num_available_blocks()\r\n     76 \r\n     77     def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks) -> None:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py](https://localhost:8080/#) in decorate_context(*args, **kwargs)\r\n    113     def decorate_context(*args, **kwargs):\r\n    114         with ctx_factory():\r\n--> 115             return func(*args, **kwargs)\r\n    116 \r\n    117     return decorate_context\r\n\r\n[/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py](https://localhost:8080/#) in determine_num_available_blocks(self)\r\n    152         # Execute a forward pass with dummy inputs to profile the memory usage\r\n    153         # of the model.\r\n--> 154         self.model_runner.profile_run()\r\n    155 \r\n    156         # Calculate the number of blocks that can be allocated with the\r\n\r\n[/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py](https://localhost:8080/#) in decorate_context(*args, **kwargs)\r\n    113     def decorate_context(*args, **kwargs):\r\n    114         with ctx_factory():\r\n--> 115             return func(*args, **kwargs)\r\n    116 \r\n    117     return decorate_context\r\n\r\n[/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py](https://localhost:8080/#) in profile_run(self)\r\n    807         num_layers = self.model_config.get_num_layers(self.parallel_config)\r\n    808         kv_caches = [None] * num_layers\r\n--> 809         self.execute_model(seqs, kv_caches)\r\n    810         torch.cuda.synchronize()\r\n    811         return\r\n\r\n[/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py](https://localhost:8080/#) in decorate_context(*args, **kwargs)\r\n    113     def decorate_context(*args, **kwargs):\r\n    114         with ctx_factory():\r\n--> 115             return func(*args, **kwargs)\r\n    116 \r\n    117     return decorate_context\r\n\r\n[/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py](https://localhost:8080/#) in execute_model(self, seq_group_metadata_list, kv_caches)\r\n    726         if self.vision_language_config:\r\n    727             execute_model_kwargs.update({\"image_input\": multi_modal_input})\r\n--> 728         hidden_states = model_executable(**execute_model_kwargs)\r\n    729 \r\n    730         # Compute the logits.\r\n\r\n[/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py](https://localhost:8080/#) in _wrapped_call_impl(self, *args, **kwargs)\r\n   1530             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n   1531         else:\r\n-> 1532             return self._call_impl(*args, **kwargs)\r\n   1533 \r\n   1534     def _call_impl(self, *args, **kwargs):\r\n\r\n[/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py](https://localhost:8080/#) in _call_impl(self, *args, **kwargs)\r\n   1539                 or _global_backward_pre_hooks or _global_backward_hooks\r\n   1540                 or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1541             return forward_call(*args, **kwargs)\r\n   1542 \r\n   1543         try:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py](https://localhost:8080/#) in forward(self, input_ids, positions, kv_caches, attn_metadata)\r\n    361         attn_metadata: AttentionMetadata,\r\n    362     ) -> torch.Tensor:\r\n--> 363         hidden_states = self.model(input_ids, positions, kv_caches,\r\n    364                                    attn_metadata)\r\n    365         return hidden_states\r\n\r\n[/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py](https://localhost:8080/#) in _wrapped_call_impl(self, *args, **kwargs)\r\n   1530             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n   1531         else:\r\n-> 1532             return self._call_impl(*args, **kwargs)\r\n   1533 \r\n   1534     def _call_impl(self, *args, **kwargs):\r\n\r\n[/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py](https://localhost:8080/#) in _call_impl(self, *args, **kwargs)\r\n   1539                 or _global_backward_pre_hooks or _global_backward_hooks\r\n   1540                 or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1541             return forward_call(*args, **kwargs)\r\n   1542 \r\n   1543         try:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py](https://localhost:8080/#) in forward(self, input_ids, positions, kv_caches, attn_metadata, inputs_embeds)\r\n    286         for i in range(len(self.layers)):\r\n    287             layer = self.layers[i]\r\n--> 288             hidden_states, residual = layer(\r\n    289                 positions,\r\n    290                 hidden_states,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py](https://localhost:8080/#) in _wrapped_call_impl(self, *args, **kwargs)\r\n   1530             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n   1531         else:\r\n-> 1532             return self._call_impl(*args, **kwargs)\r\n   1533 \r\n   1534     def _call_impl(self, *args, **kwargs):\r\n\r\n[/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py](https://localhost:8080/#) in _call_impl(self, *args, **kwargs)\r\n   1539                 or _global_backward_pre_hooks or _global_backward_hooks\r\n   1540                 or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1541             return forward_call(*args, **kwargs)\r\n   1542 \r\n   1543         try:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py](https://localhost:8080/#) in forward(self, positions, hidden_states, kv_cache, attn_metadata, residual)\r\n    221         if residual is None:\r\n    222             residual = hidden_states\r\n--> 223             hidden_states = self.input_layernorm(hidden_states)\r\n    224         else:\r\n    225             hidden_states, residual = self.input_layernorm(\r\n\r\n[/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py](https://localhost:8080/#) in _wrapped_call_impl(self, *args, **kwargs)\r\n   1530             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n   1531         else:\r\n-> 1532             return self._call_impl(*args, **kwargs)\r\n   1533 \r\n   1534     def _call_impl(self, *args, **kwargs):\r\n\r\n[/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py](https://localhost:8080/#) in _call_impl(self, *args, **kwargs)\r\n   1539                 or _global_backward_pre_hooks or _global_backward_hooks\r\n   1540                 or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1541             return forward_call(*args, **kwargs)\r\n   1542 \r\n   1543         try:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/layernorm.py](https://localhost:8080/#) in forward(self, x, residual)\r\n     57             )\r\n     58             return x, residual\r\n---> 59         out = torch.empty_like(x)\r\n     60         ops.rms_norm(\r\n     61             out,\r\n\r\nRuntimeError: CUDA error: no kernel image is available for execution on the device\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n```\r\n\r\nI would appreciate it if you could explain how to apply LoRA on a T4 GPU.\r\nI hope this helps you with your GitHub issue submission. \r\nIf you need any further adjustments or additional information included, please let me know!",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-06-02T16:03:20+00:00",
    "closed_at": "2024-06-02T16:08:39+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5198/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5198"
  },
  {
    "number": 2023,
    "title": "Does the Mixtral implementation follow the official code?",
    "body": "Hi,\nDoes your Mixtral implementation follow the newly released official Mixtral code?\n\nhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\nThanks for creating this great project!",
    "labels": [],
    "state": "closed",
    "created_at": "2023-12-11T16:03:24+00:00",
    "closed_at": "2023-12-11T16:25:46+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2023/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2023"
  },
  {
    "number": 1733,
    "title": "TypeError: 'builtins.safe_open' object is not iterable",
    "body": "```\r\n/vllm/vllm/model_executor/weight_utils.py\", line 243, in hf_model_weights_iterator\r\n    for name in f:\r\nTypeError: 'builtins.safe_open' object is not iterable\r\n```\r\n\r\nI am using the main branch build.\r\nTriggers when using /openai/api_server.py and loading the model weights for the engine object (AsyncLLMEngine)\r\n\r\nTraceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"api_server.py\", line 644, in <module>\r\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\r\n  File \"async_llm_engine.py\", line 486, in from_engine_args\r\n    engine = cls(parallel_config.worker_use_ray,\r\n  File \"async_llm_engine.py\", line 269, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n  File \"async_llm_engine.py\", line 305, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n  File \"llm_engine.py\", line 110, in __init__\r\n    self._init_workers(distributed_init_method)\r\n  File \"llm_engine.py\", line 142, in _init_workers\r\n    self._run_workers(\r\n  File \"llm_engine.py\", line 700, in _run_workers\r\n    output = executor(*args, **kwargs)\r\n  File \"worker.py\", line 70, in init_model\r\n    self.model = get_model(self.model_config)\r\n  File \"model_loader.py\", line 98, in get_model\r\n    model.load_weights(model_config.model, model_config.download_dir,\r\n  File \"mistral.py\", line 307, in load_weights\r\n    for name, loaded_weight in hf_model_weights_iterator(\r\n  File \"weight_utils.py\", line 243, in hf_model_weights_iterator\r\n    for name in f:\r\nTypeError: 'builtins.safe_open' object is not iterable\r\n```\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-11-20T23:56:44+00:00",
    "closed_at": "2023-11-21T00:01:16+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1733/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1733"
  },
  {
    "number": 16378,
    "title": "[Doc]: vllm\u90e8\u7f72Gemma-3-27b\u95ee\u9898",
    "body": "### \ud83d\udcda The doc issue\n\n\u6211\u7684\u542f\u52a8\u547d\u4ee4\u662f\n(torch) root@vvbbovkdctrbyrag-wind-b6df56d5-r96vk:/data/coding# vllm serve Gemma-3-27b --tensor-parallel-size 4 --max-model-len 65536 --max-num-batched-tokens 16384 --port 30041  --trust-remote-code  --served-model-name gemma3-27b  --max-num-seqs 64 --enable-chunked-prefill --limit-mm-per-prompt image=50,video=2 --api-key k7YgF9RwP4qXmTnV2LsJ3HdO5zIc6AeB0Uv1lKpN8Q\nINFO 04-10 08:57:28 [__init__.py:256] Automatically detected platform cuda.\nINFO 04-10 08:57:30 [api_server.py:972] vLLM API server version 0.8.0rc3.dev5+g5eeabc2a.d20250318\nINFO 04-10 08:57:30 [api_server.py:973] args: Namespace(subparser='serve', model_tag='Gemma-3-27b', config='', host=None, port=30041, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='k7YgF9RwP4qXmTnV2LsJ3HdO5zIc6AeB0Uv1lKpN8Q', lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='Gemma-3-27b', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=65536, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=4, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=16384, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=64, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt={'image': 50, 'video': 2}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=True, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['gemma3-27b'], qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x7f0b17b7fe20>)\nINFO 04-10 08:57:30 [config.py:2521] For Gemma 2 and 3, we downcast float32 to bfloat16 instead of float16 by default. Please specify `dtype` if you want to use float16.\nINFO 04-10 08:57:30 [config.py:2579] Downcasting torch.float32 to torch.bfloat16.\nINFO 04-10 08:57:37 [config.py:583] This model supports multiple tasks: {'generate', 'score', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.\nWARNING 04-10 08:57:37 [arg_utils.py:1754] ['Gemma3ForConditionalGeneration'] is not supported by the V1 Engine. Falling back to V0. \nINFO 04-10 08:57:37 [config.py:1499] Defaulting to use mp for distributed inference\nINFO 04-10 08:57:37 [config.py:1677] Chunked prefill is enabled with max_num_batched_tokens=16384.\nINFO 04-10 08:57:37 [api_server.py:236] Started engine process with PID 110018\nINFO 04-10 08:57:40 [__init__.py:256] Automatically detected platform cuda.\nINFO 04-10 08:57:42 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.0rc3.dev5+g5eeabc2a.d20250318) with config: model='Gemma-3-27b', speculative_config=None, tokenizer='Gemma-3-27b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=65536, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=gemma3-27b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":64}, use_cached_outputs=True, \nWARNING 04-10 08:57:44 [multiproc_worker_utils.py:310] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 04-10 08:57:44 [custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\nINFO 04-10 08:57:45 [cuda.py:285] Using Flash Attention backend.\nINFO 04-10 08:57:47 [__init__.py:256] Automatically detected platform cuda.\nINFO 04-10 08:57:47 [__init__.py:256] Automatically detected platform cuda.\nINFO 04-10 08:57:47 [__init__.py:256] Automatically detected platform cuda.\n(VllmWorkerProcess pid=110190) INFO 04-10 08:57:49 [multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=110191) INFO 04-10 08:57:49 [multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=110192) INFO 04-10 08:57:49 [multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=110190) INFO 04-10 08:57:50 [cuda.py:285] Using Flash Attention backend.\n(VllmWorkerProcess pid=110191) INFO 04-10 08:57:50 [cuda.py:285] Using Flash Attention backend.\n(VllmWorkerProcess pid=110192) INFO 04-10 08:57:50 [cuda.py:285] Using Flash Attention backend.\n(VllmWorkerProcess pid=110190) INFO 04-10 08:57:52 [utils.py:925] Found nccl from library libnccl.so.2\n(VllmWorkerProcess pid=110191) INFO 04-10 08:57:52 [utils.py:925] Found nccl from library libnccl.so.2\n(VllmWorkerProcess pid=110190) INFO 04-10 08:57:52 [pynccl.py:69] vLLM is using nccl==2.21.5\n(VllmWorkerProcess pid=110191) INFO 04-10 08:57:52 [pynccl.py:69] vLLM is using nccl==2.21.5\nINFO 04-10 08:57:52 [utils.py:925] Found nccl from library libnccl.so.2\nINFO 04-10 08:57:52 [pynccl.py:69] vLLM is using nccl==2.21.5\n(VllmWorkerProcess pid=110192) INFO 04-10 08:57:52 [utils.py:925] Found nccl from library libnccl.so.2\n(VllmWorkerProcess pid=110192) INFO 04-10 08:57:52 [pynccl.py:69] vLLM is using nccl==2.21.5\n(VllmWorkerProcess pid=110191) WARNING 04-10 08:57:53 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n(VllmWorkerProcess pid=110190) WARNING 04-10 08:57:53 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nWARNING 04-10 08:57:53 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n(VllmWorkerProcess pid=110192) WARNING 04-10 08:57:53 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nINFO 04-10 08:57:53 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_3af5e566'), local_subscribe_addr='ipc:///tmp/abeb4df0-b6b5-404f-8776-7175c8c7ab3c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n(VllmWorkerProcess pid=110191) INFO 04-10 08:57:53 [parallel_state.py:948] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2\n(VllmWorkerProcess pid=110192) INFO 04-10 08:57:53 [parallel_state.py:948] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3\n(VllmWorkerProcess pid=110190) INFO 04-10 08:57:53 [parallel_state.py:948] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1\nINFO 04-10 08:57:53 [parallel_state.py:948] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 04-10 08:57:53 [model_runner.py:1110] Starting to load model Gemma-3-27b...\n(VllmWorkerProcess pid=110190) INFO 04-10 08:57:53 [model_runner.py:1110] Starting to load model Gemma-3-27b...\n(VllmWorkerProcess pid=110191) INFO 04-10 08:57:53 [model_runner.py:1110] Starting to load model Gemma-3-27b...\n(VllmWorkerProcess pid=110192) INFO 04-10 08:57:53 [model_runner.py:1110] Starting to load model Gemma-3-27b...\n(VllmWorkerProcess pid=110190) INFO 04-10 08:57:53 [cuda.py:269] Cannot use FlashAttention-2 backend for head size 72.\n(VllmWorkerProcess pid=110190) INFO 04-10 08:57:53 [cuda.py:282] Using XFormers backend.\n(VllmWorkerProcess pid=110191) INFO 04-10 08:57:53 [cuda.py:269] Cannot use FlashAttention-2 backend for head size 72.\n(VllmWorkerProcess pid=110191) INFO 04-10 08:57:53 [cuda.py:282] Using XFormers backend.\n(VllmWorkerProcess pid=110192) INFO 04-10 08:57:53 [cuda.py:269] Cannot use FlashAttention-2 backend for head size 72.\nINFO 04-10 08:57:53 [cuda.py:269] Cannot use FlashAttention-2 backend for head size 72.\n(VllmWorkerProcess pid=110192) INFO 04-10 08:57:53 [cuda.py:282] Using XFormers backend.\nINFO 04-10 08:57:53 [cuda.py:282] Using XFormers backend.\n(VllmWorkerProcess pid=110190) INFO 04-10 08:57:54 [config.py:3206] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64] is overridden by config [64, 32, 2, 1, 4, 40, 8, 48, 16, 56, 24]\n(VllmWorkerProcess pid=110192) INFO 04-10 08:57:54 [config.py:3206] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64] is overridden by config [64, 32, 2, 1, 4, 40, 8, 48, 16, 56, 24]\n(VllmWorkerProcess pid=110191) INFO 04-10 08:57:54 [config.py:3206] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64] is overridden by config [64, 32, 2, 1, 4, 40, 8, 48, 16, 56, 24]\nINFO 04-10 08:57:54 [config.py:3206] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64] is overridden by config [64, 32, 2, 1, 4, 40, 8, 48, 16, 56, 24]\nLoading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:   8% Completed | 1/12 [00:00<00:05,  1.94it/s]\nLoading safetensors checkpoint shards:  17% Completed | 2/12 [00:01<00:06,  1.66it/s]\nLoading safetensors checkpoint shards:  25% Completed | 3/12 [00:01<00:05,  1.60it/s]\nLoading safetensors checkpoint shards:  33% Completed | 4/12 [00:02<00:05,  1.59it/s]\nLoading safetensors checkpoint shards:  42% Completed | 5/12 [00:03<00:04,  1.59it/s]\nLoading safetensors checkpoint shards:  50% Completed | 6/12 [00:03<00:03,  1.58it/s]\nLoading safetensors checkpoint shards:  58% Completed | 7/12 [00:04<00:03,  1.58it/s]\nLoading safetensors checkpoint shards:  67% Completed | 8/12 [00:04<00:02,  1.59it/s]\nLoading safetensors checkpoint shards:  75% Completed | 9/12 [00:05<00:01,  2.11it/s]\nLoading safetensors checkpoint shards:  83% Completed | 10/12 [00:05<00:00,  2.01it/s]\nLoading safetensors checkpoint shards:  92% Completed | 11/12 [00:06<00:00,  1.87it/s]\n(VllmWorkerProcess pid=110190) INFO 04-10 08:58:01 [loader.py:429] Loading weights took 6.74 seconds\n(VllmWorkerProcess pid=110192) INFO 04-10 08:58:01 [loader.py:429] Loading weights took 6.81 seconds\n(VllmWorkerProcess pid=110191) INFO 04-10 08:58:01 [loader.py:429] Loading weights took 6.95 seconds\nLoading safetensors checkpoint shards: 100% Completed | 12/12 [00:06<00:00,  1.82it/s]\nLoading safetensors checkpoint shards: 100% Completed | 12/12 [00:06<00:00,  1.75it/s]\n\n(VllmWorkerProcess pid=110190) INFO 04-10 08:58:01 [model_runner.py:1146] Model loading took 13.1664 GB and 7.351825 seconds\nINFO 04-10 08:58:01 [loader.py:429] Loading weights took 6.93 seconds\n(VllmWorkerProcess pid=110192) INFO 04-10 08:58:01 [model_runner.py:1146] Model loading took 13.1664 GB and 7.421994 seconds\n(VllmWorkerProcess pid=110191) INFO 04-10 08:58:01 [model_runner.py:1146] Model loading took 13.1664 GB and 7.573320 seconds\nINFO 04-10 08:58:01 [model_runner.py:1146] Model loading took 13.1664 GB and 7.645588 seconds\n(VllmWorkerProcess pid=110192) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n(VllmWorkerProcess pid=110190) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n(VllmWorkerProcess pid=110191) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n(VllmWorkerProcess pid=110192) INFO 04-10 08:58:24 [worker.py:267] Memory profiling takes 22.84 seconds\n(VllmWorkerProcess pid=110192) INFO 04-10 08:58:24 [worker.py:267] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.90) = 35.44GiB\n(VllmWorkerProcess pid=110192) INFO 04-10 08:58:24 [worker.py:267] model weights take 13.17GiB; non_torch_memory takes 0.40GiB; PyTorch activation peak memory takes 3.73GiB; the rest of the memory reserved for KV Cache is 18.14GiB.\n(VllmWorkerProcess pid=110191) INFO 04-10 08:58:24 [worker.py:267] Memory profiling takes 22.83 seconds\n(VllmWorkerProcess pid=110191) INFO 04-10 08:58:24 [worker.py:267] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.90) = 35.44GiB\n(VllmWorkerProcess pid=110191) INFO 04-10 08:58:24 [worker.py:267] model weights take 13.17GiB; non_torch_memory takes 0.48GiB; PyTorch activation peak memory takes 3.73GiB; the rest of the memory reserved for KV Cache is 18.06GiB.\n(VllmWorkerProcess pid=110190) INFO 04-10 08:58:24 [worker.py:267] Memory profiling takes 22.86 seconds\n(VllmWorkerProcess pid=110190) INFO 04-10 08:58:24 [worker.py:267] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.90) = 35.44GiB\n(VllmWorkerProcess pid=110190) INFO 04-10 08:58:24 [worker.py:267] model weights take 13.17GiB; non_torch_memory takes 0.48GiB; PyTorch activation peak memory takes 3.73GiB; the rest of the memory reserved for KV Cache is 18.06GiB.\nINFO 04-10 08:58:24 [worker.py:267] Memory profiling takes 22.92 seconds\nINFO 04-10 08:58:24 [worker.py:267] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.90) = 35.44GiB\nINFO 04-10 08:58:24 [worker.py:267] model weights take 13.17GiB; non_torch_memory takes 0.52GiB; PyTorch activation peak memory takes 3.73GiB; the rest of the memory reserved for KV Cache is 18.03GiB.\nINFO 04-10 08:58:25 [executor_base.py:111] # cuda blocks: 9526, # CPU blocks: 2114\nINFO 04-10 08:58:25 [executor_base.py:116] Maximum concurrency for 65536 tokens per request: 2.33x\nINFO 04-10 08:58:28 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nCapturing CUDA graph shapes:   0%|                                                                                                                                                                                                     | 0/11 [00:00<?, ?it/s](VllmWorkerProcess pid=110192) INFO 04-10 08:58:28 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n(VllmWorkerProcess pid=110191) INFO 04-10 08:58:29 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n(VllmWorkerProcess pid=110190) INFO 04-10 08:58:29 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nCapturing CUDA graph shapes:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                 | 10/11 [00:09<00:00,  1.13it/s](VllmWorkerProcess pid=110192) INFO 04-10 08:58:41 [model_runner.py:1570] Graph capturing finished in 12 secs, took 0.29 GiB\n(VllmWorkerProcess pid=110190) INFO 04-10 08:58:41 [model_runner.py:1570] Graph capturing finished in 12 secs, took 0.29 GiB\nCapturing CUDA graph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:12<00:00,  1.11s/it]\nINFO 04-10 08:58:41 [model_runner.py:1570] Graph capturing finished in 12 secs, took 0.29 GiB\n(VllmWorkerProcess pid=110191) INFO 04-10 08:58:41 [model_runner.py:1570] Graph capturing finished in 12 secs, took 0.29 GiB\nINFO 04-10 08:58:41 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 39.37 seconds\nINFO 04-10 08:58:41 [api_server.py:1019] Starting vLLM API server on http://0.0.0.0:30041\nINFO 04-10 08:58:41 [launcher.py:26] Available routes are:\nINFO 04-10 08:58:41 [launcher.py:34] Route: /openapi.json, Methods: GET, HEAD\nINFO 04-10 08:58:41 [launcher.py:34] Route: /docs, Methods: GET, HEAD\nINFO 04-10 08:58:41 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: GET, HEAD\nINFO 04-10 08:58:41 [launcher.py:34] Route: /redoc, Methods: GET, HEAD\nINFO 04-10 08:58:41 [launcher.py:34] Route: /health, Methods: GET\nINFO 04-10 08:58:41 [launcher.py:34] Route: /load, Methods: GET\nINFO 04-10 08:58:41 [launcher.py:34] Route: /ping, Methods: GET, POST\nINFO 04-10 08:58:41 [launcher.py:34] Route: /tokenize, Methods: POST\nINFO 04-10 08:58:41 [launcher.py:34] Route: /detokenize, Methods: POST\nINFO 04-10 08:58:41 [launcher.py:34] Route: /v1/models, Methods: GET\nINFO 04-10 08:58:41 [launcher.py:34] Route: /version, Methods: GET\nINFO 04-10 08:58:41 [launcher.py:34] Route: /v1/chat/completions, Methods: POST\nINFO 04-10 08:58:41 [launcher.py:34] Route: /v1/completions, Methods: POST\nINFO 04-10 08:58:41 [launcher.py:34] Route: /v1/embeddings, Methods: POST\nINFO 04-10 08:58:41 [launcher.py:34] Route: /pooling, Methods: POST\nINFO 04-10 08:58:41 [launcher.py:34] Route: /score, Methods: POST\nINFO 04-10 08:58:41 [launcher.py:34] Route: /v1/score, Methods: POST\nINFO 04-10 08:58:41 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST\nINFO 04-10 08:58:41 [launcher.py:34] Route: /rerank, Methods: POST\nINFO 04-10 08:58:41 [launcher.py:34] Route: /v1/rerank, Methods: POST\nINFO 04-10 08:58:41 [launcher.py:34] Route: /v2/rerank, Methods: POST\nINFO 04-10 08:58:41 [launcher.py:34] Route: /invocations, Methods: POST\nINFO:     Started server process [109649]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\n\n\n\n\u5728\u670d\u52a1\u8fd0\u884c\u8fc7\u7a0b\u4e00\u5f00\u59cb\u8fd8\u53ef\u4ee5\uff0c\u4f46\u662f\u5076\u5c14\u4f1a\u7a81\u7136\u62a5\u9519\uff0c\u4e0d\u77e5\u5230\u662f\u4ec0\u4e48\u539f\u56e0\nINFO 04-10 08:57:04 [logger.py:39] Received request chatcmpl-e63bcf8929744e8786c3bc92ee98e61f: prompt: '<bos><start_of_turn>user\\n\u4e00\u4ee5\u4e0b\u516d\u4e2a\u56fe\u7247\u4e2d,\u54ea\u4e2a\u7b26\u5408\u751c\u5473\u7684\u7269\u54c1\u7684\u7f16\u53f7\uff0c\u8fd9\u4e2a\u9898\u76ee\u5927\u6982\u7387\u662f\u591a\u9009\uff0c\u4f60\u8981\u4ed4\u7ec6\u601d\u8003\u540e\u56de\u7b54\u6211,\u8fd9\u6837\u56de\u7b54\u6211\uff1a\u7b26\u5408\u9898\u76ee\u7684\u662f[\u00d7\uff0c\u00d7\uff0c\u00d7]\u3002<start_of_image><end_of_turn>\\n<start_of_turn>model\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=65482, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\nINFO 04-10 08:57:04 [engine.py:308] Added request chatcmpl-c2dfadcdc64742d08d0aa031175f18d4.\nINFO 04-10 08:57:04 [engine.py:308] Added request chatcmpl-75ada0a216c349f181624e7a24b89bec.\nINFO 04-10 08:57:04 [engine.py:308] Added request chatcmpl-18014ac1484c48be9099aa83dceae649.\nINFO 04-10 08:57:04 [engine.py:308] Added request chatcmpl-b80eb0730ade454385fa38333a5bda55.\nINFO 04-10 08:57:04 [engine.py:308] Added request chatcmpl-e63bcf8929744e8786c3bc92ee98e61f.\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop.\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242] Traceback (most recent call last):\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/executor/multiproc_worker_utils.py\", line 236, in _run_worker_process\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/utils.py\", line 2216, in run_method\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop.\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     return func(*args, **kwargs)\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/worker/worker_base.py\", line 91, in start_worker_execution_loop\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242] Traceback (most recent call last):\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     output = self.execute_model(execute_model_req=None)\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/executor/multiproc_worker_utils.py\", line 236, in _run_worker_process\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/worker/worker_base.py\", line 420, in execute_model\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop.\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     output = self.model_runner.execute_model(\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242] Traceback (most recent call last):\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/utils.py\", line 2216, in run_method\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/miniconda/envs/torch/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/executor/multiproc_worker_utils.py\", line 236, in _run_worker_process\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     return func(*args, **kwargs)\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     return func(*args, **kwargs)\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/worker/worker_base.py\", line 91, in start_worker_execution_loop\nERROR 04-10 08:57:05 [engine.py:158] ValueError('Attempted to assign 5 x 256 = 1280 multimodal tokens to 1024 placeholders')\nERROR 04-10 08:57:05 [engine.py:158] Traceback (most recent call last):\nERROR 04-10 08:57:05 [engine.py:158]   File \"/data/coding/vllm/vllm/engine/multiprocessing/engine.py\", line 156, in start\nERROR 04-10 08:57:05 [engine.py:158]     self.run_engine_loop()\nERROR 04-10 08:57:05 [engine.py:158]   File \"/data/coding/vllm/vllm/engine/multiprocessing/engine.py\", line 219, in run_engine_loop\nERROR 04-10 08:57:05 [engine.py:158]     request_outputs = self.engine_step()\nERROR 04-10 08:57:05 [engine.py:158]   File \"/data/coding/vllm/vllm/engine/multiprocessing/engine.py\", line 245, in engine_step\nERROR 04-10 08:57:05 [engine.py:158]     raise e\nERROR 04-10 08:57:05 [engine.py:158]   File \"/data/coding/vllm/vllm/engine/multiprocessing/engine.py\", line 228, in engine_step\nERROR 04-10 08:57:05 [engine.py:158]     return self.engine.step()\nERROR 04-10 08:57:05 [engine.py:158]   File \"/data/coding/vllm/vllm/engine/llm_engine.py\", line 1435, in step\nERROR 04-10 08:57:05 [engine.py:158]     outputs = self.model_executor.execute_model(\nERROR 04-10 08:57:05 [engine.py:158]   File \"/data/coding/vllm/vllm/executor/executor_base.py\", line 284, in execute_model\nERROR 04-10 08:57:05 [engine.py:158]     driver_outputs = self._driver_execute_model(execute_model_req)\nERROR 04-10 08:57:05 [engine.py:158]   File \"/data/coding/vllm/vllm/executor/mp_distributed_executor.py\", line 144, in _driver_execute_model\nERROR 04-10 08:57:05 [engine.py:158]     return self.driver_worker.execute_model(execute_model_req)\nERROR 04-10 08:57:05 [engine.py:158]   File \"/data/coding/vllm/vllm/worker/worker_base.py\", line 420, in execute_model\nERROR 04-10 08:57:05 [engine.py:158]     output = self.model_runner.execute_model(\nERROR 04-10 08:57:05 [engine.py:158]   File \"/data/miniconda/envs/torch/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 04-10 08:57:05 [engine.py:158]     return func(*args, **kwargs)\nERROR 04-10 08:57:05 [engine.py:158]   File \"/data/coding/vllm/vllm/worker/model_runner.py\", line 1742, in execute_model\nERROR 04-10 08:57:05 [engine.py:158]     hidden_or_intermediate_states = model_executable(\nERROR 04-10 08:57:05 [engine.py:158]   File \"/data/miniconda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nERROR 04-10 08:57:05 [engine.py:158]     return self._call_impl(*args, **kwargs)\nERROR 04-10 08:57:05 [engine.py:158]   File \"/data/miniconda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\nERROR 04-10 08:57:05 [engine.py:158]     return forward_call(*args, **kwargs)\nERROR 04-10 08:57:05 [engine.py:158]   File \"/data/coding/vllm/vllm/model_executor/models/gemma3_mm.py\", line 521, in forward\nERROR 04-10 08:57:05 [engine.py:158]     inputs_embeds = self.get_input_embeddings(input_ids,\nERROR 04-10 08:57:05 [engine.py:158]   File \"/data/coding/vllm/vllm/model_executor/models/gemma3_mm.py\", line 502, in get_input_embeddings\nERROR 04-10 08:57:05 [engine.py:158]     inputs_embeds = merge_multimodal_embeddings(\nERROR 04-10 08:57:05 [engine.py:158]   File \"/data/coding/vllm/vllm/model_executor/models/utils.py\", line 455, in merge_multimodal_embeddings\nERROR 04-10 08:57:05 [engine.py:158]     return _merge_multimodal_embeddings(\nERROR 04-10 08:57:05 [engine.py:158]   File \"/data/coding/vllm/vllm/model_executor/models/utils.py\", line 371, in _merge_multimodal_embeddings\nERROR 04-10 08:57:05 [engine.py:158]     raise ValueError(\nERROR 04-10 08:57:05 [engine.py:158] ValueError: Attempted to assign 5 x 256 = 1280 multimodal tokens to 1024 placeholders\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/worker/model_runner.py\", line 1742, in execute_model\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/utils.py\", line 2216, in run_method\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     output = self.execute_model(execute_model_req=None)\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     hidden_or_intermediate_states = model_executable(\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     return func(*args, **kwargs)\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/worker/worker_base.py\", line 420, in execute_model\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/miniconda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/worker/worker_base.py\", line 91, in start_worker_execution_loop\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     output = self.model_runner.execute_model(\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     output = self.execute_model(execute_model_req=None)\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/miniconda/envs/torch/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/miniconda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/worker/worker_base.py\", line 420, in execute_model\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     return func(*args, **kwargs)\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     output = self.model_runner.execute_model(\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/worker/model_runner.py\", line 1742, in execute_model\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/model_executor/models/gemma3_mm.py\", line 521, in forward\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/miniconda/envs/torch/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     hidden_or_intermediate_states = model_executable(\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     inputs_embeds = self.get_input_embeddings(input_ids,\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     return func(*args, **kwargs)\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/miniconda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/model_executor/models/gemma3_mm.py\", line 502, in get_input_embeddings\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/worker/model_runner.py\", line 1742, in execute_model\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     inputs_embeds = merge_multimodal_embeddings(\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     hidden_or_intermediate_states = model_executable(\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/miniconda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/model_executor/models/utils.py\", line 455, in merge_multimodal_embeddings\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/miniconda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     return _merge_multimodal_embeddings(\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     return self._call_impl(*args, **kwargs)\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/model_executor/models/gemma3_mm.py\", line 521, in forward\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/model_executor/models/utils.py\", line 371, in _merge_multimodal_embeddings\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/miniconda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     inputs_embeds = self.get_input_embeddings(input_ids,\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     raise ValueError(\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     return forward_call(*args, **kwargs)\nERROR 04-10 08:57:05 [serving_chat.py:677] Error in chat completion stream generator.\nERROR 04-10 08:57:05 [serving_chat.py:677] Traceback (most recent call last):\nERROR 04-10 08:57:05 [serving_chat.py:677]   File \"/data/coding/vllm/vllm/entrypoints/openai/serving_chat.py\", line 373, in chat_completion_stream_generator\nERROR 04-10 08:57:05 [serving_chat.py:677]     async for res in result_generator:\nERROR 04-10 08:57:05 [serving_chat.py:677]   File \"/data/coding/vllm/vllm/engine/multiprocessing/client.py\", line 664, in _process_request\nERROR 04-10 08:57:05 [serving_chat.py:677]     raise request_output\nERROR 04-10 08:57:05 [serving_chat.py:677]   File \"/data/coding/vllm/vllm/entrypoints/utils.py\", line 73, in wrapper\nERROR 04-10 08:57:05 [serving_chat.py:677]     return await func(*args, raw_request=raw_request, **kwargs)\nERROR 04-10 08:57:05 [serving_chat.py:677]   File \"/data/coding/vllm/vllm/entrypoints/openai/api_server.py\", line 455, in create_chat_completion\nERROR 04-10 08:57:05 [serving_chat.py:677]     generator = await handler.create_chat_completion(request, raw_request)\nERROR 04-10 08:57:05 [serving_chat.py:677]   File \"/data/coding/vllm/vllm/entrypoints/openai/serving_chat.py\", line 277, in create_chat_completion\nERROR 04-10 08:57:05 [serving_chat.py:677]     return await self.chat_completion_full_generator(\nERROR 04-10 08:57:05 [serving_chat.py:677]   File \"/data/coding/vllm/vllm/entrypoints/openai/serving_chat.py\", line 698, in chat_completion_full_generator\nERROR 04-10 08:57:05 [serving_chat.py:677]     async for res in result_generator:\nERROR 04-10 08:57:05 [serving_chat.py:677]   File \"/data/coding/vllm/vllm/engine/multiprocessing/client.py\", line 664, in _process_request\nERROR 04-10 08:57:05 [serving_chat.py:677]     raise request_output\nERROR 04-10 08:57:05 [serving_chat.py:677]   File \"/data/coding/vllm/vllm/entrypoints/utils.py\", line 73, in wrapper\nERROR 04-10 08:57:05 [serving_chat.py:677]     return await func(*args, raw_request=raw_request, **kwargs)\nERROR 04-10 08:57:05 [serving_chat.py:677]   File \"/data/coding/vllm/vllm/entrypoints/openai/api_server.py\", line 455, in create_chat_completion\nERROR 04-10 08:57:05 [serving_chat.py:677]     generator = await handler.create_chat_completion(request, raw_request)\nERROR 04-10 08:57:05 [serving_chat.py:677]   File \"/data/coding/vllm/vllm/entrypoints/openai/serving_chat.py\", line 277, in create_chat_completion\nERROR 04-10 08:57:05 [serving_chat.py:677]     return await self.chat_completion_full_generator(\nERROR 04-10 08:57:05 [serving_chat.py:677]   File \"/data/coding/vllm/vllm/entrypoints/openai/serving_chat.py\", line 698, in chat_completion_full_generator\nERROR 04-10 08:57:05 [serving_chat.py:677]     async for res in result_generator:\nERROR 04-10 08:57:05 [serving_chat.py:677]   File \"/data/coding/vllm/vllm/engine/multiprocessing/client.py\", line 664, in _process_request\nERROR 04-10 08:57:05 [serving_chat.py:677]     raise request_output\nERROR 04-10 08:57:05 [serving_chat.py:677] vllm.engine.multiprocessing.MQEngineDeadError: Engine loop is not running. Inspect the stacktrace to find the original error: ValueError('Attempted to assign 5 x 256 = 1280 multimodal tokens to 1024 placeholders').\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/model_executor/models/gemma3_mm.py\", line 502, in get_input_embeddings\n(VllmWorkerProcess pid=75137) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242] ValueError: Attempted to assign 5 x 256 = 1280 multimodal tokens to 1024 placeholders\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/model_executor/models/gemma3_mm.py\", line 521, in forward\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     inputs_embeds = merge_multimodal_embeddings(\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     inputs_embeds = self.get_input_embeddings(input_ids,\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/model_executor/models/utils.py\", line 455, in merge_multimodal_embeddings\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/model_executor/models/gemma3_mm.py\", line 502, in get_input_embeddings\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     return _merge_multimodal_embeddings(\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     inputs_embeds = merge_multimodal_embeddings(\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/model_executor/models/utils.py\", line 371, in _merge_multimodal_embeddings\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/model_executor/models/utils.py\", line 455, in merge_multimodal_embeddings\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     raise ValueError(\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     return _merge_multimodal_embeddings(\n(VllmWorkerProcess pid=75138) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242] ValueError: Attempted to assign 5 x 256 = 1280 multimodal tokens to 1024 placeholders\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]   File \"/data/coding/vllm/vllm/model_executor/models/utils.py\", line 371, in _merge_multimodal_embeddings\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242]     raise ValueError(\n(VllmWorkerProcess pid=75134) ERROR 04-10 08:57:05 [multiproc_worker_utils.py:242] ValueError: Attempted to assign 5 x 256 = 1280 multimodal tokens to 1024 placeholders\nCRITICAL 04-10 08:57:05 [launcher.py:116] MQLLMEngine is already dead, terminating server process\nCRITICAL 04-10 08:57:05 [launcher.py:116] MQLLMEngine is already dead, terminating server process\nCRITICAL 04-10 08:57:05 [launcher.py:116] MQLLMEngine is already dead, terminating server process\nCRITICAL 04-10 08:57:05 [launcher.py:116] MQLLMEngine is already dead, terminating server process\nCRITICAL 04-10 08:57:05 [launcher.py:116] MQLLMEngine is already dead, terminating server process\nINFO:     10.222.50.203:61452 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\nINFO:     10.222.50.203:12744 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\nINFO:     10.222.50.203:55096 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\nINFO:     10.222.50.203:10830 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\nINFO:     10.222.50.203:3378 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\nINFO:     Shutting down\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\nINFO:     Finished server process [74595]\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/data/miniconda/envs/torch/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n    util._exit_function()\n  File \"/data/miniconda/envs/torch/lib/python3.10/multiprocessing/util.py\", line 357, in _exit_function\n    p.join()\n  File \"/data/miniconda/envs/torch/lib/python3.10/multiprocessing/process.py\", line 149, in join\n    res = self._popen.wait(timeout)\n  File \"/data/miniconda/envs/torch/lib/python3.10/multiprocessing/popen_fork.py\", line 43, in wait\n    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n  File \"/data/miniconda/envs/torch/lib/python3.10/multiprocessing/popen_fork.py\", line 27, in poll\n    pid, sts = os.waitpid(self.pid, flag)\n  File \"/data/coding/vllm/vllm/engine/multiprocessing/engine.py\", line 424, in signal_handler\n    raise KeyboardInterrupt(\"MQLLMEngine terminated\")\nKeyboardInterrupt: MQLLMEngine terminated\nINFO 04-10 08:57:05 [multiproc_worker_utils.py:141] Terminating local vLLM worker processes\n/data/miniconda/envs/torch/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 3 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n/data/miniconda/envs/torch/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n\n\n\u6211\u7528\u7684\u662f4\u5361A100 40G\u90e8\u7f72\uff0c\u90e8\u7f72\u6307\u4ee4\u662f vllm serve Gemma-3-27b --tensor-parallel-size 4 --max-model-len 65536 --max-num-batched-tokens 16384 --port 30041  --trust-remote-code  --served-model-name gemma3-27b  --max-num-seqs 64 --enable-chunked-prefill --limit-mm-per-prompt image=50,video=2 --api-key k7YgF9RwP4qXmTnV2LsJ3HdO5zIc6AeB0Uv1lKpN8Q\n\n\u53ef\u4ee5\u5e2e\u5fd9\u770b\u4e00\u4e0b\u662f\u4ec0\u4e48\u539f\u56e0\u5417\n\n\n\n\n### Suggest a potential alternative/fix\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2025-04-10T02:34:32+00:00",
    "closed_at": "2025-04-10T02:40:24+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16378/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16378"
  },
  {
    "number": 8352,
    "title": "[Bug]: Kernel died while waiting for execute reply in Kaggle TPU VM v3-8 (2024-08-22)",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\n2024-09-11 04:42:09.993145: F external/local_xla/xla/stream_executor/tpu/tpu_executor_init_fns.inc:25] TpuExecutor_AllocateStream not available in this library.\r\nbash: line 2:  1424 Aborted                 (core dumped) python collect_env.py\r\n---------------------------------------------------------------------------\r\nCalledProcessError                        Traceback (most recent call last)\r\nCell In[6], line 1\r\n----> 1 get_ipython().run_cell_magic('bash', '', '#wget [https://raw.githubusercontent.com/vllm-project/vllm/main/collect_env.py](https://raw.githubusercontent.com/vllm-project/vllm/main/collect_env.py%3C/span%3E%3Cspan) class=\"ansi-yellow-bg ansi-bold\" style=\"color:rgb(175,95,0)\">\\npython collect_env.py\\n')\r\n\r\nFile /usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\r\n   2539 with self.builtin_trap:\r\n   2540     args = (magic_arg_s, cell)\r\n-> 2541     result = fn(*args, **kwargs)\r\n   2543 # The code below prevents the output from being displayed\r\n   2544 # when using magics with decorator @output_can_be_silenced\r\n   2545 # when the last Python token in the expression is a ';'.\r\n   2546 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\r\n\r\nFile /usr/local/lib/python3.10/site-packages/IPython/core/magics/script.py:155, in ScriptMagics._make_script_magic.<locals>.named_script_magic(line, cell)\r\n    153 else:\r\n    154     line = script\r\n--> 155 return self.shebang(line, cell)\r\n\r\nFile /usr/local/lib/python3.10/site-packages/IPython/core/magics/script.py:315, in ScriptMagics.shebang(self, line, cell)\r\n    310 if args.raise_error and p.returncode != 0:\r\n    311     # If we get here and p.returncode is still None, we must have\r\n    312     # killed it but not yet seen its return code. We don't wait for it,\r\n    313     # in case it's stuck in uninterruptible sleep. -9 = SIGKILL\r\n    314     rc = p.returncode or -9\r\n--> 315     raise CalledProcessError(rc, cell)\r\n\r\nCalledProcessError: Command 'b'#wget [https://raw.githubusercontent.com/vllm-project/vllm/main/collect_env.py\\npython](https://raw.githubusercontent.com/vllm-project/vllm/main/collect_env.py/npython) collect_env.py\\n'' returned non-zero exit status 134.\r\n```\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nFollow instructions in https://docs.vllm.ai/en/stable/getting_started/tpu-installation.html#build-from-source, but\r\n- use `libopenblas-dev` instead of `libopenblas-base` because latter is not available for Debian Bookworm\r\n\r\nRun `import vllm`, kernel dies after about half a minute:\r\n```\r\n129.7s | 1009 | /usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\r\n-- | -- | --\r\n129.7s | 1010 | from .autonotebook import tqdm as notebook_tqdm\r\n129.7s | 1011 | \u00a0\r\n129.7s | 1012 | /usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\r\n129.7s | 1013 | from .autonotebook import tqdm as notebook_tqdm\r\n141.7s | 1014 | Kernel died while waiting for execute reply.\r\n141.8s | 1015 | Traceback (most recent call last):\r\n141.8s | 1016 | File \"<string>\", line 1, in <module>\r\n141.8s | 1017 | File \"/usr/local/lib/python3.10/site-packages/papermill/execute.py\", line 116, in execute_notebook\r\n141.8s | 1018 | nb = papermill_engines.execute_notebook_with_engine(\r\n141.8s | 1019 | File \"/usr/local/lib/python3.10/site-packages/papermill/engines.py\", line 48, in execute_notebook_with_engine\r\n141.8s | 1020 | return self.get_engine(engine_name).execute_notebook(nb, kernel_name, **kwargs)\r\n141.8s | 1021 | File \"/usr/local/lib/python3.10/site-packages/papermill/engines.py\", line 370, in execute_notebook\r\n141.8s | 1022 | cls.execute_managed_notebook(nb_man, kernel_name, log_output=log_output, **kwargs)\r\n141.8s | 1023 | File \"/usr/local/lib/python3.10/site-packages/papermill/engines.py\", line 442, in execute_managed_notebook\r\n141.8s | 1024 | return PapermillNotebookClient(nb_man, **final_kwargs).execute()\r\n141.8s | 1025 | File \"/usr/local/lib/python3.10/site-packages/papermill/clientwrap.py\", line 45, in execute\r\n141.8s | 1026 | self.papermill_execute_cells()\r\n141.8s | 1027 | File \"/usr/local/lib/python3.10/site-packages/papermill/clientwrap.py\", line 72, in papermill_execute_cells\r\n141.8s | 1028 | self.execute_cell(cell, index)\r\n141.8s | 1029 | File \"/usr/local/lib/python3.10/site-packages/jupyter_core/utils/__init__.py\", line 165, in wrapped\r\n141.8s | 1030 | return loop.run_until_complete(inner)\r\n141.8s | 1031 | File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\r\n141.8s | 1032 | return future.result()\r\n141.8s | 1033 | File \"/usr/local/lib/python3.10/site-packages/nbclient/client.py\", line 1009, in async_execute_cell\r\n141.8s | 1034 | raise DeadKernelError(\"Kernel died\") from None\r\n141.8s | 1035 | nbclient.exceptions.DeadKernelError: Kernel died\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-09-11T04:50:40+00:00",
    "closed_at": "2024-09-11T05:05:10+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8352/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8352"
  },
  {
    "number": 1716,
    "title": "ImportError: libcudart.so.12: cannot open shared object file: No such file or directory",
    "body": "My code work well yesterday but now it is not working today since the latest update (v.0.2.2)!\r\nMy code:\r\n```python\r\nfrom langchain.llms import VLLM\r\nllm = VLLM(\r\n    model=GENERATE_MODEL_NAME,\r\n    trust_remote_code=True,  # mandatory for hf models\r\n    max_new_tokens=max_new_tokens,\r\n    top_k=10,\r\n    top_p=0.95,\r\n    temperature=0.8,\r\n    # dtype=\"half\",\r\n    vllm_kwargs={\"quantization\": \"awq\"}\r\n)\r\n```\r\nThe error:\r\n```\r\nImportError: libcudart.so.12: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n[/usr/local/lib/python3.10/dist-packages/langchain/llms/vllm.py](https://localhost:8080/#) in validate_environment(cls, values)\r\n     79             from vllm import LLM as VLLModel\r\n     80         except ImportError:\r\n---> 81             raise ImportError(\r\n     82                 \"Could not import vllm python package. \"\r\n     83                 \"Please install it with `pip install vllm`.\"\r\n\r\nImportError: Could not import vllm python package. Please install it with `pip install vllm`.\r\n```\r\nI tried 'pip install vllm' many time but not fixed. I run on my Google Colab T4.\r\nAnyone has the same issue?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-11-19T07:18:47+00:00",
    "closed_at": "2023-11-19T07:48:59+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1716/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1716"
  },
  {
    "number": 16345,
    "title": "[Feature]: support for Llama 4 family models",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nLlama 4 is out there. hence, please add support for llama 4 models too\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-04-09T13:10:16+00:00",
    "closed_at": "2025-04-09T13:12:47+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16345/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16345"
  },
  {
    "number": 12825,
    "title": "[Feature]: Support Qwen2.5-VL-7B Instruct",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nQwen2.5-VL-7B Instruct\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-02-06T12:01:41+00:00",
    "closed_at": "2025-02-06T12:27:21+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12825/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12825"
  },
  {
    "number": 20685,
    "title": "[Usage]: Local model returns empty text with vLLM despite matching upstream HF files",
    "body": "### Your current environment\n\n```text\nINFO 07-09 20:53:46 [__init__.py:240] Automatically detected platform rocm.\nCollecting environment information...\n/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n  warnings.warn(\nPyTorch version: 2.4.1\nIs debug build: False\nCUDA used to build PyTorch: N/A\nROCM used to build PyTorch: 6.3.25211\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: 15.0.0\nCMake version: version 3.29.0\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-4.18.0-348.el8.x86_64-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: K100_AI (gfx928:sramecc+:xnack-)\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\nHIP runtime version: 6.3.25211\nMIOpen runtime version: 2.17.0\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   52 bits physical, 57 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          128\nOn-line CPU(s) list:             0-127\nVendor ID:                       GenuineIntel\nModel name:                      Intel(R) Xeon(R) Gold 6430\nCPU family:                      6\nModel:                           143\nThread(s) per core:              2\nCore(s) per socket:              32\nSocket(s):                       2\nStepping:                        8\nFrequency boost:                 enabled\nCPU max MHz:                     2101.0000\nCPU min MHz:                     800.0000\nBogoMIPS:                        4200.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust sgx bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd sgx_lc fsrm md_clear serialize tsxldtrk pconfig arch_lbr avx512_fp16 flush_l1d arch_capabilities\nVirtualization:                  VT-x\nL1d cache:                       3 MiB (64 instances)\nL1i cache:                       2 MiB (64 instances)\nL2 cache:                        128 MiB (64 instances)\nL3 cache:                        120 MiB (2 instances)\nNUMA node(s):                    2\nNUMA node0 CPU(s):               0-31,64-95\nNUMA node1 CPU(s):               32-63,96-127\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.25.0\n[pip3] nvidia-ml-py==12.575.51\n[pip3] onnxruntime==1.19.2+das.opt1.dtk25041\n[pip3] pynvml==12.0.0\n[pip3] pyzmq==27.0.0\n[pip3] torch==2.4.1+das.opt1.dtk25041\n[pip3] torchaudio==2.4.1+das.opt1.dtk25041\n[pip3] torchdata==0.8.0\n[pip3] torchvision==0.19.1+das.opt1.dtk25041\n[pip3] transformers==4.51.1\n[pip3] triton==3.0.0+das.opt1.dtk25041\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.5.post1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n============================ System Management Interface =============================\n======================================================================================\nLink accessible between HCUs\n          HCU[0]    HCU[1]    HCU[2]    HCU[3]    HCU[4]    HCU[5]    HCU[6]    \nHCU[0]    TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      \nHCU[1]    TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      \nHCU[2]    TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      \nHCU[3]    TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      \nHCU[4]    TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      \nHCU[5]    TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      \nHCU[6]    TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      \n======================================================================================\n======================================================================================\nWeight between HCUs\n          HCU[0]    HCU[1]    HCU[2]    HCU[3]    HCU[4]    HCU[5]    HCU[6]    \nHCU[0]    0         40        40        40        61        61        61        \nHCU[1]    40        0         40        40        61        61        61        \nHCU[2]    40        40        0         40        61        61        61        \nHCU[3]    40        40        40        0         61        61        61        \nHCU[4]    61        61        61        61        0         40        40        \nHCU[5]    61        61        61        61        40        0         40        \nHCU[6]    61        61        61        61        40        40        0         \n======================================================================================\n======================================================================================\nHops between HCUs\n          HCU[0]    HCU[1]    HCU[2]    HCU[3]    HCU[4]    HCU[5]    HCU[6]    \nHCU[0]    0         2         2         2         3         3         3         \nHCU[1]    2         0         2         2         3         3         3         \nHCU[2]    2         2         0         2         3         3         3         \nHCU[3]    2         2         2         0         3         3         3         \nHCU[4]    3         3         3         3         0         2         2         \nHCU[5]    3         3         3         3         2         0         2         \nHCU[6]    3         3         3         3         2         2         0         \n======================================================================================\n======================================================================================\nLink Type between HCUs\n          HCU[0]    HCU[1]    HCU[2]    HCU[3]    HCU[4]    HCU[5]    HCU[6]    \nHCU[0]    None      PCIE      PCIE      PCIE      PCIE      PCIE      PCIE      \nHCU[1]    PCIE      None      PCIE      PCIE      PCIE      PCIE      PCIE      \nHCU[2]    PCIE      PCIE      None      PCIE      PCIE      PCIE      PCIE      \nHCU[3]    PCIE      PCIE      PCIE      None      PCIE      PCIE      PCIE      \nHCU[4]    PCIE      PCIE      PCIE      PCIE      None      PCIE      PCIE      \nHCU[5]    PCIE      PCIE      PCIE      PCIE      PCIE      None      PCIE      \nHCU[6]    PCIE      PCIE      PCIE      PCIE      PCIE      PCIE      None      \n======================================================================================\n======================================================================================\nHCU[0]          : Numa Node:  0\nHCU[0]          : Numa Affinity:  0\nHCU[1]          : Numa Node:  0\nHCU[1]          : Numa Affinity:  0\nHCU[2]          : Numa Node:  0\nHCU[2]          : Numa Affinity:  0\nHCU[3]          : Numa Node:  0\nHCU[3]          : Numa Affinity:  0\nHCU[4]          : Numa Node:  1\nHCU[4]          : Numa Affinity:  1\nHCU[5]          : Numa Node:  1\nHCU[5]          : Numa Affinity:  1\nHCU[6]          : Numa Node:  1\nHCU[6]          : Numa Affinity:  1\n======================================================================================\n=================================== End of SMI Log ===================================\n\nLD_LIBRARY_PATH=/usr/local/lib/:/usr/local/lib64/:/opt/mpi/lib:/opt/hwloc/lib:/opt/dtk/hip/lib:/opt/dtk/llvm/lib:/opt/dtk/lib:/opt/dtk/lib64:/opt/hyhal/lib:/opt/hyhal/lib64:/opt/dtk/opencl/lib:/opt/dtk-25.04.1/hip/lib:/opt/dtk-25.04.1/llvm/lib:/opt/dtk-25.04.1/lib:/opt/dtk-25.04.1/lib64:/opt/hyhal/lib:/opt/hyhal/lib64:/opt/dtk-25.04.1/opencl/lib:/usr/local/lib/:/usr/local/lib64/:/opt/mpi/lib:/opt/hwloc/lib:/usr/local/lib/:/usr/local/lib64/:/opt/mpi/lib:/opt/hwloc/lib:/opt/dtk/hip/lib:/opt/dtk/llvm/lib:/opt/dtk/lib:/opt/dtk/lib64:/opt/hyhal/lib:/opt/hyhal/lib64:/opt/dtk/opencl/lib:/opt/dtk-25.04.1/hip/lib:/opt/dtk-25.04.1/llvm/lib:/opt/dtk-25.04.1/lib:/opt/dtk-25.04.1/lib64:/opt/hyhal/lib:/opt/hyhal/lib64:/opt/dtk-25.04.1/opencl/lib:/opt/ucx/lib:/opt/dtk/hip/lib:/opt/dtk/llvm/lib:/opt/dtk/lib:/opt/dtk/lib64:/opt/hyhal/lib:/opt/hyhal/lib64:/opt/mpi/lib:/opt/hwloc/lib:\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\nVLLM_WORKER_MULTIPROC_METHOD=spawn\n```\n\n### How would you like to use vllm\n\nP.S. I experienced the same issue on another platform using 8 NVIDIA A800 GPUs, so I believe it\u2019s not a hardware problem. My work is done in the vllm/vllm-openai:v0.8.5.post1 Docker.\n\n---\n\nI want to run inference of a [google/gemma-3-27b-it](https://huggingface.co/google/gemma-3-27b-it/tree/main) model locally, and I don't know how to integrate it with vllm properly. When serving the Gemma 3-27B IT model from the local path, the server starts successfully, but responses always return empty strings in the \"text\" field. \n\nExample command to serve the model:\n\n```bash\nvllm serve /home/gemma-3-27b-it-q/gemma-3-27b-it \\\n  --tensor-parallel-size 2 \\\n  --port 8000 \\\n  --disable-log-requests \\\n  --trust-remote-code \\\n  --max-model-len 131072\n```\n\nExample output: The model is functioning correctly, as the log shows no unusual output.\n\n```text\nINFO:     Started server process [22023]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\n```\n\nExample curl request:\n\n```bash\ncurl http://localhost:8000/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"/home/gemma-3-27b-it-q/gemma-3-27b-it\",\n    \"prompt\": \"What is the capital of France?\",\n    \"max_tokens\": 128\n  }'\n```\n\nExample result: \n\n```json\n{\n  \"id\": \"cmpl-63798c3de7a146b29e412a31641bff97\",\n  \"object\": \"text_completion\",\n  \"created\": 1751974557,\n  \"model\": \"/home/gemma-3-27b-it-q/gemma-3-27b-it\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"text\": \"\",\n      \"logprobs\": null,\n      \"finish_reason\": \"length\",\n      \"stop_reason\": null,\n      \"prompt_logprobs\": null\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 7,\n    \"total_tokens\": 135,\n    \"completion_tokens\": 128,\n    \"prompt_tokens_details\": null\n  }\n}\n```\n\nExample test file: \n\n```python\nfrom vllm import LLM, SamplingParams\nfrom vllm.assets.image import ImageAsset\nfrom transformers import AutoProcessor\n\n# Define model name once\nmodel_name = \"/home/gemma-3-27b-it-q/gemma-3-27b-it\"\n\n# Load image and processor\nimage = ImageAsset(\"cherry_blossom\").pil_image.convert(\"RGB\")\nprocessor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n\n# Build multimodal prompt\nchat = [\n    {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"What is the content of this image?\"}]},\n    {\"role\": \"assistant\", \"content\": []}\n]\nprompt = processor.apply_chat_template(chat, add_generation_prompt=True)\n\n# Initialize model\nllm = LLM(model=model_name, trust_remote_code=True)\n\n# Run inference\ninputs = {\"prompt\": prompt, \"multi_modal_data\": {\"image\": [image]}}\noutputs = llm.generate(inputs, SamplingParams(temperature=0.2, max_tokens=64))\n\n# Display result\nprint(\"RESPONSE:\", outputs[0].outputs[0].text)\n```\n\nExample result: \n\n```text\nRESPONSE: \n```\n\nWhen I use `vllm complete --url http://localhost:8000/v1` to test, it also return null result. \n\nI believe my testing environment is configured properly: when serving [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) via either `vllm serve` or directly through `LLM(model=..., trust_remote_code=True)` from Python, the model functions as expected and returns correct outputs.\n\nI verified that my local copy of the Gemma model is bit-identical with the Hugging Face official version (google/gemma-3-27b-it) using `sha256sum` on all model files. The folder structure and `tokenizer_config.json` etc. are also intact. \n\nAdditionally, when using `vllm bench serve` to benchmark this model after serving, the result shows that it generates output tokens, but the data appears strange as well:\n\n| Total input tokens | Benchmark duration (s) | Request throughput (req/s) | Output token throughput (tok/s) | Total Token throughput (tok/s)  |\n|--------------------|------------------------|----------------------------|---------------------------------|---------------------------------|\n| 1000               | 29.65                  | 3.37                       | 431.72                          | 465.45                          |\n| 2000               | 30.72                  | 3.26                       | 416.68                          | 481.79                          |\n| 3000               | 31.24                  | 3.2                        | 409.67                          | 505.68                          |\n| 4000               | 31.53                  | 3.17                       | 405.95                          | 532.82                          |\n| 5000               | 32.16                  | 3.11                       | 397.97                          | 553.42                          |\n| 6000               | 32.63                  | 3.06                       | 392.31                          | 576.21                          |\n\nI tried to set `$HF_HUB_OFFLINE=1` and  `$TRANSFORMERS_OFFLINE=1`, it doesn't solve the problem. \n\nDid I do something wrong while serving the local model?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-07-09T13:16:14+00:00",
    "closed_at": "2025-07-09T13:49:58+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20685/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20685"
  },
  {
    "number": 15518,
    "title": "[Doc]: APIConnectionError with OpenAI",
    "body": "### \ud83d\udcda The doc issue\n\nHello,\n\nWhile going through the documentation's QuickStart section (https://docs.vllm.ai/en/latest/getting_started/quickstart.html#openai-compatible-server), I haven't been able to resolve the `APIConnectionError: Connection error.` with the OpenAI API.\n\nThe very same code in the documentation, except the `openai_api_key` where I input my API key:\n```\nimport os\nos.environ['HF_HOME'] = '~/scratch/LLM/cache/'\nimport torch\ntorch.cuda.empty_cache()\n\nfrom openai import OpenAI\n\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"my own API key I got from OpenAI\"\nopenai_api_base = \"http://localhost:8000/v1\"\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\ncompletion = client.completions.create(model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n                                      prompt=\"San Francisco is a\")\nprint(\"Completion result:\", completion)\n```\nThe error:\n```\n---------------------------------------------------------------------------\nConnectError                              Traceback (most recent call last)\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/httpx/_transports/default.py:101, in map_httpcore_exceptions()\n    100 try:\n--> 101     yield\n    102 except Exception as exc:\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/httpx/_transports/default.py:250, in HTTPTransport.handle_request(self, request)\n    249 with map_httpcore_exceptions():\n--> 250     resp = self._pool.handle_request(req)\n    252 assert isinstance(resp.stream, typing.Iterable)\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256, in ConnectionPool.handle_request(self, request)\n    255     self._close_connections(closing)\n--> 256     raise exc from None\n    258 # Return the response. Note that in this case we still have to manage\n    259 # the point at which the response is closed.\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236, in ConnectionPool.handle_request(self, request)\n    234 try:\n    235     # Send the request on the assigned connection.\n--> 236     response = connection.handle_request(\n    237         pool_request.request\n    238     )\n    239 except ConnectionNotAvailable:\n    240     # In some cases a connection may initially be available to\n    241     # handle a request, but then become unavailable.\n    242     #\n    243     # In this case we clear the connection and try again.\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/httpcore/_sync/connection.py:101, in HTTPConnection.handle_request(self, request)\n    100     self._connect_failed = True\n--> 101     raise exc\n    103 return self._connection.handle_request(request)\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/httpcore/_sync/connection.py:78, in HTTPConnection.handle_request(self, request)\n     77 if self._connection is None:\n---> 78     stream = self._connect(request)\n     80     ssl_object = stream.get_extra_info(\"ssl_object\")\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/httpcore/_sync/connection.py:124, in HTTPConnection._connect(self, request)\n    123 with Trace(\"connect_tcp\", logger, request, kwargs) as trace:\n--> 124     stream = self._network_backend.connect_tcp(**kwargs)\n    125     trace.return_value = stream\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/httpcore/_backends/sync.py:207, in SyncBackend.connect_tcp(self, host, port, timeout, local_address, socket_options)\n    202 exc_map: ExceptionMapping = {\n    203     socket.timeout: ConnectTimeout,\n    204     OSError: ConnectError,\n    205 }\n--> 207 with map_exceptions(exc_map):\n    208     sock = socket.create_connection(\n    209         address,\n    210         timeout,\n    211         source_address=source_address,\n    212     )\n\nFile ~/.conda/envs/vllm/lib/python3.12/contextlib.py:158, in _GeneratorContextManager.__exit__(self, typ, value, traceback)\n    157 try:\n--> 158     self.gen.throw(value)\n    159 except StopIteration as exc:\n    160     # Suppress StopIteration *unless* it's the same exception that\n    161     # was passed to throw().  This prevents a StopIteration\n    162     # raised inside the \"with\" statement from being suppressed.\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/httpcore/_exceptions.py:14, in map_exceptions(map)\n     13     if isinstance(exc, from_exc):\n---> 14         raise to_exc(exc) from exc\n     15 raise\n\nConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nConnectError                              Traceback (most recent call last)\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/openai/_base_client.py:955, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n    954 try:\n--> 955     response = self._client.send(\n    956         request,\n    957         stream=stream or self._should_stream_response_body(request=request),\n    958         **kwargs,\n    959     )\n    960 except httpx.TimeoutException as err:\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/httpx/_client.py:914, in Client.send(self, request, stream, auth, follow_redirects)\n    912 auth = self._build_request_auth(request, auth)\n--> 914 response = self._send_handling_auth(\n    915     request,\n    916     auth=auth,\n    917     follow_redirects=follow_redirects,\n    918     history=[],\n    919 )\n    920 try:\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/httpx/_client.py:942, in Client._send_handling_auth(self, request, auth, follow_redirects, history)\n    941 while True:\n--> 942     response = self._send_handling_redirects(\n    943         request,\n    944         follow_redirects=follow_redirects,\n    945         history=history,\n    946     )\n    947     try:\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/httpx/_client.py:979, in Client._send_handling_redirects(self, request, follow_redirects, history)\n    977     hook(request)\n--> 979 response = self._send_single_request(request)\n    980 try:\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/httpx/_client.py:1014, in Client._send_single_request(self, request)\n   1013 with request_context(request=request):\n-> 1014     response = transport.handle_request(request)\n   1016 assert isinstance(response.stream, SyncByteStream)\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/httpx/_transports/default.py:249, in HTTPTransport.handle_request(self, request)\n    237 req = httpcore.Request(\n    238     method=request.method,\n    239     url=httpcore.URL(\n   (...)    247     extensions=request.extensions,\n    248 )\n--> 249 with map_httpcore_exceptions():\n    250     resp = self._pool.handle_request(req)\n\nFile ~/.conda/envs/vllm/lib/python3.12/contextlib.py:158, in _GeneratorContextManager.__exit__(self, typ, value, traceback)\n    157 try:\n--> 158     self.gen.throw(value)\n    159 except StopIteration as exc:\n    160     # Suppress StopIteration *unless* it's the same exception that\n    161     # was passed to throw().  This prevents a StopIteration\n    162     # raised inside the \"with\" statement from being suppressed.\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/httpx/_transports/default.py:118, in map_httpcore_exceptions()\n    117 message = str(exc)\n--> 118 raise mapped_exc(message) from exc\n\nConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nAPIConnectionError                        Traceback (most recent call last)\nCell In[24], line 1\n----> 1 completion = client.completions.create(model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n      2                                       prompt=\"San Francisco is a\")\n      3 print(\"Completion result:\", completion)\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/openai/_utils/_utils.py:279, in required_args.<locals>.inner.<locals>.wrapper(*args, **kwargs)\n    277             msg = f\"Missing required argument: {quote(missing[0])}\"\n    278     raise TypeError(msg)\n--> 279 return func(*args, **kwargs)\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/openai/resources/completions.py:539, in Completions.create(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\n    510 @required_args([\"model\", \"prompt\"], [\"model\", \"prompt\", \"stream\"])\n    511 def create(\n    512     self,\n   (...)    537     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,\n    538 ) -> Completion | Stream[Completion]:\n--> 539     return self._post(\n    540         \"/completions\",\n    541         body=maybe_transform(\n    542             {\n    543                 \"model\": model,\n    544                 \"prompt\": prompt,\n    545                 \"best_of\": best_of,\n    546                 \"echo\": echo,\n    547                 \"frequency_penalty\": frequency_penalty,\n    548                 \"logit_bias\": logit_bias,\n    549                 \"logprobs\": logprobs,\n    550                 \"max_tokens\": max_tokens,\n    551                 \"n\": n,\n    552                 \"presence_penalty\": presence_penalty,\n    553                 \"seed\": seed,\n    554                 \"stop\": stop,\n    555                 \"stream\": stream,\n    556                 \"stream_options\": stream_options,\n    557                 \"suffix\": suffix,\n    558                 \"temperature\": temperature,\n    559                 \"top_p\": top_p,\n    560                 \"user\": user,\n    561             },\n    562             completion_create_params.CompletionCreateParams,\n    563         ),\n    564         options=make_request_options(\n    565             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n    566         ),\n    567         cast_to=Completion,\n    568         stream=stream or False,\n    569         stream_cls=Stream[Completion],\n    570     )\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/openai/_base_client.py:1242, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)\n   1228 def post(\n   1229     self,\n   1230     path: str,\n   (...)   1237     stream_cls: type[_StreamT] | None = None,\n   1238 ) -> ResponseT | _StreamT:\n   1239     opts = FinalRequestOptions.construct(\n   1240         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1241     )\n-> 1242     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/openai/_base_client.py:919, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    916 else:\n    917     retries_taken = 0\n--> 919 return self._request(\n    920     cast_to=cast_to,\n    921     options=options,\n    922     stream=stream,\n    923     stream_cls=stream_cls,\n    924     retries_taken=retries_taken,\n    925 )\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/openai/_base_client.py:979, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n    976 log.debug(\"Encountered Exception\", exc_info=True)\n    978 if remaining_retries > 0:\n--> 979     return self._retry_request(\n    980         input_options,\n    981         cast_to,\n    982         retries_taken=retries_taken,\n    983         stream=stream,\n    984         stream_cls=stream_cls,\n    985         response_headers=None,\n    986     )\n    988 log.debug(\"Raising connection error\")\n    989 raise APIConnectionError(request=request) from err\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/openai/_base_client.py:1057, in SyncAPIClient._retry_request(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\n   1053 # In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\n   1054 # different thread if necessary.\n   1055 time.sleep(timeout)\n-> 1057 return self._request(\n   1058     options=options,\n   1059     cast_to=cast_to,\n   1060     retries_taken=retries_taken + 1,\n   1061     stream=stream,\n   1062     stream_cls=stream_cls,\n   1063 )\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/openai/_base_client.py:979, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n    976 log.debug(\"Encountered Exception\", exc_info=True)\n    978 if remaining_retries > 0:\n--> 979     return self._retry_request(\n    980         input_options,\n    981         cast_to,\n    982         retries_taken=retries_taken,\n    983         stream=stream,\n    984         stream_cls=stream_cls,\n    985         response_headers=None,\n    986     )\n    988 log.debug(\"Raising connection error\")\n    989 raise APIConnectionError(request=request) from err\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/openai/_base_client.py:1057, in SyncAPIClient._retry_request(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\n   1053 # In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\n   1054 # different thread if necessary.\n   1055 time.sleep(timeout)\n-> 1057 return self._request(\n   1058     options=options,\n   1059     cast_to=cast_to,\n   1060     retries_taken=retries_taken + 1,\n   1061     stream=stream,\n   1062     stream_cls=stream_cls,\n   1063 )\n\nFile ~/.conda/envs/vllm/lib/python3.12/site-packages/openai/_base_client.py:989, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n    979         return self._retry_request(\n    980             input_options,\n    981             cast_to,\n   (...)    985             response_headers=None,\n    986         )\n    988     log.debug(\"Raising connection error\")\n--> 989     raise APIConnectionError(request=request) from err\n    991 log.debug(\n    992     'HTTP Response: %s %s \"%i %s\" %s',\n    993     request.method,\n   (...)    997     response.headers,\n    998 )\n    999 log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n\nAPIConnectionError: Connection error.\n```\n\n### Suggest a potential alternative/fix\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2025-03-26T03:04:55+00:00",
    "closed_at": "2025-03-26T03:36:10+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15518/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15518"
  },
  {
    "number": 15452,
    "title": "[Bug]: Build error, nvcc error   : 'ptxas' died due to signal 11 (Invalid memory reference)",
    "body": "### Your current environment\n\nL20, CUDA 12.6, PyTorch 2.6.0\n\n### \ud83d\udc1b Describe the bug\n\n```bash\nFAILED: vllm-flash-attn/CMakeFiles/_vllm_fa3_C.dir/hopper/instantiations/flash_fwd_hdimall_fp16_paged_split_sm90.cu.o\n/usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DFLASHATTENTION_DISABLE_BACKWARD -DFLASHATTENTION_DISABLE_DROPOUT -DFLASHATTENTION_DISABLE_PYBIND -DFLASHATTENTION_DISABLE_UNEVEN_K -DFLASHATTENTION_VARLEN_ONLY -DPy_LIMITED_API=3 -DTORCH_EXTENSION_NAME=_vllm_fa3_C -DUSE_C10D_GLOO -DUSE_C10D_NCCL -DUSE_DISTRIBUTED -DUSE_RPC -DUSE_TENSORPIPE -D_vllm_fa3_C_EXPORTS -I/workspace/dev/vipshop/vllm/.deps/vllm-flash-attn-src/csrc -I/workspace/dev/vipshop/vllm/.deps/vllm-flash-attn-src/hopper -I/workspace/dev/vipshop/vllm/.deps/vllm-flash-attn-src/csrc/common -I/workspace/dev/vipshop/vllm/.deps/vllm-flash-attn-src/csrc/cutlass/include -isystem /usr/include/python3.12 -isystem /usr/local/lib/python3.12/dist-packages/torch/include -isystem /usr/local/lib/python3.12/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -DONNX_NAMESPACE=onnx_c2 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda -O3 -g -DNDEBUG -std=c++17 -Xcompiler=-fPIC --expt-relaxed-constexpr -DENABLE_FP8 --threads=1 --expt-extended-lambda --use_fast_math -DCUTLASS_ENABLE_DIRECT_CUDA_DRIVER_CALL=1 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_90a,code=sm_90a -gencode arch=compute_80,code=sm_80 -MD -MT vllm-flash-attn/CMakeFiles/_vllm_fa3_C.dir/hopper/instantiations/flash_fwd_hdimall_fp16_paged_split_sm90.cu.o -MF vllm-flash-attn/CMakeFiles/_vllm_fa3_C.dir/hopper/instantiations/flash_fwd_hdimall_fp16_paged_split_sm90.cu.o.d -x cu -c /workspace/dev/vipshop/vllm/.deps/vllm-flash-attn-src/hopper/instantiations/flash_fwd_hdimall_fp16_paged_split_sm90.cu -o vllm-flash-attn/CMakeFiles/_vllm_fa3_C.dir/hopper/instantiations/flash_fwd_hdimall_fp16_paged_split_sm90.cu.o\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-25T08:58:06+00:00",
    "closed_at": "2025-03-25T09:40:51+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15452/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15452"
  },
  {
    "number": 14536,
    "title": "[Usage]: Does vllm support inflight batch?",
    "body": "### Your current environment\n\n\n### How would you like to use vllm\n\nDoes vllm support inflight batch?\ntrtllm supports it but I can't find any information on vllm documentation\nCould some kind person explain it?\nThank you so much in advance\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-03-10T03:45:47+00:00",
    "closed_at": "2025-03-10T03:52:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14536/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14536"
  },
  {
    "number": 5864,
    "title": "[Bug]: ValueError: Model architectures ['Phi3VForCausalLM'] are not supported for now.",
    "body": "### Your current environment\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.16.3\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1063-aws-x86_64-with-glibc2.31\r\nIs CUDA available: N/A\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: GPU 0: Tesla T4\r\nNvidia driver version: 535.183.01\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: N/A\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nCPU(s):                             4\r\nOn-line CPU(s) list:                0-3\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 2\r\nSocket(s):                          1\r\nNUMA node(s):                       1\r\nVendor ID:                          GenuineIntel\r\nCPU family:                         6\r\nModel:                              85\r\nModel name:                         Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\r\nStepping:                           7\r\nCPU MHz:                            2499.998\r\nBogoMIPS:                           4999.99\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          64 KiB\r\nL1i cache:                          64 KiB\r\nL2 cache:                           2 MiB\r\nL3 cache:                           35.8 MiB\r\nNUMA node0 CPU(s):                  0-3\r\nVulnerability Gather data sampling: Unknown: Dependent on hypervisor status\r\nVulnerability Itlb multihit:        KVM: Mitigation: VMX unsupported\r\nVulnerability L1tf:                 Mitigation; PTE Inversion\r\nVulnerability Mds:                  Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Meltdown:             Mitigation; PTI\r\nVulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed:             Vulnerable\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Vulnerable\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Retpoline\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke avx512_vnni\r\n\r\nVersions of relevant libraries:\r\n[pip3] No relevant packages\r\n[conda] No relevant packages\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-3     0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks```\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI am running a g4dn.xlarge machine with DL ubuntu image. Checked for cuda drivers `nvidia-smi` and `nvcc --version`\r\n\r\nTried running the the phi3-vision using docker. But in the error it says that `Phi3VForCausalLM` is not supported, which it seems to be supported in https://docs.vllm.ai/\r\n\r\nAm I using any older version?\r\n\r\n```bash\r\ndocker run --runtime nvidia --gpus all -v  ~/.cache/huggingface:/root/.cache/huggingface --env \"HUGGING_FACE_HUB_TOKEN=<token>\" -p 8000:8000 --ipc=host vllm/vllm-openai:v0.5.0.post1 --model microsoft/Phi-3-vision-128k-instruct --trust-remote-code  --dtype half\r\n```\r\n\r\nError: [rank0]: ValueError: Model architectures ['Phi3VForCausalLM'] are not supported for now. Supported architectures: ",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-06-26T11:05:33+00:00",
    "closed_at": "2024-06-26T11:25:59+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5864/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5864"
  },
  {
    "number": 16525,
    "title": "[Bug]: vllm run Qwen2-Audio-7B-Instruct raise openai.InternalServerError: Error code: 500",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nvllm==0.8.2\ntransformers==4.51.0\n\n1\u3001vllm serve\uff1a\nVLLM_AUDIO_FETCH_TIMEOUT=360000 CUDA_VISIBLE_DEVICES=1 VLLM_LOGGING_LEVEL=DEBUG  vllm serve Qwen2-Audio-7B-Instruct --max-model-len 4096  --port 8000 --served-model-name qwen2-audio-7b-instruct\n\n2\u3001python code\uff1a\n openai_api_base=\"http://localhost:8000/v1\"\nAUDIO_FILE=\u201dsample-9s.wav\u201c\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\n\nwith open(AUDIO_FILE, \"rb\") as f:\n    audio_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n    audio_data_url = f\"data:audio/wav;base64,{audio_base64}\"\n\nchat_completion_from_base64 = client.chat.completions.create(\n    model=MODEL_NAME,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": USER_PROMPT},\n                {\"type\": \"audio_url\", \"audio_url\": {\"url\":audio_data_url}}\n            ]\n        }\n    ],\n    temperature=0.2,\n)\n\n\nresult = chat_completion_from_base64.choices[0].message.content\nprint(\"Chat completion output from input audio:\", result)\n\n\nraise error\uff1a\nFile \"/root/yuhp/qwen_vllm_test_cp.py\", line 38, in <module>\n    chat_completion_from_base64 = client.chat.completions.create(\n  File \"/data/miniforge3/envs/vllm_runtime/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 279, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/miniforge3/envs/vllm_runtime/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 914, in create\n    return self._post(\n  File \"/data/miniforge3/envs/vllm_runtime/lib/python3.10/site-packages/openai/_base_client.py\", line 1242, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/miniforge3/envs/vllm_runtime/lib/python3.10/site-packages/openai/_base_client.py\", line 919, in request\n    return self._request(\n  File \"/data/miniforge3/envs/vllm_runtime/lib/python3.10/site-packages/openai/_base_client.py\", line 1008, in _request\n    return self._retry_request(\n  File \"/data/miniforge3/envs/vllm_runtime/lib/python3.10/site-packages/openai/_base_client.py\", line 1057, in _retry_request\n    return self._request(\n  File \"/data/miniforge3/envs/vllm_runtime/lib/python3.10/site-packages/openai/_base_client.py\", line 1008, in _request\n    return self._retry_request(\n  File \"/data/miniforge3/envs/vllm_runtime/lib/python3.10/site-packages/openai/_base_client.py\", line 1057, in _retry_request\n    return self._request(\n  File \"/data/miniforge3/envs/vllm_runtime/lib/python3.10/site-packages/openai/_base_client.py\", line 1023, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: Error code: 500\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-04-12T04:16:29+00:00",
    "closed_at": "2025-04-12T04:37:19+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16525/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16525"
  },
  {
    "number": 16176,
    "title": "[Bug]: how to use vllm serve start BertForSequenceClassification model",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI trained an emotion classification model using BERTModel\u3002Added a classification header to classify\u3002when i start vllm serve \uff0cWhat task type should I specify\uff1f\nand how to curl the server?can provide a example\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-04-07T09:26:14+00:00",
    "closed_at": "2025-04-07T10:01:49+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16176/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16176"
  },
  {
    "number": 10153,
    "title": "[Usage]: How to use vllm to run Qwen2-VL-72B?",
    "body": "### Your current environment\n\n```\r\nfrom transformers import AutoProcessor\r\nfrom vllm import LLM, SamplingParams\r\nfrom qwen_vl_utils import process_vision_info\r\n\r\nMODEL_PATH = \"/path/Qwen2-VL-72B-Instruct\"\r\n\r\nllm = LLM(\r\n    model=MODEL_PATH,\r\n    limit_mm_per_prompt={\"image\": 10, \"video\": 10},\r\n)\r\n\r\nsampling_params = SamplingParams(\r\n    temperature=0.1,\r\n    top_p=0.001,\r\n    repetition_penalty=1.05,\r\n    max_tokens=256,\r\n    stop_token_ids=[],\r\n)\r\n\r\nmessages = [\r\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n    {\r\n        \"role\": \"user\",\r\n        \"content\": [\r\n            {\r\n                \"type\": \"image\",\r\n                \"image\": \"porsche.jpg\",\r\n                \"min_pixels\": 224 * 224,\r\n                \"max_pixels\": 1280 * 28 * 28,\r\n            },\r\n            {\"type\": \"text\", \"text\": \"What is the text in the illustrate?\"},\r\n        ],\r\n    },\r\n]\r\n# For video input, you can pass following values instead:\r\n# \"type\": \"video\",\r\n# \"video\": \"<video URL>\",\r\n\r\nprocessor = AutoProcessor.from_pretrained(MODEL_PATH)\r\nprompt = processor.apply_chat_template(\r\n    messages,\r\n    tokenize=False,\r\n    add_generation_prompt=True,\r\n)\r\nimage_inputs, video_inputs = process_vision_info(messages)\r\n\r\nmm_data = {}\r\nif image_inputs is not None:\r\n    mm_data[\"image\"] = image_inputs\r\nif video_inputs is not None:\r\n    mm_data[\"video\"] = video_inputs\r\n\r\nllm_inputs = {\r\n    \"prompt\": prompt,\r\n    \"multi_modal_data\": mm_data,\r\n}\r\n\r\noutputs = llm.generate([llm_inputs], sampling_params=sampling_params)\r\ngenerated_text = outputs[0].outputs[0].text\r\n\r\nprint(generated_text)\r\n```\r\n\n\n### How would you like to use vllm\n\nI want to run inference of a Qwen2-VL-72B. I use 4 H100-80G. However, it always has out of memory problem.\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-11-08T11:31:42+00:00",
    "closed_at": "2024-11-08T12:30:22+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10153/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10153"
  },
  {
    "number": 18888,
    "title": "[Bug]:",
    "body": "### Your current environment\n\nFlashMLA V1 with FP8 KV cache not yet supported!\n\n```\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\nNCCL_SOCKET_IFNAME=bond0 \\\nGLOO_SOCKET_IFNAME=bond0 \\\nVLLM_USE_V1=1 \\\nVLLM_USE_MODELSCOPE=true \\\nvllm serve /data/models/huggingface.co/deepseek-ai/DeepSeek-R1/DeepSeek-R1-Hzz1 \\\n--served-model-name deepseek-r1 \\\n--gpu-memory-utilization 0.8 \\\n--tensor-parallel-size 8  \\\n--trust-remote-code \\\n--enable-chunked-prefill \\\n--port 8000 \\\n--kv-cache-dtype fp8 \\\n--enable-expert-parallel\n```\n\nresult:\n\n![Image](https://github.com/user-attachments/assets/44ca8e77-1ce2-4b4c-b80b-9bda20e1ca35)\n\nGPU DEVICE h800 x 8 x 140GB\ncuda drive version 550.127.08\nvllm version 0.8.5 \n\npip list \n\n```\nPackage                                  Version\n---------------------------------------- --------------------\naccelerate                               0.34.0\naiofiles                                 23.2.1\naiohappyeyeballs                         2.4.0\naiohttp                                  3.10.5\naiohttp-cors                             0.7.0\naioprometheus                            23.12.0\naiosignal                                1.3.1\nairportsdata                             20241001\naliyun-python-sdk-core                   2.16.0\naliyun-python-sdk-kms                    2.16.5\naltair                                   5.5.0\nannotated-types                          0.7.0\nantlr4-python3-runtime                   4.9.3\nanyascii                                 0.3.2\nanyio                                    4.4.0\nargcomplete                              3.5.3\nastor                                    0.8.1\nasync-timeout                            4.0.3\nattrdict                                 2.0.1\nattrs                                    24.2.0\naudioread                                3.0.1\nauto_gptq                                0.7.1\nautoawq                                  0.2.5\nautoawq_kernels                          0.0.9\nav                                       14.0.1\nbabel                                    2.16.0\nbcrypt                                   4.2.1\nbeautifulsoup4                           4.12.3\nbitsandbytes                             0.45.1\nblack                                    24.10.0\nblake3                                   1.0.4\nblinker                                  1.9.0\nboto3                                    1.28.64\nbotocore                                 1.31.85\ncached_path                              1.6.7\ncachetools                               5.5.1\ncdifflib                                 1.2.9\ncertifi                                  2019.11.28\ncffi                                     1.17.1\nchardet                                  3.0.4\ncharset-normalizer                       3.3.2\nchattts                                  0.2.1\nclick                                    8.1.7\ncloudpickle                              3.0.0\ncn2an                                    0.5.23\ncolorama                                 0.4.6\ncoloredlogs                              15.0.1\ncolorful                                 0.5.6\ncompressed-tensors                       0.9.3\nconformer                                0.3.2\ncontourpy                                1.3.1\ncontrolnet-aux                           0.0.7\ncrcmod                                   1.7\ncryptography                             44.0.0\ncuda-python                              12.6.2.post1\ncupy-cuda12x                             13.4.0\ncycler                                   0.12.1\nCython                                   3.0.11\ndataclass-wizard                         0.35.0\ndatamodel-code-generator                 0.26.5\ndatasets                                 2.21.0\ndateparser                               1.1.8\ndbus-python                              1.2.16\ndecorator                                5.1.1\ndecord                                   0.6.0\nDeepCache                                0.1.1\nDeprecated                               1.2.15\ndepyf                                    0.18.0\ndiffusers                                0.32.2\ndill                                     0.3.8\ndiskcache                                5.6.3\nDistance                                 0.1.3\ndistlib                                  0.3.9\ndistro                                   1.9.0\ndistro-info                              0.23+ubuntu1.1\ndnspython                                2.7.0\ndocopt                                   0.6.2\necdsa                                    0.19.0\neditdistance                             0.8.1\neinops                                   0.8.0\neinx                                     0.3.0\nemail_validator                          2.2.0\nencodec                                  0.1.1\neva-decord                               0.6.1\nexceptiongroup                           1.2.2\nfastapi                                  0.115.11\nfastapi-cli                              0.0.7\nfastrlock                                0.8.3\nffmpy                                    0.5.0\nfilelock                                 3.17.0\nFlagEmbedding                            1.2.11\nflashinfer                               0.1.6+cu124torch2.4\nFlask                                    3.1.0\nflatbuffers                              25.1.21\nfonttools                                4.55.5\nfrozendict                               2.4.6\nfrozenlist                               1.4.1\nfsspec                                   2024.6.1\nfugashi                                  1.4.0\nfunasr                                   1.1.16\nfvcore                                   0.1.5.post20221221\ng2p-en                                   2.1.0\ngdown                                    5.2.0\ngekko                                    1.2.1\ngenson                                   1.3.0\ngguf                                     0.16.3\ngoogle-api-core                          2.24.0\ngoogle-auth                              2.38.0\ngoogle-cloud-core                        2.4.1\ngoogle-cloud-storage                     2.19.0\ngoogle-crc32c                            1.6.0\ngoogle-resumable-media                   2.7.2\ngoogleapis-common-protos                 1.66.0\ngradio                                   4.26.0\ngradio_client                            0.15.1\ngrpcio                                   1.70.0\ngruut                                    2.4.0\ngruut-ipa                                0.13.0\ngruut-lang-de                            2.0.1\ngruut-lang-en                            2.0.1\ngruut-lang-es                            2.0.1\ngruut-lang-fr                            2.0.2\nh11                                      0.14.0\nh2                                       4.2.0\nhf_transfer                              0.1.8\nhf-xet                                   1.0.2\nhiredis                                  3.1.0\nhpack                                    4.1.0\nhttpcore                                 1.0.5\nhttptools                                0.6.1\nhttpx                                    0.27.2\nhuggingface-hub                          0.30.1\nhumanfriendly                            10.0\nhydra-core                               1.3.2\nHypercorn                                0.17.3\nhyperframe                               6.1.0\nHyperPyYAML                              1.2.2\nidna                                     2.8\nimageio                                  2.37.0\nimageio-ffmpeg                           0.6.0\nimportlib_metadata                       8.0.0\nimportlib_resources                      6.5.2\ninflect                                  5.6.2\niniconfig                                2.0.0\ninteregular                              0.3.3\niopath                                   0.1.10\nisort                                    5.13.2\nitsdangerous                             2.2.0\njaconv                                   0.4.0\njamo                                     0.4.1\njieba                                    0.42.1\nJinja2                                   3.1.5\njiter                                    0.5.0\njj-pytorchvideo                          0.1.5\njmespath                                 0.10.0\njoblib                                   1.4.2\njsonlines                                1.2.0\njsonschema                               4.23.0\njsonschema-specifications                2023.12.1\nkaldiio                                  2.18.0\nkiwisolver                               1.4.8\nlark                                     1.2.2\nlazy_loader                              0.4\nlibnacl                                  2.1.0\nlibrosa                                  0.10.2.post1\nlightning                                2.5.0.post0\nlightning-utilities                      0.11.9\nlinkify-it-py                            2.0.3\nllama_cpp_python                         0.3.4\nllguidance                               0.7.13\nllvmlite                                 0.44.0\nlm-format-enforcer                       0.10.11\nloguru                                   0.7.3\nloralib                                  0.1.2\nmarkdown-it-py                           3.0.0\nMarkupSafe                               2.1.5\nmatplotlib                               3.10.0\nmdit-py-plugins                          0.4.2\nmdurl                                    0.1.2\nmecab-python3                            1.0.10\nmemray                                   1.15.0\nmistral_common                           1.5.4\nmodelscope                               1.18.1\nmpmath                                   1.3.0\nmsgpack                                  1.0.8\nmsgspec                                  0.18.6\nmultidict                                6.0.5\nmultiprocess                             0.70.16\nmypy-extensions                          1.0.0\nnanobind                                 2.6.1\nnarwhals                                 1.23.0\nnatsort                                  8.4.0\nnemo_text_processing                     1.0.2\nnest-asyncio                             1.6.0\nnetworkx                                 3.3\nninja                                    1.11.1.3\nnltk                                     3.9.1\nnum2words                                0.5.14\nnumba                                    0.61.2\nnumpy                                    1.26.4\nnvidia-cublas-cu12                       12.4.5.8\nnvidia-cuda-cupti-cu12                   12.4.127\nnvidia-cuda-nvrtc-cu12                   12.4.127\nnvidia-cuda-runtime-cu12                 12.4.127\nnvidia-cudnn-cu12                        9.1.0.70\nnvidia-cufft-cu12                        11.2.1.3\nnvidia-curand-cu12                       10.3.5.147\nnvidia-cusolver-cu12                     11.6.1.9\nnvidia-cusparse-cu12                     12.3.1.170\nnvidia-cusparselt-cu12                   0.6.2\nnvidia-ml-py                             12.560.30\nnvidia-nccl-cu12                         2.21.5\nnvidia-nvjitlink-cu12                    12.4.127\nnvidia-nvtx-cu12                         12.4.127\nomegaconf                                2.3.0\nonnxruntime                              1.20.1\nonnxruntime-gpu                          1.16.0\nopenai                                   1.60.0\nopencensus                               0.11.4\nopencensus-context                       0.1.3\nopencv-contrib-python-headless           4.11.0.86\nopencv-python                            4.11.0.86\nopencv-python-headless                   4.11.0.86\nopentelemetry-api                        1.26.0\nopentelemetry-exporter-otlp              1.26.0\nopentelemetry-exporter-otlp-proto-common 1.26.0\nopentelemetry-exporter-otlp-proto-grpc   1.26.0\nopentelemetry-exporter-otlp-proto-http   1.26.0\nopentelemetry-proto                      1.26.0\nopentelemetry-sdk                        1.26.0\nopentelemetry-semantic-conventions       0.47b0\nopentelemetry-semantic-conventions-ai    0.4.6\noptimum                                  1.23.3\norjson                                   3.10.15\normsgpack                                1.7.0\noss2                                     2.19.1\noutlines                                 0.1.11\noutlines_core                            0.1.26\npackaging                                24.1\npandas                                   2.2.2\nparameterized                            0.9.0\npartial-json-parser                      0.2.1.1.post4\npasslib                                  1.7.4\npathspec                                 0.12.1\npeft                                     0.14.0\npillow                                   10.4.0\npip                                      24.3.1\nplatformdirs                             4.3.6\npluggy                                   1.5.0\npooch                                    1.8.2\nportalocker                              3.1.1\nprettytable                              3.14.0\npriority                                 2.0.0\nproces                                   0.1.7\nprometheus_client                        0.20.0\nprometheus-fastapi-instrumentator        7.0.0\nproto-plus                               1.25.0\nprotobuf                                 4.25.7\npsutil                                   6.0.0\npy-cpuinfo                               9.0.0\npy-spy                                   0.4.0\npyairports                               2.1.1\npyarrow                                  17.0.0\npyasn1                                   0.6.1\npyasn1_modules                           0.4.1\npybase16384                              0.3.7\npybind11                                 2.13.6\npycountry                                24.6.1\npycparser                                2.22\npycryptodome                             3.21.0\npydantic                                 2.10.6\npydantic_core                            2.27.2\npydub                                    0.25.1\nPygments                                 2.19.1\nPyGObject                                3.36.0\npykakasi                                 2.3.0\npynini                                   2.1.5\npynndescent                              0.5.13\npyparsing                                3.2.1\npypinyin                                 0.53.0\nPySocks                                  1.7.1\npytest                                   8.3.4\npython-apt                               2.0.1+ubuntu0.20.4.1\npython-crfsuite                          0.9.11\npython-dateutil                          2.9.0.post0\npython-dotenv                            1.0.1\npython-jose                              3.3.0\npython-json-logger                       3.2.1\npython-multipart                         0.0.20\npython-rapidjson                         1.20\npytorch-lightning                        2.5.0.post0\npytorch-wpe                              0.0.1\npytz                                     2024.1\nPyYAML                                   6.0.2\npyzmq                                    26.2.0\nquantile-python                          1.1\nQuart                                    0.20.0\nqwen-vl-utils                            0.0.8\nray                                      2.43.0\nredis                                    5.2.1\nreferencing                              0.35.1\nregex                                    2024.7.24\nrequests                                 2.32.3\nrequests-unixsocket                      0.2.0\nrich                                     13.9.4\nrich-toolkit                             0.13.2\nrouge                                    1.0.1\nrpds-py                                  0.20.0\nrsa                                      4.9\nruamel.yaml                              0.18.10\nruamel.yaml.clib                         0.2.12\nruff                                     0.9.3\ns3transfer                               0.7.0\nsacremoses                               0.1.1\nsafetensors                              0.4.4\nscikit-image                             0.25.0\nscikit-learn                             1.6.1\nscipy                                    1.15.1\nsemantic-version                         2.10.0\nsentence-transformers                    3.4.0\nsentencepiece                            0.2.0\nsetproctitle                             1.3.4\nsetuptools                               75.8.0\nsglang                                   0.4.1.post7\nshellingham                              1.5.4\nsilero-vad                               5.1.2\nsix                                      1.17.0\nsmart-open                               7.1.0\nsniffio                                  1.3.1\nsoundfile                                0.13.0\nsoupsieve                                2.6\nsoxr                                     0.5.0.post1\nsse-starlette                            2.1.3\nstarlette                                0.46.0\nsympy                                    1.13.1\ntabulate                                 0.9.0\ntaskgroup                                0.2.2\ntblib                                    3.0.0\ntensorboardX                             2.6.2.2\ntensorizer                               2.9.1\ntermcolor                                2.5.0\ntext-generation                          0.7.0\ntextual                                  1.0.0\nthreadpoolctl                            3.5.0\ntifffile                                 2025.1.10\ntiktoken                                 0.7.0\ntimm                                     1.0.14\ntokenizers                               0.21.1\ntoml                                     0.10.2\ntomli                                    2.2.1\ntomlkit                                  0.12.0\ntorch                                    2.6.0\ntorch-complex                            0.4.4\ntorchao                                  0.10.0\ntorchaudio                               2.6.0\ntorchdiffeq                              0.2.5\ntorchmetrics                             1.6.1\ntorchvision                              0.21.0\ntqdm                                     4.66.5\ntransformers                             4.51.3\ntransformers-stream-generator            0.0.5\ntriton                                   3.2.0\ntritonclient                             2.54.0\ntyper                                    0.15.2\ntyping_extensions                        4.12.2\ntzdata                                   2024.1\ntzlocal                                  5.2\nuc-micro-py                              1.0.3\numap-learn                               0.5.7\nunattended-upgrades                      0.1\nunidic-lite                              1.0.8\nurllib3                                  2.0.7\nuvicorn                                  0.30.6\nuvloop                                   0.20.0\nvector-quantize-pytorch                  1.17.3\nverovio                                  4.5.1\nvirtualenv                               20.29.2\nvllm                                     0.8.5\nvllm-flash-attn                          2.6.1\nvocos                                    0.1.0\nwatchfiles                               0.24.0\nwcwidth                                  0.2.13\nwebsockets                               11.0.3\nWerkzeug                                 3.1.3\nWeTextProcessing                         1.0.3\nwget                                     3.2\nwheel                                    0.34.2\nwrapt                                    1.17.2\nwsproto                                  1.2.0\nx-transformers                           1.44.6\nxformers                                 0.0.29.post2\nxgrammar                                 0.1.18\nxinference                               1.2.2\nxoscar                                   0.4.6\nxxhash                                   3.5.0\nyacs                                     0.1.8\nyarl                                     1.9.9\nzipp                                     3.20.1\nzstandard                                0.23.0\n```\n\n\n### \ud83d\udc1b Describe the bug\n\nFlashMLA V1 with FP8 KV cache not yet supported!\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-05-29T07:49:01+00:00",
    "closed_at": "2025-05-29T08:31:27+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18888/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/18888"
  }
]
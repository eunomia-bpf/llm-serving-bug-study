[
  {
    "number": 2394,
    "title": " [Feature Request] Mixtral Offloading",
    "body": "There's a new cache technique mentioned in the paper https://arxiv.org/abs/2312.17238. (github: https://github.com/dvmazur/mixtral-offloading)\r\nThey introduced LRU cache to cache experts based on patterns they found, and also took speculative guess to pre-load experts before the computation of the next layer. The result looks quite promising. Can we support it for Mixtral? This helps a lot to run on smaller GPUs.",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-01-09T17:26:55+00:00",
    "closed_at": "2024-11-30T02:03:04+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2394/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2394"
  },
  {
    "number": 14675,
    "title": "[Bug]: Vllm automatically restarts while using cortecs/phi-4-FP8-Dynamic",
    "body": "### Your current environment\n\nVllm version used : vllm/vllm-openai:v0.7.3\n\n### \ud83d\udc1b Describe the bug\n\nVllm restarts randomly while running multiple subsequent request with **cortecs/phi-4-FP8-Dynamic.**\n\nDEBUG 03-12 03:19:03 launcher.py:59] python3 -m vllm.entrypoints.openai.api_server --model cortecs/phi-4-FP8-Dynamic --dtype auto --max-model-len 14336 --tensor-parallel-size 1 --host=0.0.0.0 --port=9000 --gpu-memory-utilization=0.9 --trust-remote-code --api-key mits-d326429bf1aa6c4c7f6f0c910fd0aa04c8976498df5e06ab --enable-prefix-caching\nINFO 03-12 03:19:03 launcher.py:62] Shutting down FastAPI HTTP server.\nINFO:     Shutting down\nDEBUG 03-12 03:19:07 client.py:174] Shutting down MQLLMEngineClient check health loop.\nDEBUG 03-12 03:19:07 client.py:257] Shutting down MQLLMEngineClient output handler.\n\n\nHere the last few logs after restarted unexpectedly. \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-12T10:37:21+00:00",
    "closed_at": "2025-07-11T02:15:41+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14675/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14675"
  },
  {
    "number": 6414,
    "title": "[Bug]: Timeout Error When Deploying Llamafied InternLM2-5-7B-Chat-1M Model via vLLM OpenAI API Server",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.6\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-172-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.91\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100 80GB PCIe\r\n  MIG 7g.80gb     Device  0:\r\n\r\nNvidia driver version: 535.129.03\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      46 bits physical, 57 bits virtual\r\nCPU(s):                             48\r\nOn-line CPU(s) list:                0-47\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 24\r\nSocket(s):                          1\r\nNUMA node(s):                       1\r\nVendor ID:                          GenuineIntel\r\nCPU family:                         6\r\nModel:                              106\r\nModel name:                         Intel(R) Xeon(R) Gold 6338 CPU @ 2.00GHz\r\nStepping:                           6\r\nCPU MHz:                            1995.297\r\nBogoMIPS:                           3990.59\r\nVirtualization:                     VT-x\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          1.1 MiB\r\nL1i cache:                          768 KiB\r\nL2 cache:                           30 MiB\r\nL3 cache:                           48 MiB\r\nNUMA node0 CPU(s):                  0-47\r\nVulnerability Gather data sampling: Unknown: Dependent on hypervisor status\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Mitigation; TSX disabled\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch topoext cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd arat avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid md_clear arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.23.5\r\n[pip3] numpyencoder==0.3.0\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] onnx==1.15.0\r\n[pip3] onnxconverter-common==1.14.0\r\n[pip3] onnxmltools==1.12.0\r\n[pip3] onnxruntime==1.16.3\r\n[pip3] skl2onnx==1.16.0\r\n[pip3] torch==2.3.0+cu121\r\n[pip3] torchaudio==2.3.0+cu121\r\n[pip3] torchvision==0.18.0+cu121\r\n[pip3] transformers==4.42.3\r\n[pip3] triton==2.3.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.4.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-47    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nI'm encountering a consistent timeout error when attempting to deploy the Llamafied version of the InternLM2-5-7B-Chat-1M model using vLLM's OpenAI API server. The specific model I'm using is \"Downtown-Case/internlm2_5-7b-chat-1m-llamafied\" (https://huggingface.co/Downtown-Case/internlm2_5-7b-chat-1m-llamafied/tree/main).\r\n\r\nError Details:\r\nThe server logs show a recurring timeout error in the engine iteration:\r\n```\r\n2024-07-13 15:06:34,921 - INFO - vLLM: INFO:     ::1:49616 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\r\n2024-07-13 15:06:34,923 - INFO - HTTP Request: POST http://localhost:9000/v1/chat/completions \"HTTP/1.1 200 OK\"\r\n2024-07-13 15:06:36,684 - INFO - vLLM: INFO 07-13 15:06:36 metrics.py:295] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.\r\n2024-07-13 15:06:46,685 - INFO - vLLM: INFO 07-13 15:06:46 metrics.py:295] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.\r\n2024-07-13 15:06:56,685 - INFO - vLLM: INFO 07-13 15:06:56 metrics.py:295] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.\r\n2024-07-13 15:07:06,686 - INFO - vLLM: INFO 07-13 15:07:06 metrics.py:295] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.\r\n2024-07-13 15:07:16,687 - INFO - vLLM: INFO 07-13 15:07:16 metrics.py:295] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.\r\n2024-07-13 15:07:26,687 - INFO - vLLM: INFO 07-13 15:07:26 metrics.py:295] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.\r\n2024-07-13 15:07:34,921 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:616] Engine iteration timed out. This should never happen!\r\n2024-07-13 15:07:34,930 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53] Engine background task failed\r\n2024-07-13 15:07:34,930 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53] Traceback (most recent call last):\r\n2024-07-13 15:07:34,932 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53]   File \"/home/sk.jung/.local/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 589, in run_engine_loop\r\n2024-07-13 15:07:34,933 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53]     done, _ = await asyncio.wait(\r\n2024-07-13 15:07:34,935 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53]   File \"/usr/lib/python3.10/asyncio/tasks.py\", line 384, in wait\r\n2024-07-13 15:07:34,937 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53]     return await _wait(fs, timeout, return_when, loop)\r\n2024-07-13 15:07:34,938 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53]   File \"/usr/lib/python3.10/asyncio/tasks.py\", line 491, in _wait\r\n2024-07-13 15:07:34,940 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53]     await waiter\r\n2024-07-13 15:07:34,941 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53] asyncio.exceptions.CancelledError\r\n2024-07-13 15:07:34,943 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53]\r\n2024-07-13 15:07:34,944 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53] During handling of the above exception, another exception occurred:\r\n2024-07-13 15:07:34,946 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53]\r\n2024-07-13 15:07:34,948 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53] Traceback (most recent call last):\r\n2024-07-13 15:07:34,949 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53]   File \"/home/sk.jung/.local/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 43, in _log_task_completion\r\n2024-07-13 15:07:34,951 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53]     return_value = task.result()\r\n2024-07-13 15:07:34,953 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53]   File \"/home/sk.jung/.local/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 588, in run_engine_loop\r\n2024-07-13 15:07:34,954 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53]     async with asyncio_timeout(ENGINE_ITERATION_TIMEOUT_S):\r\n2024-07-13 15:07:34,956 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53]   File \"/home/sk.jung/.local/lib/python3.10/site-packages/vllm/engine/async_timeout.py\", line 95, in __aexit__\r\n2024-07-13 15:07:34,958 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53]     self._do_exit(exc_type)\r\n2024-07-13 15:07:34,959 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53]   File \"/home/sk.jung/.local/lib/python3.10/site-packages/vllm/engine/async_timeout.py\", line 178, in _do_exit\r\n2024-07-13 15:07:34,961 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53]     raise asyncio.TimeoutError\r\n2024-07-13 15:07:34,962 - INFO - vLLM: ERROR 07-13 15:07:34 async_llm_engine.py:53] asyncio.exceptions.TimeoutError\r\n```\r\n\r\nThis issue seems to be specific to the Llamafied version of the InternLM2-5-7B-Chat-1M model. It would be helpful to know if this is a known issue with Llamafied models or if there are any specific considerations when deploying such models with vLLM.\r\nAny assistance or insights into resolving this issue would be greatly appreciated. Thank you for your time and support.\r\n",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-13T15:17:03+00:00",
    "closed_at": "2024-11-24T02:08:20+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6414/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6414"
  },
  {
    "number": 4694,
    "title": "[Feature]: bind python and c++ through tools other than pybind11",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nAs vLLM goes into a fast release schedule (currently one release every two weeks), we will quickly hit the project-wide limit of pypi (around 5GB per project). One solution, as pointed out in https://github.com/pypi/support/issues/3792#issuecomment-2099941677 , is to build one wheel for all python versions (Python 3.8+).\r\n\r\nI have figured out the procedure https://github.com/pypi/support/issues/3792#issuecomment-2101360740 , but pybind11 does not support this Python Limited API protocol.\r\n\r\nOne possible solution is to replace pybind11 with some other tools, so that the binding procedure can be used with Python Limited API.\r\n\r\nPossible solutions:\r\n\r\n- Nanobind (seems to support it starting from Python 3.12 only: https://github.com/wjakob/nanobind/pull/561 )\r\n- register ops through pytorch directly https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_",
    "labels": [
      "help wanted",
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-08T23:18:56+00:00",
    "closed_at": "2024-10-27T22:53:29+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4694/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/4694"
  },
  {
    "number": 2264,
    "title": "The Effect of Chinese Alpaca",
    "body": "Why does the effect of the Chinese alphaca chat model after starting the model with VLLM look like the effect of the Chinese Lama generated model.\r\n<img width=\"773\" alt=\"Snipaste_2023-12-26_11-25-23\" src=\"https://github.com/vllm-project/vllm/assets/136042459/1bed4c73-998a-434e-bfa8-a4764a94020f\">\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-12-26T03:25:46+00:00",
    "closed_at": "2024-03-28T12:03:09+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2264/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2264"
  },
  {
    "number": 5605,
    "title": "[Feature]: Access to user information in scheduler",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\r\n\r\nTo my knowledge, there is no user awareness in the core implementation of vLLM. However, in order to perform optimizations having the final user in mind, it would be very useful to be able to receive and use this information.\r\n\r\nI see that there is a parameter called _user_ in the openAI API, i.e. (line 353 of file vllm/entrypoints/openai/protocol.py), but this information is not transferred further to the core of vLLM.\r\n```python\r\nclass CompletionRequest(OpenAIBaseModel):\r\n    [...]\r\n    user: Optional[str] = None\r\n``` \r\n\r\nSpecifically, I am interested in creating scheduling policies based on the users, i.e.,  a scheduler that divides service fairly among multiple users. For that, it would be necessary to receive the user identifier in the scheduler. The scheduler only receives _SequenceGroup_ objects without user information in method _add_seq_group_ (line 320 of file vllm/core/scheduler.py).\r\n\r\nIs there any way to access this information through other ways?\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\nWe have modified the code to add the user information in the call _add_seq_group_ of the scheduler, with all required changes to receive this information from the openAI API. However, we do not know up to what point it is of interest for the vllm community.",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-06-17T17:59:37+00:00",
    "closed_at": "2024-11-25T02:05:51+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5605/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5605"
  },
  {
    "number": 4760,
    "title": "[Performance]: Why the avg. througput generation is low?",
    "body": "### Report of performance regression\r\n\r\nHi I use this:\r\n```\r\nserver_vllm.py \\\r\n  --model \"/data/models_temp/functionary-small-v2.4/\" \\\r\n  --served-model-name \"functionary\" \\\r\n  --dtype=bfloat16 \\\r\n  --max-model-len 2048 \\\r\n  --host 0.0.0.0 \\\r\n  --port 8000 \\\r\n  --enforce-eager \\\r\n  --gpu-memory-utilization 0.94\r\n```\r\non rtx 3090 24gb\r\n\r\nWhy I've got low speed?:\r\n`Avg prompt throughput: 102.2 tokens/s, Avg generation throughput: 2.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%`\r\n\r\n\r\nThis is my config:\r\n```\r\n| INFO 05-11 08:17:48 server_vllm.py:473] args: Namespace(host='0.0.0.0', port=8000, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], served_model_name='functionary', grammar_sampling=False, model='/data/models_temp/functionary-small-v2.4/', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=2048, guided_decoding_backend='outlines', worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.94, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=True, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, num_speculative_tokens=None, speculative_max_model_len=None, model_loader_extra_config=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\nfunctionary  | You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\r\nfunctionary  | Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nfunctionary  | INFO 05-11 08:17:49 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='/data/models_temp/functionary-small-v2.4/', speculative_config=None, tokenizer='/data/models_temp/functionary-small-v2.4/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\r\nfunctionary  | Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nfunctionary  | INFO 05-11 08:17:50 utils.py:608] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\r\nfunctionary  | INFO 05-11 08:17:50 selector.py:28] Using FlashAttention backend.\r\nfunctionary  | INFO 05-11 08:17:53 model_runner.py:173] Loading model weights took 13.4976 GB\r\nfunctionary  | INFO 05-11 08:17:53 gpu_executor.py:119] # GPU blocks: 4185, # CPU blocks: 2048\r\nfunctionary  | INFO:     Started server process [19]\r\nfunctionary  | INFO:     Waiting for application startup.\r\nfunctionary  | INFO:     Application startup complete.\r\nfunctionary  | INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\r\n```",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-11T08:22:52+00:00",
    "closed_at": "2025-05-01T02:13:38+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4760/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4760"
  },
  {
    "number": 2981,
    "title": "\u6c42\u95ee qwen-14b\u5fae\u8c03\u540e\u7684\u6a21\u578b\u7528vllm\u63a8\u7406\u540e\u7ed3\u679c\u90fd\u4e3a\u7a7a ",
    "body": null,
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-02-22T07:49:32+00:00",
    "closed_at": "2024-11-29T02:08:15+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2981/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2981"
  },
  {
    "number": 4260,
    "title": "[Feature]: Add argument terminators \"eos_token_id\" to serving models api_server.py",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nNew models as LLama-3 use different end terminator, that are need to be specified. \r\nFor example when using the API the client response return \"me know if this is correct!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThat\\'s correct! The output is\", thats seems the roles are not well parsed. \n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-04-22T09:19:28+00:00",
    "closed_at": "2024-06-28T06:11:12+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4260/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4260"
  },
  {
    "number": 1875,
    "title": "Update documentation for OpenAI API > 1.0.0",
    "body": "Hi, I'd like to use vllm with the openAI python API, so I can switch between VLLM and OpenAI just by changing the URL. I have `openai` version 1.3.5.\r\n\r\n[Here are the docs ](https://docs.vllm.ai/en/latest/getting_started/quickstart.html#using-openai-chat-api-with-vllm):\r\n```python\r\nimport openai\r\n# Set OpenAI's API key and API base to use vLLM's API server.\r\nopenai.api_key = \"EMPTY\"\r\nopenai.api_base = \"http://localhost:8000/v1\"\r\nchat_response = openai.ChatCompletion.create(\r\n    model=\"facebook/opt-125m\",\r\n    messages=[\r\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n        {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\r\n    ]\r\n)\r\nprint(\"Chat response:\", chat_response)\r\n```\r\n\r\nThis results in the following error: \r\n```bash\r\nopenai.lib._old_api.APIRemovedInV1: \r\n\r\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\r\n\r\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \r\n\r\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\r\n\r\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\r\n```\r\n\r\nUsing issue #1644, the correct version is: \r\n```python\r\nfrom openai import OpenAI\r\n    # Set OpenAI's API key and API base to use vLLM's API server.\r\n    api_key = \"EMPTY\"\r\n    api_base = \"http://localhost:8000/v1\"\r\n    client = OpenAI(api_key=api_key, base_url=api_base)\r\n    completion = client.chat.completions.create(\r\n        # model=\"facebook/opt-125m\",\r\n        model=\"facebook/opt-125m\",\r\n        messages=[\r\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n            {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\r\n        ]\r\n    )\r\n    print(\"Chat response:\", completion.choices[0].message.content)\r\n```\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-12-01T02:43:36+00:00",
    "closed_at": "2024-04-04T08:12:57+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1875/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1875"
  },
  {
    "number": 973,
    "title": "Deployment stuck when using kuberay to scale Multi-GPU LLM on Kubernetes",
    "body": "I want to use kuberay to serve and horizontaly-scale my LLM on Kubernetes.\r\n\r\nThe python code i want to deploy looks somewhat like this:\r\n```\r\nimport json\r\nimport logging\r\nfrom typing import AsyncGenerator\r\n\r\nimport ray\r\nfrom fastapi import BackgroundTasks\r\nfrom huggingface_hub import login\r\nfrom ray import serve\r\nfrom starlette.requests import Request\r\nfrom starlette.responses import StreamingResponse, Response\r\nfrom vllm.engine.arg_utils import AsyncEngineArgs\r\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\r\nfrom vllm.sampling_params import SamplingParams\r\nfrom vllm.utils import random_uuid\r\n\r\nlogger = logging.getLogger(\"ray.serve\")\r\n@serve.deployment()\r\nclass VLLMPredictDeployment:\r\n    def __init__(self, **kwargs):\r\n        \"\"\"\r\n        Construct a VLLM deployment.\r\n\r\n        Refer to https://github.com/vllm-project/vllm/blob/main/vllm/engine/arg_utils.py\r\n        for the full list of arguments.\r\n\r\n        Args:\r\n            model: name or path of the huggingface model to use\r\n            download_dir: directory to download and load the weights,\r\n                default to the default cache dir of huggingface.\r\n            use_np_weights: save a numpy copy of model weights for\r\n                faster loading. This can increase the disk usage by up to 2x.\r\n            use_dummy_weights: use dummy values for model weights.\r\n            dtype: data type for model weights and activations.\r\n                The \"auto\" option will use FP16 precision\r\n                for FP32 and FP16 models, and BF16 precision.\r\n                for BF16 models.\r\n            seed: random seed.\r\n            worker_use_ray: use Ray for distributed serving, will be\r\n                automatically set when using more than 1 GPU\r\n            pipeline_parallel_size: number of pipeline stages.\r\n            tensor_parallel_size: number of tensor parallel replicas.\r\n            block_size: token block size.\r\n            swap_space: CPU swap space size (GiB) per GPU.\r\n            gpu_memory_utilization: the percentage of GPU memory to be used for\r\n                the model executor\r\n            max_num_batched_tokens: maximum number of batched tokens per iteration\r\n            max_num_seqs: maximum number of sequences per iteration.\r\n            disable_log_stats: disable logging statistics.\r\n            engine_use_ray: use Ray to start the LLM engine in a separate\r\n                process as the server process.\r\n            disable_log_requests: disable logging requests.\r\n        \"\"\"\r\n        login(token='[HF_TOKEN]')\r\n        kwargs = {**kwargs, 'tensor_parallel_size': 2, 'model': 'meta-llama/Llama-2-7b-chat-hf'}\r\n        args = AsyncEngineArgs(**kwargs)\r\n        self.engine = AsyncLLMEngine.from_engine_args(args)\r\n\r\n    async def stream_results(self, results_generator) -> AsyncGenerator[bytes, None]:\r\n        num_returned = 0\r\n        async for request_output in results_generator:\r\n            text_outputs = [output.text for output in request_output.outputs]\r\n            assert len(text_outputs) == 1\r\n            text_output = text_outputs[0][num_returned:]\r\n            ret = {\"text\": text_output}\r\n            yield (json.dumps(ret) + \"\\n\").encode(\"utf-8\")\r\n            num_returned += len(text_output)\r\n\r\n    async def may_abort_request(self, request_id) -> None:\r\n        await self.engine.abort(request_id)\r\n\r\n    async def __call__(self, request: Request) -> Response:\r\n        \"\"\"Generate completion for the request.\r\n\r\n        The request should be a JSON object with the following fields:\r\n        - prompt: the prompt to use for the generation.\r\n        - stream: whether to stream the results or not.\r\n        - other fields: the sampling parameters (See `SamplingParams` for details).\r\n        \"\"\"\r\n        request_dict = await request.json()\r\n        prompt = request_dict.pop(\"prompt\")\r\n        stream = request_dict.pop(\"stream\", False)\r\n        sampling_params = SamplingParams(**request_dict)\r\n        request_id = random_uuid()\r\n        results_generator = self.engine.generate(prompt, sampling_params, request_id)\r\n        if stream:\r\n            background_tasks = BackgroundTasks()\r\n            # Using background_taks to abort the the request\r\n            # if the client disconnects.\r\n            background_tasks.add_task(self.may_abort_request, request_id)\r\n            return StreamingResponse(\r\n                self.stream_results(results_generator), background=background_tasks\r\n            )\r\n\r\n        # Non-streaming case\r\n        final_output = None\r\n        async for request_output in results_generator:\r\n            if await request.is_disconnected():\r\n                # Abort the request if the client disconnects.\r\n                await self.engine.abort(request_id)\r\n                return Response(status_code=499)\r\n            final_output = request_output\r\n\r\n        assert final_output is not None\r\n        prompt = final_output.prompt\r\n        text_outputs = [prompt + output.text for output in final_output.outputs]\r\n        ret = {\"text\": text_outputs}\r\n        return Response(content=json.dumps(ret))\r\n\r\n\r\ndeployment = VLLMPredictDeployment.bind()\r\n```\r\n\r\nThe kubernetes configuration looks like this:\r\n```\r\napiVersion: ray.io/v1alpha1\r\nkind: RayService\r\nmetadata:\r\n  name: llm-manager\r\nspec:\r\n  serviceUnhealthySecondThreshold: 3600\r\n  deploymentUnhealthySecondThreshold: 3600\r\n  serveConfigV2: |\r\n    applications:\r\n    - name: model\r\n      route_prefix: /\r\n      import_path: model.ray.deployment_vllm_test:deployment\r\n      runtime_env:\r\n        working_dir: \"[DIRECTORY_TO_PYTHON_CODE]\"\r\n      deployments:\r\n      - name: VLLMPredictDeployment\r\n        autoscaling_config:\r\n          min_replicas: 1\r\n          max_replicas: 1\r\n          target_num_ongoing_requests_per_replica: 3\r\n          downscale_delay_s: 3600\r\n        ray_actor_options:\r\n          num_gpus: 2\r\n  rayClusterConfig:\r\n    rayVersion: '2.6.2'\r\n    enableInTreeAutoscaling: true\r\n    autoscalerOptions:\r\n      upscalingMode: Default\r\n      idleTimeoutSeconds: 60\r\n    headGroupSpec:\r\n      serviceType: LoadBalancer # change to LoadBalancer for external access\r\n      rayStartParams:\r\n        dashboard-host: '0.0.0.0'\r\n      template:\r\n        spec:\r\n          imagePullSecrets:\r\n            - name: docker-secret\r\n          containers:\r\n            - name: ray-head\r\n              image: cortecs.ai/ray-worker:0.0.3\r\n              resources:\r\n                limits:\r\n                  cpu: 1\r\n                  memory: 8Gi\r\n                requests:\r\n                  cpu: 1\r\n                  memory: 8Gi\r\n              ports:\r\n                - containerPort: 6379\r\n                  name: gcs-server\r\n                - containerPort: 8265 # Ray dashboard\r\n                  name: dashboard\r\n                - containerPort: 10001\r\n                  name: client\r\n                - containerPort: 8000\r\n                  name: serve\r\n    workerGroupSpecs:\r\n      - replicas: 1\r\n        minReplicas: 1\r\n        maxReplicas: 1\r\n        groupName: llm-group\r\n        rayStartParams: { }\r\n        template:\r\n          spec:\r\n            nodeSelector:\r\n              cloud.google.com/gke-nodepool: gpu-pool\r\n            imagePullSecrets:\r\n              - name: docker-secret\r\n            containers:\r\n              - name: ray-worker\r\n                image: cortecs.ai/ray-worker:0.0.3\r\n                lifecycle:\r\n                  preStop:\r\n                    exec:\r\n                      command: [ \"/bin/sh\",\"-c\",\"ray stop\" ]\r\n                resources:\r\n                  limits:\r\n                    cpu: 2\r\n                    memory: 32Gi\r\n                    nvidia.com/gpu: 2\r\n                  requests:\r\n                    cpu: 2\r\n                    memory: 32Gi\r\n                    nvidia.com/gpu: 2\r\n            tolerations:\r\n              - key: \"ray.io/node-type\"\r\n                operator: \"Equal\"\r\n                value: \"worker\"\r\n                effect: \"NoSchedule\"\r\n    headServiceAnnotations: { }\r\n```\r\n\r\nWhen I am using `tensor_parallel_size=1` and therefore deploy the LLM on a single GPU everything works as expected. \r\nBut in the momement I use `tensor_parallel_size=2` or something higher the deployment get stuck and hangs forever with the following message `Deployment model_VLLMPredictDeployment has 1 replicas that have taken more than 30s to initialize. This may be caused by a slow __init__ or reconfigure method.`\r\nAfter checking all of the logs in the ray dashboard the only thing I found which could be the issue is the following:\r\n```\r\nUsing address llm-manager-raycluster-bpfjf-head-svc.ray-system.svc.cluster.local:6379 set in the environment variable RAY_ADDRESS\r\nConnecting to existing Ray cluster at address: llm-manager-raycluster-bpfjf-head-svc.ray-system.svc.cluster.local:6379...\r\nCalling ray.init() again after it has already been called.\r\n```\r\n\r\nI am using vllm version 0.1.4 and want to deploy this on Google Kubernetes Engine.\r\nAny ideas whats the problem here?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-09-07T07:24:02+00:00",
    "closed_at": "2024-04-04T08:12:03+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/973/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/973"
  },
  {
    "number": 1995,
    "title": "How can I deploy vllm model with multi-replicas",
    "body": "I want to deploy a LLM model on 8 A100 gpus. \r\nTo support the higher concurrency, I want to deploy 8 replicas (one replica on one gpu), and I want to expose one service to handle user requests, how can I do it?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-12-09T03:39:26+00:00",
    "closed_at": "2024-08-28T19:04:12+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1995/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1995"
  },
  {
    "number": 11839,
    "title": "[Bug]: ValueError: There is no module or parameter named 'lm_head.qweight_type' in Qwen2ForCausalLM.When use GGUF and draft model",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 24.04.1 LTS (x86_64)\r\nGCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.39\r\n\r\nPython version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.39\r\nIs CUDA available: True\r\nCUDA runtime version: 12.6.77\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090\r\nNvidia driver version: 560.94\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.5.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.5.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.5.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.5.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.5.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.5.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.5.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.5.1\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             32\r\nOn-line CPU(s) list:                0-31\r\nVendor ID:                          GenuineIntel\r\nModel name:                         13th Gen Intel(R) Core(TM) i9-13900KS\r\nCPU family:                         6\r\nModel:                              183\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 16\r\nSocket(s):                          1\r\nStepping:                           1\r\nBogoMIPS:                           6374.40\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves avx_vnni umip waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nHypervisor vendor:                  Microsoft\r\nVirtualization type:                full\r\nL1d cache:                          768 KiB (16 instances)\r\nL1i cache:                          512 KiB (16 instances)\r\nL2 cache:                           32 MiB (16 instances)\r\nL3 cache:                           36 MiB (1 instance)\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Mitigation; Enhanced IBRS\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==7.0.0\r\n[pip3] gptqmodel==1.5.1+cu124torch2.5\r\n[pip3] mypy==1.11.2\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] numpydoc==1.7.0\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvcc-cu12==12.6.85\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pytorch-lightning==2.5.0.post0\r\n[pip3] pyzmq==25.1.2\r\n[pip3] torch==2.5.1\r\n[pip3] torchaudio==2.4.0\r\n[pip3] torchmetrics==1.6.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.46.1\r\n[pip3] triton==3.1.0\r\n[conda] _anaconda_depends         2024.10             py312_mkl_0  \r\n[conda] blas                      1.0                         mkl  \r\n[conda] cuda-cudart               12.4.127                      0    nvidia\r\n[conda] cuda-cupti                12.4.127                      0    nvidia\r\n[conda] cuda-libraries            12.4.1                        0    nvidia\r\n[conda] cuda-nvrtc                12.4.127                      0    nvidia\r\n[conda] cuda-nvtx                 12.4.127                      0    nvidia\r\n[conda] cuda-opencl               12.6.77                       0    nvidia\r\n[conda] cuda-runtime              12.4.1                        0    nvidia\r\n[conda] cuda-version              12.6                          3    nvidia\r\n[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\r\n[conda] gptqmodel                 1.5.1+cu124torch2.5          pypi_0    pypi\r\n[conda] libcublas                 12.4.5.8                      0    nvidia\r\n[conda] libcufft                  11.2.1.3                      0    nvidia\r\n[conda] libcufile                 1.11.1.6                      0    nvidia\r\n[conda] libcurand                 10.3.7.77                     0    nvidia\r\n[conda] libcusolver               11.6.1.9                      0    nvidia\r\n[conda] libcusparse               12.3.1.170                    0    nvidia\r\n[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\r\n[conda] libnpp                    12.2.5.30                     0    nvidia\r\n[conda] libnvfatbin               12.6.77                       0    nvidia\r\n[conda] libnvjitlink              12.4.127                      0    nvidia\r\n[conda] libnvjpeg                 12.3.1.117                    0    nvidia\r\n[conda] mkl                       2023.1.0         h213fc3f_46344  \r\n[conda] mkl-service               2.4.0           py312h5eee18b_1  \r\n[conda] mkl_fft                   1.3.10          py312h5eee18b_0  \r\n[conda] mkl_random                1.2.7           py312h526ad5a_0  \r\n[conda] numpy                     1.26.4          py312hc5e2394_0  \r\n[conda] numpy-base                1.26.4          py312h0da6c21_0  \r\n[conda] numpydoc                  1.7.0           py312h06a4308_0  \r\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvcc-cu12     12.6.85                  pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\r\n[conda] pytorch-cuda              12.4                 hc786d27_7    pytorch\r\n[conda] pytorch-lightning         2.5.0.post0              pypi_0    pypi\r\n[conda] pytorch-mutex             1.0                        cuda    pytorch\r\n[conda] pyzmq                     25.1.2          py312h6a678d5_0  \r\n[conda] torch                     2.5.1                    pypi_0    pypi\r\n[conda] torchaudio                2.4.0               py312_cu124    pytorch\r\n[conda] torchmetrics              1.6.1                    pypi_0    pypi\r\n[conda] torchvision               0.20.1                   pypi_0    pypi\r\n[conda] transformers              4.46.1                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.6.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X                              N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nLD_LIBRARY_PATH=/root/anaconda3/lib/python3.12/site-packages/cv2/../../lib64:/usr/local/cuda-12.6/lib64:/usr/local/cuda-12.6/lib64\r\nMKL_THREADING_LAYER=GNU\r\nMKL_SERVICE_FORCE_INTEL=1\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n```output\r\nValueError                                Traceback (most recent call last)\r\nCell In[5], [line 6](vscode-notebook-cell:?execution_count=5&line=6)\r\n      [3](vscode-notebook-cell:?execution_count=5&line=3) prompts = [\"The future of AI is\"]\r\n      [4](vscode-notebook-cell:?execution_count=5&line=4) sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\r\n----> [6](vscode-notebook-cell:?execution_count=5&line=6) llm = LLM(\r\n      [7](vscode-notebook-cell:?execution_count=5&line=7)     model=MODLE_PATH,\r\n      [8](vscode-notebook-cell:?execution_count=5&line=8)     tensor_parallel_size=1,\r\n      [9](vscode-notebook-cell:?execution_count=5&line=9)     speculative_model=SPECULATIVE_MODLE_PATH,\r\n     [10](vscode-notebook-cell:?execution_count=5&line=10)     num_speculative_tokens=5,\r\n     [11](vscode-notebook-cell:?execution_count=5&line=11)     use_v2_block_manager=True,\r\n     [12](vscode-notebook-cell:?execution_count=5&line=12) )\r\n\r\nFile ~/anaconda3/lib/python3.12/site-packages/vllm/utils.py:986, in deprecate_args.<locals>.wrapper.<locals>.inner(*args, **kwargs)\r\n    [979](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/utils.py:979)             msg += f\" {additional_message}\"\r\n    [981](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/utils.py:981)         warnings.warn(\r\n    [982](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/utils.py:982)             DeprecationWarning(msg),\r\n    [983](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/utils.py:983)             stacklevel=3,  # The inner function takes up one level\r\n    [984](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/utils.py:984)         )\r\n--> [986](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/utils.py:986) return fn(*args, **kwargs)\r\n\r\nFile ~/anaconda3/lib/python3.12/site-packages/vllm/entrypoints/llm.py:230, in LLM.__init__(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\r\n    [227](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/entrypoints/llm.py:227) self.engine_class = self.get_engine_class()\r\n    [229](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/entrypoints/llm.py:229) # TODO(rob): enable mp by default (issue with fork vs spawn)\r\n--> [230](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/entrypoints/llm.py:230) self.llm_engine = self.engine_class.from_engine_args(\r\n    [231](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/entrypoints/llm.py:231)     engine_args, usage_context=UsageContext.LLM_CLASS)\r\n    [233](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/entrypoints/llm.py:233) self.request_counter = Counter()\r\n\r\nFile ~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:517, in LLMEngine.from_engine_args(cls, engine_args, usage_context, stat_loggers)\r\n    [515](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:515) executor_class = cls._get_executor_cls(engine_config)\r\n    [516](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:516) # Create the LLM engine.\r\n--> [517](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:517) engine = cls(\r\n    [518](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:518)     vllm_config=engine_config,\r\n    [519](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:519)     executor_class=executor_class,\r\n    [520](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:520)     log_stats=not engine_args.disable_log_stats,\r\n    [521](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:521)     usage_context=usage_context,\r\n    [522](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:522)     stat_loggers=stat_loggers,\r\n    [523](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:523) )\r\n    [525](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:525) return engine\r\n\r\nFile ~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:273, in LLMEngine.__init__(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\r\n    [269](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:269) self.input_registry = input_registry\r\n    [270](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:270) self.input_processor = input_registry.create_input_processor(\r\n    [271](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:271)     self.model_config)\r\n--> [273](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:273) self.model_executor = executor_class(vllm_config=vllm_config, )\r\n    [275](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:275) if self.model_config.runner_type != \"pooling\":\r\n    [276](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:276)     self._initialize_kv_caches()\r\n\r\nFile ~/anaconda3/lib/python3.12/site-packages/vllm/executor/executor_base.py:36, in ExecutorBase.__init__(self, vllm_config)\r\n     [34](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/executor/executor_base.py:34) self.prompt_adapter_config = vllm_config.prompt_adapter_config\r\n     [35](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/executor/executor_base.py:35) self.observability_config = vllm_config.observability_config\r\n---> [36](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/executor/executor_base.py:36) self._init_executor()\r\n\r\nFile ~/anaconda3/lib/python3.12/site-packages/vllm/executor/gpu_executor.py:34, in GPUExecutor._init_executor(self)\r\n     [30](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/executor/gpu_executor.py:30) assert self.parallel_config.world_size == 1, (\r\n     [31](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/executor/gpu_executor.py:31)     \"GPUExecutor only supports single GPU.\")\r\n     [33](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/executor/gpu_executor.py:33) self.driver_worker = self._create_worker()\r\n---> [34](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/executor/gpu_executor.py:34) self.driver_worker.init_device()\r\n     [35](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/executor/gpu_executor.py:35) self.driver_worker.load_model()\r\n\r\nFile ~/anaconda3/lib/python3.12/site-packages/vllm/spec_decode/spec_decode_worker.py:324, in SpecDecodeWorker.init_device(self)\r\n    [322](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/spec_decode/spec_decode_worker.py:322) # NOTE(cade): load_model is not part of the WorkerBase interface.\r\n    [323](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/spec_decode/spec_decode_worker.py:323) self.scorer_worker.load_model()\r\n--> [324](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/spec_decode/spec_decode_worker.py:324) self.proposer_worker.load_model()\r\n    [326](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/spec_decode/spec_decode_worker.py:326) self._metrics.init_tensors(self.rank, device_type=self.device)\r\n    [327](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/spec_decode/spec_decode_worker.py:327) self.spec_decode_sampler.init_tensors(self.rank,\r\n    [328](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/spec_decode/spec_decode_worker.py:328)                                       device_type=self.device)\r\n\r\nFile ~/anaconda3/lib/python3.12/site-packages/vllm/worker/worker.py:155, in Worker.load_model(self)\r\n    [154](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/worker/worker.py:154) def load_model(self):\r\n--> [155](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/worker/worker.py:155)     self.model_runner.load_model()\r\n\r\nFile ~/anaconda3/lib/python3.12/site-packages/vllm/worker/model_runner.py:1096, in GPUModelRunnerBase.load_model(self)\r\n   [1094](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/worker/model_runner.py:1094) logger.info(\"Starting to load model %s...\", self.model_config.model)\r\n   [1095](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/worker/model_runner.py:1095) with DeviceMemoryProfiler() as m:\r\n-> [1096](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/worker/model_runner.py:1096)     self.model = get_model(vllm_config=self.vllm_config)\r\n   [1098](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/worker/model_runner.py:1098) self.model_memory_usage = m.consumed_memory\r\n   [1099](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/worker/model_runner.py:1099) logger.info(\"Loading model weights took %.4f GB\",\r\n   [1100](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/worker/model_runner.py:1100)             self.model_memory_usage / float(2**30))\r\n\r\nFile ~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py:12, in get_model(vllm_config)\r\n     [10](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py:10) def get_model(*, vllm_config: VllmConfig) -> nn.Module:\r\n     [11](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py:11)     loader = get_model_loader(vllm_config.load_config)\r\n---> [12](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py:12)     return loader.load_model(vllm_config=vllm_config)\r\n\r\nFile ~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py:1234, in GGUFModelLoader.load_model(self, vllm_config)\r\n   [1232](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py:1232)     with torch.device(device_config.device):\r\n   [1233](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py:1233)         model = _initialize_model(vllm_config=vllm_config)\r\n-> [1234](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py:1234)     model.load_weights(\r\n   [1235](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py:1235)         self._get_weights_iterator(local_model_path, gguf_weights_map))\r\n   [1236](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py:1236) return model\r\n\r\nFile ~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py:506, in Qwen2ForCausalLM.load_weights(self, weights)\r\n    [499](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py:499) def load_weights(self, weights: Iterable[Tuple[str,\r\n    [500](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py:500)                                                torch.Tensor]]) -> Set[str]:\r\n    [501](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py:501)     loader = AutoWeightsLoader(\r\n    [502](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py:502)         self,\r\n    [503](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py:503)         skip_prefixes=([\"lm_head.\"]\r\n    [504](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py:504)                        if self.config.tie_word_embeddings else None),\r\n    [505](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py:505)     )\r\n--> [506](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py:506)     return loader.load_weights(weights)\r\n\r\nFile ~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:237, in AutoWeightsLoader.load_weights(self, weights, mapper)\r\n    [234](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:234) if mapper is not None:\r\n    [235](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:235)     weights = mapper.apply(weights)\r\n--> [237](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:237) autoloaded_weights = set(self._load_module(\"\", self.module, weights))\r\n    [238](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:238) return autoloaded_weights\r\n\r\nFile ~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:198, in AutoWeightsLoader._load_module(self, base_prefix, module, weights)\r\n    [194](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:194)         logger.debug(\"Skipping module %s\", prefix)\r\n    [196](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:196)         continue\r\n--> [198](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:198)     yield from self._load_module(prefix,\r\n    [199](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:199)                                  child_modules[child_prefix],\r\n    [200](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:200)                                  child_weights)\r\n    [201](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:201) elif child_prefix in child_params:\r\n    [202](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:202)     if self._can_skip(prefix):\r\n\r\nFile ~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:226, in AutoWeightsLoader._load_module(self, base_prefix, module, weights)\r\n    [222](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:222)     continue\r\n    [224](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:224) msg = (f\"There is no module or parameter named '{prefix}' \"\r\n    [225](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:225)        f\"in {type(self.module).__name__}\")\r\n--> [226](https://vscode-remote+wsl-002bubuntu-002d24-002e04.vscode-resource.vscode-cdn.net/mnt/d/my/work/study/ai/kaggle_code/aimo2/test/draft/~/anaconda3/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:226) raise ValueError(msg)\r\n\r\nValueError: There is no module or parameter named 'lm_head.qweight_type' in Qwen2ForCausalLM\r\n```\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nWhen use draft model in GGUF model.Then error.\r\n```python\r\nfrom modelscope import snapshot_download\r\nfrom modelscope.hub.file_download import model_file_download\r\n# SPECULATIVE_MODLE_PATH = snapshot_download('PowerInfer/SmallThinker-3B-Preview')\r\nSPECULATIVE_MODLE_PATH = model_file_download(model_id='bartowski/SmallThinker-3B-Preview-GGUF',file_path='SmallThinker-3B-Preview-IQ2_M.gguf')\r\nMODLE_PATH = model_file_download(model_id='bartowski/QwQ-32B-Preview-GGUF',file_path='QwQ-32B-Preview-IQ2_XS.gguf')\r\n\r\nfrom vllm import LLM, SamplingParams\r\n\r\nprompts = [\"The future of AI is\"]\r\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\r\n\r\nllm = LLM(\r\n    model=MODLE_PATH,\r\n    tensor_parallel_size=1,\r\n    speculative_model=SPECULATIVE_MODLE_PATH,\r\n    num_speculative_tokens=5,\r\n    use_v2_block_manager=True,\r\n)\r\n\r\n\r\noutputs = llm.generate(prompts, sampling_params)\r\nfor output in outputs:\r\n    print(f\"Prompt: {output.prompt!r}, Generated text: {output.outputs[0].text!r}\")\r\n```\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-08T10:22:59+00:00",
    "closed_at": "2025-05-09T02:10:03+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11839/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11839"
  },
  {
    "number": 10627,
    "title": "[Bug]: Crash with Qwen2-Audio Model in vLLM During Audio Processing",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-122-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA RTX A6000\r\nNvidia driver version: 560.35.03\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        46 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               64\r\nOn-line CPU(s) list:                  0-63\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz\r\nCPU family:                           6\r\nModel:                                79\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   16\r\nSocket(s):                            2\r\nStepping:                             1\r\nCPU max MHz:                          3000.0000\r\nCPU min MHz:                          1200.0000\r\nBogoMIPS:                             4200.09\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d\r\nVirtualization:                       VT-x\r\nL1d cache:                            1 MiB (32 instances)\r\nL1i cache:                            1 MiB (32 instances)\r\nL2 cache:                             8 MiB (32 instances)\r\nL3 cache:                             80 MiB (2 instances)\r\nNUMA node(s):                         2\r\nNUMA node0 CPU(s):                    0-15,32-47\r\nNUMA node1 CPU(s):                    16-31,48-63\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          KVM: Mitigation: VMX disabled\r\nVulnerability L1tf:                   Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable\r\nVulnerability Mds:                    Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Meltdown:               Mitigation; PTI\r\nVulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP conditional; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Mitigation; Clear CPU buffers; SMT vulnerable\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.46.2\r\n[pip3] triton==3.1.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.20.1                   pypi_0    pypi\r\n[conda] transformers              4.46.2                   pypi_0    pypi\r\n[conda] triton                    3.1.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.4.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-15,32-47      0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNVIDIA_VISIBLE_DEVICES=GPU-72601697-9e67-4365-1e25-cff04423fb9f\r\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\r\nNCCL_VERSION=2.21.5-1\r\nNVIDIA_DRIVER_CAPABILITIES=compute,display,graphics,utility,video\r\nNVIDIA_PRODUCT_NAME=CUDA\r\nCUDA_VERSION=12.4.1\r\nLD_LIBRARY_PATH=/workspace/miniconda3/envs/vllm/lib/python3.12/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n[dump.zip](https://github.com/user-attachments/files/17899909/dump.zip)\r\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen running the Qwen2-Audio model with vLLM for audio transcription tasks, the process may crash with a RuntimeError due to a shape mismatch during tensor operations. \r\n\r\nHost the Qwen2-Audio Server:\r\n```bash\r\nvllm serve Qwen/Qwen2-Audio-7B-Instruct--dtype=bfloat16 --port=5000 --served-model-name qwen2-audio-7b-instruct --gpu_memory_utilization=0.95\r\n```\r\n\r\nSubmit an audio file for text transcription:\r\n```bash\r\ncurl https://mtt0f4kcgk6iw5-5000.proxy.runpod.net//v1/chat/completions \\\r\n    -X POST \\\r\n    -H 'Content-Type: application/json' \\\r\n    -d '{\r\n    \"model\": \"qwen2-audio-7b-instruc\", \"max_tokens\":512, \"temperature\":0.01,\r\n    \"messages\" : [{ \"role\": \"user\", \"content\": [ {\"type\": \"audio_url\", \"audio_url\": {\"url\": \"xxx.mp3\"}}, {\"type\": \"text\", \"text\": \u201c\u63d0\u53d6\u6587\u672c\"}]}]  }'\r\n```\r\n\r\nError log:\r\n```bash\r\nNFO 11-24 17:46:31 logger.py:37] Received request chatcmpl-247fa5bbcca64896b269dc58fe916d23: prompt: '<|im_start|>system\\nYou are responsible for transcribing audio recordings into text.<|im_end|>\\n<|im_start|>user\\nAudio 1: <|audio_bos|><|AUDIO|><|audio_eos|>\\n\u63d0\u53d6\u6587\u5b57<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\r\nINFO 11-24 17:46:31 preprocess.py:215] Your model uses the legacy input pipeline instead of the new multi-modal processor. Please note that the legacy pipeline will be removed in a future release. For more details, see: https://github.com/vllm-project/vllm/issues/10114\r\nINFO 11-24 17:46:31 engine.py:267] Added request chatcmpl-247fa5bbcca64896b269dc58fe916d23.\r\nINFO 11-24 17:46:32 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20241124-174632.pkl...\r\nINFO 11-24 17:46:32 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20241124-174632.pkl.\r\nCRITICAL 11-24 17:46:32 launcher.py:99] MQLLMEngine is already dead, terminating server process\r\nINFO:     100.64.0.25:41350 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\r\nERROR 11-24 17:46:32 engine.py:135] RuntimeError('Error in model execution (input dumped to /tmp/err_execute_model_input_20241124-174632.pkl): shape mismatch: value tensor of shape [211, 4096] cannot be broadcast to indexing result of shape [212, 4096]')\r\nERROR 11-24 17:46:32 engine.py:135] Traceback (most recent call last):\r\nERROR 11-24 17:46:32 engine.py:135]   File \"/workspace/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/worker/model_runner_base.py\", line 116, in _wrapper\r\nERROR 11-24 17:46:32 engine.py:135]     return func(*args, **kwargs)\r\nERROR 11-24 17:46:32 engine.py:135]            ^^^^^^^^^^^^^^^^^^^^^\r\nERROR 11-24 17:46:32 engine.py:135]   File \"/workspace/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 1654, in execute_model\r\nERROR 11-24 17:46:32 engine.py:135]     hidden_or_intermediate_states = model_executable(\r\nERROR 11-24 17:46:32 engine.py:135]                                     ^^^^^^^^^^^^^^^^^\r\nERROR 11-24 17:46:32 engine.py:135]   File \"/workspace/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nERROR 11-24 17:46:32 engine.py:135]     return self._call_impl(*args, **kwargs)\r\nERROR 11-24 17:46:32 engine.py:135]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 11-24 17:46:32 engine.py:135]   File \"/workspace/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\nERROR 11-24 17:46:32 engine.py:135]     return forward_call(*args, **kwargs)\r\nERROR 11-24 17:46:32 engine.py:135]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 11-24 17:46:32 engine.py:135]   File \"/workspace/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/qwen2_audio.py\", line 396, in forward\r\nERROR 11-24 17:46:32 engine.py:135]     inputs_embeds[mask, :] = masked_audio_features\r\nERROR 11-24 17:46:32 engine.py:135]     ~~~~~~~~~~~~~^^^^^^^^^\r\nERROR 11-24 17:46:32 engine.py:135] RuntimeError: shape mismatch: value tensor of shape [211, 4096] cannot be broadcast to indexing result of shape [212, 4096]\r\nERROR 11-24 17:46:32 engine.py:135] \r\nERROR 11-24 17:46:32 engine.py:135] The above exception was the direct cause of the following exception:\r\nERROR 11-24 17:46:32 engine.py:135] \r\nERROR 11-24 17:46:32 engine.py:135] Traceback (most recent call last):\r\nERROR 11-24 17:46:32 engine.py:135]   File \"/workspace/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 133, in start\r\nERROR 11-24 17:46:32 engine.py:135]     self.run_engine_loop()\r\nERROR 11-24 17:46:32 engine.py:135]   File \"/workspace/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 196, in run_engine_loop\r\nERROR 11-24 17:46:32 engine.py:135]     request_outputs = self.engine_step()\r\nERROR 11-24 17:46:32 engine.py:135]                       ^^^^^^^^^^^^^^^^^^\r\nERROR 11-24 17:46:32 engine.py:135]   File \"/workspace/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 214, in engine_step\r\nERROR 11-24 17:46:32 engine.py:135]     raise e\r\nERROR 11-24 17:46:32 engine.py:135]   File \"/workspace/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 205, in engine_step\r\nERROR 11-24 17:46:32 engine.py:135]     return self.engine.step()\r\nERROR 11-24 17:46:32 engine.py:135]            ^^^^^^^^^^^^^^^^^^\r\nERROR 11-24 17:46:32 engine.py:135]   File \"/workspace/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/llm_engine.py\", line 1454, in step\r\nERROR 11-24 17:46:32 engine.py:135]     outputs = self.model_executor.execute_model(\r\nERROR 11-24 17:46:32 engine.py:135]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 11-24 17:46:32 engine.py:135]   File \"/workspace/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/executor/gpu_executor.py\", line 125, in execute_model\r\nERROR 11-24 17:46:32 engine.py:135]     output = self.driver_worker.execute_model(execute_model_req)\r\nERROR 11-24 17:46:32 engine.py:135]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 11-24 17:46:32 engine.py:135]   File \"/workspace/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/worker/worker_base.py\", line 343, in execute_model\r\nERROR 11-24 17:46:32 engine.py:135]     output = self.model_runner.execute_model(\r\nERROR 11-24 17:46:32 engine.py:135]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 11-24 17:46:32 engine.py:135]   File \"/workspace/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nERROR 11-24 17:46:32 engine.py:135]     return func(*args, **kwargs)\r\nERROR 11-24 17:46:32 engine.py:135]            ^^^^^^^^^^^^^^^^^^^^^\r\nERROR 11-24 17:46:32 engine.py:135]   File \"/workspace/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/worker/model_runner_base.py\", line 152, in _wrapper\r\nERROR 11-24 17:46:32 engine.py:135]     raise type(err)(\r\nERROR 11-24 17:46:32 engine.py:135] RuntimeError: Error in model execution (input dumped to /tmp/err_execute_model_input_20241124-174632.pkl): shape mismatch: value tensor of shape [211, 4096] cannot be broadcast to indexing result of shape [212, 4096]\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-25T08:49:42+00:00",
    "closed_at": "2025-04-24T02:08:13+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10627/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10627"
  },
  {
    "number": 9992,
    "title": "[Performance]: FP8 performance worse than FP16 for Qwen2-VL-2B-Instruct",
    "body": "### Proposal to improve performance\r\n\r\n_No response_\r\n\r\n### Report of performance regression\r\n\r\n_No response_\r\n\r\n### Misc discussion on performance\r\n\r\nestimated QPS is as follows:\r\nbs=1\uff1a11.402357925880366 for FP16 and 10.642891382295932 for FP8\r\nbs=8\uff1a51.62193861376064 for FP16 and 49.57986576846022 for FP8\r\nbs=16\uff1a61.87048607358999 for FP16 and 57.58566218192532 for FP8\r\nbs=32:\r\nFor FP8:\r\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 32/32 [00:00<00:00, 67.85it/s, est. speed input: 11468.33 toks/s, output: 271.44 toks/s]\r\n\r\nFor FP16:\r\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 32/32 [00:00<00:00, 74.14it/s, est. speed input: 12531.11 toks/s, output: 296.59 toks/s]\r\n\r\nThe FP8 model convert script is as follow:\r\n```\r\nfrom transformers import AutoProcessor, Qwen2VLForConditionalGeneration\r\n\r\nfrom llmcompressor.modifiers.quantization import QuantizationModifier\r\nfrom llmcompressor.transformers import oneshot, wrap_hf_model_class\r\nMODEL_ID = \"/home/hadoop-platcv/qwen2-vl-2b-instruct/00-src-files/00-model/qwen2-vl-2b-instruct/v4-20241028-151341/checkpoint-14427-merged\"\r\n\r\n# Load model.\r\nmodel_class = wrap_hf_model_class(Qwen2VLForConditionalGeneration)\r\nmodel = model_class.from_pretrained(MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\")\r\nprocessor = AutoProcessor.from_pretrained(MODEL_ID)\r\n\r\n# Configure the quantization algorithm and scheme.\r\n# In this case, we:\r\n#   * quantize the weights to fp8 with per channel via ptq\r\n#   * quantize the activations to fp8 with dynamic per token\r\nrecipe = QuantizationModifier(\r\n    targets=\"Linear\",\r\n    scheme=\"FP8_DYNAMIC\",\r\n    ignore=[\"re:.*lm_head\", \"re:visual.*\"],\r\n)\r\n\r\n# Apply quantization and save to disk in compressed-tensors format.\r\nSAVE_DIR = MODEL_ID.split(\"/\")[1] + \"-FP8-Dynamic\"\r\nSAVE_DIR = MODEL_ID + \"-FP8-Dynamic\"\r\noneshot(model=model, recipe=recipe, output_dir=SAVE_DIR)\r\nprocessor.save_pretrained(SAVE_DIR)\r\n\r\n# Confirm generations of the quantized model look sane.\r\nprint(\"========== SAMPLE GENERATION ==============\")\r\ninput_ids = processor(text=\"Hello my name is\", return_tensors=\"pt\").input_ids.to(\"cuda\")\r\noutput = model.generate(input_ids, max_new_tokens=20)\r\nprint(processor.decode(output[0]))\r\nprint(\"==========================================\")\r\n```\r\nThe inference script is as follows\r\n```\r\nfrom transformers import AutoProcessor\r\nfrom vllm import LLM, SamplingParams\r\nfrom qwen_vl_utils import process_vision_info\r\n\r\n\r\nMODEL_PATH = \"/home/hadoop-platcv/qwen2-vl-2b-instruct/00-src-files/00-model/qwen2-vl-2b-instruct/v4-20241028-151341/checkpoint-14427-merged-FP8-Dynamic\"\r\n#MODEL_PATH = \"/home/hadoop-platcv/qwen2-vl-2b-instruct/00-src-files/00-model/qwen2-vl-2b-instruct/v4-20241028-151341/checkpoint-14427-merged\"\r\ndevice = \"cuda\" # the device to load the model onto\r\n\r\nllm = LLM(\r\n    model=MODEL_PATH,\r\n    limit_mm_per_prompt={\"image\": 10, \"video\": 10},\r\n)\r\nsampling_params = SamplingParams(\r\n    temperature=0.1,\r\n    top_p=0.001,\r\n    repetition_penalty=1.05,\r\n    max_tokens=256,\r\n    stop_token_ids=[],\r\n)\r\nmessages = [\r\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n    {\r\n        \"role\": \"user\",\r\n        \"content\": [\r\n            {\r\n                \"type\": \"image\",\r\n                \"image\": \"/home/hadoop-platcv/qwen2-vl-2b-instruct/00-src-files/01-datasets/02_data/MLLM_moderator/09_\u56fd\u5bb6\u5f62\u8c61/test/01_secure_llm_shezheng_\u4e2d\u56fd\u5730\u56fe/images_part0/25332605707.jpg\",\r\n                \"min_pixels\":3136,\r\n                \"max_pixels\": 602112,\r\n            },\r\n            {\"type\": \"text\", \"text\": \"\u8bf7\u4e3a\u4ee5\u4e0b\u56fe\u7247\u6253\u6807\u7b7e\u3002\u53ef\u80fd\u7684\u6807\u7b7e\u5305\u62ec\uff1a\u5730\u56fe\u3001\u56fd\u65d7\u3001\u515a\u65d7\u3001\u519b\u65d7\u3001\u5fbd\u7ae0\u3001\u6b8b\u5965\u4f1a\u3001\u5317\u4eac\u51ac\u5965\u5409\u7965\u7269\u3001\u5317\u4eac\u51ac\u5965\u4f1a\u4f1a\u5fbd\u3001\u5965\u8fd0\u4e94\u73af\u3001\u6b8b\u8054\u3001\u7ea2\u5341\u5b57\u3001\u5171\u9752\u56e2\u3001\u5c11\u5148\u961f\u3001\u5de5\u4f1a\u3001\u5987\u8054\u3001\u515a\u653f\u519b\u5236\u670d\u3002\u4e00\u4e2a\u56fe\u7247\u53ef\u80fd\u6709\u591a\u4e2a\u6807\u7b7e\u5171\u5b58\uff0c\u8bf7\u7528\u9017\u53f7\u5206\u9694\u6bcf\u4e2a\u6807\u7b7e\u3002\u6ca1\u6709\u5219\u8fd4\u56de\u65e0\u3002\"},\r\n        ],\r\n    },\r\n]\r\nprocessor = AutoProcessor.from_pretrained(MODEL_PATH)\r\nprompt = processor.apply_chat_template(\r\n    messages,\r\n    tokenize=False,\r\n    add_generation_prompt=True,\r\n)\r\nimage_inputs, video_inputs = process_vision_info(messages)\r\n\r\nmm_data = {}\r\nif image_inputs is not None:\r\n    mm_data[\"image\"] = image_inputs\r\nif video_inputs is not None:\r\n    mm_data[\"video\"] = video_inputs\r\n\r\nllm_inputs = {\r\n    \"prompt\": prompt,\r\n    \"multi_modal_data\": mm_data,\r\n}\r\n\r\n_REPEAT = 100\r\n_bs = 32\r\nimport time\r\ntic = time.time()\r\nfor _ in range(_REPEAT):\r\n    outputs = llm.generate([llm_inputs]*_bs, sampling_params=sampling_params)\r\ntoc = time.time()\r\nprint(f'process {_REPEAT*_bs} query cost {toc-tic} seconds, QPS estimated:{_REPEAT*_bs/(toc-tic)}')\r\n```\r\n\r\n### Your current environment (if you think it is necessary)\r\n\r\n```text\r\nThe output of `python collect_env.py`\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: CentOS Linux 7 (Core) (x86_64)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)\r\nClang version: Could not collect\r\nCMake version: version 3.25.0-rc2\r\nLibc version: glibc-2.17\r\n\r\nPython version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0] (64-bit runtime)\r\nPython platform: Linux-4.18.0-147.mt20200626.413.el8_1.x86_64-x86_64-with-glibc2.17\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.99\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA L40\r\nNvidia driver version: 535.129.03\r\ncuDNN version: Probably one of the following:\r\n/usr/lib64/libcudnn.so.8.9.6\r\n/usr/lib64/libcudnn_adv_infer.so.8.9.6\r\n/usr/lib64/libcudnn_adv_train.so.8.9.6\r\n/usr/lib64/libcudnn_cnn_infer.so.8.9.6\r\n/usr/lib64/libcudnn_cnn_train.so.8.9.6\r\n/usr/lib64/libcudnn_ops_infer.so.8.9.6\r\n/usr/lib64/libcudnn_ops_train.so.8.9.6\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   52 bits physical, 57 bits virtual\r\nCPU(s):                          192\r\nOn-line CPU(s) list:             0-22\r\nOff-line CPU(s) list:            23-191\r\nThread(s) per core:              0\r\nCore(s) per socket:              48\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           143\r\nModel name:                      Intel(R) Xeon(R) Platinum 8468V\r\nStepping:                        8\r\nCPU MHz:                         2900.000\r\nCPU max MHz:                     3800.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        4800.00\r\nVirtualization:                  VT-x\r\nL1d cache:                       2.3 MiB\r\nL1i cache:                       1.5 MiB\r\nL2 cache:                        96 MiB\r\nL3 cache:                        97.5 MiB\r\nNUMA node0 CPU(s):               0-47,96-143\r\nNUMA node1 CPU(s):               48-95,144-191\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid cldemote movdiri movdir64b md_clear pconfig flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.2.65\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.99\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.99\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.99\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.0.44\r\n[pip3] nvidia-curand-cu12==10.3.5.119\r\n[pip3] nvidia-cusolver-cu12==11.6.0.99\r\n[pip3] nvidia-cusparse-cu12==12.3.0.142\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.99\r\n[pip3] nvidia-nvtx-cu12==12.4.99\r\n[pip3] pynvml==11.5.3\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0+cu124\r\n[pip3] torchaudio==2.4.0+cu124\r\n[pip3] torchvision==0.19.0+cu124\r\n[pip3] transformers==4.46.1\r\n[pip3] transformers-stream-generator==0.0.5\r\n[pip3] triton==3.0.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.4.2.65                pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.4.99                  pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.4.99                  pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.4.99                  pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.2.0.44                pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.5.119               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.6.0.99                pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.3.0.142               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.4.99                  pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.4.99                  pypi_0    pypi\r\n[conda] pynvml                    11.5.3                   pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.4.0+cu124              pypi_0    pypi\r\n[conda] torchvision               0.19.0+cu124             pypi_0    pypi\r\n[conda] transformers              4.46.1                   pypi_0    pypi\r\n[conda] transformers-stream-generator 0.0.5                    pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.3.post1\r\nvLLM Build Flags:\r\nCUDA Archs: ; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tNIC0\tNIC1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tSYS\tSYS\t0-22\t0\t\tN/A\r\nNIC0\tSYS\t X \tSYS\r\nNIC1\tSYS\tSYS\t X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_bond_0\r\n  NIC1: mlx5_bond_1\r\n```\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-04T13:29:35+00:00",
    "closed_at": "2025-03-06T02:02:32+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9992/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9992"
  },
  {
    "number": 6465,
    "title": "[Bug]: failed when run Qwen2-54B-A14B-GPTQ-Int4(MOE)",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Mar 22 2024, 16:50:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-91-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA RTX 6000 Ada Generation\r\nNvidia driver version: 545.23.08\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             128\r\nOn-line CPU(s) list:                0-127\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 9354 32-Core Processor\r\nCPU family:                         25\r\nModel:                              17\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 32\r\nSocket(s):                          2\r\nStepping:                           1\r\nFrequency boost:                    enabled\r\nCPU max MHz:                        3799.0720\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           6500.47\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov succor smca fsrm flush_l1d\r\nVirtualization:                     AMD-V\r\nL1d cache:                          2 MiB (64 instances)\r\nL1i cache:                          2 MiB (64 instances)\r\nL2 cache:                           64 MiB (64 instances)\r\nL3 cache:                           512 MiB (16 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-31,64-95\r\nNUMA node1 CPU(s):                  32-63,96-127\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; safe RET\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.1\r\n[pip3] torchvision==0.18.1\r\n[pip3] transformers==4.42.4\r\n[pip3] triton==2.3.1\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tSYS\t32-63,96-127\t1\t\tN/A\r\nNIC0\tSYS\t X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_bond_0\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\ngot the following error:\r\n\r\n```\r\n[rank0]:     self.layers = nn.ModuleList([\r\n[rank0]:   File \"/workspace/heliumos-env/lib/python3.10/site-packages/vllm/model_executor/models/qwen2_moe.py\", line 328, in <listcomp>\r\n[rank0]:     Qwen2MoeDecoderLayer(config,\r\n[rank0]:   File \"/workspace/heliumos-env/lib/python3.10/site-packages/vllm/model_executor/models/qwen2_moe.py\", line 268, in __init__\r\n[rank0]:     self.mlp = Qwen2MoeSparseMoeBlock(config=config,\r\n[rank0]:   File \"/workspace/heliumos-env/lib/python3.10/site-packages/vllm/model_executor/models/qwen2_moe.py\", line 103, in __init__\r\n[rank0]:     self.experts = FusedMoE(num_experts=config.num_experts,\r\n[rank0]:   File \"/workspace/heliumos-env/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 145, in __init__\r\n[rank0]:     assert self.quant_method is not None\r\n[rank0]: AssertionError\r\n```",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-16T07:35:14+00:00",
    "closed_at": "2025-03-01T02:05:58+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6465/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6465"
  },
  {
    "number": 1000,
    "title": "Inference server not working with models tuned on <|system|>,<|prompter|>,<|assistant|> or <|im_start|>,<|im_end|> format",
    "body": "Trying to run the vLLM server with https://huggingface.co/Open-Orca/LlongOrca-13B-16k but it returns just white space.\r\n\r\nIt uses messages formatted as:\r\n\r\n```\r\n<|im_start|>system\r\nYou are LlongOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\r\n<|im_end|>\r\n```\r\n\r\nAlso tried https://huggingface.co/OpenAssistant/llama2-13b-orca-8k-3319 but it returns empty.\r\nMessage format:\r\n```\r\n<|system|>system message</s><|prompter|>user prompt</s><|assistant|>\r\n```\r\n\r\nIs it possible to use models that require such different formatting? The vLLM request is abstracted away and only sends messages list. I tried wrapping the content with the special tokens.\r\n\r\nThe only prompt format that works for me on vLLM server is\r\n```\r\n### Instruction:\r\n<prompt>\r\n### Response:\r\n```\r\nfrom Open-Orca/OpenOrca-Platypus2-13B\r\n\r\nMaybe I am missing an argument when running the server?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-09-09T14:51:10+00:00",
    "closed_at": "2024-03-20T12:43:35+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1000/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1000"
  },
  {
    "number": 14394,
    "title": "[Bug]: vllm-0.7.3. gptq-int3 model cannot run.",
    "body": "### Your current environment\n\n<details>\n<summary>Error when running gptq-int3 model</summary>\n\n- python3.10\n- vllm==0.7.3\n- transformers==4.49.0\n- torch==2.5.1\n\n</details>\n\n\n\n\n### \ud83d\udc1b Describe the bug\n\ninfer code:\n```python\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n\nmodel_name = \"Xu-Ouyang/Qwen2-1.5B-int3-GPTQ-wikitext2\"\nmax_model_len, tp_size = 1024, 1\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True, enforce_eager=True)\nsampling_params = SamplingParams(temperature=0.3, max_tokens=128, stop_token_ids=[tokenizer.eos_token_id])\n\nmessages_list = [[{\"role\": \"user\", \"content\": \"Who are you?\"}],]\n\nprompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\n\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)\n```\n\n</details>\n\nerror:\n```text\n[rank0]:   File \"/media/zjin/Data/projects/tst/vllm/vllm/model_executor/layers/linear.py\", line 336, in __init__\n[rank0]:     self.quant_method.create_weights(\n[rank0]:   File \"/media/zjin/Data/projects/tst/vllm/vllm/model_executor/layers/quantization/gptq.py\", line 153, in create_weights\n[rank0]:     raise ValueError(\n[rank0]: ValueError: The output size is not aligned with the quantized weight shape. This can be caused by too large tensor parallel size.\n[rank0]:[W307 09:43:03.196837779 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n```\n\nfix:\nupdate - vllm/vllm/model_executor/layers/quantization/utils/gptq_utils.py\n```python\ndef override_config(config: QuantizationConfig, prefix: str):\n    weight_bits = get_dynamic_override(config, prefix, \"bits\",\n                                       config.weight_bits)\n    if isinstance(weight_bits, int):\n        config.weight_bits = weight_bits\n    group_size = get_dynamic_override(config, prefix, \"group_size\",\n                                      config.group_size)\n    if isinstance(group_size, int):\n        config.group_size = group_size\n    desc_act = get_dynamic_override(config, prefix, \"desc_act\",\n                                    config.desc_act)\n    if isinstance(desc_act, bool):\n        config.desc_act = desc_act\n    \n    # -------------------------------------------------del code---------------------------------------\n    #config.pack_factor = 32 // config.weight_bits  # packed into int32\n     # -------------------------------------------------add code---------------------------------------\n    config.pack_factor = Fraction(32, config.weight_bits)\n\n    if config.get_name() == \"gptq_marlin\":\n        is_sym = get_dynamic_override(config, prefix, \"sym\", config.is_sym)\n        if isinstance(is_sym, bool):\n            config.is_sym = is_sym\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-07T01:51:21+00:00",
    "closed_at": "2025-07-05T02:11:58+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14394/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14394"
  },
  {
    "number": 11686,
    "title": "[Bug]: vLLM is erroneously sending some information outputs into the error stream",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\n--2025-01-02 11:38:20--  https://raw.githubusercontent.com/vllm-project/vllm/main/collect_env.py\r\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\r\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 26218 (26K) [text/plain]\r\nSaving to: 'collect_env.py'\r\n\r\ncollect_env.py               100%[=============================================>]  25.60K  --.-KB/s    in 0.001s  \r\n\r\n2025-01-02 11:38:20 (34.2 MB/s) - 'collect_env.py' saved [26218/26218]\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-125-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.6.77\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 1: NVIDIA H100 80GB HBM3\r\nGPU 2: NVIDIA H100 80GB HBM3\r\nGPU 3: NVIDIA H100 80GB HBM3\r\nGPU 4: NVIDIA H100 80GB HBM3\r\nGPU 5: NVIDIA H100 80GB HBM3\r\nGPU 6: NVIDIA H100 80GB HBM3\r\nGPU 7: NVIDIA H100 80GB HBM3\r\n\r\nNvidia driver version: 560.35.03\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.5.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.5.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.5.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.5.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.5.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.5.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.5.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.5.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        46 bits physical, 57 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               192\r\nOn-line CPU(s) list:                  0-191\r\nVendor ID:                            GenuineIntel\r\nModel name:                           INTEL(R) XEON(R) PLATINUM 8558\r\nCPU family:                           6\r\nModel:                                207\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   48\r\nSocket(s):                            2\r\nStepping:                             2\r\nFrequency boost:                      enabled\r\nCPU max MHz:                          2101.0000\r\nCPU min MHz:                          800.0000\r\nBogoMIPS:                             4200.00\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                       VT-x\r\nL1d cache:                            4.5 MiB (96 instances)\r\nL1i cache:                            3 MiB (96 instances)\r\nL2 cache:                             192 MiB (96 instances)\r\nL3 cache:                             520 MiB (2 instances)\r\nNUMA node(s):                         4\r\nNUMA node0 CPU(s):                    0-23,96-119\r\nNUMA node1 CPU(s):                    24-47,120-143\r\nNUMA node2 CPU(s):                    48-71,144-167\r\nNUMA node3 CPU(s):                    72-95,168-191\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.47.1\r\n[pip3] triton==3.1.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.20.1                   pypi_0    pypi\r\n[conda] transformers              4.47.1                   pypi_0    pypi\r\n[conda] triton                    3.1.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.6.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6       NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    SYS     SYS     SYSSYS     SYS     0-23,96-119     0               N/A\r\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     NODE    SYS     SYS     SYSSYS     SYS     0-23,96-119     0               N/A\r\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    PIX     SYS     SYS     SYSSYS     SYS     0-23,96-119     0               N/A\r\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     PIX     SYS     SYSSYS     SYS     24-47,120-143   1               N/A\r\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     NODE       NODE    SYS     48-71,144-167   2               N/A\r\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE    PIXNODE    SYS     48-71,144-167   2               N/A\r\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE    NODE       PIX     SYS     48-71,144-167   2               N/A\r\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYSSYS     PIX     72-95,168-191   3               N/A\r\nNIC0    PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS      X      NODE    NODE    SYS     SYS     SYSSYS     SYS\r\nNIC1    NODE    PIX     NODE    SYS     SYS     SYS     SYS     SYS     NODE     X      NODE    SYS     SYS     SYSSYS     SYS\r\nNIC2    NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      SYS     SYS     SYSSYS     SYS\r\nNIC3    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYSSYS     SYS\r\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS      X      NODE       NODE    SYS\r\nNIC5    SYS     SYS     SYS     SYS     NODE    PIX     NODE    SYS     SYS     SYS     SYS     SYS     NODE     X NODE    SYS\r\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS     NODE    NODE        X      SYS\r\nNIC7    SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYSSYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n\r\nNVIDIA_VISIBLE_DEVICES=all\r\nCUBLAS_VERSION=12.6.3.3\r\nNVIDIA_REQUIRE_CUDA=cuda>=9.0\r\nCUDA_CACHE_DISABLE=1\r\nNCCL_VERSION=2.22.3\r\nNVIDIA_DRIVER_CAPABILITIES=compute,utility,video\r\nNVIDIA_PRODUCT_NAME=CUDA\r\nCUDA_VERSION=12.6.2.004\r\nCUDNN_FRONTEND_VERSION=1.7.0\r\nCUDNN_VERSION=9.5.0.50\r\nLD_LIBRARY_PATH=/root/miniconda3/envs/vllm/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/compat/lib.real:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nNVIDIA_BUILD_ID=114391310\r\nCUDA_DRIVER_VERSION=560.35.03\r\nNVIDIA_REQUIRE_JETPACK_HOST_MOUNTS=\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\n![image](https://github.com/user-attachments/assets/a8bc2180-37a3-49f2-a3e6-0b6185846d38)\r\n![image](https://github.com/user-attachments/assets/d93422a8-1da1-40d6-b96b-b75a2db65447)\r\nIn order to benchmarking of vLLM servers I am doing output redirection and parsing the output so I can tell when the server is ready to accept inference requests. It turns out, it is outputting parts of the output into the error stream for whatever reason. This tripped me up.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-02T11:42:49+00:00",
    "closed_at": "2025-05-09T02:45:25+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11686/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11686"
  },
  {
    "number": 8356,
    "title": "[Usage]: how to shutdown vllm server",
    "body": "### Your current environment\r\n\r\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\r\n\r\n### How would you like to use vllm\r\n\r\nif i use \r\n> vllm serve llm/qwen/Qwen2-0.5B-Instruct\r\n\r\nhow to shutdown vllm server use command?\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-11T06:34:38+00:00",
    "closed_at": "2025-01-11T01:59:46+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8356/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8356"
  },
  {
    "number": 4806,
    "title": "[Performance]: Qwen 7b chat model, under 128 concurrency, the CPU utilization rate is 100%, and the GPU SM utilization rate is only about 60%-75%. Is it a CPU bottleneck?",
    "body": "### Proposal to improve performance\r\n\r\n_No response_\r\n\r\n### Report of performance regression\r\n\r\n_No response_\r\n\r\n### Misc discussion on performance\r\n\r\nI am using vllm to deploy the qwen 7b chat model service. In a very high concurrency scenario, such as 128 concurrency, I found that the CPU utilization reached 100%, but I saw the GPU utilization rate is less than 60%\r\n\r\nMy question is, because a lot of vllm's scheduling and calculation logic is implemented by Python coroutines, it can only use the computing power of a single CPU. In a scenario like this with 128 concurrency, is the CPU becoming a computing bottleneck, causing GPU CUDA to be unable to achieve higher performance?\r\n\r\nModel download address\uff1ahttps://huggingface.co/Qwen/Qwen-7B-Chat/tree/main\r\n\r\n1. For sever scenario\r\n![image](https://github.com/vllm-project/vllm/assets/94596925/28d326a1-9d7a-437b-b503-6db4dc559a70)\r\n![image](https://github.com/vllm-project/vllm/assets/94596925/ccda7dcc-b793-4d18-a7bf-096e05be10ff)\r\n2. For offline batch inference scenario\r\n![image](https://github.com/vllm-project/vllm/assets/94596925/92f3e4cd-dce5-4952-89d2-e23622df6d55)\r\n![image](https://github.com/vllm-project/vllm/assets/94596925/37263e7f-3a20-4bbd-9485-b4d29f0c04a2)\r\n\r\n\r\n```python\r\nimport random\r\nimport json\r\nfrom vllm import LLM, SamplingParams\r\n\r\nconc = 128\r\njsonl_path = \"xxx.jsonl\"\r\n\r\n# \u4ecejsonl\u6587\u4ef6\u4e2d\u8bfb\u53d6concurrent\u6761\u6570\u636e\r\nall_prompts = []\r\nwith open(jsonl_path, \"r\") as f:\r\n    for line in f:\r\n        line_obj = json.loads(line)\r\n        print(\"line_obj as: \", line_obj)\r\n        try:\r\n            prompt = line_obj[-1][\"content\"]\r\n        except Exception as e:\r\n            prompt = line_obj[-1][\"Content\"]\r\n\r\n        all_prompts.append(prompt)\r\n\r\n# Sample prompts.\r\nif len(all_prompts) > conc:\r\n    prompts = all_prompts[:conc]\r\nelse:\r\n    prompts = random.choices(all_prompts, k=conc)\r\n\r\n# Create a sampling params object.\r\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=500)\r\n\r\n# Create an LLM.\r\n#llm = LLM(model=\"facebook/opt-125m\")\r\n# llama2 7b chat\r\nllm = LLM(model=\"/models/models--Qwen--Qwen-7B-Chat-new\", trust_remote_code=True)\r\n# Generate texts from the prompts. The output is a list of RequestOutput objects\r\n# that contain the prompt, generated text, and other information.\r\noutputs = llm.generate(prompts, sampling_params)\r\n# Print the outputs.\r\nfor output in outputs:\r\n    prompt = output.prompt\r\n    generated_text = output.outputs[0].text\r\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\r\n```\r\n\r\n\r\n### Your current environment (if you think it is necessary)\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.2.2+cu118\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.8\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 18.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.27\r\n\r\nPython version: 3.9.16 (main, May 15 2023, 23:46:34)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-3.10.0-1160.83.1.el7.x86_64-x86_64-with-glibc2.27\r\nIs CUDA available: True\r\nCUDA runtime version: 11.8.89\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-80GB\r\nNvidia driver version: 535.154.05\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.6.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.6.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.6.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.6.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.6.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.6.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.6.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              14\r\nOn-line CPU(s) list: 0-13\r\nThread(s) per core:  2\r\nCore(s) per socket:  7\r\nSocket(s):           1\r\nNUMA node(s):        1\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               106\r\nModel name:          Intel(R) Xeon(R) Platinum 8350C CPU @ 2.60GHz\r\nStepping:            6\r\nCPU MHz:             2593.904\r\nBogoMIPS:            5187.80\r\nHypervisor vendor:   KVM\r\nVirtualization type: full\r\nL1d cache:           48K\r\nL1i cache:           32K\r\nL2 cache:            1280K\r\nL3 cache:            49152K\r\nNUMA node0 CPU(s):   0-13\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 arat avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq spec_ctrl\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu11==2.19.3\r\n[pip3] nvidia-nccl-cu12==2.19.3\r\n[pip3] torch==2.2.2+cu118\r\n[pip3] triton==2.2.0\r\n[pip3] vllm-nccl-cu11==2.18.1.0.4.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu11          2.19.3                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.19.3                   pypi_0    pypi\r\n[conda] torch                     2.2.2+cu118              pypi_0    pypi\r\n[conda] triton                    2.2.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu11            2.18.1.0.4.0             pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tSYS\t0-13\t0\t\tN/A\r\nNIC0\tSYS\t X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n```\r\n",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-14T07:48:00+00:00",
    "closed_at": "2024-11-27T02:08:34+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4806/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4806"
  },
  {
    "number": 11560,
    "title": "[Usage]:  torch.OutOfMemoryError: CUDA out of memory.",
    "body": "### Your current environment\r\n\r\n```\r\nI  get this error when load \"Qwen2-VL-72B-Instruct\"\r\n\r\nHere is the detail of error :\r\n```\r\n\r\n```\r\nINFO 12-27 08:52:17 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\r\nINFO 12-27 08:52:17 llm_engine.py:249] Initializing an LLM engine (v0.6.4) with config: model='/data/fffan/model/Qwen2-VL-72B-Instruct', speculative_config=None, tokenizer='/data/fffan/model/Qwen2-VL-72B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/data/fffan/model/Qwen2-VL-72B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\r\nINFO 12-27 08:52:17 selector.py:135] Using Flash Attention backend.\r\nINFO 12-27 08:52:17 model_runner.py:1072] Starting to load model /data/fffan/model/Qwen2-VL-72B-Instruct...\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/data/fffan/test_file.py\", line 121, in <module>\r\n[rank0]:     main()\r\n[rank0]:   File \"/data/fffan/test_file.py\", line 13, in main\r\n[rank0]:     llm = LLM('/data/fffan/model/Qwen2-VL-72B-Instruct')\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/utils.py\", line 1027, in inner\r\n[rank0]:     return fn(*args, **kwargs)\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 210, in __init__\r\n[rank0]:     self.llm_engine = self.engine_class.from_engine_args(\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 585, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 347, in __init__\r\n[rank0]:     self.model_executor = executor_class(vllm_config=vllm_config, )\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 36, in __init__\r\n[rank0]:     self._init_executor()\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/executor/gpu_executor.py\", line 40, in _init_executor\r\n[rank0]:     self.driver_worker.load_model()\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/worker/worker.py\", line 152, in load_model\r\n[rank0]:     self.model_runner.load_model()\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1074, in load_model\r\n[rank0]:     self.model = get_model(vllm_config=self.vllm_config)\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 12, in get_model\r\n[rank0]:     return loader.load_model(vllm_config=vllm_config)\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 316, in load_model\r\n[rank0]:     model = _initialize_model(vllm_config=vllm_config)\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 108, in _initialize_model\r\n[rank0]:     return model_class(vllm_config=vllm_config, prefix=prefix)\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/model_executor/models/qwen2_vl.py\", line 1087, in __init__\r\n[rank0]:     self.model = Qwen2Model(vllm_config=vllm_config,\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 126, in __init__\r\n[rank0]:     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 279, in __init__\r\n[rank0]:     self.start_layer, self.end_layer, self.layers = make_layers(\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 509, in make_layers\r\n[rank0]:     [PPMissingLayer() for _ in range(start_layer)] + [\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 510, in <listcomp>\r\n[rank0]:     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 281, in <lambda>\r\n[rank0]:     lambda prefix: Qwen2DecoderLayer(config=config,\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 201, in __init__\r\n[rank0]:     self.mlp = Qwen2MLP(\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 76, in __init__\r\n[rank0]:     self.down_proj = RowParallelLinear(\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 995, in __init__\r\n[rank0]:     self.quant_method.create_weights(\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 122, in create_weights\r\n[rank0]:     weight = Parameter(torch.empty(sum(output_partition_sizes),\r\n[rank0]:   File \"/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/torch/utils/_device.py\", line 106, in __torch_function__\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 278.75 MiB is free. Including non-PyTorch memory, this process has 78.97 GiB memory in use. Of the allocated memory 78.38 GiB is allocated by PyTorch, and 115.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\n[rank0]:[W1227 08:52:18.060131299 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\r\n```\r\n\r\n\r\n### How would you like to use vllm\r\n\r\n```\r\nfrom vllm import LLM\r\n\r\nllm = LLM('/data/fffan/model/Qwen2-VL-72B-Instruct')\r\n```\r\n\r\n\r\n**vllm:  0.6.6\r\nGPU:  8 * A800**\r\n\r\nI try many version of vllm (0.6.5,   0.6.4,  ....).\r\nand I also use `gpu_memory_utilization`\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-12-27T08:57:15+00:00",
    "closed_at": "2025-03-07T13:48:29+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11560/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11560"
  },
  {
    "number": 3861,
    "title": "[Roadmap] vLLM Roadmap Q2 2024",
    "body": "This document includes the features in vLLM's roadmap for Q2 2024. Please feel free to discuss and contribute to the specific features at related RFC/Issues/PRs and add anything else you'd like to talk about in this issue.\r\n\r\nYou can see our historical roadmap at #2681, #244. This roadmap contains work committed by the vLLM team from UC Berkeley, as well as the broader vLLM contributor groups including but not limited to Anyscale, IBM, NeuralMagic, Roblox, Oracle Cloud. You can also find help wanted items in this roadmap as well! Additionally, this roadmap is shaped by you, our user community!\r\n\r\n### Themes. \r\n\r\nWe categorized our roadmap into 6 broad themes:\r\n\r\n* **Broad model support**: vLLM should support a wide range of transformer based models. It should be kept up to date as much as possible. This includes new auto-regressive decoder models, encoder-decoder models, hybrid architectures, and models supporting multi-modal inputs. \r\n* **Excellent hardware coverage**: vLLM should run on a wide range of accelerators for production AI workload. This includes GPUs, tensor accelerators, and CPUs. We will work closely with hardware vendors to ensure vLLM utilizes the greatest performance out of the chip. \r\n* **Performance optimization**:vLLM should be kept up to date with the latest performance optimization techniques. Users of vLLM can trust its performance to be competitive and strong.\r\n* **Production level engine**: vLLM should be the go-to choice for production level serving engine with a suite of features bridging the gaps from single forward pass to 24/7 service. \r\n* **Strong OSS product**: vLLM is and will be a true community project. We want it to be a healthy project with regular release cadence, good documentation, and adding new reviewers to the codebase.\r\n* **Extensible architectures**: For vLLM to grow at an even faster pace, it needs good abstractions to support a wide range of scheduling policies, hardware backends, and inference optimizations. We will work on refactoring the codebase to support that.\r\n\r\n\r\n### Broad Model Support\r\n- [ ] Encoder Decoder Models\r\n  - [ ] T5 #3117\r\n  - [ ] Whisper\r\n  - [x] Embedding #3187 \r\n- [ ] Hybrid Architecture (Jamba) #3690 \r\n- [x] Decoder Only Embedding Models #3734\r\n- [ ] Prefix tuning support\r\n\r\nHelp Wanted:\r\n- [x] More vision transformers beyond llava\r\n- [x] Support private model registration #172 \r\n- [ ] Control vector support #3451 \r\n- [ ] Fallback support for arbitrary `transformers` text generation model\r\n- [ ] Long context investigation of LongRoPE\r\n- [ ] RWKV\r\n\r\n### Excellent Hardware Coverage\r\n- [x] AMD MI300x: enhancing fp8 performance [enable FP8 compute]\r\n- [x] NVIDIA H100: enhancing fp8 performance\r\n- [x] AWS Trainium and Inferentia\r\n- [x] Google TPU\r\n- [x] Intel CPU\r\n- [x] Intel GPU\r\n- [x] Intel Gaudi\r\n\r\n### Performance Optimization\r\n* Speculative decoding\r\n  - [x] Speculative decoding framework for top-1 proposals w/draft model\r\n  - [x] Proposer improvement: Prompt-lookup n-gram speculations\r\n  - [ ] Scoring improvement: Make batch expansion optional\r\n  - [ ] Scoring improvement: dynamic scoring length policy\r\n* Kernels:\r\n  - [x] FlashInfer integration #2767 \r\n  - [ ] Sampler optimizations leveraging triton compiler\r\n* Quantization:\r\n  - [x] FP8 format support for NVIDIA Ammo and AMD Quantizer\r\n  - [x] Weight only quantization (Marlin) improvements: act_order, int8, Exllama2 compatibility, fused MoE, AWQ kernels.\r\n  - [x] Activation quantization (W8A8, FP8, etc)\r\n  - [x] Quantized lora support #3225\r\n  - [x] AQLM quantization\r\n- [ ] Constrained decoding performance (batch, async, acceleration) and extensibility (Outlines #3715, LMFormatEnforcer #3713, AICI #2888 ) \r\n\r\n\r\nHelp Wanted:\r\n* Sparse kv cache (H2O, compression, FastDecode)\r\n* Speculative decoding\r\n  - [ ] Proposer/scoring/verifier improvement: Top-k \u201ctree attention\u201d proposals for Eagle/Medusa/Draft model\r\n  - [x] Proposer improvement: RAG n-gram speculations\r\n  - [ ] Proposer improvement: Eagle/Medusa top-1 proposals\r\n  - [ ] Proposer improvement: Quantized draft models\r\n  - [ ] Verifier improvement: Typical acceptance\r\n\r\n### Production Level Engine\r\n* Scheduling\r\n  - [x] Prototype Disaggregated prefill (#2370)\r\n  - [x] Speculative decoding fully merged in (#2188)\r\n  - [x] Turn chunked prefill/sarathi/splitfuse on by default (#3538)\r\n* Memory management\r\n  - [x] Automatic prefix caching enhancement\r\n- [x] TGI feature parity (stop string handling, logging and  metrics, test improvements)\r\n- [x] Provide non-ray option for single node inference\r\n- [ ] Optimize api server performance\r\n- [ ] OpenAI server feature completeness (function calling) (#3237)\r\n- Model Loading\r\n  - [x] Optimize model weights loading by directly loading from hub/s3 #3533 \r\n  - [x] Fully offline mode\r\n\r\nHelp Wanted:\r\n- [ ] Logging serving FLOPs for performance analysis \r\n- [ ] Dynamic LoRA adapter downloads from hub/S3\r\n\r\n### Strong OSS Product\r\n- [x] Continuous benchmarks (resource needed!)\r\n- [x] Commit to 2wk release cadence\r\n- [x] Growing reviewer and committer base\r\n* Better docs\r\n  - [x] doc: memory and performance tuning guide\r\n  - [x] doc: apc documentation\r\n  - [ ] doc: hardware support levels, feature matrix, and policies\r\n  - [ ] doc: guide to horizontally scale up vLLM service\r\n  - [ ] doc: developer guide for adding new draft based models or draft-less optimizations\r\n- [ ] Automatic CD of nightly wheels and docker images\r\n\r\nHelp Wanted:\r\n* ARM aarch-64 support for AWS Graviton based instances and GH200\r\n* Full correctness test with HuggingFace transformers. Resources needed.\r\n* Well tested support for `lm-eval-harness` (logprobs, get tokenizers)\r\n* Local development workflow without cuda\r\n\r\n### Extensible Architecture \r\n\r\n- [x] Prototype pipeline parallelism\r\n- [x] Extensible memory manager\r\n- [ ] Extensible scheduler\r\n- [ ] `torch.compile` investigations\r\n  - [x] use compile for quantization kernel fusion\r\n  - [ ] use compile for future proofing graph mode\r\n  - [ ] use compile for xpu or other accelerators\r\n- Architecture for queue management and request prioritization\r\n- Streaming LLM, prototype it on new block manager\r\n- Investigate Tensor + Pipeline parallelism ([LIGER](https://dl.acm.org/doi/pdf/10.1145/3627535.3638466))\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-04-04T22:38:01+00:00",
    "closed_at": "2024-06-25T00:08:31+00:00",
    "comments": 39,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3861/reactions",
      "total_count": 87,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 52,
      "confused": 0,
      "heart": 20,
      "rocket": 15,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3861"
  },
  {
    "number": 7125,
    "title": "[Feature]: How to run the int4 quantized version of the gemma2-27b model",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nHow to run the int4 quantized version of the gemma2-27b model\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-04T13:15:30+00:00",
    "closed_at": "2024-12-08T02:10:51+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7125/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7125"
  },
  {
    "number": 3771,
    "title": "[Bug]: docker \u542f\u52a8vllm,\u914d\u7f6e\u4e86host_IP \uff0c\u8fd8\u662f [W socket.cpp:663] [c10d] The client socket has failed to connect to [::ffff:172.16.8.232]:39623 (errno: 110 - Connection timed out)",
    "body": "### Your current environment\n\nCollecting environment information...\r\nPyTorch version: 1.12.1+cu113\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.3\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.8.16 (default, Mar  2 2023, 03:21:46)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-101-generic-x86_64-with-glibc2.17\r\nIs CUDA available: True\r\nCUDA runtime version: 12.3.107\r\nCUDA_MODULE_LOADING set to: \r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090\r\nNvidia driver version: 550.54.14\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn.so.8.8.0\r\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.8.0\r\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.8.0\r\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.8.0\r\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.8.0\r\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.8.0\r\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.8.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\n\u67b6\u6784\uff1a                              x86_64\r\nCPU \u8fd0\u884c\u6a21\u5f0f\uff1a                      32-bit, 64-bit\r\n\u5b57\u8282\u5e8f\uff1a                            Little Endian\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nCPU:                                32\r\n\u5728\u7ebf CPU \u5217\u8868\uff1a                     0-31\r\n\u6bcf\u4e2a\u6838\u7684\u7ebf\u7a0b\u6570\uff1a                    2\r\n\u6bcf\u4e2a\u5ea7\u7684\u6838\u6570\uff1a                      16\r\n\u5ea7\uff1a                                1\r\nNUMA \u8282\u70b9\uff1a                         1\r\n\u5382\u5546 ID\uff1a                           AuthenticAMD\r\nCPU \u7cfb\u5217\uff1a                          25\r\n\u578b\u53f7\uff1a                              33\r\n\u578b\u53f7\u540d\u79f0\uff1a                          AMD Ryzen 9 5950X 16-Core Processor\r\n\u6b65\u8fdb\uff1a                              0\r\nFrequency boost:                    enabled\r\nCPU MHz\uff1a                           2200.000\r\nCPU \u6700\u5927 MHz\uff1a                      3400.0000\r\nCPU \u6700\u5c0f MHz\uff1a                      2200.0000\r\nBogoMIPS\uff1a                          6800.07\r\n\u865a\u62df\u5316\uff1a                            AMD-V\r\nL1d \u7f13\u5b58\uff1a                          512 KiB\r\nL1i \u7f13\u5b58\uff1a                          512 KiB\r\nL2 \u7f13\u5b58\uff1a                           8 MiB\r\nL3 \u7f13\u5b58\uff1a                           64 MiB\r\nNUMA \u8282\u70b90 CPU\uff1a                    0-31\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; safe RET, no microcode\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\u6807\u8bb0\uff1a                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.24.4\r\n[pip3] pytorch-fid==0.3.0\r\n[pip3] pytorch-lightning==1.5.9\r\n[pip3] torch==1.12.1+cu113\r\n[pip3] torch-fidelity==0.3.0\r\n[pip3] torchmetrics==0.6.0\r\n[pip3] torchvision==0.13.1+cu113\r\n[pip3] triton==2.2.0\r\n[conda] No relevant packagesROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-31    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\n\n### \ud83d\udc1b Describe the bug\n\nversion: '3.9'\r\nservices:\r\n  vllm:\r\n    image: vllm/vllm-openai:latest\r\n    container_name: qwen1.5\r\n    ulimits:\r\n      stack: 67108864\r\n      memlock: -1\r\n    environment:\r\n      - TZ=Asia/Shanghai\r\n      - CUDA_VISIBLE_DEVICES=0\r\n      - HOST_IP=172.16.8.232\r\n        #- HOST_IP=fe80::c366:3233:9937:a509\r\n    restart: always\r\n    shm_size: '10.24gb'\r\n    ipc: host\r\n    command:\r\n      --model /Qwen1.5-14B-Chat-GPTQ-Int4\r\n      --tensor-parallel-size 4\r\n      --host 0.0.0.0\r\n      --port 8009\r\n      --dtype float16\r\n      --quantization gptq\r\n      --served-model-name qwen\r\n      --gpu-memory-utilization=0.95\r\n      --trust-remote-code\r\n      --max-model-len 10240\r\n      --enforce-eager\r\n      --tensor-parallel-size 1\r\nvolumes:\r\n     #- /data/models/OrionStarAI/Orion-14B-Chat-Int4:/Orion-14B-Chat-Int4\r\n      - /data/llmservice/models/Qwen1.5-14B-Chat-GPTQ-Int4:/Qwen1.5-14B-Chat-GPTQ-Int4\r\n    ports:\r\n      - \"8009:8009\"\r\n    deploy:\r\n      resources:\r\n        reservations:\r\n          devices:\r\n            - driver: nvidia\r\n              device_ids: ['0']\r\n              capabilities: [gpu]\r\n\r\n\u542f\u52a8\u540e\u4e00\u76f4\u5361\u4f4f\uff1a\r\n",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-01T08:57:27+00:00",
    "closed_at": "2024-11-28T02:07:11+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3771/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3771"
  },
  {
    "number": 6099,
    "title": "[Bug]: enable_prefix_caching cause a triron crash",
    "body": "### Your current environment\r\n\r\n`Collecting environment information...\r\nPyTorch version: 2.3.0+cu118\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.8\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: CentOS Linux 7 (Core) (x86_64)\r\nGCC version: (GCC) 11.2.1 20220127 (Red Hat 11.2.1-9)\r\nClang version: Could not collect\r\nCMake version: version 3.29.3\r\nLibc version: glibc-2.17\r\n\r\nPython version: 3.9.16 (main, Jul 10 2023, 11:13:07)  [GCC 8.3.1 20190311 (Red Hat 8.3.1-3)] (64-bit runtime)\r\nPython platform: Linux-4.18.0-147.20200626.413.el8_1.x86_64-x86_64-with-glibc2.17\r\nIs CUDA available: True\r\nCUDA runtime version: 11.8.89\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nGPU 1: NVIDIA A100-SXM4-80GB\r\n\r\nNvidia driver version: 470.103.01\r\ncuDNN version: Probably one of the following:\r\n/usr/lib64/libcudnn.so.8.9.2\r\n/usr/lib64/libcudnn_adv_infer.so.8.9.2\r\n/usr/lib64/libcudnn_adv_train.so.8.9.2\r\n/usr/lib64/libcudnn_cnn_infer.so.8.9.2\r\n/usr/lib64/libcudnn_cnn_train.so.8.9.2\r\n/usr/lib64/libcudnn_ops_infer.so.8.9.2\r\n/usr/lib64/libcudnn_ops_train.so.8.9.2\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   43 bits physical, 48 bits virtual\r\nCPU(s):                          192\r\nOn-line CPU(s) list:             0-45\r\nOff-line CPU(s) list:            46-191\r\nThread(s) per core:              0\r\nCore(s) per socket:              48\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       AuthenticAMD\r\nCPU family:                      23\r\nModel:                           49\r\nModel name:                      AMD EPYC 7642 48-Core Processor\r\nStepping:                        0\r\nFrequency boost:                 enabled\r\nCPU MHz:                         3291.355\r\nCPU max MHz:                     2300.0000\r\nCPU min MHz:                     1500.0000\r\nBogoMIPS:                        4591.38\r\nVirtualization:                  AMD-V\r\nL1d cache:                       1.5 MiB\r\nL1i cache:                       1.5 MiB\r\nL2 cache:                        24 MiB\r\nL3 cache:                        256 MiB\r\nNUMA node0 CPU(s):               0-47,96-143\r\nNUMA node1 CPU(s):               48-95,144-191\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate sme ssbd mba sev ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip rdpid overflow_recov succor smca\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==0.4.3\r\n[pip3] numpy==1.25.0\r\n[pip3] nvidia-nccl-cu11==2.20.5\r\n[pip3] onnx==1.12.0\r\n[pip3] onnx-graphsurgeon==0.3.12\r\n[pip3] onnxruntime==1.15.1\r\n[pip3] torch==2.3.0+cu118\r\n[pip3] torchvision==0.14.1\r\n[pip3] transformers==4.41.1\r\n[pip3] triton==2.3.0\r\n[pip3] vllm-nccl-cu11==2.18.1.0.4.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.2\r\nvLLM Build Flags:\r\nCUDA Archs: ; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tmlx5_0\tmlx5_1\tmlx5_2\tmlx5_3\tmlx5_4\tmlx5_5\tmlx5_6\tmlx5_7\tCPU Affinity\tNUMA Affinity\r\nGPU0\t X \tNV12\tPXB\tPXB\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t0-47,96-143\t0\r\nGPU1\tNV12\t X \tPXB\tPXB\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t0-47,96-143\t0\r\nmlx5_0\tPXB\tPXB\t X \tPIX\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\r\nmlx5_1\tPXB\tPXB\tPIX\t X \tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\r\nmlx5_2\tNODE\tNODE\tNODE\tNODE\t X \tPIX\tSYS\tSYS\tSYS\tSYS\r\nmlx5_3\tNODE\tNODE\tNODE\tNODE\tPIX\t X \tSYS\tSYS\tSYS\tSYS\r\nmlx5_4\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPIX\tNODE\tNODE\r\nmlx5_5\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPIX\t X \tNODE\tNODE\r\nmlx5_6\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\t X \tPIX\r\nmlx5_7\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPIX\t X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\n#### our model is llama-33B when we try to add enable_prefix_caching=True in  LLM(**kwargs) and batch_size = 100 it will cause error which is never happend when with out enable_prefix_caching.\r\n\r\n\r\nERROR 07-02 16:38:17 worker_base.py:145] Error executing method execute_model. This might cause deadlock in distributed execution.\r\nERROR 07-02 16:38:17 worker_base.py:145] Traceback (most recent call last):\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/vllm-0.4.2+cu118-py3.9-linux-x86_64.egg/vllm/worker/worker_base.py\", line 137, in execute_method\r\nERROR 07-02 16:38:17 worker_base.py:145]     return executor(*args, **kwargs)\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 07-02 16:38:17 worker_base.py:145]     return func(*args, **kwargs)\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/vllm-0.4.2+cu118-py3.9-linux-x86_64.egg/vllm/worker/worker.py\", line 262, in execute_model\r\nERROR 07-02 16:38:17 worker_base.py:145]     output = self.model_runner.execute_model(seq_group_metadata_list,\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 07-02 16:38:17 worker_base.py:145]     return func(*args, **kwargs)\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/vllm-0.4.2+cu118-py3.9-linux-x86_64.egg/vllm/worker/model_runner.py\", line 793, in execute_model\r\nERROR 07-02 16:38:17 worker_base.py:145]     hidden_states = model_executable(**execute_model_kwargs)\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 07-02 16:38:17 worker_base.py:145]     return self._call_impl(*args, **kwargs)\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 07-02 16:38:17 worker_base.py:145]     return forward_call(*args, **kwargs)\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/vllm-0.4.2+cu118-py3.9-linux-x86_64.egg/vllm/model_executor/models/llama.py\", line 364, in forward\r\nERROR 07-02 16:38:17 worker_base.py:145]     hidden_states = self.model(input_ids, positions, kv_caches,\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 07-02 16:38:17 worker_base.py:145]     return self._call_impl(*args, **kwargs)\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 07-02 16:38:17 worker_base.py:145]     return forward_call(*args, **kwargs)\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/vllm-0.4.2+cu118-py3.9-linux-x86_64.egg/vllm/model_executor/models/llama.py\", line 291, in forward\r\nERROR 07-02 16:38:17 worker_base.py:145]     hidden_states, residual = layer(\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 07-02 16:38:17 worker_base.py:145]     return self._call_impl(*args, **kwargs)\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 07-02 16:38:17 worker_base.py:145]     return forward_call(*args, **kwargs)\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/vllm-0.4.2+cu118-py3.9-linux-x86_64.egg/vllm/model_executor/models/llama.py\", line 233, in forward\r\nERROR 07-02 16:38:17 worker_base.py:145]     hidden_states = self.self_attn(\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 07-02 16:38:17 worker_base.py:145]     return self._call_impl(*args, **kwargs)\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 07-02 16:38:17 worker_base.py:145]     return forward_call(*args, **kwargs)\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/vllm-0.4.2+cu118-py3.9-linux-x86_64.egg/vllm/model_executor/models/llama.py\", line 167, in forward\r\nERROR 07-02 16:38:17 worker_base.py:145]     attn_output = self.attn(q, k, v, kv_cache, attn_metadata,\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 07-02 16:38:17 worker_base.py:145]     return self._call_impl(*args, **kwargs)\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 07-02 16:38:17 worker_base.py:145]     return forward_call(*args, **kwargs)\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/vllm-0.4.2+cu118-py3.9-linux-x86_64.egg/vllm/attention/layer.py\", line 48, in forward\r\nERROR 07-02 16:38:17 worker_base.py:145]     return self.impl.forward(query, key, value, kv_cache, attn_metadata,\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/vllm-0.4.2+cu118-py3.9-linux-x86_64.egg/vllm/attention/backends/xformers.py\", line 240, in forward\r\nERROR 07-02 16:38:17 worker_base.py:145]     out = PagedAttention.forward_prefix(\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/vllm-0.4.2+cu118-py3.9-linux-x86_64.egg/vllm/attention/ops/paged_attn.py\", line 177, in forward_prefix\r\nERROR 07-02 16:38:17 worker_base.py:145]     context_attention_fwd(\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 07-02 16:38:17 worker_base.py:145]     return func(*args, **kwargs)\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/vllm-0.4.2+cu118-py3.9-linux-x86_64.egg/vllm/attention/ops/prefix_prefill.py\", line 753, in context_attention_fwd\r\nERROR 07-02 16:38:17 worker_base.py:145]     _fwd_kernel[grid](\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/triton/runtime/jit.py\", line 167, in <lambda>\r\nERROR 07-02 16:38:17 worker_base.py:145]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/triton/runtime/jit.py\", line 425, in run\r\nERROR 07-02 16:38:17 worker_base.py:145]     kernel.run(grid_0, grid_1, grid_2, kernel.num_warps, kernel.num_ctas,  # number of warps/ctas per instance\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 255, in __getattribute__\r\nERROR 07-02 16:38:17 worker_base.py:145]     self._init_handles()\r\nERROR 07-02 16:38:17 worker_base.py:145]   File \"/usr/local/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 250, in _init_handles\r\nERROR 07-02 16:38:17 worker_base.py:145]     self.module, self.function, self.n_regs, self.n_spills = driver.utils.load_binary(\r\nERROR 07-02 16:38:17 worker_base.py:145] RuntimeError: Triton Error [CUDA]: device kernel image is invalid",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-07-03T09:10:43+00:00",
    "closed_at": "2024-08-12T09:47:09+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6099/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6099"
  },
  {
    "number": 4995,
    "title": "[Feature]: Chunked prefill + lora",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently lora doesn't work with chunked prefill because some of lora index logic doesn't cover the case where sampling is not required. This also means lora is not working with sampling_params do_sample=True. \r\n\r\nWe need to add test cases for these. WIP https://github.com/vllm-project/vllm/pull/4994\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-23T01:12:17+00:00",
    "closed_at": "2025-04-02T02:06:40+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4995/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4995"
  },
  {
    "number": 596,
    "title": "ValueError: The number of GPUs per node is not divisible by the number of tensor parallelism.",
    "body": "I have 3 GPUs (3x3090). When I try to load `LLaMA-2-13B` and set the `tensor_parallel_size` to 2 it gives me this error. When I set it to 3 error follows like `ValueError: Total number of attention heads (40) must be divisible by tensor parallel size (3).`",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-07-27T07:11:53+00:00",
    "closed_at": "2024-03-25T10:57:02+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/596/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/596"
  },
  {
    "number": 11705,
    "title": "[Bug]: Extremely slow inference speed when deploying with vLLM on 16 H100 GPUs according to instructions on DeepSeekV3",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.22.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.10.134-008.7.kangaroo.al8.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA L20Z\r\nGPU 1: NVIDIA L20Z\r\nGPU 2: NVIDIA L20Z\r\nGPU 3: NVIDIA L20Z\r\nGPU 4: NVIDIA L20Z\r\nGPU 5: NVIDIA L20Z\r\nGPU 6: NVIDIA L20Z\r\nGPU 7: NVIDIA L20Z\r\n\r\nNvidia driver version: 535.161.08\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   52 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          130\r\nOn-line CPU(s) list:             0-129\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Processor\r\nCPU family:                      6\r\nModel:                           143\r\nThread(s) per core:              1\r\nCore(s) per socket:              130\r\nSocket(s):                       1\r\nStepping:                        8\r\nBogoMIPS:                        4000.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd avx512vbmi umip pku waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk avx512_fp16 arch_capabilities\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       6.1 MiB (130 instances)\r\nL1i cache:                       4.1 MiB (130 instances)\r\nL2 cache:                        260 MiB (130 instances)\r\nL3 cache:                        105 MiB (1 instance)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-64\r\nNUMA node1 CPU(s):               65-129\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Vulnerable\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.45.2\r\n[pip3] triton==3.1.0\r\n[conda] numpy                     1.26.3                   pypi_0    pypi\r\n[conda] torch                     2.5.0+cpu                pypi_0    pypi\r\n[conda] torchmetrics              1.0.3                    pypi_0    pypi\r\n[conda] torchrec                  1.0.0+cpu                pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.6.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\n\u001b[4mGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tNIC4\tNIC5\tNIC6\tNIC7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\r\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t0-129\t0-1\t\tN/A\r\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t0-129\t0-1\t\tN/A\r\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t0-129\t0-1\t\tN/A\r\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t0-129\t0-1\t\tN/A\r\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t0-129\t0-1\t\tN/A\r\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t0-129\t0-1\t\tN/A\r\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t0-129\t0-1\t\tN/A\r\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t0-129\t0-1\t\tN/A\r\nNIC0\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t X \tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t\t\t\t\r\nNIC1\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t X \tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t\t\t\t\r\nNIC2\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t X \tPHB\tPHB\tPHB\tPHB\tPHB\t\t\t\t\r\nNIC3\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t X \tPHB\tPHB\tPHB\tPHB\t\t\t\t\r\nNIC4\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t X \tPHB\tPHB\tPHB\t\t\t\t\r\nNIC5\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t X \tPHB\tPHB\t\t\t\t\r\nNIC6\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t X \tPHB\t\t\t\t\r\nNIC7\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n\r\nNVIDIA_VISIBLE_DEVICES=all\r\nNCCL_IB_TC=16\r\nNCCL_MIN_NCHANNELS=4\r\nNCCL_NET_PLUGIN=none\r\nNCCL_SOCKET_IFNAME=eth\r\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\r\nNCCL_DEBUG=INFO\r\nNCCL_IB_HCA=mlx5\r\nNCCL_IB_GID_INDEX=3\r\nNCCL_IB_QPS_PER_CONNECTION=8\r\nNCCL_IB_TIMEOUT=22\r\nNCCL_IB_SL=5\r\nLD_LIBRARY_PATH=/cpfs/user/chenge/miniconda3/envs/vllm/lib/python3.11/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nCUDA_MODULE_LOADING=LAZY\r\n\r\n\r\n\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nI'm deploying the model using the following command:\r\n```\r\nvllm serve local_deepseekv3_path --trust-remote-code --tensor-parallel-size 8 --pipeline-parallel-size 2 --model-max-len 16384 --served-model-name deepseek-v3 deepseek\r\n```\r\nI'm using the official Ray example, and NCCL is enabled. After launching the model with the above command, the inference speed is extremely slow.\r\nThe inference speed is almost 5 times slower than an unquantized Qwen-72B model.\r\n\r\nINFO: 10.39.129.93:36766 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\r\nINFO 01-02 16:16:48 async_llm_engine.py:211] Added request chatcmpl-bc1d5239d4c743aabedf1249038b99da.\r\nINFO 01-02 16:16:56 metrics.py:467] Avg prompt throughput: 1.9 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\r\nINFO 01-02 16:17:02 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.INFO 01-02 16:17:07 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\r\nINFO 01-02 16:17:10 async_llm_engine.py:179] Finished request chatcmpl-bc1d5239d4c743aabedf1249038b99da.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-03T05:25:02+00:00",
    "closed_at": "2025-06-11T02:14:13+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11705/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11705"
  },
  {
    "number": 4655,
    "title": "[Bug]: Ray on multi machine cluster fails to detect all nodes.",
    "body": "### Your current environment\n\n```text\r\npython collect_env.py\r\n--2024-05-07 16:14:33--  https://raw.githubusercontent.com/vllm-project/vllm/main/collect_env.py\r\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\r\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 24877 (24K) [text/plain]\r\nSaving to: \u2018collect_env.py\u2019\r\n\r\ncollect_env.py                                                                     100%[================================================================================================================================================================================================================>]  24.29K  --.-KB/s    in 0.003s  \r\n\r\n2024-05-07 16:14:33 (9.38 MB/s) - \u2018collect_env.py\u2019 saved [24877/24877]\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-102-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100 80GB PCIe\r\nGPU 1: NVIDIA A100 80GB PCIe\r\nGPU 2: NVIDIA A100 80GB PCIe\r\nGPU 3: NVIDIA A100 80GB PCIe\r\n\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             32\r\nOn-line CPU(s) list:                0-31\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 73F3 16-Core Processor\r\nCPU family:                         25\r\nModel:                              1\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 16\r\nSocket(s):                          1\r\nStepping:                           1\r\nFrequency boost:                    enabled\r\nCPU max MHz:                        4036.6211\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           6986.18\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\nVirtualization:                     AMD-V\r\nL1d cache:                          512 KiB (16 instances)\r\nL1i cache:                          512 KiB (16 instances)\r\nL2 cache:                           8 MiB (16 instances)\r\nL3 cache:                           256 MiB (8 instances)\r\nNUMA node(s):                       1\r\nNUMA node0 CPU(s):                  0-31\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; safe RET\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] triton==2.3.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.4.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.4.0             pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tSYS\tSYS\tSYS\t0-31\t0\t\tN/A\r\nGPU1\tSYS\t X \tSYS\tSYS\t0-31\t0\t\tN/A\r\nGPU2\tSYS\tSYS\t X \tSYS\t0-31\t0\t\tN/A\r\nGPU3\tSYS\tSYS\tSYS\t X \t0-31\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nI am running vllm on using ray on two machines each having 4 A100 79Gb. I ran the commands ray start head and ray start address on head and child node. when i run ray status I see I have 8 GPUs. In the next step when I launch vllm with tp 8, i get the error as follows\r\n```\r\n2024-05-07 16:12:51,393\tINFO worker.py:1564 -- Connecting to existing Ray cluster at address: 141.195.90.35:6379...\r\n2024-05-07 16:12:51,398\tINFO worker.py:1749 -- Connected to Ray cluster.\r\nTraceback (most recent call last):\r\n  File \"miniconda3/envs/vllm_cohere/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/miniconda3/envs/vllm_cohere/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File miniconda3/envs/vllm_cohere/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 168, in <module>\r\n    engine = AsyncLLMEngine.from_engine_args(\r\n  File \"envs/vllm_cohere/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 357, in from_engine_args\r\n    initialize_ray_cluster(engine_config.parallel_config)\r\n  File \"/miniconda3/envs/vllm_cohere/lib/python3.10/site-packages/vllm/executor/ray_utils.py\", line 106, in initialize_ray_cluster\r\n    raise ValueError(\r\nValueError: The number of required GPUs exceeds the total number of available GPUs in the cluster.\r\n```\r\nWhen i check the ray status again, i only see 4 GPUs. I am not sure why ray cant see my 8 GPus after i try to launch it with vllm when it is obviously visible before. I use the following command to launch\r\n```/python -m vllm.entrypoints.openai.api_server --model ibm-granite/granite-34b-code-instruct --worker-use-ray --tensor-parallel-size 8 --trust-remote-code --port 40023 --host 0.0.0.0  --gpu-memory-utilization .65  --tokenizer ibm-granite/granite-34b-code-instruct --worker-use-ray ```\r\n\r\n",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-07T14:19:54+00:00",
    "closed_at": "2024-11-28T02:05:19+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4655/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4655"
  }
]
[
  {
    "number": 9794,
    "title": "[Bug]: failed to test tests/lora/test_layers.py::test_embeddings[True-512-cuda:1-1]",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nWARNING 10-29 04:15:30 _custom_ops.py:19] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.12.7 (main, Oct  1 2024, 08:52:12) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-40GB\r\nGPU 1: NVIDIA A100-SXM4-40GB\r\n\r\nNvidia driver version: 535.183.06\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      43 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             256\r\nOn-line CPU(s) list:                0-255\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 7742 64-Core Processor\r\nCPU family:                         23\r\nModel:                              49\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 64\r\nSocket(s):                          2\r\nStepping:                           0\r\nFrequency boost:                    enabled\r\nCPU max MHz:                        2250.0000\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           4491.84\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\r\nVirtualization:                     AMD-V\r\nL1d cache:                          4 MiB (128 instances)\r\nL1i cache:                          4 MiB (128 instances)\r\nL2 cache:                           64 MiB (128 instances)\r\nL3 cache:                           512 MiB (32 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-63,128-191\r\nNUMA node1 CPU(s):                  64-127,192-255\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec rstack overflow: Mitigation; safe RET\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.6+cu121torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.77\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.2\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.3\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV12\t64-127,192-255\t1\t\tN/A\r\nGPU1\tNV12\t X \t64-127,192-255\t1\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\nNone\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\n- pytest tests/lora/test_layers.py::test_embeddings[True-512-cuda:0-1]: **passed**\r\n- pytest tests/lora/test_layers.py::test_embeddings[True-512-cuda:1-1]: **failed**\r\n\r\nseems that DummyLoRAManager().init_random_lora puts lora weight on the wrong device, error msg:\r\n\r\n```bash\r\n=========================================================================================== test session starts ===========================================================================================\r\nplatform linux -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0\r\nrootdir: /root/vllm\r\nconfigfile: pyproject.toml\r\nplugins: anyio-4.6.2.post1\r\ncollected 1 item                                                                                                                                                                                          \r\n\r\ntests/lora/test_layers.py F                                                                                                                                                                         [100%]\r\n\r\n================================================================================================ FAILURES =================================================================================================\r\n___________________________________________________________________________________ test_embeddings[True-512-cuda:1-1] ____________________________________________________________________________________\r\n\r\ndist_init = None, num_loras = 1, device = 'cuda:1', vocab_size = 512, stage = True\r\n\r\n    @torch.inference_mode()\r\n    @pytest.mark.parametrize(\"num_loras\", [1, 2, 4, 8])\r\n    @pytest.mark.parametrize(\"device\", CUDA_DEVICES)\r\n    @pytest.mark.parametrize(\"vocab_size\", [512, 32000, 64000, 128000])\r\n    @pytest.mark.parametrize(\"stage\", STAGES)\r\n    def test_embeddings(dist_init, num_loras, device, vocab_size, stage) -> None:\r\n    \r\n        torch.set_default_device(device)\r\n        max_loras = 8\r\n        punica_wrapper = PunicaWrapper(8192, 256, device)\r\n        lora_config = LoRAConfig(max_loras=max_loras,\r\n                                 max_lora_rank=8,\r\n                                 lora_dtype=torch.float16)\r\n    \r\n        def create_random_embedding_layer():\r\n            embedding = VocabParallelEmbedding(vocab_size, 256)\r\n            embedding.weight.data = torch.rand_like(embedding.weight.data)\r\n            embedding.weight.data[vocab_size:, :] = 0\r\n            lora_embedding = VocabParallelEmbeddingWithLoRA(embedding)\r\n            lora_embedding.create_lora_weights(max_loras, lora_config)\r\n    \r\n            return embedding, lora_embedding\r\n    \r\n        for i in range(10):\r\n            set_random_seed(i)\r\n    \r\n            id_to_index = get_random_id_to_index(num_loras, max_loras)\r\n            embedding, lora_embedding = create_random_embedding_layer()\r\n            lora_embedding.set_mapping(punica_wrapper)\r\n            lora_dict, _ = populate_loras(\r\n                id_to_index,\r\n                layer=lora_embedding,\r\n                layer_weights=embedding.weight.T,\r\n            )\r\n    \r\n            inputs, index_mapping, prompt_mapping = create_random_inputs(\r\n                active_lora_ids=list(lora_dict.keys()),\r\n                num_inputs=num_loras * 3,\r\n                input_size=(200, ),\r\n                input_range=(1, vocab_size),\r\n            )\r\n            lora_mapping = LoRAMapping(index_mapping,\r\n                                       prompt_mapping,\r\n                                       is_prefill=stage)\r\n            punica_wrapper.update_metadata(lora_mapping, id_to_index, max_loras,\r\n                                           vocab_size,\r\n                                           lora_config.lora_extra_vocab_size)\r\n    \r\n            lora_result = lora_embedding(torch.cat(inputs))\r\n    \r\n            expected_results: List[torch.Tensor] = []\r\n            for input_, lora_id in zip(inputs, prompt_mapping):\r\n                lora = lora_dict[lora_id]\r\n                result = embedding(input_)\r\n>               after_a = F.embedding(\r\n                    input_,\r\n                    lora.lora_a,\r\n                )\r\n\r\ntests/lora/test_layers.py:242: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2236: in embedding\r\n    return handle_torch_function(\r\n/usr/local/lib/python3.12/dist-packages/torch/overrides.py:1630: in handle_torch_function\r\n    result = mode.__torch_function__(public_api, types, args, kwargs)\r\n/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py:79: in __torch_function__\r\n    return func(*args, **kwargs)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\ninput = tensor([ 36, 331, 238, 230, 382, 423, 240, 180, 416, 450, 480, 129, 485, 348,\r\n        223, 312, 257, 249,  30, 111, 41...    324, 503, 414,  97,  23, 229, 426, 194, 113, 366, 147, 411, 326, 236,\r\n        374, 254, 448,  55], device='cuda:1')\r\nweight = tensor([[0.3990, 0.5167, 0.0249,  ..., 0.7967, 0.4150, 0.8203],\r\n        [0.2290, 0.9096, 0.1183,  ..., 0.9601, 0.2093,... ..., 0.2047, 0.2683, 0.8661],\r\n        [0.9411, 0.3439, 0.2431,  ..., 0.2671, 0.1570, 0.2273]],\r\n       device='cuda:0')\r\npadding_idx = -1, max_norm = None, norm_type = 2.0, scale_grad_by_freq = False, sparse = False\r\n\r\n    def embedding(\r\n        input: Tensor,\r\n        weight: Tensor,\r\n        padding_idx: Optional[int] = None,\r\n        max_norm: Optional[float] = None,\r\n        norm_type: float = 2.0,\r\n        scale_grad_by_freq: bool = False,\r\n        sparse: bool = False,\r\n    ) -> Tensor:\r\n        r\"\"\"Generate a simple lookup table that looks up embeddings in a fixed dictionary and size.\r\n    \r\n        This module is often used to retrieve word embeddings using indices.\r\n        The input to the module is a list of indices, and the embedding matrix,\r\n        and the output is the corresponding word embeddings.\r\n    \r\n        See :class:`torch.nn.Embedding` for more details.\r\n    \r\n        .. note::\r\n            Note that the analytical gradients of this function with respect to\r\n            entries in :attr:`weight` at the row specified by :attr:`padding_idx`\r\n            are expected to differ from the numerical ones.\r\n    \r\n        .. note::\r\n            Note that `:class:`torch.nn.Embedding` differs from this function in\r\n            that it initializes the row of :attr:`weight` specified by\r\n            :attr:`padding_idx` to all zeros on construction.\r\n    \r\n        Args:\r\n            input (LongTensor): Tensor containing indices into the embedding matrix\r\n            weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,\r\n                and number of columns equal to the embedding size\r\n            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\r\n                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\r\n                                         i.e. it remains as a fixed \"pad\".\r\n            max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\r\n                                        is renormalized to have norm :attr:`max_norm`.\r\n                                        Note: this will modify :attr:`weight` in-place.\r\n            norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.\r\n            scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of\r\n                                                    the words in the mini-batch. Default ``False``.\r\n            sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under\r\n                                     :class:`torch.nn.Embedding` for more details regarding sparse gradients.\r\n    \r\n        Shape:\r\n            - Input: LongTensor of arbitrary shape containing the indices to extract\r\n            - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,\r\n              where V = maximum index + 1 and embedding_dim = the embedding size\r\n            - Output: `(*, embedding_dim)`, where `*` is the input shape\r\n    \r\n        Examples::\r\n    \r\n            >>> # a batch of 2 samples of 4 indices each\r\n            >>> input = torch.tensor([[1, 2, 4, 5], [4, 3, 2, 9]])\r\n            >>> # an embedding matrix containing 10 tensors of size 3\r\n            >>> embedding_matrix = torch.rand(10, 3)\r\n            >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\r\n            >>> F.embedding(input, embedding_matrix)\r\n            tensor([[[ 0.8490,  0.9625,  0.6753],\r\n                     [ 0.9666,  0.7761,  0.6108],\r\n                     [ 0.6246,  0.9751,  0.3618],\r\n                     [ 0.4161,  0.2419,  0.7383]],\r\n    \r\n                    [[ 0.6246,  0.9751,  0.3618],\r\n                     [ 0.0237,  0.7794,  0.0528],\r\n                     [ 0.9666,  0.7761,  0.6108],\r\n                     [ 0.3385,  0.8612,  0.1867]]])\r\n    \r\n            >>> # example with padding_idx\r\n            >>> weights = torch.rand(10, 3)\r\n            >>> weights[0, :].zero_()\r\n            >>> embedding_matrix = weights\r\n            >>> input = torch.tensor([[0, 2, 0, 5]])\r\n            >>> F.embedding(input, embedding_matrix, padding_idx=0)\r\n            tensor([[[ 0.0000,  0.0000,  0.0000],\r\n                     [ 0.5609,  0.5384,  0.8720],\r\n                     [ 0.0000,  0.0000,  0.0000],\r\n                     [ 0.6262,  0.2438,  0.7471]]])\r\n        \"\"\"\r\n        if has_torch_function_variadic(input, weight):\r\n            return handle_torch_function(\r\n                embedding,\r\n                (input, weight),\r\n                input,\r\n                weight,\r\n                padding_idx=padding_idx,\r\n                max_norm=max_norm,\r\n                norm_type=norm_type,\r\n                scale_grad_by_freq=scale_grad_by_freq,\r\n                sparse=sparse,\r\n            )\r\n        if padding_idx is not None:\r\n            if padding_idx > 0:\r\n                assert padding_idx < weight.size(0), \"Padding_idx must be within num_embeddings\"\r\n            elif padding_idx < 0:\r\n                assert padding_idx >= -weight.size(0), \"Padding_idx must be within num_embeddings\"\r\n                padding_idx = weight.size(0) + padding_idx\r\n        else:\r\n            padding_idx = -1\r\n        if max_norm is not None:\r\n            # Note [embedding_renorm contiguous]\r\n            # `embedding_renorm_` will call .contiguous() on input anyways, so we\r\n            # call it here and take advantage of the improved locality in the\r\n            # `embedding` call below too.\r\n            input = input.contiguous()\r\n            # Note [embedding_renorm set_grad_enabled]\r\n            # XXX: equivalent to\r\n            # with torch.no_grad():\r\n            #   torch.embedding_renorm_\r\n            # remove once script supports set_grad_enabled\r\n            _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\r\n>       return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\nE       RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument index in method wrapper_CUDA__index_select)\r\n\r\n/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2267: RuntimeError\r\n------------------------------------------------------------------------------------------ Captured stdout call -------------------------------------------------------------------------------------------\r\nCreated lora_id_to_index mapping: [None, None, None, None, None, 1, None, None].\r\n========================================================================================= short test summary info =========================================================================================\r\nFAILED tests/lora/test_layers.py::test_embeddings[True-512-cuda:1-1] - RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument index in method wrapper_CUDA__index_select)\r\n============================================================================================ 1 failed in 1.86s ============================================================================================\r\n```\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-10-29T11:19:41+00:00",
    "closed_at": "2024-11-12T03:10:16+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9794/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9794"
  },
  {
    "number": 18008,
    "title": "[Feature]: Support FP8 Marlin MoE for CompressedTensorsW8A8Fp8MoEMethod",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nLike what was added in https://github.com/vllm-project/vllm/pull/16850 for enabling marlin in fp8.py MoE layers, we should enable FP8 Marlin MoE for compressed tensors models to support users wanting to run them on older hardware.\n\nBasically you want to take the changes in fp8.py's moe method (https://github.com/vllm-project/vllm/pull/16850/files#diff-5511bfcc9c53f7d96517ad43e4087f6777bef21302da983f42cafae40a866644) and apply them to `CompressedTensorsW8A8Fp8MoEMethod`\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "feature request",
      "quantization"
    ],
    "state": "closed",
    "created_at": "2025-05-12T18:02:12+00:00",
    "closed_at": "2025-05-20T11:58:40+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18008/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18008"
  },
  {
    "number": 8002,
    "title": "[RFC]: Build `vllm-flash-attn` from source",
    "body": "### Motivation.\n\nTo use a custom version of PyTorch in vLLM, `vllm-flash-attn` needs to be built with the same version. The easiest way to achieve that is by building it from source during the vLLM build.\n\n### Proposed Change.\n\nWe propose 3 different ways of building `vllm-flash-attn` from source: absorbing the package completely, building it as a CMake dependency, or running a nested `pip install`. Currently, alternative 2 is preferred, but we'd like to get feedback on that. I will update this RFC once we decide on an approach.\r\n\r\nMore details here: https://docs.google.com/document/d/1njmz8NPT3am5gNcjbjzZG1BN-v8wIxWq6vb5QoctuZ0/edit?usp=sharing\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n@WoosukKwon @youkaichao @tlrmchlsmth @bnellnm \n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "closed",
    "created_at": "2024-08-29T16:09:02+00:00",
    "closed_at": "2024-09-21T06:27:12+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8002/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8002"
  },
  {
    "number": 17965,
    "title": "[Bug]: TimeoutError and EngineDeadError in vLLM: RPC Call to execute_model Timed Out and EngineCore Failure",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nINFO 05-12 00:28:31 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: TencentOS Server 3.2 (Final) (x86_64)\nGCC version: (GCC) 11.2.1 20210728 (Red Hat 11.2.1-1)\nClang version: 16.0.6 (Red Hat 16.0.6-2.module+el8.8.0+557+454507bd)\nCMake version: version 3.28.0\nLibc version: glibc-2.28\n\nPython version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.4.241-1-tlinux4-0017.7-x86_64-with-glibc2.28\nIs CUDA available: True\nCUDA runtime version: 12.8.61\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA H20\nGPU 1: NVIDIA H20\nGPU 2: NVIDIA H20\nGPU 3: NVIDIA H20\nGPU 4: NVIDIA H20\nGPU 5: NVIDIA H20\nGPU 6: NVIDIA H20\nGPU 7: NVIDIA H20\n\nNvidia driver version: 535.161.08\ncuDNN version: Probably one of the following:\n/usr/lib64/libcudnn.so.9.7.1\n/usr/lib64/libcudnn_adv.so.9.7.1\n/usr/lib64/libcudnn_cnn.so.9.7.1\n/usr/lib64/libcudnn_engines_precompiled.so.9.7.1\n/usr/lib64/libcudnn_engines_runtime_compiled.so.9.7.1\n/usr/lib64/libcudnn_graph.so.9.7.1\n/usr/lib64/libcudnn_heuristic.so.9.7.1\n/usr/lib64/libcudnn_ops.so.9.7.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:        x86_64\nCPU op-mode(s):      32-bit, 64-bit\nByte Order:          Little Endian\nCPU(s):              384\nOn-line CPU(s) list: 0-383\nThread(s) per core:  2\nCore(s) per socket:  96\nSocket(s):           2\nNUMA node(s):        2\nVendor ID:           AuthenticAMD\nBIOS Vendor ID:      Advanced Micro Devices, Inc.\nCPU family:          25\nModel:               17\nModel name:          AMD EPYC 9K84 96-Core Processor\nBIOS Model name:     AMD EPYC 9K84 96-Core Processor                \nStepping:            1\nCPU MHz:             3699.861\nCPU max MHz:         2600.0000\nCPU min MHz:         1500.0000\nBogoMIPS:            5200.50\nVirtualization:      AMD-V\nL1d cache:           32K\nL1i cache:           32K\nL2 cache:            1024K\nL3 cache:            32768K\nNUMA node0 CPU(s):   0-95,192-287\nNUMA node1 CPU(s):   96-191,288-383\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov succor smca fsrm flush_l1d\n\nVersions of relevant libraries:\n[pip3] numpy==2.2.5\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.3\n[pip3] triton==3.2.0\n[conda] numpy                     2.2.5                    pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.4.0                   pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] transformers              4.51.3                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.5\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n\t\u001b[4mGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tNIC4\tNIC5\tNIC6\tNIC7\tNIC8\tNIC9\tNIC10\tNIC11\tNIC12\tNIC13\tNIC14\tNIC15\tNIC16\tNIC17\tNIC18\tNIC19\tNIC20\tNIC21\tNIC22\tNIC23\tNIC24\tNIC25\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPIX\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t0-95,192-287\t0\t\tN/A\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPHB\tPIX\tSYS\tSYS\tSYS\tSYS\t0-95,192-287\t0\t\tN/A\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPIX\tPHB\tSYS\tSYS\tSYS\tSYS\t0-95,192-287\t0\t\tN/A\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tPIX\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t0-95,192-287\t0\t\tN/A\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPIX\tNODE\t96-191,288-383\t1\t\tN/A\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\tPIX\tNODE\tNODE\t96-191,288-383\t1\t\tN/A\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tPHB\tNODE\tNODE\tPIX\t96-191,288-383\t1\t\tN/A\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tPIX\tNODE\tNODE\tPHB\t96-191,288-383\t1\t\tN/A\nNIC0\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t X \tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t\t\t\t\nNIC1\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tPIX\t X \tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t\t\t\t\nNIC2\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tPIX\tPIX\t X \tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t\t\t\t\nNIC3\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tPIX\tPIX\tPIX\t X \tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t\t\t\t\nNIC4\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tPIX\tPIX\tPIX\tPIX\t X \tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t\t\t\t\nNIC5\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tPIX\tPIX\tPIX\tPIX\tPIX\t X \tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t\t\t\t\nNIC6\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\t X \tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t\t\t\t\nNIC7\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\t X \tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t\t\t\t\nNIC8\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\t X \tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t\t\t\t\nNIC9\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\t X \tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t\t\t\t\nNIC10\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\t X \tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t\t\t\t\nNIC11\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\t X \tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t\t\t\t\nNIC12\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\t X \tPIX\tPIX\tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t\t\t\t\nNIC13\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\t X \tPIX\tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t\t\t\t\nNIC14\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\t X \tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t\t\t\t\nNIC15\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\t X \tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t\t\t\t\nNIC16\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\t X \tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t\t\t\t\nNIC17\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\tPIX\t X \tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\t\t\t\t\nNIC18\tPIX\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t\t\t\t\nNIC19\tNODE\tNODE\tNODE\tPIX\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\t X \tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t\t\t\t\nNIC20\tNODE\tPHB\tPIX\tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\t X \tPHB\tSYS\tSYS\tSYS\tSYS\t\t\t\t\nNIC21\tNODE\tPIX\tPHB\tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPHB\t X \tSYS\tSYS\tSYS\tSYS\t\t\t\t\nNIC22\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPHB\tPIX\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t X \tNODE\tNODE\tPHB\t\t\t\t\nNIC23\tSYS\tSYS\tSYS\tSYS\tNODE\tPIX\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\t X \tNODE\tNODE\t\t\t\t\nNIC24\tSYS\tSYS\tSYS\tSYS\tPIX\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\t X \tNODE\t\t\t\t\nNIC25\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPIX\tPHB\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tPHB\tNODE\tNODE\t X \t\t\t\t\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n  NIC10: mlx5_10\n  NIC11: mlx5_11\n  NIC12: mlx5_12\n  NIC13: mlx5_13\n  NIC14: mlx5_14\n  NIC15: mlx5_15\n  NIC16: mlx5_16\n  NIC17: mlx5_17\n  NIC18: mlx5_bond_1\n  NIC19: mlx5_bond_2\n  NIC20: mlx5_bond_3\n  NIC21: mlx5_bond_4\n  NIC22: mlx5_bond_5\n  NIC23: mlx5_bond_6\n  NIC24: mlx5_bond_7\n  NIC25: mlx5_bond_8\n\nLD_LIBRARY_PATH=:/usr/local/nvshmem/lib:/opt/gdrcopy/lib/:/opt/gdrcopy/lib/:/opt/nvidia/nsight-systems/2024.6.2/host-linux-x64/:/opt/rh/gcc-toolset-11/root/usr/lib64:/opt/rh/gcc-toolset-11/root/usr/lib:/opt/rh/gcc-toolset-11/root/usr/lib64/dyninst:/opt/rh/gcc-toolset-11/root/usr/lib/dyninst:/usr/local/cuda-12.8/lib64:\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n```\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/contextlib.py\", line 137, in __enter__\n    return next(self.gen)\n           ^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/site-packages/vllm/distributed/device_communicators/shm_broadcast.py\", line 443, in acquire_read\n    raise TimeoutError\nTimeoutError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 400, in run_engine_core\n    raise e\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 389, in run_engine_core\n    engine_core.run_busy_loop()\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 413, in run_busy_loop\n    self._process_engine_step()\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 438, in _process_engine_step\n    outputs = self.step_fn()\n              ^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 203, in step\n    output = self.model_executor.execute_model(scheduler_output)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 146, in execute_model\n    (output, ) = self.collective_rpc(\"execute_model\",\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 193, in collective_rpc\n    raise TimeoutError(f\"RPC call to {method} timed out.\") from e\nTimeoutError: RPC call to execute_model timed out.\n/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/multiprocessing/resource_tracker.py:255: UserWarning: resource_tracker: There appear to be 2 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n\n  0%|          | 0/1 [19:42<?, ?it/s]\nTraceback (most recent call last):\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/run.py\", line 91, in <module>\n    main()\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/run.py\", line 87, in main\n    runner.run()\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/run.py\", line 57, in run\n    datas, filtered_datas = self.pipeline(datas, filtered_datas)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/lmmdp/pipelines/standard.py\", line 51, in __call__\n    datas, filtered_datas = op(datas, filtered_datas)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/lmmdp/filter/filters_compose.py\", line 25, in __call__\n    datas, filtered_datas = f(datas, filtered_datas)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/lmmdp/filter/filter_by_mllm.py\", line 199, in __call__\n    all_responses = self.processor.generate(all_requests)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/lmmdp/utils/mllm_conv.py\", line 583, in generate\n    responses = self.model.generate(requests)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/lmmdp/models/local_vllm_model.py\", line 214, in generate\n    responses = self.generate_inner(results)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/lmmdp/models/local_vllm_model.py\", line 196, in generate_inner\n    raise e\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/lmmdp/models/local_vllm_model.py\", line 189, in generate_inner\n    return self.llm.generate(inputs,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/site-packages/vllm/utils.py\", line 1196, in inner\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/site-packages/vllm/entrypoints/llm.py\", line 473, in generate\n    outputs = self._run_engine(use_tqdm=use_tqdm)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/site-packages/vllm/entrypoints/llm.py\", line 1423, in _run_engine\n    step_outputs = self.llm_engine.step()\n                   ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py\", line 218, in step\n    outputs = self.engine_core.get_output()\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 558, in get_output\n    raise self._format_exception(outputs) from None\nvllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\nTraceback (most recent call last):\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/run.py\", line 91, in <module>\n    main()\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/run.py\", line 87, in main\n    runner.run()\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/run.py\", line 57, in run\n    datas, filtered_datas = self.pipeline(datas, filtered_datas)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/lmmdp/pipelines/standard.py\", line 51, in __call__\n    datas, filtered_datas = op(datas, filtered_datas)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/lmmdp/filter/filters_compose.py\", line 25, in __call__\n    datas, filtered_datas = f(datas, filtered_datas)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/lmmdp/filter/filter_by_mllm.py\", line 199, in __call__\n    all_responses = self.processor.generate(all_requests)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/lmmdp/utils/mllm_conv.py\", line 583, in generate\n    responses = self.model.generate(requests)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/lmmdp/models/local_vllm_model.py\", line 214, in generate\n    responses = self.generate_inner(results)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/lmmdp/models/local_vllm_model.py\", line 196, in generate_inner\n    raise e\n  File \"/apdcephfs/share_303576955/hunyuan/leoyizhang/sz/projects/zy/MLLM_scaling/LMMDP/lmmdp/models/local_vllm_model.py\", line 189, in generate_inner\n    return self.llm.generate(inputs,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/site-packages/vllm/utils.py\", line 1196, in inner\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/site-packages/vllm/entrypoints/llm.py\", line 473, in generate\n    outputs = self._run_engine(use_tqdm=use_tqdm)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/site-packages/vllm/entrypoints/llm.py\", line 1423, in _run_engine\n    step_outputs = self.llm_engine.step()\n                   ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py\", line 218, in step\n    outputs = self.engine_core.get_output()\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/vllm_0.8.5/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 558, in get_output\n    raise self._format_exception(outputs) from None\nvllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-05-11T16:29:17+00:00",
    "closed_at": "2025-05-23T02:22:12+00:00",
    "comments": 36,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17965/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17965"
  },
  {
    "number": 169,
    "title": "How does this compare to MQA (multi-query attention)?",
    "body": "https://arxiv.org/abs/1911.02150\r\n\r\nFor example, StarCoder uses MQA to speed up inference. How does PagedAttention compare to Multi-Query Attention? Are they compatible?",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2023-06-20T21:11:29+00:00",
    "closed_at": "2023-07-16T21:57:12+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/169/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/169"
  },
  {
    "number": 1192,
    "title": "run new qwen 7b v1.1 results error?",
    "body": "python -m vllm.entrypoints.api_server  --model  /***/Qwen-7B-Chat --swap-space 16   --disable-log-requests --host 192.168.19.14 --port 10860 --max-num-seqs 256   --trust-remote-code --tensor-parallel-size 2  --dtype=half\r\n\r\nIt turned out to be full of exclamation marks!!!\r\n![image](https://github.com/vllm-project/vllm/assets/40717349/b3140269-d8e0-4ed2-ac69-8afd9d2292c9)\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-09-27T07:57:14+00:00",
    "closed_at": "2023-10-07T01:37:58+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1192/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1192"
  },
  {
    "number": 17273,
    "title": "[Usage]: How to terminate vllm completely?",
    "body": "### Your current environment\n\n```text\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: CentOS Linux 8 (x86_64)\nGCC version: (GCC) 10.5.0\nClang version: Could not collect\nCMake version: version 3.20.2\nLibc version: glibc-2.29\n\nPython version: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0] (64-bit runtime)\nPython platform: Linux-4.18.0-348.7.1.el8_5.x86_64-x86_64-with-glibc2.29\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 550.135\ncuDNN version: Probably one of the following:\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn.so.8.9.7\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.9.7\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.9.7\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.9.7\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.9.7\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.9.7\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.9.7\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\n\u67b6\u6784\uff1a           x86_64\nCPU \u8fd0\u884c\u6a21\u5f0f\uff1a   32-bit, 64-bit\n\u5b57\u8282\u5e8f\uff1a         Little Endian\nCPU:             144\n\u5728\u7ebf CPU \u5217\u8868\uff1a  0-143\n\u6bcf\u4e2a\u6838\u7684\u7ebf\u7a0b\u6570\uff1a 2\n\u6bcf\u4e2a\u5ea7\u7684\u6838\u6570\uff1a   36\n\u5ea7\uff1a             2\nNUMA \u8282\u70b9\uff1a      2\n\u5382\u5546 ID\uff1a        GenuineIntel\nCPU \u7cfb\u5217\uff1a       6\n\u578b\u53f7\uff1a           106\n\u578b\u53f7\u540d\u79f0\uff1a       Intel(R) Xeon(R) Platinum 8352V CPU @ 2.10GHz\n\u6b65\u8fdb\uff1a           6\nCPU MHz\uff1a        1973.831\nCPU \u6700\u5927 MHz\uff1a   2101.0000\nCPU \u6700\u5c0f MHz\uff1a   800.0000\nBogoMIPS\uff1a       4200.00\n\u865a\u62df\u5316\uff1a         VT-x\nL1d \u7f13\u5b58\uff1a       48K\nL1i \u7f13\u5b58\uff1a       32K\nL2 \u7f13\u5b58\uff1a        1280K\nL3 \u7f13\u5b58\uff1a        55296K\nNUMA \u8282\u70b90 CPU\uff1a 0-35,72-107\nNUMA \u8282\u70b91 CPU\uff1a 36-71,108-143\n\u6807\u8bb0\uff1a           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdqdtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust sgx bmi1 hle avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cdsha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid sgx_lc fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] gptqmodel==1.9.0+cu124torch2.5\n[pip3] numpy==1.25.0\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] triton==3.1.0\n[conda] gptqmodel                 1.9.0+cu124torch2.5          pypi_0    pypi\n[conda] numpy                     1.25.0                   pypi_0    pypi\n[conda] torch                     2.5.1                    pypi_0    pypi\n[conda] torchaudio                2.5.1                    pypi_0    pypi\n[conda] torchvision               0.20.1                   pypi_0    pypi\n[conda] triton                    3.1.0                    pypi_0    pypi\n```\n\n\n### How would you like to use vllm\n\nIn some cases, (interrupte by heads, or other way),  vllm are not terminate clearly\u3002When I restart the serve in shell\uff0cIt stack at loading models. I can't find any solutions except restart the machine\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-04-28T01:19:40+00:00",
    "closed_at": "2025-05-05T13:41:59+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17273/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17273"
  },
  {
    "number": 606,
    "title": "Memory usage decreases as batch size increases",
    "body": "Hi all,\r\n\r\nI am running OPT-6.7B [https://huggingface.co/facebook/opt-6.7b] on an A100 GPU with 80GB.\r\n\r\nThe 'gpu_memory_utilization' is 0.9 (as default) and I am using `torch.cuda.memory_allocated` to get the GPU memory that's allocated.\r\n\r\nFor input length and output length of 40 and 156, respectively, this is the allocated memory (GB) I see across batch sizes:\r\nBatch | Allocated memory\r\n2        | 72.8 GB\r\n4        | 72.68 GB \r\n8        | 72.68 GB\r\n16      | 72.55 GB\r\n32      | 72.18 GB\r\n64      | 71.68 GB\r\n128    | 70.55 GB\r\n256    | 68.18 GB\r\n512    | 63.68 GB\r\nFor smaller batch sizes, the allocated memory is around 80 * 0.9 as expected, but it becomes smaller as the batch size increases.\r\n\r\nIs there a reason to allocate less memory for larger batch sizes? Is the unallocated memory used for some other purposes?\r\nFollowing the discussion in other issues, the allocated memory for engine includes both the model for inference and KV cache.\r\nWith the allocate memory numbers above, does vLLM allocates a smaller KV cache when batch size is bigger (where it is supposed to have larger KV cache)?\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-07-27T22:57:59+00:00",
    "closed_at": "2023-08-07T22:34:52+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/606/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/606"
  },
  {
    "number": 4981,
    "title": "[Usage]: How to start vLLM on a particular GPU?",
    "body": "### Your current environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 \r\nClang version: Could not collect\r\nCMake version: version 3.29.3\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1056-azure-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 11.8.89\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA A100 80GB PCIe\r\nGPU 1: NVIDIA A100 80GB PCIe\r\n\r\nNvidia driver version: 545.23.08\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn.so.8.7.0\r\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.7.0\r\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.7.0\r\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.7.0\r\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.7.0\r\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.7.0\r\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.7.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit \r\nByte Order:                         Little Endian  \r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nCPU(s):                             48\r\nOn-line CPU(s) list:                0-47\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 48\r\nSocket(s):                          1\r\nNUMA node(s):                       2\r\nVendor ID:                          AuthenticAMD   \r\nCPU family:                         25\r\nModel:                              1\r\nModel name:                         AMD EPYC 7V13 64-Core Processor\r\nStepping:                           1\r\nCPU MHz:                            2445.437\r\nBogoMIPS:                           4890.87\r\nHypervisor vendor:                  Microsoft\r\nVirtualization type:                full\r\nL1d cache:                          1.5 MiB\r\nL1i cache:                          1.5 MiB\r\nL2 cache:                           24 MiB\r\nL3 cache:                           192 MiB\r\nNUMA node0 CPU(s):                  0-23\r\nNUMA node1 CPU(s):                  24-47\r\nVulnerability Gather data sampling: Not affected   \r\nVulnerability Itlb multihit:        Not affected   \r\nVulnerability L1tf:                 Not affected   \r\nVulnerability Mds:                  Not affected   \r\nVulnerability Meltdown:             Not affected   \r\nVulnerability Mmio stale data:      Not affected   \r\nVulnerability Retbleed:             Not affected   \r\nVulnerability Spec rstack overflow: Mitigation; safe RET, no microcode\r\nVulnerability Spec store bypass:    Vulnerable\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected   \r\nVulnerability Tsx async abort:      Not affected   \r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr rdpru arat umip vaes vpclmulqdq rdpid fsrm\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] triton==2.3.0\r\n[pip3] vllm_nccl_cu12==2.18.1.0.4.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.4.0             pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV12    SYS     0-23    0               N/A\r\nGPU1    NV12     X      SYS     24-47   1               N/A\r\nNIC0    SYS     SYS      X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n```\r\n\r\n\r\n### How would you like to use vllm\r\n\r\nI have two GPUs in my VM... I am already using vLLM on one of the GPUs and the other one is vacant.\r\nHow can I start a second vLLM instance on the second GPU of mine?\r\n\r\nI tried:\r\n```bash\r\n--device cuda    |    --device auto    |    --device cuda:1\r\n```\r\nbut they don't seem to work as I was expecting...\r\n\r\n\r\nCould you please tell me what am I missing here?\r\n\r\n\r\nRegards!",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-05-22T12:41:56+00:00",
    "closed_at": "2024-06-13T23:06:51+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4981/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4981"
  },
  {
    "number": 8306,
    "title": "[RFC]: Reimplement and separate beam search on top of vLLM core",
    "body": "### Motivation.\n\nA rework of https://github.com/vllm-project/vllm/issues/6226 \r\n\r\nAfter discussing further with the community, we find that the common use case for beam search is: \r\n1. throughput oriented\r\n2. mainly offline batch inference\r\n3. use one beam search parameter for all the prompts in the batch\r\n\r\nAfter discussing with many contributors, we find:\r\n\r\nbecause beam search is a **search** algorithm, it conflicts with all the rest **sampling** algorithm. As a result, many features in vllm already directly assert beam search is not used, e.g.\r\n\r\nhttps://github.com/vllm-project/vllm/blob/6e36f4fa6ce64619b9ea94c88a157f5783a63a65/vllm/spec_decode/batch_expansion.py#L303-L305\r\n\r\nhttps://github.com/vllm-project/vllm/blob/6e36f4fa6ce64619b9ea94c88a157f5783a63a65/vllm/engine/output_processor/multi_step.py#L100-L103\r\n\r\n**keeping beam-search as-is in the codebase, will not benefit current beam search user, as no optimization will target at better beam search performance. What's worse, very few developers understand beam search. Keeping beam-search as-is will not only increase the bugs for beam search as the codebase evolves, but also increase the maintenance cost of all contributors.**\r\n\r\nin search of a win-win solution, on behalf of the vllm team, I propose to separate and reimplement beam search on top of the vllm core code.\r\n\r\nto be specific, we can:\r\n1. remove beam search logic from the scheduler\r\n2. add an `LLM.beam_search` interface, that calls the engine to generate 1 tokens with logprobs every step, and maintain beam-search logic only in the `LLM.beam_search` function.\r\n3. add a beam search emulator over commonly used openai api server, which internally calls the generation endpoint to generate one step with logprobs, and maintain beam-search logic only in the emulator.\r\n\r\nFrom the initial discussion, one concern is the efficiency of such implementation, as the request will come and go again and again from the vllm core's perspective. It should be solvable in two-folds:\r\n1. turning on prefix caching can reuse computation from the last step so that we don't need to recompute the kv cache of prompt again and again.\r\n2. after separating beam search and the vllm core, they can be optimized individually. The simplified code will be much easier to optimize.\r\n\r\nvLLM is a community project, and we'd like to not only seek opinions from beam-search users, but also seek contributions from beam-search users. Your help is truly needed to shape the future of beam-search support in vLLM.\n\n### Proposed Change.\n\nsummary of the change: implement beam-search on top of vllm core and add wrappers for users. remove beam-search from the vllm core (scheduler).\n\n### Feedback Period.\n\n1 week, from 9/9 to 9/15 (both inclusive)\n\n### CC List.\n\n@hrsmanian @zhouyuan @lanking520 @nightflight-dk @HeegonJin @SemMulder @darabos @DhruvaBansal00 @tmostak @physicsrob @YooSungHyun @denadai2 @sjmielke @Reichenbachian @AaronFriel @hinnefe2 @mflaxman10 \r\n@WoosukKwon @zhuohan123 @simon-mo \n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "closed",
    "created_at": "2024-09-09T20:17:13+00:00",
    "closed_at": "2024-10-07T05:47:05+00:00",
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8306/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/8306"
  },
  {
    "number": 12940,
    "title": "[Bug]: Qwen2_5_VL-3B :When running the multi-modal model, encountered multiple critical issues related to sequence length and context window limitations.",
    "body": "### Your current environment\n\n\nQwen2_5_VL-3B\n\n[<!-- Failed to upload \"\u4f01\u4e1a\u5fae\u4fe1\u622a\u56fe_17389938499377.png\" -->](url)\n\n### \ud83d\udc1b Describe the bug\n\nThe model throws three main warnings/errors:\nImage rescaling issues\nToken sequence length exceeding maximum limit\nInsufficient context length for multi-modal embeddings\n\n![Image](https://github.com/user-attachments/assets/6634d3e6-fe7c-41dd-aa19-bda346cc780f)\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-02-08T06:34:26+00:00",
    "closed_at": "2025-03-07T13:46:48+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12940/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12940"
  },
  {
    "number": 9192,
    "title": "[RFC]: Adopt mergify for auto-labeling PRs",
    "body": "### Motivation.\n\nvLLM is a very active project with a large and busy queue of PRs. Additional usage of github labels would assist with narrowing down which PRs to look at given a person's interests, as well as the state of the PR.\n\n### Proposed Change.\n\nAdopt mergify to perform automated labeling of PRs. https://docs.mergify.com/\r\n\r\nWhile github also provides an [action for PR labeling](https://github.com/actions/labeler), it only supports labeling based on the branch and the files changed. Mergify supports labeling based on more criteria, such as whether a branch has conflicts with `main`.\r\n\r\nConfiguration would go into `.github/mergify.yml`. An example entry to auto-label PRs that touch files in the `docs/` directory:\r\n\r\n```yaml\r\n- name: label-documentation\r\n  description: Automatically apply documentation label\r\n  conditions:\r\n    - or:\r\n      - files~=^[^/]+\\.md$\r\n      - files~=^CONTRIBUTING/\r\n      - files~=^docs/\r\n  actions:\r\n    label:\r\n      add:\r\n        - documentation\r\n```\r\n\r\nHere is an example that is a bit more unique to Mergify's capabilities. This sample configuration will label a PR and comment tagging the PR author if the PR goes into conflict with the target branch (`main`). It will also automatically remove the label once conflicts are resolved.\r\n\r\n```yaml\r\n- name: ping author on conflicts and add 'needs-rebase' label\r\n  conditions:\r\n      - conflict\r\n      - -closed\r\n  actions:\r\n    label:\r\n      add:\r\n        - needs-rebase\r\n    comment:\r\n      message: |\r\n       This pull request has merge conflicts that must be resolved before it can be\r\n       merged. @{{author}} please rebase it. https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork\r\n\r\n- name: remove 'needs-rebase' label when conflict is resolved\r\n  conditions:\r\n      - -conflict\r\n      - -closed\r\n  actions:\r\n    label:\r\n      remove:\r\n        - needs-rebase\r\n```\r\n\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n_No response_\n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "closed",
    "created_at": "2024-10-09T13:32:20+00:00",
    "closed_at": "2024-10-28T16:38:11+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9192/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/9192"
  },
  {
    "number": 11471,
    "title": "[Usage]: About '--chat-template' parameters for model google/paligemma2-3b-ft-docci-448",
    "body": "### Your current environment\n\nI use [template_llava.jinja](https://github.com/vllm-project/vllm/blob/v0.6.5/examples/template_llava.jinja) to launch the model google/paligemma2-3b-ft-docci-448. Despite working, I wonder 1) how to decide on a template for a specific model and 2) whether my setting is correct for the model google/paligemma2-3b-ft-docci-448?\r\n```text\r\nvllm serve google/paligemma2-3b-ft-docci-448 --chat-template template_llava.jinja  --host 0.0.0.0  --port 8001 --enforce-eager --dtype auto\r\n```\r\non a \n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-12-24T21:23:14+00:00",
    "closed_at": "2025-01-04T09:49:42+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11471/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11471"
  },
  {
    "number": 12820,
    "title": "[Bug]: vllm:0.7.1 \u542f\u52a8MiniCPM-o-2_6\u62a5\u9519",
    "body": "### Your current environment\n\nvLLM API server version 0.7.1\nINFO 02-06 18:01:41 api_server.py:839] args: Namespace(subparser='serve', model_tag='/xiaobaogong/ai/model/MiniCPM-o-2_6/', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='/xiaobaogong/ai/model/MiniCPM-o-2_6/', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=2048, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=40, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7f9e8360b760>)\nINFO 02-06 18:01:41 api_server.py:204] Started engine process with PID 68\nINFO 02-06 18:01:44 __init__.py:183] Automatically detected platform cuda.\nINFO 02-06 18:01:47 config.py:526] This model supports multiple tasks: {'classify', 'reward', 'score', 'embed', 'generate'}. Defaulting to 'generate'.\nINFO 02-06 18:01:51 config.py:526] This model supports multiple tasks: {'embed', 'score', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.\nINFO 02-06 18:01:51 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='/xiaobaogong/ai/model/MiniCPM-o-2_6/', speculative_config=None, tokenizer='/xiaobaogong/ai/model/MiniCPM-o-2_6/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/xiaobaogong/ai/model/MiniCPM-o-2_6/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[40,32,24,16,8,4,2,1],\"max_capture_size\":40}, use_cached_outputs=True, \nINFO 02-06 18:01:53 cuda.py:235] Using Flash Attention backend.\nINFO 02-06 18:01:53 model_runner.py:1111] Starting to load model /xiaobaogong/ai/model/MiniCPM-o-2_6/...\nINFO 02-06 18:01:54 cuda.py:219] Cannot use FlashAttention-2 backend for head size 72.\nINFO 02-06 18:01:54 cuda.py:232] Using XFormers backend.\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.49it/s]\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.19it/s]\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.19it/s]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.16it/s]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.19it/s]\n\nINFO 02-06 18:01:58 model_runner.py:1116] Loading model weights took 15.7985 GB\n/usr/local/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nERROR 02-06 18:02:00 engine.py:387] 'MiniCPMOProcessor' object has no attribute 'get_audio_placeholder'\nERROR 02-06 18:02:00 engine.py:387] Traceback (most recent call last):\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 378, in run_mp_engine\nERROR 02-06 18:02:00 engine.py:387]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 121, in from_engine_args\nERROR 02-06 18:02:00 engine.py:387]     return cls(ipc_path=ipc_path,\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 73, in __init__\nERROR 02-06 18:02:00 engine.py:387]     self.engine = LLMEngine(*args, **kwargs)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 274, in __init__\nERROR 02-06 18:02:00 engine.py:387]     self._initialize_kv_caches()\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 414, in _initialize_kv_caches\nERROR 02-06 18:02:00 engine.py:387]     self.model_executor.determine_num_available_blocks())\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 99, in determine_num_available_blocks\nERROR 02-06 18:02:00 engine.py:387]     results = self.collective_rpc(\"determine_num_available_blocks\")\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 49, in collective_rpc\nERROR 02-06 18:02:00 engine.py:387]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/utils.py\", line 2208, in run_method\nERROR 02-06 18:02:00 engine.py:387]     return func(*args, **kwargs)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 02-06 18:02:00 engine.py:387]     return func(*args, **kwargs)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/worker/worker.py\", line 228, in determine_num_available_blocks\nERROR 02-06 18:02:00 engine.py:387]     self.model_runner.profile_run()\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 02-06 18:02:00 engine.py:387]     return func(*args, **kwargs)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1236, in profile_run\nERROR 02-06 18:02:00 engine.py:387]     self._dummy_run(max_num_batched_tokens, max_num_seqs)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1301, in _dummy_run\nERROR 02-06 18:02:00 engine.py:387]     .dummy_data_for_profiling(self.model_config,\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/inputs/registry.py\", line 333, in dummy_data_for_profiling\nERROR 02-06 18:02:00 engine.py:387]     dummy_data = profiler.get_dummy_data(seq_len)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/profiling.py\", line 161, in get_dummy_data\nERROR 02-06 18:02:00 engine.py:387]     mm_inputs = self._get_dummy_mm_inputs(seq_len, mm_counts)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/profiling.py\", line 139, in _get_dummy_mm_inputs\nERROR 02-06 18:02:00 engine.py:387]     return self.processor.apply(\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/minicpmv.py\", line 803, in apply\nERROR 02-06 18:02:00 engine.py:387]     result = super().apply(prompt, mm_data, hf_processor_mm_kwargs)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 1230, in apply\nERROR 02-06 18:02:00 engine.py:387]     hf_mm_placeholders = self._find_mm_placeholders(\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 793, in _find_mm_placeholders\nERROR 02-06 18:02:00 engine.py:387]     return find_mm_placeholders(mm_prompt_repls, new_token_ids,\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 579, in find_mm_placeholders\nERROR 02-06 18:02:00 engine.py:387]     return dict(full_groupby_modality(it))\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 184, in full_groupby_modality\nERROR 02-06 18:02:00 engine.py:387]     return full_groupby(values, key=lambda x: x.modality)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/utils.py\", line 873, in full_groupby\nERROR 02-06 18:02:00 engine.py:387]     for value in values:\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 534, in _iter_placeholders\nERROR 02-06 18:02:00 engine.py:387]     replacement = repl_info.get_replacement(item_idx)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 270, in get_replacement\nERROR 02-06 18:02:00 engine.py:387]     replacement = replacement(item_idx)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/minicpmo.py\", line 354, in get_replacement_minicpmv\nERROR 02-06 18:02:00 engine.py:387]     return self.get_audio_prompt_texts(\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/minicpmo.py\", line 231, in get_audio_prompt_texts\nERROR 02-06 18:02:00 engine.py:387]     return self.info.get_hf_processor().get_audio_placeholder(\nERROR 02-06 18:02:00 engine.py:387] AttributeError: 'MiniCPMOProcessor' object has no attribute 'get_audio_placeholder'\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/usr/local/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 389, in run_mp_engine\n    raise e\n  File \"/usr/local/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 378, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n  File \"/usr/local/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 121, in from_engine_args\n    return cls(ipc_path=ipc_path,\n  File \"/usr/local/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 73, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 274, in __init__\n    self._initialize_kv_caches()\n  File \"/usr/local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 414, in _initialize_kv_caches\n    self.model_executor.determine_num_available_blocks())\n  File \"/usr/local/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 99, in determine_num_available_blocks\n    results = self.collective_rpc(\"determine_num_available_blocks\")\n  File \"/usr/local/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 49, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/utils.py\", line 2208, in run_method\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/worker/worker.py\", line 228, in determine_num_available_blocks\n    self.model_runner.profile_run()\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1236, in profile_run\n    self._dummy_run(max_num_batched_tokens, max_num_seqs)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1301, in _dummy_run\n    .dummy_data_for_profiling(self.model_config,\n  File \"/usr/local/lib/python3.10/site-packages/vllm/inputs/registry.py\", line 333, in dummy_data_for_profiling\n    dummy_data = profiler.get_dummy_data(seq_len)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/profiling.py\", line 161, in get_dummy_data\n    mm_inputs = self._get_dummy_mm_inputs(seq_len, mm_counts)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/profiling.py\", line 139, in _get_dummy_mm_inputs\n    return self.processor.apply(\n  File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/minicpmv.py\", line 803, in apply\n    result = super().apply(prompt, mm_data, hf_processor_mm_kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 1230, in apply\n    hf_mm_placeholders = self._find_mm_placeholders(\n  File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 793, in _find_mm_placeholders\n    return find_mm_placeholders(mm_prompt_repls, new_token_ids,\n  File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 579, in find_mm_placeholders\n    return dict(full_groupby_modality(it))\n  File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 184, in full_groupby_modality\n    return full_groupby(values, key=lambda x: x.modality)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/utils.py\", line 873, in full_groupby\n    for value in values:\n  File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 534, in _iter_placeholders\n    replacement = repl_info.get_replacement(item_idx)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 270, in get_replacement\n    replacement = replacement(item_idx)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/minicpmo.py\", line 354, in get_replacement_minicpmv\n    return self.get_audio_prompt_texts(\n  File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/minicpmo.py\", line 231, in get_audio_prompt_texts\n    return self.info.get_hf_processor().get_audio_placeholder(\nAttributeError: 'MiniCPMOProcessor' object has no attribute 'get_audio_placeholder'\n[rank0]:[W206 18:02:01.559899899 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n\n\n### \ud83d\udc1b Describe the bug\n\nvLLM API server version 0.7.1\nINFO 02-06 18:01:41 api_server.py:839] args: Namespace(subparser='serve', model_tag='/xiaobaogong/ai/model/MiniCPM-o-2_6/', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='/xiaobaogong/ai/model/MiniCPM-o-2_6/', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=2048, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=40, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7f9e8360b760>)\nINFO 02-06 18:01:41 api_server.py:204] Started engine process with PID 68\nINFO 02-06 18:01:44 __init__.py:183] Automatically detected platform cuda.\nINFO 02-06 18:01:47 config.py:526] This model supports multiple tasks: {'classify', 'reward', 'score', 'embed', 'generate'}. Defaulting to 'generate'.\nINFO 02-06 18:01:51 config.py:526] This model supports multiple tasks: {'embed', 'score', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.\nINFO 02-06 18:01:51 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='/xiaobaogong/ai/model/MiniCPM-o-2_6/', speculative_config=None, tokenizer='/xiaobaogong/ai/model/MiniCPM-o-2_6/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/xiaobaogong/ai/model/MiniCPM-o-2_6/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[40,32,24,16,8,4,2,1],\"max_capture_size\":40}, use_cached_outputs=True, \nINFO 02-06 18:01:53 cuda.py:235] Using Flash Attention backend.\nINFO 02-06 18:01:53 model_runner.py:1111] Starting to load model /xiaobaogong/ai/model/MiniCPM-o-2_6/...\nINFO 02-06 18:01:54 cuda.py:219] Cannot use FlashAttention-2 backend for head size 72.\nINFO 02-06 18:01:54 cuda.py:232] Using XFormers backend.\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.49it/s]\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.19it/s]\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.19it/s]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.16it/s]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.19it/s]\n\nINFO 02-06 18:01:58 model_runner.py:1116] Loading model weights took 15.7985 GB\n/usr/local/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nERROR 02-06 18:02:00 engine.py:387] 'MiniCPMOProcessor' object has no attribute 'get_audio_placeholder'\nERROR 02-06 18:02:00 engine.py:387] Traceback (most recent call last):\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 378, in run_mp_engine\nERROR 02-06 18:02:00 engine.py:387]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 121, in from_engine_args\nERROR 02-06 18:02:00 engine.py:387]     return cls(ipc_path=ipc_path,\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 73, in __init__\nERROR 02-06 18:02:00 engine.py:387]     self.engine = LLMEngine(*args, **kwargs)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 274, in __init__\nERROR 02-06 18:02:00 engine.py:387]     self._initialize_kv_caches()\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 414, in _initialize_kv_caches\nERROR 02-06 18:02:00 engine.py:387]     self.model_executor.determine_num_available_blocks())\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 99, in determine_num_available_blocks\nERROR 02-06 18:02:00 engine.py:387]     results = self.collective_rpc(\"determine_num_available_blocks\")\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 49, in collective_rpc\nERROR 02-06 18:02:00 engine.py:387]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/utils.py\", line 2208, in run_method\nERROR 02-06 18:02:00 engine.py:387]     return func(*args, **kwargs)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 02-06 18:02:00 engine.py:387]     return func(*args, **kwargs)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/worker/worker.py\", line 228, in determine_num_available_blocks\nERROR 02-06 18:02:00 engine.py:387]     self.model_runner.profile_run()\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 02-06 18:02:00 engine.py:387]     return func(*args, **kwargs)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1236, in profile_run\nERROR 02-06 18:02:00 engine.py:387]     self._dummy_run(max_num_batched_tokens, max_num_seqs)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1301, in _dummy_run\nERROR 02-06 18:02:00 engine.py:387]     .dummy_data_for_profiling(self.model_config,\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/inputs/registry.py\", line 333, in dummy_data_for_profiling\nERROR 02-06 18:02:00 engine.py:387]     dummy_data = profiler.get_dummy_data(seq_len)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/profiling.py\", line 161, in get_dummy_data\nERROR 02-06 18:02:00 engine.py:387]     mm_inputs = self._get_dummy_mm_inputs(seq_len, mm_counts)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/profiling.py\", line 139, in _get_dummy_mm_inputs\nERROR 02-06 18:02:00 engine.py:387]     return self.processor.apply(\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/minicpmv.py\", line 803, in apply\nERROR 02-06 18:02:00 engine.py:387]     result = super().apply(prompt, mm_data, hf_processor_mm_kwargs)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 1230, in apply\nERROR 02-06 18:02:00 engine.py:387]     hf_mm_placeholders = self._find_mm_placeholders(\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 793, in _find_mm_placeholders\nERROR 02-06 18:02:00 engine.py:387]     return find_mm_placeholders(mm_prompt_repls, new_token_ids,\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 579, in find_mm_placeholders\nERROR 02-06 18:02:00 engine.py:387]     return dict(full_groupby_modality(it))\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 184, in full_groupby_modality\nERROR 02-06 18:02:00 engine.py:387]     return full_groupby(values, key=lambda x: x.modality)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/utils.py\", line 873, in full_groupby\nERROR 02-06 18:02:00 engine.py:387]     for value in values:\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 534, in _iter_placeholders\nERROR 02-06 18:02:00 engine.py:387]     replacement = repl_info.get_replacement(item_idx)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 270, in get_replacement\nERROR 02-06 18:02:00 engine.py:387]     replacement = replacement(item_idx)\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/minicpmo.py\", line 354, in get_replacement_minicpmv\nERROR 02-06 18:02:00 engine.py:387]     return self.get_audio_prompt_texts(\nERROR 02-06 18:02:00 engine.py:387]   File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/minicpmo.py\", line 231, in get_audio_prompt_texts\nERROR 02-06 18:02:00 engine.py:387]     return self.info.get_hf_processor().get_audio_placeholder(\nERROR 02-06 18:02:00 engine.py:387] AttributeError: 'MiniCPMOProcessor' object has no attribute 'get_audio_placeholder'\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/usr/local/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 389, in run_mp_engine\n    raise e\n  File \"/usr/local/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 378, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n  File \"/usr/local/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 121, in from_engine_args\n    return cls(ipc_path=ipc_path,\n  File \"/usr/local/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 73, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 274, in __init__\n    self._initialize_kv_caches()\n  File \"/usr/local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 414, in _initialize_kv_caches\n    self.model_executor.determine_num_available_blocks())\n  File \"/usr/local/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 99, in determine_num_available_blocks\n    results = self.collective_rpc(\"determine_num_available_blocks\")\n  File \"/usr/local/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 49, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/utils.py\", line 2208, in run_method\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/worker/worker.py\", line 228, in determine_num_available_blocks\n    self.model_runner.profile_run()\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1236, in profile_run\n    self._dummy_run(max_num_batched_tokens, max_num_seqs)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1301, in _dummy_run\n    .dummy_data_for_profiling(self.model_config,\n  File \"/usr/local/lib/python3.10/site-packages/vllm/inputs/registry.py\", line 333, in dummy_data_for_profiling\n    dummy_data = profiler.get_dummy_data(seq_len)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/profiling.py\", line 161, in get_dummy_data\n    mm_inputs = self._get_dummy_mm_inputs(seq_len, mm_counts)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/profiling.py\", line 139, in _get_dummy_mm_inputs\n    return self.processor.apply(\n  File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/minicpmv.py\", line 803, in apply\n    result = super().apply(prompt, mm_data, hf_processor_mm_kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 1230, in apply\n    hf_mm_placeholders = self._find_mm_placeholders(\n  File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 793, in _find_mm_placeholders\n    return find_mm_placeholders(mm_prompt_repls, new_token_ids,\n  File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 579, in find_mm_placeholders\n    return dict(full_groupby_modality(it))\n  File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 184, in full_groupby_modality\n    return full_groupby(values, key=lambda x: x.modality)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/utils.py\", line 873, in full_groupby\n    for value in values:\n  File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 534, in _iter_placeholders\n    replacement = repl_info.get_replacement(item_idx)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/multimodal/processing.py\", line 270, in get_replacement\n    replacement = replacement(item_idx)\n  File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/minicpmo.py\", line 354, in get_replacement_minicpmv\n    return self.get_audio_prompt_texts(\n  File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/minicpmo.py\", line 231, in get_audio_prompt_texts\n    return self.info.get_hf_processor().get_audio_placeholder(\nAttributeError: 'MiniCPMOProcessor' object has no attribute 'get_audio_placeholder'\n[rank0]:[W206 18:02:01.559899899 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-02-06T10:02:43+00:00",
    "closed_at": "2025-02-28T06:56:33+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12820/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12820"
  },
  {
    "number": 8204,
    "title": "[Bug]: [Errno 98] error while attempting to bind on address ('0.0.0.0', 8000): address already in use",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nThis is a bug we encounter a lot in our ci, e.g. https://buildkite.com/vllm/ci-aws/builds/8098#0191bf43-446d-411d-80c7-3ba10bc392e8/192-1557\r\n\r\nI have been tracking this for months, and try to add more logging information to help debugging.\r\n\r\nfrom the logging information:\r\n\r\n\r\n> [2024-09-05T00:38:34Z] INFO:     Started server process [60858]\r\n> --\r\n> \u00a0 | [2024-09-05T00:38:34Z] INFO:     Waiting for application startup.\r\n> \u00a0 | [2024-09-05T00:38:34Z] INFO:     Application startup complete.\r\n> \u00a0 | [2024-09-05T00:38:34Z] ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 44319): [errno 98] address already in use\r\n> \u00a0 | [2024-09-05T00:38:34Z] INFO:     Waiting for application shutdown.\r\n> \u00a0 | [2024-09-05T00:38:34Z] INFO:     Application shutdown complete.\r\n> \u00a0 | [2024-09-05T00:38:34Z] DEBUG 09-04 17:38:34 launcher.py:64] port 44319 is used by process psutil.Process(pid=60914, name='pt_main_thread', status='sleeping', started='17:37:05') launched with command:\r\n> \u00a0 | [2024-09-05T00:38:34Z] DEBUG 09-04 17:38:34 launcher.py:64] /usr/bin/python3 -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=16, pipe_handle=18) --multiprocessing-fork\r\n> \r\n> \r\n\r\nwe can see that the server process is pid 60858 , and the port 44319 is used by process 60914. scrolling up a little bit, we can find:\r\n\r\n\r\n> [2024-09-05T00:37:05Z] INFO 09-04 17:37:05 api_server.py:160] Multiprocessing frontend to use ipc:///tmp/b6851f4d-4d78-46b8-baba-ae179b0088c2 for RPC Path.\r\n> --\r\n> \u00a0 | [2024-09-05T00:37:05Z] INFO 09-04 17:37:05 api_server.py:176] Started engine process with PID 60914\r\n> \r\n\r\nit becomes clear that this is the engine process.\r\n\r\nI think the problem here, is that we only bind the port after the engine is ready. During engine setup, it might use some ports for ray, or for distributed communication.\r\n\r\nthere are two possible solutions:\r\n1. the api server immediately binds to the port after start, and returns unready status when client queries the `/healthy` endpoint\r\n2. the api server binds the port immediately (via `socket.socket(socket.AF_INET, socket.SOCK_STREAM).bind((\"\", uvicorn_kwargs[\"port\"]))`), and after engine is up, it releases the port, and bind again to serve requests\r\n\r\nI think 1 might be better. 2 would suffer from the fact that client will get 404 not found before the engine is up, because this is just a raw socket without any response.\r\n\r\ncc @robertgshaw2-neuralmagic @njhill @joerunde \r\n\r\nalso cc @richardliaw @rkooo567 how to turn on verbose ray logging, so that we can verify if the port is indeed used by ray.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-09-05T17:35:19+00:00",
    "closed_at": "2024-09-16T20:56:29+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8204/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/8204"
  },
  {
    "number": 14239,
    "title": "[Misc]: [V1] prompt logprobs + chunked prefill can result in `EngineCore` partial prefill output",
    "body": "See https://github.com/vllm-project/vllm/blob/4f5b059f146adeecd153fa781cf21863ed6679d8/vllm/v1/engine/output_processor.py#L277\n\nPrompt logprobs + chunked prefill can result in engine core returning an output for a partial prefill (in order to send back partial prompt logprobs.) This breaks the invariant that process_outputs is only operating on engine core outputs associated with non-partial completions. Currently this is handled by having `is_prefilling` in `OutputProcessor` check for new decoded tokens, indicating that the completion is not partial.\n\nA follow-up PR should aggregate partial prompt logprobs in the EngineCore.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2025-03-04T22:43:32+00:00",
    "closed_at": "2025-03-24T16:42:01+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14239/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14239"
  },
  {
    "number": 9186,
    "title": "[Feature]: Output state configuration of vision encoder In VLM",
    "body": "### Anything you want to discuss about vllm.\n\nWhen siglip or clip acts as a multimodal vision encoder,  there will have several cases:\r\n- The output state of an intermediate layer is used without layer normalization\r\n- The output state of the last layer is used without layer normalization\r\n- The output state of the last layer is used with layer normalization\r\n\r\nFor example, In the `LLaVA-Next` code implementation, `post_layernorm` is not used.\r\n\r\n#8106 #8155\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-10-09T08:50:00+00:00",
    "closed_at": "2024-10-23T11:27:38+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9186/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/9186"
  },
  {
    "number": 69,
    "title": "Decrease the default size of swap space",
    "body": "The current default swap space size (20 GiB per GPU) is a bit too large. It can lead to OOM especially for the machine with multiple GPUs.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-05-04T09:54:55+00:00",
    "closed_at": "2023-05-24T01:22:27+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/69/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/69"
  },
  {
    "number": 12754,
    "title": "[Bug]: vllm v1: RuntimeError: Cannot re-initialize CUDA in forked subprocess",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 02-04 23:23:46 __init__.py:186] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.31.4\nLibc version: glibc-2.35\n\nPython version: 3.12.8 (main, Jan 14 2025, 22:49:14) [Clang 19.1.6 ] (64-bit runtime)\nPython platform: Linux-5.15.0-122-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H200\nNvidia driver version: 550.90.12\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               44\nOn-line CPU(s) list:                  0-43\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 9654 96-Core Processor\nCPU family:                           25\nModel:                                17\nThread(s) per core:                   1\nCore(s) per socket:                   1\nSocket(s):                            44\nStepping:                             1\nBogoMIPS:                             4792.79\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm rep_good nopl cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw perfctr_core invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr wbnoinvd arat npt lbrv nrip_save tsc_scale vmcb_clean flushbyasid pausefilter pfthreshold v_vmsave_vmload vgif avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm flush_l1d arch_capabilities\nVirtualization:                       AMD-V\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            2.8 MiB (44 instances)\nL1i cache:                            2.8 MiB (44 instances)\nL2 cache:                             22 MiB (44 instances)\nL3 cache:                             704 MiB (44 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-43\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] mypy==1.11.1\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] sentence-transformers==3.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.2\n[pip3] transformers-stream-generator==0.0.5\n[pip3] triton==3.1.0\n[pip3] tritonclient==2.51.0\n[pip3] vector-quantize-pytorch==1.21.2\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.2.dev36+g18016a5e\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \t0-43\t0\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nvllm v1 seems broken in tot.\n\nEven tinyllama fails to run.\n\n```\n$ export VLLM_USE_V1=1\n$ python -m vllm.entrypoints.openai.api_server --model TinyLlama/TinyLlama-1.1B-Chat-v1.0\n\nINFO 02-04 23:21:26 __init__.py:186] Automatically detected platform cuda.\nINFO 02-04 23:21:31 api_server.py:840] vLLM API server version 0.7.2.dev36+g18016a5e\nINFO 02-04 23:21:31 api_server.py:841] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='TinyLlama/TinyLlama-1.1B-Chat-v1.0', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)\nWARNING 02-04 23:21:31 arg_utils.py:1326] Setting max_num_batched_tokens to 8192 for OPENAI_API_SERVER usage context.\nINFO 02-04 23:21:44 config.py:542] This model supports multiple tasks: {'generate', 'embed', 'reward', 'score', 'classify'}. Defaulting to 'generate'.\nINFO 02-04 23:21:44 config.py:1557] Chunked prefill is enabled with max_num_batched_tokens=8192.\nINFO 02-04 23:21:45 core.py:47] Initializing a V1 LLM engine (v0.7.2.dev36+g18016a5e) with config: model='TinyLlama/TinyLlama-1.1B-Chat-v1.0', speculative_config=None, tokenizer='TinyLlama/TinyLlama-1.1B-Chat-v1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=TinyLlama/TinyLlama-1.1B-Chat-v1.0, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\nERROR 02-04 23:21:45 core.py:210] EngineCore hit an exception: Traceback (most recent call last):\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/vllm/v1/engine/core.py\", line 202, in run_engine_core\nERROR 02-04 23:21:45 core.py:210]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 02-04 23:21:45 core.py:210]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/vllm/v1/engine/core.py\", line 156, in __init__\nERROR 02-04 23:21:45 core.py:210]     super().__init__(vllm_config, executor_class)\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/vllm/v1/engine/core.py\", line 51, in __init__\nERROR 02-04 23:21:45 core.py:210]     self.model_executor = executor_class(vllm_config)\nERROR 02-04 23:21:45 core.py:210]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/vllm/executor/executor_base.py\", line 51, in __init__\nERROR 02-04 23:21:45 core.py:210]     self._init_executor()\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/vllm/executor/uniproc_executor.py\", line 41, in _init_executor\nERROR 02-04 23:21:45 core.py:210]     self.collective_rpc(\"init_device\")\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/vllm/executor/uniproc_executor.py\", line 51, in collective_rpc\nERROR 02-04 23:21:45 core.py:210]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 02-04 23:21:45 core.py:210]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/vllm/utils.py\", line 2210, in run_method\nERROR 02-04 23:21:45 core.py:210]     return func(*args, **kwargs)\nERROR 02-04 23:21:45 core.py:210]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/vllm/v1/worker/gpu_worker.py\", line 113, in init_device\nERROR 02-04 23:21:45 core.py:210]     torch.cuda.set_device(self.device)\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 478, in set_device\nERROR 02-04 23:21:45 core.py:210]     torch._C._cuda_setDevice(device)\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\nERROR 02-04 23:21:45 core.py:210]     raise RuntimeError(\nERROR 02-04 23:21:45 core.py:210] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\nERROR 02-04 23:21:45 core.py:210]\nCRITICAL 02-04 23:21:45 core_client.py:158] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\nKilled\n\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-02-04T23:27:14+00:00",
    "closed_at": "2025-02-13T18:30:01+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12754/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/12754"
  },
  {
    "number": 6471,
    "title": "[Feature]: Pipeline parallelism support for qwen model",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nPipeline parallelism is only supported for the following architectures: ['AquilaModel', 'AquilaForCausalLM', 'InternLMForCausalLM', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'Phi3ForCausalLM', 'GPT2LMHeadModel'].\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-07-16T11:32:20+00:00",
    "closed_at": "2024-08-01T19:41:07+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6471/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6471"
  },
  {
    "number": 15186,
    "title": "[New Model]: please surport  model   https://huggingface.co/Skywork/Skywork-R1V-38B",
    "body": "### The model to consider.\n\nhttps://huggingface.co/Skywork/Skywork-R1V-38B\n\n### The closest model vllm already supports.\n\nhttps://huggingface.co/Skywork/Skywork-R1V-38B\n\n### What's your difficulty of supporting the model you want?\n\nhttps://huggingface.co/Skywork/Skywork-R1V-38B\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model",
      "multi-modality"
    ],
    "state": "closed",
    "created_at": "2025-03-20T04:43:12+00:00",
    "closed_at": "2025-03-29T03:39:22+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15186/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15186"
  },
  {
    "number": 10758,
    "title": "[Feature]: ChatCompletionRequest get default value from generation_config.json",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\ntemperature/top_k/top_p These values \u200b\u200bwill affect the model output\uff0cThe default value should be read from generation_config.json if the user does not set it.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-11-29T03:14:25+00:00",
    "closed_at": "2024-12-19T10:50:40+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10758/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/10758"
  },
  {
    "number": 11899,
    "title": "[Bug]: VLLM get stucks with Qwen VL 7B",
    "body": "### Your current environment\r\n\r\nI'm using [\"v0.6.5\"] of VLLM.\r\nWhen I try to launch  Qwen VL 7B with 100% of the GPU (24GB VRAM) it's ok.\r\nThen even if the model is only 4GB when I reduce to a little bit less the launch of VLLM is getting stuck by printing an endless: 'INFO: 127.0.0.6:XXX - \"GET /metrics HTTP/1.1\" 200 OK'\r\nI'm confused because I know that I have enough space for the model.\r\n\r\n```\r\n      vllm serve Qwen/Qwen2-VL-7B-Instruct-AWQ --trust-remote-code --enable-chunked-prefill --max_model_len 4096 --quantization awq_marlin --gpu_memory_utilization=0.8 --max-num-batched-tokens 4097 --kv-cache-dtype fp8_e4m3\r\n\r\n```\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nINFO 01-09 06:23:59 api_server.py:651] vLLM API server version 0.6.5\r\nINFO 01-09 06:23:59 api_server.py:652] args: Namespace(subparser='serve', model_tag='Qwen/Qwen2-VL-7B-Instruct-AWQ', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2-VL-7B-Instruct-AWQ', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='fp8_e4m3', quantization_param_path=None, max_model_len=4096, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.8, num_gpu_blocks_override=None, max_num_batched_tokens=4097, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization='awq_marlin', rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, mm_cache_preprocessor=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=True, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7c62441bcea0>)\r\nINFO 01-09 06:23:59 api_server.py:199] Started engine process with PID 38\r\nINFO 01-09 06:24:07 config.py:478] This model supports multiple tasks: {'reward', 'classify', 'score', 'embed', 'generate'}. Defaulting to 'generate'.\r\nINFO 01-09 06:24:08 awq_marlin.py:109] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\r\nINFO 01-09 06:24:08 config.py:925] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor\r\nINFO 01-09 06:24:08 config.py:1364] Chunked prefill is enabled with max_num_batched_tokens=4097.\r\nINFO 01-09 06:24:12 config.py:478] This model supports multiple tasks: {'classify', 'generate', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\r\nINFO 01-09 06:24:13 awq_marlin.py:109] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\r\nINFO 01-09 06:24:13 config.py:925] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor\r\nINFO 01-09 06:24:13 config.py:1364] Chunked prefill is enabled with max_num_batched_tokens=4097.\r\nINFO 01-09 06:24:13 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='Qwen/Qwen2-VL-7B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2-VL-7B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=fp8_e4m3, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2-VL-7B-Instruct-AWQ, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True,\r\nINFO 01-09 06:24:14 selector.py:227] Cannot use FlashAttention-2 backend for FP8 KV cache.\r\nWARNING 01-09 06:24:14 selector.py:229] Please use FlashInfer backend with FP8 KV Cache for better performance by setting environment variable  VLLM_ATTENTION_BACKEND=FLASHINFER\r\nINFO 01-09 06:24:14 selector.py:129] Using XFormers backend.\r\nINFO 01-09 06:24:15 model_runner.py:1092] Starting to load model Qwen/Qwen2-VL-7B-Instruct-AWQ...\r\nWARNING 01-09 06:24:15 utils.py:624] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\r\nINFO 01-09 06:24:15 weight_utils.py:243] Using model weights format ['*.safetensors']\r\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.85s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.31s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.39s/it]\r\nINFO 01-09 06:24:19 model_runner.py:1097] Loading model weights took 6.4651 GB\r\nINFO 01-09 06:24:22 worker.py:241] Memory profiling takes 2.64 seconds\r\nINFO 01-09 06:24:22 worker.py:241] the current vLLM instance can use total_gpu_memory (21.95GiB) x gpu_memory_utilization (0.80) = 17.56GiB\r\nINFO 01-09 06:24:22 worker.py:241] model weights take 6.47GiB; non_torch_memory takes 0.33GiB; PyTorch activation peak memory takes 0.74GiB; the rest of the memory reserved for KV Cache is 10.02GiB.\r\nINFO 01-09 06:24:22 gpu_executor.py:76] # GPU blocks: 23460, # CPU blocks: 9362\r\nINFO 01-09 06:24:22 gpu_executor.py:80] Maximum concurrency for 4096 tokens per request: 91.64x\r\nINFO 01-09 06:24:26 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 01-09 06:24:26 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\nINFO 01-09 06:24:42 model_runner.py:1527] Graph capturing finished in 17 secs, took 0.42 GiB\r\nINFO 01-09 06:24:42 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 23.34 seconds\r\nINFO 01-09 06:24:43 api_server.py:586] Using supplied chat template:\r\nINFO 01-09 06:24:43 api_server.py:586] None\r\nINFO 01-09 06:24:43 launcher.py:19] Available routes are:\r\nINFO 01-09 06:24:43 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\r\nINFO 01-09 06:24:43 launcher.py:27] Route: /docs, Methods: GET, HEAD\r\nINFO 01-09 06:24:43 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\r\nINFO 01-09 06:24:43 launcher.py:27] Route: /redoc, Methods: GET, HEAD\r\nINFO 01-09 06:24:43 launcher.py:27] Route: /health, Methods: GET\r\nINFO 01-09 06:24:43 launcher.py:27] Route: /tokenize, Methods: POST\r\nINFO 01-09 06:24:43 launcher.py:27] Route: /detokenize, Methods: POST\r\nINFO 01-09 06:24:43 launcher.py:27] Route: /v1/models, Methods: GET\r\nINFO 01-09 06:24:43 launcher.py:27] Route: /version, Methods: GET\r\nINFO 01-09 06:24:43 launcher.py:27] Route: /v1/chat/completions, Methods: POST\r\nINFO 01-09 06:24:43 launcher.py:27] Route: /v1/completions, Methods: POST\r\nINFO 01-09 06:24:43 launcher.py:27] Route: /v1/embeddings, Methods: POST\r\nINFO 01-09 06:24:43 launcher.py:27] Route: /score, Methods: POST\r\nINFO 01-09 06:24:43 launcher.py:27] Route: /v1/score, Methods: POST\r\nINFO:     Started server process [1]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\r\nINFO:     127.0.0.6:39795 - \"GET /metrics HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.6:52875 - \"GET /metrics HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.6:43741 - \"GET /metrics HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.6:39081 - \"GET /metrics HTTP/1.1\" 200 OK\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-01-09T14:40:25+00:00",
    "closed_at": "2025-01-20T14:29:50+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11899/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11899"
  },
  {
    "number": 7856,
    "title": "[Bug]: minicpmv2_6 OOM",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: CentOS Linux 7 (Core) (x86_64)\r\nGCC version: (GCC) 9.3.1 20200408 (Red Hat 9.3.1-2)\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.17\r\n\r\nPython version: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21)  [GCC 12.3.0] (64-bit runtime)\r\nPython platform: Linux-4.18.0-147.mt20200626.413.el8_1.x86_64-x86_64-with-glibc2.17\r\nIs CUDA available: True\r\nCUDA runtime version: 11.8.89\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA L40\r\nNvidia driver version: 535.129.03\r\ncuDNN version: Probably one of the following:\r\n/usr/lib64/libcudnn.so.8.9.2\r\n/usr/lib64/libcudnn_adv_infer.so.8.9.2\r\n/usr/lib64/libcudnn_adv_train.so.8.9.2\r\n/usr/lib64/libcudnn_cnn_infer.so.8.9.2\r\n/usr/lib64/libcudnn_cnn_train.so.8.9.2\r\n/usr/lib64/libcudnn_ops_infer.so.8.9.2\r\n/usr/lib64/libcudnn_ops_train.so.8.9.2\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   52 bits physical, 57 bits virtual\r\nCPU(s):                          192\r\nOn-line CPU(s) list:             0-22\r\nOff-line CPU(s) list:            23-191\r\nThread(s) per core:              0\r\nCore(s) per socket:              48\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           143\r\nModel name:                      Intel(R) Xeon(R) Platinum 8468V\r\nStepping:                        8\r\nCPU MHz:                         2900.000\r\nCPU max MHz:                     3800.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        4805.30\r\nVirtualization:                  VT-x\r\nL1d cache:                       2.3 MiB\r\nL1i cache:                       1.5 MiB\r\nL2 cache:                        96 MiB\r\nL3 cache:                        97.5 MiB\r\nNUMA node0 CPU(s):               0-47,96-143\r\nNUMA node1 CPU(s):               48-95,144-191\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid cldemote movdiri movdir64b md_clear pconfig flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu11==11.10.3.66\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu11==11.7.99\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu11==11.7.99\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu11==8.5.0.96\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.555.43\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.20\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.1.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.2\r\n[pip3] triton==3.0.0\r\n[conda] numpy                     2.0.1                    pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.6.20                  pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.44.2                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.4@e20233d361b4e6a7cb8e37c6d7f85e9900527802\r\nvLLM Build Flags:\r\nCUDA Archs: ; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tNIC0\tNIC1\tNIC2\tNIC3\tNIC4\tNIC5\tNIC6\tNIC7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\t\t\t\tN/A\r\nNIC0\tSYS\t X \tPIX\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\r\nNIC1\tSYS\tPIX\t X \tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\r\nNIC2\tSYS\tSYS\tSYS\t X \tPIX\tSYS\tSYS\tSYS\tSYS\r\nNIC3\tSYS\tSYS\tSYS\tPIX\t X \tSYS\tSYS\tSYS\tSYS\r\nNIC4\tPXB\tSYS\tSYS\tSYS\tSYS\t X \tPIX\tSYS\tSYS\r\nNIC5\tPXB\tSYS\tSYS\tSYS\tSYS\tPIX\t X \tSYS\tSYS\r\nNIC6\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPIX\r\nNIC7\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPIX\t X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n```\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nmy code:\r\n```python\r\ndef run_minicpmv(question):\r\n\r\n    # 2.0\r\n    # The official repo doesn't work yet, so we need to use a fork for now\r\n    # For more details, please see: See: https://github.com/vllm-project/vllm/pull/4087#issuecomment-2250397630 # noqa\r\n    # model_name = \"HwwwH/MiniCPM-V-2\"\r\n\r\n    # 2.5\r\n    # model_name = \"openbmb/MiniCPM-Llama3-V-2_5\"\r\n\r\n    #2.6\r\n    model_name_or_path = \"/home/hadoop-platcv/minicpm-v-v2_6/checkpoint-2709-merged\"\r\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\r\n                                              trust_remote_code=True)\r\n    llm = LLM(\r\n        model=model_name_or_path,\r\n        trust_remote_code=True,\r\n        # quantization=\"fp8\",\r\n        # gpu_memory_utilization=0.5,\r\n        enforce_eager=True,\r\n        # swap_space=30,\r\n        # cpu_offload_gb=30,\r\n    )\r\n    # NOTE The stop_token_ids are different for various versions of MiniCPM-V\r\n    # 2.0\r\n    # stop_token_ids = [tokenizer.eos_id]\r\n\r\n    # 2.5\r\n    # stop_token_ids = [tokenizer.eos_id, tokenizer.eot_id]\r\n\r\n    # 2.6\r\n    stop_tokens = ['<|im_end|>', '<|endoftext|>']\r\n    stop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\r\n\r\n    messages = [{\r\n        'role': 'user',\r\n        'content': f'(<image>./</image>)\\n{question}'\r\n    }]\r\n    prompt = tokenizer.apply_chat_template(messages,\r\n                                           tokenize=False,\r\n                                           add_generation_prompt=True)\r\n    return llm, prompt, stop_token_ids\r\n\r\n\r\ndef main():\r\n    question = '\u8be6\u7ec6\u63cf\u8ff0\u56fe\u7247\u5185\u5bb9'\r\n    llm, prompt, stop_token_ids = run_minicpmv(question)\r\n    image = Image.open('../demo.png').convert('RGB')\r\n    sampling_params = SamplingParams(temperature=1.0,\r\n                                     max_tokens=128,\r\n                                     stop_token_ids=stop_token_ids)\r\n\r\n\r\n    # Single inference\r\n    inputs = {\r\n        \"prompt\": prompt,\r\n        \"multi_modal_data\": {\r\n            \"image\": image\r\n        },\r\n    }\r\n    outputs = llm.generate(inputs, sampling_params=sampling_params)\r\n\r\n    for o in outputs:\r\n        generated_text = o.outputs[0].text\r\n        print(generated_text)\r\n\r\n```\r\nPyTorch only occupies 16GB GPU memory, but vllm occurs OOM, details as follows:\r\n```sh\r\n[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 44.32 GiB of which 15.81 GiB is free. Process 48868 has 28.50 GiB memory in use. Of the allocated memory 27.67 GiB is allocated by PyTorch, and 335.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-08-26T03:20:31+00:00",
    "closed_at": "2024-09-02T04:28:00+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7856/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7856"
  },
  {
    "number": 8401,
    "title": "[Usage]: how to use openai compatible api to run GGUF model?",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-09-12T06:26:12+00:00",
    "closed_at": "2024-09-19T19:15:56+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8401/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8401"
  },
  {
    "number": 12058,
    "title": "[Usage]: Running Tensor Parallel on TPUs on Ray Cluster",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\nThe output of `python collect_env.py`\n(test_hf_qwen pid=17527, ip=10.130.4.26) Environment Information:\n(test_hf_qwen pid=17527, ip=10.130.4.26) Collecting environment information...\n(test_hf_qwen pid=17527, ip=10.130.4.26) PyTorch version: 2.6.0.dev20241126+cpu\n(test_hf_qwen pid=17527, ip=10.130.4.26) Is debug build: False\n(test_hf_qwen pid=17527, ip=10.130.4.26) CUDA used to build PyTorch: None\n(test_hf_qwen pid=17527, ip=10.130.4.26) ROCM used to build PyTorch: N/A\n(test_hf_qwen pid=17527, ip=10.130.4.26) \n(test_hf_qwen pid=17527, ip=10.130.4.26) OS: Ubuntu 22.04.4 LTS (x86_64)\n(test_hf_qwen pid=17527, ip=10.130.4.26) GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n(test_hf_qwen pid=17527, ip=10.130.4.26) Clang version: 14.0.0-1ubuntu1.1\n(test_hf_qwen pid=17527, ip=10.130.4.26) CMake version: version 3.31.2\n(test_hf_qwen pid=17527, ip=10.130.4.26) Libc version: glibc-2.35\n(test_hf_qwen pid=17527, ip=10.130.4.26) \n(test_hf_qwen pid=17527, ip=10.130.4.26) Python version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\n(test_hf_qwen pid=17527, ip=10.130.4.26) Python platform: Linux-5.19.0-1022-gcp-x86_64-with-glibc2.35\n(test_hf_qwen pid=17527, ip=10.130.4.26) Is CUDA available: False\n(test_hf_qwen pid=17527, ip=10.130.4.26) CUDA runtime version: No CUDA\n(test_hf_qwen pid=17527, ip=10.130.4.26) CUDA_MODULE_LOADING set to: N/A\n(test_hf_qwen pid=17527, ip=10.130.4.26) GPU models and configuration: No CUDA\n(test_hf_qwen pid=17527, ip=10.130.4.26) Nvidia driver version: No CUDA\n(test_hf_qwen pid=17527, ip=10.130.4.26) cuDNN version: No CUDA\n(test_hf_qwen pid=17527, ip=10.130.4.26) HIP runtime version: N/A\n(test_hf_qwen pid=17527, ip=10.130.4.26) MIOpen runtime version: N/A\n(test_hf_qwen pid=17527, ip=10.130.4.26) Is XNNPACK available: True\n(test_hf_qwen pid=17527, ip=10.130.4.26) \n(test_hf_qwen pid=17527, ip=10.130.4.26) CPU:\n(test_hf_qwen pid=17527, ip=10.130.4.26) Architecture:                    x86_64\n(test_hf_qwen pid=17527, ip=10.130.4.26) CPU op-mode(s):                  32-bit, 64-bit\n(test_hf_qwen pid=17527, ip=10.130.4.26) Address sizes:                   48 bits physical, 48 bits virtual\n(test_hf_qwen pid=17527, ip=10.130.4.26) Byte Order:                      Little Endian\n(test_hf_qwen pid=17527, ip=10.130.4.26) CPU(s):                          240\n(test_hf_qwen pid=17527, ip=10.130.4.26) On-line CPU(s) list:             0-239\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vendor ID:                       AuthenticAMD\n(test_hf_qwen pid=17527, ip=10.130.4.26) Model name:                      AMD EPYC 7B12\n(test_hf_qwen pid=17527, ip=10.130.4.26) CPU family:                      23\n(test_hf_qwen pid=17527, ip=10.130.4.26) Model:                           49\n(test_hf_qwen pid=17527, ip=10.130.4.26) Thread(s) per core:              2\n(test_hf_qwen pid=17527, ip=10.130.4.26) Core(s) per socket:              60\n(test_hf_qwen pid=17527, ip=10.130.4.26) Socket(s):                       2\n(test_hf_qwen pid=17527, ip=10.130.4.26) Stepping:                        0\n(test_hf_qwen pid=17527, ip=10.130.4.26) BogoMIPS:                        4499.99\n(test_hf_qwen pid=17527, ip=10.130.4.26) Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid\n(test_hf_qwen pid=17527, ip=10.130.4.26) Hypervisor vendor:               KVM\n(test_hf_qwen pid=17527, ip=10.130.4.26) Virtualization type:             full\n(test_hf_qwen pid=17527, ip=10.130.4.26) L1d cache:                       3.8 MiB (120 instances)\n(test_hf_qwen pid=17527, ip=10.130.4.26) L1i cache:                       3.8 MiB (120 instances)\n(test_hf_qwen pid=17527, ip=10.130.4.26) L2 cache:                        60 MiB (120 instances)\n(test_hf_qwen pid=17527, ip=10.130.4.26) L3 cache:                        480 MiB (30 instances)\n(test_hf_qwen pid=17527, ip=10.130.4.26) NUMA node(s):                    2\n(test_hf_qwen pid=17527, ip=10.130.4.26) NUMA node0 CPU(s):               0-59,120-179\n(test_hf_qwen pid=17527, ip=10.130.4.26) NUMA node1 CPU(s):               60-119,180-239\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Itlb multihit:     Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability L1tf:              Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Mds:               Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Meltdown:          Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Mmio stale data:   Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Retbleed:          Mitigation; untrained return thunk; SMT enabled with STIBP protection\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Srbds:             Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Tsx async abort:   Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) \n(test_hf_qwen pid=17527, ip=10.130.4.26) Versions of relevant libraries:\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] mypy-extensions==1.0.0\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] numpy==1.26.4\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cublas-cu12==12.4.5.8\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cuda-cupti-cu12==12.4.127\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cuda-runtime-cu12==12.4.127\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cudnn-cu12==9.1.0.70\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cufft-cu12==11.2.1.3\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-curand-cu12==10.3.5.147\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cusolver-cu12==11.6.1.9\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cusparse-cu12==12.3.1.170\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-nccl-cu12==2.21.5\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-nvjitlink-cu12==12.4.127\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-nvtx-cu12==12.4.127\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] pyzmq==26.2.0\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] torch==2.6.0.dev20241126+cpu\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] torch-xla==2.6.0+git39e67b5\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] torchvision==0.20.0.dev20241126+cpu\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] transformers==4.47.1\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] triton==3.1.0\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] numpy                     1.26.4                   pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] pyzmq                     26.2.0                   pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] torch                     2.6.0.dev20241126+cpu          pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] torch-xla                 2.6.0+git39e67b5          pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] torchvision               0.20.0.dev20241126+cpu          pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] transformers              4.47.1                   pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] triton                    3.1.0                    pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) ROCM Version: Could not collect\n(test_hf_qwen pid=17527, ip=10.130.4.26) Neuron SDK Version: N/A\n(test_hf_qwen pid=17527, ip=10.130.4.26) vLLM Version: N/A (dev)\n(test_hf_qwen pid=17527, ip=10.130.4.26) vLLM Build Flags:\n(test_hf_qwen pid=17527, ip=10.130.4.26) CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\n(test_hf_qwen pid=17527, ip=10.130.4.26) GPU Topology:\n(test_hf_qwen pid=17527, ip=10.130.4.26) Could not collect\n(test_hf_qwen pid=17527, ip=10.130.4.26) \n(test_hf_qwen pid=17527, ip=10.130.4.26) LD_LIBRARY_PATH=/home/ray/anaconda3/lib/python3.11/site-packages/cv2/../../lib64:/home/ray/anaconda3/lib/python3.11/site-packages/cv2/../../lib64::/usr/lib/x86_64-linux-gnu/:/home/ray/anaconda3/lib\n(test_hf_qwen pid=17527, ip=10.130.4.26) OMP_NUM_THREADS=1\n(test_hf_qwen pid=17527, ip=10.130.4.26) CUDA_VISIBLE_DEVICES=\n(test_hf_qwen pid=17527, ip=10.130.4.26) CUDA_VISIBLE_DEVICES=\n(test_hf_qwen pid=17527, ip=10.130.4.26) TORCHINDUCTOR_COMPILE_THREADS=1\n(test_hf_qwen pid=17527, ip=10.130.4.26) TORCHINDUCTOR_CACHE_DIR=/tmp/torchinductor_ray\n```\n\n\n### How would you like to use vllm\n\nI want to run tensor-parallel inference using TPUs in a ray cluster. It seems like the Ray cluster picks up the accelerator that we need but then when vllm tries to initialize the ray cluster, it doesn't know that, so it doesn't reuse the TPUs that the cluster has already picked up. I was wondering how people would implement this? Thanks!\n\n\nCode:\n```\nfrom vllm import LLM\n\n@ray.remote(resources={\"TPU\": 4, \"TPU-v4-8-head\": 1})\ndef test():\n    llm = LLM(model=Qwen/Qwen2.5-7B-Instruct, enforce_eager=True, max_model_len=8192, tensor_parallel_size=4)\n```\n\nError:\n```\n(test_hf pid=1616, ip=10.130.0.8) INFO 01-15 09:04:53 config.py:510] This model supports multiple tasks: {'generate', 'score', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.\n(test_hf pid=1616, ip=10.130.0.8) Connecting to existing Ray cluster at address: 10.130.2.110:6379...\n(test_hf pid=1616, ip=10.130.0.8) Calling ray.init() again after it has already been called.\nTraceback (most recent call last):\n  File \"/tmp/ray/session_2025-01-09_10-24-52_724484_545/runtime_resources/working_dir_files/_ray_pkg_0e6ac5e67e3f89c8/experiments/llama_test.py\", line 75, in <module>\n    raise e\n  File \"/tmp/ray/session_2025-01-09_10-24-52_724484_545/runtime_resources/working_dir_files/_ray_pkg_0e6ac5e67e3f89c8/experiments/llama_test.py\", line 73, in <module>\n    ray.get(ref)\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n    raise value.as_instanceof_cause()\nray.exceptions.RayTaskError(ValueError): ray::test_hf() (pid=1616, ip=10.130.0.8)\n  File \"/tmp/ray/session_2025-01-09_10-24-52_724484_545/runtime_resources/working_dir_files/_ray_pkg_0e6ac5e67e3f89c8/experiments/llama_test.py\", line 59, in test_hf\n    classifier = AutoClassifier.from_model_path(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ray/session_2025-01-09_10-24-52_724484_545/runtime_resources/working_dir_files/_ray_pkg_0e6ac5e67e3f89c8/marin/processing/classification/classifier.py\", line 281, in from_model_path\n    return cls._MODEL_NAME_TO_CLS_DICT[key](model_name_or_path, attribute_name, model_type, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ray/session_2025-01-09_10-24-52_724484_545/runtime_resources/working_dir_files/_ray_pkg_0e6ac5e67e3f89c8/marin/processing/classification/classifier.py\", line 213, in __init__\n    self.llm = LLM(model=model_name, enforce_eager=True, max_model_len=8192, tensor_parallel_size=4)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm-0.6.6.post1/vllm/utils.py\", line 986, in inner\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm-0.6.6.post1/vllm/entrypoints/llm.py\", line 230, in __init__\n    self.llm_engine = self.engine_class.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm-0.6.6.post1/vllm/engine/llm_engine.py\", line 515, in from_engine_args\n    executor_class = cls._get_executor_cls(engine_config)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm-0.6.6.post1/vllm/engine/llm_engine.py\", line 453, in _get_executor_cls\n    initialize_ray_cluster(engine_config.parallel_config)\n  File \"/opt/vllm/vllm-0.6.6.post1/vllm/executor/ray_utils.py\", line 300, in initialize_ray_cluster\n    raise ValueError(\nValueError: Current node has no TPU available. current_node_resource={'ray-marin-us-central2-worker-2c310153-tpu': 1.0, 'CPU': 118.0, 'memory': 328490690150.0, 'object_store_memory': 32641751449.0, 'accelerator_type:TPU-V4': 1.0, 'node:10.130.0.8': 1.0}. vLLM engine cannot start without TPU. Make sure you have at least 1 TPU available in a node current_node_id='70354097fbebce320701224b766747b2c30936f9c1edf1d930d7723b' current_ip='10.130.0.8'.\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "tpu",
      "ray"
    ],
    "state": "closed",
    "created_at": "2025-01-14T21:32:12+00:00",
    "closed_at": "2025-01-24T05:41:50+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12058/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12058"
  },
  {
    "number": 19673,
    "title": "[New Model]: Support BAAI/bge-reranker-v2-gemma model",
    "body": "### The model to consider.\n\nhttps://huggingface.co/BAAI/bge-reranker-v2-gemma\n\n### The closest model vllm already supports.\n\nBAAI/bge-reranker-v2-m3\ngoogle/gemma-2b\n\n### What's your difficulty of supporting the model you want?\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2025-06-16T04:19:03+00:00",
    "closed_at": "2025-07-07T14:46:05+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19673/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19673"
  },
  {
    "number": 19259,
    "title": "[Bug]: 'dict' object has no attribute 'is_kv_transfer_instance'",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n- run vllm container\n```\ndocker run -d -it --rm --privileged --entrypoint /bin/bash --network host --name poolv1-mbl-test-2  --shm-size 512g --gpus all   -v /:/disc       vllm/vllm-openai:v0.9.0.1\n\ndocker exec -it poolv1-mbl-test-2 bash\npip install lmcache\n```\n\n- start vllm by lmcache example.\n\nThe following python script is copied from `examples/others/lmcache/cpu_offload_lmcache.py` and did some minor changes to run model locally.\n```python\n# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\"\"\"\nThis file demonstrates the example usage of cpu offloading\nwith LMCache in vLLM v1 or v0.\n\nUsage:\n\n    Specify vLLM version\n\n    -v v0 : Use LMCacheConnector\n            model = mistralai/Mistral-7B-Instruct-v0.2\n            (Includes enable_chunked_prefill = True)\n\n    -v v1 : Use LMCacheConnectorV1 (default)\n            model = meta-llama/Meta-Llama-3.1-8B-Instruct\n            (Without enable_chunked_prefill)\n\nNote that `lmcache` is needed to run this example.\nRequirements: Linux, Python: 3.10 or higher, CUDA: 12.1\nLearn more about LMCache environment setup, please refer to:\nhttps://docs.lmcache.ai/getting_started/installation.html\n\"\"\"\n\nimport argparse\nimport contextlib\nimport os\nimport time\nfrom dataclasses import asdict\n\nfrom lmcache.v1.cache_engine import LMCacheEngineBuilder\nfrom lmcache.integration.vllm.utils import ENGINE_NAME\n\nfrom vllm import LLM, SamplingParams\nfrom vllm.config import KVTransferConfig\nfrom vllm.engine.arg_utils import EngineArgs\n\n\ndef setup_environment_variables(vllm_version: str):\n    # LMCache-related environment variables\n    # Use experimental features in LMCache\n    os.environ[\"LMCACHE_USE_EXPERIMENTAL\"] = \"True\"\n    # LMCache is set to use 256 tokens per chunk\n    os.environ[\"LMCACHE_CHUNK_SIZE\"] = \"256\"\n    # Enable local CPU backend in LMCache\n    os.environ[\"LMCACHE_LOCAL_CPU\"] = \"True\"\n    # Set local CPU memory limit to 5.0 GB\n    os.environ[\"LMCACHE_MAX_LOCAL_CPU_SIZE\"] = \"5.0\"\n    if vllm_version == \"v0\":\n        os.environ[\"VLLM_USE_V1\"] = \"0\"\n\n\n@contextlib.contextmanager\ndef build_llm_with_lmcache(lmcache_connector: str, model: str, vllm_version: str):\n    ktc = KVTransferConfig(\n        kv_connector=lmcache_connector,\n        kv_role=\"kv_both\",\n    )\n    # Set GPU memory utilization to 0.8 for an A40 GPU with 40GB\n    # memory. Reduce the value if your GPU has less memory.\n    # Note: LMCache supports chunked prefill (see vLLM#14505, LMCache#392).\n    if vllm_version == \"v0\":\n        llm_args = EngineArgs(\n            model=model,\n            kv_transfer_config=ktc,\n            max_model_len=8000,\n            gpu_memory_utilization=0.8,\n            enable_chunked_prefill=True,  # Only in v0\n            trust_remote_code=True,\n        )\n    else:\n        llm_args = EngineArgs(\n            model=model,\n            kv_transfer_config=ktc,\n            max_model_len=8000,\n            gpu_memory_utilization=0.8,\n            trust_remote_code=True,\n        )\n\n    llm = LLM(**asdict(llm_args))\n    try:\n        yield llm\n    finally:\n        # Clean up lmcache backend\n        LMCacheEngineBuilder.destroy(ENGINE_NAME)\n\n\ndef print_output(\n    llm: LLM,\n    prompt: list[str],\n    sampling_params: SamplingParams,\n    req_str: str,\n):\n    # Should be able to see logs like the following:\n    # `LMCache INFO: Storing KV cache for 6006 out of 6006 tokens for request 0`\n    # This indicates that the KV cache has been stored in LMCache.\n    start = time.time()\n    outputs = llm.generate(prompt, sampling_params)\n    print(\"-\" * 50)\n    for output in outputs:\n        generated_text = output.outputs[0].text\n        print(f\"Generated text: {generated_text!r}\")\n    print(f\"Generation took {time.time() - start:.2f} seconds, {req_str} request done.\")\n    print(\"-\" * 50)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-v\",\n        \"--version\",\n        choices=[\"v0\", \"v1\"],\n        default=\"v1\",\n        help=\"Specify vLLM version (default: v1)\",\n    )\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    if args.version == \"v0\":\n        lmcache_connector = \"LMCacheConnector\"\n        model = \"/disc/data1/deepseek/DeepSeek-V2-Lite-Chat\"\n    else:\n        lmcache_connector = \"LMCacheConnectorV1\"\n        model = \"/disc/data1/deepseek/DeepSeek-V2-Lite-Chat\"\n\n    setup_environment_variables(args.version)\n\n    with build_llm_with_lmcache(lmcache_connector, model, args.version) as llm:\n        # This example script runs two requests with a shared prefix.\n        # Define the shared prompt and specific prompts\n        shared_prompt = \"Hello, how are you?\" * 1000\n        first_prompt = [\n            shared_prompt + \"Hello, my name is\",\n        ]\n        second_prompt = [\n            shared_prompt + \"Tell me a very long story\",\n        ]\n\n        sampling_params = SamplingParams(temperature=0, top_p=0.95, max_tokens=10)\n\n        # Print the first output\n        print_output(llm, first_prompt, sampling_params, \"first\")\n\n        time.sleep(1)\n\n        # print the second output\n        print_output(llm, second_prompt, sampling_params, \"second\")\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n- Execute the python example.\n```\nLMCACHE_REMOTE_SERDE=\"naive\"  python3 cpu_offload_lmcache.py\n```\n\n- Output\n```\nERROR 06-06 00:28:27 [core.py:500] EngineCore failed to start.\nERROR 06-06 00:28:27 [core.py:500] Traceback (most recent call last):\nERROR 06-06 00:28:27 [core.py:500]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 491, in run_engine_core\nERROR 06-06 00:28:27 [core.py:500]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 06-06 00:28:27 [core.py:500]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-06 00:28:27 [core.py:500]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 390, in __init__\nERROR 06-06 00:28:27 [core.py:500]     super().__init__(vllm_config, executor_class, log_stats,\nERROR 06-06 00:28:27 [core.py:500]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 71, in __init__\nERROR 06-06 00:28:27 [core.py:500]     self.model_executor = executor_class(vllm_config)\nERROR 06-06 00:28:27 [core.py:500]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-06 00:28:27 [core.py:500]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 52, in __init__\nERROR 06-06 00:28:27 [core.py:500]     self._init_executor()\nERROR 06-06 00:28:27 [core.py:500]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 46, in _init_executor\nERROR 06-06 00:28:27 [core.py:500]     self.collective_rpc(\"init_device\")\nERROR 06-06 00:28:27 [core.py:500]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\nERROR 06-06 00:28:27 [core.py:500]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 06-06 00:28:27 [core.py:500]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-06 00:28:27 [core.py:500]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2605, in run_method\nERROR 06-06 00:28:27 [core.py:500]     return func(*args, **kwargs)\nERROR 06-06 00:28:27 [core.py:500]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 06-06 00:28:27 [core.py:500]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 604, in init_device\nERROR 06-06 00:28:27 [core.py:500]     self.worker.init_device()  # type: ignore\nERROR 06-06 00:28:27 [core.py:500]     ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-06 00:28:27 [core.py:500]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 137, in init_device\nERROR 06-06 00:28:27 [core.py:500]     init_worker_distributed_environment(self.vllm_config, self.rank,\nERROR 06-06 00:28:27 [core.py:500]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 353, in init_worker_distributed_environment\nERROR 06-06 00:28:27 [core.py:500]     ensure_kv_transfer_initialized(vllm_config)\nERROR 06-06 00:28:27 [core.py:500]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_transfer_state.py\", line 61, in ensure_kv_transfer_initialized\nERROR 06-06 00:28:27 [core.py:500]     if (vllm_config.kv_transfer_config.is_kv_transfer_instance\nERROR 06-06 00:28:27 [core.py:500]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-06 00:28:27 [core.py:500] AttributeError: 'dict' object has no attribute 'is_kv_transfer_instance'\nProcess EngineCore_0:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 504, in run_engine_core\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 491, in run_engine_core\n    engine_core = EngineCoreProc(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 390, in __init__\n    super().__init__(vllm_config, executor_class, log_stats,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 71, in __init__\n    self.model_executor = executor_class(vllm_config)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 52, in __init__\n    self._init_executor()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 46, in _init_executor\n    self.collective_rpc(\"init_device\")\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2605, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 604, in init_device\n    self.worker.init_device()  # type: ignore\n    ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 137, in init_device\n    init_worker_distributed_environment(self.vllm_config, self.rank,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 353, in init_worker_distributed_environment\n    ensure_kv_transfer_initialized(vllm_config)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_transfer_state.py\", line 61, in ensure_kv_transfer_initialized\n    if (vllm_config.kv_transfer_config.is_kv_transfer_instance\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'dict' object has no attribute 'is_kv_transfer_instance'\nTraceback (most recent call last):\n  File \"/disc/data1/baoloongmao/lmcache_whl/20250606/blend_kv_v1/cpu_offload_lmcache.py\", line 154, in <module>\n    main()\n  File \"/disc/data1/baoloongmao/lmcache_whl/20250606/blend_kv_v1/cpu_offload_lmcache.py\", line 131, in main\n    with build_llm_with_lmcache(lmcache_connector, model, args.version) as llm:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 137, in __enter__\n    return next(self.gen)\n           ^^^^^^^^^^^^^^\n  File \"/disc/data1/baoloongmao/lmcache_whl/20250606/blend_kv_v1/cpu_offload_lmcache.py\", line 80, in build_llm_with_lmcache\n    llm = LLM(**asdict(llm_args))\n          ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 1183, in inner\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py\", line 255, in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 501, in from_engine_args\n    return engine_cls.from_vllm_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/llm_engine.py\", line 123, in from_vllm_config\n    return cls(vllm_config=vllm_config,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/llm_engine.py\", line 100, in __init__\n    self.engine_core = EngineCoreClient.make_client(\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 75, in make_client\n    return SyncMPClient(vllm_config, executor_class, log_stats)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 580, in __init__\n    super().__init__(\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 418, in __init__\n    self._wait_for_engine_startup(output_address, parallel_config)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 484, in _wait_for_engine_startup\n    raise RuntimeError(\"Engine core initialization failed. \"\nRuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-06-06T07:36:09+00:00",
    "closed_at": "2025-06-14T19:32:08+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19259/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19259"
  },
  {
    "number": 18835,
    "title": "[Bug]: Help, RuntimeError: CUDA error: no kernel image is available for execution on the device",
    "body": "### Your current environment\n\n\nERROR 05-28 19:38:44 [dump_input.py:68] Dumping input data\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 207, in execute_model\n    return self.model_executor.execute_model(scheduler_output)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 86, in execute_model\n    output = self.collective_rpc(\"execute_model\",\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/vllm/utils.py\", line 2605, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 276, in execute_model\n    output = self.model_runner.execute_model(scheduler_output,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1196, in execute_model\n    model_output = self.model(\n                   ^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1767, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1778, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py\", line 300, in forward\n    hidden_states = self.model(input_ids, positions, intermediate_tensors,\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 245, in __call__\n    model_output = self.forward(*args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py\", line 340, in forward\n    def forward(\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 372, in __call__\n    return super().__call__(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1767, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1778, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 893, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 840, in call_wrapped\n    return self._wrapped_call(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 416, in __call__\n    raise e\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 403, in __call__\n    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1767, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1778, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<eval_with_key>.74\", line 304, in forward\n    submod_1 = self.submod_1(getitem, s72, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 840, in call_wrapped\n    return self._wrapped_call(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 416, in __call__\n    raise e\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 403, in __call__\n    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1767, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1778, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<eval_with_key>.2\", line 5, in forward\n    unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_1, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_1 = unified_attention_with_output = None\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/_ops.py\", line 1208, in __call__\n    return self._op(*args, **(kwargs or {}))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/vllm/attention/layer.py\", line 425, in unified_attention_with_output\n    self.impl.forward(self,\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/vllm/v1/attention/backends/flash_attn.py\", line 622, in forward\n    flash_attn_varlen_func(\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 227, in flash_attn_varlen_func\n    out, softmax_lse = torch.ops._vllm_fa2_C.varlen_fwd(\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages/torch/_ops.py\", line 1208, in __call__\n    return self._op(*args, **(kwargs or {}))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\n\n### run:\n  CUDA_VISIBLE_DEVICES=0 vllm serve qwen3-4B/ --served-model-name Qwen3-4B  --port 11435 --gpu-memory-utilization 0.5 --host 0.0.0.0  --enable-auto-tool-choice  --tool-call-parser hermes --max-model-len 8192\n\n\n### **python collect_env.py**\n\nINFO 05-28 19:51:57 [__init__.py:243] Automatically detected platform cuda.\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : Could not collect\nCMake version                : version 3.22.1\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.8.0.dev20250527+cu128\nIs debug build               : False\nCUDA used to build PyTorch   : 12.8\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.10 | packaged by conda-forge | (main, Apr 10 2025, 22:21:13) [GCC 13.3.0] (64-bit runtime)\nPython platform              : Linux-6.8.0-59-generic-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.8.61\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : GPU 0: NVIDIA GeForce RTX 5090\nNvidia driver version        : 570.144\ncuDNN version                : Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.10.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.10.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.10.1\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.10.1\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.10.1\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.10.1\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.10.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.10.1\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\n\u67b6\u6784\uff1a                                x86_64\nCPU \u8fd0\u884c\u6a21\u5f0f\uff1a                        32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\n\u5b57\u8282\u5e8f\uff1a                              Little Endian\nCPU:                                  28\n\u5728\u7ebf CPU \u5217\u8868\uff1a                       0-27\n\u5382\u5546 ID\uff1a                             GenuineIntel\n\u578b\u53f7\u540d\u79f0\uff1a                            Intel(R) Xeon(R) Platinum 8336C CPU @ 2.30GHz\nCPU \u7cfb\u5217\uff1a                            6\n\u578b\u53f7\uff1a                                106\n\u6bcf\u4e2a\u6838\u7684\u7ebf\u7a0b\u6570\uff1a                      1\n\u6bcf\u4e2a\u5ea7\u7684\u6838\u6570\uff1a                        28\n\u5ea7\uff1a                                  1\n\u6b65\u8fdb\uff1a                                6\nBogoMIPS\uff1a                            4599.95\n\u6807\u8bb0\uff1a                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq dtes64 vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd arat vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm md_clear flush_l1d arch_capabilities\n\u865a\u62df\u5316\uff1a                              VT-x\n\u8d85\u7ba1\u7406\u5668\u5382\u5546\uff1a                        KVM\n\u865a\u62df\u5316\u7c7b\u578b\uff1a                          \u5b8c\u5168\nL1d \u7f13\u5b58\uff1a                            896 KiB (28 instances)\nL1i \u7f13\u5b58\uff1a                            896 KiB (28 instances)\nL2 \u7f13\u5b58\uff1a                             112 MiB (28 instances)\nL3 \u7f13\u5b58\uff1a                             16 MiB (1 instance)\nNUMA \u8282\u70b9\uff1a                           1\nNUMA \u8282\u70b90 CPU\uff1a                      0-27\nVulnerability Gather data sampling:   Unknown: Dependent on hypervisor status\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT Host state unknown\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Mitigation; TSX disabled\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.1.2\n[pip3] nvidia-cublas-cu12==12.8.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.8.90\n[pip3] nvidia-cuda-nvrtc-cu12==12.8.93\n[pip3] nvidia-cuda-runtime-cu12==12.8.90\n[pip3] nvidia-cudnn-cu12==9.8.0.87\n[pip3] nvidia-cufft-cu12==11.3.3.83\n[pip3] nvidia-cufile-cu12==1.13.1.3\n[pip3] nvidia-curand-cu12==10.3.9.90\n[pip3] nvidia-cusolver-cu12==11.7.3.90\n[pip3] nvidia-cusparse-cu12==12.5.8.93\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.5\n[pip3] nvidia-nvjitlink-cu12==12.8.93\n[pip3] nvidia-nvtx-cu12==12.8.90\n[pip3] pytorch-triton==3.3.1+gitc8757738\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.8.0.dev20250527+cu128\n[pip3] torchaudio==2.6.0.dev20250527+cu128\n[pip3] torchvision==0.22.0.dev20250527+cu128\n[pip3] transformers==4.52.3\n[pip3] triton==3.3.0\n[conda] numpy                     2.1.2                    pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.8.4.1                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.8.90                  pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.8.93                  pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.8.90                  pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.8.0.87                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.3.3.83                pypi_0    pypi\n[conda] nvidia-cufile-cu12        1.13.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.9.90                pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.7.3.90                pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.5.8.93                pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.3                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.26.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.8.93                  pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.8.90                  pypi_0    pypi\n[conda] pytorch-triton            3.3.1+gitc8757738          pypi_0    pypi\n[conda] pyzmq                     26.4.0                   pypi_0    pypi\n[conda] torch                     2.8.0.dev20250527+cu128          pypi_0    pypi\n[conda] torchaudio                2.6.0.dev20250527+cu128          pypi_0    pypi\n[conda] torchvision               0.22.0.dev20250527+cu128          pypi_0    pypi\n[conda] transformers              4.52.3                   pypi_0    pypi\n[conda] triton                    3.3.0                    pypi_0    pypi\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.0\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n  \tGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \t0-27\t0\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n==============================\n     Environment Variables\n==============================\nLD_LIBRARY_PATH=/usr/local/cuda-12.8/lib64:/usr/local/cuda-12.8/lib64:/usr/local/cuda-12.8/lib64:\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n\n\n\n\n\n\n\n### **environment**\n\n### python --version\nPython 3.12.10\n\n### pip show vllm\nName: vllm\nVersion: 0.9.0\nSummary: A high-throughput and memory-efficient inference and serving engine for LLMs\nHome-page: https://github.com/vllm-project/vllm\nAuthor: vLLM Team\nAuthor-email: \nLicense-Expression: Apache-2.0\nLocation: /root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages\nRequires: aiohttp, blake3, cachetools, cloudpickle, compressed-tensors, depyf, einops, fastapi, filelock, gguf, huggingface-hub, lark, llguidance, lm-format-enforcer, mistral_common, msgspec, ninja, numba, numpy, openai, opencv-python-headless, opentelemetry-api, opentelemetry-exporter-otlp, opentelemetry-sdk, opentelemetry-semantic-conventions-ai, outlines, partial-json-parser, pillow, prometheus-fastapi-instrumentator, prometheus_client, protobuf, psutil, py-cpuinfo, pydantic, python-json-logger, pyyaml, pyzmq, ray, regex, requests, scipy, sentencepiece, setuptools, six, tiktoken, tokenizers, torch, torchaudio, torchvision, tqdm, transformers, typing_extensions, watchfiles, xformers, xgrammar\nRequired-by:\n\n### nvcc -V:\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2025 NVIDIA Corporation\nBuilt on Wed_Jan_15_19:20:09_PST_2025\nCuda compilation tools, release 12.8, V12.8.61\nBuild cuda_12.8.r12.8/compiler.35404655_0\n\n### pip show torch\nName: torch\nVersion: 2.8.0.dev20250527+cu128\nSummary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\nHome-page: https://pytorch.org/\nAuthor: PyTorch Team\nAuthor-email: packages@pytorch.org\nLicense: BSD-3-Clause\nLocation: /root/miniconda3/envs/vllm-qwen3/lib/python3.12/site-packages\nRequires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-cufile-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-cusparselt-cu12, nvidia-nccl-cu12, nvidia-nvjitlink-cu12, nvidia-nvtx-cu12, pytorch-triton, setuptools, sympy, typing-extensions\nRequired-by: compressed-tensors, outlines, torchaudio, torchvision, vllm, xformers, xgrammar\n\n### nvidia-smi:\n| NVIDIA-SMI 570.144                Driver Version: 570.144        CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 5090        Off |   00000000:06:10.0 Off |                  N/A |\n|  0%   51C    P8             17W /  575W |       3MiB /  32607MiB |      0%      Default |\n|                                         |                        |                  N/A |\n\n### \ud83d\udc1b Describe the bug\n\n### run  \nCUDA_VISIBLE_DEVICES=0 vllm serve qwen3-4B/ --served-model-name Qwen3-4B  --port 11435 --gpu-memory-utilization 0.5 --host 0.0.0.0  --enable-auto-tool-choice  --tool-call-parser hermes --max-model-len 8192\n\n### output\nRuntimeError: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-05-28T11:59:00+00:00",
    "closed_at": "2025-06-26T17:16:43+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18835/reactions",
      "total_count": 9,
      "+1": 9,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/18835"
  },
  {
    "number": 767,
    "title": "CUDA error: an illegal memory acces with Falcon 40B",
    "body": "Hi,\r\nI am testing different models with vllm. I see \r\n```CUDA error: an illegal memory access``` when I use falcon 40 b. The code I use is \r\n```\r\nllm = LLM(model=ckpt_dir,tensor_parallel_size=4,trust_remote_code=True,gpu_memory_utilization=0.8)\r\nsampling_params = SamplingParams(temperature=0, top_p=1.0, max_tokens=300)\r\nresults = llm.generate(prompts, sampling_params)\r\n```\r\nI am using an A100 with 4 GPUs. Please let me know if you have any questions",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-08-16T00:11:11+00:00",
    "closed_at": "2023-09-10T08:39:04+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/767/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/767"
  }
]
[
  {
    "number": 11905,
    "title": "[Feature]: Support Multiple Tasks Per Model",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\r\n\r\nRequesting this for **V1** #11862 \r\n\r\nThe idea is pretty simple, it would be nice to be able to, e.g., get generations and embeddings out of a single model. An example use case is when you have a LoRA for generation and a LoRA for embedding on top of the same base model. Deploying two vLLM servers is really inefficient for accomplishing this. \r\n\r\n### Alternatives\r\n\r\nA lesser feature would be one task per LoRA, but it's better to be general if possible.\r\n\r\n### Additional context\r\n\r\n_No response_\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-01-09T18:22:13+00:00",
    "closed_at": null,
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11905/reactions",
      "total_count": 16,
      "+1": 16,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11905"
  },
  {
    "number": 11345,
    "title": "[Performance]: 1P1D Disaggregation performance",
    "body": "### Proposal to improve performance\r\n\r\nI try to reproduce the P&D 1P1D benchmark to compare performance with chunked prefill  https://github.com/vllm-project/vllm/blob/main/benchmarks/disagg_benchmarks/disagg_performance_benchmark.sh. TTFL is higher than what I expected. Because the overhead benchmark only shows ~20-30ms level. What's more, seems ITL is also much higher than chunked prefill. \r\n\r\n- GPU device: 2* L40S.\r\n- Model: Qwen/Qwen2.5-7B-Instruct\r\n- Parameters: gpu-memory-utilization 0.6 + kv_buffer_size 10e9 \r\n- dataset input 1024 output 50.\r\n\r\n/cc @KuntaiDu \r\n\r\n\r\n### Report of performance regression\r\n\r\n![image](https://github.com/user-attachments/assets/2c5ec50f-1e5b-48c6-aca2-ab0be42935ed)\r\n\r\n### Misc discussion on performance\r\n\r\n_No response_\r\n\r\n### Your current environment (if you think it is necessary)\r\n\r\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance"
    ],
    "state": "open",
    "created_at": "2024-12-19T18:37:57+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11345/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11345"
  },
  {
    "number": 5898,
    "title": "[Bug]: Inconsistent Responses with VLLM When Batch Size > 1 even temperature = 0",
    "body": "### \ud83d\udc1b Describe the bug\r\n\r\nTest Environment:\r\n- Hardware: A100 80GB GPU\r\n- Model: Llama3-8b \r\n**- Parameters: temperature = 0, max_tokens = 1024, max_num_seqs = 256, seed=1**\r\n- I make OpenAI-Compatilbe Server using python -m vllm.entrypoints.openai.api_server  ...\r\n\r\nTest Method:\r\n- First, requests were sent one at a time to verify if the same prompt consistently produces the same response.\r\n- Second, more than one requests with same prompt at a time were sent  to verify if the same prompt consistently produces the same response.\r\n\r\nDiscovered Issue:\r\n**- Consistency with Single Request: When the batch size is 1, the same prompt consistently produces the same response.**\r\n**- Inconsistency with Multiple Requests: When the batch size increases to more than 1, the responses for the same prompt become inconsistent.**\r\n- When I set vllm server with parameter max_num_seqs = 1, the result is all same\r\n\r\nI think this suggests that the issue arises from the way the Batch Scheduler processes multiple requests together.\r\n\r\nMy problem is similar with others\r\nhttps://github.com/vllm-project/vllm/issues/3544\r\nhttps://github.com/vllm-project/vllm/issues/4606\r\nhttps://github.com/vllm-project/vllm/issues/608\r\n\r\n\r\n\r\n\r\n\r\n",
    "labels": [
      "bug",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-06-27T09:34:57+00:00",
    "closed_at": null,
    "comments": 36,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5898/reactions",
      "total_count": 5,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 5
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5898"
  },
  {
    "number": 9451,
    "title": "[Feature]: Consider parallel_tool_calls parameter at the API level",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently, there is a [parallel_tool_calls](https://github.com/vllm-project/vllm/blob/18b296fdb2248e8a65bf005e7193ebd523b875b6/vllm/entrypoints/openai/protocol.py#L177) field that is part of the `ChatCompletionRequest` pydantic class. However, this field is only there for being compatible with OpenAI's API.\r\n\r\nIn other words, it's not being used at all according to the documentation or the code:\r\n\r\n```\r\n# NOTE this will be ignored by VLLM -- the model determines the behavior\r\nparallel_tool_calls: Optional[bool] = False\r\n```\r\n\r\nWould it be possible to consider implementing the logic behind this field for different model families. For instance, in the case of llama3.1-8b-insturct, tool calling works, but the model ends up returning three tool calls instead of one by one.\r\nThis makes me lose compatibility with frameworks like LangGraph.\r\n\r\nHere's an example request and response:\r\n\r\n**Request**\r\n```\r\n{\r\n  \"messages\": [\r\n    {\r\n      \"content\": \"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\",\r\n      \"role\": \"system\"\r\n    },\r\n    {\r\n      \"content\": \"Add 3 and 4. Multiply the output by 2. Divide the output by 5\",\r\n      \"role\": \"user\"\r\n    }\r\n  ],\r\n  \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\r\n  \"stream\": false,\r\n  \"n\": 1,\r\n  \"temperature\": 0.0,\r\n  \"max_tokens\": 256,\r\n  \"tools\": [\r\n    {\r\n      \"type\": \"function\",\r\n      \"function\": {\r\n        \"name\": \"add\",\r\n        \"description\": \"Adds a and b.\",\r\n        \"parameters\": {\r\n          \"properties\": {\r\n            \"a\": {\r\n              \"description\": \"first int\",\r\n              \"type\": \"integer\"\r\n            },\r\n            \"b\": {\r\n              \"description\": \"second int\",\r\n              \"type\": \"integer\"\r\n            }\r\n          },\r\n          \"required\": [\"a\", \"b\"],\r\n          \"type\": \"object\"\r\n        }\r\n      }\r\n    },\r\n    {\r\n      \"type\": \"function\",\r\n      \"function\": {\r\n        \"name\": \"multiply\",\r\n        \"description\": \"Multiply a and b.\",\r\n        \"parameters\": {\r\n          \"properties\": {\r\n            \"a\": {\r\n              \"description\": \"first int\",\r\n              \"type\": \"integer\"\r\n            },\r\n            \"b\": {\r\n              \"description\": \"second int\",\r\n              \"type\": \"integer\"\r\n            }\r\n          },\r\n          \"required\": [\"a\", \"b\"],\r\n          \"type\": \"object\"\r\n        }\r\n      }\r\n    },\r\n    {\r\n      \"type\": \"function\",\r\n      \"function\": {\r\n        \"name\": \"divide\",\r\n        \"description\": \"Divide a and b.\",\r\n        \"parameters\": {\r\n          \"properties\": {\r\n            \"a\": {\r\n              \"description\": \"first int\",\r\n              \"type\": \"integer\"\r\n            },\r\n            \"b\": {\r\n              \"description\": \"second int\",\r\n              \"type\": \"integer\"\r\n            }\r\n          },\r\n          \"required\": [\"a\", \"b\"],\r\n          \"type\": \"object\"\r\n        }\r\n      }\r\n    }\r\n  ],\r\n  \"parallel_tool_calls\": false\r\n}\r\n```\r\n\r\n**Response**\r\n```\r\n{\r\n  \"ChatCompletion\": {\r\n    \"id\": \"chat-32cb47446c5b471eba5c91be1755811e\",\r\n    \"choices\": [\r\n      {\r\n        \"finish_reason\": \"tool_calls\",\r\n        \"index\": 0,\r\n        \"logprobs\": null,\r\n        \"message\": {\r\n          \"content\": null,\r\n          \"refusal\": null,\r\n          \"role\": \"assistant\",\r\n          \"function_call\": null,\r\n          \"tool_calls\": [\r\n            {\r\n              \"id\": \"chatcmpl-tool-f8c832f4a42445f899a229063004cae9\",\r\n              \"function\": {\r\n                \"arguments\": '{\"a\": 3, \"b\": 4}',\r\n                \"name\": \"add\"\r\n              },\r\n              \"type\": \"function\"\r\n            },\r\n            {\r\n              \"id\": \"chatcmpl-tool-4b44f70f7dde47d0820f8a3b9018b897\",\r\n              \"function\": {\r\n                \"arguments\": '{\"a\": 7, \"b\": 2}',\r\n                \"name\": \"multiply\"\r\n              },\r\n              \"type\": \"function\"\r\n            },\r\n            {\r\n              \"id\": \"chatcmpl-tool-d897bd7ecb4b44e59eb718aff21cbfa8\",\r\n              \"function\": {\r\n                \"arguments\": '{\"a\": 14, \"b\": 5}',\r\n                \"name\": \"divide\"\r\n              },\r\n              \"type\": \"function\"\r\n            }\r\n          ]\r\n        },\r\n        \"stop_reason\": 128008\r\n      }\r\n    ],\r\n    \"created\": 1729149431,\r\n    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\r\n    \"object\": \"chat.completion\",\r\n    \"service_tier\": null,\r\n    \"system_fingerprint\": null,\r\n    \"usage\": {\r\n      \"completion_tokens\": 67,\r\n      \"prompt_tokens\": 466,\r\n      \"total_tokens\": 533,\r\n      \"completion_tokens_details\": null,\r\n      \"prompt_tokens_details\": null\r\n    },\r\n    \"prompt_logprobs\": null\r\n  }\r\n}\r\n```\r\n\r\nEven if I wanted to do a posterior call to the model using the three tool calls at the same time, it will complain with an error of: \r\n\r\n`BadRequestError: Error code: 400 - {'object': 'error', 'message': 'This model only supports single tool-calls at once!', 'type': 'BadRequestError', 'param': None, 'code': 400}`\r\n\r\nWhich comes from this [llama3_json](https://gist.github.com/K-Mistele/820d142b4dab50bd8ef0c7bbcad4515c#file-tool_chat_template_llama31_json-jinja-L48) template.\r\n\r\nThank you Team!.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2024-10-17T07:41:26+00:00",
    "closed_at": null,
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9451/reactions",
      "total_count": 6,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9451"
  },
  {
    "number": 6918,
    "title": "[Bug]:  error: Segmentation fault(SIGSEGV received at time)",
    "body": "### Your current environment\n\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-25-generic-x86_64-with-glibc2.35\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration:\r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 1: NVIDIA H100 80GB HBM3\r\nGPU 2: NVIDIA H100 80GB HBM3\r\nGPU 3: NVIDIA H100 80GB HBM3\r\nGPU 4: NVIDIA H100 80GB HBM3\r\nGPU 5: NVIDIA H100 80GB HBM3\r\nGPU 6: NVIDIA H100 80GB HBM3\r\nGPU 7: NVIDIA H100 80GB HBM3\r\n\r\nNvidia driver version: 535.86.10\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: N/A\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   52 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          96\r\nOn-line CPU(s) list:             0-95\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Platinum 8468\r\nCPU family:                      6\r\nModel:                           143\r\nThread(s) per core:              1\r\nCore(s) per socket:              48\r\nSocket(s):                       2\r\nStepping:                        8\r\nCPU max MHz:                     2100.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        4200.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr avx512_fp16 flush_l1d arch_capabilities\r\nL1d cache:                       4.5 MiB (96 instances)\r\nL1i cache:                       3 MiB (96 instances)\r\nL2 cache:                        192 MiB (96 instances)\r\nL3 cache:                        210 MiB (2 instances)\r\nNUMA node(s):                    8\r\nNUMA node0 CPU(s):               0-11\r\nNUMA node1 CPU(s):               12-23\r\nNUMA node2 CPU(s):               24-35\r\nNUMA node3 CPU(s):               36-47\r\nNUMA node4 CPU(s):               48-59\r\nNUMA node5 CPU(s):               60-71\r\nNUMA node6 CPU(s):               72-83\r\nNUMA node7 CPU(s):               84-95\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] transformers==4.42.3\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-11    0               N/A\r\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-11    0               N/A\r\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     PIX     PIX     PXB     SYS     SYS     SYS     SYS     24-35   2               N/A\r\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     PXB     PXB     PIX     SYS     SYS     SYS     SYS     24-35   2               N/A\r\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     PIX     PXB     SYS     48-59   4               N/A\r\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     PXB     PXB     PIX     SYS     48-59   4               N/A\r\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB     72-83   6               N/A\r\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     72-83   6               N/A\r\nNIC0    PIX     PXB     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC1    SYS     SYS     PIX     PXB     SYS     SYS     SYS     SYS     SYS      X      PIX     PXB     SYS     SYS     SYS     SYS\r\nNIC2    SYS     SYS     PIX     PXB     SYS     SYS     SYS     SYS     SYS     PIX      X      PXB     SYS     SYS     SYS     SYS\r\nNIC3    SYS     SYS     PXB     PIX     SYS     SYS     SYS     SYS     SYS     PXB     PXB      X      SYS     SYS     SYS     SYS\r\nNIC4    SYS     SYS     SYS     SYS     PIX     PXB     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     PXB     SYS\r\nNIC5    SYS     SYS     SYS     SYS     PIX     PXB     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      PXB     SYS\r\nNIC6    SYS     SYS     SYS     SYS     PXB     PIX     SYS     SYS     SYS     SYS     SYS     SYS     PXB     PXB      X      SYS\r\nNIC7    SYS     SYS     SYS     SYS     SYS     SYS     PXB     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n\n\n### \ud83d\udc1b Describe the bug\n\n(RayWorkerWrapper pid=5993) *** SIGSEGV received at time=1722289279 on cpu 61 ***\r\n(RayWorkerWrapper pid=5993) PC: @     0x7f0c73d2944a  (unknown)  addProxyOpIfNeeded()\r\n(RayWorkerWrapper pid=5993)     @     0x7f3c7fab6090  1954032864  (unknown)\r\n(RayWorkerWrapper pid=5993)     @                0xd        240  (unknown)\r\n(RayWorkerWrapper pid=5993)     @        0x100000001  (unknown)  (unknown)\r\n(RayWorkerWrapper pid=5993)     @     0x7f0c6ed5a2f0  (unknown)  (unknown)\r\n(RayWorkerWrapper pid=5993)     @ 0x4f3625ff0c74321c  (unknown)  (unknown)\r\n(RayWorkerWrapper pid=5993) [2024-07-29 21:41:19,242 E 5993 5993] logging.cc:440: *** SIGSEGV received at time=1722289279 on cpu 61 ***\r\n(RayWorkerWrapper pid=5993) [2024-07-29 21:41:19,242 E 5993 5993] logging.cc:440: PC: @     0x7f0c73d2944a  (unknown)  addProxyOpIfNeeded()\r\n(RayWorkerWrapper pid=5993) [2024-07-29 21:41:19,243 E 5993 5993] logging.cc:440:     @     0x7f3c7fab6090  1954032864  (unknown)\r\n(RayWorkerWrapper pid=5993) [2024-07-29 21:41:19,244 E 5993 5993] logging.cc:440:     @                0xd        240  (unknown)\r\n(RayWorkerWrapper pid=5993) [2024-07-29 21:41:19,247 E 5993 5993] logging.cc:440:     @        0x100000001  (unknown)  (unknown)\r\n(RayWorkerWrapper pid=5993) [2024-07-29 21:41:19,248 E 5993 5993] logging.cc:440:     @     0x7f0c6ed5a2f0  (unknown)  (unknown)\r\n(RayWorkerWrapper pid=5993) [2024-07-29 21:41:19,249 E 5993 5993] logging.cc:440:     @ 0x4f3625ff0c74321c  (unknown)  (unknown)\r\n(RayWorkerWrapper pid=5993) Fatal Python error: Segmentation fault\r\n(RayWorkerWrapper pid=5993)\r\n(RayWorkerWrapper pid=5993) Stack (most recent call first):\r\n(RayWorkerWrapper pid=5993)   File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 257 in ncclAllReduce\r\n(RayWorkerWrapper pid=5993)   File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/device_communicators/pynccl.py\", line 118 in all_reduce\r\n(RayWorkerWrapper pid=5993)   File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/parallel_state.py\", line 312 in all_reduce\r\n(RayWorkerWrapper pid=5993)\r\n(RayWorkerWrapper pid=5993) Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, charset_normalizer.md, uvloop.loop, ray._raylet, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, sentencepiece._sentencepiece, multidict._multidict, yarl._quoting_c, aiohttp._helpers, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket, frozenlist._frozenlist, pyarrow.lib, pyarrow._json, PIL._imaging, zmq.backend.cython._zmq (total: 41)\r\n\r\n\r\nI am testing llama3.1 405b in a Multi-Node Multi-GPU (tensor parallel plus pipeline parallel inference) environment. The engine runs properly with tensor parallel size set to 8 and pipeline parallel size set to 2, but timeouts or segmentation faults occur when the tensor parallel size is set to 16. Can you identify the cause of this issue?",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-07-29T21:52:17+00:00",
    "closed_at": null,
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6918/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6918"
  },
  {
    "number": 12053,
    "title": "[Usage]: who to run cluster withou docker",
    "body": "### Your current environment\n\nif i cannot use docker ,how to run vllm on multiple nodes ?\n\n\n\n### How would you like to use vllm\n\nif i cannot use docker ,how to run vllm on multiple nodes ?\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "unstale"
    ],
    "state": "open",
    "created_at": "2025-01-14T19:24:08+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12053/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12053"
  },
  {
    "number": 11799,
    "title": "[Feature]: Llama3.3 Tool calling support or a Geneneric and extensible llama tool calling support",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nWe have customer moving from llama3.1/3.2 to 3.3 and further when available\n\n### Alternatives\n\nNot yet explored\n\n### Additional context\n\nA generic way where we can use use tool calling support against llms instead of using specific params like \r\n--tool-call-parser llama3_json  /instead of --tool-call-parser <whatever model supports> as an external reference via chat template or so ?\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "unstale",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-01-07T07:01:45+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11799/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11799"
  },
  {
    "number": 10611,
    "title": "[Feature]: load and save kv cache from disk",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nFor prefix cache, cache hits can significantly reduce FTT. However, kv cache occupies a large amount of storage space, and the space in CPU memory and GPU video memory is very expensive and limited, resulting in limited prefix cache and decreased hit probability. By caching the kv cache on disk/SSD, the kv-cache can be reused, greatly improving the hit rate.\r\n\r\n\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-11-25T02:17:06+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10611/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10611"
  },
  {
    "number": 8902,
    "title": "[Feature]: Guided Decoding Schema Cache Store",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\r\n\r\n# Problem\r\n\r\nI am currently working with structured outputs and experimented a little with VLLM + Outlines. Since our JSON Schemas can get quite complex the generation of the FSM can take around 2 Minutes per Schema. It would be great to have a feature where you can provide a Schema-Store to save your generated schemas over time in a local file and reload them when you restart your deployment. Ideally this would be implemented as flag in the vllm serve arguments:\r\n\r\nhttps://docs.vllm.ai/en/latest/models/engine_args.html\r\n\r\n# Current Implementation\r\nI assume that this is currently not supported and the code to not recompute the schema is handled with the @cache() decorator here:\r\n![Screenshot 2024-09-27 134948](https://github.com/user-attachments/assets/4d6480a8-5a79-40ab-8b5c-6023b0551233)\r\n\r\n\r\n### Alternatives\r\n\r\nAlternative solution would probably be to create custom python code to handle this for my use-case and use the VLLM python functions for generation instead of the \"VLLM serve\" command. However not sure how you could handle this with the API Deployment.\r\n\r\n### Additional context\r\n\r\nPS: Happy to contribute to this feature if this is something that can be useful to other people / makes also sense for the people who understand the code base better.\r\n\r\n### Before submitting a new issue...\r\n\r\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "structured-output",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-09-27T12:07:20+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8902/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8902"
  },
  {
    "number": 9469,
    "title": "[Bug]: I want to integrate vllm into LLaMA-Factory, a transformers-based LLM training framework. However, I encountered two bugs: RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method & RuntimeError: NCCL error: invalid usage (run with NCCL_DEBUG=WARN for details)",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-50-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.66\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A800 80GB PCIe\r\nGPU 1: NVIDIA A800 80GB PCIe\r\nGPU 2: NVIDIA A800 80GB PCIe\r\nGPU 3: NVIDIA A800 80GB PCIe\r\nGPU 4: NVIDIA A800 80GB PCIe\r\nGPU 5: NVIDIA A800 80GB PCIe\r\nGPU 6: NVIDIA A800 80GB PCIe\r\nGPU 7: NVIDIA A800 80GB PCIe\r\n\r\nNvidia driver version: 530.30.02\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nThread(s) per core:              2\r\nCore(s) per socket:              32\r\nSocket(s):                       2\r\nNUMA node(s):                    4\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           106\r\nModel name:                      Intel(R) Xeon(R) Platinum 8358P CPU @ 2.60GHz\r\nStepping:                        6\r\nCPU MHz:                         800.000\r\nCPU max MHz:                     3400.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        5200.00\r\nL1d cache:                       3 MiB\r\nL1i cache:                       2 MiB\r\nL2 cache:                        80 MiB\r\nL3 cache:                        96 MiB\r\nNUMA node0 CPU(s):               0-15,64-79\r\nNUMA node1 CPU(s):               16-31,80-95\r\nNUMA node2 CPU(s):               32-47,96-111\r\nNUMA node3 CPU(s):               48-63,112-127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT vulnerable\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.68\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.0\r\n[pip3] triton==3.0.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.6.68                  pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.45.0                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A (dev)\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    CPU Affinity    NUMA Affinity\r\nGPU0     X      PIX     NV8     PIX     SYS     SYS     SYS     SYS     NODE    PIX     0-15,64-79      0\r\nGPU1    PIX      X      PIX     NV8     SYS     SYS     SYS     SYS     NODE    PIX     0-15,64-79      0\r\nGPU2    NV8     PIX      X      PIX     SYS     SYS     SYS     SYS     NODE    PIX     0-15,64-79      0\r\nGPU3    PIX     NV8     PIX      X      SYS     SYS     SYS     SYS     NODE    PIX     0-15,64-79      0\r\nGPU4    SYS     SYS     SYS     SYS      X      PIX     PIX     NV6     SYS     SYS     32-47,96-111    2\r\nGPU5    SYS     SYS     SYS     SYS     PIX      X      NV4     PIX     SYS     SYS     32-47,96-111    2\r\nGPU6    SYS     SYS     SYS     SYS     PIX     NV4      X      PIX     SYS     SYS     32-47,96-111    2\r\nGPU7    SYS     SYS     SYS     SYS     NV6     PIX     PIX      X      SYS     SYS     32-47,96-111    2\r\nNIC0    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE\r\nNIC1    PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE     X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI want to utilize VLLM to conduct LLM inference and efficiently derive the output probability distribution for token-level knowledge distillation. To achieve this, I need to first use VLLM for inference and then use its output to train student models. For the implementation, I integrated VLLM (0.6.2) into LLaMA-Factory (0.9.0), a Transformers (4.45.0)-based LLM training framework. However, when I run my code, I encounter the following bug:\r\n\r\n<details>\r\n<summary>ERROR-1</summary>\r\n\r\n```\r\n(VllmWorkerProcess pid=3624105) INFO 10-17 22:24:11 multiproc_worker_utils.py:218] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=3624105) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233] Exception in worker VllmWorkerProcess while processing method init_device: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method, Traceback (most recent call last):\r\n(VllmWorkerProcess pid=3624105) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233]   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 226, in _run_worker_process\r\n(VllmWorkerProcess pid=3624105) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=3624105) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233]   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/worker/worker.py\", line 166, in init_device\r\n(VllmWorkerProcess pid=3624105) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233]     torch.cuda.set_device(self.device)\r\n(VllmWorkerProcess pid=3624105) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233]   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 420, in set_device\r\n(VllmWorkerProcess pid=3624105) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233]     torch._C._cuda_setDevice(device)\r\n(VllmWorkerProcess pid=3624105) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233]   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\r\n(VllmWorkerProcess pid=3624105) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233]     raise RuntimeError(\r\n(VllmWorkerProcess pid=3624105) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\r\n(VllmWorkerProcess pid=3624105) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233] \r\nINFO 10-17 22:24:11 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n(VllmWorkerProcess pid=3624112) INFO 10-17 22:24:11 multiproc_worker_utils.py:218] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=3624112) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233] Exception in worker VllmWorkerProcess while processing method init_device: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method, Traceback (most recent call last):\r\n(VllmWorkerProcess pid=3624112) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233]   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 226, in _run_worker_process\r\n(VllmWorkerProcess pid=3624112) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=3624112) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233]   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/worker/worker.py\", line 166, in init_device\r\n(VllmWorkerProcess pid=3624112) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233]     torch.cuda.set_device(self.device)\r\n(VllmWorkerProcess pid=3624112) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233]   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 420, in set_device\r\n(VllmWorkerProcess pid=3624112) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233]     torch._C._cuda_setDevice(device)\r\n(VllmWorkerProcess pid=3624112) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233]   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\r\n(VllmWorkerProcess pid=3624112) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233]     raise RuntimeError(\r\n(VllmWorkerProcess pid=3624112) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\r\n(VllmWorkerProcess pid=3624112) ERROR 10-17 22:24:11 multiproc_worker_utils.py:233] \r\nINFO 10-17 22:24:12 utils.py:993] Found nccl from library libnccl.so.2\r\nINFO 10-17 22:24:12 pynccl.py:63] vLLM is using nccl==2.20.5\r\nINFO 10-17 22:24:12 utils.py:993] Found nccl from library libnccl.so.2\r\nINFO 10-17 22:24:12 pynccl.py:63] vLLM is using nccl==2.20.5\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/localnvme/application/sc_new/myy_world_consistency/LLaMA-Factory-0.9.0-9.27main/src/llamafactory/launcher.py\", line 23, in <module>\r\n[rank0]:     launch()\r\n[rank0]:   File \"/localnvme/application/sc_new/myy_world_consistency/LLaMA-Factory-0.9.0-9.27main/src/llamafactory/launcher.py\", line 19, in launch\r\n[rank0]:     run_exp()\r\n[rank0]:   File \"/localnvme/application/sc_new/myy_world_consistency/LLaMA-Factory-0.9.0-9.27main/src/llamafactory/train/tuner.py\", line 61, in run_exp\r\n[rank0]:     run_cwc(model_args, data_args, training_args, finetuning_args, callbacks)\r\n[rank0]:   File \"/localnvme/application/sc_new/myy_world_consistency/LLaMA-Factory-0.9.0-9.27main/src/llamafactory/train/cwc/workflow.py\", line 70, in run_cwc\r\n[rank0]:     source_model = LLM(model=model_args.cwc_source_model_name_or_path, tensor_parallel_size=training_args.world_size)\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 214, in __init__\r\n[rank0]:     self.llm_engine = LLMEngine.from_engine_args(\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 564, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 325, in __init__\r\n[rank0]:     self.model_executor = executor_class(\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py\", line 26, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 47, in __init__\r\n[rank0]:     self._init_executor()\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 110, in _init_executor\r\n[rank0]:     self._run_workers(\"init_device\")\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 185, in _run_workers\r\n[rank0]:     driver_worker_output = driver_worker_method(*args, **kwargs)\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/worker/worker.py\", line 176, in init_device\r\n[rank0]:     init_worker_distributed_environment(self.parallel_config, self.rank,\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/worker/worker.py\", line 451, in init_worker_distributed_environment\r\n[rank0]:     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 1059, in ensure_model_parallel_initialized\r\n[rank0]:     initialize_model_parallel(tensor_model_parallel_size,\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 1023, in initialize_model_parallel\r\n[rank0]:     _TP = init_model_parallel_group(group_ranks,\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 864, in init_model_parallel_group\r\n[rank0]:     return GroupCoordinator(\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 214, in __init__\r\n[rank0]:     self.pynccl_comm = PyNcclCommunicator(\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl.py\", line 89, in __init__\r\n[rank0]:     self.comm: ncclComm_t = self.nccl.ncclCommInitRank(\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 244, in ncclCommInitRank\r\n[rank0]:     self.NCCL_CHECK(self._funcs[\"ncclCommInitRank\"](ctypes.byref(comm),\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 223, in NCCL_CHECK\r\n[rank0]:     raise RuntimeError(f\"NCCL error: {error_str}\")\r\n[rank0]: RuntimeError: NCCL error: invalid usage (run with NCCL_DEBUG=WARN for details)\r\n[rank1]: Traceback (most recent call last):\r\n[rank1]:   File \"/localnvme/application/sc_new/myy_world_consistency/LLaMA-Factory-0.9.0-9.27main/src/llamafactory/launcher.py\", line 23, in <module>\r\n[rank1]:     launch()\r\n[rank1]:   File \"/localnvme/application/sc_new/myy_world_consistency/LLaMA-Factory-0.9.0-9.27main/src/llamafactory/launcher.py\", line 19, in launch\r\n[rank1]:     run_exp()\r\n[rank1]:   File \"/localnvme/application/sc_new/myy_world_consistency/LLaMA-Factory-0.9.0-9.27main/src/llamafactory/train/tuner.py\", line 61, in run_exp\r\n[rank1]:     run_cwc(model_args, data_args, training_args, finetuning_args, callbacks)\r\n[rank1]:   File \"/localnvme/application/sc_new/myy_world_consistency/LLaMA-Factory-0.9.0-9.27main/src/llamafactory/train/cwc/workflow.py\", line 70, in run_cwc\r\n[rank1]:     source_model = LLM(model=model_args.cwc_source_model_name_or_path, tensor_parallel_size=training_args.world_size)\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 214, in __init__\r\n[rank1]:     self.llm_engine = LLMEngine.from_engine_args(\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 564, in from_engine_args\r\n[rank1]:     engine = cls(\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 325, in __init__\r\n[rank1]:     self.model_executor = executor_class(\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py\", line 26, in __init__\r\n[rank1]:     super().__init__(*args, **kwargs)\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 47, in __init__\r\n[rank1]:     self._init_executor()\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 110, in _init_executor\r\n[rank1]:     self._run_workers(\"init_device\")\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 185, in _run_workers\r\n[rank1]:     driver_worker_output = driver_worker_method(*args, **kwargs)\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/worker/worker.py\", line 176, in init_device\r\n[rank1]:     init_worker_distributed_environment(self.parallel_config, self.rank,\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/worker/worker.py\", line 451, in init_worker_distributed_environment\r\n[rank1]:     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 1059, in ensure_model_parallel_initialized\r\n[rank1]:     initialize_model_parallel(tensor_model_parallel_size,\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 1023, in initialize_model_parallel\r\n[rank1]:     _TP = init_model_parallel_group(group_ranks,\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 864, in init_model_parallel_group\r\n[rank1]:     return GroupCoordinator(\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 214, in __init__\r\n[rank1]:     self.pynccl_comm = PyNcclCommunicator(\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl.py\", line 89, in __init__\r\n[rank1]:     self.comm: ncclComm_t = self.nccl.ncclCommInitRank(\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 244, in ncclCommInitRank\r\n[rank1]:     self.NCCL_CHECK(self._funcs[\"ncclCommInitRank\"](ctypes.byref(comm),\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 223, in NCCL_CHECK\r\n[rank1]:     raise RuntimeError(f\"NCCL error: {error_str}\")\r\n[rank1]: RuntimeError: NCCL error: invalid usage (run with NCCL_DEBUG=WARN for details)\r\nINFO 10-17 22:24:13 multiproc_worker_utils.py:124] Killing local vLLM worker processes\r\nINFO 10-17 22:24:13 multiproc_worker_utils.py:124] Killing local vLLM worker processes\r\n```\r\n\r\n</details>\r\n\r\nAfter debugging for hours and searching through [issues](https://github.com/vllm-project/vllm/issues/8937), I realized that both LLaMA-Factory and Transformers are calling ```torch.cuda``` functions, such as ```torch.cuda.is_available()```, before the initialization of vllm models (```source_model = LLM(model=model_args.cwc_source_model_name_or_path, tensor_parallel_size=training_args.world_size)```), which leads to the bug. However, there are numerous calls to ```torch.cuda``` functions throughout the project, and I cannot remove all of them. :(\r\nAfter reading this [issue](https://github.com/vllm-project/vllm/issues/6152#issuecomment-2211709345), I set ```export VLLM_WORKER_MULTIPROC_METHOD=spawn```, but there is another bug.\r\n\r\n<details>\r\n<summary>ERROR-2</summary>\r\n\r\n```\r\n[rank1]: Traceback (most recent call last):\r\n[rank1]:   File \"/localnvme/application/sc_new/myy_world_consistency/LLaMA-Factory-0.9.0-9.27main/src/llamafactory/launcher.py\", line 23, in <module>\r\n[rank1]:     launch()\r\n[rank1]:   File \"/localnvme/application/sc_new/myy_world_consistency/LLaMA-Factory-0.9.0-9.27main/src/llamafactory/launcher.py\", line 19, in launch\r\n[rank1]:     run_exp()\r\n[rank1]:   File \"/localnvme/application/sc_new/myy_world_consistency/LLaMA-Factory-0.9.0-9.27main/src/llamafactory/train/tuner.py\", line 61, in run_exp\r\n[rank1]:     run_cwc(model_args, data_args, training_args, finetuning_args, callbacks)\r\n[rank1]:   File \"/localnvme/application/sc_new/myy_world_consistency/LLaMA-Factory-0.9.0-9.27main/src/llamafactory/train/cwc/workflow.py\", line 70, in run_cwc\r\n[rank1]:     source_model = LLM(model=model_args.cwc_source_model_name_or_path, tensor_parallel_size=training_args.world_size)\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 214, in __init__\r\n[rank1]:     self.llm_engine = LLMEngine.from_engine_args(\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 564, in from_engine_args\r\n[rank1]:     engine = cls(\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 325, in __init__\r\n[rank1]:     self.model_executor = executor_class(\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py\", line 26, in __init__\r\n[rank1]:     super().__init__(*args, **kwargs)\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 47, in __init__\r\n[rank1]:     self._init_executor()\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 110, in _init_executor\r\n[rank1]:     self._run_workers(\"init_device\")\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 185, in _run_workers\r\n[rank1]:     driver_worker_output = driver_worker_method(*args, **kwargs)\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/worker/worker.py\", line 176, in init_device\r\n[rank1]:     init_worker_distributed_environment(self.parallel_config, self.rank,\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/worker/worker.py\", line 451, in init_worker_distributed_environment\r\n[rank1]:     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 1059, in ensure_model_parallel_initialized\r\n[rank1]:     initialize_model_parallel(tensor_model_parallel_size,\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 1023, in initialize_model_parallel\r\n[rank1]:     _TP = init_model_parallel_group(group_ranks,\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 864, in init_model_parallel_group\r\n[rank1]:     return GroupCoordinator(\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 214, in __init__\r\n[rank1]:     self.pynccl_comm = PyNcclCommunicator(\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl.py\", line 89, in __init__\r\n[rank1]:     self.comm: ncclComm_t = self.nccl.ncclCommInitRank(\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 244, in ncclCommInitRank\r\n[rank1]:     self.NCCL_CHECK(self._funcs[\"ncclCommInitRank\"](ctypes.byref(comm),\r\n[rank1]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 223, in NCCL_CHECK\r\n[rank1]:     raise RuntimeError(f\"NCCL error: {error_str}\")\r\n[rank1]: RuntimeError: NCCL error: invalid usage (run with NCCL_DEBUG=WARN for details)\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/localnvme/application/sc_new/myy_world_consistency/LLaMA-Factory-0.9.0-9.27main/src/llamafactory/launcher.py\", line 23, in <module>\r\n[rank0]:     launch()\r\n[rank0]:   File \"/localnvme/application/sc_new/myy_world_consistency/LLaMA-Factory-0.9.0-9.27main/src/llamafactory/launcher.py\", line 19, in launch\r\n[rank0]:     run_exp()\r\n[rank0]:   File \"/localnvme/application/sc_new/myy_world_consistency/LLaMA-Factory-0.9.0-9.27main/src/llamafactory/train/tuner.py\", line 61, in run_exp\r\n[rank0]:     run_cwc(model_args, data_args, training_args, finetuning_args, callbacks)\r\n[rank0]:   File \"/localnvme/application/sc_new/myy_world_consistency/LLaMA-Factory-0.9.0-9.27main/src/llamafactory/train/cwc/workflow.py\", line 70, in run_cwc\r\n[rank0]:     source_model = LLM(model=model_args.cwc_source_model_name_or_path, tensor_parallel_size=training_args.world_size)\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 214, in __init__\r\n[rank0]:     self.llm_engine = LLMEngine.from_engine_args(\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 564, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 325, in __init__\r\n[rank0]:     self.model_executor = executor_class(\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py\", line 26, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 47, in __init__\r\n[rank0]:     self._init_executor()\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 110, in _init_executor\r\n[rank0]:     self._run_workers(\"init_device\")\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 185, in _run_workers\r\n[rank0]:     driver_worker_output = driver_worker_method(*args, **kwargs)\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/worker/worker.py\", line 176, in init_device\r\n[rank0]:     init_worker_distributed_environment(self.parallel_config, self.rank,\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/worker/worker.py\", line 451, in init_worker_distributed_environment\r\n[rank0]:     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 1059, in ensure_model_parallel_initialized\r\n[rank0]:     initialize_model_parallel(tensor_model_parallel_size,\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 1023, in initialize_model_parallel\r\n[rank0]:     _TP = init_model_parallel_group(group_ranks,\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 864, in init_model_parallel_group\r\n[rank0]:     return GroupCoordinator(\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\", line 214, in __init__\r\n[rank0]:     self.pynccl_comm = PyNcclCommunicator(\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl.py\", line 89, in __init__\r\n[rank0]:     self.comm: ncclComm_t = self.nccl.ncclCommInitRank(\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 244, in ncclCommInitRank\r\n[rank0]:     self.NCCL_CHECK(self._funcs[\"ncclCommInitRank\"](ctypes.byref(comm),\r\n[rank0]:   File \"/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 223, in NCCL_CHECK\r\n[rank0]:     raise RuntimeError(f\"NCCL error: {error_str}\")\r\n[rank0]: RuntimeError: NCCL error: invalid usage (run with NCCL_DEBUG=WARN for details)\r\n```\r\n\r\n</details>\r\n\r\nThen I check my driver and hardware following the python script provided in https://docs.vllm.ai/en/latest/getting_started/debugging.html. Below is the report. It seems that everything is okay.\r\n\r\n<details>\r\n<summary>driver & hardware checking report</summary>\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=6,7 NCCL_DEBUG=TRACE torchrun --nproc-per-node=2 test_vllm_env.py\r\nW1017 23:04:55.958511 23456244184000 torch/distributed/run.py:779] \r\nW1017 23:04:55.958511 23456244184000 torch/distributed/run.py:779] *****************************************\r\nW1017 23:04:55.958511 23456244184000 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\nW1017 23:04:55.958511 23456244184000 torch/distributed/run.py:779] *****************************************\r\nnode05:3662915:3662915 [0] NCCL INFO Bootstrap : Using ibs110:192.168.99.105<0>\r\nnode05:3662915:3662915 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation\r\nnode05:3662915:3662915 [0] NCCL INFO cudaDriverVersion 12010\r\nNCCL version 2.20.5+cuda12.4\r\nnode05:3662915:3663002 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ibs110:192.168.99.105<0>\r\nnode05:3662915:3663002 [0] NCCL INFO Using non-device net plugin version 0\r\nnode05:3662915:3663002 [0] NCCL INFO Using network IB\r\nnode05:3662916:3662916 [1] NCCL INFO cudaDriverVersion 12010\r\nnode05:3662916:3662916 [1] NCCL INFO Bootstrap : Using ibs110:192.168.99.105<0>\r\nnode05:3662916:3662916 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation\r\nnode05:3662916:3663010 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ibs110:192.168.99.105<0>\r\nnode05:3662916:3663010 [1] NCCL INFO Using non-device net plugin version 0\r\nnode05:3662916:3663010 [1] NCCL INFO Using network IB\r\nnode05:3662915:3663002 [0] NCCL INFO comm 0x5555b6a34880 rank 0 nranks 2 cudaDev 0 nvmlDev 6 busId 9c000 commId 0x194370b6111a740b - Init START\r\nnode05:3662916:3663010 [1] NCCL INFO comm 0x5555b6a32e10 rank 1 nranks 2 cudaDev 1 nvmlDev 7 busId 9e000 commId 0x194370b6111a740b - Init START\r\nnode05:3662915:3663002 [0] NCCL INFO Setting affinity for GPU 6 to ffff,00000000,0000ffff,00000000\r\nnode05:3662916:3663010 [1] NCCL INFO Setting affinity for GPU 7 to ffff,00000000,0000ffff,00000000\r\nnode05:3662916:3663010 [1] NCCL INFO comm 0x5555b6a32e10 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0\r\nnode05:3662916:3663010 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1 [2] -1/-1/-1->1->0 [3] 0/-1/-1->1->-1\r\nnode05:3662916:3663010 [1] NCCL INFO P2P Chunksize set to 131072\r\nnode05:3662915:3663002 [0] NCCL INFO comm 0x5555b6a34880 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0\r\nnode05:3662915:3663002 [0] NCCL INFO Channel 00/04 :    0   1\r\nnode05:3662915:3663002 [0] NCCL INFO Channel 01/04 :    0   1\r\nnode05:3662915:3663002 [0] NCCL INFO Channel 02/04 :    0   1\r\nnode05:3662915:3663002 [0] NCCL INFO Channel 03/04 :    0   1\r\nnode05:3662915:3663002 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1 [2] 1/-1/-1->0->-1 [3] -1/-1/-1->0->1\r\nnode05:3662915:3663002 [0] NCCL INFO P2P Chunksize set to 131072\r\nnode05:3662916:3663010 [1] NCCL INFO Channel 00/0 : 1[7] -> 0[6] via P2P/CUMEM\r\nnode05:3662915:3663002 [0] NCCL INFO Channel 00/0 : 0[6] -> 1[7] via P2P/CUMEM\r\nnode05:3662916:3663010 [1] NCCL INFO Channel 01/0 : 1[7] -> 0[6] via P2P/CUMEM\r\nnode05:3662915:3663002 [0] NCCL INFO Channel 01/0 : 0[6] -> 1[7] via P2P/CUMEM\r\nnode05:3662916:3663010 [1] NCCL INFO Channel 02/0 : 1[7] -> 0[6] via P2P/CUMEM\r\nnode05:3662915:3663002 [0] NCCL INFO Channel 02/0 : 0[6] -> 1[7] via P2P/CUMEM\r\nnode05:3662916:3663010 [1] NCCL INFO Channel 03/0 : 1[7] -> 0[6] via P2P/CUMEM\r\nnode05:3662915:3663002 [0] NCCL INFO Channel 03/0 : 0[6] -> 1[7] via P2P/CUMEM\r\nnode05:3662916:3663010 [1] NCCL INFO Connected all rings\r\nnode05:3662916:3663010 [1] NCCL INFO Connected all trees\r\nnode05:3662916:3663010 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\r\nnode05:3662916:3663010 [1] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer\r\nnode05:3662915:3663002 [0] NCCL INFO Connected all rings\r\nnode05:3662915:3663002 [0] NCCL INFO Connected all trees\r\nnode05:3662915:3663002 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\r\nnode05:3662915:3663002 [0] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer\r\nnode05:3662916:3663010 [1] NCCL INFO comm 0x5555b6a32e10 rank 1 nranks 2 cudaDev 1 nvmlDev 7 busId 9e000 commId 0x194370b6111a740b - Init COMPLETE\r\nnode05:3662915:3663002 [0] NCCL INFO comm 0x5555b6a34880 rank 0 nranks 2 cudaDev 0 nvmlDev 6 busId 9c000 commId 0x194370b6111a740b - Init COMPLETE\r\nPyTorch NCCL is successful!\r\nPyTorch NCCL is successful!\r\nPyTorch GLOO is successful!\r\nPyTorch GLOO is successful!\r\n/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\r\nNo module named 'vllm._version'\r\n  from vllm.version import __version__ as VLLM_VERSION\r\n/localnvme/application/sc_new/miniconda3/envs/cwc/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\r\nNo module named 'vllm._version'\r\n  from vllm.version import __version__ as VLLM_VERSION\r\nINFO 10-17 23:05:03 utils.py:993] Found nccl from library libnccl.so.2\r\nINFO 10-17 23:05:03 pynccl.py:63] vLLM is using nccl==2.20.5\r\nINFO 10-17 23:05:03 utils.py:993] Found nccl from library libnccl.so.2\r\nINFO 10-17 23:05:03 pynccl.py:63] vLLM is using nccl==2.20.5\r\nnode05:3662916:3662916 [1] NCCL INFO Using non-device net plugin version 0\r\nnode05:3662916:3662916 [1] NCCL INFO Using network IB\r\nnode05:3662915:3662915 [0] NCCL INFO Using non-device net plugin version 0\r\nnode05:3662915:3662915 [0] NCCL INFO Using network IB\r\nnode05:3662915:3662915 [0] NCCL INFO comm 0x5555cbb18260 rank 0 nranks 2 cudaDev 0 nvmlDev 6 busId 9c000 commId 0x34dae9aadb417959 - Init START\r\nnode05:3662916:3662916 [1] NCCL INFO comm 0x5555cbb17600 rank 1 nranks 2 cudaDev 1 nvmlDev 7 busId 9e000 commId 0x34dae9aadb417959 - Init START\r\nnode05:3662915:3662915 [0] NCCL INFO Setting affinity for GPU 6 to ffff,00000000,0000ffff,00000000\r\nnode05:3662916:3662916 [1] NCCL INFO Setting affinity for GPU 7 to ffff,00000000,0000ffff,00000000\r\nnode05:3662916:3662916 [1] NCCL INFO comm 0x5555cbb17600 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0\r\nnode05:3662916:3662916 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1 [2] -1/-1/-1->1->0 [3] 0/-1/-1->1->-1\r\nnode05:3662916:3662916 [1] NCCL INFO P2P Chunksize set to 131072\r\nnode05:3662915:3662915 [0] NCCL INFO comm 0x5555cbb18260 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0\r\nnode05:3662915:3662915 [0] NCCL INFO Channel 00/04 :    0   1\r\nnode05:3662915:3662915 [0] NCCL INFO Channel 01/04 :    0   1\r\nnode05:3662915:3662915 [0] NCCL INFO Channel 02/04 :    0   1\r\nnode05:3662915:3662915 [0] NCCL INFO Channel 03/04 :    0   1\r\nnode05:3662915:3662915 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1 [2] 1/-1/-1->0->-1 [3] -1/-1/-1->0->1\r\nnode05:3662915:3662915 [0] NCCL INFO P2P Chunksize set to 131072\r\nnode05:3662916:3662916 [1] NCCL INFO Channel 00/0 : 1[7] -> 0[6] via P2P/CUMEM\r\nnode05:3662916:3662916 [1] NCCL INFO Channel 01/0 : 1[7] -> 0[6] via P2P/CUMEM\r\nnode05:3662916:3662916 [1] NCCL INFO Channel 02/0 : 1[7] -> 0[6] via P2P/CUMEM\r\nnode05:3662916:3662916 [1] NCCL INFO Channel 03/0 : 1[7] -> 0[6] via P2P/CUMEM\r\nnode05:3662915:3662915 [0] NCCL INFO Channel 00/0 : 0[6] -> 1[7] via P2P/CUMEM\r\nnode05:3662915:3662915 [0] NCCL INFO Channel 01/0 : 0[6] -> 1[7] via P2P/CUMEM\r\nnode05:3662915:3662915 [0] NCCL INFO Channel 02/0 : 0[6] -> 1[7] via P2P/CUMEM\r\nnode05:3662915:3662915 [0] NCCL INFO Channel 03/0 : 0[6] -> 1[7] via P2P/CUMEM\r\nnode05:3662916:3662916 [1] NCCL INFO Connected all rings\r\nnode05:3662916:3662916 [1] NCCL INFO Connected all trees\r\nnode05:3662915:3662915 [0] NCCL INFO Connected all rings\r\nnode05:3662916:3662916 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\r\nnode05:3662916:3662916 [1] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer\r\nnode05:3662915:3662915 [0] NCCL INFO Connected all trees\r\nnode05:3662915:3662915 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\r\nnode05:3662915:3662915 [0] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer\r\nnode05:3662915:3662915 [0] NCCL INFO comm 0x5555cbb18260 rank 0 nranks 2 cudaDev 0 nvmlDev 6 busId 9c000 commId 0x34dae9aadb417959 - Init COMPLETE\r\nnode05:3662916:3662916 [1] NCCL INFO comm 0x5555cbb17600 rank 1 nranks 2 cudaDev 1 nvmlDev 7 busId 9e000 commId 0x34dae9aadb417959 - Init COMPLETE\r\nvLLM NCCL is successful!\r\nvLLM NCCL is successful!\r\nvLLM NCCL with cuda graph is successful!\r\nnode05:3662915:3663042 [0] NCCL INFO [Service thread] Connection closed by localRank 0\r\nvLLM NCCL with cuda graph is successful!\r\nnode05:3662916:3663040 [1] NCCL INFO [Service thread] Connection closed by localRank 1\r\nnode05:3662915:3663083 [0] NCCL INFO comm 0x5555b6a34880 rank 0 nranks 2 cudaDev 0 busId 9c000 - Abort COMPLETE\r\nnode05:3662916:3663084 [1] NCCL INFO comm 0x5555b6a32e10 rank 1 nranks 2 cudaDev 1 busId 9e000 - Abort COMPLETE\r\n```\r\n\r\n</details>\r\n\r\nIs there any way to fix this bug? @DarkLight1337 @youkaichao\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-10-17T15:17:07+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9469/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9469"
  },
  {
    "number": 9496,
    "title": "[Bug]: [Performance] 100% performance drop using multiple lora vs no lora(qwen-chat model)",
    "body": "### Your current environment\n\n[Performance] 100% performance drop using multiple lora vs no lora(qwen-chat model)\r\ngpu: 4 * T4\r\nvllm version\uff1a v0.5.4\r\nmodel\uff1aqwenhalf-14b-chat\r\n\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\n[Performance] 100% performance drop using multiple lora vs no lora(qwen-chat model)\r\ngpu: 4*T4\r\n2k prompt,500 output tokens, in multiple lora model infer by vllm cost 32s\r\nwhen 2k prompt,500 output tokens, in sinle model infer by vllm cost 16s;\r\nwhy is there such a speed difference? the reason\uff1f\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-10-18T08:14:35+00:00",
    "closed_at": null,
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9496/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9496"
  },
  {
    "number": 5060,
    "title": "[Bug]: vllm.engine.async_llm_engine.AsyncEngineDeadError: Background loop has errored already.",
    "body": "### Your current environment\r\n\r\ndocker image: vllm/vllm-openai:0.4.2\r\nModel: https://huggingface.co/alpindale/c4ai-command-r-plus-GPTQ\r\nGPUs: RTX8000 * 2\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nThe model works fine until the following error is raised. \r\n-------------------------------------------------------\r\n\r\n\r\nINFO 05-26 22:28:18 async_llm_engine.py:529] Received request cmpl-10dff83cb4b6422ba8c64213942a7e46: prompt: '<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>\"Question: Is Korea the name of a Nation?\\nGuideline: No explanation.\\nFormat: {\"Answer\": \"<your yes/no answer>\"}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=['---'], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [5, 5, 255000, 255006, 9, 60478, 33, 3294, 13489, 1690, 2773, 1719, 1671, 20611, 38, 206, 46622, 7609, 33, 3679, 33940, 21, 206, 8961, 33, 19586, 61664, 2209, 31614, 28131, 20721, 22, 3598, 11205, 37, 22631, 255001, 255000, 255007], lora_request: None.\r\nINFO 05-26 22:28:18 async_llm_engine.py:154] Aborted request cmpl-10dff83cb4b6422ba8c64213942a7e46.\r\nINFO:     10.11.3.150:6231 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 475, in engine_step\r\n    request_outputs = await self.engine.step_async()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 221, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 110, in execute_model_async\r\n    all_outputs = await self._run_workers_async(\"execute_model\",\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/ray_gpu_executor.py\", line 326, in _run_workers_async\r\n    all_outputs = await asyncio.gather(*coros)\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 456, in wait_for\r\n    return fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 411, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 123, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 756, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 776, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 297, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 77, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 72, in app\r\n    response = await func(request)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 278, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 99, in create_chat_completion\r\n    generator = await openai_serving_chat.create_chat_completion(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 138, in create_chat_completion\r\n    return await self.chat_completion_full_generator(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 301, in chat_completion_full_generator\r\n    async for res in result_generator:\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 666, in generate\r\n    raise e\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 660, in generate\r\n    async for request_output in stream:\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 77, in __anext__\r\n    raise result\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 38, in _raise_exception_on_finish\r\n    task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 501, in run_engine_loop\r\n    has_requests_in_progress = await asyncio.wait_for(\r\n  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 458, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 411, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 123, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 756, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 776, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 297, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 77, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 72, in app\r\n    response = await func(request)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 278, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 99, in create_chat_completion\r\n    generator = await openai_serving_chat.create_chat_completion(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 138, in create_chat_completion\r\n    return await self.chat_completion_full_generator(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 301, in chat_completion_full_generator\r\n    async for res in result_generator:\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 666, in generate\r\n    raise e\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 650, in generate\r\n    stream = await self.add_request(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 537, in add_request\r\n    self.start_background_loop()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 411, in start_background_loop\r\n    raise AsyncEngineDeadError(\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Background loop has errored already",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-05-26T22:44:41+00:00",
    "closed_at": null,
    "comments": 45,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5060/reactions",
      "total_count": 29,
      "+1": 20,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 9
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5060"
  },
  {
    "number": 10478,
    "title": "[Bug]: vLLM CPU mode broken Unable to get JIT kernel for brgemm",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.5.1+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0\r\nClang version: Could not collect\r\nCMake version: version 3.31.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.8.0-1018-gcp-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               8\r\nOn-line CPU(s) list:                  0-7\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD EPYC 7B12\r\nCPU family:                           23\r\nModel:                                49\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   4\r\nSocket(s):                            1\r\nStepping:                             0\r\nBogoMIPS:                             4499.99\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            128 KiB (4 instances)\r\nL1i cache:                            128 KiB (4 instances)\r\nL2 cache:                             2 MiB (4 instances)\r\nL3 cache:                             16 MiB (1 instance)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-7\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] intel_extension_for_pytorch==2.5.0\r\n[pip3] numpy==1.26.4\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1+cpu\r\n[pip3] torchvision==0.20.1+cpu\r\n[pip3] transformers==4.46.3\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.4.post2.dev22+g47826cac\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n\r\nVLLM_WORKER_MULTIPROC_METHOD=spawn\r\nVLLM_LOGGING_LEVEL=DEBUG\r\nLD_LIBRARY_PATH=/usr/local/lib/python3.10/dist-packages/cv2/../../lib64:\r\n\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nSeems oneDNN is missing in latest Dockerfile or at least some part of it seems missing.\r\n\r\nThis used to work in v0.6.3 fine though. I was suspecting this PR since it touched how oneDNN is included: https://github.com/vllm-project/vllm/pull/9344\r\n\r\nSteps to reproduce:\r\n1. Git clone vllm repo latest main branch or v0.6.4.post1\r\n2.  Build the cpu docker image: `docker build -t test-cpu -f Dockerfile.cpu .`\r\n3.  Run the openai server: \r\n```\r\ndocker run -d --name vllm -p 8000:8000 -e VLLM_LOGGING_LEVEL=DEBUG -e ONEDNN_VERBOSE=all -e VLLM_WORKER_MULTIPROC_METHOD=spawn test-cpu --model facebook/opt-125m --disable-frontend-multiprocessing\r\n```\r\n\r\n4. Wait for server to be ready and then send a simple prompt:\r\n```\r\ncurl -v --fail-with-body --show-error http://localhost:8000/v1/completions   -H \"Content-Type: application/json\"   -d '{\r\n  \"model\": \"facebook/opt-125m\",\r\n  \"prompt\": \"San Francisco is a\",\r\n  \"max_tokens\": 7,\r\n  \"temperature\": 0\r\n}'\r\n```\r\n\r\nFull logs:\r\n```\r\nINFO 11-20 06:32:30 importing.py:15] Triton not installed or not compatible; certain GPU-related functions will not be available.\r\nINFO 11-20 06:32:31 api_server.py:592] vLLM API server version 0.6.4.post2.dev22+g47826cac\r\nINFO 11-20 06:32:31 api_server.py:593] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=True, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='facebook/opt-125m', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)\r\nINFO 11-20 06:32:31 __init__.py:31] No plugins found.\r\nWARNING 11-20 06:32:37 _logger.py:72] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\r\nWARNING 11-20 06:32:37 _logger.py:72] Async output processing is only supported for CUDA, TPU, XPU and HPU.Disabling it for other platforms.\r\nWARNING 11-20 06:32:37 _logger.py:72] CUDA graph is not supported on CPU, fallback to the eager mode.\r\nWARNING 11-20 06:32:37 _logger.py:72] Environment variable VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by default.\r\nINFO 11-20 06:32:37 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post2.dev22+g47826cac) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=facebook/opt-125m, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None, pooler_config=None)\r\nINFO 11-20 06:32:42 importing.py:15] Triton not installed or not compatible; certain GPU-related functions will not be available.\r\n(VllmWorkerProcess pid=29) INFO 11-20 06:32:43 __init__.py:31] No plugins found.\r\n(VllmWorkerProcess pid=29) INFO 11-20 06:32:43 selector.py:221] Cannot use _Backend.FLASH_ATTN backend on CPU.\r\n(VllmWorkerProcess pid=29) INFO 11-20 06:32:43 selector.py:156] Using Torch SDPA backend.\r\n(VllmWorkerProcess pid=29) INFO 11-20 06:32:43 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=29) DEBUG 11-20 06:32:43 parallel_state.py:983] world_size=1 rank=0 local_rank=-1 distributed_init_method=tcp://127.0.0.1:35451 backend=gloo\r\n(VllmWorkerProcess pid=29) DEBUG 11-20 06:32:43 decorators.py:84] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.opt.OPTModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']\r\n(VllmWorkerProcess pid=29) INFO 11-20 06:32:43 weight_utils.py:243] Using model weights format ['*.bin']\r\nLoading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n(VllmWorkerProcess pid=29) /usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n(VllmWorkerProcess pid=29)   state = torch.load(bin_file, map_location=\"cpu\")\r\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.62it/s]\r\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.61it/s]\r\n(VllmWorkerProcess pid=29)\r\nINFO 11-20 06:32:45 cpu_executor.py:195] # CPU blocks: 7281\r\nDEBUG 11-20 06:32:45 decorators.py:84] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.opt.OPTModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']\r\nINFO 11-20 06:32:45 api_server.py:534] Using supplied chat template:\r\nINFO 11-20 06:32:45 api_server.py:534] None\r\nINFO 11-20 06:32:45 launcher.py:19] Available routes are:\r\nINFO 11-20 06:32:45 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\r\nINFO 11-20 06:32:45 launcher.py:27] Route: /docs, Methods: HEAD, GET\r\nINFO 11-20 06:32:45 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\r\nINFO 11-20 06:32:45 launcher.py:27] Route: /redoc, Methods: HEAD, GET\r\nINFO 11-20 06:32:45 launcher.py:27] Route: /health, Methods: GET\r\nINFO 11-20 06:32:45 launcher.py:27] Route: /tokenize, Methods: POST\r\nINFO 11-20 06:32:45 launcher.py:27] Route: /detokenize, Methods: POST\r\nINFO 11-20 06:32:45 launcher.py:27] Route: /v1/models, Methods: GET\r\nINFO 11-20 06:32:45 launcher.py:27] Route: /version, Methods: GET\r\nINFO 11-20 06:32:45 launcher.py:27] Route: /v1/chat/completions, Methods: POST\r\nINFO 11-20 06:32:45 launcher.py:27] Route: /v1/completions, Methods: POST\r\nINFO 11-20 06:32:45 launcher.py:27] Route: /v1/embeddings, Methods: POST\r\nINFO:     Started server process [1]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\r\nINFO 11-20 06:32:55 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 11-20 06:33:05 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 11-20 06:33:15 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 11-20 06:33:18 logger.py:37] Received request cmpl-3706abf5f25b4d21a79004dd76c22c0f-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [2, 16033, 2659, 16, 10], lora_request: None, prompt_adapter_request: None.\r\nINFO 11-20 06:33:18 async_llm_engine.py:208] Added request cmpl-3706abf5f25b4d21a79004dd76c22c0f-0.\r\nDEBUG 11-20 06:33:18 async_llm_engine.py:836] Waiting for new requests...\r\nDEBUG 11-20 06:33:18 async_llm_engine.py:855] Got new requests!\r\nERROR 11-20 06:33:19 multiproc_worker_utils.py:116] Worker VllmWorkerProcess pid 29 died, exit code: 1\r\nINFO 11-20 06:33:19 multiproc_worker_utils.py:120] Killing local vLLM worker processes\r\nERROR 11-20 06:33:19 async_llm_engine.py:65] Engine background task failed\r\nERROR 11-20 06:33:19 async_llm_engine.py:65] Traceback (most recent call last):\r\nERROR 11-20 06:33:19 async_llm_engine.py:65]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 55, in _log_task_completion\r\nERROR 11-20 06:33:19 async_llm_engine.py:65]     return_value = task.result()\r\nERROR 11-20 06:33:19 async_llm_engine.py:65]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 872, in run_engine_loop\r\nERROR 11-20 06:33:19 async_llm_engine.py:65]     result = task.result()\r\nERROR 11-20 06:33:19 async_llm_engine.py:65]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 795, in engine_step\r\nERROR 11-20 06:33:19 async_llm_engine.py:65]     request_outputs = await self.engine.step_async(virtual_engine)\r\nERROR 11-20 06:33:19 async_llm_engine.py:65]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 347, in step_async\r\nERROR 11-20 06:33:19 async_llm_engine.py:65]     outputs = await self.model_executor.execute_model_async(\r\nERROR 11-20 06:33:19 async_llm_engine.py:65]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/cpu_executor.py\", line 298, in execute_model_async\r\nERROR 11-20 06:33:19 async_llm_engine.py:65]     output = await make_async(self.execute_model\r\nERROR 11-20 06:33:19 async_llm_engine.py:65]   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\nERROR 11-20 06:33:19 async_llm_engine.py:65]     result = self.fn(*self.args, **self.kwargs)\r\nERROR 11-20 06:33:19 async_llm_engine.py:65]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/cpu_executor.py\", line 210, in execute_model\r\nERROR 11-20 06:33:19 async_llm_engine.py:65]     output = self.driver_method_invoker(self.driver_worker,\r\nERROR 11-20 06:33:19 async_llm_engine.py:65]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/cpu_executor.py\", line 311, in _async_driver_method_invoker\r\nERROR 11-20 06:33:19 async_llm_engine.py:65]     return driver.execute_method(method, *args, **kwargs).get()\r\nERROR 11-20 06:33:19 async_llm_engine.py:65]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 54, in get\r\nERROR 11-20 06:33:19 async_llm_engine.py:65]     raise self.result.exception\r\nERROR 11-20 06:33:19 async_llm_engine.py:65] ChildProcessError: worker died\r\nUnable to get JIT kernel for brgemm. Params: M=5, N=5, K=64, str_a=1, str_b=1, brgemm_type=1, beta=0, a_trans=0, unroll_hint=1, lda=2304, ldb=5, ldc=5, config=0, b_vnni=02024-11-20 06:33:19,225 - __init__.py - asyncio - ERROR - Exception in callback functools.partial(<function _log_task_completion at 0x7646b03997e0>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7646ae8d3310>>)\r\nhandle: <Handle functools.partial(<function _log_task_completion at 0x7646b03997e0>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7646ae8d3310>>)>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 55, in _log_task_completion\r\n    return_value = task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 872, in run_engine_loop\r\n    result = task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 795, in engine_step\r\n    request_outputs = await self.engine.step_async(virtual_engine)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 347, in step_async\r\n    outputs = await self.model_executor.execute_model_async(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/cpu_executor.py\", line 298, in execute_model_async\r\n    output = await make_async(self.execute_model\r\n  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/cpu_executor.py\", line 210, in execute_model\r\n    output = self.driver_method_invoker(self.driver_worker,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/cpu_executor.py\", line 311, in _async_driver_method_invoker\r\n    return driver.execute_method(method, *args, **kwargs).get()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 54, in get\r\n    raise self.result.exception\r\nChildProcessError: worker died\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 67, in _log_task_completion\r\n    raise AsyncEngineDeadError(\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for the actual cause.\r\nINFO:     172.17.0.1:37946 - \"POST /v1/completions HTTP/1.1\" 500 Internal Server Error\r\nERROR:    Exception in ASGI application\r\n  + Exception Group Traceback (most recent call last):\r\n  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/_utils.py\", line 76, in collapse_excgroups\r\n  |     yield\r\n  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/base.py\", line 186, in __call__\r\n  |     async with anyio.create_task_group() as task_group:\r\n  |   File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 763, in __aexit__\r\n  |     raise BaseExceptionGroup(\r\n  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\r\n  +-+---------------- 1 ----------------\r\n    | Traceback (most recent call last):\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 401, in run_asgi\r\n    |     result = await app(  # type: ignore[func-returns-value]\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\r\n    |     return await self.app(scope, receive, send)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\r\n    |     await super().__call__(scope, receive, send)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 113, in __call__\r\n    |     await self.middleware_stack(scope, receive, send)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\r\n    |     raise exc\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n    |     await self.app(scope, receive, _send)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/base.py\", line 185, in __call__\r\n    |     with collapse_excgroups():\r\n    |   File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\r\n    |     self.gen.throw(typ, value, traceback)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/_utils.py\", line 82, in collapse_excgroups\r\n    |     raise exc\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/base.py\", line 187, in __call__\r\n    |     response = await self.dispatch_func(request, call_next)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 491, in add_request_id\r\n    |     response = await call_next(request)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/base.py\", line 163, in call_next\r\n    |     raise app_exc\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/base.py\", line 149, in coro\r\n    |     await self.app(scope, receive_or_disconnect, send_no_error)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    |     await self.app(scope, receive, send)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    |     raise exc\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n    |     await app(scope, receive, sender)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 715, in __call__\r\n    |     await self.middleware_stack(scope, receive, send)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 735, in app\r\n    |     await route.handle(scope, receive, send)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 288, in handle\r\n    |     await self.app(scope, receive, send)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 76, in app\r\n    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    |     raise exc\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n    |     await app(scope, receive, sender)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 73, in app\r\n    |     response = await f(request)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 301, in app\r\n    |     raw_response = await run_endpoint_function(\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 212, in run_endpoint_function\r\n    |     return await dependant.call(**values)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 367, in create_completion\r\n    |     generator = await handler.create_completion(request, raw_request)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_completion.py\", line 189, in create_completion\r\n    |     async for i, res in result_generator:\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/vllm/utils.py\", line 444, in merge_async_iterators\r\n    |     item = await d\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 1051, in generate\r\n    |     async for output in await self.add_request(\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 113, in generator\r\n    |     raise result\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 55, in _log_task_completion\r\n    |     return_value = task.result()\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 872, in run_engine_loop\r\n    |     result = task.result()\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 795, in engine_step\r\n    |     request_outputs = await self.engine.step_async(virtual_engine)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 347, in step_async\r\n    |     outputs = await self.model_executor.execute_model_async(\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/cpu_executor.py\", line 298, in execute_model_async\r\n    |     output = await make_async(self.execute_model\r\n    |   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    |     result = self.fn(*self.args, **self.kwargs)\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/cpu_executor.py\", line 210, in execute_model\r\n    |     output = self.driver_method_invoker(self.driver_worker,\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/cpu_executor.py\", line 311, in _async_driver_method_invoker\r\n    |     return driver.execute_method(method, *args, **kwargs).get()\r\n    |   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 54, in get\r\n    |     raise self.result.exception\r\n    | ChildProcessError: worker died\r\n    +------------------------------------\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 401, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 113, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/base.py\", line 185, in __call__\r\n    with collapse_excgroups():\r\n  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\r\n    self.gen.throw(typ, value, traceback)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_utils.py\", line 82, in collapse_excgroups\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/base.py\", line 187, in __call__\r\n    response = await self.dispatch_func(request, call_next)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 491, in add_request_id\r\n    response = await call_next(request)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/base.py\", line 163, in call_next\r\n    raise app_exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/base.py\", line 149, in coro\r\n    await self.app(scope, receive_or_disconnect, send_no_error)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 715, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 735, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 288, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 76, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 73, in app\r\n    response = await f(request)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 301, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 212, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 367, in create_completion\r\n    generator = await handler.create_completion(request, raw_request)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_completion.py\", line 189, in create_completion\r\n    async for i, res in result_generator:\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/utils.py\", line 444, in merge_async_iterators\r\n    item = await d\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 1051, in generate\r\n    async for output in await self.add_request(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 113, in generator\r\n    raise result\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 55, in _log_task_completion\r\n    return_value = task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 872, in run_engine_loop\r\n    result = task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 795, in engine_step\r\n    request_outputs = await self.engine.step_async(virtual_engine)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 347, in step_async\r\n    outputs = await self.model_executor.execute_model_async(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/cpu_executor.py\", line 298, in execute_model_async\r\n    output = await make_async(self.execute_model\r\n  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/cpu_executor.py\", line 210, in execute_model\r\n    output = self.driver_method_invoker(self.driver_worker,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/cpu_executor.py\", line 311, in _async_driver_method_invoker\r\n    return driver.execute_method(method, *args, **kwargs).get()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 54, in get\r\n    raise self.result.exception\r\nChildProcessError: worker died\r\nINFO 11-20 06:33:25 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 11-20 06:33:35 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\n```\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-11-20T06:39:21+00:00",
    "closed_at": null,
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10478/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/10478"
  },
  {
    "number": 9032,
    "title": "[Performance]: Transformers 4.45.1 slows down `outlines` guided decoding",
    "body": "### Report of performance regression\r\n\r\nI noticed that guided decoding was a bit slower on newer builds of vllm, but couldn't track down a commit that caused a performance regression. Instead it looks like upgrading transformers from `4.44.2` to `4.45.1` causes the issue.\r\n\r\nI ran a small artillery test with requests using guided decoding, using the code from commit `4f1ba0844`. This is the last commit before `mllama` support was added, so it's the last point where vllm will work with both transformers versions `4.44.2` and `4.45.1`. VLLM was run with 1xA100 gpu, using model `mistralai/Mistral-7B-Instruct-v0.2`\r\n\r\nThe results with `4.44.2` installed:\r\n```\r\nhttp.codes.200: ................................................................ 240\r\nhttp.downloaded_bytes: ......................................................... 91928\r\nhttp.request_rate: ............................................................. 3/sec\r\nhttp.requests: ................................................................. 240\r\nhttp.response_time:\r\n  min: ......................................................................... 105\r\n  max: ......................................................................... 16348\r\n  mean: ........................................................................ 6655.3\r\n  median: ...................................................................... 3905.8\r\n  p95: ......................................................................... 15526\r\n  p99: ......................................................................... 16159.7\r\nhttp.responses: ................................................................ 240\r\nvusers.completed: .............................................................. 60\r\nvusers.created: ................................................................ 60\r\nvusers.created_by_name.Test completions: ....................................... 60\r\nvusers.failed: ................................................................. 0\r\nvusers.session_length:\r\n  min: ......................................................................... 15318.1\r\n  max: ......................................................................... 38021.7\r\n  mean: ........................................................................ 26628.2\r\n  median: ...................................................................... 27730.6\r\n  p95: ......................................................................... 33199.7\r\n  p99: ......................................................................... 35964.9\r\n```\r\n\r\n\r\nand with `4.45.1` installed:\r\n```\r\nhttp.codes.200: ................................................................ 240\r\nhttp.downloaded_bytes: ......................................................... 92209\r\nhttp.request_rate: ............................................................. 3/sec\r\nhttp.requests: ................................................................. 240\r\nhttp.response_time:\r\n  min: ......................................................................... 100\r\n  max: ......................................................................... 27083\r\n  mean: ........................................................................ 10279.2\r\n  median: ...................................................................... 5065.6\r\n  p95: ......................................................................... 26115.6\r\n  p99: ......................................................................... 27181.5\r\nhttp.responses: ................................................................ 240\r\nvusers.completed: .............................................................. 60\r\nvusers.created: ................................................................ 60\r\nvusers.created_by_name.Test completions: ....................................... 60\r\nvusers.failed: ................................................................. 0\r\nvusers.session_length:\r\n  min: ......................................................................... 19387.6\r\n  max: ......................................................................... 55055.6\r\n  mean: ........................................................................ 41123.7\r\n  median: ...................................................................... 43928\r\n  p95: ......................................................................... 51550.2\r\n  p99: ......................................................................... 53654.1\r\n```\r\n\r\nThe slowdown looks pretty significant to me \ud83d\udc0c\ud83d\udc0c\ud83d\udc0c\r\n\r\nI wasn't able to get the vllm profiling to work to try to dig in at all, unfortunately it kept crashing with encoding errors whenever I ran any requests with guided decoding. So, I don't know if this is a problem with vllm, with outlines, or with transformers. But given that outlines hasn't been updated in quite a while and `sglang` went and forked it- I'm not sure if this is worth investigating as is or if it'll be overcome by events.\r\n\r\nAnybody have ideas about what could be going wrong?\r\n\r\n<details>\r\n<summary>The scripts I ran:</summary>\r\n<br>\r\n\r\nartillery.yaml\r\n```\r\nconfig:\r\n  timeout: 100\r\n  target: http://rundemc-dev-service:8000\r\n  phases:\r\n    - duration: 180\r\n      arrivalRate: 1\r\n      name: Load test\r\n\r\n  payload:\r\n    # path is relative to the location of the test script\r\n    path: 'payloads.csv'\r\n    fields:\r\n      - prompt\r\n    name: unused\r\n\r\n  variables:\r\n    model_id:\r\n      - \"mistralai/Mistral-7B-Instruct-v0.2\"\r\n    backend:\r\n      - \"lm-format-enforcer\"\r\n\r\n\r\nscenarios:\r\n  - name: Test completions\r\n    flow:\r\n      - post:\r\n          url: \"/v1/completions\"\r\n          json:\r\n            model: \"{{ model_id }}\"\r\n            prompt: \"{{ prompt }}\"\r\n            max_tokens: 40\r\n      - post:\r\n          url: \"/v1/completions\"\r\n          json:\r\n            model: \"{{ model_id }}\"\r\n            prompt: \"{{ prompt }}\"\r\n            max_tokens: 40\r\n            guided_decoding_backend: \"{{ backend }}\"\r\n            guided_choice:\r\n              - \"foo\"\r\n              - \"bar\"\r\n              - \"baz\"\r\n              - \"buzz\"\r\n      - post:\r\n          url: \"/v1/completions\"\r\n          json:\r\n            model: \"{{ model_id }}\"\r\n            prompt: \"{{ prompt }}\"\r\n            max_tokens: 40\r\n            guided_decoding_backend: \"{{ backend }}\"\r\n            response_format:\r\n              type: \"json_object\"\r\n      - post:\r\n          url: \"/v1/completions\"\r\n          json:\r\n            model: \"{{ model_id }}\"\r\n            prompt: \"{{ prompt }}\"\r\n            max_tokens: 40\r\n            guided_decoding_backend: \"{{ backend }}\"\r\n            guided_json:\r\n              type: \"object\"\r\n              properties:\r\n                name:\r\n                  type: string\r\n                age:\r\n                  type: integer\r\n```\r\n\r\npayloads.csv\r\n```\r\n\"hello world this is jesus\"\r\n\"Lorem ipsum dolor\"\r\n\"Write a function that sums two numbers together\"\r\n```\r\n\r\n(obviously very scientific \ud83d\ude09 )\r\n\r\n</details>\r\n\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance",
      "structured-output"
    ],
    "state": "open",
    "created_at": "2024-10-02T22:28:16+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9032/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/9032"
  },
  {
    "number": 11255,
    "title": "[Feature]: LoRA support for qwen2-vl Models",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nI fine-tuned a qwen2-vl-7b model using llama factory, deployed it with AsyncLLMEngine, and loaded the LoRA adapter using lora_request. However, the inference results are significantly worse compared to the merged model.\r\n\r\n<img width=\"822\" alt=\"image\" src=\"https://github.com/user-attachments/assets/29bb01c1-2507-4fab-972f-ac4245d884a3\" />\r\n\r\n\r\nIt would be great if we can have the support for LoRA for multimodal models as our team wants to use multiple LoRAs and merging the LoRA adapters to original model weights is not feasible for us. We are short on time for this project and as far as I can tell no other framework supports LoRA in this way. Also we need outlines for structured generation so vLLM (being the most user friendly, stable and mature framework ) is our best bet now. Can we get a timeline when will this be supported ? Also are there any workarounds possible until this feature is officially supported ?\r\n\r\nThank you for your adaptation.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "open",
    "created_at": "2024-12-17T06:41:58+00:00",
    "closed_at": null,
    "comments": 16,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11255/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11255"
  },
  {
    "number": 12052,
    "title": "[Bug]: PaliGemma2 not working with OpenAI Docker serve",
    "body": "### Your current environment\n\nJust using Docker image 0.6.6post1\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nJust try to run https://huggingface.co/google/paligemma2-3b-pt-896 using Docker vllm image. My docker compose follows:\n\n```\nservices:\n  app:\n    image: vllm/vllm-openai:latest\n    runtime: nvidia\n    restart: unless-stopped\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./cache:/root/.cache/huggingface\n    environment:\n      - HUGGING_FACE_HUB_TOKEN=hf_\n    ipc: host\n    command:\n      - --host\n      - 0.0.0.0\n      - --model\n      - google/paligemma2-3b-pt-896\n      - --limit-mm-per-prompt\n      - 'image=1'\n      - --trust-remote-code\n      - --max-model-len\n      - \"8192\"\n\n```\n\nIt does NOT work, the issue is the same reported in the pull request here: https://github.com/vllm-project/vllm/pull/11142#issuecomment-2541342321\nand is:\n`ValueError: As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.`\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "unstale"
    ],
    "state": "open",
    "created_at": "2025-01-14T19:22:48+00:00",
    "closed_at": null,
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12052/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12052"
  },
  {
    "number": 11168,
    "title": "[Bug]: vllm.core.block.interfaces.BlockAllocator.NoFreeBlocksError to old Mistral Model",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.26.4\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.25-051525-generic-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   39 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          16\r\nOn-line CPU(s) list:             0-15\r\nVendor ID:                       GenuineIntel\r\nModel name:                      13th Gen Intel(R) Core(TM) i5-13400\r\nCPU family:                      6\r\nModel:                           191\r\nThread(s) per core:              2\r\nCore(s) per socket:              10\r\nSocket(s):                       1\r\nStepping:                        2\r\nCPU max MHz:                     3425.8350\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        4992.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx\r\npdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx\r\n est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb\r\n invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap\r\n clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req umip pku ospke wa\r\nitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize arch_lbr flush_l1d arch_capabilities\r\nVirtualization:                  VT-x\r\nL1d cache:                       416 KiB (10 instances)\r\nL1i cache:                       448 KiB (10 instances)\r\nL2 cache:                        9.5 MiB (7 instances)\r\nL3 cache:                        20 MiB (1 instance)\r\nNUMA node(s):                    1\r\nNUMA node0 CPU(s):               0-15\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] optree==0.12.1\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchaudio==2.4.0\r\n[pip3] torchelastic==0.2.2\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.46.0\r\n[pip3] triton==3.0.0\r\n[conda] blas                      1.0                         mkl\r\n[conda] cuda-cudart               12.1.105                      0    nvidia\r\n[conda] cuda-cupti                12.1.105                      0    nvidia\r\n[conda] cuda-libraries            12.1.0                        0    nvidia\r\n[conda] cuda-nvrtc                12.1.105                      0    nvidia\r\n[conda] cuda-nvtx                 12.1.105                      0    nvidia\r\n[conda] cuda-opencl               12.5.39                       0    nvidia\r\n[conda] cuda-runtime              12.1.0                        0    nvidia\r\n[conda] cuda-version              12.5                          3    nvidia\r\n[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\r\n[conda] libcublas                 12.1.0.26                     0    nvidia\r\n[conda] libcufft                  11.0.2.4                      0    nvidia\r\n[conda] libcufile                 1.10.1.7                      0    nvidia\r\n[conda] libcurand                 10.3.6.82                     0    nvidia\r\n[conda] libcusolver               11.4.4.55                     0    nvidia\r\n[conda] libcusparse               12.0.2.55                     0    nvidia\r\n[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\r\n[conda] libnpp                    12.0.2.50                     0    nvidia\r\n[conda] libnvjitlink              12.1.105                      0    nvidia\r\n[conda] libnvjpeg                 12.1.1.14                     0    nvidia\r\n[conda] mkl                       2023.1.0         h213fc3f_46344\r\n[conda] mkl-service               2.4.0           py311h5eee18b_1\r\n[conda] mkl_fft                   1.3.8           py311h5eee18b_0\r\n[conda] mkl_random                1.2.4           py311hdb19cb5_0\r\n[conda] numpy                     1.26.4          py311h08b1b3b_0\r\n[conda] numpy-base                1.26.4          py311hf175353_0\r\n[conda] optree                    0.12.1                   pypi_0    pypi\r\n[conda] pytorch                   2.4.0           py3.11_cuda12.1_cudnn9.1.0_0    pytorch\r\n[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch\r\n[conda] pytorch-mutex             1.0                        cuda    pytorch\r\n[conda] torchaudio                2.4.0               py311_cu121    pytorch\r\n[conda] torchelastic              0.2.2                    pypi_0    pypi\r\n[conda] torchtriton               3.0.0                     py311    pytorch\r\n[conda] torchvision               0.19.0              py311_cu121    pytorch\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.1.dev3151+g31c7178 (git sha: 31c7178\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n\r\nNVIDIA_VISIBLE_DEVICES=GPU-bce6a93f-3c2e-5454-a4e9-ab8b98b80cae\r\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\r\nPYTORCH_VERSION=2.4.0\r\nLD_LIBRARY_PATH=/workspace-lib/lib/python3.11/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nHere is what is logged from `vllm serve`\r\n\r\n```\r\nERROR 12-13 08:58:11 engine.py:165] NoFreeBlocksError()\r\nERROR 12-13 08:58:11 engine.py:165] Traceback (most recent call last):\r\nERROR 12-13 08:58:11 engine.py:165] File \"/workspace-lib/lib/python3.11/site-packages/vllm/engine/multiprocessing/engine.py\", line 163, in start\r\nERROR 12-13 08:58:11 engine.py:165] self.run_engine_loop()\r\nERROR 12-13 08:58:11 engine.py:165] File \"/workspace-lib/lib/python3.11/site-packages/vllm/engine/multiprocessing/engine.py\", line 226, in run_engine_loop\r\nERROR 12-13 08:58:11 engine.py:165] request_outputs = self.engine_step()\r\nERROR 12-13 08:58:11 engine.py:165] ^^^^^^^^^^^^^^^^^^\r\nERROR 12-13 08:58:11 engine.py:165] File \"/workspace-lib/lib/python3.11/site-packages/vllm/engine/multiprocessing/engine.py\", line 244, in engine_step\r\nERROR 12-13 08:58:11 engine.py:165] raise e\r\nERROR 12-13 08:58:11 engine.py:165] File \"/workspace-lib/lib/python3.11/site-packages/vllm/engine/multiprocessing/engine.py\", line 235, in engine_step\r\nERROR 12-13 08:58:11 engine.py:165] return self.engine.step()\r\nERROR 12-13 08:58:11 engine.py:165] ^^^^^^^^^^^^^^^^^^\r\nERROR 12-13 08:58:11 engine.py:165] File \"/workspace-lib/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 1374, in step\r\nERROR 12-13 08:58:11 engine.py:165] ) = self.scheduler[virtual_engine].schedule()\r\nERROR 12-13 08:58:11 engine.py:165] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-13 08:58:11 engine.py:165] File \"/workspace-lib/lib/python3.11/site-packages/vllm/core/scheduler.py\", line 1219, in schedule\r\nERROR 12-13 08:58:11 engine.py:165] scheduler_outputs: SchedulerOutputs = self._schedule()\r\nERROR 12-13 08:58:11 engine.py:165] ^^^^^^^^^^^^^^^^\r\nERROR 12-13 08:58:11 engine.py:165] File \"/workspace-lib/lib/python3.11/site-packages/vllm/core/scheduler.py\", line 1178, in _schedule\r\nERROR 12-13 08:58:11 engine.py:165] return self._schedule_default()\r\nERROR 12-13 08:58:11 engine.py:165] ^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-13 08:58:11 engine.py:165] File \"/workspace-lib/lib/python3.11/site-packages/vllm/core/scheduler.py\", line 1013, in _schedule_default\r\nERROR 12-13 08:58:11 engine.py:165] prefills = self._schedule_prefills(budget,\r\nERROR 12-13 08:58:11 engine.py:165] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-13 08:58:11 engine.py:165] File \"/workspace-lib/lib/python3.11/site-packages/vllm/core/scheduler.py\", line 949, in _schedule_prefills\r\nERROR 12-13 08:58:11 engine.py:165] self._allocate_and_set_running(seq_group)\r\nERROR 12-13 08:58:11 engine.py:165] File \"/workspace-lib/lib/python3.11/site-packages/vllm/core/scheduler.py\", line 1413, in _allocate_and_set_running\r\nERROR 12-13 08:58:11 engine.py:165] self.block_manager.allocate(seq_group)\r\nERROR 12-13 08:58:11 engine.py:165] File \"/workspace-lib/lib/python3.11/site-packages/vllm/core/block_manager.py\", line 169, in allocate\r\nERROR 12-13 08:58:11 engine.py:165] block_table: BlockTable = self._allocate_sequence(seq)\r\nERROR 12-13 08:58:11 engine.py:165] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-13 08:58:11 engine.py:165] File \"/workspace-lib/lib/python3.11/site-packages/vllm/core/block_manager.py\", line 155, in _allocate_sequence\r\nERROR 12-13 08:58:11 engine.py:165] block_table.allocate(seq.get_token_ids())\r\nERROR 12-13 08:58:11 engine.py:165] File \"/workspace-lib/lib/python3.11/site-packages/vllm/core/block/block_table.py\", line 96, in allocate\r\nERROR 12-13 08:58:11 engine.py:165] blocks = self._allocate_blocks_for_token_ids(prev_block=None,\r\nERROR 12-13 08:58:11 engine.py:165] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-13 08:58:11 engine.py:165] File \"/workspace-lib/lib/python3.11/site-packages/vllm/core/block/block_table.py\", line 277, in _allocate_blocks_for_token_ids\r\nERROR 12-13 08:58:11 engine.py:165] self._allocator.allocate_immutable_blocks(\r\nERROR 12-13 08:58:11 engine.py:165] File \"/workspace-lib/lib/python3.11/site-packages/vllm/core/block/cpu_gpu_block_allocator.py\", line 150, in allocate_immutable_blocks\r\nERROR 12-13 08:58:11 engine.py:165] return self._allocators[device].allocate_immutable_blocks(\r\nERROR 12-13 08:58:11 engine.py:165] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-13 08:58:11 engine.py:165] File \"/workspace-lib/lib/python3.11/site-packages/vllm/core/block/naive_block.py\", line 94, in allocate_immutable_blocks\r\nERROR 12-13 08:58:11 engine.py:165] block_ids.append(self._allocate_block_id())\r\nERROR 12-13 08:58:11 engine.py:165] ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-13 08:58:11 engine.py:165] File \"/workspace-lib/lib/python3.11/site-packages/vllm/core/block/naive_block.py\", line 130, in _allocate_block_id\r\nERROR 12-13 08:58:11 engine.py:165] raise BlockAllocator.NoFreeBlocksError()\r\nERROR 12-13 08:58:11 engine.py:165] vllm.core.block.interfaces.BlockAllocator.NoFreeBlocksError\r\n```\r\n\r\nHere is the config file of the model I serve\r\n\r\n```json\r\n{\r\n  \"_name_or_path\": \"mistral-7B\",\r\n  \"architectures\": [\r\n    \"MistralForCausalLM\"\r\n  ],\r\n  \"attention_dropout\": 0.0,\r\n  \"bos_token_id\": 1,\r\n  \"eos_token_id\": 32000,\r\n  \"hidden_act\": \"silu\",\r\n  \"hidden_size\": 4096,\r\n  \"initializer_range\": 0.02,\r\n  \"intermediate_size\": 14336,\r\n  \"max_position_embeddings\": 8192,\r\n  \"model_type\": \"mistral\",\r\n  \"num_attention_heads\": 32,\r\n  \"num_hidden_layers\": 32,\r\n  \"num_key_value_heads\": 8,\r\n  \"rms_norm_eps\": 1e-05,\r\n  \"rope_theta\": 10000.0,\r\n  \"sliding_window\": 4096,\r\n  \"tie_word_embeddings\": false,\r\n  \"torch_dtype\": \"bfloat16\",\r\n  \"transformers_version\": \"4.36.2\",\r\n  \"use_cache\": true,\r\n  \"vocab_size\": 32019\r\n}\r\n```\r\n\r\nI suspect there is problem because of the model. I'm using mistral as base model.\r\n\r\nHere is how I serve the model\r\n\r\n```bash\r\nvllm serve my-model --host 0.0.0.0 --served-model-name my-model --port 8000 --enforce-eager\r\n```\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-12-13T09:06:59+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11168/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11168"
  },
  {
    "number": 8432,
    "title": "[Bug]: ValueError: could not broadcast input array from shape (513,) into shape (512,)",
    "body": "### Your current environment\n\nCollecting environment information...\r\n/home/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/cuda/init.py:128: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 2: out of memory (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\r\nreturn torch._C._cuda_getDeviceCount() > 0\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.22.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:12:24) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: 12.5.82\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration:\r\nGPU 0: NVIDIA RTX 4000 Ada Generation\r\nGPU 1: NVIDIA RTX 4000 Ada Generation\r\nGPU 2: NVIDIA RTX 4000 Ada Generation\r\nGPU 3: NVIDIA RTX 4000 Ada Generation\r\nGPU 4: NVIDIA RTX 4000 Ada Generation\r\nGPU 5: NVIDIA RTX 4000 Ada Generation\r\nGPU 6: NVIDIA RTX 4000 Ada Generation\r\nGPU 7: NVIDIA RTX 4000 Ada Generation\r\n\r\nNvidia driver version: 555.99\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture: x86_64\r\nCPU op-mode(s): 32-bit, 64-bit\r\nAddress sizes: 48 bits physical, 48 bits virtual\r\nByte Order: Little Endian\r\nCPU(s): 48\r\nOn-line CPU(s) list: 0-47\r\nVendor ID: AuthenticAMD\r\nModel name: AMD Ryzen Threadripper 7960X 24-Cores\r\nCPU family: 25\r\nModel: 24\r\nThread(s) per core: 2\r\nCore(s) per socket: 24\r\nSocket(s): 1\r\nStepping: 1\r\nBogoMIPS: 8387.54\r\nFlags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr arat npt nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm\r\nVirtualization: AMD-V\r\nHypervisor vendor: Microsoft\r\nVirtualization type: full\r\nL1d cache: 768 KiB (24 instances)\r\nL1i cache: 768 KiB (24 instances)\r\nL2 cache: 24 MiB (24 instances)\r\nL3 cache: 32 MiB (1 instance)\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit: Not affected\r\nVulnerability L1tf: Not affected\r\nVulnerability Mds: Not affected\r\nVulnerability Meltdown: Not affected\r\nVulnerability Mmio stale data: Not affected\r\nVulnerability Retbleed: Not affected\r\nVulnerability Spec rstack overflow: Mitigation; safe RET\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2: Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds: Not affected\r\nVulnerability Tsx async abort: Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.68\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.2\r\n[pip3] triton==3.0.0\r\n[conda] numpy 1.26.4 pypi_0 pypi\r\n[conda] nvidia-cublas-cu12 12.1.3.1 pypi_0 pypi\r\n[conda] nvidia-cuda-cupti-cu12 12.1.105 pypi_0 pypi\r\n[conda] nvidia-cuda-nvrtc-cu12 12.1.105 pypi_0 pypi\r\n[conda] nvidia-cuda-runtime-cu12 12.1.105 pypi_0 pypi\r\n[conda] nvidia-cudnn-cu12 9.1.0.70 pypi_0 pypi\r\n[conda] nvidia-cufft-cu12 11.0.2.54 pypi_0 pypi\r\n[conda] nvidia-curand-cu12 10.3.2.106 pypi_0 pypi\r\n[conda] nvidia-cusolver-cu12 11.4.5.107 pypi_0 pypi\r\n[conda] nvidia-cusparse-cu12 12.1.0.106 pypi_0 pypi\r\n[conda] nvidia-ml-py 12.560.30 pypi_0 pypi\r\n[conda] nvidia-nccl-cu12 2.20.5 pypi_0 pypi\r\n[conda] nvidia-nvjitlink-cu12 12.6.68 pypi_0 pypi\r\n[conda] nvidia-nvtx-cu12 12.1.105 pypi_0 pypi\r\n[conda] pyzmq 26.2.0 pypi_0 pypi\r\n[conda] torch 2.4.0 pypi_0 pypi\r\n[conda] torchvision 0.19.0 pypi_0 pypi\r\n[conda] transformers 4.44.2 pypi_0 pypi\r\n[conda] triton 3.0.0 pypi_0 pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0 GPU1 GPU2 GPU3 GPU4 GPU5 GPU6 GPU7 CPU Affinity NUMA Affinity GPU NUMA ID\r\nGPU0 X SYS SYS SYS SYS SYS SYS SYS N/A\r\nGPU1 SYS X SYS SYS SYS SYS SYS SYS N/A\r\nGPU2 SYS SYS X SYS SYS SYS SYS SYS N/A\r\nGPU3 SYS SYS SYS X SYS SYS SYS SYS N/A\r\nGPU4 SYS SYS SYS SYS X SYS SYS SYS N/A\r\nGPU5 SYS SYS SYS SYS SYS X SYS SYS N/A\r\nGPU6 SYS SYS SYS SYS SYS SYS X SYS N/A\r\nGPU7 SYS SYS SYS SYS SYS SYS SYS X N/A\r\n\r\nLegend:\r\n\r\nX = Self\r\nSYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\nNODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\nPHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\nPXB = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\nPIX = Connection traversing at most a single PCIe bridge\r\nNV# = Connection traversing a bonded set of # NVLinks\r\n\n\n### Model Input Dumps\n\nINFO:     127.0.0.1:47618 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nERROR 09-12 19:15:10 async_llm_engine.py:63] Engine background task failed\r\nERROR 09-12 19:15:10 async_llm_engine.py:63] Traceback (most recent call last):\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]   File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 53, in _log_task_completion\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]     return_value = task.result()\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]                    ^^^^^^^^^^^^^\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]   File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 939, in run_engine_loop\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]     result = task.result()\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]              ^^^^^^^^^^^^^\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]   File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 868, in engine_step\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]     request_outputs = await self.engine.step_async(virtual_engine)\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]   File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 345, in step_async\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]     output = await self.model_executor.execute_model_async(\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]   File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/executor/distributed_gpu_executor.py\", line 177, in execute_model_async\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]     return await self._driver_execute_model_async(execute_model_req)\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]   File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 231, in _driver_execute_model_async\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]     return await self.driver_exec_model(execute_model_req)\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]   File \"/home/0/miniconda3/envs/vllm/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]     result = self.fn(*self.args, **self.kwargs)\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]   File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/worker/worker_base.py\", line 303, in execute_model\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]     inputs = self.prepare_input(execute_model_req)\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]   File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/worker/multi_step_worker.py\", line 164, in prepare_input\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]     kwargs) = self._get_driver_input_and_broadcast(execute_model_req)\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]   File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/worker/multi_step_worker.py\", line 62, in _get_driver_input_and_broadcast\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]     self.model_runner.prepare_model_input(\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]   File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/worker/multi_step_model_runner.py\", line 254, in prepare_model_input\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]     frozen_model_input = self._base_model_runner.prepare_model_input(\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]   File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 1380, in prepare_model_input\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]     model_input = self._prepare_model_input_tensors(\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]   File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 1042, in _prepare_model_input_tensors\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]     return builder.build()  # type: ignore\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]            ^^^^^^^^^^^^^^^\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]   File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 741, in build\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]     attn_metadata = self.attn_metadata_builder.build(\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]   File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/attention/backends/flash_attn.py\", line 467, in build\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]     input_block_tables[i, :len(block_table)] = block_table\r\nERROR 09-12 19:15:10 async_llm_engine.py:63]     ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-12 19:15:10 async_llm_engine.py:63] ValueError: could not broadcast input array from shape (513,) into shape (512,)\r\nException in callback functools.partial(<function _log_task_completion at 0x7f81020a6ca0>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7f80fe6315e0>>)\r\nhandle: <Handle functools.partial(<function _log_task_completion at 0x7f81020a6ca0>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7f80fe6315e0>>)>\r\nTraceback (most recent call last):\r\n  File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 53, in _log_task_completion\r\n    return_value = task.result()\r\n                   ^^^^^^^^^^^^^\r\n  File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 939, in run_engine_loop\r\n    result = task.result()\r\n             ^^^^^^^^^^^^^\r\n  File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 868, in engine_step\r\n    request_outputs = await self.engine.step_async(virtual_engine)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 345, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/executor/distributed_gpu_executor.py\", line 177, in execute_model_async\r\n    return await self._driver_execute_model_async(execute_model_req)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 231, in _driver_execute_model_async\r\n    return await self.driver_exec_model(execute_model_req)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/0/miniconda3/envs/vllm/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/worker/worker_base.py\", line 303, in execute_model\r\n    inputs = self.prepare_input(execute_model_req)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/worker/multi_step_worker.py\", line 164, in prepare_input\r\n    kwargs) = self._get_driver_input_and_broadcast(execute_model_req)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/worker/multi_step_worker.py\", line 62, in _get_driver_input_and_broadcast\r\n    self.model_runner.prepare_model_input(\r\n  File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/worker/multi_step_model_runner.py\", line 254, in prepare_model_input\r\n    frozen_model_input = self._base_model_runner.prepare_model_input(\r\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 1380, in prepare_model_input\r\n    model_input = self._prepare_model_input_tensors(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 1042, in _prepare_model_input_tensors\r\n    return builder.build()  # type: ignore\r\n           ^^^^^^^^^^^^^^^\r\n  File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 741, in build\r\n    attn_metadata = self.attn_metadata_builder.build(\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/attention/backends/flash_attn.py\", line 467, in build\r\n    input_block_tables[i, :len(block_table)] = block_table\r\n    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\nValueError: could not broadcast input array from shape (513,) into shape (512,)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\r\n  File \"/home/0/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 65, in _log_task_completion\r\n    raise AsyncEngineDeadError(\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for the actual cause.\r\nERROR 09-12 19:15:10 client.py:266] Got Unhealthy response from RPC Server\r\nERROR 09-12 19:15:10 client.py:412] AsyncEngineDeadError('Background loop is stopped.')\r\n\n\n### \ud83d\udc1b Describe the bug\n\nThe error: \"ValueError: could not broadcast input array from shape (513,) into shape (512,)\"\r\nIt looks like when I use --num-scheduler-step of any values I experience the above message mid-processing. When I remove it, I no longer have the same error. \r\nI have tried to change context length, max tokens, batch size, reinstall vllm, etc.. nothing helps. \r\nThank you for the help!\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-09-12T23:40:44+00:00",
    "closed_at": null,
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8432/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8432"
  },
  {
    "number": 9462,
    "title": "[Bug]: Error with structured output inference after upgrade 0.6.2->0.6.3",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nollecting environment information...\r\n/opt/conda/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\r\nNo module named 'vllm._version'\r\n  from vllm.version import __version__ as VLLM_VERSION\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:53:32) [GCC 12.3.0] (64-bit runtime)\r\nPython platform: Linux-6.1.109-118.189.amzn2023.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A10G\r\nNvidia driver version: 560.35.03\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               8\r\nOn-line CPU(s) list:                  0-7\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD EPYC 7R32\r\nCPU family:                           23\r\nModel:                                49\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   4\r\nSocket(s):                            1\r\nStepping:                             0\r\nBogoMIPS:                             5599.99\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save rdpid\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            128 KiB (4 instances)\r\nL1i cache:                            128 KiB (4 instances)\r\nL2 cache:                             2 MiB (4 instances)\r\nL3 cache:                             16 MiB (1 instance)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-7\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec rstack overflow:   Mitigation; safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.99\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==25.1.2\r\n[pip3] torch==2.4.0\r\n[pip3] torchaudio==2.2.1+cu121\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.2\r\n[pip3] triton==3.0.0\r\n[conda] nomkl                     1.0                  h5ca1d4c_0    conda-forge\r\n[conda] numpy                     1.26.4          py311h64a7726_0    conda-forge\r\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.4.99                  pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\r\n[conda] pyzmq                     25.1.2          py311h34ded2d_0    conda-forge\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchaudio                2.2.1+cu121              pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.45.2                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A (dev)\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-7     0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nAfter upgrading from version 0.6.2 to 0.6.3 I started getting a validation error while generating structured input.\r\n\r\nTo reproduce:\r\n1. vllm serve NousResearch/Meta-Llama-3-8B-Instruct --dtype auto\r\n2. Execute the following code. In my case, I do it from a Jupyter Notebook:\r\n\r\n````{python}\r\n#### OUTPUT DEFINITION\r\n\r\nfrom pydantic import BaseModel, Field\r\nfrom enum import Enum\r\nfrom typing import List\r\nfrom typing import Optional\r\nimport json\r\nfrom openai import OpenAI\r\n\r\nclass BedType(Enum):\r\n    Twin = \"Twin\"\r\n    Double = \"Double\"\r\n    Queen = \"Queen\"\r\n    King = \"King\"\r\n    \r\nclass RoomBeds(BaseModel):\r\n    bed_type: BedType = Field(...,description=\"Type of the bed in the hotel room\")\r\n    quantity: int = Field(...,description=\"Number of beds of the given bed type within the hotel room\")\r\n\r\nclass HotelRoom(BaseModel):\r\n    \"\"\"\r\n    Represents a hotel room.\r\n    \"\"\"\r\n    room_id: str = Field(...,description=\"Id of the room from the input\")\r\n    room_name: Optional[str] = Field(...,description=\"Freetext name of the hotel room\")\r\n    room_class: Optional[str] = Field(..., description=\"Room class of the hotel room.\")\r\n    bed_types: Optional[List[RoomBeds]] = Field(..., description=\"List of beds within the hotel room.\")\r\n    smoking_allowed: Optional[bool] = Field(..., description=\"Flag that indicates whether smoking is allowed or not in the hotel room. Unknown value used if it cannot be infered from the room description\")\r\n\r\n\r\nclass Hotel(BaseModel):\r\n    \"\"\"\r\n    Represents an entry about a hotel.\r\n    \"\"\"\r\n    hotel_rooms: List[HotelRoom] = Field(..., description=\"List of hotel rooms within a hotel\")\r\n\r\n#### ONLINE INFERENCE\r\n\r\nclient = OpenAI(\r\n        base_url=\"http://localhost:8000/v1\",\r\n        api_key=\"token-abc123\",\r\n    )\r\n\r\n\r\ncompletion = client.beta.chat.completions.parse(\r\n            seed=42,\r\n            model= \"NousResearch/Meta-Llama-3-8B-Instruct\",\r\n            messages=[\r\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\r\n                {\"role\": \"user\", \"content\": \"Generate synthetic data for a fictitious hotel.\" },\r\n            ],\r\n            temperature=0.8,\r\n            top_p=0.95,\r\n            response_format=Hotel\r\n            )\r\n\r\n````\r\n\r\nWith version 0.6.2 I was always getting a structured output with the specified format. However, after upgrading to 0.6.3 I get a validation error as it seems the response does not match the expected format:\r\n\r\n```ValidationError                           Traceback (most recent call last)\r\nCell In[10], line 1\r\n----> 1 completion = client.beta.chat.completions.parse(\r\n      2             seed=42,\r\n      3             model= \"NousResearch/Meta-Llama-3-8B-Instruct\", # \"NousResearch/Meta-Llama-3-8B-Instruct\", #Hermes-2-Pro-Llama-3-8B-GGUF\r\n      4             messages=[\r\n      5                 {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\r\n      6                 {\"role\": \"user\", \"content\": \"Generate synthetic data for a fictitious hotel.\" },\r\n      7             ],\r\n      8             temperature=0.8,\r\n      9             top_p=0.95,\r\n     10             response_format=Hotel\r\n     11             )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/resources/beta/chat/completions.py:150, in Completions.parse(self, messages, model, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\r\n    143 def parser(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\r\n    144     return _parse_chat_completion(\r\n    145         response_format=response_format,\r\n    146         chat_completion=raw_completion,\r\n    147         input_tools=tools,\r\n    148     )\r\n--> 150 return self._post(\r\n    151     \"/chat/completions\",\r\n    152     body=maybe_transform(\r\n    153         {\r\n    154             \"messages\": messages,\r\n    155             \"model\": model,\r\n    156             \"frequency_penalty\": frequency_penalty,\r\n    157             \"function_call\": function_call,\r\n    158             \"functions\": functions,\r\n    159             \"logit_bias\": logit_bias,\r\n    160             \"logprobs\": logprobs,\r\n    161             \"max_completion_tokens\": max_completion_tokens,\r\n    162             \"max_tokens\": max_tokens,\r\n    163             \"metadata\": metadata,\r\n    164             \"n\": n,\r\n    165             \"parallel_tool_calls\": parallel_tool_calls,\r\n    166             \"presence_penalty\": presence_penalty,\r\n    167             \"response_format\": _type_to_response_format(response_format),\r\n    168             \"seed\": seed,\r\n    169             \"service_tier\": service_tier,\r\n    170             \"stop\": stop,\r\n    171             \"store\": store,\r\n    172             \"stream\": False,\r\n    173             \"stream_options\": stream_options,\r\n    174             \"temperature\": temperature,\r\n    175             \"tool_choice\": tool_choice,\r\n    176             \"tools\": tools,\r\n    177             \"top_logprobs\": top_logprobs,\r\n    178             \"top_p\": top_p,\r\n    179             \"user\": user,\r\n    180         },\r\n    181         completion_create_params.CompletionCreateParams,\r\n    182     ),\r\n    183     options=make_request_options(\r\n    184         extra_headers=extra_headers,\r\n    185         extra_query=extra_query,\r\n    186         extra_body=extra_body,\r\n    187         timeout=timeout,\r\n    188         post_parser=parser,\r\n    189     ),\r\n    190     # we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\r\n    191     # in the `parser` function above\r\n    192     cast_to=cast(Type[ParsedChatCompletion[ResponseFormatT]], ChatCompletion),\r\n    193     stream=False,\r\n    194 )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/_base_client.py:1277, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)\r\n   1263 def post(\r\n   1264     self,\r\n   1265     path: str,\r\n   (...)\r\n   1272     stream_cls: type[_StreamT] | None = None,\r\n   1273 ) -> ResponseT | _StreamT:\r\n   1274     opts = FinalRequestOptions.construct(\r\n   1275         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\r\n   1276     )\r\n-> 1277     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/_base_client.py:954, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\r\n    951 else:\r\n    952     retries_taken = 0\r\n--> 954 return self._request(\r\n    955     cast_to=cast_to,\r\n    956     options=options,\r\n    957     stream=stream,\r\n    958     stream_cls=stream_cls,\r\n    959     retries_taken=retries_taken,\r\n    960 )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/_base_client.py:1060, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\r\n   1057     log.debug(\"Re-raising status error\")\r\n   1058     raise self._make_status_error_from_response(err.response) from None\r\n-> 1060 return self._process_response(\r\n   1061     cast_to=cast_to,\r\n   1062     options=options,\r\n   1063     response=response,\r\n   1064     stream=stream,\r\n   1065     stream_cls=stream_cls,\r\n   1066     retries_taken=retries_taken,\r\n   1067 )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/_base_client.py:1159, in SyncAPIClient._process_response(self, cast_to, options, response, stream, stream_cls, retries_taken)\r\n   1156 if bool(response.request.headers.get(RAW_RESPONSE_HEADER)):\r\n   1157     return cast(ResponseT, api_response)\r\n-> 1159 return api_response.parse()\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/_response.py:319, in APIResponse.parse(self, to)\r\n    317 parsed = self._parse(to=to)\r\n    318 if is_given(self._options.post_parser):\r\n--> 319     parsed = self._options.post_parser(parsed)\r\n    321 if isinstance(parsed, BaseModel):\r\n    322     add_request_id(parsed, self.request_id)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/resources/beta/chat/completions.py:144, in Completions.parse.<locals>.parser(raw_completion)\r\n    143 def parser(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\r\n--> 144     return _parse_chat_completion(\r\n    145         response_format=response_format,\r\n    146         chat_completion=raw_completion,\r\n    147         input_tools=tools,\r\n    148     )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/lib/_parsing/_completions.py:110, in parse_chat_completion(response_format, input_tools, chat_completion)\r\n    100             else:\r\n    101                 tool_calls.append(tool_call)\r\n    103     choices.append(\r\n    104         construct_type_unchecked(\r\n    105             type_=cast(Any, ParsedChoice)[solve_response_format_t(response_format)],\r\n    106             value={\r\n    107                 **choice.to_dict(),\r\n    108                 \"message\": {\r\n    109                     **message.to_dict(),\r\n--> 110                     \"parsed\": maybe_parse_content(\r\n    111                         response_format=response_format,\r\n    112                         message=message,\r\n    113                     ),\r\n    114                     \"tool_calls\": tool_calls,\r\n    115                 },\r\n    116             },\r\n    117         )\r\n    118     )\r\n    120 return cast(\r\n    121     ParsedChatCompletion[ResponseFormatT],\r\n    122     construct_type_unchecked(\r\n   (...)\r\n    128     ),\r\n    129 )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/lib/_parsing/_completions.py:161, in maybe_parse_content(response_format, message)\r\n    155 def maybe_parse_content(\r\n    156     *,\r\n    157     response_format: type[ResponseFormatT] | ResponseFormatParam | NotGiven,\r\n    158     message: ChatCompletionMessage | ParsedChatCompletionMessage[object],\r\n    159 ) -> ResponseFormatT | None:\r\n    160     if has_rich_response_format(response_format) and message.content is not None and not message.refusal:\r\n--> 161         return _parse_content(response_format, message.content)\r\n    163     return None\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/lib/_parsing/_completions.py:221, in _parse_content(response_format, content)\r\n    219 def _parse_content(response_format: type[ResponseFormatT], content: str) -> ResponseFormatT:\r\n    220     if is_basemodel_type(response_format):\r\n--> 221         return cast(ResponseFormatT, model_parse_json(response_format, content))\r\n    223     if is_dataclass_like_type(response_format):\r\n    224         if not PYDANTIC_V2:\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/_compat.py:166, in model_parse_json(model, data)\r\n    164 def model_parse_json(model: type[_ModelT], data: str | bytes) -> _ModelT:\r\n    165     if PYDANTIC_V2:\r\n--> 166         return model.model_validate_json(data)\r\n    167     return model.parse_raw(data)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pydantic/main.py:625, in BaseModel.model_validate_json(cls, json_data, strict, context)\r\n    623 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\r\n    624 __tracebackhide__ = True\r\n--> 625 return cls.__pydantic_validator__.validate_json(json_data, strict=strict, context=context)\r\n\r\nValidationError: 1 validation error for Hotel\r\n  Invalid JSON: expected ident at line 1 column 2 [type=json_invalid, input_value='I\\'d be happy to help ge... requests or questions.', input_type=str]\r\n    For further information visit https://errors.pydantic.dev/2.9/v/json_invalid```\r\n\r\n\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-10-17T12:35:39+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9462/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9462"
  },
  {
    "number": 11010,
    "title": "[Bug]: unsloth/Llama-3.3-70B-Instruct-bnb-4bit can't work on vllm 0.6.4.post1",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nI'm using following command to load Llama3.3 on one GPU:\r\n```\r\nCUDA_VISIBLE_DEVICES=1  python3 -m vllm.entrypoints.openai.api_server --model unsloth/Llama-3.3-70B-Instruct-bnb-4bit            --host 0.0.0.0 --port 8081  --seed 42 --trust-remote-code --enable-chunked-prefill            --tensor-parallel-size 1 --max-model-len 1024\r\n```\r\n\r\nGot error:\r\n```\r\nLoading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\r\nERROR 12-09 00:49:29 engine.py:366] 'layers.0.mlp.down_proj.weight.absmax'\r\nERROR 12-09 00:49:29 engine.py:366] Traceback (most recent call last):\r\nERROR 12-09 00:49:29 engine.py:366]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 357, in run_mp_engine\r\nERROR 12-09 00:49:29 engine.py:366]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\r\nERROR 12-09 00:49:29 engine.py:366]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-09 00:49:29 engine.py:366]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 119, in from_engine_args\r\nERROR 12-09 00:49:29 engine.py:366]     return cls(ipc_path=ipc_path,\r\nERROR 12-09 00:49:29 engine.py:366]            ^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-09 00:49:29 engine.py:366]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 71, in __init__\r\nERROR 12-09 00:49:29 engine.py:366]     self.engine = LLMEngine(*args, **kwargs)\r\nERROR 12-09 00:49:29 engine.py:366]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-09 00:49:29 engine.py:366]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 347, in __init__\r\nERROR 12-09 00:49:29 engine.py:366]     self.model_executor = executor_class(vllm_config=vllm_config, )\r\nERROR 12-09 00:49:29 engine.py:366]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-09 00:49:29 engine.py:366]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 36, in __init__\r\nERROR 12-09 00:49:29 engine.py:366]     self._init_executor()\r\nERROR 12-09 00:49:29 engine.py:366]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/gpu_executor.py\", line 40, in _init_executor\r\nERROR 12-09 00:49:29 engine.py:366]     self.driver_worker.load_model()\r\nERROR 12-09 00:49:29 engine.py:366]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 152, in load_model\r\nERROR 12-09 00:49:29 engine.py:366]     self.model_runner.load_model()\r\nERROR 12-09 00:49:29 engine.py:366]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1074, in load_model\r\nERROR 12-09 00:49:29 engine.py:366]     self.model = get_model(vllm_config=self.vllm_config)\r\nERROR 12-09 00:49:29 engine.py:366]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-09 00:49:29 engine.py:366]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 12, in get_model\r\nERROR 12-09 00:49:29 engine.py:366]     return loader.load_model(vllm_config=vllm_config)\r\nERROR 12-09 00:49:29 engine.py:366]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-09 00:49:29 engine.py:366]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 334, in load_model\r\nERROR 12-09 00:49:29 engine.py:366]     model.load_weights(self._get_all_weights(model_config, model))\r\nERROR 12-09 00:49:29 engine.py:366]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 586, in load_weights\r\nERROR 12-09 00:49:29 engine.py:366]     loader.load_weights(\r\nERROR 12-09 00:49:29 engine.py:366]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 229, in load_weights\r\nERROR 12-09 00:49:29 engine.py:366]     autoloaded_weights = list(self._load_module(\"\", self.module, weights))\r\nERROR 12-09 00:49:29 engine.py:366]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-09 00:49:29 engine.py:366]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 190, in _load_module\r\nERROR 12-09 00:49:29 engine.py:366]     yield from self._load_module(prefix,\r\nERROR 12-09 00:49:29 engine.py:366]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 175, in _load_module\r\nERROR 12-09 00:49:29 engine.py:366]     module_load_weights(weights)\r\nERROR 12-09 00:49:29 engine.py:366]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 407, in load_weights\r\nERROR 12-09 00:49:29 engine.py:366]     param = params_dict[name]\r\nERROR 12-09 00:49:29 engine.py:366]             ~~~~~~~~~~~^^^^^^\r\nERROR 12-09 00:49:29 engine.py:366] KeyError: 'layers.0.mlp.down_proj.weight.absmax'\r\n```\r\n\r\nCould you please advice what is going on and how to fix it? \r\nThanks in advance!\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-12-09T08:56:39+00:00",
    "closed_at": null,
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11010/reactions",
      "total_count": 7,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11010"
  },
  {
    "number": 3203,
    "title": "ExLlamaV2: exl2 support",
    "body": "If is possible ExLlamaV2 is a very fast and good library to Run [LLM](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html)\r\n\r\n[ExLlamaV2 Repo](https://github.com/turboderp/exllamav2)",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-03-05T14:54:03+00:00",
    "closed_at": null,
    "comments": 38,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3203/reactions",
      "total_count": 35,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 35,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3203"
  },
  {
    "number": 8779,
    "title": "vLLM's V1 Engine Architecture",
    "body": "This issues describes the high level directions that \"create LLM Engine V1\". We want the design to be as transparent as possible and created this issue to track progress and solicit feedback. \r\n\r\nGoal:\r\n* The new engine will be simple and performant. We found the first iteration of the engine to be simple, the multistep engine to be performant, but we want best of the both worlds. For it to be performat, we want to **minimize GPU idle time**. \r\n* The new architecture will be extensible and modular. We found the current codebase becoming difficult to extend and add new features (both production and experimental features) due to the hard tangling of different features. In the new design, features should be compatible with each other.\r\n* Tech debts will be cleaned up. We will remove optimizations that compromise code readability. We will also redo ad-hoc implementations to support certain features/models. \r\n\r\nNon-goals, the following are important but orthogonal:\r\n* Optimize GPU time/kernels\r\n* Add new features/optimizations\r\n* Performance in rare cases\r\n\r\nThe scope is exclusively in the scheduler, memory manager, distributed architecture. We will not touch APIs, models, kernels, and most parts of the model runner. \r\n\r\nHighlights of the new design:\r\n* **Driver process + SPMD workers**\r\n    * When TP=n & PP=m, vLLM engine will have n*m + 1 processes in total.\r\n        * Corollary: even when using a single GPU, we will have 2 processes.\r\n    * The driver process will have the scheduler, memory manager, etc.\r\n    * The workers are stateful, maintaining most of the request states.\r\n        * The driver will only send the \u201cdiffs\u201d\r\n            * New request: input token IDs & block tables & sampling params, etc.\r\n            * In-flight request: scheduled request IDs, new block IDs (no token IDs, sampling params, etc.)\r\n    * Clean up data structures like SeqGroupMetadata\r\n* **Async single-step scheduling**, instead of multi-step scheduling\r\n    * Scheduler will schedule the n+1-th step, while the worker is executing the n-th step. \r\n    * We will reuse the code from multi-step scheduling to incrementally update the model inputs.\r\n    * Needs a special care for PP, since the output token IDs from the last stage should be sent to the first stage.\r\n* **De-tokenizer moves to the driver process**\r\n    * Async de-tokenization can be regarded as part of async scheduling\r\n* **Native support for different types of model states**\r\n    * Regular KV cache, Mamba cache, encoder cache, etc.\r\n    * Dedicated memory manager & block table for each type of cache\r\n* **Drop beam search from vLLM engine**\r\n    * Provide a solution to emulate beam search outside vLLM engine\r\n* **Prefix-caching as a first-class feature**\r\n    * Implement parallel sampling via prefix caching\r\n    * Remove the concept of SequenceGroup\r\n    * Optimize prefix caching overheads\r\n* **Remove/minimize PyObjectCache**\r\n\r\nLessons we learned from V1:\r\n\r\n* To achieve high GPU utilization, we should care about everything happening on the CPU.\r\n    * Python is slow.\r\n    * Fast GPUs like H100 do not necessarily have fast CPUs. They may have hundreds of CPU cores, but each with low clock speed.\r\n    * Moreover, GPUs will get faster and faster, while CPUs will not.\r\n* Scheduling is not cheap.\r\n    * For every step, the vLLM scheduler goes over the whole `self.running` queue and performs some operations for each request (e.g., allocating a new block). And this is written in Python.\r\n* Input broadcasting is expensive.\r\n    * Instead of sending request information from scheduler to workers every step, **the workers should be stateful** and maintain most of the request states.\r\n* Preparing the model & sampler inputs (e.g., block table) is expensive.\r\n    * We should cache the inputs of the previous steps, and** build new inputs incrementally from the cached inputs**, if possible.\r\n    * However, not every state should be kept in GPU memory. It\u2019s OK to cache & incrementally build some inputs in CPU memory, and send them to GPU every step.\r\n* De-tokenization is expensive.\r\n    * For every step, vLLM de-tokenizes the generated output token IDs and checks the stop criteria.\r\n    * The overhead becomes significant for large batch sizes.\r\n* Sampler is expensive.\r\n    * The GPU operations themselves are not very expensive.\r\n    * However, \u201cpythonizing\u201d the sampler outputs is expensive.\r\n    * Plus, the sampler can launch many small GPU kernels with CPU-GPU synchronizations. \r\n* Supporting different types of model states (e.g., KV cache, Mamba cache, encoder cache) is challenging.\r\n    * We need native cache managers for these different types of caches.\r\n    * We need to deal with memory fragmentation due to the different sizes of the different states\r\n\r\nTimeline wise, we plan to execute the changes incrementally. Overtime we will add PRs and issues related to the new architecture here. \r\n\r\n- [x] #8726\r\n\r\nThe design is led by the vLLM maintainers @WoosukKwon @zhuohan123 @youkaichao @simon-mo @LiuXiaoxuanPKU @comaniac @alexm-neuralmagic @njhill @robertgshaw2-neuralmagic @rkooo567  and many others!",
    "labels": [
      "RFC",
      "keep-open"
    ],
    "state": "open",
    "created_at": "2024-09-24T18:25:22+00:00",
    "closed_at": null,
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8779/reactions",
      "total_count": 86,
      "+1": 53,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 33,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8779"
  },
  {
    "number": 7366,
    "title": "[RFC]: Encoder/decoder models & feature compatibility",
    "body": "## Motivation <a href=\"#user-content-motivation\" id=\"motivation\">#</a>\r\n\r\nThere is significant interest in vLLM supporting encoder/decoder models. Issues #187  and #180 , for example, request encoder/decoder model support. As a result encoder/decoder support was recently introduced to vLLM via the following three PRs:\r\n\r\n* #4837 \r\n* #4888 \r\n* #4942 \r\n\r\nThese three PRs make encoder/decoder model inference possible; however, they leave more to be desired in terms of (1) parity between vLLM's decoder-only & encoder/decoder request processing pipelines with respect to feature support, and (2) the number of encoder/decoder models which are supported.\r\n\r\nThe ask for the vLLM community is to contribute PRs which help bring vLLM encoder/decoder functionality to a similar level of maturity as that of vLLM's decoder-only functionality.\r\n\r\n## Proposed changes <a href=\"#user-content-proposed-changes\" id=\"proposed-changes\">#</a>\r\n\r\nThe support matrix below summarizes which encoder/decoder models have already been added & which features are currently compatible with the vLLM encoder/decoder pipeline, versus which features & models will require additional PRs to implement in the long-term:\r\n\r\n<table>\r\n  <tr>\r\n    <th>Model/feature</th>\r\n    <th>Model is already available/feature is already compatible with encoder-decoder?</th>\r\n    <th>Having this model/making this feature compatible is a long-term goal?</th>\r\n  </tr>\r\n  <tr>\r\n    <td>Encoder/decoder infrastructure</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>BART</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Whisper</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>T5</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Other enc/dec models</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Quantization</td>\r\n    <td><strong><u>Untested</u></strong></td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Multimodality</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Attention backends other than Xformers (esp. flash-attn, flashinfer)</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Custom attention bias support</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>CUDAGraph</td>\r\n    <td>No<br>(Issue #7447)</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Pipeline parallelism</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Speculative decoding</td>\r\n    <td>No</td>\r\n    <td><strong>Low-priority but nice-to-have; difficult.</strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Automatic prefix caching</td>\r\n    <td>No</td>\r\n    <td><strong>Low-priority; difficult.</strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Sliding window</td>\r\n    <td>No</td>\r\n    <td>No</td>\r\n  </tr>\r\n  <tr>\r\n    <td>Chunked prefill</td>\r\n    <td>No</td>\r\n    <td>No</td>\r\n  </tr>\r\n  <tr>\r\n    <td>LoRA</td>\r\n    <td>No</td>\r\n    <td>No</td>\r\n  </tr>\r\n</table>\r\n\r\nThis RFC gives an overview of those features & models which **are not compatible with encoder/decoder currently, but which should be made compatible eventually** (i.e. **No** in the second column, **Yes** in the third column in the support matrix.)\r\n\r\nNote that there are features (automatic prefix caching/sliding window/chunked prefill/LoRA) which are not long-term compatibility goals.\r\n\r\n## Background <a href=\"#user-content-background\" id=\"background\">#</a>\r\n\r\nBefore continuing, it will be helpful to review [the details of the new vLLM encoder/decoder infrastructure](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/infra-enc-dec.md). \r\n\r\nIt will also be helpful to review [this how-to guide](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/how-to.md) for adding new encoder/decoder models & improving encoder/decoder feature compatibility.\r\n\r\n## Initial goal <a href=\"#user-content-initial-goal\" id=\"initial-goal\">#</a>\r\n\r\nMembers of the vLLM contributor community identify models/features in the support matrix above, for which they will work on writing a PR.\r\n\r\n## Detailed long-term goals <a href=\"#user-content-detailed-long-term-goals\" id=\"detailed-long-term-goals\">#</a>\r\n\r\n### Add new models to vLLM <a href=\"#user-content-add-new-models-to-vllm\" id=\"add-new-models-to-vllm\">#</a>\r\n\r\nPlease review the [how-to guide for adding new models to vLLM](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/how-to.md#guide-to-adding-new-encoderdecoder-models-to-vllm)\r\n\r\nSee `tests/models/test_bart.py` for an example of an encoder/decoder model unit test. See `tests/distributed/test_basic_distributed_correctness_enc_dec.py` for an example of an encoder/decoder model test with TP > 1.\r\n\r\n#### Add Whisper model <a href=\"#user-content-add-whisper-model\" id=\"add-whisper-model\">#</a>\r\n\r\nSteps to add support for [Whisper](https://paperswithcode.com/paper/robust-speech-recognition-via-large-scale-1f), a multimodal encoder/decoder speech recognition model:\r\n* [Extend existing vLLM multimodality support to encoder/decoder models](#support-encoderdecoder-multimodality)\r\n* Extend existing vLLM prompt processing pipeline to support audio\r\n* Port [HuggingFace Whisper model](https://github.com/huggingface/transformers/blob/main/src/transformers/models/whisper/modeling_whisper.py) to vLLM; an existing open PR for this workstream is #5964 \r\n* Modify each Whisper layer, where appropriate, to support TP > 1\r\n* Add a Whisper test under `tests/models/`\r\n\r\nProposal: consider whether or not it makes sense to implement encoder/decoder multimodality, audio support, and Whisper in the same PR; that way, the Whisper model may be used to facilitate an end-to-end test with of audio multimodality.\r\n\r\n#### Add T5 model <a href=\"#user-content-add-t5-model\" id=\"add-t5-model\">#</a>\r\n\r\nNote: T5 depends on [custom attention bias being supported](#support-custom-attention-bias) by at least one of the attention backends which [also supports encoder attention & cross-attention](#add-support-for-encoder-attention-and-cross-attention-to-additional-backends); at time of writing no vLLM attention backend fulfills this requirement. The vLLM XFormers attention backend is the only backend which supports encoder/decoder models but neither it nor any other vLLM attention backend supports custom attention bias. (Custom attention bias is required in order to support T5 [relative positional encoding.](#custom-attention-bias-and-relative-positional-encoding))\r\n\r\nSteps to add support for the [T5 model](https://paperswithcode.com/method/t5):\r\n* Port [HuggingFace T5 model](https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py) to vLLM\r\n  * This includes porting over the method which computes the custom attention bias matrix for T5 relative position encoding\r\n* Modify each T5 layer, where appropriate, to support TP > 1\r\n  * The custom attention bias computation must also support TP > 1\r\n* Add a T5 test to `tests/models/`\r\n\r\nNote: T5 was added to an older version of vLLM in #3117 , which could be a helpful starting-point\r\n\r\n#### Add other encoder/decoder models\r\n\r\n* Review open vLLM issues on GitHub and identify other encoder/decoder models which are requested by users\r\n\r\n### Quantization <a href=\"#user-content-quantization\" id=\"quantization\">#</a>\r\n\r\nThe goal of this workstream is to make sure that quantization + encoder/decoder models is fully-tested, and to fill in any gaps (should they exist) in vLLM's support for quantized encoder/decoder models.\r\n\r\nSteps to ensure that vLLM supports encoder/decoder models in combination with all existing vLLM quantization methods:\r\n\r\n* Identify the list of quantization methods which vLLM currently supports with decoder-only models.\r\n* Add unit tests for encoder/decoder models with all of these quantization methods.\r\n* Determine which quantization methods are currently incompatible with vLLM encoder/decoder infrastructure.\r\n* Scope out the effort involved in making these quantization methods compatible & submit a PR making the change.\r\n\r\nvLLM encoder/decoder infrastructure should be compatible with most of the existing vLLM quantization methods, because the specialized quantization kernels are only employed for GEMM operations involving the learned weight matrices ($W_q$, $W_k$, etc.), whereas the encoder/decoder work really only modifies how the `Attention(q, k, v, kv_cache)` layer behaves & does not impact the learned weight matrices at all.\r\n\r\nIt is less clear whether vLLM encoder/decoder infrastructure would be incompatible with FP8. It does appear that a specialized quantized KV cache kernel is employed by the `Attention(q, k, v, kv_cache)` layer when FP8 quantization is employed.\r\n\r\n### Support encoder/decoder multimodality <a href=\"#user-content-support-encoderdecoder-multimodality\" id=\"support-encoderdecoder-multimodality\">#</a>\r\n\r\nTechnically, vLLM already supports multimodality for models which have an \"encoder\" and a \"decoder\", i.e. Llava. However, Llava's decoder does not utilize cross-attention & the model is basically compatible with vLLM's pre-existing decoder-only infrastructure.\r\n\r\nBut critically, for **encoder/decoder models with cross-attention** such as Whisper vLLM does not currently support multimodality of any sort. The processing pipeline does not extract or utilize multimodal data from the input prompt, and the `EncoderDecoderModelRunner` has an assert which fails if the multimodal config is not `None`. Addressing this is what is meant by \"supporting encoder/decoder multimodality\".\r\n\r\nSteps to extend existing vLLM multimodality support to encoder/decoder models:\r\n* Review [existing vLLM multimodality support in the decoder-only pipeline](https://docs.vllm.ai/en/latest/dev/multimodal/multimodal_index.html)\r\n* Scope out a plan for adding encoder/decoder multimodality support.\r\n* Propose & implement one or more multimodal prompt formats for encoder/decoder models\r\n* Integrate multimodality support into encoder/decoder processing pipeline\r\n* Remove the assertion which fails when multimodality is enabled for an encoder/decoder model (see `assert_enc_dec_mr_supported_scenario()` in `vllm/worker/utils.py`)\r\n* Add one or more unit tests with multimodal data\r\n\r\nThere are a number of multimodal encoder/decoder models which will benefit from this feature. One possibility is to add multimodality support & a multimodal model such as [Whisper](#add-whisper-model) in the same PR, so that Whisper may be used to facilitate an end-to-end test with multimodality.\r\n\r\nAnother possibility is to implement multimodality support in its own PR.\r\n\r\n#### Considerations for designing multimodal encoder/decoder prompt formats <a href=\"#user-content-considerations-for-designing-multimodal-encoderdecoder-prompt-formats\" id=\"considerations-for-designing-multimodal-encoderdecoder-prompt-formats\">#</a>\r\n\r\nOne approach to designing the vLLM multimodal encoder/decoder prompt formats, is to consider what we want the user experience to be for high-priority multimodal encoder/decoder models such as\r\n* [Llama 3.1 multimodal](https://github.com/vllm-project/vllm/pull/7258#discussion_r1710915145)\r\n* [Whisper](#add-whisper-model)\r\n\r\n#### Initial proposal for multimodal encoder/decoder prompt formats\r\n\r\nIt may be helpful to review\r\n* [The non-multimodal encoder/decoder prompt formats which are currently supported by vLLM](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/infra-enc-dec.md#supported-encoderdecoder-prompt-formats): singleton prompts (raw text prompt, `TextPrompt`, `TokensPrompt`) as well as `ExplicitEncoderDecoder` prompts\r\n* The multimodal decoder-only prompt formats which are currently supported by vLLM; search for `multi_modal_data` [here](https://github.com/vllm-project/vllm/blob/main/vllm/inputs/data.py) and also review the [vLLM documentation on multimodality](https://docs.vllm.ai/en/latest/dev/multimodal/multimodal_index.html)\r\n\r\nGenerally speaking, in encoder/decoder models based on cross-attention, the non-text input modality is passed to the encoder as input. Conversely, any text prompt is typically passed to the decoder as a input prompt.\r\n\r\nThe following two encoder/decoder multimodal prompt formats are tentatively proposed:\r\n\r\n* Singleton `TextPrompt` with `multi_modal_data` field\r\n    * vLLM will extract the `multi_modal_data` and pass it to the encoder module\r\n    * vLLM will extract the prompt text, tokenize it and pass the token-list to the *decoder* (note that this is the opposite of vLLM behavior for non-multimodal prompts, where the prompt text would be passed to the encoder.)\r\n\r\n    For example passing the `TextPrompt` below to vLLM BART\r\n\r\n    ```\r\n    TextPrompt(\r\n        'prompt': \"The rain in spain falls mainly on the\",\r\n        'multi_modal_data': <multi modal data structure>\r\n    )\r\n    ```\r\n\r\n    results in\r\n\r\n    ```\r\n    Encoder input: <multi modal data structure>\r\n    Decoder prompt: \"The rain in spain falls mainly on the\"\r\n    ```\r\n\r\n* Singleton `TokensPrompt` with `multi_modal_data` field\r\n    * vLLM will extract the `multi_modal_data` and pass it to the encoder module\r\n    * vLLM will extract the token list and pass it unmodified to the *decoder* (note that this is the opposite of vLLM behavior for non-multimodal prompts, where the prompt tokens would be passed to the encoder.)\r\n\r\n    For example passing the `TokensPrompt` below to vLLM BART\r\n\r\n    ```\r\n    TokensPrompt(\r\n        'prompt_tokens': [2,0,171,5,2],\r\n        'multi_modal_data': <multi modal data structure>\r\n    )\r\n    ```\r\n\r\n    results in\r\n\r\n    ```\r\n    Encoder prompt: <multi modal data structure>\r\n    Decoder prompt: [2,0,171,5,2]\r\n    ```\r\n\r\nIt may also be worth considering whether or how to support\r\n* `ExplicitEncoderDecoderPrompt`s with multimodality\r\n* An input prompt format which encapsulates *only* multimodal encoder inputs, with no associated decoder text/tokens prompt (this would result in the decoder being passed a \"default\" or empty prompt.)\r\n\r\n### Add support for encoder attention and cross-attention to additional backends <a href=\"#user-content-add-support-for-encoder-attention-and-cross-attention-to-additional-backends\" id=\"add-support-for-encoder-attention-and-cross-attention-to-additional-backends\">#</a>\r\n\r\nAt time of writing, XFormers is the only vLLM attention backend which supports encoder attention & cross-attention. \r\n\r\nThe goal of this workstream would be to extend encoder attention & cross-attention support to additional backends, the highest-priority being flash-attention and flashinfer.\r\n\r\nReviewing [encoder attention and cross-attention support in the XFormers backend would be a good starting-point](https://github.com/vllm-project/vllm/blob/main/vllm/attention/backends/xformers.py) for extending support to other models.\r\n\r\nFor context on the requirements for a backend to support encoder and cross-attention, it may help to review the [encoder/decoder architecture](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/infra-enc-dec.md#encoderdecoder-architecture-diagram-prefill--and-decode-phase), the [way that attention masks are currently constructed in the XFormers backend](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/infra-enc-dec.md#default-encoderdecoder-attention-bias-or-mask), and the [recommended architecture for vLLM encoder/decoder models](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/how-to.md#25-optional-but-strongly-recommended-implement-the-following-encoderdecoder-model-architecture).\r\n\r\nA summary of the key changes required for an attention backend to support encoder attention and cross-attention:\r\n* The backend's `AttentionMetadata` subclass must support fields for encoder sequence lengths, encoder sequence token count, cross-attention blocktables, and cross-attention slot mapping. XFormers examples:\r\n  * [`AttentionMetadata` subclass' encoder field declarations](https://github.com/vllm-project/vllm/blob/67abdbb42fdbb59c274130368981c0d0ac3539e3/vllm/attention/backends/xformers.py#L127-L140)\r\n  * [Handle encoder & cross-attention fields in `prefill_metadata()` method](https://github.com/vllm-project/vllm/blob/67abdbb42fdbb59c274130368981c0d0ac3539e3/vllm/attention/backends/xformers.py#L216-L221)\r\n  * [Handle encoder & cross-attention fields in `decode_metadata()` method](https://github.com/vllm-project/vllm/blob/67abdbb42fdbb59c274130368981c0d0ac3539e3/vllm/attention/backends/xformers.py#L255-L260)\r\n* The `forward()` method of the backend implementation must accept an `attn_type` argument of type `AttentionType`, which allows choosing between encoder attention, decoder attention, or encoder/decoder cross-attention. [XFormers example](https://github.com/vllm-project/vllm/blob/67abdbb42fdbb59c274130368981c0d0ac3539e3/vllm/attention/backends/xformers.py#L447)\r\n* The backend implementation must recognize which option has been chosen for `attn_type`, and adjust accordingly in terms of (1) how it utilizes `attn_metadata` when invoking the attention kernels (review [XFormers `forward()`](https://github.com/vllm-project/vllm/blob/67abdbb42fdbb59c274130368981c0d0ac3539e3/vllm/attention/backends/xformers.py#L438) for context), and (2) the choice of causal or non-causal attention, as well the choice of attention mask shape ([XFormers example](https://github.com/vllm-project/vllm/blob/67abdbb42fdbb59c274130368981c0d0ac3539e3/vllm/attention/backends/xformers.py#L689-L714)).\r\n\r\n#### Initial goals\r\n* Identify the changes required to add encoder attention & cross-attention support to flash-attention and flashinfer\r\n* PR the required changes\r\n  * Remove/modify any asserts which fail if the vLLM attention backend is not XFormers\r\n    * Currently, [the `__init__()` method of `EncoderDecoderModelRunner`](https://github.com/vllm-project/vllm/blob/b4e9528f9569d6eb8c29624771a4058fe794cb5a/vllm/worker/enc_dec_model_runner.py#L95) invokes a method `EncoderDecoderModelRunner._maybe_force_supported_attention_backend()` defined [here](https://github.com/vllm-project/vllm/blob/b4e9528f9569d6eb8c29624771a4058fe794cb5a/vllm/worker/enc_dec_model_runner.py#L112-L144) which (1) attempts to force encoder/decoder models to use XFormers attention backend, and (2) raises an exception if the user has overridden the attention backend to be anything other than XFormers. \r\n\r\n#### Long-term goals\r\n* All vLLM attention backends support encoder attention and cross-attention\r\n\r\n### Support custom attention bias <a href=\"#user-content-support-custom-attention-bias\" id=\"support-custom-attention-bias\">#</a>\r\n\r\nNote: [T5](#add-t5-model) takes a dependency on custom attention bias. Custom attention bias is likely complex enough to merit its own PR.\r\n\r\nNote: custom bias support was added to `PagedAttention` in an older version of vLLM as part of #3117 ; given changes in vLLM since then, additional work would be required to integrate this implementation.\r\n\r\n#### Custom attention bias and relative positional encoding\r\n\r\nAttention bias refers to adding a matrix $A$ to the scaled dot-product (SDP) attention scores matrix before performing softmax, as shown below:\r\n\r\n$$\r\nattn(Q,K,V,A) = softmax(\\frac{Q K^T + A}{\\sqrt{d}})V\r\n$$\r\n\r\nHere, *custom* attention bias is understood to mean that the vLLM attention backend allows $A$ to be an arbitrary matrix, provided the tensor dimensions are commensurate with the shape of the SDP attention scores matrix. This is in contrast to the existing vLLM attention backend implementations, which can only accommodate simple block-diagonal causal or non-causal masks which are uniformly either $0$ or $-\\infty$. \r\n\r\nThere are broadly two possible approaches to custom attention bias, which do not necessarily have to be mutually-exclusive:\r\n* $A$ is a fully-materialized attention bias matrix passed to the attention backend\r\n* $A$ is computed on-the-fly by the attention kernel, using an element-wise formula for the attention bias which is fused with the $Q K^T$ and $softmax$ computations\r\n\r\nT5 employs custom attention bias in order to implement [relative positional encoding](https://jaketae.github.io/study/relative-positional-encoding/#bridging-shaw-and-huang), wherein pairwise positional relationships between tokens are represented by the bias matrix. The HuggingFace Transformers T5 implementation provides an example of [how the relative positional encoding matrix is computed](https://github.com/huggingface/transformers/blob/c1aa0edb48217f416f4bbe6e3a9db1500284513b/src/transformers/models/t5/modeling_t5.py#L428).\r\n\r\n#### Existing attention bias support\r\n\r\n*Currently, no vLLM attention backend fully supports passing in a custom attention bias*. This is primarily due to underlying kernel limitations. For example, the [xFormers `memory_efficient_attention_forward` kernel](https://facebookresearch.github.io/xformers/components/ops.html) is the only NVIDIA-GPU-oriented kernel which permits passing in an arbitrary PyTorch tensor as a materialized attention bias (via the `attn_bias` argument) (at time of writing I have not investigated if custom attention bias is supported by any of the kernels for AMD GPU, CPU, etc.) Regardless, vLLM only employs xFormers `memory_efficient_attention_forward` for prefill; to my knowledge, none of the decode-phase kernels employed by vLLM can accept an arbitrary tensor as a custom attention bias, making custom attention bias impossible to apply end-to-end for both prefill and decode under the current vLLM implementation.\r\n\r\nIn addition to lack of kernel-level support for custom attention bias, most vLLM backends also prevent passing a custom attention bias matrix to the underlying kernel. The exception is the XFormers backend, which accepts an attention bias via `XFormersMetadata.attn_bias` attribute (however the XFormers backend only utilizes `attn_bias` in the prefill phase.)\r\n\r\n#### Proposed methods for supporting custom attention bias\r\n\r\nHere the following two approaches for supporting custom attention bias in vLLM are proposed:\r\n* **Fully-materialized bias matrix:** Modify vLLM attention backends to accept an arbitrary PyTorch tensor, passed into the backend via the `AttentionMetadata.attn_bias` field.\r\n* **On-the-fly/fused bias matrix computation:** Enable an efficient workflow whereby vLLM developers can tweak an attention kernel to compute the custom attention bias on the fly\r\n  * For example: rather than computing the T5 relative position encoder bias matrix once, instead the attention kernel can fuse the element-wise bias matrix formula with the $Q K^T$ and $softmax()$. The attention bias matrix is never fully materialized.\r\n  * [FlexAttention](https://pytorch.org/blog/flexattention/#relative-position-encodings) enables fused custom attention bias computations in a FlashAttention-style kernel, using torch.compile.\r\n\r\n![image](https://pytorch.org/assets/images/flexattention/fg4.png)\r\n\r\nIt may make sense to support one or both of these methods.\r\n\r\nNote that custom attention bias support must be added on a backend-by-backend basis, because of the kernel modifications & backend logic changes required.\r\n\r\n#### Initial goals for introducing custom attention bias support\r\n\r\n1. Focus on a particular vLLM attention backend\r\n  * Suggestion: focus on an attention backend which also supports encoder/decoder models, in order to facilitate [running T5](#add-t5-model). At time of writing, XFormers is the only backend which supports encoder/decoder models, however there will likely be work on [supporting encoder/decoder in additional attention backends.](#add-support-for-encoder-attention-and-cross-attention-to-additional-backends)\r\n2. Scope out the effort involved in introducing custom attention bias support to this backend\r\n3. Some steps which will likely be involved in introducing custom attention bias support:\r\n  * Augment attention backend's kernels to accept custom attention bias; for example, the PagedAttention kernel (for XFormers backend), the Flash-attention kernel (for the flash-attn backend), or the Flashinfer kernels (for the Flashinfer backend)\r\n  * (Except for XFormers) add an `attn_bias` attribute to attention backend's `AttentionMetadata` subclass\r\n  * Ensure that the attention backend passes the `attn_bias` attribute to both the prefill and decode kernels\r\n4. Add at least two custom attention bias unit tests (for prefill & decode respectively)\r\n\r\n#### Final goals for introducing custom attention bias support\r\n\r\n* All vLLM attention backends should support custom attention bias, with unit tests\r\n\r\n#### Some links which may be helpful for understanding how causal & non-causal attention masks are currently configured in vLLM:\r\n\r\n* [Invocation of flash-attention for prefill in vLLM backend, using `causal` flag](https://github.com/vllm-project/vllm/blob/db35186391a2abfc6c91d703527dac20d2488107/vllm/attention/backends/flash_attn.py#L522)\r\n\r\n* [Invocation of xFormers attention kernel for prefill in vLLM backend, using `BlockDiagonalMask` and `BlockDiagonalCausalMask`](https://github.com/vllm-project/vllm/blob/db35186391a2abfc6c91d703527dac20d2488107/vllm/attention/backends/xformers.py#L689-L738)\r\n\r\n* [Invocation of FlashInfer attention kernel for prefill in backend, using `causal` flag](https://github.com/vllm-project/vllm/blob/db35186391a2abfc6c91d703527dac20d2488107/vllm/attention/backends/flashinfer.py#L539)\r\n\r\n* [Invocation of PagedAttention kernel for decode in vLLM backend](https://github.com/vllm-project/vllm/blob/db35186391a2abfc6c91d703527dac20d2488107/vllm/attention/backends/xformers.py#L628)\r\n\r\n* [Invocation of FlashInfer kernel for decode in vLLM backend](https://github.com/vllm-project/vllm/blob/db35186391a2abfc6c91d703527dac20d2488107/vllm/attention/backends/flashinfer.py#L543)\r\n\r\n### Support CUDAGraph with encoder/decoder models <a href=\"#user-content-support-cudagraph-with-encoderdecoder-models\" id=\"support-cudagraph-with-encoderdecoder-models\">#</a>\r\n\r\nNote: this topic is being tracked by Issue #7447\r\n\r\nSteps to support CUDAGraph with encoder/decoder models:\r\n* Scope out the effort require to support CUDAGraph with encoder/decoder models\r\n* Write a PR for CUDAGraph + encoder/decoder\r\n  * Remove the assertion which fails when CUDAGraph is enabled for an encoder/decoder model (see `assert_enc_dec_mr_supported_scenario()` in `vllm/worker/utils.py`)\r\n\r\n### Support pipeline-parallelism with encoder/decoder models <a href=\"#user-content-support-pipeline-parallelism-with-encoderdecoder-models\" id=\"support-pipeline-parallelism-with-encoderdecoder-models\">#</a>\r\n\r\nSteps to support pipeline-parallelism with encoder/decoder models:\r\n* Scope out the effort required to support pipeline-parallelism with encoder/decoder models\r\n* Write a PR for pipeline-parallelism + encoder/decoder\r\n  * Remove the assertion which fails when pipeline-parallelism is enabled for an encoder/decoder model (see `assert_enc_dec_mr_supported_scenario()` in `vllm/worker/utils.py`)\r\n\r\n### Support multi-step scheduling with encoder/decoder models <a href=\"#user-content-support-multi-step-scheduling-with-encoder-decoder-models\" id=\"support-multi-step-scheduling-with-encoder-decoder-models\">#</a>\r\n\r\nNote: depends on #7000 landing in order to add multi-step scheduling support; it may be helpful to review the multi-step scheduling RFC ( #6854 )\r\n\r\nSteps to support multi-step scheduling with encoder/decoder models:\r\n* Scope out the effort required to support multi-step scheduling\r\n  * `EncoderDecoderModelRunner` multi-step support\r\n* Write a PR for multi-step scheduling + encoder/decoder\r\n* Write at least one test of an encoder/decoder model with multi-step scheduling\r\n\r\n### Low-priority high-effort tasks <a href=\"#user-content-low-priority-high-effort-tasks\" id=\"low-priority-high-effort-tasks\">#</a>\r\n\r\n* Speculative decoding\r\n* Automatic prefix caching\r\n\r\nHere it is proposed that these features are low-priority. Adding support for speculative decoder and automatic prefix caching would require a significant of effort to scope out and design the implementations.\r\n\r\nNote that adding support for either of these features would require removing the assertions which fail when speculative decoding or automatic prefix caching are enabled for an encoder/decoder model (see `assert_enc_dec_mr_supported_scenario()` in `vllm/worker/utils.py`)\r\n\r\n### Feedback Period.\r\n\r\nClosed.\r\n\r\n### CC List.\r\n\r\n@WoosukKwon \r\n@robertgshaw2-neuralmagic \r\n@mgoin \r\n@tms\r\n@njhill \r\n@sroy745 \r\n@ywang96 \r\n@DarkLight1337 \r\n@js8544 \r\n\r\n### Any Other Things.\r\n\r\n_No response_",
    "labels": [
      "RFC",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-08-09T15:03:54+00:00",
    "closed_at": null,
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7366/reactions",
      "total_count": 14,
      "+1": 14,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7366"
  },
  {
    "number": 11655,
    "title": "[Feature]: Support Inflight quantization: load as 8bit quantization.",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nVLLM supports [4bit inflight quantification](https://docs.vllm.ai/en/stable/quantization/bnb.html#inflight-quantization-load-as-4bit-quantization), but does not support 8bit, 8bit speed is faster than 4bit, request support for support.\r\n\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-12-31T08:42:16+00:00",
    "closed_at": null,
    "comments": 20,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11655/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11655"
  },
  {
    "number": 12179,
    "title": "[Bug]: Multi-Node Online Inference on TPUs Failing",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nroot@t1v-n-4d36f9a1-w-0:/workspace/vllm# python collect_env.py\nINFO 01-17 23:21:42 __init__.py:179] Automatically detected platform tpu.\nCollecting environment information...\nPyTorch version: 2.6.0.dev20241126+cpu\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\nClang version: Could not collect\nCMake version: version 3.31.4\nLibc version: glibc-2.31\n\nPython version: 3.10.15 (main, Oct 17 2024, 02:58:23) [GCC 10.2.1 20210110] (64-bit runtime)\nPython platform: Linux-5.19.0-1022-gcp-x86_64-with-glibc2.31\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nByte Order:                      Little Endian\nAddress sizes:                   48 bits physical, 48 bits virtual\nCPU(s):                          240\nOn-line CPU(s) list:             0-239\nThread(s) per core:              2\nCore(s) per socket:              60\nSocket(s):                       2\nNUMA node(s):                    2\nVendor ID:                       AuthenticAMD\nCPU family:                      23\nModel:                           49\nModel name:                      AMD EPYC 7B12\nStepping:                        0\nCPU MHz:                         2249.998\nBogoMIPS:                        4499.99\nHypervisor vendor:               KVM\nVirtualization type:             full\nL1d cache:                       3.8 MiB\nL1i cache:                       3.8 MiB\nL2 cache:                        60 MiB\nL3 cache:                        480 MiB\nNUMA node0 CPU(s):               0-59,120-179\nNUMA node1 CPU(s):               60-119,180-239\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Not affected\nVulnerability Retbleed:          Mitigation; untrained return thunk; SMT enabled with STIBP protection\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.6.0.dev20241126+cpu\n[pip3] torch-xla==2.6.0+git39e67b5\n[pip3] torchvision==0.20.0.dev20241126+cpu\n[pip3] transformers==4.48.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.6.6.post2.dev250+gd1adb9b40\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nVLLM_TARGET_DEVICE=tpu\nLD_LIBRARY_PATH=/usr/local/lib/python3.10/site-packages/cv2/../../lib64:\n```\n\n</details>\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nI'm trying to run multi-node inference on TPUs, so that I'd be able to fit large models like Llama-3.1-70B-Instruct which will require more than one pod of v4-8.\n\nHere are some of my settings:\n- TPU Type: v4-32\n- TPU Software Version: tpu-ubuntu2204-base\n- Docker Version: vllm/vllm-tpu:nightly\n\nI followed the examples shown in distributed inference and serving in the [docs](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). I modified it to fit the TPU case. The gist was to add `--privileged` flag and also add `TPU` as a resource to the RAY START COMMANDS. Check out my repo for exact information: [link](https://github.com/BabyChouSr/vllm/blob/befc0727ac1e4704e1a2c7f41c180205e046b873/examples/online_serving/run_cluster.sh)\n\nI first tested out running with just two of the four hosts in a v4-32 setting. Here are the commands I used.\n\n```\n# ssh into head and worker and setup containers\ngcloud compute tpus tpu-vm ssh --zone \"us-central2-b\" \"chris-vllm-labelling\" --project \"hai-gcp-models\" --worker 0\ngit clone https://github.com/BabyChouSr/vllm.git\ncd vllm \ngit checkout chris/vllm-tpu-multi\n\nsudo bash examples/online_serving/run_cluster.sh \\\n                  vllm/vllm-tpu:nightly \\\n                  10.130.0.9 \\\n                  --head \\\n                  /home/chrischou/.cache/huggingface\n\n# Repeat above for worker node\n```\n\nThen I ran `ray status` inside the container of the head node. I get the following as expected. I see 8 TPUs registered since I have a head and a worker.\n```\nroot@t1v-n-4d36f9a1-w-0:/workspace/vllm# ray status\n======== Autoscaler status: 2025-01-17 23:33:55.419278 ========\nNode status\n---------------------------------------------------------------\nActive:\n 1 node_6ef276d8e9c645cf2b3ef633fa3e40daa64c635cc5f4accb7fa05659\n 1 node_3e3224930b5214adc9afebf72acea691d97f608dbe26e222875d9af2\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/240.0 CPU\n 0.0/8.0 TPU\n 0.0/1.0 TPU-v4-32-head\n 0.0/2.0 chris-vllm-labelling\n 0B/760.02GiB memory\n 0B/30.40GiB object_store_memory\n\nDemands:\n (no resource demands)\n```\n\nThen I try to serve a small model just to see if tensor parallelism works. I run `vllm serve Qwen/Qwen2.5-7B-Instruct --tensor-parallel-size 4`. I get the following error:\n```\nroot@t1v-n-4d36f9a1-w-0:/workspace/vllm# vllm serve Qwen/Qwen2.5-7B-Instruct --tensor-parallel-size 4\nINFO 01-17 23:34:35 __init__.py:179] Automatically detected platform tpu.\nINFO 01-17 23:34:36 api_server.py:768] vLLM API server version 0.6.6.post2.dev250+gd1adb9b40\nINFO 01-17 23:34:36 api_server.py:769] args: Namespace(subparser='serve', model_tag='Qwen/Qwen2.5-7B-Instruct', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2.5-7B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7fb2e47c4af0>)\nINFO 01-17 23:34:36 api_server.py:195] Started engine process with PID 2002\nINFO 01-17 23:34:40 __init__.py:179] Automatically detected platform tpu.\nINFO 01-17 23:34:44 config.py:520] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed', 'score'}. Defaulting to 'generate'.\nINFO 01-17 23:34:48 config.py:520] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify', 'score'}. Defaulting to 'generate'.\nINFO 01-17 23:34:48 llm_engine.py:232] Initializing an LLM engine (v0.6.6.post2.dev250+gd1adb9b40) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":2,\"backend\":\"openxla\",\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \n2025-01-17 23:34:48,948\tINFO worker.py:1636 -- Connecting to existing Ray cluster at address: 10.130.0.9:6379...\n2025-01-17 23:34:48,961\tINFO worker.py:1812 -- Connected to Ray cluster. View the dashboard at 127.0.0.1:8265 \nINFO 01-17 23:34:50 ray_distributed_executor.py:152] use_ray_spmd_worker: False\n(pid=2257) INFO 01-17 23:34:53 __init__.py:179] Automatically detected platform tpu.\n(RayWorkerWrapper pid=2253) WARNING 01-17 23:34:54 utils.py:546] Overwriting environment variable TPU_VISIBLE_CHIPS from '3' to '0,1,2,3'\nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n(RayWorkerWrapper pid=2251) WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\nINFO 01-17 23:34:55 tpu.py:34] Cannot use None backend on TPU.\nINFO 01-17 23:34:55 tpu.py:35] Using Pallas backend.\nWARNING 01-17 23:34:55 tpu_model_runner.py:127] The max_model_len (32768) is too large. This may degrade the performance due to the insufficient smem size. Consider setting --max-model-len to a smaller value, like 8192.\n(RayWorkerWrapper pid=2253) INFO 01-17 23:34:55 tpu.py:34] Cannot use None backend on TPU.\n(RayWorkerWrapper pid=2253) INFO 01-17 23:34:55 tpu.py:35] Using Pallas backend.\n(RayWorkerWrapper pid=2253) WARNING 01-17 23:34:55 tpu_model_runner.py:127] The max_model_len (32768) is too large. This may degrade the performance due to the insufficient smem size. Consider setting --max-model-len to a smaller value, like 8192.\nERROR 01-17 23:34:56 worker_base.py:554] Error executing method init_device. This might cause deadlock in distributed execution.\nERROR 01-17 23:34:56 worker_base.py:554] Traceback (most recent call last):\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/worker/worker_base.py\", line 546, in execute_method\nERROR 01-17 23:34:56 worker_base.py:554]     return executor(*args, **kwargs)\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\nERROR 01-17 23:34:56 worker_base.py:554]     ensure_model_parallel_initialized(\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 1093, in ensure_model_parallel_initialized\nERROR 01-17 23:34:56 worker_base.py:554]     initialize_model_parallel(tensor_model_parallel_size,\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 1037, in initialize_model_parallel\nERROR 01-17 23:34:56 worker_base.py:554]     _TP = init_model_parallel_group(group_ranks,\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 866, in init_model_parallel_group\nERROR 01-17 23:34:56 worker_base.py:554]     return GroupCoordinator(\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\nERROR 01-17 23:34:56 worker_base.py:554]     self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 53, in __init__\nERROR 01-17 23:34:56 worker_base.py:554]     pjrt.initialize_multiprocess(local_rank, local_world_size)\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 121, in initialize_multiprocess\nERROR 01-17 23:34:56 worker_base.py:554]     devices = xm.get_xla_supported_devices()\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/core/xla_model.py\", line 93, in get_xla_supported_devices\nERROR 01-17 23:34:56 worker_base.py:554]     devices = torch_xla._XLAC._xla_get_devices()\nERROR 01-17 23:34:56 worker_base.py:554] RuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: Invalid --2a886c8_slice_builder_worker_addresses specified. Expected 16 worker addresses, got 8.\nERROR 01-17 23:34:56 engine.py:380] Bad StatusOr access: UNKNOWN: TPU initialization failed: Invalid --2a886c8_slice_builder_worker_addresses specified. Expected 16 worker addresses, got 8.\nERROR 01-17 23:34:56 engine.py:380] Traceback (most recent call last):\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 371, in run_mp_engine\nERROR 01-17 23:34:56 engine.py:380]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 120, in from_engine_args\nERROR 01-17 23:34:56 engine.py:380]     return cls(ipc_path=ipc_path,\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 72, in __init__\nERROR 01-17 23:34:56 engine.py:380]     self.engine = LLMEngine(*args, **kwargs)\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/engine/llm_engine.py\", line 271, in __init__\nERROR 01-17 23:34:56 engine.py:380]     self.model_executor = executor_class(vllm_config=vllm_config, )\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/executor/executor_base.py\", line 222, in __init__\nERROR 01-17 23:34:56 engine.py:380]     super().__init__(*args, **kwargs)\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/executor/executor_base.py\", line 42, in __init__\nERROR 01-17 23:34:56 engine.py:380]     self._init_executor()\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/executor/ray_distributed_executor.py\", line 87, in _init_executor\nERROR 01-17 23:34:56 engine.py:380]     self._init_workers_ray(placement_group)\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/executor/ray_distributed_executor.py\", line 342, in _init_workers_ray\nERROR 01-17 23:34:56 engine.py:380]     self._run_workers(\"init_device\")\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/executor/ray_distributed_executor.py\", line 458, in _run_workers\nERROR 01-17 23:34:56 engine.py:380]     self.driver_worker.execute_method(method, *args, **kwargs)\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/worker/worker_base.py\", line 555, in execute_method\nERROR 01-17 23:34:56 engine.py:380]     raise e\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/worker/worker_base.py\", line 546, in execute_method\nERROR 01-17 23:34:56 engine.py:380]     return executor(*args, **kwargs)\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\nERROR 01-17 23:34:56 engine.py:380]     ensure_model_parallel_initialized(\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 1093, in ensure_model_parallel_initialized\nERROR 01-17 23:34:56 engine.py:380]     initialize_model_parallel(tensor_model_parallel_size,\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 1037, in initialize_model_parallel\nERROR 01-17 23:34:56 engine.py:380]     _TP = init_model_parallel_group(group_ranks,\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 866, in init_model_parallel_group\nERROR 01-17 23:34:56 engine.py:380]     return GroupCoordinator(\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\nERROR 01-17 23:34:56 engine.py:380]     self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 53, in __init__\nERROR 01-17 23:34:56 engine.py:380]     pjrt.initialize_multiprocess(local_rank, local_world_size)\nERROR 01-17 23:34:56 engine.py:380]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 121, in initialize_multiprocess\nERROR 01-17 23:34:56 engine.py:380]     devices = xm.get_xla_supported_devices()\nERROR 01-17 23:34:56 engine.py:380]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/core/xla_model.py\", line 93, in get_xla_supported_devices\nERROR 01-17 23:34:56 engine.py:380]     devices = torch_xla._XLAC._xla_get_devices()\nERROR 01-17 23:34:56 engine.py:380] RuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: Invalid --2a886c8_slice_builder_worker_addresses specified. Expected 16 worker addresses, got 8.\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/local/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 382, in run_mp_engine\n    raise e\n  File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 371, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n  File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 120, in from_engine_args\n    return cls(ipc_path=ipc_path,\n  File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 72, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n  File \"/workspace/vllm/vllm/engine/llm_engine.py\", line 271, in __init__\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n  File \"/workspace/vllm/vllm/executor/executor_base.py\", line 222, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/workspace/vllm/vllm/executor/executor_base.py\", line 42, in __init__\n    self._init_executor()\n  File \"/workspace/vllm/vllm/executor/ray_distributed_executor.py\", line 87, in _init_executor\n    self._init_workers_ray(placement_group)\n  File \"/workspace/vllm/vllm/executor/ray_distributed_executor.py\", line 342, in _init_workers_ray\n    self._run_workers(\"init_device\")\n  File \"/workspace/vllm/vllm/executor/ray_distributed_executor.py\", line 458, in _run_workers\n    self.driver_worker.execute_method(method, *args, **kwargs)\n  File \"/workspace/vllm/vllm/worker/worker_base.py\", line 555, in execute_method\n    raise e\n  File \"/workspace/vllm/vllm/worker/worker_base.py\", line 546, in execute_method\n    return executor(*args, **kwargs)\n  File \"/workspace/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\n    ensure_model_parallel_initialized(\n  File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 1093, in ensure_model_parallel_initialized\n    initialize_model_parallel(tensor_model_parallel_size,\n  File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 1037, in initialize_model_parallel\n    _TP = init_model_parallel_group(group_ranks,\n  File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 866, in init_model_parallel_group\n    return GroupCoordinator(\n  File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\n    self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\n  File \"/workspace/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 53, in __init__\n    pjrt.initialize_multiprocess(local_rank, local_world_size)\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 121, in initialize_multiprocess\n    devices = xm.get_xla_supported_devices()\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/core/xla_model.py\", line 93, in get_xla_supported_devices\n    devices = torch_xla._XLAC._xla_get_devices()\nRuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: Invalid --2a886c8_slice_builder_worker_addresses specified. Expected 16 worker addresses, got 8.\n(pid=2251) INFO 01-17 23:34:53 __init__.py:179] Automatically detected platform tpu. [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\n(RayWorkerWrapper pid=2251) WARNING 01-17 23:34:54 utils.py:546] Overwriting environment variable TPU_VISIBLE_CHIPS from '2' to '0,1,2,3' [repeated 2x across cluster]\n(RayWorkerWrapper pid=2251) INFO 01-17 23:34:55 tpu.py:34] Cannot use None backend on TPU. [repeated 2x across cluster]\n(RayWorkerWrapper pid=2251) INFO 01-17 23:34:55 tpu.py:35] Using Pallas backend. [repeated 2x across cluster]\n(RayWorkerWrapper pid=2251) WARNING 01-17 23:34:55 tpu_model_runner.py:127] The max_model_len (32768) is too large. This may degrade the performance due to the insufficient smem size. Consider setting --max-model-len to a smaller value, like 8192. [repeated 2x across cluster]\n(RayWorkerWrapper pid=2248) WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU. [repeated 2x across cluster]\nTask exception was never retrieved\nfuture: <Task finished name='Task-2' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /workspace/vllm/vllm/engine/multiprocessing/client.py:180> exception=ZMQError('Operation not supported')>\nTraceback (most recent call last):\n  File \"/workspace/vllm/vllm/engine/multiprocessing/client.py\", line 186, in run_output_handler_loop\n    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n  File \"/usr/local/lib/python3.10/site-packages/zmq/_future.py\", line 400, in poll\n    raise _zmq.ZMQError(_zmq.ENOTSUP)\nzmq.error.ZMQError: Operation not supported\nTraceback (most recent call last):\n  File \"/usr/local/bin/vllm\", line 33, in <module>\n    sys.exit(load_entry_point('vllm', 'console_scripts', 'vllm')())\n  File \"/workspace/vllm/vllm/scripts.py\", line 201, in main\n    args.dispatch_function(args)\n  File \"/workspace/vllm/vllm/scripts.py\", line 42, in serve\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.10/site-packages/uvloop/__init__.py\", line 82, in run\n    return loop.run_until_complete(wrapper())\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.10/site-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 796, in run_server\n    async with build_async_engine_client(args) as engine_client:\n  File \"/usr/local/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 125, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/usr/local/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 219, in build_async_engine_client_from_engine_args\n    raise RuntimeError(\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\n```\n\nIt seems like there is some issues with the spawning of the multiple processes needed to run distributed inference. I also tested the case without tensor parallelism since Qwen7B fits on a single node. It ends up hanging:\n```\nroot@t1v-n-4d36f9a1-w-0:/workspace/vllm# vllm serve Qwen/Qwen2.5-7B-Instruct\nINFO 01-17 23:36:53 __init__.py:179] Automatically detected platform tpu.\nINFO 01-17 23:36:55 api_server.py:768] vLLM API server version 0.6.6.post2.dev250+gd1adb9b40\nINFO 01-17 23:36:55 api_server.py:769] args: Namespace(subparser='serve', model_tag='Qwen/Qwen2.5-7B-Instruct', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2.5-7B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7f6874240af0>)\nINFO 01-17 23:36:55 api_server.py:195] Started engine process with PID 18709\nINFO 01-17 23:36:58 __init__.py:179] Automatically detected platform tpu.\nINFO 01-17 23:37:02 config.py:520] This model supports multiple tasks: {'score', 'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\nINFO 01-17 23:37:06 config.py:520] This model supports multiple tasks: {'generate', 'reward', 'classify', 'embed', 'score'}. Defaulting to 'generate'.\nINFO 01-17 23:37:06 llm_engine.py:232] Initializing an LLM engine (v0.6.6.post2.dev250+gd1adb9b40) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":2,\"backend\":\"openxla\",\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\nINFO 01-17 23:37:08 tpu.py:34] Cannot use None backend on TPU.\nINFO 01-17 23:37:08 tpu.py:35] Using Pallas backend.\nWARNING 01-17 23:37:08 tpu_model_runner.py:127] The max_model_len (32768) is too large. This may degrade the performance due to the insufficient smem size. Consider setting --max-model-len to a smaller value, like 8192.\n```\n^ It stops right here and does not output anything anymore. Running ray status on a separate terminal window, it says that there are no demanded resources, so nothing is being used.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tpu",
      "ray",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-01-17T23:38:35+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12179/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12179"
  },
  {
    "number": 8735,
    "title": "[Bug]: stuck at  \"generating GPU P2P access cache in /home/luban/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\"",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\npython collect_env.py \r\nCollecting environment information...\r\n2024-09-23 17:57:46.577274: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-09-23 17:57:46.594737: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-09-23 17:57:46.616458: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-09-23 17:57:46.622847: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-09-23 17:57:46.638311: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-09-23 17:57:47.734082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.11.10 | packaged by conda-forge | (main, Sep 10 2024, 11:01:28) [GCC 13.3.0] (64-bit runtime)\r\nPython platform: Linux-4.18.0-193.6.3.el8_2.v1.4.x86_64-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA RTX A6000\r\nGPU 1: NVIDIA RTX A6000\r\n\r\nNvidia driver version: 535.129.03\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nThread(s) per core:              2\r\nCore(s) per socket:              32\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           106\r\nModel name:                      Intel(R) Xeon(R) Platinum 8352Y CPU @ 2.20GHz\r\nStepping:                        6\r\nCPU MHz:                         2200.000\r\nCPU max MHz:                     3400.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        4400.00\r\nVirtualization:                  VT-x\r\nL1d cache:                       3 MiB\r\nL1i cache:                       2 MiB\r\nL2 cache:                        80 MiB\r\nL3 cache:                        96 MiB\r\nNUMA node0 CPU(s):               0-31,64-95\r\nNUMA node1 CPU(s):               32-63,96-127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid md_clear pconfig flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] galore-torch==1.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.68\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] optree==0.12.1\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchao==0.5.0\r\n[pip3] torchaudio==2.4.0\r\n[pip3] torchtext==0.18.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.0.dev0\r\n[pip3] triton==3.0.0\r\n[pip3] zmq==0.0.0\r\n[conda] galore-torch              1.0                      pypi_0    pypi\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.6.68                  pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\r\n[conda] optree                    0.12.1                   pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchao                   0.5.0                    pypi_0    pypi\r\n[conda] torchaudio                2.4.0                    pypi_0    pypi\r\n[conda] torchtext                 0.18.0                   pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.45.0.dev0              pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\n[conda] zmq                       0.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.5@09c7792610ada9f88bbf87d32b472dd44bf23cc2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      SYS     SYS     SYS     0-31,64-95      0               N/A\r\nGPU1    SYS      X      SYS     SYS     32-63,96-127    1               N/A\r\nNIC0    SYS     SYS      X      PIX\r\nNIC1    SYS     SYS     PIX      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\npython vllm_test.py \r\n2024-09-23 17:49:44.893334: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-09-23 17:49:44.910873: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-09-23 17:49:44.932328: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-09-23 17:49:44.938677: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-09-23 17:49:44.954346: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-09-23 17:49:46.068503: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nWARNING 09-23 17:49:48 utils.py:721] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).\r\nINFO 09-23 17:49:48 config.py:813] Defaulting to use mp for distributed inference\r\nINFO 09-23 17:49:48 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='/ofs/llm-for-dptd/modelscope/LLM-Research/gemma-2-27b-it', speculative_config=None, tokenizer='/ofs/llm-for-dptd/modelscope/LLM-Research/gemma-2-27b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/ofs/llm-for-dptd/modelscope/LLM-Research/gemma-2-27b-it, use_v2_block_manager=False, enable_prefix_caching=False)\r\nWARNING 09-23 17:49:50 multiproc_gpu_executor.py:59] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\nINFO 09-23 17:49:50 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n(VllmWorkerProcess pid=2348) INFO 09-23 17:49:50 selector.py:142] Using Flashinfer backend.\r\nINFO 09-23 17:49:50 selector.py:142] Using Flashinfer backend.\r\n(VllmWorkerProcess pid=2348) INFO 09-23 17:49:50 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n[W923 17:49:50.014460463 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:37597 (errno: 97 - Address family not supported by protocol).\r\n[W923 17:49:51.076798643 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:37597 (errno: 97 - Address family not supported by protocol).\r\nINFO 09-23 17:49:51 utils.py:975] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=2348) INFO 09-23 17:49:51 utils.py:975] Found nccl from library libnccl.so.2\r\nINFO 09-23 17:49:51 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(VllmWorkerProcess pid=2348) INFO 09-23 17:49:51 pynccl.py:63] vLLM is using nccl==2.20.5\r\nINFO 09-23 17:49:51 custom_all_reduce_utils.py:203] generating GPU P2P access cache in /home/luban/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n\r\n^CINFO 09-23 17:54:16 multiproc_worker_utils.py:136] Terminating local vLLM worker processes\r\n\r\n\r\n\r\n^C[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\", line 220, in gpu_p2p_access_check\r\n[rank0]:     returned.check_returncode()\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/subprocess.py\", line 502, in check_returncode\r\n[rank0]:     raise CalledProcessError(self.returncode, self.args, self.stdout,\r\n[rank0]: subprocess.CalledProcessError: Command '['/home/data/miniconda/envs/llm/bin/python', '/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py']' died with <Signals.SIGINT: 2>.\r\n\r\n[rank0]: The above exception was the direct cause of the following exception:\r\n\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/ofs/llm-for-dptd/modelscope/vllm_test.py\", line 10, in <module>\r\n[rank0]:     model = LLM(model=\"/ofs/llm-for-dptd/modelscope/LLM-Research/gemma-2-27b-it\",\r\n[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/entrypoints/llm.py\", line 175, in __init__\r\n[rank0]:     self.llm_engine = LLMEngine.from_engine_args(\r\n[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 473, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:              ^^^^\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 270, in __init__\r\n[rank0]:     self.model_executor = executor_class(\r\n[rank0]:                           ^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/executor/distributed_gpu_executor.py\", line 25, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/executor/executor_base.py\", line 46, in __init__\r\n[rank0]:     self._init_executor()\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 137, in _init_executor\r\n[rank0]:     self._run_workers(\"init_device\")\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 192, in _run_workers\r\n[rank0]:     driver_worker_output = driver_worker_method(*args, **kwargs)\r\n[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/worker/worker.py\", line 175, in init_device\r\n[rank0]:     init_worker_distributed_environment(self.parallel_config, self.rank,\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/worker/worker.py\", line 450, in init_worker_distributed_environment\r\n[rank0]:     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py\", line 965, in ensure_model_parallel_initialized\r\n[rank0]:     initialize_model_parallel(tensor_model_parallel_size,\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py\", line 931, in initialize_model_parallel\r\n[rank0]:     _TP = init_model_parallel_group(group_ranks,\r\n[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py\", line 773, in init_model_parallel_group\r\n[rank0]:     return GroupCoordinator(\r\n[rank0]:            ^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py\", line 164, in __init__\r\n[rank0]:     self.ca_comm = CustomAllreduce(\r\n[rank0]:                    ^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\", line 130, in __init__\r\n[rank0]:     if not _can_p2p(rank, world_size):\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\", line 31, in _can_p2p\r\n[rank0]:     if not gpu_p2p_access_check(rank, i):\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\", line 223, in gpu_p2p_access_check\r\n[rank0]:     raise RuntimeError(\r\n[rank0]: RuntimeError: Error happened when batch testing peer-to-peer access from (0, 0, 1, 1) to (0, 1, 0, 1):\r\n[rank0]: 2024-09-23 17:49:54.998211: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n[rank0]: 2024-09-23 17:49:55.013891: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n[rank0]: 2024-09-23 17:49:55.032927: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n[rank0]: 2024-09-23 17:49:55.038641: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n[rank0]: 2024-09-23 17:49:55.054505: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n[rank0]: To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n[rank0]: 2024-09-23 17:49:56.155623: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n[rank0]: 2024-09-23 17:50:02.535955: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n[rank0]: 2024-09-23 17:50:02.536583: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n[rank0]: 2024-09-23 17:50:02.551631: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n[rank0]: 2024-09-23 17:50:02.551680: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n[rank0]: 2024-09-23 17:50:02.571366: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n[rank0]: 2024-09-23 17:50:02.571366: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n[rank0]: 2024-09-23 17:50:02.577084: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n[rank0]: 2024-09-23 17:50:02.577130: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n[rank0]: 2024-09-23 17:50:02.591725: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n[rank0]: To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n[rank0]: 2024-09-23 17:50:02.591727: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n[rank0]: To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n[rank0]: 2024-09-23 17:50:03.665481: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n[rank0]: 2024-09-23 17:50:03.791268: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n[rank0]: Process SpawnProcess-1:\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\n[rank0]:     self.run()\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\n[rank0]:     self._target(*self._args, **self._kwargs)\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\", line 37, in producer\r\n[rank0]:     handle = lib.cudaIpcGetMemHandle(pointer)\r\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/distributed/device_communicators/cuda_wrapper.py\", line 162, in cudaIpcGetMemHandle\r\n[rank0]:     self.CUDART_CHECK(self.funcs[\"cudaIpcGetMemHandle\"](\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/distributed/device_communicators/cuda_wrapper.py\", line 127, in CUDART_CHECK\r\n[rank0]:     raise RuntimeError(f\"CUDART error: {error_str}\")\r\n[rank0]: RuntimeError: CUDART error: invalid argument\r\n[rank0]: Process SpawnProcess-2:\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\", line 245, in <module>\r\n[rank0]:     result = can_actually_p2p(batch_src, batch_tgt)\r\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\", line 147, in can_actually_p2p\r\n[rank0]:     p_tgt.join()\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/multiprocessing/process.py\", line 149, in join\r\n[rank0]:     res = self._popen.wait(timeout)\r\n[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/multiprocessing/popen_fork.py\", line 43, in wait\r\n[rank0]:     return self.poll(os.WNOHANG if timeout == 0.0 else 0)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/multiprocessing/popen_fork.py\", line 27, in poll\r\n[rank0]:     pid, sts = os.waitpid(self.pid, flag)\r\n[rank0]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: KeyboardInterrupt\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\n[rank0]:     self.run()\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\n[rank0]:     self._target(*self._args, **self._kwargs)\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\", line 67, in consumer\r\n[rank0]:     handle = producer_queue.get()\r\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/multiprocessing/queues.py\", line 103, in get\r\n[rank0]:     res = self._recv_bytes()\r\n[rank0]:           ^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/multiprocessing/connection.py\", line 216, in recv_bytes\r\n[rank0]:     buf = self._recv_bytes(maxlength)\r\n[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\r\n[rank0]:     buf = self._recv(4)\r\n[rank0]:           ^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/data/miniconda/envs/llm/lib/python3.11/multiprocessing/connection.py\", line 395, in _recv\r\n[rank0]:     chunk = read(handle, remaining)\r\n[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: KeyboardInterrupt\r\n\r\n^C\r\n\r\n### the code i'm using \r\n'''\r\nfrom vllm import LLM\r\nimport os\r\n\r\nos.environ['VLLM_ATTENTION_BACKEND'] = 'FLASHINFER'\r\n\r\n\r\nmodel = LLM(model=\"/ofs/llm-for-dptd/modelscope/LLM-Research/gemma-2-27b-it\",\r\n                    dtype=\"auto\",\r\n                    trust_remote_code=True,\r\n                    tokenizer_mode=\"auto\",\r\n                    tensor_parallel_size=2)\r\n'''\r\n\r\n### \ud83d\udc1b Describe the bug\r\nit just stuck here for a few hours...\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-09-23T09:59:54+00:00",
    "closed_at": null,
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8735/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8735"
  },
  {
    "number": 10919,
    "title": "[Feature]: Provide pre-built CPU docker image",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nHi thanks for the lib! Currently it seems that https://docs.vllm.ai/en/v0.6.1/getting_started/cpu-installation.html requires build the docker image by oneself, thus it would be great to have a prebuilt one (probably by CI).\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-12-05T06:41:08+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10919/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/10919"
  },
  {
    "number": 12005,
    "title": "[Bug]: Very slow guided decoding with Outlines backend since v0.6.5",
    "body": "### Your current environment\n\n<details>\r\n<summary>Output of `pip list`</summary>\r\nPackage                           Version\r\n--------------------------------- -------------\r\nabsl-py                           2.1.0\r\naccelerate                        1.1.1\r\naiofiles                          23.2.1\r\naiohappyeyeballs                  2.4.4\r\naiohttp                           3.11.9\r\naiohttp-cors                      0.7.0\r\naiosignal                         1.3.1\r\nairportsdata                      20241001\r\nannotated-types                   0.7.0\r\nanyio                             4.6.2.post1\r\nastor                             0.8.1\r\nattrs                             24.2.0\r\nbert-score                        0.3.13\r\nbitsandbytes                      0.44.1\r\nblake3                            1.0.1\r\ncachetools                        5.5.0\r\ncertifi                           2024.8.30\r\ncharset-normalizer                3.4.0\r\nchex                              0.1.87\r\nclick                             8.1.7\r\ncloudpickle                       3.1.0\r\ncolorful                          0.5.6\r\ncompressed-tensors                0.8.1\r\ncontourpy                         1.3.1\r\ncycler                            0.12.1\r\ndatasets                          3.1.0\r\ndemjson3                          3.0.6\r\ndepyf                             0.18.0\r\ndill                              0.3.8\r\ndiskcache                         5.6.3\r\ndistlib                           0.3.9\r\ndistro                            1.9.0\r\neinops                            0.8.0\r\netils                             1.11.0\r\nevaluate                          0.4.3\r\nfastapi                           0.115.5\r\nfbgemm_gpu                        1.0.0\r\nffmpy                             0.4.0\r\nfilelock                          3.16.1\r\nflash-attn                        2.7.0.post2\r\nflax                              0.10.2\r\nfonttools                         4.55.0\r\nfrozenlist                        1.5.0\r\nfsspec                            2024.9.0\r\ngguf                              0.10.0\r\ngoogle-api-core                   2.24.0\r\ngoogle-auth                       2.37.0\r\ngoogleapis-common-protos          1.66.0\r\ngradio                            5.7.1\r\ngradio_client                     1.5.0\r\ngrpcio                            1.69.0\r\nh11                               0.14.0\r\nhttpcore                          1.0.7\r\nhttptools                         0.6.4\r\nhttpx                             0.27.2\r\nhuggingface-hub                   0.26.3\r\nhumanize                          4.11.0\r\nidna                              3.10\r\nimportlib_metadata                8.5.0\r\nimportlib_resources               6.4.5\r\niniconfig                         2.0.0\r\ninteregular                       0.3.3\r\njax                               0.4.35\r\njaxlib                            0.4.35\r\nJinja2                            3.1.4\r\njiter                             0.8.0\r\njoblib                            1.4.2\r\njsonschema                        4.23.0\r\njsonschema-specifications         2024.10.1\r\nkiwisolver                        1.4.7\r\nlark                              1.2.2\r\nLevenshtein                       0.26.1\r\nlinkify-it-py                     2.0.3\r\nlitellm                           1.53.7\r\nllvmlite                          0.43.0\r\nlm-format-enforcer                0.10.9\r\nmarkdown-it-py                    3.0.0\r\nMarkupSafe                        2.1.5\r\nmatplotlib                        3.9.3\r\nmdit-py-plugins                   0.4.2\r\nmdurl                             0.1.2\r\nmemray                            1.15.0\r\nmistral_common                    1.5.1\r\nml_dtypes                         0.5.0\r\nmore-itertools                    10.5.0\r\nmpmath                            1.3.0\r\nmsgpack                           1.1.0\r\nmsgspec                           0.18.6\r\nmultidict                         6.1.0\r\nmultiprocess                      0.70.16\r\nnest-asyncio                      1.6.0\r\nnetworkx                          3.4.2\r\nnltk                              3.9.1\r\nnumba                             0.60.0\r\nnumpy                             1.26.4\r\nnvidia-cublas-cu12                12.4.5.8\r\nnvidia-cuda-cupti-cu12            12.4.127\r\nnvidia-cuda-nvrtc-cu12            12.4.127\r\nnvidia-cuda-runtime-cu12          12.4.127\r\nnvidia-cudnn-cu12                 9.1.0.70\r\nnvidia-cufft-cu12                 11.2.1.3\r\nnvidia-curand-cu12                10.3.5.147\r\nnvidia-cusolver-cu12              11.6.1.9\r\nnvidia-cusparse-cu12              12.3.1.170\r\nnvidia-ml-py                      12.560.30\r\nnvidia-nccl-cu12                  2.21.5\r\nnvidia-nvjitlink-cu12             12.4.127\r\nnvidia-nvtx-cu12                  12.4.127\r\nopenai                            1.56.0\r\nopencensus                        0.11.4\r\nopencensus-context                0.1.3\r\nopencv-python-headless            4.10.0.84\r\nopt_einsum                        3.4.0\r\noptax                             0.2.4\r\norbax-checkpoint                  0.10.1\r\norjson                            3.10.12\r\noutlines                          0.1.11\r\noutlines_core                     0.1.26\r\npackaging                         24.2\r\npandas                            2.2.3\r\npartial-json-parser               0.2.1.1.post4\r\npillow                            10.4.0\r\npip                               24.3.1\r\nplatformdirs                      4.3.6\r\npluggy                            1.5.0\r\nprometheus_client                 0.21.0\r\nprometheus-fastapi-instrumentator 7.0.0\r\npropcache                         0.2.1\r\nproto-plus                        1.25.0\r\nprotobuf                          3.20.3\r\npsutil                            6.1.0\r\npy-cpuinfo                        9.0.0\r\npy-spy                            0.4.0\r\npyairports                        2.1.1\r\npyarrow                           18.1.0\r\npyasn1                            0.6.1\r\npyasn1_modules                    0.4.1\r\npybind11                          2.13.6\r\npycountry                         24.6.1\r\npydantic                          2.10.2\r\npydantic_core                     2.27.1\r\npydub                             0.25.1\r\nPygments                          2.18.0\r\npyinfer                           0.0.3\r\npyparsing                         3.2.0\r\npytest                            8.3.4\r\npython-dateutil                   2.9.0.post0\r\npython-dotenv                     1.0.1\r\npython-multipart                  0.0.12\r\npytz                              2024.2\r\nPyYAML                            6.0.2\r\npyzmq                             26.2.0\r\nRapidFuzz                         3.10.1\r\nray                               2.39.0\r\nreferencing                       0.35.1\r\nregex                             2024.11.6\r\nrequests                          2.32.3\r\nrich                              13.9.4\r\nrouge_score                       0.1.2\r\nrpds-py                           0.22.0\r\nrsa                               4.9\r\nruff                              0.8.1\r\nsacremoses                        0.1.1\r\nsafehttpx                         0.1.6\r\nsafetensors                       0.4.5\r\nScandEval                         14.2.0.dev0\r\nscikit-learn                      1.5.2\r\nscipy                             1.14.1\r\nsemantic-version                  2.10.0\r\nsentencepiece                     0.2.0\r\nseqeval                           1.2.2\r\nsetuptools                        75.6.0\r\nshellingham                       1.5.4\r\nsimplejson                        3.19.3\r\nsix                               1.16.0\r\nsmart-open                        7.1.0\r\nsniffio                           1.3.1\r\nstarlette                         0.41.3\r\nsympy                             1.13.1\r\ntabulate                          0.9.0\r\ntenacity                          9.0.0\r\ntensorstore                       0.1.69\r\ntermcolor                         2.5.0\r\ntextual                           1.0.0\r\nthreadpoolctl                     3.5.0\r\ntiktoken                          0.7.0\r\ntokenizers                        0.21.0\r\ntomlkit                           0.12.0\r\ntoolz                             1.0.0\r\ntorch                             2.5.1\r\ntorchvision                       0.20.1\r\ntqdm                              4.67.1\r\ntransformers                      4.48.0\r\ntriton                            3.1.0\r\ntyper                             0.14.0\r\ntyping_extensions                 4.12.2\r\ntzdata                            2024.2\r\nuc-micro-py                       1.0.3\r\nurllib3                           2.2.3\r\nuvicorn                           0.32.1\r\nuvloop                            0.21.0\r\nvirtualenv                        20.28.1\r\nvllm                              0.6.6.post1\r\nwatchfiles                        1.0.0\r\nwebsockets                        12.0\r\nwheel                             0.45.1\r\nwrapt                             1.17.1\r\nxformers                          0.0.28.post3\r\nxgrammar                          0.1.9\r\nxxhash                            3.5.0\r\nyarl                              1.18.3\r\nzipp                              3.21.0\r\n</details>\r\n\r\n<details>\r\n<summary>Output of `lsb_release -a`</summary>\r\nNo LSB modules are available.\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 24.04.1 LTS\r\nRelease:\t24.04\r\nCodename:\tnoble\r\n</details>\r\n\r\n<details>\r\n<summary>Output of `nvidia-smi -L`</summary>\r\nGPU 0: NVIDIA GeForce RTX 3090 Ti (UUID: GPU-555d27a4-2596-131d-330e-1d8aecba86f6)\r\n</details>\n\n### Model Input Dumps\n\nN/A\n\n### \ud83d\udc1b Describe the bug\n\n```python\r\nfrom vllm import LLM, SamplingParams\r\nfrom vllm.sampling_params import GuidedDecodingParams\r\nfrom pydantic import BaseModel\r\nfrom transformers import AutoTokenizer\r\n\r\nclass Person(BaseModel):\r\n    name: str\r\n    description: str\r\n\r\nprompt = \"\"\"<s>[INST] <<SYS>>\r\n    You are a json text extractor. return the following json {\"name\": \"the game name\", \"description\": \"description of the game in around 400 words\"}\r\n    <</SYS>>\r\n    { CD Projekt Red is ramping up production on The Witcher 4, and of course it's looking into using AI } [/INST]\"\"\"\r\n\r\nllm = LLM(model=\"meta-llama/Llama-3.1-8B-Instruct\")\r\nresult = llm.generate(\r\n    prompts=[prompt] * 2000,\r\n    sampling_params=SamplingParams(\r\n        temperature=0.6,\r\n        max_tokens=1024,\r\n        guided_decoding=GuidedDecodingParams(json=Person.model_json_schema(), backend=\"outlines\"),\r\n    ),\r\n)\r\nprint(result)\r\n```\r\n\r\nRunning the above with vLLM <= 0.6.4 and vLLM > 0.6.4, there are massive differences in both time and (CPU) memory consumption. With the older versions it consumes ~4GB RAM and the newer ones it consumes >50GB RAM, and also takes orders of magnitude longer to even start generating.\r\n\r\nRelated to this Outlines issue: https://github.com/dottxt-ai/outlines/issues/1351.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "open",
    "created_at": "2025-01-13T11:06:36+00:00",
    "closed_at": null,
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12005/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12005"
  },
  {
    "number": 4241,
    "title": "[Bug]: Ray memory leak",
    "body": "### Your current environment\r\n\r\n```text\r\nPyTorch version: 2.1.2+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.11.3 (main, Apr 19 2024, 17:22:27) [GCC 9.4.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-177-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 10.1.243\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA A40\r\nGPU 1: NVIDIA A40\r\nGPU 2: NVIDIA A40\r\n\r\nNvidia driver version: 535.161.08\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      40 bits physical, 48 bits virtual\r\nCPU(s):                             64\r\nOn-line CPU(s) list:                0-63\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 1\r\nSocket(s):                          64\r\nNUMA node(s):                       1\r\nVendor ID:                          AuthenticAMD\r\nCPU family:                         25\r\nModel:                              1\r\nModel name:                         AMD EPYC-Milan Processor\r\nStepping:                           1\r\nCPU MHz:                            2994.374\r\nBogoMIPS:                           5988.74\r\nVirtualization:                     AMD-V\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          2 MiB\r\nL1i cache:                          2 MiB\r\nL2 cache:                           32 MiB\r\nL3 cache:                           2 GiB\r\nNUMA node0 CPU(s):                  0-63\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm rep_good nopl cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr wbnoinvd arat npt nrip_save umip vaes vpclmulqdq rdpid arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.18.1\r\n[pip3] torch==2.1.2\r\n[pip3] triton==2.1.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tPHB\tPHB\t0-63\t0\t\tN/A\r\nGPU1\tPHB\t X \tPHB\t0-63\t0\t\tN/A\r\nGPU2\tPHB\tPHB\t X \t0-63\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI'm using vLLM with several models within the same Python session (one model at a time), and I'm running on a multi-GPU setup. After each model run I need to clear the GPU memory to leave room for the next model, which means (among other things) that I need to shut down the Ray cluster (via `ray.shutdown()`). That's all fine, but this only clears the GPU memory of one of the GPUs.\r\n\r\nMinimal example:\r\n\r\n```python\r\nfrom vllm import LLM\r\nimport ray\r\n\r\n#\u00a0Instantiate first model\r\nllm = LLM(\"mhenrichsen/danskgpt-tiny\", tensor_parallel_size=2)\r\n\r\n# Destroy Ray cluster; this only clears the GPU memory on one of the GPUs\r\n# Note that adding any combination of `torch.cuda.empty_cache()`, \r\n# `gc.collect()` or `destroy_model_parallel()` doesn't help here\r\nray.shutdown()\r\n\r\n#\u00a0Instantiate second model; this now causes OOM errors\r\nllm = LLM(\"mhenrichsen/danskgpt-tiny-chat\", tensor_parallel_size=2)\r\n```\r\n\r\n[This is a known Ray issue](https://github.com/ray-project/ray/issues/29606), where the solution as mentioned in that issue as well as [the official Ray docs](https://github.com/ray-project/ray/blob/master/doc/source/ray-core/fault_tolerance/tasks.rst#cancelling-misbehaving-tasks) is to include `max_calls=1` in `ray.remote` calls, which supposedly fixes it. In vLLM this is called in [these lines in the vllm.executor.ray_gpu_executor module](https://github.com/vllm-project/vllm/blob/cc74b2b232070f74d8765a5eefa49ae93ee45490/vllm/executor/ray_gpu_executor.py#L94-L98) and [these lines in the vllm.engine.async_llm_engine module](https://github.com/vllm-project/vllm/blob/cc74b2b232070f74d8765a5eefa49ae93ee45490/vllm/engine/async_llm_engine.py#L420). However, in those applications the decorator is wrapping a class (an \"actor\" in Ray speak), where the `max_calls` argument is not allowed, so I'm not sure if this solution helps here.",
    "labels": [
      "bug",
      "ray",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-04-21T14:06:32+00:00",
    "closed_at": null,
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4241/reactions",
      "total_count": 5,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 5
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4241"
  },
  {
    "number": 7382,
    "title": "[Bug]: LLaMa 3.1 8B/70B/405B all behave poorly and differently using completions API as compared to good chat API",
    "body": "### Your current environment\r\n\r\nDocker latest 0.5.4\r\n\r\n```\r\ndocker pull vllm/vllm-openai:latest\r\ndocker run -d --restart=always \\\r\n    --runtime=nvidia \\\r\n    --gpus '\"device=0\"' \\\r\n    --shm-size=10.24gb \\\r\n    -p 5000:5000 \\\r\n        -e NCCL_IGNORE_DISABLED_P2P=1 \\\r\n    -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\\r\n    -e VLLM_NCCL_SO_PATH=/usr/local/lib/python3.10/dist-packages/nvidia/nccl/lib/libnccl.so.2 \\\r\n    -v /etc/passwd:/etc/passwd:ro \\\r\n    -v /etc/group:/etc/group:ro \\\r\n    -u `id -u`:`id -g` \\\r\n    -v \"${HOME}\"/.cache:$HOME/.cache/ \\\r\n    -v \"${HOME}\"/.cache/huggingface:$HOME/.cache/huggingface \\\r\n    -v \"${HOME}\"/.cache/huggingface/hub:$HOME/.cache/huggingface/hub \\\r\n    -v \"${HOME}\"/.config:$HOME/.config/   -v \"${HOME}\"/.triton:$HOME/.triton/  \\\r\n    --network host \\\r\n    --name llama31_8b \\\r\n    vllm/vllm-openai:latest \\\r\n        --port=5000 \\\r\n        --host=0.0.0.0 \\\r\n        --model=meta-llama/Meta-Llama-3.1-8B-Instruct \\\r\n        --seed 1234 \\\r\n        --tensor-parallel-size=1 \\\r\n        --max-model-len=131072 \\\r\n        --max-num-batched-tokens=131072 --max-log-len=100 \\\r\n        --served-model-name meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3-8B-Instruct \\\r\n        --download-dir=$HOME/.cache/huggingface/hub &>> logs.vllm_server.llama3-8b.txt\r\n```\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI cannot reproduce HuggingFace transformers (works) or Chat API with vLLM (works) against Completions API (fails).\r\n\r\nBoth streaming and non-streaming completions API behave poorly.\r\n\r\ncode:\r\n```\r\nbase_url = '' # FILL\r\n\r\nfrom transformers import AutoTokenizer\r\n\r\nmodel = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\r\nsystem_prompt = \"\"\"Follow these steps in solving any problem:\r\n1) Know: This will help students find the important information.\r\n2) Need to Know: This will force students to reread the question and write down what they are trying to solve for.\r\n3) Organize:  I think this would be a great place for teachers to emphasize drawing a model or picture.\r\n4) Work: Students show their calculations here.\r\n5) Solution: This is where students will ask themselves if the answer is reasonable and whether it answered the question.\"\"\"\r\nprompt = \"What is bigger, 9.9 or 9.11?\"\r\n\r\nmessages = [\r\n    {\r\n        \"role\": \"system\",\r\n        \"content\": system_prompt,\r\n    },\r\n    {\r\n        \"role\": \"user\",\r\n        \"content\": prompt,\r\n    }\r\n]\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\nadd_generation_prompt = True\r\nprompt_llm = tokenizer.apply_chat_template(conversation=messages, tokenize=False,\r\n                                           add_generation_prompt=add_generation_prompt)\r\nprint(\"prompt_llm: %s\\n\\n\" % prompt_llm)\r\n\r\nkwargs = dict(max_tokens=1024, temperature=0)\r\n\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(base_url=base_url)\r\nkwargs['model'] = model\r\n\r\nfor i in range(1, 2):\r\n    kwargs['seed'] = i\r\n\r\n    responses = client.completions.create(**kwargs, stream=False, prompt=prompt_llm)\r\n    text = responses.choices[0].text\r\n    print(\"seed: %s: stream nochat:\\n\\n %s\" % (i, text))\r\n\r\n    responses = client.completions.create(**kwargs, stream=True, prompt=prompt_llm)\r\n    text = ''\r\n    for event in responses:\r\n        delta = event.choices[0].text if event.choices else None  # extract the text\r\n        if delta:\r\n            text += delta  # append the text\r\n    print(\"seed: %s: stream nochat:\\n\\n %s\" % (i, text))\r\n\r\n    response = client.chat.completions.create(messages=messages, **kwargs, stream=False).choices[0].message.content\r\n    print(\"seed: %s: nostream chat:\\n\\n %s\" % (i, response))\r\n\r\n    responses = client.chat.completions.create(stream=True, messages=messages, **kwargs)\r\n    text = ''\r\n    for chunk in responses:\r\n        delta = chunk.choices[0].delta.content if chunk.choices else None\r\n        if delta:\r\n            text += delta\r\n    print(\"seed: %s: stream chat:\\n\\n %s\" % (i, text))\r\n```\r\n\r\nThis always gives something like:\r\n\r\n```\r\nprompt_llm: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\r\n\r\nCutting Knowledge Date: December 2023\r\nToday Date: 26 Jul 2024\r\n\r\nFollow these steps in solving any problem:\r\n1) Know: This will help students find the important information.\r\n2) Need to Know: This will force students to reread the question and write down what they are trying to solve for.\r\n3) Organize:  I think this would be a great place for teachers to emphasize drawing a model or picture.\r\n4) Work: Students show their calculations here.\r\n5) Solution: This is where students will ask themselves if the answer is reasonable and whether it answered the question.<|eot_id|><|start_header_id|>user<|end_header_id|>\r\n\r\nWhat is bigger, 9.9 or 9.11?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\r\n\r\n\r\n\r\n\r\nseed: 1: stream nochat:\r\n\r\n To solve this problem, we will follow the steps:\r\n\r\n1. Know: We know that we are comparing two decimal numbers, 9.9 and 9.11.\r\n\r\n2. Need to Know: We need to know which number is bigger. \r\n\r\n3. Organize: We can organize this by comparing the numbers. Since they are decimals, we can compare them by looking at the tenths place and the hundredths place.\r\n\r\n4. Work: \r\n- The tenths place is the same in both numbers (9).\r\n- The hundredths place in 9.9 is 0, and in 9.11, it is 1.\r\n- Since 1 is greater than 0, 9.11 is greater than 9.9.\r\n\r\n5. Solution: Yes, 9.11 is indeed bigger than 9.9.\r\nseed: 1: stream nochat:\r\n\r\n To solve this problem, we will follow the steps:\r\n\r\n1. Know: We know that we are comparing two decimal numbers, 9.9 and 9.11.\r\n\r\n2. Need to Know: We need to know which number is bigger. \r\n\r\n3. Organize: We can organize this by comparing the numbers. Since they are decimals, we can compare them by looking at the tenths place and the hundredths place.\r\n\r\n4. Work: \r\n- The tenths place is the same in both numbers (9).\r\n- The hundredths place in 9.9 is 0, and in 9.11, it is 1.\r\n- Since 1 is greater than 0, 9.11 is greater than 9.9.\r\n\r\n5. Solution: Yes, 9.11 is indeed bigger than 9.9.\r\nseed: 1: nostream chat:\r\n\r\n Let's follow the steps to solve this problem:\r\n\r\n**Know**: We need to compare two decimal numbers: 9.9 and 9.11.\r\n\r\n**Need to Know**: We are trying to determine which number is bigger.\r\n\r\n**Organize**: Let's think about the numbers. We can compare them by looking at the decimal part. If the decimal part is greater, the number is bigger.\r\n\r\n**Work**: We can compare the decimal parts: 0.9 is less than 0.11. Since 0.11 is greater, 9.11 is bigger than 9.9.\r\n\r\n**Solution**: Yes, 9.11 is indeed bigger than 9.9. This makes sense because 0.11 is greater than 0.9.\r\nseed: 1: stream chat:\r\n\r\n Let's follow the steps to solve this problem:\r\n\r\n**Know**: We need to compare two decimal numbers: 9.9 and 9.11.\r\n\r\n**Need to Know**: We are trying to determine which number is bigger.\r\n\r\n**Organize**: Let's think about the numbers. We can compare them by looking at the decimal part. If the decimal part is greater, the number is bigger.\r\n\r\n**Work**: We can compare the decimal parts: 0.9 is less than 0.11. Since 0.11 is greater, 9.11 is bigger than 9.9.\r\n\r\n**Solution**: Yes, 9.11 is indeed bigger than 9.9. This makes sense because 0.11 is greater than 0.9.\r\n```\r\n\r\nThat is, the Chat API output disagrees with the completions output, and it's always (varying seed, penalty, etc.) is always worse than the Chat API.\r\n\r\nI checked the code in vLLM and can't see what I'm doing differently in using the chat template.  With temperature=0, should be deterministic to good extent.  One can add repeats and see that it's always this way.",
    "labels": [
      "bug",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-08-10T01:47:36+00:00",
    "closed_at": null,
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7382/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7382"
  }
]
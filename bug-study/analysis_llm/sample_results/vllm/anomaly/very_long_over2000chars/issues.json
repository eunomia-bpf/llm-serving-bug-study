[
  {
    "number": 17431,
    "title": "[Bug]: A800 GPU set VLLM_USE_V1=1 ValueError: No available memory for the cache blocks",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 04-30 10:44:38 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\n/home/python_vllm_env_085/lib/python3.11/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n  warnings.warn(\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: BigCloud Enterprise Linux For Euler 21.10 LTS (x86_64)\nGCC version: (GCC) 7.3.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.28\n\nPython version: 3.11.9 (main, Jan 14 2025, 14:39:54) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] (64-bit runtime)\nPython platform: Linux-4.19.90-2107.6.0.0192.8.oe1.bclinux.x86_64-x86_64-with-glibc2.28\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA A800 80GB PCIe\nNvidia driver version: 550.54.14\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\n\u67b6\u6784\uff1a                           x86_64\nCPU \u8fd0\u884c\u6a21\u5f0f\uff1a                   32-bit, 64-bit\n\u5b57\u8282\u5e8f\uff1a                         Little Endian\nAddress sizes:                   46 bits physical, 57 bits virtual\nCPU:                             64\n\u5728\u7ebf CPU \u5217\u8868\uff1a                  0-63\n\u6bcf\u4e2a\u6838\u7684\u7ebf\u7a0b\u6570\uff1a                 2\n\u6bcf\u4e2a\u5ea7\u7684\u6838\u6570\uff1a                   32\n\u5ea7\uff1a                             1\nNUMA \u8282\u70b9\uff1a                      1\n\u5382\u5546 ID\uff1a                        GenuineIntel\nCPU \u7cfb\u5217\uff1a                       6\n\u578b\u53f7\uff1a                           106\n\u578b\u53f7\u540d\u79f0\uff1a                       Intel(R) Xeon(R) Gold 6330 CPU @ 2.00GHz\n\u6b65\u8fdb\uff1a                           6\nCPU MHz\uff1a                        1995.312\nBogoMIPS\uff1a                       3990.62\n\u865a\u62df\u5316\uff1a                         VT-x\n\u8d85\u7ba1\u7406\u5668\u5382\u5546\uff1a                   KVM\n\u865a\u62df\u5316\u7c7b\u578b\uff1a                     \u5b8c\u5168\nL1d \u7f13\u5b58\uff1a                       1 MiB\nL1i \u7f13\u5b58\uff1a                       1 MiB\nL2 \u7f13\u5b58\uff1a                        128 MiB\nL3 \u7f13\u5b58\uff1a                        16 MiB\nNUMA \u8282\u70b90 CPU\uff1a                 0-63\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Vulnerable: eIBRS with unprivileged eBPF\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Mitigation; TSX disabled\n\u6807\u8bb0\uff1a                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq dtes64 vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid md_clear arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==2.2.5\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.3\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.5\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-63    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=/home/python_vllm_env_085/lib/:\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\nif I set VLLM_USE_V1=0 ,vllm server can start success. otherwise fail.\n\n\n### \ud83d\udc1b Describe the bug\n\n### success case\n```\nVLLM_USE_V1=0  /home/python_vllm_env_085/bin/python -m vllm.entrypoints.openai.api_server     --model /root/yjpzmodel/basemodel/Qwen3-8B     --served-model-name Qwen3-8B --gpu-memory-utilization 0.4 --port 8001\n```\n### error case \n```\n/home/python_vllm_env_085/bin/python -m vllm.entrypoints.openai.api_server     --model /root/yjpzmodel/basemodel/Qwen3-8B     --served-model-name Qwen3-8B --gpu-memory-utilization 0.4 --port 8001\n```\n### error log\n```\nINFO 04-30 10:20:35 [monitor.py:33] torch.compile takes 13.93 s in total\nERROR 04-30 10:20:36 [core.py:396] EngineCore failed to start.\nERROR 04-30 10:20:36 [core.py:396] Traceback (most recent call last):\nERROR 04-30 10:20:36 [core.py:396]   File \"/home/python_vllm_env_085/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\nERROR 04-30 10:20:36 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 04-30 10:20:36 [core.py:396]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-30 10:20:36 [core.py:396]   File \"/home/python_vllm_env_085/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\nERROR 04-30 10:20:36 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,\nERROR 04-30 10:20:36 [core.py:396]   File \"/home/python_vllm_env_085/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 71, in __init__\nERROR 04-30 10:20:36 [core.py:396]     self._initialize_kv_caches(vllm_config)\nERROR 04-30 10:20:36 [core.py:396]   File \"/home/python_vllm_env_085/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 133, in _initialize_kv_caches\nERROR 04-30 10:20:36 [core.py:396]     kv_cache_configs = [\nERROR 04-30 10:20:36 [core.py:396]                        ^\nERROR 04-30 10:20:36 [core.py:396]   File \"/home/python_vllm_env_085/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 134, in <listcomp>\nERROR 04-30 10:20:36 [core.py:396]     get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,\nERROR 04-30 10:20:36 [core.py:396]   File \"/home/python_vllm_env_085/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 699, in get_kv_cache_config\nERROR 04-30 10:20:36 [core.py:396]     check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)\nERROR 04-30 10:20:36 [core.py:396]   File \"/home/python_vllm_env_085/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 527, in check_enough_kv_cache_memory\nERROR 04-30 10:20:36 [core.py:396]     raise ValueError(\"No available memory for the cache blocks. \"\nERROR 04-30 10:20:36 [core.py:396] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.\nProcess EngineCore_0:\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-04-30T02:52:40+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17431/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17431"
  },
  {
    "number": 6322,
    "title": "[Bug]: VLLM 0.5.1 with LLaVA 1.6 exceptions",
    "body": "### Your current environment\n\nSee https://github.com/vllm-project/vllm/issues/6176\r\n\n\n### \ud83d\udc1b Describe the bug\n\nI have lots of image, where the service throws exception and after that must be restarted, because it stucks in exception mode, even for images, that worked before.\r\nExample image below.\r\n\r\n```\r\ncurl 'https://ai1.dev.init/multimodal-llava/v1/chat/completions' -k -H 'Content-Type: application/json' -d @- <<EOF\r\n{\r\n    \"model\": \"llava-hf/llava-v1.6-mistral-7b-hf\",\r\n    \"messages\": [\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": [\r\n                {\r\n                    \"type\": \"image_url\",\r\n                    \"image_url\": {\r\n                        \"url\": \"data:image/jpeg;base64,$(base64 -w 0 /opt/initai_copilot/data/images/image2015-7-20_16_17_54.png)\"\r\n                    }\r\n                },\r\n                {\r\n                    \"type\": \"text\",\r\n                    \"text\": \"Was ist in dem Bild?\"\r\n                }\r\n            ]\r\n        }\r\n    ],\r\n    \"temperature\": 0.2,\r\n    \"top_p\": 0.1,\r\n    \"top_k\": 20,\r\n    \"frequency_penalty\": 0.2\r\n}\r\nEOF\r\n```\r\n\r\nImage:\r\n![image2015-7-20_16_17_54](https://github.com/vllm-project/vllm/assets/2312884/7cde0c86-0b25-4b78-b662-129d584f4f8c)\r\n\r\nException\r\n\r\n```\r\nINFO:     Started server process [1]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nINFO 07-11 06:04:31 metrics.py:295] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\r\nINFO 07-11 06:04:36 metrics.py:295] Avg prompt throughput: 208.2 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\r\nINFO 07-11 06:04:42 metrics.py:295] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop: Attempted to assign 776 = 776 image tokens to 826 placeholders, Traceback (most recent call last):\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 64, in start_worker_execution_loop\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     output = self.execute_model(execute_model_req=None)\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 271, in execute_model\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     output = self.model_runner.execute_model(\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1243, in execute_model\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     hidden_or_intermediate_states = model_executable(\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llava_next.py\", line 494, in forward\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     inputs_embeds = merge_vision_embeddings(\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 35, in merge_vision_embeddings\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     raise ValueError(\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226] ValueError: Attempted to assign 776 = 776 image tokens to 826 placeholders\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]\r\nERROR 07-11 06:04:42 async_llm_engine.py:53] Engine background task failed\r\nERROR 07-11 06:04:42 async_llm_engine.py:53] Traceback (most recent call last):\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 43, in _log_task_completion\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     return_value = task.result()\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 595, in run_engine_loop\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     result = task.result()\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 540, in engine_step\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     request_outputs = await self.engine.step_async(virtual_engine)\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 241, in step_async\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     output = await self.model_executor.execute_model_async(\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 173, in execute_model_async\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     return await self._driver_execute_model_async(execute_model_req)\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 160, in _driver_execute_model_async\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     return await self.driver_exec_model(execute_model_req)\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     result = self.fn(*self.args, **self.kwargs)\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 271, in execute_model\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     output = self.model_runner.execute_model(\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     return func(*args, **kwargs)\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1243, in execute_model\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     hidden_or_intermediate_states = model_executable(\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     return self._call_impl(*args, **kwargs)\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     return forward_call(*args, **kwargs)\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llava_next.py\", line 494, in forward\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     inputs_embeds = merge_vision_embeddings(\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 35, in merge_vision_embeddings\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     raise ValueError(\r\nERROR 07-11 06:04:42 async_llm_engine.py:53] ValueError: Attempted to assign 776 = 776 image tokens to 826 placeholders\r\nException in callback functools.partial(<function _log_task_completion at 0x7d88c3437880>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7d88ab11c1f0>>)\r\nhandle: <Handle functools.partial(<function _log_task_completion at 0x7d88c3437880>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7d88ab11c1f0>>)>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 43, in _log_task_completion\r\n    return_value = task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 595, in run_engine_loop\r\n    result = task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 540, in engine_step\r\n    request_outputs = await self.engine.step_async(virtual_engine)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 241, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 173, in execute_model_async\r\n    return await self._driver_execute_model_async(execute_model_req)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 160, in _driver_execute_model_async\r\n    return await self.driver_exec_model(execute_model_req)\r\n  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 271, in execute_model\r\n    output = self.model_runner.execute_model(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1243, in execute_model\r\n    hidden_or_intermediate_states = model_executable(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llava_next.py\", line 494, in forward\r\n    inputs_embeds = merge_vision_embeddings(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 35, in merge_vision_embeddings\r\n    raise ValueError(\r\nValueError: Attempted to assign 776 = 776 image tokens to 826 placeholders\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 55, in _log_task_completion\r\n    raise AsyncEngineDeadError(\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for theactual cause.\r\nINFO:     192.168.6.1:46738 - \"POST /v1/chat/completions HTTP/1.0\" 400 Bad Request\r\n```\r\n\r\nConfig of Docker image:\r\n\r\n```\r\nservices:\r\n  vllm-llava:\r\n    image: vllm/vllm-openai:v0.5.1\r\n    container_name: vllm-llava\r\n    ipc: host\r\n    deploy:\r\n      resources:\r\n        reservations:\r\n          devices:\r\n            - driver: nvidia\r\n              capabilities: [ gpu ]\r\n    environment:\r\n      - HTTPS_PROXY=http://proxy.dev.init:3128\r\n      - HTTP_PROXY=http://proxy.dev.init:3128\r\n      - NO_PROXY=10.9.240.0/22,127.0.0.0/8\r\n      - NVIDIA_VISIBLE_DEVICES=2,3\r\n      - HF_TOKEN=$HF_TOKEN\r\n      - VLLM_NO_USAGE_STATS=1\r\n    volumes:\r\n      - /mnt/sda/huggingface:/root/.cache/huggingface\r\n      - .:/opt/vllm\r\n    ports:\r\n      - \"8003:8000\"\r\n    command:\r\n      - --model=llava-hf/llava-v1.6-mistral-7b-hf\r\n      # - --chat-template=/opt/vllm/template_mixtral.jinja\r\n      # - --max-model-len=24576\r\n      - --tensor-parallel-size=2\r\n      # - --swap-space=5\r\n      - --gpu-memory-utilization=0.3\r\n      # - --max-num-batched-tokens=2048\r\n      - --disable-log-requests\r\n      - --enforce-eager\r\n      # - --enable-chunked-prefill\r\n    restart: unless-stopped\r\n\r\n```\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-07-11T06:11:25+00:00",
    "closed_at": "2024-07-11T17:21:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6322/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6322"
  },
  {
    "number": 7112,
    "title": "[Bug]: Cannot use FlashAttention-2 backend because the vllm_flash_attn package is not found. But I have installed vllm-flash-attn.",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\n/app/apps/anaconda3/envs/vllm_040p1/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.16) or char\r\ndet (5.2.0)/charset_normalizer (2.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\r\nWARNING 08-03 23:03:20 _custom_ops.py:15] Failed to import from vllm._C with ImportError('libcudart.so.12: cannot open shared object file: No \r\nsuch file or directory')\r\nPyTorch version: 2.4.0+cu118\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.8\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.12 (main, Apr  5 2022, 06:56:58)  [GCC 7.5.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-148-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 11.8.89\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-80GB\r\nNvidia driver version: 470.161.03\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-11.4/targets/x86_64-linux/lib/libcudnn.so.8.2.4\r\n/usr/local/cuda-11.4/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.2.4\r\n/usr/local/cuda-11.4/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.2.4\r\n/usr/local/cuda-11.4/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.2.4\r\n/usr/local/cuda-11.4/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.2.4\r\n/usr/local/cuda-11.4/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.2.4\r\n/usr/local/cuda-11.4/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.2.4\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   46 bits physical, 48 bits virtual\r\nCPU(s):                          16\r\nOn-line CPU(s) list:             0-15\r\nThread(s) per core:              2\r\nCore(s) per socket:              8\r\nSocket(s):                       1\r\nNUMA node(s):                    1\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           106\r\nModel name:                      Intel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz\r\nStepping:                        6\r\nCPU MHz:                         2899.998\r\nBogoMIPS:                        5799.99\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       384 KiB\r\nL1i cache:                       256 KiB\r\nL2 cache:                        10 MiB\r\nL3 cache:                        48 MiB\r\nNUMA node0 CPU(s):               0-15\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht sysca\r\nll nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pdcm pcid s\r\nse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stib\r\np ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_\r\nni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd arat avx512vbmi avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx51\r\n2_vpopcntdq rdpid arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==3.8.2\r\n[pip3] flake8-bugbear==22.9.23\r\n[pip3] flake8-comprehensions==3.10.0\r\n[pip3] flake8-executable==2.1.2\r\n[pip3] flake8-pyi==20.5.0\r\n[pip3] mypy-extensions==0.4.3\r\n[pip3] numpy==1.21.5\r\n[pip3] numpydoc==1.2\r\n[pip3] nvidia-nccl-cu11==2.20.5\r\n[pip3] nvidia-nccl-cu12==2.18.1\r\n[pip3] pytorch-crf==0.7.2\r\n[pip3] sentence-transformers==2.2.2\r\n[pip3] torch==2.4.0+cu118\r\n[pip3] torchaudio==0.12.1+cu116\r\n[pip3] torchnet==0.0.4\r\n[pip3] torchstat==0.0.7\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchvision==0.18.1+cu118\r\n[pip3] transformers==4.42.4\r\n[pip3] transformers-stream-generator==0.0.4\r\n[pip3] triton==3.0.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2021.4.0           h06a4308_640  \r\n[conda] mkl-service               2.4.0            py39h7f8727e_0  \r\n[conda] mkl_fft                   1.3.1            py39hd3c417c_0  \r\n[conda] mkl_random                1.2.2            py39h51133e4_0  \r\n[conda] numpy                     1.21.5           py39he7a7128_1  \r\n[conda] numpy-base                1.21.5           py39hf524024_1  \r\n[conda] numpydoc                  1.2                pyhd3eb1b0_0  \r\n[conda] nvidia-nccl-cu11          2.20.5                   pypi_0    pypi \r\n[conda] nvidia-nccl-cu12          2.18.1                   pypi_0    pypi \r\n[conda] pytorch-crf               0.7.2                    pypi_0    pypi\r\n[conda] sentence-transformers     2.2.2                    pypi_0    pypi\r\n[conda] torch                     2.4.0+cu118              pypi_0    pypi \r\n[conda] torchaudio                0.12.1+cu116             pypi_0    pypi\r\n[conda] torchnet                  0.0.4                    pypi_0    pypi \r\n[conda] torchstat                 0.0.7                    pypi_0    pypi\r\n[conda] torchsummary              1.5.1                    pypi_0    pypi \r\n[conda] torchvision               0.18.1+cu118             pypi_0    pypi\r\n[conda] transformers              4.42.4                   pypi_0    pypi \r\n[conda] transformers-stream-generator 0.0.4                    pypi_0    pypi \r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect \r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.3.post1 \r\nvLLM Build Flags: \r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled \r\nGPU Topology:\r\nGPU0\tCPU Affinity\tNUMA Affinity\r\nGPU0\t X \t0-15\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\n```\r\nINFO 08-03 22:48:53 selector.py:189] Cannot use FlashAttention-2 backend because the vllm_flash_attn package is not found. `pip install vllm-flash-attn` for better performance.\r\nINFO 08-03 22:48:53 selector.py:54] Using XFormers backend.\r\n```\r\n\r\nBut I have installed vllm-flash-attn\uff1a\r\n```\r\nvllm-flash-attn    2.6.1\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-08-03T15:07:46+00:00",
    "closed_at": "2024-09-16T13:54:15+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7112/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7112"
  },
  {
    "number": 15571,
    "title": "[Feature]: Output the JSON for the response payload when VLLM_LOGGING_LEVEL=DEBUG",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nIt's difficult to debug VLLM requests and responses when we as developers can't see the JSON request and response payloads in debug mode. This is especially important when using VLLM as an inference server with MCP or an agentic framework.  Differences in how the models format their function-calling and tool-invocation responses can cause problems, so we need to be able to see exactly what the response was.\n\nWhen VLLM_LOGGING_LEVEL=DEBUG I am currently not able to see this information. For example, here's what I see in the logs.  The request payload is there even with the logging level set to INFO, so that's good.  However, the response payload is not there even if we set the logging level to DEBUG.  All we get back is `\"POST /v1/chat/completions HTTP/1.1\" 200 OK`:\n\n```\nINFO 03-05 19:47:39 logger.py:37] Received request chatcmpl-xxxx: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nEnvironment: ipython\\nCutting Knowledge Date: December 2023\\nToday Date: 05 Mar 2025\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nGiven the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\\n\\nRespond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. Do not use variables.\\n\\n{\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"getforecast\",\\n        \"description\": \"Get real time weather forecast for a location\",\\n        \"parameters\": {\\n            \"type\": \"object\",\\n            \"required\": [\\n                \"latitude\",\\n                \"longitude\"\\n            ],\\n            \"properties\": {\\n                \"latitude\": {\\n                    \"type\": \"string\",\\n                    \"description\": \"Latitude  of the location\"\\n                },\\n                \"longitude\": {\\n                    \"type\": \"string\",\\n                    \"description\": \"Longitude of the location\"\\n                }\\n            }\\n        }\\n    }\\n}\\n\\nBased on the weather this week in New York, what\\'s the best day to walk around the city?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.9, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\nINFO:     x.x.x.x:xxxxx - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n```\n\nThis should only show up in the logs if the log level is DEBUG.\n\nIt's important to handle streaming responses. When the response is a streaming response, it would be acceptable to concatenate the various chunks into one entry in the logs. Or we could have a debug log entry for each individual chunk if that's the only reasonable way to implement it.\n\n### Alternatives\n\nMany of us have resorted to adding a proxy layer between the application and VLLM just to log the request and response.  Unfortunately, sometimes this proxy layer is changing things like timing and streaming behavior, so adding the proxy occasionally fixes problems in our apps/agents or causes new problems.  This means adding a proxy for debugging isn't a good solution. \n\n### Additional context\n\nThis is most important for function calling and tool calling.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-03-26T20:19:33+00:00",
    "closed_at": "2025-03-27T17:49:40+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15571/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15571"
  },
  {
    "number": 2386,
    "title": "Failed when load a merged Mistral 8x7b model",
    "body": "  I merged a mistal 8x7b model with the lora adapter, and I save the .pt with torch.save(model.state_dict(), 'path_to_model.pt')\r\n\r\nHowever, when I use vllm to inference on the new merged model, I failed with this:\r\n\r\n```\r\nFile \"/home/zhh/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/entrypoints/llm.py\", line 93, in __init__\r\n    self.llm_engine = LLMEngine.from_engine_args(engine_args)\r\n  File \"/home/zhh/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/llm_engine.py\", line 246, in from_engine_args\r\n    engine = cls(*engine_configs,\r\n  File \"/home/zhh/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/llm_engine.py\", line 107, in __init__\r\n    self._init_workers_ray(placement_group)\r\n  File \"/home/zhh/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/llm_engine.py\", line 194, in _init_workers_ray\r\n    self._run_workers(\r\n  File \"/home/zhh/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/llm_engine.py\", line 750, in _run_workers\r\n    self._run_workers_in_batch(workers, method, *args, **kwargs))\r\n  File \"/home/zhh/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/llm_engine.py\", line 727, in _run_workers_in_batch\r\n    all_outputs = ray.get(all_outputs)\r\n  File \"/home/zhh/miniconda3/envs/vllm/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/zhh/miniconda3/envs/vllm/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/zhh/miniconda3/envs/vllm/lib/python3.9/site-packages/ray/_private/worker.py\", line 2624, in get\r\n    raise value.as_instanceof_cause()\r\nray.exceptions.RayTaskError(KeyError): ray::RayWorkerVllm.execute_method() (pid=2596933, ip=192.254.110.7, actor_id=afac0d35c8217a762419a5cc01000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7efd70ee22e0>)\r\n  File \"/home/zhh/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/ray_utils.py\", line 32, in execute_method\r\n    return executor(*args, **kwargs)\r\n  File \"/home/zhh/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/worker/worker.py\", line 72, in load_model\r\n    self.model_runner.load_model()\r\n  File \"/home/zhh/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/worker/model_runner.py\", line 36, in load_model\r\n    self.model = get_model(self.model_config)\r\n  File \"/home/zhh/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/model_loader.py\", line 124, in get_model\r\n    model.load_weights(model_config.model, model_config.download_dir,\r\n  File \"/home/zhh/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/models/mixtral.py\", line 525, in load_weights\r\n    param = params_dict[name]\r\nKeyError: 'model.embed_tokens.weight'\r\n```",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-01-09T06:09:22+00:00",
    "closed_at": "2024-11-30T02:03:06+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2386/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2386"
  },
  {
    "number": 15744,
    "title": "[Bug]: client socket has timed out while trying to connect to GPU node, when initializing DeepSeek R1 in ray vllm serving",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI construct a ray cluster and try to deploy several DeepSeek-R1 replicas. pipeline-parallel-size: 3\nOften, the model initialization would fail (not every time, could turn succeed after several retries)\nI'm on ray[serve]==2.44.0, vllm==0.8.2. This issue starts ever since 0.7.0. I've verified that it works well on 0.6.6.post1, but every version after that would possible to trigger below error msg when initializing multi-node models, (in our case is R1)\n\nError:\n\n```\n:job_id:02000000\n:actor_name:ServeReplica:DS-R1:vllmDeployment\nINFO 2025-03-29 06:17:53,742 DS-R1_vllmDeployment a3e34qi7 -- Starting with engine args: AsyncEngineArgs(model='DeepSeek-R1', served_model_name=None, tokenizer='DeepSeek-R1', hf_config_path=None, task='auto', skip_tokenizer_init=False, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', seed=None, max_model_len=16384, distributed_executor_backend='ray', pipeline_parallel_size=3, tensor_parallel_size=8, enable_expert_parallel=False, max_parallel_loading_workers=None, block_size=None, enable_prefix_caching=True, disable_sliding_window=False, disable_cascade_attn=False, use_v2_block_manager=True, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, revision=None, code_revision=None, rope_scaling=None, rope_theta=None, hf_overrides=None, tokenizer_revision=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, fully_sharded_loras=False, lora_extra_vocab_size=256, long_lora_scaling_factors=None, lora_dtype='auto', max_cpu_loras=None, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, ray_workers_use_nsight=False, num_gpu_blocks_override=None, num_lookahead_slots=0, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, scheduler_delay_factor=0.0, enable_chunked_prefill=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, speculative_config=None, speculative_model=None, speculative_model_quantization=None, speculative_draft_tensor_parallel_size=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, worker_cls='auto', worker_extension_cls='', kv_transfer_config=None, generation_config='auto', override_generation_config=None, enable_sleep_mode=False, model_impl='auto', calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, use_tqdm_on_load=True, disable_log_requests=False)\n[E329 06:28:12.542939754 socket.cpp:1023] [c10d] The client socket has timed out after 600000ms while trying to connect to (10.xxx.xxx.33, 54485).\n[W329 06:28:12.581551416 TCPStore.cpp:330] [c10d] TCP client failed to connect/validate to host 10.xxx.xxx.33:54485 - retrying (try=0, timeout=600000ms, delay=84662ms): The client socket has timed out after 600000ms while trying to connect to (10.xxx.xxx.33, 54485).\nException raised from throwTimeoutError at /pytorch/torch/csrc/distributed/c10d/socket.cpp:1025 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x74f4a7f6c1b6 in /conda/envs/vllm/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x16144fe (0x74c4ec5584fe in /conda/envs/vllm/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\nframe #2: <unknown function> + 0x63501ce (0x74c4f12941ce in /conda/envs/vllm/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\nframe #3: <unknown function> + 0x6350386 (0x74c4f1294386 in /conda/envs/vllm/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\nframe #4: <unknown function> + 0x63507f4 (0x74c4f12947f4 in /conda/envs/vllm/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\nframe #5: <unknown function> + 0x630d216 (0x74c4f1251216 in /conda/envs/vllm/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\nframe #6: c10d::TCPStore::TCPStore(std::string, c10d::TCPStoreOptions const&) + 0x20c (0x74c4f125414c in /conda/envs/vllm/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0xe402df (0x74f44b8cf2df in /conda/envs/vllm/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\nframe #8: <unknown function> + 0x518d37 (0x74f44afa7d37 in /conda/envs/vllm/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\nframe #9: ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata() [0x4fdcf7]\nframe #10: _PyObject_MakeTpCall + 0x25b (0x4f747b in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #11: ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata() [0x509d6f]\nframe #12: PyVectorcall_Call + 0xb9 (0x50a909 in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #13: ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata() [0x507a4c]\nframe #14: ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata() [0x4f77e6]\nframe #15: <unknown function> + 0x51752b (0x74f44afa652b in /conda/envs/vllm/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\nframe #16: _PyObject_MakeTpCall + 0x25b (0x4f747b in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #17: _PyEval_EvalFrameDefault + 0x56d2 (0x4f3802 in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #18: _PyFunction_Vectorcall + 0x6f (0x4fe13f in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #19: _PyEval_EvalFrameDefault + 0x31f (0x4ee44f in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #20: ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata() [0x572387]\nframe #21: ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata() [0x4fe324]\nframe #22: _PyEval_EvalFrameDefault + 0x31f (0x4ee44f in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #23: _PyFunction_Vectorcall + 0x6f (0x4fe13f in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #24: PyObject_Call + 0xb8 (0x50a5a8 in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #25: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0ca9 in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #26: _PyFunction_Vectorcall + 0x6f (0x4fe13f in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #27: PyObject_Call + 0xb8 (0x50a5a8 in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #28: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0ca9 in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #29: _PyFunction_Vectorcall + 0x6f (0x4fe13f in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #30: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4e3 in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #31: _PyFunction_Vectorcall + 0x6f (0x4fe13f in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #32: _PyEval_EvalFrameDefault + 0x31f (0x4ee44f in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #33: _PyFunction_Vectorcall + 0x6f (0x4fe13f in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #34: _PyEval_EvalFrameDefault + 0x31f (0x4ee44f in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #35: _PyFunction_Vectorcall + 0x6f (0x4fe13f in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #36: _PyEval_EvalFrameDefault + 0x731 (0x4ee861 in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #37: ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata() [0x509d07]\nframe #38: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0ca9 in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #39: _PyFunction_Vectorcall + 0x6f (0x4fe13f in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #40: _PyEval_EvalFrameDefault + 0x31f (0x4ee44f in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #41: ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata() [0x509bd6]\nframe #42: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0ca9 in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #43: _PyFunction_Vectorcall + 0x6f (0x4fe13f in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #44: _PyEval_EvalFrameDefault + 0x731 (0x4ee861 in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #45: _PyFunction_Vectorcall + 0x6f (0x4fe13f in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #46: _PyEval_EvalFrameDefault + 0x731 (0x4ee861 in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #47: _PyFunction_Vectorcall + 0x6f (0x4fe13f in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #48: _PyEval_EvalFrameDefault + 0x731 (0x4ee861 in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #49: ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata() [0x509a7e]\nframe #50: PyObject_Call + 0xb8 (0x50a5a8 in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #51: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0ca9 in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #52: _PyFunction_Vectorcall + 0x6f (0x4fe13f in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #53: _PyObject_FastCallDictTstate + 0x17d (0x4f687d in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #54: ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata() [0x5075b8]\nframe #55: _PyObject_MakeTpCall + 0x2ab (0x4f74cb in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #56: _PyEval_EvalFrameDefault + 0x56d2 (0x4f3802 in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #57: ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata() [0x509a7e]\nframe #58: PyObject_Call + 0xb8 (0x50a5a8 in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #59: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0ca9 in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #60: _PyFunction_Vectorcall + 0x6f (0x4fe13f in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #61: _PyObject_FastCallDictTstate + 0x17d (0x4f687d in ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata)\nframe #62: ray::ServeReplica:DS-R1:vllmDeployment.initialize_and_get_metadata() [0x5075b8]\n```\n\ncould be similar issue with https://github.com/vllm-project/vllm/issues/13052\n\nWhen the model loads well, it should look like\n```\n:job_id:02000000\n:actor_name:ServeReplica:DS-R1:vllmDeployment\nINFO 2025-03-29 06:17:53,742 DS-R1_vllmDeployment a3e34qi7 -- Starting with engine args: AsyncEngineArgs(model='DeepSeek-R1', served_model_name=None, tokenizer='DeepSeek-R1', hf_config_path=None, task='auto', skip_tokenizer_init=False, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', seed=None, max_model_len=16384, distributed_executor_backend='ray', pipeline_parallel_size=3, tensor_parallel_size=8, enable_expert_parallel=False, max_parallel_loading_workers=None, block_size=None, enable_prefix_caching=True, disable_sliding_window=False, disable_cascade_attn=False, use_v2_block_manager=True, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, revision=None, code_revision=None, rope_scaling=None, rope_theta=None, hf_overrides=None, tokenizer_revision=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, fully_sharded_loras=False, lora_extra_vocab_size=256, long_lora_scaling_factors=None, lora_dtype='auto', max_cpu_loras=None, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, ray_workers_use_nsight=False, num_gpu_blocks_override=None, num_lookahead_slots=0, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, scheduler_delay_factor=0.0, enable_chunked_prefill=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, speculative_config=None, speculative_model=None, speculative_model_quantization=None, speculative_draft_tensor_parallel_size=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, worker_cls='auto', worker_extension_cls='', kv_transfer_config=None, generation_config='auto', override_generation_config=None, enable_sleep_mode=False, model_impl='auto', calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, use_tqdm_on_load=True, disable_log_requests=False)\n Loading safetensors checkpoint shards: 0% Completed | 0/163 [00:00<?, ?it/s]\n Loading safetensors checkpoint shards: 1% Completed | 1/163 [00:00<01:28, 1.83it/s]\n Loading safetensors checkpoint shards: 1% Completed | 2/163 [00:16<25:20, 9.45s/it]\n Loading safetensors checkpoint shards: 2% Completed | 3/163 [00:16<14:02, 5.27s/it]\n Loading safetensors checkpoint shards: 2% Completed | 4/163 [00:29<21:32, 8.13s/it]\n Loading safetensors checkpoint shards: 3% Completed | 5/163 [00:34<18:39, 7.09s/it]\n Loading safetensors checkpoint shards: 4% Completed | 6/163 [00:34<12:35, 4.81s/it]\n Loading safetensors checkpoint shards: 4% Completed | 7/163 [00:34<08:37, 3.32s/it]\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "open",
    "created_at": "2025-03-29T07:02:08+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15744/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15744"
  },
  {
    "number": 20742,
    "title": "[Bug]: [V1 Engine] GLM4-1V video processing fails with token count mismatch: \"Attempted to assign X multimodal tokens to Y placeholders\"",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n+ cuda==12.8\n+ pip3 install vllm==0.9.2 \n\nexport CUDA_VISIBLE_DEVICES=\"0\"\nexport server_ip=0.0.0.0\nexport server_port=8000\nexport model_path=/workspace/data/GLM-4.1V-9B-Thinking\nexport server_name=glm-41v-9b\nexport VLLM_USE_V1=\"1\"\n\npython3 -m vllm.entrypoints.openai.api_server --model ${model_path} \\\n--max-num-seqs=32 \\\n--tensor-parallel-size=1 \\\n--gpu-memory-utilization=0.8 \\\n--no-enable-chunked-prefill \\\n--limit-mm-per-prompt video=1 \\\n--enable-prefix-caching \\\n--served-model-name=${server_name} \n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n### Description\n\n**Problem Summary:**\nWhen processing video inputs with GLM4-1V model using vLLM V1 engine, the system crashes with a ValueError indicating a mismatch between the number of multimodal tokens generated by the vision encoder and the number of placeholder tokens reserved during preprocessing.\n\n**Error Message:**\n```\nValueError: Attempted to assign 2029 multimodal tokens to 2024 placeholders\n```\n\n**Root Cause:**\nThe issue stems from inconsistent token count calculation between two critical phases:\n1. **Preprocessing phase** (`Glm4vProcessingInfo._get_vision_info()` \u2192 `get_video_replacement_glm4v()`): Estimates ~2400 tokens including special tokens and timestamp tokens\n2. **Execution phase** (`Glm4vVisionTransformer.forward()`): Actually generates 2029 pure vision tokens\n\n### Steps to Reproduce\n\n1. Set up vLLM V1 engine with GLM4-1V model\n2. Submit a video processing request with any video input\n3. Observe the crash during model execution phase\n\n**Environment:**\n- vLLM Version: [Latest with V1 engine enabled]\n- Model: GLM4-1V (zhipuai/glm-4v-9b)\n- Engine: V1 (VLLM_USE_V1=1)\n- Input: Video multimodal data\n\n### Expected Behavior\nVideo processing should complete successfully without token count mismatches.\n\n### Actual Behavior\nSystem crashes with ValueError during the `merge_multimodal_embeddings()` function call in `gpu_model_runner.py`.\n\n### Technical Analysis\n\n**Two-Phase Token Calculation Problem:**\n\n1. **Phase 1 (Preprocessing):** \n   - Location: `vllm/model_executor/models/glm4_1v.py` \u2192 `Glm4vProcessingInfo._get_vision_info()`\n   - Uses mathematical estimation: `padded_num_frames // temporal_patch_size` + special tokens\n   - Generates `PlaceholderRange` objects for memory allocation\n\n2. **Phase 2 (Execution):**\n   - Location: `Glm4vVisionTransformer.forward()`\n   - Performs actual vision encoding with different logic\n   - Produces different token count than estimated\n\n**Code Flow:**\n```\nUser Request \u2192 Processor.process_inputs() [Phase 1: Estimation] \n\u2192 Scheduler.schedule() [Uses estimated counts] \n\u2192 GPUModelRunner.execute_model() [Phase 2: Actual execution] \n\u2192 CRASH in merge_multimodal_embeddings()\n```\n\n### Proposed Solutions\n\n**Option 1: Fix Token Count Estimation (Recommended)**\nModify `Glm4vProcessingInfo._get_vision_info()` to use the same calculation logic as the actual vision transformer:\n- Call `_get_video_second_idx()` for actual frame sampling\n- Apply `smart_resize()` for each frame\n- Simulate the exact patch embedding and merging process\n\n**Option 2: Add Token Count Tolerance**\nModify `vllm/model_executor/models/utils.py` \u2192 `_merge_multimodal_embeddings()` to handle minor token count mismatches with padding/truncation.\n\n**Option 3: Lazy Token Calculation**\nImplement a two-stage approach where initial memory allocation is conservative, and exact token counts are calculated just before model execution.\n\n### Affected Components\n- `vllm/model_executor/models/glm4_1v.py`\n- `vllm/model_executor/models/utils.py` (merge_multimodal_embeddings)\n- `vllm/v1/worker/gpu_model_runner.py`\n- V1 scheduler and memory management\n\n### Impact\n- Blocks all video processing functionality for GLM4-1V in V1 engine\n- May affect other multimodal models with complex token generation patterns\n- Critical for production deployments using GLM4-1V video capabilities\n\n### Workarounds\n- Use V0 engine temporarily (`VLLM_USE_V1=0`)\n- Process videos as individual frame images instead of video inputs\n- Adjust video resolution/duration to find parameter ranges that don't trigger the bug\n\n### Additional Context\nThis issue appears to be specific to GLM4-1V's complex video token generation logic, which includes temporal patches, spatial merging, and additional special tokens for frame timestamps. The same inconsistency may exist in other models with similar multi-stage token generation processes.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-07-10T07:38:16+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20742/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20742"
  },
  {
    "number": 10150,
    "title": "[Bug]: AMD GPU tp>1 \u6a21\u578b\u4e0a\u7ebf\u5361\u4f4f",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0+rocm6.1\r\nIs debug build: False\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: 6.1.40091-a8dbc0c19\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.4\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-124-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: Radeon RX 7900 XTX (gfx1100)\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: 6.1.40091\r\nMIOpen runtime version: 3.1.0\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        43 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               192\r\nOn-line CPU(s) list:                  0-191\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD EPYC 7K62 48-Core Processor\r\nCPU family:                           23\r\nModel:                                49\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   48\r\nSocket(s):                            2\r\nStepping:                             0\r\nFrequency boost:                      enabled\r\nCPU max MHz:                          2600.0000\r\nCPU min MHz:                          1500.0000\r\nBogoMIPS:                             5200.00\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\r\nVirtualization:                       AMD-V\r\nL1d cache:                            3 MiB (96 instances)\r\nL1i cache:                            3 MiB (96 instances)\r\nL2 cache:                             48 MiB (96 instances)\r\nL3 cache:                             384 MiB (24 instances)\r\nNUMA node(s):                         2\r\nNUMA node0 CPU(s):                    0-47,96-143\r\nNUMA node1 CPU(s):                    48-95,144-191\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec rstack overflow:   Mitigation; safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] jj-pytorchvideo==0.1.5\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] onnxruntime==1.16.0\r\n[pip3] pytorch-lightning==2.4.0\r\n[pip3] pytorch-triton-rocm==3.0.0\r\n[pip3] pytorch-wpe==0.0.1\r\n[pip3] pyzmq==26.2.0\r\n[pip3] sentence-transformers==3.2.1\r\n[pip3] torch==2.4.0+rocm6.1\r\n[pip3] torch-complex==0.4.4\r\n[pip3] torchaudio==2.4.0+rocm6.1\r\n[pip3] torchmetrics==1.4.3\r\n[pip3] torchvision==0.19.0+rocm6.1\r\n[pip3] transformers==4.45.2\r\n[pip3] transformers-stream-generator==0.0.5\r\n[pip3] triton==3.1.0\r\n[pip3] vector-quantize-pytorch==1.18.1\r\n[conda] Could not collect\r\nROCM Version: 6.1.40092-038397aaa\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.4.dev8+ge9d517f2.d20241017\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\n============================ ROCm System Management Interface ============================\r\n================================ Weight between two GPUs =================================\r\n       GPU0         GPU1\r\nGPU0   0            72\r\nGPU1   72           0\r\n\r\n================================= Hops between two GPUs ==================================\r\n       GPU0         GPU1\r\nGPU0   0            3\r\nGPU1   3            0\r\n\r\n=============================== Link Type between two GPUs ===============================\r\n       GPU0         GPU1\r\nGPU0   0            PCIE\r\nGPU1   PCIE         0\r\n\r\n======================================= Numa Nodes =======================================\r\nGPU[0]          : (Topology) Numa Node: 0\r\nGPU[0]          : (Topology) Numa Affinity: 0\r\nGPU[1]          : (Topology) Numa Node: 1\r\nGPU[1]          : (Topology) Numa Affinity: 1\r\n================================== End of ROCm SMI Log ===================================\r\n\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\ntp>1\uff0c\u6a21\u578b\u4e0a\u7ebf\u5361\u4f4f\uff0c\u663e\u5b58\u5360\u7528\u4f46\u662f\u5229\u7528\u7387\u4e3a0\uff0c\r\nruijie:849:1185 [0] NCCL INFO Connected all rings comm 0x55a968d59160 nRanks 02 busId 3000\r\nruijie:849:1185 [0] NCCL INFO Connected all trees\r\nruijie:849:1185 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 256 | 256\r\nruijie:849:1185 [0] NCCL INFO 64 coll channels, 0 nvls channels, 64 p2p channels, 2 p2p channels per peer\r\nruijie:1122:1186 [1] NCCL INFO Connected all rings comm 0x557519a97630 nRanks 02 busId c3000\r\nruijie:1122:1186 [1] NCCL INFO Connected all trees\r\nruijie:1122:1186 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 256 | 256\r\nruijie:1122:1186 [1] NCCL INFO 64 coll channels, 0 nvls channels, 64 p2p channels, 2 p2p channels per peer\r\nruijie:849:1185 [0] NCCL INFO comm 0x55a968d59160 rank 0 nranks 2 cudaDev 0 busId 3000 commId 0xdf38d22419fae72b localSize 192 used 134323280 bytes - Init COMPLETE\r\nruijie:1122:1186 [1] NCCL INFO comm 0x557519a97630 rank 1 nranks 2 cudaDev 1 busId c3000 commId 0xdf38d22419fae72b localSize 192 used 134323280 bytes - Init COMPLETE\r\nruijie:1122:1663 [1] NCCL INFO Channel 32 : 1[c3000] -> 0[3000] via SHM/direct/direct comm 0x557519a97630 nRanks 02\r\nruijie:1122:1663 [1] NCCL INFO Channel 33 : 1[c3000] -> 0[3000] via SHM/direct/direct comm 0x557519a97630 nRanks 02\r\n2024-11-08 09:37:39,378 vllm.executor.distributed_gpu_executor 849 INFO     # GPU blocks: 31800, # CPU blocks: 9362\r\n2024-11-08 09:37:39,378 vllm.executor.distributed_gpu_executor 849 INFO     Maximum concurrency for 32768 tokens per request: 15.53x\r\n2024-11-08 09:37:40,924 vllm.worker.model_runner 849 INFO     Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n2024-11-08 09:37:40,925 vllm.worker.model_runner 849 INFO     CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(VllmWorkerProcess pid=1122) INFO 11-08 09:37:40 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n(VllmWorkerProcess pid=1122) INFO 11-08 09:37:40 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n\u662f\u5426\u9700\u8981\u8df3\u8fc7NCCL\uff1f\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-08T09:47:36+00:00",
    "closed_at": "2025-03-11T02:03:37+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10150/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10150"
  },
  {
    "number": 8990,
    "title": "[Bug]: MiniCPMV Raises UnboundLocalError When Image Placeholder is Omitted",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Red Hat Enterprise Linux release 8.9 (Ootpa) (x86_64)\r\nGCC version: (GCC) 8.5.0 20210514 (Red Hat 8.5.0-20)\r\nClang version: Could not collect\r\nCMake version: version 3.30.2\r\nLibc version: glibc-2.28\r\n\r\nPython version: 3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0] (64-bit runtime)\r\nPython platform: Linux-4.18.0-513.11.1.el8_9.x86_64-x86_64-with-glibc2.28\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-80GB\r\nNvidia driver version: 535.54.03\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              128\r\nOn-line CPU(s) list: 0-127\r\nThread(s) per core:  1\r\nCore(s) per socket:  64\r\nSocket(s):           2\r\nNUMA node(s):        2\r\nVendor ID:           AuthenticAMD\r\nCPU family:          25\r\nModel:               1\r\nModel name:          AMD EPYC 7763 64-Core Processor\r\nStepping:            1\r\nCPU MHz:             2850.418\r\nCPU max MHz:         3529.0520\r\nCPU min MHz:         1500.0000\r\nBogoMIPS:            4890.70\r\nVirtualization:      AMD-V\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            512K\r\nL3 cache:            32768K\r\nNUMA node0 CPU(s):   0-63\r\nNUMA node1 CPU(s):   64-127\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.11.1\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.555.43\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.20\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.1.0\r\n[pip3] sentence-transformers==3.0.1\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.1\r\n[pip3] transformers-stream-generator==0.0.5\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: dev\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      SYS     SYS     SYS     SYS     PXB     PXB     SYS     SYS     SYS     SYS     105     0-1             N/A\r\nNIC0    SYS      X      PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC1    SYS     PIX      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC2    SYS     SYS     SYS      X      PXB     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC3    SYS     SYS     SYS     PXB      X      SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC4    PXB     SYS     SYS     SYS     SYS      X      PXB     SYS     SYS     SYS     SYS\r\nNIC5    PXB     SYS     SYS     SYS     SYS     PXB      X      SYS     SYS     SYS     SYS\r\nNIC6    SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     SYS     SYS\r\nNIC7    SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      SYS     SYS\r\nNIC8    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PXB\r\nNIC9    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  NIC8: mlx5_8\r\n  NIC9: mlx5_9\r\n```\r\n\r\n</details>\r\n\r\n\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nThere's a small bug in `MiniCPM-V` if a prompt is provided, but the user mistakenly omits the image placeholder. Example:\r\n\r\n```python\r\nfrom vllm import LLM, SamplingParams\r\nfrom PIL import Image\r\n\r\nmodel_name = \"openbmb/MiniCPM-V-2_6\"\r\nllm = LLM(\r\n    model=model_name,\r\n    trust_remote_code=True,\r\n)\r\nsampling_params = SamplingParams()\r\n\r\n\r\nimg = Image.open(\"cherry_blossom.jpg\") \r\n\r\noutputs = llm.generate(\r\n    {\r\n        \"prompt\": \"I have no image tag\",\r\n        \"multi_modal_data\": {\"image\": img}\r\n    },\r\n    sampling_params=sampling_params\r\n)\r\n```\r\n\r\nraises `UnboundLocalError: local variable 'token_ids' referenced before assignment`, because the variable is only defined if the prompt is `None` ([here](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/minicpmv.py#L278)), but it's used if there are no image tags [here](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/minicpmv.py#L288).\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-10-01T06:16:15+00:00",
    "closed_at": "2024-10-01T09:52:45+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8990/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8990"
  },
  {
    "number": 8736,
    "title": "[Bug]: InternVl2-8B-AWQ gives error when trying to run with vllm-openai cuda 11.8 docker image",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nProcess SpawnProcess-1:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 319, in run_mp_engine\r\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 113, in from_engine_args\r\n    return cls(\r\n           ^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 69, in __init__\r\n    self.engine = LLMEngine(*args, **kwargs)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 317, in __init__\r\n    self.model_executor = executor_class(\r\n                          ^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 47, in __init__\r\n    self._init_executor()\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/gpu_executor.py\", line 40, in _init_executor\r\n    self.driver_worker.load_model()\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 183, in load_model\r\n    self.model_runner.load_model()\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1016, in load_model\r\n    self.model = get_model(model_config=self.model_config,\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 19, in get_model\r\n    return loader.load_model(model_config=model_config,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 363, in load_model\r\n    model.load_weights(\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/internvl.py\", line 538, in load_weights\r\n    self.language_model.load_weights(llm_weights)\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/internlm2.py\", line 380, in load_weights\r\n    param = params_dict[name]\r\n            ~~~~~~~~~~~^^^^^^\r\nKeyError: 'model.layers.23.attention.wqkv.qweight'\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 571, in <module>\r\n    uvloop.run(run_server(args))\r\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\r\n    return __asyncio.run(\r\n           ^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\r\n    return runner.run(main)\r\n           ^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\r\n    return self._loop.run_until_complete(task)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\r\n    return await main\r\n           ^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 538, in run_server\r\n    async with build_async_engine_client(args) as engine_client:\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\r\n    return await anext(self.gen)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 105, in build_async_engine_client\r\n    async with build_async_engine_client_from_engine_args(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\r\n    return await anext(self.gen)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 192, in build_async_engine_client_from_engine_args\r\n    raise RuntimeError(\r\nRuntimeError: Engine process failed to start\r\nGetting such a error when trying to run AWQ version of Internvl2.\r\nAlready checked that awq version is supported: https://github.com/vllm-project/vllm/pull/7187\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-09-23T10:16:32+00:00",
    "closed_at": "2024-09-23T10:28:46+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8736/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8736"
  },
  {
    "number": 16678,
    "title": "[RFC]: tool_calls and None types.",
    "body": "### Motivation.\n\n### Summary\nExploring tool_calls in VLLM.  Seems to be a need for additional handling of non-iterable None type responses in the tool_calls field.  \n\n### Motivation\n\nI've been testing with Google's new ADK (backed by LiteLLM) against a VLLM hosted Qwen 2.5 Instruct model and the Hermes parser.  (Using Kserve).  \n\nhttps://github.com/google/adk-python/blob/290058eb05211ef531b1752c6290da3f365e4e73/src/google/adk/models/lite_llm.py#L194\nADK explicitly returns a None in the tool_calls field.  \n` tool_calls=tool_calls or None,`\n\nFrom what I understand, some models and hosting do expect this None type, however VLLM does not.  This means that on the second message of the chat, you get the following:  \n`ERROR - fast_api.py:616 - Error in event_generator: litellm.BadRequestError: OpenAIException - 'NoneType' object is not iterable`\n\nThis seems to bring us to \nhttps://github.com/vllm-project/vllm/blob/54a66e5fee4a1ea62f1e4c79a078b20668e408c6/vllm/entrypoints/chat_utils.py#L1072\n\nin which the iteration takes place here.  \nhttps://github.com/vllm-project/vllm/blob/54a66e5fee4a1ea62f1e4c79a078b20668e408c6/vllm/entrypoints/chat_utils.py#L1097\n\nIt would be nice if VLLM could work cleanly with ADK and other tool calling solutions that make use of the None type.  The tool calls themselves seem to work well, this is an issue of follow up chats after no tool calls.  \n\n### Proposed Change.\n\nCurrently I am doing a conditional modification to the ADK LiteLLM, modifying the None to [] for models where appropriate, but it seems like it would be cleaner if VLLM could handle a None type in the tool_calls field.  \n\nhttps://github.com/vllm-project/vllm/blob/54a66e5fee4a1ea62f1e4c79a078b20668e408c6/vllm/entrypoints/chat_utils.py#L1097\nBecomes ->\n`if \"tool_calls\" in parsed_msg and parsed_msg[\"tool_calls\"] is not None:`\n\nI am fairly new to VLLM and want to make sure I'm not missing anything before a pull request goes in.  Are there any larger implications of this change that anyone can see?  If anyone has a more elegant solution that would also be appreciated.  \n\n### Feedback Period.\n\nA week seems fine.  Sanity check/alternate solution methods appreciated.  \n\n### CC List.\n\n_No response_\n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "closed",
    "created_at": "2025-04-15T18:51:52+00:00",
    "closed_at": "2025-04-22T15:40:25+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16678/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16678"
  },
  {
    "number": 9556,
    "title": "[Bug]:  \"address already in use\" while deploying pipeline parallel",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 24.04.1 LTS (x86_64)\r\nGCC version: (Ubuntu 13.2.0-23ubuntu4) 13.2.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.39\r\n\r\nPython version: 3.12.3 (main, Sep 11 2024, 14:17:37) [GCC 13.2.0] (64-bit runtime)\r\nIs CUDA available: True\r\nCUDA runtime version: 12.6.77\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA L40S\r\nGPU 1: NVIDIA L40S\r\nGPU 2: NVIDIA L40S\r\nGPU 3: NVIDIA L40S\r\nGPU 4: NVIDIA L40S\r\nGPU 5: NVIDIA L40S\r\nGPU 6: NVIDIA L40S\r\nGPU 7: NVIDIA L40S\r\n\r\nNvidia driver version: 560.35.03\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               192\r\nOn-line CPU(s) list:                  0-191\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD EPYC 7R13 Processor\r\nCPU family:                           25\r\nModel:                                1\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   48\r\nSocket(s):                            2\r\nStepping:                             1\r\nBogoMIPS:                             5299.99\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            3 MiB (96 instances)\r\nL1i cache:                            3 MiB (96 instances)\r\nL2 cache:                             48 MiB (96 instances)\r\nL3 cache:                             384 MiB (12 instances)\r\nNUMA node(s):                         2\r\nNUMA node0 CPU(s):                    0-47,96-143\r\nNUMA node1 CPU(s):                    48-95,144-191\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Vulnerable: Safe RET, no microcode\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.77\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.2\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.1.dev238+ge2c6e0a82\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\r\nGPU1    NODE     X      NODE    NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\r\nGPU2    NODE    NODE     X      NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\r\nGPU3    NODE    NODE    NODE     X      SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\r\nGPU4    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    48-95,144-191   1               N/A\r\nGPU5    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    48-95,144-191   1               N/A\r\nGPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    48-95,144-191   1               N/A\r\nGPU7    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      48-95,144-191   1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\n**the command:** \r\n\r\nvllm serve MODELPATH --dtype auto --api-key TOKEN --tensor-parallel-size 4 --pipeline-parallel-size 2  --gpu-memory-utilization 0.90 --enable_prefix_caching --max_model_len 8192\r\n\r\n**returns the error:**\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 8000): address already in use\r\nINFO:     Waiting for application shutdown.\r\nINFO:     Application shutdown complete.\r\n(VllmWorkerProcess pid=29772) INFO 10-21 15:37:38 multiproc_worker_utils.py:244] Worker exiting\r\n(VllmWorkerProcess pid=29774) INFO 10-21 15:37:38 multiproc_worker_utils.py:244] Worker exiting\r\n(VllmWorkerProcess pid=29771) INFO 10-21 15:37:38 multiproc_worker_utils.py:244] Worker exiting\r\n(VllmWorkerProcess pid=29773) INFO 10-21 15:37:38 multiproc_worker_utils.py:244] Worker exiting\r\n/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\nERROR 10-21 15:37:40 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 29772 died, exit code: 1\r\nINFO 10-21 15:37:40 multiproc_worker_utils.py:124] Killing local vLLM worker processes\r\n\r\n\r\n\r\n**When I'm not using pipeline parallel, there is no issue**\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-10-21T15:38:41+00:00",
    "closed_at": "2024-10-21T16:13:52+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9556/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9556"
  },
  {
    "number": 17906,
    "title": "[Installation]: cc1plus: error: invalid feature modifier in \u2018-march=armv8.2-a+dotprod+fp16\u2019",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\n```\n\n  -- CPU extension compile flags: -fopenmp;-DVLLM_CPU_EXTENSION;-march=armv8.2-a+dotprod+fp16\n      -- Enabling C extension.\n      -- Configuring done (3.5s)\n      -- Generating done (0.0s)\n      -- Build files have been written to: /tmp/pip-install-b261gxij/vllm_e99d65c51e5c4cea948384f47668e6c3/build/temp.linux-aarch64-cpython-310\n      [1/9] Building CXX object CMakeFiles/_C.dir/csrc/cpu/activation.cpp.o\n      FAILED: CMakeFiles/_C.dir/csrc/cpu/activation.cpp.o\n      /usr/bin/c++ -DPy_LIMITED_API=3 -DTORCH_EXTENSION_NAME=_C -DUSE_C10D_GLOO -DUSE_DISTRIBUTED -DUSE_RPC -DUSE_TENSORPIPE -D_C_EXPORTS -I/tmp/pip-install-b261gxij/vllm_e99d65c51e5c4cea948384f47668e6c3/csrc -isystem /home/ma-user/anaconda3/envs/py310/include/python3.10 -isystem /tmp/pip-build-env-mwllr031/overlay/lib/python3.10/site-packages/torch/include -isystem /tmp/pip-build-env-mwllr031/overlay/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -O2 -g -DNDEBUG -std=gnu++1z -fPIC -fopenmp -DVLLM_CPU_EXTENSION -march=armv8.2-a+dotprod+fp16 -D_GLIBCXX_USE_CXX11_ABI=1 -MD -MT CMakeFiles/_C.dir/csrc/cpu/activation.cpp.o -MF CMakeFiles/_C.dir/csrc/cpu/activation.cpp.o.d -o CMakeFiles/_C.dir/csrc/cpu/activation.cpp.o -c /tmp/pip-install-b261gxij/vllm_e99d65c51e5c4cea948384f47668e6c3/csrc/cpu/activation.cpp\n      cc1plus: error: invalid feature modifier in \u2018-march=armv8.2-a+dotprod+fp16\u2019\n      [2/9] Building CXX object CMakeFiles/_C.dir/csrc/cpu/cache.cpp.o\n      FAILED: CMakeFiles/_C.dir/csrc/cpu/cache.cpp.o\n      /usr/bin/c++ -DPy_LIMITED_API=3 -DTORCH_EXTENSION_NAME=_C -DUSE_C10D_GLOO -DUSE_DISTRIBUTED -DUSE_RPC -DUSE_TENSORPIPE -D_C_EXPORTS -I/tmp/pip-install-b261gxij/vllm_e99d65c51e5c4cea948384f47668e6c3/csrc -isystem /home/ma-user/anaconda3/envs/py310/include/python3.10 -isystem /tmp/pip-build-env-mwllr031/overlay/lib/python3.10/site-packages/torch/include -isystem /tmp/pip-build-env-mwllr031/overlay/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -O2 -g -DNDEBUG -std=gnu++1z -fPIC -fopenmp -DVLLM_CPU_EXTENSION -march=armv8.2-a+dotprod+fp16 -D_GLIBCXX_USE_CXX11_ABI=1 -MD -MT CMakeFiles/_C.dir/csrc/cpu/cache.cpp.o -MF CMakeFiles/_C.dir/csrc/cpu/cache.cpp.o.d -o CMakeFiles/_C.dir/csrc/cpu/cache.cpp.o -c /tmp/pip-install-b261gxij/vllm_e99d65c51e5c4cea948384f47668e6c3/csrc/cpu/cache.cpp\n      cc1plus: error: invalid feature modifier in \u2018-march=armv8.2-a+dotprod+fp16\u2019\n      [3/9] Building CXX object CMakeFiles/_C.dir/csrc/cpu/utils.cpp.o\n      FAILED: CMakeFiles/_C.dir/csrc/cpu/utils.cpp.o\n      /usr/bin/c++ -DPy_LIMITED_API=3 -DTORCH_EXTENSION_NAME=_C -DUSE_C10D_GLOO -DUSE_DISTRIBUTED -DUSE_RPC -DUSE_TENSORPIPE -D_C_EXPORTS -I/tmp/pip-install-b261gxij/vllm_e99d65c51e5c4cea948384f47668e6c3/csrc -isystem /home/ma-user/anaconda3/envs/py310/include/python3.10 -isystem /tmp/pip-build-env-mwllr031/overlay/lib/python3.10/site-packages/torch/include -isystem /tmp/pip-build-env-mwllr031/overlay/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -O2 -g -DNDEBUG -std=gnu++1z -fPIC -fopenmp -DVLLM_CPU_EXTENSION -march=armv8.2-a+dotprod+fp16 -D_GLIBCXX_USE_CXX11_ABI=1 -MD -MT CMakeFiles/_C.dir/csrc/cpu/utils.cpp.o -MF CMakeFiles/_C.dir/csrc/cpu/utils.cpp.o.d -o CMakeFiles/_C.dir/csrc/cpu/utils.cpp.o -c /tmp/pip-install-b261gxij/vllm_e99d65c51e5c4cea948384f47668e6c3/csrc/cpu/utils.cpp\n      cc1plus: error: invalid feature modifier in \u2018-march=armv8.2-a+dotprod+fp16\u2019\n      [4/9] Building CXX object CMakeFiles/_C.dir/csrc/cpu/mla_decode.cpp.o\n      FAILED: CMakeFiles/_C.dir/csrc/cpu/mla_decode.cpp.o\n      /usr/bin/c++ -DPy_LIMITED_API=3 -DTORCH_EXTENSION_NAME=_C -DUSE_C10D_GLOO -DUSE_DISTRIBUTED -DUSE_RPC -DUSE_TENSORPIPE -D_C_EXPORTS -I/tmp/pip-install-b261gxij/vllm_e99d65c51e5c4cea948384f47668e6c3/csrc -isystem /home/ma-user/anaconda3/envs/py310/include/python3.10 -isystem /tmp/pip-build-env-mwllr031/overlay/lib/python3.10/site-packages/torch/include -isystem /tmp/pip-build-env-mwllr031/overlay/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -O2 -g -DNDEBUG -std=gnu++1z -fPIC -fopenmp -DVLLM_CPU_EXTENSION -march=armv8.2-a+dotprod+fp16 -D_GLIBCXX_USE_CXX11_ABI=1 -MD -MT CMakeFiles/_C.dir/csrc/cpu/mla_decode.cpp.o -MF CMakeFiles/_C.dir/csrc/cpu/mla_decode.cpp.o.d -o CMakeFiles/_C.dir/csrc/cpu/mla_decode.cpp.o -c /tmp/pip-install-b261gxij/vllm_e99d65c51e5c4cea948384f47668e6c3/csrc/cpu/mla_decode.cpp\n      cc1plus: error: invalid feature modifier in \u2018-march=armv8.2-a+dotprod+fp16\u2019\n      [5/9] Building CXX object CMakeFiles/_C.dir/csrc/cpu/attention.cpp.o\n      FAILED: CMakeFiles/_C.dir/csrc/cpu/attention.cpp.o\n      /usr/bin/c++ -DPy_LIMITED_API=3 -DTORCH_EXTENSION_NAME=_C -DUSE_C10D_GLOO -DUSE_DISTRIBUTED -DUSE_RPC -DUSE_TENSORPIPE -D_C_EXPORTS -I/tmp/pip-install-b261gxij/vllm_e99d65c51e5c4cea948384f47668e6c3/csrc -isystem /home/ma-user/anaconda3/envs/py310/include/python3.10 -isystem /tmp/pip-build-env-mwllr031/overlay/lib/python3.10/site-packages/torch/include -isystem /tmp/pip-build-env-mwllr031/overlay/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -O2 -g -DNDEBUG -std=gnu++1z -fPIC -fopenmp -DVLLM_CPU_EXTENSION -march=armv8.2-a+dotprod+fp16 -D_GLIBCXX_USE_CXX11_ABI=1 -MD -MT CMakeFiles/_C.dir/csrc/cpu/attention.cpp.o -MF CMakeFiles/_C.dir/csrc/cpu/attention.cpp.o.d -o CMakeFiles/_C.dir/csrc/cpu/attention.cpp.o -c /tmp/pip-install-b261gxij/vllm_e99d65c51e5c4cea948384f47668e6c3/csrc/cpu/attention.cpp\n      cc1plus: error: invalid feature modifier in \u2018-march=armv8.2-a+dotprod+fp16\u2019\n      [6/9] Building CXX object CMakeFiles/_C.dir/csrc/cpu/torch_bindings.cpp.o\n      FAILED: CMakeFiles/_C.dir/csrc/cpu/torch_bindings.cpp.o\n      /usr/bin/c++ -DPy_LIMITED_API=3 -DTORCH_EXTENSION_NAME=_C -DUSE_C10D_GLOO -DUSE_DISTRIBUTED -DUSE_RPC -DUSE_TENSORPIPE -D_C_EXPORTS -I/tmp/pip-install-b261gxij/vllm_e99d65c51e5c4cea948384f47668e6c3/csrc -isystem /home/ma-user/anaconda3/envs/py310/include/python3.10 -isystem /tmp/pip-build-env-mwllr031/overlay/lib/python3.10/site-packages/torch/include -isystem /tmp/pip-build-env-mwllr031/overlay/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -O2 -g -DNDEBUG -std=gnu++1z -fPIC -fopenmp -DVLLM_CPU_EXTENSION -march=armv8.2-a+dotprod+fp16 -D_GLIBCXX_USE_CXX11_ABI=1 -MD -MT CMakeFiles/_C.dir/csrc/cpu/torch_bindings.cpp.o -MF CMakeFiles/_C.dir/csrc/cpu/torch_bindings.cpp.o.d -o CMakeFiles/_C.dir/csrc/cpu/torch_bindings.cpp.o -c /tmp/pip-install-b261gxij/vllm_e99d65c51e5c4cea948384f47668e6c3/csrc/cpu/torch_bindings.cpp\n      cc1plus: error: invalid feature modifier in \u2018-march=armv8.2-a+dotprod+fp16\u2019\n      [7/9] Building CXX object CMakeFiles/_C.dir/csrc/cpu/pos_encoding.cpp.o\n      FAILED: CMakeFiles/_C.dir/csrc/cpu/pos_encoding.cpp.o\n      /usr/bin/c++ -DPy_LIMITED_API=3 -DTORCH_EXTENSION_NAME=_C -DUSE_C10D_GLOO -DUSE_DISTRIBUTED -DUSE_RPC -DUSE_TENSORPIPE -D_C_EXPORTS -I/tmp/pip-install-b261gxij/vllm_e99d65c51e5c4cea948384f47668e6c3/csrc -isystem /home/ma-user/anaconda3/envs/py310/include/python3.10 -isystem /tmp/pip-build-env-mwllr031/overlay/lib/python3.10/site-packages/torch/include -isystem /tmp/pip-build-env-mwllr031/overlay/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -O2 -g -DNDEBUG -std=gnu++1z -fPIC -fopenmp -DVLLM_CPU_EXTENSION -march=armv8.2-a+dotprod+fp16 -D_GLIBCXX_USE_CXX11_ABI=1 -MD -MT CMakeFiles/_C.dir/csrc/cpu/pos_encoding.cpp.o -MF CMakeFiles/_C.dir/csrc/cpu/pos_encoding.cpp.o.d -o CMakeFiles/_C.dir/csrc/cpu/pos_encoding.cpp.o -c /tmp/pip-install-b261gxij/vllm_e99d65c51e5c4cea948384f47668e6c3/csrc/cpu/pos_encoding.cpp\n      cc1plus: error: invalid feature modifier in \u2018-march=armv8.2-a+dotprod+fp16\u2019\n      [8/9] Building CXX object CMakeFiles/_C.dir/csrc/cpu/layernorm.cpp.o\n      FAILED: CMakeFiles/_C.dir/csrc/cpu/layernorm.cpp.o\n      /usr/bin/c++ -DPy_LIMITED_API=3 -DTORCH_EXTENSION_NAME=_C -DUSE_C10D_GLOO -DUSE_DISTRIBUTED -DUSE_RPC -DUSE_TENSORPIPE -D_C_EXPORTS -I/tmp/pip-install-b261gxij/vllm_e99d65c51e5c4cea948384f47668e6c3/csrc -isystem /home/ma-user/anaconda3/envs/py310/include/python3.10 -isystem /tmp/pip-build-env-mwllr031/overlay/lib/python3.10/site-packages/torch/include -isystem /tmp/pip-build-env-mwllr031/overlay/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -O2 -g -DNDEBUG -std=gnu++1z -fPIC -fopenmp -DVLLM_CPU_EXTENSION -march=armv8.2-a+dotprod+fp16 -D_GLIBCXX_USE_CXX11_ABI=1 -MD -MT CMakeFiles/_C.dir/csrc/cpu/layernorm.cpp.o -MF CMakeFiles/_C.dir/csrc/cpu/layernorm.cpp.o.d -o CMakeFiles/_C.dir/csrc/cpu/layernorm.cpp.o -c /tmp/pip-install-b261gxij/vllm_e99d65c51e5c4cea948384f47668e6c3/csrc/cpu/layernorm.cpp\n      cc1plus: error: invalid feature modifier in \u2018-march=armv8.2-a+dotprod+fp16\u2019\n      ninja: build stopped: subcommand failed.\n      Traceback (most recent call last):\n\n### How you are installing vllm\n\n```sh\npip install -vvv vllm\n```\npip install vllm\npackages/vllm/0.8.5.post1/vllm-0.8.5.post1.tar.gz\n\nARMv8\u5e73\u53f0\u5b89\u88c5\u62a5\u9519\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "open",
    "created_at": "2025-05-09T14:23:47+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17906/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17906"
  },
  {
    "number": 12168,
    "title": "[Bug]: Unable to serve Qwen2-audio in V1",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 01-17 22:19:48 __init__.py:179] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu118\nIs debug build: False\nCUDA used to build PyTorch: 11.8\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 18.04.6 LTS (x86_64)\nGCC version: (Ubuntu 10.3.0-1ubuntu1~18.04~1) 10.3.0\nClang version: Could not collect\nCMake version: version 3.31.2\nLibc version: glibc-2.27\n\nPython version: 3.12.8 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:31:09) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.4.0-169-generic-x86_64-with-glibc2.27\nIs CUDA available: True\nCUDA runtime version: 11.7.99\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA A100-SXM4-80GB\nGPU 1: NVIDIA A100-SXM4-80GB\nGPU 2: NVIDIA A100-SXM4-80GB\nGPU 3: NVIDIA A100-SXM4-80GB\n\nNvidia driver version: 535.129.03\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.5.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.5.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.5.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.5.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.5.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.5.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.5.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:        x86_64\nCPU op-mode(s):      32-bit, 64-bit\nByte Order:          Little Endian\nCPU(s):              128\nOn-line CPU(s) list: 0-127\nThread(s) per core:  1\nCore(s) per socket:  64\nSocket(s):           2\nNUMA node(s):        4\nVendor ID:           AuthenticAMD\nCPU family:          25\nModel:               1\nModel name:          AMD EPYC 7763 64-Core Processor\nStepping:            1\nCPU MHz:             2635.266\nCPU max MHz:         2450.0000\nCPU min MHz:         1500.0000\nBogoMIPS:            4890.87\nVirtualization:      AMD-V\nL1d cache:           32K\nL1i cache:           32K\nL2 cache:            512K\nL3 cache:            32768K\nNUMA node0 CPU(s):   0-31\nNUMA node1 CPU(s):   32-63\nNUMA node2 CPU(s):   64-95\nNUMA node3 CPU(s):   96-127\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca sme sev sev_es\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.3\n[pip3] nvidia-cublas-cu11==11.11.3.6\n[pip3] nvidia-cuda-cupti-cu11==11.8.87\n[pip3] nvidia-cuda-nvrtc-cu11==11.8.89\n[pip3] nvidia-cuda-runtime-cu11==11.8.89\n[pip3] nvidia-cudnn-cu11==9.1.0.70\n[pip3] nvidia-cufft-cu11==10.9.0.58\n[pip3] nvidia-curand-cu11==10.3.0.86\n[pip3] nvidia-cusolver-cu11==11.4.1.48\n[pip3] nvidia-cusparse-cu11==11.7.5.86\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu11==2.21.5\n[pip3] nvidia-nvtx-cu11==11.8.86\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.5.1+cu118\n[pip3] torchaudio==2.5.1+cu118\n[pip3] torchvision==0.20.1+cu118\n[pip3] transformers==4.48.0\n[pip3] triton==3.1.0\n[conda] numpy                     1.26.3                   pypi_0    pypi\n[conda] nvidia-cublas-cu11        11.11.3.6                pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu11    11.8.87                  pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu11    11.8.89                  pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu11  11.8.89                  pypi_0    pypi\n[conda] nvidia-cudnn-cu11         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu11         10.9.0.58                pypi_0    pypi\n[conda] nvidia-curand-cu11        10.3.0.86                pypi_0    pypi\n[conda] nvidia-cusolver-cu11      11.4.1.48                pypi_0    pypi\n[conda] nvidia-cusparse-cu11      11.7.5.86                pypi_0    pypi\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\n[conda] nvidia-nccl-cu11          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvtx-cu11          11.8.86                  pypi_0    pypi\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\n[conda] torch                     2.5.1+cu118              pypi_0    pypi\n[conda] torchaudio                2.5.1+cu118              pypi_0    pypi\n[conda] torchvision               0.20.1+cu118             pypi_0    pypi\n[conda] transformers              4.48.0                   pypi_0    pypi\n[conda] triton                    3.1.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.6.6.post2.dev249+gb8bfa46a\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    NIC0    NIC1    NIC2    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV12    NV12    NV12    SYS     PXB     SYS     0-31    0               N/A\nGPU1    NV12     X      NV12    NV12    SYS     PXB     SYS     0-31    0               N/A\nGPU2    NV12    NV12     X      NV12    SYS     SYS     PXB     96-127  3               N/A\nGPU3    NV12    NV12    NV12     X      SYS     SYS     PXB     96-127  3               N/A\nNIC0    SYS     SYS     SYS     SYS      X      SYS     SYS\nNIC1    PXB     PXB     SYS     SYS     SYS      X      SYS\nNIC2    SYS     SYS     PXB     PXB     SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n\nLD_LIBRARY_PATH=/xxx/.conda/envs/vllm_v1/lib/python3.12/site-packages/cv2/../../lib64:/xxx/.local/bin:/usr/local/cuda-11.7/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNVIDIA_VISIBLE_DEVICES=GPU-dad83af5-de81-eafa-7fd4-ff1b5e460e6e,GPU-2fd3e2ae-f180-a811-3484-2ec565c2d55c,GPU-0d90c382-438d-0572-8fc0-751f6d5fcc69,GPU-77f75057-c7a9-a82f-5a47-3b17a4bc973e\nNVIDIA_PRODUCT_NAME=CUDA\nNCCL_VERSION=2.13.4-1\nNVIDIA_CUDA_END_OF_LIFE=1\nPYTORCH_VERSION=v2.0.0\nCUDA_VERSION=11.7.0\nNVIDIA_DRIVER_CAPABILITIES=video,compute,utility,graphics\nNVIDIA_REQUIRE_CUDA=cuda>=11.7 brand=tesla,driver>=450,driver<451 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=510,driver<511 brand=unknown,driver>=510,driver<511 brand=nvidia,driver>=510,driver<511 brand=nvidiartx,driver>=510,driver<511 brand=quadro,driver>=510,driver<511 brand=quadrortx,driver>=510,driver<511 brand=titan,driver>=510,driver<511 brand=titanrtx,driver>=510,driver<511 brand=geforce,driver>=510,driver<511 brand=geforcertx,driver>=510,driver<511\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nFailed to serve Qwen2-audio with  V1 engine (would like to enable prefix caching):\n\n`VLLM_TRACE_FUNCTION=1 NCCL_DEBUG=TRACE VLLM_LOGGING_LEVEL=DEBUG VLLM_USE_V1=1 VLLM_ENABLE_V1_MULTIPROCESSING=1 vllm serve /xxx/omni/Qwen2-Audio/Qwen2-Audio-7B-Instruct --limit_mm_per_prompt 'audio=5'`\n\nTraceback:\n\n```python\nINFO 01-17 22:16:00 __init__.py:179] Automatically detected platform cuda.                        \nINFO 01-17 22:16:03 api_server.py:768] vLLM API server version 0.6.6.post2.dev249+gb8bfa46a       \nINFO 01-17 22:16:03 api_server.py:769] args: Namespace(subparser='serve', model_tag='/xxx/omni/Qwen2-Audio/Qwen2-Audio-7B-Instruct', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/xxx/omni/Qwen2-Audio/Qwen2-Audio-7B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt={'audio': 5}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7fde8a988b80>)                                                                       \nWARNING 01-17 22:16:03 arg_utils.py:1283] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.                  \nINFO 01-17 22:16:21 config.py:520] This model supports multiple tasks: {'embed', 'score', 'classify', 'reward', 'generate'}. Defaulting to 'generate'.\nINFO 01-17 22:16:21 config.py:1482] Chunked prefill is enabled with max_num_batched_tokens=2048.                                                                                                                                                                   [33/1875]\nINFO 01-17 22:16:35 __init__.py:179] Automatically detected platform cuda.                                                                                                                                                                                                  \nINFO 01-17 22:16:38 core.py:45] Initializing an LLM engine (v0.6.6.post2.dev249+gb8bfa46a) with config: model='/xxx/omni/Qwen2-Audio/Qwen2-Audio-7B-Instruct', speculative_config=None, tokenizer='/xxx/omni/Qwen2-Audio/Qwen2-Audio-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/xxx/omni/Qwen2-Audio/Qwen2-Audio-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"candidate_compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"compile_sizes\":[],\"capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}                                                                                                                                   \nINFO 01-17 22:16:41 gpu_model_runner.py:688] Starting to load model /xxx/omni/Qwen2-Audio/Qwen2-Audio-7B-Instruct...                                                                                                                                            \nINFO 01-17 22:16:42 cuda.py:179] Using Flash Attention backend on V1 engine.                                                                                                                                                                                                \nWARNING 01-17 22:16:42 topk_topp_sampler.py:44] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.                                                              \nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]                                                                                                                                                                                                \nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:02,  1.52it/s]                                                                                                                                                                                        \nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:02,  1.36it/s]                                                                                                                                                                                        \nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:02<00:01,  1.18it/s]                                                                                                                                                                                        \nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:03<00:00,  1.17it/s]                                                                                                                                                                                        \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.45it/s]                                                                                                                                                                                        \nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.35it/s]                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                            \nINFO 01-17 22:16:46 gpu_model_runner.py:693] Loading model weights took 15.6454 GB                                                                                                                                                                                          \nINFO 01-17 22:16:46 gpu_model_runner.py:767] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 3 audio items of the maximum feature size.                                                                                                   \nERROR 01-17 22:16:46 core.py:205] EngineCore hit an exception: Traceback (most recent call last):                                                                                                                                                                           \nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/inputs/registry.py\", line 160, in call_hf_processor                                                                                                                                            \nERROR 01-17 22:16:46 core.py:205]     return hf_processor(**data, **merged_kwargs, return_tensors=\"pt\")                                                                                                                                                                     \nERROR 01-17 22:16:46 core.py:205]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                     \nERROR 01-17 22:16:46 core.py:205]   File \"/home/yyy/.conda/envs/vllm_v1/lib/python3.12/site-packages/transformers/models/qwen2_audio/processing_qwen2_audio.py\", line 115, in __call__                                                                                \nERROR 01-17 22:16:46 core.py:205]     num_audios = 1 if type(audios) == np.ndarray else len(audios)                                                                                                                                                                         \nERROR 01-17 22:16:46 core.py:205]                                                       ^^^^^^^^^^^                                                                                                                                                                         \nERROR 01-17 22:16:46 core.py:205] TypeError: object of type 'NoneType' has no len()                                                                                                                                                                                         \nERROR 01-17 22:16:46 core.py:205]                                                                                                                                                                                                                                           \nERROR 01-17 22:16:46 core.py:205] The above exception was the direct cause of the following exception:                                                                                                                                                                      \nERROR 01-17 22:16:46 core.py:205] \nERROR 01-17 22:16:46 core.py:205] Traceback (most recent call last):\nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/v1/engine/core.py\", line 197, in run_engine_core\nERROR 01-17 22:16:46 core.py:205]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 01-17 22:16:46 core.py:205]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/v1/engine/core.py\", line 151, in __init__\nERROR 01-17 22:16:46 core.py:205]     super().__init__(vllm_config, executor_class)\nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/v1/engine/core.py\", line 52, in __init__\nERROR 01-17 22:16:46 core.py:205]     num_gpu_blocks, num_cpu_blocks = self._initialize_kv_caches(\nERROR 01-17 22:16:46 core.py:205]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/v1/engine/core.py\", line 77, in _initialize_kv_caches\nERROR 01-17 22:16:46 core.py:205]     availble_gpu_memory = self.model_executor.determine_available_memory()\nERROR 01-17 22:16:46 core.py:205]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/v1/executor/uniproc_executor.py\", line 57, in determine_available_memory\nERROR 01-17 22:16:46 core.py:205]     return self.worker.determine_available_memory()\nERROR 01-17 22:16:46 core.py:205]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-17 22:16:46 core.py:205]   File \"/home/yyy/.conda/envs/vllm_v1/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 01-17 22:16:46 core.py:205]     return func(*args, **kwargs)\nERROR 01-17 22:16:46 core.py:205]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/v1/worker/gpu_worker.py\", line 134, in determine_available_memory\nERROR 01-17 22:16:46 core.py:205]     self.model_runner.profile_run()\nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/v1/worker/gpu_model_runner.py\", line 773, in profile_run\nERROR 01-17 22:16:46 core.py:205]     dummy_request_data = self.input_registry.dummy_data_for_profiling(\nERROR 01-17 22:16:46 core.py:205]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/inputs/registry.py\", line 333, in dummy_data_for_profiling\nERROR 01-17 22:16:46 core.py:205]     dummy_data = profiler.get_dummy_data(seq_len)\nERROR 01-17 22:16:46 core.py:205]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/multimodal/profiling.py\", line 161, in get_dummy_data\nERROR 01-17 22:16:46 core.py:205]     mm_inputs = self._get_dummy_mm_inputs(seq_len, mm_counts)\nERROR 01-17 22:16:46 core.py:205]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/multimodal/profiling.py\", line 139, in _get_dummy_mm_inputs\nERROR 01-17 22:16:46 core.py:205]     return self.processor.apply(\nERROR 01-17 22:16:46 core.py:205]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/multimodal/processing.py\", line 1104, in apply\nERROR 01-17 22:16:46 core.py:205]     prompt_ids, mm_kwargs = self._cached_apply_hf_processor(\nERROR 01-17 22:16:46 core.py:205]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/multimodal/processing.py\", line 880, in _cached_apply_hf_processor\nERROR 01-17 22:16:46 core.py:205]     prompt_ids, mm_missing_kwargs = self._apply_hf_processor_main(\nERROR 01-17 22:16:46 core.py:205]                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/multimodal/processing.py\", line 826, in _apply_hf_processor_main\nERROR 01-17 22:16:46 core.py:205]     prompt_ids = self._apply_hf_processor_text_only(prompt)\nERROR 01-17 22:16:46 core.py:205]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/multimodal/processing.py\", line 753, in _apply_hf_processor_text_only\nERROR 01-17 22:16:46 core.py:205]     prompt_ids, _ = self._apply_hf_processor_text_mm(\nERROR 01-17 22:16:46 core.py:205]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/multimodal/processing.py\", line 729, in _apply_hf_processor_text_mm\nERROR 01-17 22:16:46 core.py:205]     processed_data = self._call_hf_processor(\nERROR 01-17 22:16:46 core.py:205]                      ^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/model_executor/models/qwen2_audio.py\", line 171, in _call_hf_processor\nERROR 01-17 22:16:46 core.py:205]     processed_outputs = super()._call_hf_processor(\nERROR 01-17 22:16:46 core.py:205]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/multimodal/processing.py\", line 711, in _call_hf_processor\nERROR 01-17 22:16:46 core.py:205]     return self.info.ctx.call_hf_processor(\nERROR 01-17 22:16:46 core.py:205]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-17 22:16:46 core.py:205]   File \"/xxx/code/vllm_v1/vllm/inputs/registry.py\", line 165, in call_hf_processor\nERROR 01-17 22:16:46 core.py:205]     raise RuntimeError(msg) from exc\nERROR 01-17 22:16:46 core.py:205] RuntimeError: Failed to apply Qwen2AudioProcessor on data={'text': '<|AUDIO|><|AUDIO|><|AUDIO|><|AUDIO|><|AUDIO|>'} with kwargs={}\nERROR 01-17 22:16:46 core.py:205]\nCRITICAL 01-17 22:16:46 core_client.py:146] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n```\n\ncommit id=87a0c076afafb93dd082ff3876bea08adca56c56\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-01-17T14:30:27+00:00",
    "closed_at": "2025-01-19T03:16:35+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12168/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12168"
  },
  {
    "number": 10831,
    "title": "[Bug]: ERROR hermes_tool_parser.py:108] Error in extracting tool call from response.",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.12.7 (main, Oct  1 2024, 08:52:12) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-125-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA RTX A6000\r\nGPU 1: NVIDIA RTX A6000\r\nGPU 2: NVIDIA RTX A6000\r\nGPU 3: NVIDIA RTX A6000\r\n\r\nNvidia driver version: 535.183.01\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        52 bits physical, 57 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               48\r\nOn-line CPU(s) list:                  0-47\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Xeon(R) Silver 4410Y\r\nCPU family:                           6\r\nModel:                                143\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   12\r\nSocket(s):                            2\r\nStepping:                             8\r\nCPU max MHz:                          3900.0000\r\nCPU min MHz:                          800.0000\r\nBogoMIPS:                             4000.00\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nL1d cache:                            1.1 MiB (24 instances)\r\nL1i cache:                            768 KiB (24 instances)\r\nL2 cache:                             48 MiB (24 instances)\r\nL3 cache:                             60 MiB (2 instances)\r\nNUMA node(s):                         2\r\nNUMA node0 CPU(s):                    0-11,24-35\r\nNUMA node1 CPU(s):                    12-23,36-47\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.6+cu121torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.46.2\r\n[pip3] triton==3.1.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.4.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NODE    NODE    NODE    12-23,36-47     1               N/A\r\nGPU1    NODE     X      NODE    NODE    12-23,36-47     1               N/A\r\nGPU2    NODE    NODE     X      NODE    12-23,36-47     1               N/A\r\nGPU3    NODE    NODE    NODE     X      12-23,36-47     1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNVIDIA_VISIBLE_DEVICES=all\r\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\r\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\r\nVLLM_USAGE_SOURCE=production-docker-image\r\nCUDA_VERSION=12.4.1\r\nLD_LIBRARY_PATH=/usr/local/lib/python3.12/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nCUDA_MODULE_LOADING=LAZY```\r\n```\r\n</details>\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nThis error occurred when I used the Qwen2.5-72B-AWQ model to make tool calls (with `stream=False`)\r\n```\r\nERROR 12-02 05:59:06 hermes_tool_parser.py:108] Error in extracting tool call from response.\r\nERROR 12-02 05:59:06 hermes_tool_parser.py:108] Traceback (most recent call last):\r\nERROR 12-02 05:59:06 hermes_tool_parser.py:108]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py\", line 87, in extract_tool_calls\r\nERROR 12-02 05:59:06 hermes_tool_parser.py:108]     json.loads(match[0] if match[0] else match[1])\r\nERROR 12-02 05:59:06 hermes_tool_parser.py:108]   File \"/usr/lib/python3.12/json/__init__.py\", line 346, in loads\r\nERROR 12-02 05:59:06 hermes_tool_parser.py:108]     return _default_decoder.decode(s)\r\nERROR 12-02 05:59:06 hermes_tool_parser.py:108]            ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-02 05:59:06 hermes_tool_parser.py:108]   File \"/usr/lib/python3.12/json/decoder.py\", line 337, in decode\r\nERROR 12-02 05:59:06 hermes_tool_parser.py:108]     obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\nERROR 12-02 05:59:06 hermes_tool_parser.py:108]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-02 05:59:06 hermes_tool_parser.py:108]   File \"/usr/lib/python3.12/json/decoder.py\", line 355, in raw_decode\r\nERROR 12-02 05:59:06 hermes_tool_parser.py:108]     raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\nERROR 12-02 05:59:06 hermes_tool_parser.py:108] json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n```\r\n\r\nThen the output will become like this:\r\n```\r\n\u73fe\u5728\u6211\u5c07\u67e5\u8a62 DB \u4e2d\u7684\u6240\u6709 controller \u4fe1\u606f\u3002\r\n<tool_call>\r\n{\"name\": \"text_to_sql\", \"arguments\": {\"retrieve_steps\": \"\u67e5\u5168\u90e8\u7684controller\", \"columns\": \"*\", \"analysis_method\": \"summary the data and get max\", \"db_name\": \"21\", \"plot\": false, \"return_sql\": false}}}\r\n</tool_call>\r\n```\r\n\r\nHere is how I start vllm service:\r\n```\r\ndocker run --runtime nvidia --gpus='\"device=4,5,6,7\"' \\\r\n--mount type=bind,source=/home/llm,target=/llm \\\r\n-p 7415:8000 \\\r\n--ipc=host \\\r\nvllm/vllm-openai:v0.6.4.post1  \\\r\n--model /llm/Qwen2.5-72B-Instruct-AWQ \\\r\n--tensor-parallel-size 4 \\\r\n--gpu-memory-utilization 0.95 \\\r\n--max-model-len 128000 \\\r\n--enforce-eager \\\r\n--disable_custom_all_reduce \\\r\n--enable-auto-tool-choice \\\r\n--tool-call-parser hermes  \\\r\n--chat-template /llm/qwen.jinja\r\n```\r\n\r\nStrangely, this only happens in `v0.6.4.post1` and works fine in `v0.6.3.post1`\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-02T11:55:32+00:00",
    "closed_at": "2025-04-04T02:05:36+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10831/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10831"
  },
  {
    "number": 2310,
    "title": "Runtime Error When Running the Throughput Benchmarks WIth 2000 Requests (Invalid Token Generated) (Model: OPT-2.7B, Dataset: SharedGPT)",
    "body": "When running the throughput benchmark on the OPT-2.7B model on the SharedGPT dataset (2000 requests), a None token is generated, incurring a runtime error within the de-tokenizer. The ID of the problematic token is 50265 while the vocab size of the OPT tokenizer is only 50265. \r\n\r\nReproduce the bug:\r\n```bash\r\npython benchmark_throughput.py --backend vllm --dataset ../ShareGPT_V3_unfiltered_cleaned_split.json --model facebook/opt-2.7b --num-prompts 2000\r\n``` \r\n\r\nThe logging output:\r\n```bash\r\nNamespace(backend='vllm', dataset='../ShareGPT_V3_unfiltered_cleaned_split.json', dtype='auto', enforce_eager=False, hf_max_batch_size=None, input_len=None, max_model_len=None, model='facebook/opt-2.7b', n=1, num_prompts=2000, output_len=None, quantization=None, seed=0, tensor_parallel_size=1, tokenizer='facebook/opt-2.7b', trust_remote_code=False, use_beam_search=False)\r\nINFO 12-30 22:13:01 llm_engine.py:73] Initializing an LLM engine with config: model='facebook/opt-2.7b', tokenizer='facebook/opt-2.7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\r\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\nTo disable this warning, you can either:\r\n\t- Avoid using `tokenizers` before the fork if possible\r\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\nTo disable this warning, you can either:\r\n\t- Avoid using `tokenizers` before the fork if possible\r\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\nTo disable this warning, you can either:\r\n\t- Avoid using `tokenizers` before the fork if possible\r\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\nTo disable this warning, you can either:\r\n\t- Avoid using `tokenizers` before the fork if possible\r\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\nINFO 12-30 22:13:09 llm_engine.py:227] # GPU blocks: 3154, # CPU blocks: 819\r\nINFO 12-30 22:13:11 model_runner.py:403] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 12-30 22:13:11 model_runner.py:407] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.\r\nINFO 12-30 22:13:14 model_runner.py:449] Graph capturing finished in 4 secs.\r\nProcessed prompts:  54%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                           | 1078/2000 [02:11<01:32,  9.96it/s]Traceback (most recent call last):\r\n  File \"benchmark_throughput.py\", line 318, in <module>\r\n    main(args)\r\n  File \"benchmark_throughput.py\", line 205, in main\r\n    elapsed_time = run_vllm(requests, args.model, args.tokenizer,\r\n  File \"benchmark_throughput.py\", line 107, in run_vllm\r\n    llm._run_engine(use_tqdm=True)\r\n  File \"/home/jingjichen/vllm/vllm/entrypoints/llm.py\", line 185, in _run_engine\r\n    step_outputs = self.llm_engine.step()\r\n  File \"/home/jingjichen/vllm/vllm/engine/llm_engine.py\", line 589, in step\r\n    return self._process_model_outputs(output, scheduler_outputs)\r\n  File \"/home/jingjichen/vllm/vllm/engine/llm_engine.py\", line 551, in _process_model_outputs\r\n    self._process_sequence_group_outputs(seq_group, outputs)\r\n  File \"/home/jingjichen/vllm/vllm/engine/llm_engine.py\", line 422, in _process_sequence_group_outputs\r\n    self._decode_sequence(seq, seq_group.sampling_params)\r\n  File \"/home/jingjichen/vllm/vllm/engine/llm_engine.py\", line 667, in _decode_sequence\r\n    read_offset) = detokenize_incrementally(\r\n  File \"/home/jingjichen/vllm/vllm/transformers_utils/tokenizer.py\", line 141, in detokenize_incrementally\r\n    new_text = tokenizer.convert_tokens_to_string(\r\n  File \"/shared_ssd_storage/jingjichen/anaconda3/envs/llmenv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 612, in convert_tokens_to_string\r\n    return self.backend_tokenizer.decoder.decode(tokens)\r\nTypeError: argument 'tokens': 'NoneType' object cannot be converted to 'PyString'\r\nProcessed prompts:  54%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                           | 1078/2000 [02:11<01:52,  8.21it/s]\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-12-31T03:30:09+00:00",
    "closed_at": "2024-03-28T13:31:24+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2310/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2310"
  },
  {
    "number": 15182,
    "title": "[Bug]:  the issue of \"cuda out of memory\" arises",
    "body": "### Your current environment\n\n### current environment\nUse 8 A800 80 GPU cards, report the following error: Deepseek is using the Q4 gguf model.\n### command: \npython3 api_server.py --host 0.0.0.0 --port 7803 --model /data/models/DeepSeek-R1-Q2/ --served-model-name deepseek-r1 --max-model-len 8192 --enable-reasoning --reasoning-parser deepseek_r1 --gpu-memory-utilization 0.9 --tensor-parallel-size 8 --trust-remote-code\n\n(VllmWorker rank=0 pid=3013) INFO 03-20 03:14:59 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_5030d6c2'), local_subscribe_addr='ipc:///tmp/54feae1f-3d13-452d-9e9e-1bf1fffe271e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n(VllmWorker rank=1 pid=3024) INFO 03-20 03:14:59 [parallel_state.py:967] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1\n(VllmWorker rank=6 pid=3085) INFO 03-20 03:14:59 [parallel_state.py:967] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6\n(VllmWorker rank=1 pid=3024) INFO 03-20 03:14:59 [cuda.py:186] Using Triton MLA backend on V1 engine.\n(VllmWorker rank=0 pid=3013) INFO 03-20 03:14:59 [parallel_state.py:967] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0\n(VllmWorker rank=6 pid=3085) INFO 03-20 03:14:59 [cuda.py:186] Using Triton MLA backend on V1 engine.\n(VllmWorker rank=5 pid=3073) INFO 03-20 03:14:59 [parallel_state.py:967] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5\n(VllmWorker rank=0 pid=3013) INFO 03-20 03:14:59 [cuda.py:186] Using Triton MLA backend on V1 engine.\n(VllmWorker rank=7 pid=3101) INFO 03-20 03:14:59 [parallel_state.py:967] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7\n(VllmWorker rank=3 pid=3049) INFO 03-20 03:14:59 [parallel_state.py:967] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3\n(VllmWorker rank=5 pid=3073) INFO 03-20 03:14:59 [cuda.py:186] Using Triton MLA backend on V1 engine.\n(VllmWorker rank=7 pid=3101) INFO 03-20 03:14:59 [cuda.py:186] Using Triton MLA backend on V1 engine.\n(VllmWorker rank=2 pid=3037) INFO 03-20 03:14:59 [parallel_state.py:967] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2\n(VllmWorker rank=3 pid=3049) INFO 03-20 03:14:59 [cuda.py:186] Using Triton MLA backend on V1 engine.\n(VllmWorker rank=6 pid=3085) WARNING 03-20 03:14:59 [triton_decode_attention.py:44] The following error message 'operation scheduled before its operands' can be ignored.\n(VllmWorker rank=1 pid=3024) WARNING 03-20 03:14:59 [triton_decode_attention.py:44] The following error message 'operation scheduled before its operands' can be ignored.\n(VllmWorker rank=2 pid=3037) INFO 03-20 03:14:59 [cuda.py:186] Using Triton MLA backend on V1 engine.\n(VllmWorker rank=5 pid=3073) WARNING 03-20 03:14:59 [triton_decode_attention.py:44] The following error message 'operation scheduled before its operands' can be ignored.\n(VllmWorker rank=0 pid=3013) WARNING 03-20 03:14:59 [triton_decode_attention.py:44] The following error message 'operation scheduled before its operands' can be ignored.\n(VllmWorker rank=2 pid=3037) WARNING 03-20 03:14:59 [triton_decode_attention.py:44] The following error message 'operation scheduled before its operands' can be ignored.\n(VllmWorker rank=7 pid=3101) WARNING 03-20 03:14:59 [triton_decode_attention.py:44] The following error message 'operation scheduled before its operands' can be ignored.\n(VllmWorker rank=3 pid=3049) WARNING 03-20 03:14:59 [triton_decode_attention.py:44] The following error message 'operation scheduled before its operands' can be ignored.\n(VllmWorker rank=4 pid=3061) INFO 03-20 03:14:59 [parallel_state.py:967] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4\n(VllmWorker rank=4 pid=3061) INFO 03-20 03:14:59 [cuda.py:186] Using Triton MLA backend on V1 engine.\n(VllmWorker rank=4 pid=3061) WARNING 03-20 03:14:59 [triton_decode_attention.py:44] The following error message 'operation scheduled before its operands' can be ignored.\n(VllmWorker rank=5 pid=3073) INFO 03-20 03:14:59 [gpu_model_runner.py:1164] Starting to load model /data/models/DeepSeek-R1-Q2/...\n(VllmWorker rank=6 pid=3085) INFO 03-20 03:14:59 [gpu_model_runner.py:1164] Starting to load model /data/models/DeepSeek-R1-Q2/...\n(VllmWorker rank=1 pid=3024) INFO 03-20 03:14:59 [gpu_model_runner.py:1164] Starting to load model /data/models/DeepSeek-R1-Q2/...\n(VllmWorker rank=4 pid=3061) INFO 03-20 03:14:59 [gpu_model_runner.py:1164] Starting to load model /data/models/DeepSeek-R1-Q2/...\n(VllmWorker rank=3 pid=3049) INFO 03-20 03:14:59 [gpu_model_runner.py:1164] Starting to load model /data/models/DeepSeek-R1-Q2/...\n(VllmWorker rank=2 pid=3037) INFO 03-20 03:14:59 [gpu_model_runner.py:1164] Starting to load model /data/models/DeepSeek-R1-Q2/...\n(VllmWorker rank=0 pid=3013) INFO 03-20 03:14:59 [gpu_model_runner.py:1164] Starting to load model /data/models/DeepSeek-R1-Q2/...\n(VllmWorker rank=7 pid=3101) INFO 03-20 03:14:59 [gpu_model_runner.py:1164] Starting to load model /data/models/DeepSeek-R1-Q2/...\n(VllmWorker rank=4 pid=3061) WARNING 03-20 03:15:06 [config.py:3670] `torch.compile` is turned on, but the model /data/models/DeepSeek-R1-Q2/ does not support it. Please open an issue on GitHub if you want it to be supported.\n(VllmWorker rank=5 pid=3073) WARNING 03-20 03:15:06 [config.py:3670] `torch.compile` is turned on, but the model /data/models/DeepSeek-R1-Q2/ does not support it. Please open an issue on GitHub if you want it to be supported.\n(VllmWorker rank=0 pid=3013) WARNING 03-20 03:15:06 [config.py:3670] `torch.compile` is turned on, but the model /data/models/DeepSeek-R1-Q2/ does not support it. Please open an issue on GitHub if you want it to be supported.\n(VllmWorker rank=5 pid=3073) Process ForkProcess-1:6:\nCRITICAL 03-20 03:15:06 [multiproc_executor.py:48] MulitprocExecutor got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\nCRITICAL 03-20 03:15:06 [core_client.py:269] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n(VllmWorker rank=5 pid=3073) Traceback (most recent call last):\n(VllmWorker rank=5 pid=3073)   File \"/root/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n(VllmWorker rank=5 pid=3073)     self.run()\n(VllmWorker rank=5 pid=3073)   File \"/root/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py\", line 108, in run\n(VllmWorker rank=5 pid=3073)     self._target(*self._args, **self._kwargs)\n(VllmWorker rank=5 pid=3073)   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 311, in worker_main\n(VllmWorker rank=5 pid=3073)     worker = WorkerProc(*args, **kwargs)\n(VllmWorker rank=5 pid=3073)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=5 pid=3073)   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 244, in __init__\n(VllmWorker rank=5 pid=3073)     self.worker.load_model()\n(VllmWorker rank=5 pid=3073)   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 136, in load_model\n(VllmWorker rank=5 pid=3073)     self.model_runner.load_model()\n(VllmWorker rank=5 pid=3073)   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1167, in load_model\n(VllmWorker rank=5 pid=3073)     self.model = get_model(vllm_config=self.vllm_config)\n(VllmWorker rank=5 pid=3073)                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=5 pid=3073)   File \"/opt/venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n(VllmWorker rank=5 pid=3073)     return loader.load_model(vllm_config=vllm_config)\n(VllmWorker rank=5 pid=3073)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=5 pid=3073)   File \"/opt/venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py\", line 423, in load_model\n(VllmWorker rank=5 pid=3073)     model = _initialize_model(vllm_config=vllm_config)\n(VllmWorker rank=5 pid=3073)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=5 pid=3073)   File \"/opt/venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py\", line 126, in _initialize_model\n(VllmWorker rank=5 pid=3073)     return model_class(vllm_config=vllm_config, prefix=prefix)\n(VllmWorker rank=5 pid=3073)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=5 pid=3073)   File \"/opt/venv/lib/python3.12/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 665, in __init__\n(VllmWorker rank=5 pid=3073)     self.model = DeepseekV2Model(vllm_config=vllm_config,\n(VllmWorker rank=5 pid=3073)                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=5 pid=3073)   File \"/opt/venv/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n(VllmWorker rank=5 pid=3073)     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n(VllmWorker rank=5 pid=3073)   File \"/opt/venv/lib/python3.12/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 604, in __init__\n(VllmWorker rank=5 pid=3073)     self.start_layer, self.end_layer, self.layers = make_layers(\n(VllmWorker rank=5 pid=3073)                                                     ^^^^^^^^^^^^\n(VllmWorker rank=5 pid=3073)   File \"/opt/venv/lib/python3.12/site-packages/vllm/model_executor/models/utils.py\", line 558, in make_layers\n(VllmWorker rank=5 pid=3073)     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n(VllmWorker rank=5 pid=3073)                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=5 pid=3073)   File \"/opt/venv/lib/python3.12/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 606, in <lambda>\n(VllmWorker rank=5 pid=3073)     lambda prefix: DeepseekV2DecoderLayer(\n(VllmWorker rank=5 pid=3073)                    ^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=5 pid=3073)   File \"/opt/venv/lib/python3.12/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 527, in __init__\n(VllmWorker rank=5 pid=3073)     self.mlp = DeepseekV2MoE(\n(VllmWorker rank=5 pid=3073)                ^^^^^^^^^^^^^^\n(VllmWorker rank=5 pid=3073)   File \"/opt/venv/lib/python3.12/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 124, in __init__\n(VllmWorker rank=5 pid=3073)     self.experts = FusedMoE(\n(VllmWorker rank=5 pid=3073)                    ^^^^^^^^^\n(VllmWorker rank=5 pid=3073)   File \"/opt/venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 437, in __init__\n(VllmWorker rank=5 pid=3073)     self.quant_method.create_weights(layer=self, **moe_quant_params)\n(VllmWorker rank=5 pid=3073)   File \"/opt/venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 90, in create_weights\n(VllmWorker rank=5 pid=3073)     w2_weight = torch.nn.Parameter(torch.empty(\n(VllmWorker rank=5 pid=3073)                                    ^^^^^^^^^^^^\n(VllmWorker rank=5 pid=3073)   File \"/opt/venv/lib/python3.12/site-packages/torch/utils/_device.py\", line 104, in __torch_function__\n(VllmWorker rank=5 pid=3073)     return func(*args, **kwargs)\n(VllmWorker rank=5 pid=3073)            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=5 pid=3073) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 5 has a total capacity of 79.14 GiB of which 72.75 MiB is free. Process 1001250 has 79.06 GiB memory in use. Of the allocated memory 78.43 GiB is allocated by PyTorch, and 6.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nKilled\n\n\n### \ud83d\udc1b Describe the bug\n\nNo matter if using gguf Q2 or Q4, as soon as they are launched, the issue of \"cuda out of memory\" arises. No matter how we modify --max-model-len and --gpu-memory-utilization, it doesn't help.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-03-20T03:31:44+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15182/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15182"
  },
  {
    "number": 8613,
    "title": "[Bug]: vllm deploy qwen1.5-14b/qwen2-7b+medusa, RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x5120 and 4096x4096)",
    "body": "### Your current environment\r\nvllm=0.6.1\r\n\r\n### Model Input Dumps\r\n\r\nCUDA_VISIBLE_DEVICES=7 python3 -m vllm.entrypoints.openai.api_server --port 8010 \\\r\n  --served-model-name qwen2-7b \\\r\n  --model /mnt/user/deploy/qwen15_14b_finetuning_chatbot_v1_0914_deploy --dtype auto -tp 1 \\\r\n  --max-model-len 4096 --gpu-memory-utilization 0.9 \\\r\n  --max-num-seqs 1 \\\r\n  --speculative-model /mnt/user/deploy/qwen15_14b_finetuning_chatbot_v1_0914_deploy/medusa \\\r\n  --speculative-draft-tensor-parallel-size 1 \\\r\n  --num-speculative-tokens 3 \\\r\n  --speculative-disable-by-batch-size 3 \\\r\n  --use-v2-block-manager \\\r\n  --spec-decoding-acceptance-method typical_acceptance_sampler\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nERROR 09-19 10:53:38 async_llm_engine.py:63] Engine background task failed\r\nERROR 09-19 10:53:38 async_llm_engine.py:63] Traceback (most recent call last):\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 53, in _log_task_completion\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     return_value = task.result()\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 939, in run_engine_loop\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     result = task.result()\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 868, in engine_step\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     request_outputs = await self.engine.step_async(virtual_engine)\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 345, in step_async\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     outputs = await self.model_executor.execute_model_async(\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/executor/gpu_executor.py\", line 185, in execute_model_async\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     output = await make_async(self.driver_worker.execute_model\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     result = self.fn(*self.args, **self.kwargs)\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     return func(*args, **kwargs)\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/spec_decode/spec_decode_worker.py\", line 406, in execute_model\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     return self._run_speculative_decoding_step(execute_model_req,\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/contextlib.py\", line 79, in inner\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     return func(*args, **kwds)\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/spec_decode/spec_decode_worker.py\", line 572, in _run_speculative_decoding_step\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     proposals = self.proposer_worker.get_spec_proposals(\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/spec_decode/medusa_worker.py\", line 113, in get_spec_proposals\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     return self._proposer.get_spec_proposals(\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/spec_decode/top1_proposer.py\", line 77, in get_spec_proposals\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     maybe_sampler_output, transposed = self._worker.sampler_output(\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     return func(*args, **kwargs)\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/spec_decode/medusa_worker.py\", line 69, in sampler_output\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     model_outputs = self.model_runner.model.generate_proposals(\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/medusa.py\", line 148, in generate_proposals\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     hidden_states=self.forward(previous_hidden_states),\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/medusa.py\", line 82, in forward\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     return [block(hidden_states) for block in self.blocks]\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/medusa.py\", line 82, in <listcomp>\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     return [block(hidden_states) for block in self.blocks]\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     return self._call_impl(*args, **kwargs)\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     return forward_call(*args, **kwargs)\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/medusa.py\", line 28, in forward\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     x = x + self.act(layer(x))\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     return self._call_impl(*args, **kwargs)\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     return forward_call(*args, **kwargs)\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 117, in forward\r\nERROR 09-19 10:53:38 async_llm_engine.py:63]     return F.linear(input, self.weight, self.bias)\r\nERROR 09-19 10:53:38 async_llm_engine.py:63] **RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x5120 and 4096x4096)**\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-09-19T02:57:41+00:00",
    "closed_at": "2024-09-19T03:52:25+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8613/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8613"
  },
  {
    "number": 19741,
    "title": "[Bug]: Incorrect kernel selected when multiple GPUs",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nINFO 06-17 14:15:03 [__init__.py:243] Automatically detected platform cuda.\nWARNING 06-17 14:15:03 [cuda.py:435] Detected different devices in the system: NVIDIA GeForce RTX 5090, NVIDIA GeForce RTX 4090, NVIDIA GeForce RTX 3090. Please make sure to set `CUDA_DEVICE_ORDER=PCI_BUS_ID` to avoid unexpected behavior.\nCollecting environment information...\n/home/unat/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:287: UserWarning:\nNVIDIA GeForce RTX 5090 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\nThe current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_70 sm_75 sm_80 sm_86 sm_90.\nIf you want to use the NVIDIA GeForce RTX 5090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n\n  warnings.warn(\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 24.04.1 LTS (x86_64)\nGCC version                  : (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\nClang version                : Could not collect\nCMake version                : version 3.28.3\nLibc version                 : glibc-2.39\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu126\nIs debug build               : False\nCUDA used to build PyTorch   : 12.6\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0] (64-bit runtime)\nPython platform              : Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.0.140\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration :\nGPU 0: NVIDIA GeForce RTX 5090\nGPU 1: NVIDIA GeForce RTX 4090\nGPU 2: NVIDIA GeForce RTX 3090\n\nNvidia driver version        : 576.52\ncuDNN version                : Could not collect\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               24\nOn-line CPU(s) list:                  0-23\nVendor ID:                            AuthenticAMD\nModel name:                           AMD Ryzen 9 7900X 12-Core Processor\nCPU family:                           25\nModel:                                97\nThread(s) per core:                   2\nCore(s) per socket:                   12\nSocket(s):                            1\nStepping:                             2\nBogoMIPS:                             9399.76\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr arat npt nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm\nVirtualization:                       AMD-V\nHypervisor vendor:                    Microsoft\nVirtualization type:                  full\nL1d cache:                            384 KiB (12 instances)\nL1i cache:                            384 KiB (12 instances)\nL2 cache:                             12 MiB (12 instances)\nL3 cache:                             32 MiB (1 instance)\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] flashinfer-python==0.2.5+cu126torch2.6\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-cufile-cu12==1.11.1.6\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.0+cu126\n[pip3] torchaudio==2.7.0+cu126\n[pip3] torchvision==0.22.0+cu126\n[pip3] transformers==4.52.4\n[pip3] triton==3.3.0\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.1.dev132+g118ff9211 (git sha: 118ff9211)\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     SYS                             N/A\nGPU1    SYS      X      SYS                             N/A\nGPU2    SYS     SYS      X                              N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n==============================\n     Environment Variables\n==============================\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI'm running docker image with following params:\n\n```\n      docker run --name vllm-qwen3-30b --rm --gpus all --init\n      -e \"CUDA_VISIBLE_DEVICES=1,2\"\n      -e \"VLLM_ATTENTION_BACKEND=FLASH_ATTN\"\n      -e \"VLLM_USE_V1=0\"\n      -e \"CUDA_DEVICE_ORDER=PCI_BUS_ID\"\n      -v \"\\\\wsl$\\Ubuntu\\home\\unat\\vllm\\huggingface:/root/.cache/huggingface\"\n      -v \"\\\\wsl$\\Ubuntu\\home\\unat\\vllm\\cache:/root/.cache/vllm\"\n      -p ${PORT}:8000\n      --ipc=host\n      vllm/vllm-openai:v0.9.0.1\n      --model /root/.cache/huggingface/Qwen3-30B-A3B-FP8\n      -tp 2\n      --enable-expert-parallel\n      --enable-auto-tool-choice\n      --tool-call-parser hermes\n      --reasoning-parser qwen3\n      --max-model-len 65536\n      --served-model-name qwen3-30b\n      --max-seq-len-to-capture 65536\n      --max_num_seqs 2\n      --cuda_graph_sizes 4\n      --rope-scaling {\\\"rope_type\\\":\\\"yarn\\\",\\\"factor\\\":2.0,\\\"original_max_position_embeddings\\\":32768}\n      --gpu-memory-utilization 0.95\n      --enable-prefix-caching\n      --enable-chunked-prefill\n      --dtype half\n```\n\n* CUDA0 - 5090\n* CUDA1 - 4090\n* CUDA2 - 3090\n\nAnd it crashes with error `ValueError(\"type fp8e4nv not supported in this architecture. The supported fp8 dtypes are ('fp8e4b15', 'fp8e5')\")`. But same parameters on same system works fine with RTX3090 + RTX3090. \n\nNow, I change `CUDA_VISIBLE_DEVICES=1,2` to `CUDA_VISIBLE_DEVICES=2,1` and it works fine with RTX4090+RTX3090 with warning about poor performance.\n\nSo, I came to conclusion, VLLM chose kernel with native FP8 because first GPU (4090) has FP8 support, and then crashed because not all GPUs has native FP8.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-06-17T11:19:39+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19741/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19741"
  },
  {
    "number": 1774,
    "title": "Confusing problems generated by  Qwen14b-int4",
    "body": "Confusing problems generated by qwen14b-int\r\n- If max-tokens is not added, it will stop after a few words.\r\njust like this:\r\n\r\n```\r\n# Sample prompts.\r\nprompts = [\r\n    \"Hello, my name is\",\r\n    \"The president of the United States is\",\r\n    \"The capital of France is\",\r\n    \"\u6765\u4e00\u9996\u5b8b\u8bcd\u5427:\",\r\n]\r\n# Create a sampling params object.\r\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95,max_tokens=1024)\r\n\r\n\r\nand this result:\r\nINFO 11-24 16:22:09 llm_engine.py:72] Initializing an LLM engine with config: model='/home/incar/newdata2/tms/llm/QWenData/Qwen-14B-Chat-Int4', tokenizer='/home/incar/newdata2/tms/llm/QWenData/Qwen-14B-Chat-Int4', tokenizer_mode=auto, revision=v1.1.8, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=gptq, seed=0)\r\nWARNING 11-24 16:22:10 tokenizer.py:66] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\r\nINFO 11-24 16:22:19 llm_engine.py:207] # GPU blocks: 784, # CPU blocks: 327\r\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  8.36it/s]\r\nPrompt: 'Hello, my name is', Generated text: \" Xie Lei. I'm a student at Sunshine Middle School. I'm in\"\r\nPrompt: 'The president of the United States is', Generated text: ' the head of state and head of government of the United States. The president directs'\r\nPrompt: 'The capital of France is', Generated text: ' Paris\uff0e\uff08\u4fdd\u6301\u539f\u53e5\u610f\u601d\uff09 Paris is the capital of France \uff0e'\r\nPrompt: '\u6765\u4e00\u9996\u5b8b\u8bcd\u5427:', Generated text: '\u300a\u4e11\u5974\u513f\u00b7\u4e66\u535a\u5c71\u9053\u4e2d\u58c1\u300b\u8f9b\u5f03\u75be\\n\\n'\r\n```\r\nu can found that too short and no completed!\r\n\r\n- If max-tokens is added, the subsequent data will be confusing and meaningless.\r\nthis like this:\r\n```\r\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95,max_tokens=1024)\r\n\r\n\r\nand this's result:\r\n\r\nPrompt: 'Hello, my name is', Generated text: \" Xie Lei. I'm a student at Sunshine Middle School. I'm in Class One, Grade Seven. I'm a tidy girl. In my room, my blue quilt is on the bed. My yellow schoolbag is on the chair. Some books and a pencil box are in it. My red notebook is on the desk. My clock is on the desk, too. It's green. I like the color. What's that on the sofa? It's my dog. Its name is Bobby. Bobby is very smart. It can play ball games with me. I like it very much. I have a photo. In the photo, I'm in the zoo with my parents. We're having a good time. \u6839\u636e\u77ed\u6587\u5185\u5bb9\uff0c\u9009\u62e9\u6b63\u786e\u7b54\u6848\u3002 ( ) 1. What color is Xie Lei's schoolbag? A. Blue. B. Yellow. C. Green. D. Red. ( ) 2. Where is Xie Lei's clock? A. On the bed. B. On the chair. C. On the desk. D. On the sofa. ( ) 3. What's on the sofa? A. A dog. B. A cat. C. A book. D. A clock. ( ) 4. What's the meaning(\u610f\u601d) of the word \u201csmart\u201d? A. \u6f02\u4eae\u7684 B. \u53ef\u7231\u7684 C. \u806a\u660e\u7684 D. \u597d\u73a9\u7684 ( ) 5. What's in Xie Lei's pencil box? A. Some books. B. A notebook. C. A ruler and some pencils. D. We don't know.\\n\\n1.\u6839\u636eMy yellow schoolbag is on the chair. \u53ef\u77e5\u7b54\u6848\u3002 2.\u6839\u636eMy clock is on the desk, too. \u53ef\u77e5\u7b54\u6848\u3002 3.\u6839\u636eWhat's that on the sofa? It's my dog. \u53ef\u77e5\u7b54\u6848\u3002 4.\u6839\u636eBobby is very smart. It can play ball games with me. \u53ef\u77e5 smart \u610f\u4e3a\u201c\u806a\u660e\u7684\u201d\u3002 5.\u6839\u636eSome books and a pencil box are in it. \u53ef\u77e5\u7b54\u6848\u3002 1.B\uff1b2.C\uff1b3.A\uff1b4.C\uff1b5.D.<|endoftext|>\u5df2\u77e5\u66f2\u7ebf y=ax^{2}+1 \u5728\u70b9 (1,2) \u5904\u7684\u5207\u7ebf\u65b9\u7a0b\u4e3a 2 x-y+b=0 ,\u5219 a,b \u7684\u503c\u5206\u522b\u4e3a a.1,0 b.1,-1 c.-1,0 d.-1,-1\\n\\n[\u8003\u70b9\u63d0\u793a]\u672c\u9898\u662f\u4e00\u9053\u8003\u67e5\u5bfc\u6570\u51e0\u4f55\u610f\u4e49\u7684\u9898\u76ee,\u89e3\u9898\u7684\u5173\u952e\u662f\u638c\u63e1\u5bfc\u6570\u7684\u51e0\u4f55\u610f\u4e49;[\u89e3\u9898\u65b9\u6cd5\u63d0\u793a]\u9996\u5148\u6839\u636e\u5bfc\u6570\u7684\u51e0\u4f55\u610f\u4e49,\u6c42\u51fa\u66f2\u7ebf\u5728\u70b9 (1,2) \u5904\u7684\u5207\u7ebf\u659c\u7387,\u5373y\u2032 |_{x=1}; \u63a5\u4e0b\u6765,\u7ed3\u5408\u5207\u7ebf\u65b9\u7a0b 2 x-y+b=0 ,\u53ef\u4ee5\u5f97\u5230 a+b=0 ,\u518d\u7ed3\u5408 (1,2) \u5728\u5207\u7ebf\u4e0a,\u53ef\u4ee5\u5f97\u5230 2-1+b=0 ,\u8054\u7acb\u6c42\u89e3\u5373\u53ef\u5f97\u5230\u7b54\u6848.\\n\\nb.\u2235y\u2032 =2 ax ,\u2234y\u2032 |_{x=1}=2 a ,\u2234\u66f2\u7ebf\u5728\u70b9 (1,2) \u5904\u7684\u5207\u7ebf\u659c\u7387\u4e3a 2 a. \u53c8\u2235\u5207\u7ebf\u65b9\u7a0b\u4e3a 2 x-y+b=0 ,\u2234 2 a=2 ,\u2234 a=1. \u53c8\u2235 (1,2) \u5728\u5207\u7ebf\u4e0a,\u2234 2-1+b=0 ,\u2234 b=-1. \u6545\u9009b.<|endoftext|>\u9605\u8bfb\u4e0b\u9762\u7684\u6587\u5b57\uff0c\u5b8c\u6210\u4e0b\u9762\u5c0f\u9898\u3002 \u4e34\u5ddd\u56db\u68a6\u6307\u7684\u662f\u660e\u4ee3\u5267\u4f5c\u5bb6\u6c64\u663e\u7956\u7684\u300a\u7261\u4e39\u4ead\u300b\u300a\u7d2b\u9497\u8bb0\u300b\u300a\u90af\u90f8\u8bb0\u300b\u548c\u300a\u5357\u67ef\u8bb0\u300b\u56db\u5267\u7684\u5408\u79f0\u3002\u56e0\u4e3a\u8fd9\u56db\u90e8\u5267\u90fd\u662f\u5728\u6c5f\u897f\u629a\u5dde\uff08\u4e34\u5ddd\uff09\u7684\u7389\u8317\u5802\u5b8c\u6210\uff0c\u6240\u4ee5\u88ab\u79f0\u4e3a\u201c\u4e34\u5ddd\u56db\u68a6\u201d\u3002\u8fd9\u56db\u90e8\u4f5c\u54c1\uff0c\u4ece\u5404\u4e2a\u4e0d\u540c\u7684\u89d2\u5ea6\uff0c\u5199\u51fa\u4e86\u4eba\u4eec\u5bf9\u7231\u60c5\u3001\u5bf9\u5e78\u798f\u751f\u6d3b\u7684\u5411\u5f80\u548c\u8ffd\u6c42\uff0c\u5199\u51fa\u4e86\u4eba\u4eec\u5bf9\u7406\u60f3\u7684\u738b\u56fd\u7684\u70ed\u70c8\u7684\u61a7\u61ac\u548c\u5927\u80c6\u7684\u60f3\u8c61\u3002 \u300a\u7261\u4e39\u4ead\u300b\u662f\u6c64\u663e\u7956\u7684\u4ee3\u8868\u4f5c\uff0c\u4e5f\u662f\u660e\u4ee3\u620f\u66f2\u521b\u4f5c\u7684\u9ad8\u5cf0\u3002\u5b83\u628a\u4f20\u8bf4\u4e2d\u7684\u675c\u4e3d\u5a18\u548c\u67f3\u68a6\u6885\u7684\u6545\u4e8b\uff0c\u5199\u6210\u4e86\u4e00\u90e8\u6279\u5224\u5403\u4eba\u7684\u5c01\u5efa\u793c\u6559\uff0c\u6b4c\u9882\u81ea\u7531\u604b\u7231\u548c\u4e2a\u6027\u89e3\u653e\u7684\u65b0\u5267\uff0c\u4f7f\u6c64\u663e\u7956\u8d62\u5f97\u4e86\u201c\u4e1c\u65b9\u7684\u838e\u58eb\u6bd4\u4e9a\u201d\u7684\u79f0\u53f7\u3002\u300a\u7261\u4e39\u4ead\u300b\u7684\u51fa\u73b0\uff0c\u4f7f\u6211\u56fd\u620f\u66f2\u4ece\u5143\u6742\u5267\u8f6c\u53d8\u5230\u4e86\u660e\u4ee3\u4f20\u5947\uff0c\u5bf9\u6211\u56fd\u620f\u66f2\u7684\u53d1\u5c55\u4ea7\u751f\u4e86\u6df1\u8fdc\u7684\u5f71\u54cd\u3002 \u300a\u7d2b\u9497\u8bb0\u300b\u662f\u6c64\u663e\u7956\u7684\u65e9\u671f\u4f5c\u54c1\uff0c\u4e3b\u8981\u5199\u6797\u6708\u5982\u548c\u970d\u5c0f\u7389\u7684\u7231\u60c5\u6545\u4e8b\u3002\u6797\u6708\u5982\u662f\u674e\u76ca\u7684\u8868\u59b9\uff0c\u56e0\u4e3a\u674e\u76ca\u5bf9\u5979\u6001\u5ea6\u51b7\u6de1\uff0c\u6797\u6708\u5982\u5c31\u4e0e\u970d\u5c0f\u7389\u4ea4\u597d\uff0c\u4e92\u76f8\u503e\u8bc9\u81ea\u5df1\u7684\u60c5\u611f\u3002\u970d\u5c0f\u7389\u559c\u6b22\u674e\u76ca\uff0c\u4f46\u674e\u76ca\u5374\u949f\"\r\nPrompt: 'The president of the United States is', Generated text: ' the head of state and head of government of the United States. The president directs the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces.\\n\\nAnswer this question based on the passage: what type of government does the united states have?\\n\u9996\u5148\u8bf4\u51fa\u4f60\u7684\u601d\u8003\u8fc7\u7a0b\uff0c\u7136\u540e\u5f97\u51fa\u7b54\u6848\uff1a\\n\\nTo answer the above question, the relevant sentence is: The president of the United States is the head of state and head of government of the United States.\\n\u6700\u7ec8\u7684\u7b54\u6848\u662f\uff1a\\nthe United States has a presidential government.<|endoftext|>\u5c06\u4e0b\u9762\u7684\u53e5\u5b50\u7ffb\u8bd1\u6210\u6cd5\u8bed\uff1a\u6211\u559c\u6b22\u5403\u62ab\u8428\u3002\\n\\nJ\\'aime manger de la pizza.<|endoftext|>\u5c0f\u660e\u6709 5 \u4e2a\u82f9\u679c\uff0c\u4ed6\u60f3\u5206\u7ed9\u4ed6\u7684\u4e24\u4e2a\u670b\u53cb\u3002\u5982\u679c\u6bcf\u4e2a\u670b\u53cb\u5206\u5230\u7684\u82f9\u679c\u6570\u91cf\u76f8\u540c\uff0c\u90a3\u4e48\u6bcf\u4e2a\u670b\u53cb\u5206\u5230\u51e0\u4e2a\u82f9\u679c\uff1f \u5c0f\u660e\u6709 5 \u4e2a\u82f9\u679c\uff0c\u4ed6\u60f3\u5206\u7ed9\u4ed6\u7684\u4e24\u4e2a\u670b\u53cb\uff0c\u90a3\u4e48\u4ed6\u9700\u8981\u5c06 5 \u4e2a\u82f9\u679c\u5e73\u5747\u5206\u7ed9\u4e24\u4e2a\u670b\u53cb\u3002\u4e3a\u4e86\u627e\u5230\u6bcf\u4e2a\u670b\u53cb\u5206\u5230\u7684\u82f9\u679c\u6570\u91cf\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u9664\u6cd5\u3002\\n\u9996\u5148\uff0c\u6211\u4eec\u5c06 5 \u4e2a\u82f9\u679c\u9664\u4ee5 2 \u4e2a\u670b\u53cb\uff0c\u5f97\u5230\uff1a\\n5 \u00f7 2 = 2\u4f591\\n\u8fd9\u610f\u5473\u7740\u6bcf\u4e2a\u670b\u53cb\u53ef\u4ee5\u5206\u5230 2 \u4e2a\u82f9\u679c\uff0c\u8fd8\u5269\u4e0b 1 \u4e2a\u82f9\u679c\u65e0\u6cd5\u5e73\u5206\u3002\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u5c06\u8fd9\u4e2a\u82f9\u679c\u7559\u7ed9\u5c0f\u660e\u81ea\u5df1\u5403\uff0c\u6216\u8005\u518d\u5206\u7ed9\u4e24\u4e2a\u670b\u53cb\u4e2d\u7684\u4e00\u4e2a\u4eba\u3002<|endoftext|>\u7ed9\u5b9a\u4e00\u6bb5\u82f1\u6587\u6587\u672c\uff0c\u5c06\u5176\u7ffb\u8bd1\u6210\u4e2d\u6587\u3002\\n\r\n```\r\n\r\nso,how can i do for this?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-11-24T08:28:55+00:00",
    "closed_at": "2024-03-25T09:42:02+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1774/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1774"
  },
  {
    "number": 13715,
    "title": "[New Model]: Qwen2.5-VL",
    "body": "### The model to consider.\n\nIt looks like vllm doesn't currently support Qwen 2.5-VL?\n\nray.exceptions.RayTaskError(ValueError): ray::WorkerDict.actor_rollout_generate_sequences() (pid=67366, ip=192.168.128.5, actor_id=2681de9b19487eb6c57dea4401000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7ee6341e7dc0>)\n  File \"/mnt/2050data/wentao.zhang/MultiModalMath/verl/single_controller/ray/base.py\", line 399, in func\n    return getattr(self.worker_dict[key], name)(*args, **kwargs)\n  File \"/mnt/2050data/wentao.zhang/MultiModalMath/verl/single_controller/base/decorator.py\", line 404, in inner\n    return func(*args, **kwargs)\n  File \"/mnt/2050data/wentao.zhang/MultiModalMath/verl/workers/fsdp_workers.py\", line 463, in generate_sequences\n    with self.rollout_sharding_manager:\n  File \"/mnt/2050data/wentao.zhang/MultiModalMath/verl/workers/sharding_manager/fsdp_vllm.py\", line 83, in __enter__\n    load_dtensor_weights(\n  File \"/mnt/2050data/wentao.zhang/MultiModalMath/verl/third_party/vllm/vllm_spmd/dtensor_weight_loaders.py\", line 364, in load_dtensor_weights\n    weight_loader = _get_model_weight_loader(vllm_model.__class__.__name__)\n  File \"/mnt/2050data/wentao.zhang/MultiModalMath/verl/third_party/vllm/vllm_spmd/dtensor_weight_loaders.py\", line 374, in _get_model_weight_loader\n    raise ValueError(f\"Model architectures {arch} are not supported for now. \"\nValueError: Model architectures Qwen2_5_VLForConditionalGeneration are not supported for now. Supported architectures: dict_keys(['GPT2LMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'InternLMForCausalLM', 'AquilaModel', 'AquilaForCausalLM', 'Phi3ForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPTBigCodeForCausalLM', 'Starcoder2ForCausalLM', 'Qwen2ForCausalLM', 'DeepseekV2ForCausalLM', 'Qwen2VLForConditionalGeneration'])\n\n### The closest model vllm already supports.\n\n_No response_\n\n### What's your difficulty of supporting the model you want?\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2025-02-23T04:36:34+00:00",
    "closed_at": "2025-02-23T04:44:32+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13715/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13715"
  },
  {
    "number": 13676,
    "title": "[Bug]: Error while running Deepseek-R1: vllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly.",
    "body": "### Your current environment\n\n<details>\nWas working 10minutes ago, seems like after vllm crash whole GPUs are stuck ?\n\n```\nTraceback (most recent call last):\n  File \"/root/collect_env.py\", line 17, in <module>\n    from vllm.envs import environment_variables\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/__init__.py\", line 7, in <module>\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/arg_utils.py\", line 20, in <module>\n    from vllm.executor.executor_base import ExecutorBase\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\", line 15, in <module>\n    from vllm.platforms import current_platform\n  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/platforms/__init__.py\", line 222, in __getattr__\n    _current_platform = resolve_obj_by_qualname(\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/utils.py\", line 1906, in resolve_obj_by_qualname\n    module = importlib.import_module(module_name)\n  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/platforms/cuda.py\", line 390, in <module>\n    CudaPlatform.log_warnings()\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/platforms/cuda.py\", line 63, in wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/platforms/cuda.py\", line 331, in log_warnings\n    device_names = [\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/platforms/cuda.py\", line 332, in <listcomp>\n    cls._get_physical_device_name(i) for i in range(device_ids)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/platforms/cuda.py\", line 324, in _get_physical_device_name\n    return pynvml.nvmlDeviceGetName(handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml.py\", line 2342, in wrapper\n    res = func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml.py\", line 2639, in nvmlDeviceGetName\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml.py\", line 1042, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.NVMLError_Unknown: Unknown Error\n```\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhile running vllm as OpenAI compatible server to server Deepseek-R1 across 2 DGX-H100 with LWS, following error happened:\n```\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581] Error executing method 'start_worker_execution_loop'. This might cause deadlock in distributed execution.\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581] Traceback (most recent call last):\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 573, in execute_method\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     return run_method(target, method, args, kwargs)\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2196, in run_method\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     return func(*args, **kwargs)\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 91, in start_worker_exe\ncution_loop\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     output = self.execute_model(execute_model_req=None)\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 420, in execute_model\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     output = self.model_runner.execute_model(\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_contex\nt\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     return func(*args, **kwargs)\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1724, in execute_model\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     hidden_or_intermediate_states = model_executable(\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]                                     ^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_\nimpl\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     return self._call_impl(*args, **kwargs)\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     return forward_call(*args, **kwargs)\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py\", line 677, in \nforward\n\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     hidden_states = self.model(input_ids, positions, kv_caches,\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 172, in __call__\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     return self.forward(*args, **kwargs)\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py\", line 633, in \nforward\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     hidden_states, residual = layer(positions, hidden_states,\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_\nimpl\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     return self._call_impl(*args, **kwargs)\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     return forward_call(*args, **kwargs)\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py\", line 550, in \nforward\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     hidden_states = self.self_attn(\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]                     ^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_\nimpl\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     return self._call_impl(*args, **kwargs)\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     return forward_call(*args, **kwargs)\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py\", line 469, in \nforward\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     return self.mla_attn(hidden_states_or_q_c, kv_c_normed, k_pe, kv_cache,\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     return self._call_impl(*args, **kwargs)\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     return forward_call(*args, **kwargs)\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py\", line 201, in forward\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     return torch.ops.vllm.unified_attention(\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/torch/_ops.py\", line 1116, in __call__\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     return self._op(*args, **(kwargs or {}))\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py\", line 307, in unified_attention\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     return self.impl.forward(self, query, key, value, kv_cache, attn_metadata)\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/attention/backends/mla/utils.py\", line 457, in forward\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     self.rotary_emb(\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     return self._call_impl(*args, **kwargs)\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     return forward_call(*args, **kwargs)\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding.py\", line 710, in forward\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]     query_rot = query_rot * cos + rotate_fn(query_rot) * sin\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581]                 ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581] RuntimeError: CUDA error: unspecified launch failure\n(RayWorkerWrapper pid=570) ERROR 02-21 08:03:30 worker_base.py:581] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n\nERROR 02-21 08:05:44 async_llm_engine.py:68] Engine background task failed\nERROR 02-21 08:05:44 async_llm_engine.py:68] Traceback (most recent call last):\nERROR 02-21 08:05:44 async_llm_engine.py:68]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 58, in _log_task_completion\nERROR 02-21 08:05:44 async_llm_engine.py:68]     return_value = task.result()\nERROR 02-21 08:05:44 async_llm_engine.py:68]                    ^^^^^^^^^^^^^\nERROR 02-21 08:05:44 async_llm_engine.py:68]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 825, in run_engine_loop\nERROR 02-21 08:05:44 async_llm_engine.py:68]     result = task.result()\nERROR 02-21 08:05:44 async_llm_engine.py:68]              ^^^^^^^^^^^^^\nERROR 02-21 08:05:44 async_llm_engine.py:68]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 748, in engine_step\nERROR 02-21 08:05:44 async_llm_engine.py:68]     request_outputs = await self.engine.step_async(virtual_engine)\nERROR 02-21 08:05:44 async_llm_engine.py:68]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 08:05:44 async_llm_engine.py:68]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 353, in step_async\nERROR 02-21 08:05:44 async_llm_engine.py:68]     outputs = await self.model_executor.execute_model_async(\nERROR 02-21 08:05:44 async_llm_engine.py:68]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 08:05:44 async_llm_engine.py:68]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py\", line 583, in execute_model_async\nERROR 02-21 08:05:44 async_llm_engine.py:68]     return await super().execute_model_async(execute_model_req)\nERROR 02-21 08:05:44 async_llm_engine.py:68]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 08:05:44 async_llm_engine.py:68]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 354, in execute_model_async\nERROR 02-21 08:05:44 async_llm_engine.py:68]     return await self._driver_execute_model_async(execute_model_req)\nERROR 02-21 08:05:44 async_llm_engine.py:68]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 08:05:44 async_llm_engine.py:68]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py\", line 625, in _driver_execute_model_a\nsync\nERROR 02-21 08:05:44 async_llm\nERROR 02-21 08:05:44 async_llm_engine.py:68]     results = await asyncio.gather(*tasks)\nERROR 02-21 08:05:44 async_llm_engine.py:68]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 08:05:44 async_llm_engine.py:68]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 1312, in _run_task_with_lock\nERROR 02-21 08:05:44 async_llm_engine.py:68]     return await task(*args, **kwargs)\nERROR 02-21 08:05:44 async_llm_engine.py:68]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 08:05:44 async_llm_engine.py:68] ray.exceptions.RayTaskError(RuntimeError): ray::RayWorkerWrapper.execute_method() (pid=256, ip=172.29.46.242, actor_id=b18ba8248\nd2f7d0a697062b002000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x7ffade21c6b0>)\nERROR 02-21 08:05:44 async_llm_engine.py:68]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 08:05:44 async_llm_engine.py:68]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 08:05:44 async_llm_engine.py:68]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 582, in execute_method\nERROR 02-21 08:05:44 async_llm_engine.py:68]     raise e\nERROR 02-21 08:05:44 async_llm_engine.py:68]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 573, in execute_method\nERROR 02-21 08:05:44 async_llm_engine.py:68]     return run_method(target, method, args, kwargs)\nERROR 02-21 08:05:44 async_llm_engine.py:68]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 08:05:44 async_llm_engine.py:68]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2196, in run_method\nERROR 02-21 08:05:44 async_llm_engine.py:68]     return func(*args, **kwargs)\nERROR 02-21 08:05:44 async_llm_engine.py:68]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 08:05:44 async_llm_engine.py:68]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 413, in execute_model\nERROR 02-21 08:05:44 async_llm_engine.py:68]     get_pp_group().recv_tensor_dict(\nERROR 02-21 08:05:44 async_llm_engine.py:68]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 626, in recv_tensor_dict\nERROR 02-21 08:05:44 async_llm_engine.py:68]     recv_metadata_list = self.recv_object(src=src)\nERROR 02-21 08:05:44 async_llm_engine.py:68]                          ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 08:05:44 async_llm_engine.py:68]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 440, in recv_object\nERROR 02-21 08:05:44 async_llm_engine.py:68]     rank_size = torch.distributed.recv(size_tensor,\nERROR 02-21 08:05:44 async_llm_engine.py:68]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 08:05:44 async_llm_engine.py:68]   File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 83, in wrapper\nERROR 02-21 08:05:44 async_llm_engine.py:68]     return func(*args, **kwargs)\nERROR 02-21 08:05:44 async_llm_engine.py:68]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 08:05:44 async_llm_engine.py:68]   File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 2203, in recv\nERROR 02-21 08:05:44 async_llm_engine.py:68]     pg.recv([tensor], group_src_rank, tag).wait()\n\nERROR 02-21 08:05:44 async_llm_engine.py:68] RuntimeError: [../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:81] Timed out waiting 1800000ms for recv operation to co\nmplete\nException in callback functools.partial(<function _log_task_completion at 0x7ffe81c3aac0>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_\nllm_engine.AsyncLLMEngine object at 0x7ffe697ecda0>>)\nhandle: <Handle functools.partial(<function _log_task_completion at 0x7ffe81c3aac0>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_en\ngine.AsyncLLMEngine object at 0x7ffe697ecda0>>)>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 58, in _log_task_completion\n    return_value = task.result()\n                   ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 825, in run_engine_loop\n    result = task.result()\n             ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 748, in engine_step\n    request_outputs = await self.engine.step_async(virtual_engine)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 353, in step_async\n    outputs = await self.model_executor.execute_model_async(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py\", line 583, in execute_model_async\n    return await super().execute_model_async(execute_model_req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 354, in execute_model_async\n    return await self._driver_execute_model_async(execute_model_req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py\", line 625, in _driver_execute_model_async\n    results = await asyncio.gather(*tasks)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 1312, in _run_task_with_lock\n    return await task(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nray.exceptions.RayTaskError(RuntimeError): ray::RayWorkerWrapper.execute_method() (pid=256, ip=172.29.46.242, actor_id=b18ba8248d2f7d0a697062b002000000, repr=<vllm.executor.\nray_utils.RayWorkerWrapper object at 0x7ffade21c6b0>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 582, in execute_method\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 573, in execute_method\n    return run_method(target, method, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2196, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 413, in execute_model\n    get_pp_group().recv_tensor_dict(\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 626, in recv_tensor_dict\n    recv_metadata_list = self.recv_object(src=src)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 440, in recv_object\n    rank_size = torch.distributed.recv(size_tensor,\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 83, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 2203, in recv\n    pg.recv([tensor], group_src_rank, tag).wait()\nRuntimeError: [../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:81] Timed out waiting 1800000ms for recv operation to complete\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 70, in _log_task_completion\n    raise AsyncEngineDeadError(\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for the actual cause.\n```\nvLLM launched with:\n```python3 -m vllm.entrypoints.openai.api_server --port 8080 --model deepseek-ai/DeepSeek-R1 --tensor-parallel-size 8 --pipeline_parallel_size 2 --trust-remote-code --max-model-len 16384 --gpu-memory-utilization 0.85 --max-num-seqs 4 --served-model-name DeepSeek-R1-671b --disable-log-requests```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-21T16:25:09+00:00",
    "closed_at": "2025-06-25T02:16:28+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13676/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13676"
  },
  {
    "number": 12549,
    "title": "[Installation]: Docker novice installation help (urgent)",
    "body": "### Your current environment\n\nI used this command \"DOCKER_BUILDKIT=1 docker build . --target vllm-openai-base --tag vllm/vllm-openai\" to build the vllm image, but at one step the build became unusually slow\n\n`root@iZwz9av7dpqr38k3rph9ziZ:~/vllm# DOCKER_BUILDKIT=1 docker build . --target vllm-openai-base --tag vllm/vllm-openai --build-arg torch_cuda_arch_list=\"\"\n[+] Building 418.7s (28/37)                                                                                  docker:default\n => [internal] load build definition from Dockerfile                                                                   0.0s\n => => transferring dockerfile: 12.57kB                                                                                0.0s\n => WARN: FromAsCasing: 'as' and 'FROM' keywords' casing do not match (line 141)                                       0.0s\n => [internal] load metadata for docker.io/nvidia/cuda:12.4.1-devel-ubuntu22.04                                        0.3s\n => [internal] load metadata for docker.io/nvidia/cuda:12.4.1-devel-ubuntu20.04                                        0.3s\n => [internal] load .dockerignore                                                                                      0.0s\n => => transferring context: 387B                                                                                      0.0s\n => [vllm-base  1/11] FROM docker.io/nvidia/cuda:12.4.1-devel-ubuntu22.04@sha256:da6791294b0b04d7e65d87b7451d6f2390b4  0.0s\n => CACHED [vllm-base  6/11] RUN --mount=type=cache,target=/root/.cache/pip     if [ \"linux/amd64\" = \"linux/arm64\" ];  0.0s\n => CACHED [vllm-base  2/11] WORKDIR /vllm-workspace                                                                   0.0s\n => CACHED [vllm-base  3/11] RUN PYTHON_VERSION_STR=$(echo 3.12 | sed 's/\\.//g') &&     echo \"export PYTHON_VERSION_S  0.0s\n => CACHED [vllm-base  4/11] RUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections     && echo 'tzda  0.0s\n => CACHED [vllm-base  5/11] RUN ldconfig /usr/local/cuda-$(echo 12.4.1 | cut -d. -f1,2)/compat/                       0.0s\n => [base  1/11] FROM docker.io/nvidia/cuda:12.4.1-devel-ubuntu20.04@sha256:8d577fd078ae56c37493af4454a5b700c72a7f1ae  0.0s\n => [internal] load build context                                                                                      0.3s\n => => transferring context: 50.06MB                                                                                   0.3s\n => CACHED [build 1/8] COPY requirements-build.txt requirements-build.txt                                              0.0s\n => CACHED [base  2/11] RUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections     && echo 'tzdata tz  0.0s\n => CACHED [base  3/11] RUN apt-get install -y gcc-10 g++-10                                                           0.0s\n => CACHED [base  4/11] RUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g  0.0s\n => CACHED [base  5/11] RUN <<EOF (gcc --version)                                                                      0.0s\n => CACHED [base  6/11] RUN ldconfig /usr/local/cuda-$(echo 12.4.1 | cut -d. -f1,2)/compat/                            0.0s\n => CACHED [base  7/11] WORKDIR /workspace                                                                             0.0s\n => CACHED [base  8/11] RUN --mount=type=cache,target=/root/.cache/pip     if [ \"linux/amd64\" = \"linux/arm64\" ]; then  0.0s\n => CACHED [base  9/11] COPY requirements-common.txt requirements-common.txt                                           0.0s\n => CACHED [base 10/11] COPY requirements-cuda.txt requirements-cuda.txt                                               0.0s\n => CACHED [base 11/11] RUN --mount=type=cache,target=/root/.cache/pip     python3 -m pip install -r requirements-cud  0.0s\n => CACHED [build 1/8] COPY requirements-build.txt requirements-build.txt                                              0.0s\n => [build 2/8] RUN --mount=type=cache,target=/root/.cache/pip     python3 -m pip install -r requirements-build.txt -  3.4s\n => [build 3/8] COPY . .                                                                                               0.4s \n => [build 4/8] RUN --mount=type=bind,source=.git,target=.git     if [ \"0\" != 0 ]; then bash tools/check_repo.sh ; fi  0.3s \n => [build 5/8] RUN --mount=type=cache,target=/root/.cache/pip     --mount=type=bind,source=.git,target=.git     if [  0.4s \n => [build 6/8] RUN --mount=type=cache,target=/root/.cache/ccache     --mount=type=cache,target=/root/.cache/pip     413.6s \n => => # Using NVCC_THREADS=8 as the number of nvcc threads.                                                                \n => => # [1/256] Building CXX object CMakeFiles/_moe_C.dir/csrc/moe/torch_bindings.cpp.o                                    \n => => # [2/256] Building CXX object CMakeFiles/cumem_allocator.dir/csrc/cumem_allocator.cpp.o                             \n => => # [3/256] Linking CXX shared module cumem_allocator.abi3.so                                                         \n => => # [4/256] Building CUDA object CMakeFiles/_C.dir/csrc/cache_kernels.cu.o                                            \n => => # [5/256] Building CUDA object CMakeFiles/_moe_C.dir/csrc/moe/moe_align_sum_kernels.cu.o  `\n\n### How you are installing vllm\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2025-01-29T13:17:11+00:00",
    "closed_at": "2025-01-30T17:36:22+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12549/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12549"
  },
  {
    "number": 2703,
    "title": "[BUG]: Streaming `logprob` & `echo` combo.",
    "body": "I'm trying to start writing a `logprob` & `echo` support for chat request. \r\n\r\nUnfortunately, running test like #1992 when `echo` is setted as `true` server doesn't respond.\r\n\r\nSeeing furtherer I checked that the **bug** begging in #2449 (sha: dd7e8f5f643167e3f13045cf75cbead54cb2ccfe).\r\nPrevious commit #2463 (sha: d2a68364c473a3167a1c2b90f947bb611322a867) worked ok.\r\n\r\n## LOG:\r\n```\r\nvllm-openai-main  | INFO 02-01 04:31:38 async_llm_engine.py:385] Received request cmpl-dc7fb40d1b534a879768966f3dc50d39: prompt: None, prefix_pos: None,sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=20, logprobs=0, prompt_logprobs=0, skip_special_tokens=True, spaces_between_special_tokens=True), prompt token ids: [2, 12375, 351, 5, 232, 651, 11, 2760, 116, 50118, 6557, 45117, 35, 50118].\r\nvllm-openai-main  | ERROR:    Exception in ASGI application\r\nvllm-openai-main  | Traceback (most recent call last):\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 259, in __call__\r\nvllm-openai-main  |     await wrap(partial(self.listen_for_disconnect, receive))\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 255, in wrap\r\nvllm-openai-main  |     await func()\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 232, in listen_for_disconnect\r\nvllm-openai-main  |     message = await receive()\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 587, in receive\r\nvllm-openai-main  |     await self.message_event.wait()\r\nvllm-openai-main  |   File \"/usr/lib/python3.10/asyncio/locks.py\", line 214, in wait\r\nvllm-openai-main  |     await fut\r\nvllm-openai-main  | asyncio.exceptions.CancelledError: Cancelled by cancel scope 7f20005d5150\r\nvllm-openai-main  | \r\nvllm-openai-main  | During handling of the above exception, another exception occurred:\r\nvllm-openai-main  | \r\nvllm-openai-main  | Traceback (most recent call last):\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 426, in run_asgi\r\nvllm-openai-main  |     result = await app(  # type: ignore[func-returns-value]\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\r\nvllm-openai-main  |     return await self.app(scope, receive, send)\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\r\nvllm-openai-main  |     await super().__call__(scope, receive, send)\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 116, in __call__\r\nvllm-openai-main  |     await self.middleware_stack(scope, receive, send)\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\r\nvllm-openai-main  |     raise exc\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\r\nvllm-openai-main  |     await self.app(scope, receive, _send)\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 83, in __call__\r\nvllm-openai-main  |     await self.app(scope, receive, send)\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/aioprometheus/asgi/middleware.py\", line 184, in __call__\r\nvllm-openai-main  |     await self.asgi_callable(scope, receive, wrapped_send)\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\nvllm-openai-main  |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 55, in wrapped_app\r\nvllm-openai-main  |     raise exc\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 44, in wrapped_app\r\nvllm-openai-main  |     await app(scope, receive, sender)\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 746, in __call__\r\nvllm-openai-main  |     await route.handle(scope, receive, send)\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 288, in handle\r\nvllm-openai-main  |     await self.app(scope, receive, send)\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 75, in app\r\nvllm-openai-main  |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 55, in wrapped_app\r\nvllm-openai-main  |     raise exc\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 44, in wrapped_app\r\nvllm-openai-main  |     await app(scope, receive, sender)\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 73, in app\r\nvllm-openai-main  |     await response(scope, receive, send)\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 252, in __call__\r\nvllm-openai-main  |     async with anyio.create_task_group() as task_group:\r\nvllm-openai-main  |   File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 678, in __aexit__\r\nvllm-openai-main  |     raise BaseExceptionGroup(\r\nvllm-openai-main  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\r\nvllm-openai-main  | INFO 02-01 04:31:38 async_llm_engine.py:111] Finished request cmpl-dc7fb40d1b534a879768966f3dc50d39.\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-01T04:54:55+00:00",
    "closed_at": "2024-04-11T22:15:52+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2703/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2703"
  },
  {
    "number": 13448,
    "title": "[Bug]: Guided decoding only generating single character during inference with finetuned model",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nRuntimeWarning: Failed to read commit hash:\nNo module named 'vllm._version'\n  from vllm.version import __version__ as VLLM_VERSION\nCollecting environment information...\nPyTorch version: 2.4.0+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.6 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: version 3.26.3\nLibc version: glibc-2.31\n\nPython version: 3.10.15 (main, Dec  2 2024, 18:21:11) [GCC 9.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-1035-aws-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: 12.2.140\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA A10G\nGPU 1: NVIDIA A10G\nGPU 2: NVIDIA A10G\nGPU 3: NVIDIA A10G\n\nNvidia driver version: 535.183.01\ncuDNN version: Probably one of the following:\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn.so.8.9.5\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.9.5\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.9.5\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.9.5\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.9.5\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.9.5\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.9.5\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nByte Order:                      Little Endian\nAddress sizes:                   48 bits physical, 48 bits virtual\nCPU(s):                          48\nOn-line CPU(s) list:             0-47\nThread(s) per core:              2\nCore(s) per socket:              24\nSocket(s):                       1\nNUMA node(s):                    1\nVendor ID:                       AuthenticAMD\nCPU family:                      23\nModel:                           49\nModel name:                      AMD EPYC 7R32\nStepping:                        0\nCPU MHz:                         2799.998\nBogoMIPS:                        5599.99\nHypervisor vendor:               KVM\nVirtualization type:             full\nL1d cache:                       768 KiB\nL1i cache:                       768 KiB\nL2 cache:                        12 MiB\nL3 cache:                        96 MiB\nNUMA node0 CPU(s):               0-47\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Not affected\nVulnerability Retbleed:          Mitigation; untrained return thunk; SMT enabled with STIBP protection\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save rdpid\n\nVersions of relevant libraries:\n[pip3] mypy-extensions==1.0.0\n[pip3] mypy-protobuf==3.6.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.1.3.1\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.0.2.54\n[pip3] nvidia-curand-cu12==10.3.2.106\n[pip3] nvidia-cusolver-cu12==11.4.5.107\n[pip3] nvidia-cusparse-cu12==12.1.0.106\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.20.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.1.105\n[pip3] onnxruntime==1.20.1\n[pip3] pyzmq==26.2.0\n[pip3] sentence-transformers==3.3.1\n[pip3] torch==2.4.0\n[pip3] torchvision==0.19.0\n[pip3] transformers==4.48.3\n[pip3] triton==3.0.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: N/A (dev)\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PHB     PHB     PHB     0-47    0               N/A\nGPU1    PHB      X      PHB     PHB     0-47    0               N/A\nGPU2    PHB     PHB      X      PHB     0-47    0               N/A\nGPU3    PHB     PHB     PHB      X      0-47    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nGetting below generated text while using guided decoding with choices in a list.\n\nit happens after the first inference. so if I want to run the inference on 100 test samples. the first one generates required tokens but from next one it starts generating single characters only.\n\nGenerated text: 'A'\nGenerated text: 'A'\nGenerated text: 'C'\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-18T02:58:39+00:00",
    "closed_at": "2025-06-20T02:13:26+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13448/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13448"
  },
  {
    "number": 9176,
    "title": "[Bug]: Extreme low throughput when using pipeline parallelism when Batch Size(running req) is small",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.5\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-78-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 1: NVIDIA H100 80GB HBM3\r\nGPU 2: NVIDIA H100 80GB HBM3\r\nGPU 3: NVIDIA H100 80GB HBM3\r\nGPU 4: NVIDIA H100 80GB HBM3\r\nGPU 5: NVIDIA H100 80GB HBM3\r\nGPU 6: NVIDIA H100 80GB HBM3\r\nGPU 7: NVIDIA H100 80GB HBM3\r\n\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Gold 6430\r\nCPU family:                      6\r\nModel:                           143\r\nThread(s) per core:              2\r\nCore(s) per socket:              32\r\nSocket(s):                       2\r\nStepping:                        8\r\nFrequency boost:                 enabled\r\nCPU max MHz:                     2101.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        4200.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nL1d cache:                       3 MiB (64 instances)\r\nL1i cache:                       2 MiB (64 instances)\r\nL2 cache:                        128 MiB (64 instances)\r\nL3 cache:                        120 MiB (2 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-31,64-95\r\nNUMA node1 CPU(s):               32-63,96-127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.68\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.2\r\n[pip3] triton==3.0.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.6.68                  pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.44.2                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.0@COMMIT_HASH_PLACEHOLDER\r\nvLLM Build Flags:\r\nCUDA Archs: 7.0 7.5 8.0 8.6 8.9 9.0+PTX; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tNIC4\tNIC5\tNIC6\tNIC7\tNIC8\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tPIX\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\t0-31,64-95\t0\t\tN/A\r\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNODE\tPIX\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\t0-31,64-95\t0\t\tN/A\r\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNODE\tNODE\tPIX\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\t0-31,64-95\t0\t\tN/A\r\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNODE\tNODE\tNODE\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\t0-31,64-95\t0\t\tN/A\r\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tPIX\tNODE\tNODE\tNODE\tSYS\t32-63,96-127\t1\t\tN/A\r\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tNODE\tPIX\tNODE\tNODE\tSYS\t32-63,96-127\t1\t\tN/A\r\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPIX\tNODE\tSYS\t32-63,96-127\t1\t\tN/A\r\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tPIX\tSYS\t32-63,96-127\t1\t\tN/A\r\nNIC0\tPIX\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t X \tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\r\nNIC1\tNODE\tPIX\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\t X \tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\r\nNIC2\tNODE\tNODE\tPIX\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\t X \tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\r\nNIC3\tNODE\tNODE\tNODE\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\t X \tSYS\tSYS\tSYS\tSYS\tNODE\r\nNIC4\tSYS\tSYS\tSYS\tSYS\tPIX\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t X \tNODE\tNODE\tNODE\tSYS\r\nNIC5\tSYS\tSYS\tSYS\tSYS\tNODE\tPIX\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\t X \tNODE\tNODE\tSYS\r\nNIC6\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPIX\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\t X \tNODE\tSYS\r\nNIC7\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\t X \tSYS\r\nNIC8\tNODE\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  NIC8: mlx5_8\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nRunning Command:\r\n```\r\npython api_server.py --model=/data/Meta-Llama-3___1-8B-Instruct \\\r\n-tp 1 \\\r\n-pp 2 \\\r\n--max-num-seqs 256 \\\r\n--dtype bfloat16 \\\r\n--max-num-batched-tokens 2048 \\\r\n--enable-chunked-prefill \\\r\n--enforce-eager \\\r\n--gpu-memory-utilization 0.85\r\n```\r\n\r\nRequest clients number : 64\r\ninput token number: 6000\r\nmax out token number:256\r\n**large batch prompt throughput(mix infer)**(prompt throughput is as expected):\r\n<img width=\"1350\" alt=\"image\" src=\"https://github.com/user-attachments/assets/9b9d93ba-930d-4346-99ba-f36bd6c024f8\">\r\n\r\n**large batch decoding throughput**(relatively low):\r\n<img width=\"1457\" alt=\"image\" src=\"https://github.com/user-attachments/assets/53dade43-996f-4653-9817-47579b4cfc8b\">\r\n\r\n**small batch decoding throughput**(extremely low):\r\n<img width=\"1247\" alt=\"image\" src=\"https://github.com/user-attachments/assets/32b1326b-614b-4317-a234-56068af82f70\">\r\n\r\nI have investigated the potential bottlenecks in serving process, including tensor parallelism (TP) reduce operations when TP is greater than 1, pipeline parallelism (PP) intermediate result transmission, input preparation, and sampling. My analysis reveals that all these components are performing as expected, with one exception: the model's forward pass on **pp rank 0 and tp rank 0** appears to be significantly slower than anticipated (no communication happened here when TP=1 which is wierd).\r\nWhen TP is greater than 1,  model forward in all TP ranks in PP rank 0 are slow, but PP rank 1 is still fast.\r\n\r\n![img_v3_02fg_cd9abef7-3fcb-4921-8270-8221f096501g](https://github.com/user-attachments/assets/bae7e070-bf02-4161-9d1e-cec74c9f482e)\r\n![img_v3_02fg_3c55acd6-8d3e-4b0c-87e1-277864eaa51g](https://github.com/user-attachments/assets/bce4b6ae-3988-4be2-bf7f-cc100cddc495)\r\nYou could see rank0 is multiple times slower than rank1.\r\n\r\nIf setting pp=1, everything is good(pp=1, tp=1):\r\n\r\nLarge batch:\r\n<img width=\"1215\" alt=\"image\" src=\"https://github.com/user-attachments/assets/697e8a69-0bf8-4646-90ff-794a6e16c166\">\r\nSmall batch:\r\n<img width=\"1208\" alt=\"image\" src=\"https://github.com/user-attachments/assets/beacd054-7917-4fbf-99b3-0c78b86a3992\">\r\n\r\nHow I time forward pass:\r\n<img width=\"983\" alt=\"image\" src=\"https://github.com/user-attachments/assets/58c3585d-6d19-417a-b373-2f860fdc07e8\">\r\n\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-10-09T02:46:05+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9176/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9176"
  },
  {
    "number": 20658,
    "title": "[Bug]: When qwen3-reranker-0.6B is loaded using Tesla T4, the CPU memory continues to grow until the system crashes",
    "body": "### Your current environment\n\nThe graphics card is Tesla T4, and NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0 \n[pip-requirements-all.txt](https://github.com/user-attachments/files/21133901/pip-requirements-all.txt)\n\n### \ud83d\udc1b Describe the bug\n\nThe machine is a single card machine with 32G memory. When executed using the following code, the memory grows until it crashes,\uff1a\n[https://github.com/QwenLM/Qwen3-Embedding/blob/main/examples/qwen3_reranker_vllm.py](url)\nI think it's the distributed parameter: distributed_executor_backend='ray'.\nThis doesn't happen when I load it like this\uff1a\n```\nself.lm = LLM(\nmodel=model_name_or_path,\n#   tensor_parallel_size=number_of_gpu,\nmax_model_len=self.max_length,\n#   enable_prefix_caching=True,\n#   distributed_executor_backend='ray',\nenforce_eager=True,\nGpu_memory_utilization = 0.5,\ndtype=kwargs.get('dtype', 'float16'),\n)\n```\nenable_prefix_caching is not used, presumably because triton is not yet compatible with T4\n\n\n```\nimport logging\n\nimport json\nimport logging\n\nfrom collections import defaultdict\nfrom contextlib import nullcontext\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom typing import Union, List, Tuple, Any\n\nimport numpy as np\nimport torch\nfrom torch import Tensor, nn\nimport torch.nn.functional as F\nfrom torch.utils.data._utils.worker import ManagerWatchdog\n\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoModel, is_torch_npu_available\nlogger = logging.getLogger(__name__)\nfrom vllm import LLM, SamplingParams\nfrom vllm.distributed.parallel_state import destroy_model_parallel\nimport gc\nimport math\nfrom sentence_transformers import CrossEncoder, SentenceTransformer\nfrom vllm.inputs.data import TokensPrompt\n\n\nclass Qwen3Rerankervllm(CrossEncoder):\n    def __init__(self, model_name_or_path, instruction=\"Given the user query, retrieval the relevant passages\", **kwargs):\n        number_of_gpu=torch.cuda.device_count()\n        self.instruction = instruction\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n        self.tokenizer.padding_side = \"left\"\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.suffix = \"<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n        self.max_length=kwargs.get('max_length', 8192)\n        self.suffix_tokens = self.tokenizer.encode(self.suffix, add_special_tokens=False)\n        self.true_token = self.tokenizer(\"yes\", add_special_tokens=False).input_ids[0]\n        self.false_token = self.tokenizer(\"no\", add_special_tokens=False).input_ids[0]\n        self.sampling_params = SamplingParams(temperature=0, \n            top_p=0.95, \n            max_tokens=1,\n            logprobs=20, \n            allowed_token_ids=[self.true_token,self.false_token],\n        )\n        self.lm = LLM(model=model_name_or_path, tensor_parallel_size=number_of_gpu, max_model_len=10000, enable_prefix_caching=True, distributed_executor_backend='ray', gpu_memory_utilization=0.8)\n\n        \n\n    def format_instruction(self, instruction, query, doc):\n        if isinstance(query, tuple):\n            instruction = query[0]\n            query = query[1]\n        text = [\n            {\"role\": \"system\", \"content\": \"Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".\"},\n            {\"role\": \"user\", \"content\": f\"<Instruct>: {instruction}\\n\\n<Query>: {query}\\n\\n<Document>: {doc}\"}\n        ]\n        return text\n\n    def compute_scores(self, pairs, **kwargs):\n        messages = [self.format_instruction(self.instruction, query, doc) for query, doc in pairs]\n        messages =  self.tokenizer.apply_chat_template(\n            messages, tokenize=True, add_generation_prompt=False, enable_thinking=False\n        )\n        messages = [ele[:self.max_length] + self.suffix_tokens for ele in messages]\n        messages = [TokensPrompt(prompt_token_ids=ele) for ele in messages]\n        outputs = self.lm.generate(messages, self.sampling_params, use_tqdm=False)\n        scores = []\n        for i in range(len(outputs)):\n            final_logits = outputs[i].outputs[0].logprobs[-1]\n            token_count = len(outputs[i].outputs[0].token_ids)\n            if self.true_token not in final_logits:\n                true_logit = -10\n            else:\n                true_logit = final_logits[self.true_token].logprob\n            if self.false_token not in final_logits:\n                false_logit = -10\n            else:\n                false_logit = final_logits[self.false_token].logprob\n            true_score = math.exp(true_logit)\n            false_score = math.exp(false_logit)\n            score = true_score / (true_score + false_score)\n            scores.append(score)\n\n        return scores\n\n    def stop(self):\n        destroy_model_parallel()\n\nif __name__ == '__main__':\n    model = Qwen3Rerankervllm(model_name_or_path='Qwen/Qwen3-Reranker-0.6B', instruction=\"Retrieval document that can answer user's query\", max_length=2048)\n    queries = ['What is the capital of China?', 'Explain gravity']\n    documents = [\n        \"The capital of China is Beijing.\",\n        \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n    ]\n    pairs = list(zip(queries, documents))\n    new_scores = model.compute_scores(pairs)\n    print('scores', new_scores)\n    model.stop()\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-07-09T03:15:57+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20658/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20658"
  },
  {
    "number": 20671,
    "title": "[Bug]: Whisper not working on 0.9.2 docker image",
    "body": "### Your current environment\n\nDocker image 0.9.2 on NVidia L40S.\n(The docker image has to be modified, because librosa dependency is missing.)\n\n```\nservices:\n  vllm-whisper-large-v3:\n    # Must modify image for <= v0.9.2\n    # ImportError: Please install vllm[audio] for audio support\n    # image: vllm/vllm-openai:v0.9.2\n    image: vllm/vllm-openai-audio:v0.9.2\n    build:\n      context: .\n    container_name: vllm-whisper-large-v3\n    environment:\n      - HF_TOKEN=$HF_TOKEN\n      - VLLM_NO_USAGE_STATS=1\n    ipc: host\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              device_ids: ['2']\n              capabilities: [ gpu ]\n    network_mode: host\n    volumes:\n      - /mnt/sda/huggingface:/root/.cache/huggingface\n      - .:/opt/vllm\n    command:\n      - --port=8006\n      - --disable-log-requests\n      - --model=openai/whisper-large-v3\n      - --gpu-memory-utilization=0.40\n      - --swap-space=5\n    restart: unless-stopped\n```\n\n```\n# Use the base vLLM image\nFROM vllm/vllm-openai:v0.9.2\n\nRUN apt-get update \\\n    && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \\\n    libsndfile1 \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nRUN pip install --upgrade --no-cache-dir \\\n    #        \"git+https://github.com/huggingface/transformers.git\" \\\n    \"librosa>=0.10,<0.11\"\n```\n\n### \ud83d\udc1b Describe the bug\n\n- Cannot start image, see log below.\n- Whisper worked on 0.9.1 (regression)\n- Would also be nice to add librosa to standard image - it doesnt make the image any larger (in relation to >20 GB...)\n\n\n\nLog:\n```\n$ docker logs -f vllm-whisper-large-v3\nINFO 07-09 02:08:41 [__init__.py:244] Automatically detected platform cuda.\nINFO 07-09 02:08:44 [api_server.py:1395] vLLM API server version 0.9.2\nINFO 07-09 02:08:44 [cli_args.py:325] non-default args: {'port': 8006, 'model': 'openai/whisper-large-v3', 'gpu_memory_utilization': 0.4, 'swap_space': 5.0, 'disable_log_requests': True}\nINFO 07-09 02:08:49 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed', 'transcription'}. Defaulting to 'transcription'.\nINFO 07-09 02:08:49 [config.py:1472] Using max model len 448\nWARNING 07-09 02:08:49 [arg_utils.py:1735] ['WhisperForConditionalGeneration'] is not supported by the V1 Engine. Falling back to V0.\nINFO 07-09 02:08:50 [api_server.py:268] Started engine process with PID 266\nINFO 07-09 02:08:53 [__init__.py:244] Automatically detected platform cuda.\nINFO 07-09 02:08:54 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='openai/whisper-large-v3', speculative_config=None, tokenizer='openai/whisper-large-v3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=448, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=openai/whisper-large-v3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=True,\nINFO 07-09 02:08:55 [cuda.py:363] Using Flash Attention backend.\nINFO 07-09 02:08:55 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nINFO 07-09 02:08:55 [model_runner.py:1171] Starting to load model openai/whisper-large-v3...\nINFO 07-09 02:08:56 [weight_utils.py:292] Using model weights format ['*.safetensors']\nINFO 07-09 02:08:57 [weight_utils.py:345] No model.safetensors.index.json found in remote.\nLoading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  33% Completed | 1/3 [00:03<00:07,  3.90s/it]\nLoading safetensors checkpoint shards:  67% Completed | 2/3 [00:10<00:05,  5.75s/it]\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:12<00:00,  3.98s/it]\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:12<00:00,  4.27s/it]\n\nINFO 07-09 02:09:09 [default_loader.py:272] Loading weights took 12.84 seconds\nINFO 07-09 02:09:10 [model_runner.py:1203] Model loading took 2.8764 GiB and 13.830811 seconds\nProcess SpawnProcess-1:\nERROR 07-09 02:09:12 [engine.py:458] Received a CachedWhisperTokenizerFast for argument tokenizer, but a WhisperTokenizer was expected.\nERROR 07-09 02:09:12 [engine.py:458] Traceback (most recent call last):\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 446, in run_mp_engine\nERROR 07-09 02:09:12 [engine.py:458]     engine = MQLLMEngine.from_vllm_config(\nERROR 07-09 02:09:12 [engine.py:458]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 133, in from_vllm_config\nERROR 07-09 02:09:12 [engine.py:458]     return cls(\nERROR 07-09 02:09:12 [engine.py:458]            ^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 87, in __init__\nERROR 07-09 02:09:12 [engine.py:458]     self.engine = LLMEngine(*args, **kwargs)\nERROR 07-09 02:09:12 [engine.py:458]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 268, in __init__\nERROR 07-09 02:09:12 [engine.py:458]     self._initialize_kv_caches()\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 413, in _initialize_kv_caches\nERROR 07-09 02:09:12 [engine.py:458]     self.model_executor.determine_num_available_blocks())\nERROR 07-09 02:09:12 [engine.py:458]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 104, in determine_num_available_blocks\nERROR 07-09 02:09:12 [engine.py:458]     results = self.collective_rpc(\"determine_num_available_blocks\")\nERROR 07-09 02:09:12 [engine.py:458]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\nERROR 07-09 02:09:12 [engine.py:458]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 07-09 02:09:12 [engine.py:458]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py\", line 2736, in run_method\nERROR 07-09 02:09:12 [engine.py:458]     return func(*args, **kwargs)\nERROR 07-09 02:09:12 [engine.py:458]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 07-09 02:09:12 [engine.py:458]     return func(*args, **kwargs)\nERROR 07-09 02:09:12 [engine.py:458]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 256, in determine_num_available_blocks\nERROR 07-09 02:09:12 [engine.py:458]     self.model_runner.profile_run()\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 07-09 02:09:12 [engine.py:458]     return func(*args, **kwargs)\nERROR 07-09 02:09:12 [engine.py:458]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/enc_dec_model_runner.py\", line 312, in profile_run\nERROR 07-09 02:09:12 [engine.py:458]     max_mm_tokens = self.mm_registry.get_max_multimodal_tokens(\nERROR 07-09 02:09:12 [engine.py:458]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/registry.py\", line 183, in get_max_multimodal_tokens\nERROR 07-09 02:09:12 [engine.py:458]     return sum(self.get_max_tokens_by_modality(model_config).values())\nERROR 07-09 02:09:12 [engine.py:458]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/registry.py\", line 170, in get_max_tokens_by_modality\nERROR 07-09 02:09:12 [engine.py:458]     mm_limits = self.get_mm_limits_per_prompt(model_config)\nERROR 07-09 02:09:12 [engine.py:458]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/registry.py\", line 206, in get_mm_limits_per_prompt\nERROR 07-09 02:09:12 [engine.py:458]     processor = self.create_processor(model_config, disable_cache=False)\nERROR 07-09 02:09:12 [engine.py:458]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/registry.py\", line 281, in create_processor\nERROR 07-09 02:09:12 [engine.py:458]     return factories.build_processor(ctx, cache=cache)\nERROR 07-09 02:09:12 [engine.py:458]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/registry.py\", line 88, in build_processor\nERROR 07-09 02:09:12 [engine.py:458]     return self.processor(info, dummy_inputs_builder, cache=cache)\nERROR 07-09 02:09:12 [engine.py:458]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/processing.py\", line 1152, in __init__\nERROR 07-09 02:09:12 [engine.py:458]     self.data_parser = self._get_data_parser()\nERROR 07-09 02:09:12 [engine.py:458]                        ^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/whisper.py\", line 680, in _get_data_parser\nERROR 07-09 02:09:12 [engine.py:458]     feature_extractor = self.info.get_feature_extractor()\nERROR 07-09 02:09:12 [engine.py:458]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/whisper.py\", line 643, in get_feature_extractor\nERROR 07-09 02:09:12 [engine.py:458]     hf_processor = self.get_hf_processor()\nERROR 07-09 02:09:12 [engine.py:458]                    ^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/whisper.py\", line 637, in get_hf_processor\nERROR 07-09 02:09:12 [engine.py:458]     return self.ctx.get_hf_processor(WhisperProcessor)\nERROR 07-09 02:09:12 [engine.py:458]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/inputs/registry.py\", line 138, in get_hf_processor\nERROR 07-09 02:09:12 [engine.py:458]     return super().get_hf_processor(\nERROR 07-09 02:09:12 [engine.py:458]            ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/inputs/registry.py\", line 96, in get_hf_processor\nERROR 07-09 02:09:12 [engine.py:458]     return cached_processor_from_config(\nERROR 07-09 02:09:12 [engine.py:458]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/transformers_utils/processor.py\", line 110, in cached_processor_from_config\nERROR 07-09 02:09:12 [engine.py:458]     return cached_get_processor(\nERROR 07-09 02:09:12 [engine.py:458]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/transformers_utils/processor.py\", line 72, in get_processor\nERROR 07-09 02:09:12 [engine.py:458]     processor = processor_factory.from_pretrained(\nERROR 07-09 02:09:12 [engine.py:458]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/transformers/processing_utils.py\", line 1308, in from_pretrained\nERROR 07-09 02:09:12 [engine.py:458]     return cls.from_args_and_dict(args, processor_dict, **kwargs)\nERROR 07-09 02:09:12 [engine.py:458]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/transformers/processing_utils.py\", line 1109, in from_args_and_dict\nERROR 07-09 02:09:12 [engine.py:458]     processor = cls(*args, **valid_kwargs)\nERROR 07-09 02:09:12 [engine.py:458]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/whisper/processing_whisper.py\", line 41, in __init__\nERROR 07-09 02:09:12 [engine.py:458]     super().__init__(feature_extractor, tokenizer)\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/transformers/processing_utils.py\", line 551, in __init__\nERROR 07-09 02:09:12 [engine.py:458]     self.check_argument_for_proper_class(attribute_name, arg)\nERROR 07-09 02:09:12 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/transformers/processing_utils.py\", line 569, in check_argument_for_proper_class\nERROR 07-09 02:09:12 [engine.py:458]     raise TypeError(\nERROR 07-09 02:09:12 [engine.py:458] TypeError: Received a CachedWhisperTokenizerFast for argument tokenizer, but a WhisperTokenizer was expected.\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 460, in run_mp_engine\n    raise e from None\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 446, in run_mp_engine\n    engine = MQLLMEngine.from_vllm_config(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 133, in from_vllm_config\n    return cls(\n           ^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 87, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 268, in __init__\n    self._initialize_kv_caches()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 413, in _initialize_kv_caches\n    self.model_executor.determine_num_available_blocks())\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 104, in determine_num_available_blocks\n    results = self.collective_rpc(\"determine_num_available_blocks\")\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py\", line 2736, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 256, in determine_num_available_blocks\n    self.model_runner.profile_run()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/enc_dec_model_runner.py\", line 312, in profile_run\n    max_mm_tokens = self.mm_registry.get_max_multimodal_tokens(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/registry.py\", line 183, in get_max_multimodal_tokens\n    return sum(self.get_max_tokens_by_modality(model_config).values())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/registry.py\", line 170, in get_max_tokens_by_modality\n    mm_limits = self.get_mm_limits_per_prompt(model_config)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/registry.py\", line 206, in get_mm_limits_per_prompt\n    processor = self.create_processor(model_config, disable_cache=False)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/registry.py\", line 281, in create_processor\n    return factories.build_processor(ctx, cache=cache)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/registry.py\", line 88, in build_processor\n    return self.processor(info, dummy_inputs_builder, cache=cache)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/processing.py\", line 1152, in __init__\n    self.data_parser = self._get_data_parser()\n                       ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/whisper.py\", line 680, in _get_data_parser\n    feature_extractor = self.info.get_feature_extractor()\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/whisper.py\", line 643, in get_feature_extractor\n    hf_processor = self.get_hf_processor()\n                   ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/whisper.py\", line 637, in get_hf_processor\n    return self.ctx.get_hf_processor(WhisperProcessor)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/inputs/registry.py\", line 138, in get_hf_processor\n    return super().get_hf_processor(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/inputs/registry.py\", line 96, in get_hf_processor\n    return cached_processor_from_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/transformers_utils/processor.py\", line 110, in cached_processor_from_config\n    return cached_get_processor(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/transformers_utils/processor.py\", line 72, in get_processor\n    processor = processor_factory.from_pretrained(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/processing_utils.py\", line 1308, in from_pretrained\n    return cls.from_args_and_dict(args, processor_dict, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/processing_utils.py\", line 1109, in from_args_and_dict\n    processor = cls(*args, **valid_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/whisper/processing_whisper.py\", line 41, in __init__\n    super().__init__(feature_extractor, tokenizer)\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/processing_utils.py\", line 551, in __init__\n    self.check_argument_for_proper_class(attribute_name, arg)\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/processing_utils.py\", line 569, in check_argument_for_proper_class\n    raise TypeError(\nTypeError: Received a CachedWhisperTokenizerFast for argument tokenizer, but a WhisperTokenizer was expected.\n[rank0]:[W709 02:09:13.530824767 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1495, in <module>\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1431, in run_server\n    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1451, in run_server_worker\n    async with build_async_engine_client(args, client_config) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 158, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 291, in build_async_engine_client_from_engine_args\n    raise RuntimeError(\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-07-09T09:16:41+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20671/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20671"
  },
  {
    "number": 13969,
    "title": "[Bug]:  Have the same bug with Issue #11762, using vllm>=0.7.2",
    "body": "### Your current environment\n\n\nvllm = 0.7.2 or 0.7.3\n\n\n\n### \ud83d\udc1b Describe the bug\n\nExactly the same bug on Qwen2.5 VL with issue [#11732](https://github.com/vllm-project/vllm/issues/11762)\n```python\nERROR 02-27 18:50:20 engine.py:389] RuntimeError: Failed to apply Qwen2_5_VLProcessor on data={'text': '<|image_pad|><|image_pad|><|image_pad|><|image_pad|><|video_pad|>', 'images': [<PIL.Image.Image image mode=RGB size=3584x3584 at 0x791D887EC2B0>, <PIL.Image.Image image mode=RGB si\nze=3584x3584 at 0x791D887EC2B0>, <PIL.Image.Image image mode=RGB size=3584x3584 at 0x791D887EC2B0>, <PIL.Image.Image image mode=RGB size=3584x3584 at 0x791D887EC2B0>], 'videos': [array([[[[0., 0., 0.],                                                                                   \nERROR 02-27 18:50:20 engine.py:389]          [0., 0., 0.],                                                                                                                                                                                                                                  \nERROR 02-27 18:50:20 engine.py:389]          [0., 0., 0.],                                                                                                                                                                                                                                  \nERROR 02-27 18:50:20 engine.py:389]          ...,                                                                                                                                                                                                                                           \nERROR 02-27 18:50:20 engine.py:389]          [0., 0., 0.],                                                                                                                                                                                                                                  \nERROR 02-27 18:50:20 engine.py:389]          [0., 0., 0.],                                                                                                                                                                                                                                  \nERROR 02-27 18:50:20 engine.py:389]          [0., 0., 0.]],                                                                                                                                                                                                                                 \nERROR 02-27 18:50:20 engine.py:389]                                                                                                                                                                                                                                                         \nERROR 02-27 18:50:20 engine.py:389]         [[0., 0., 0.],                                                                                                                                                                                                                                  \nERROR 02-27 18:50:20 engine.py:389]          [0., 0., 0.],                                                                                                                                                                                                                                  \nERROR 02-27 18:50:20 engine.py:389]          [0., 0., 0.],                                                                                                                                                                                                                                  \nERROR 02-27 18:50:20 engine.py:389]          ...,                                                                                                                                                                                                                                           \nERROR 02-27 18:50:20 engine.py:389]          [0., 0., 0.],                                                                                                                                                                                                                                  \nERROR 02-27 18:50:20 engine.py:389]          [0., 0., 0.],                                                                                                                                                                                                                                  \nERROR 02-27 18:50:20 engine.py:389]          [0., 0., 0.]],         \n.......\n```\n\nThe issue [#11732](https://github.com/vllm-project/vllm/issues/11762) has been closed, but I face the same problem with the latest vllm. Thank you.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-02-27T14:59:01+00:00",
    "closed_at": "2025-03-20T11:14:28+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13969/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13969"
  },
  {
    "number": 5071,
    "title": "[Bug]: Build/Install Issues with pip install -e .",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.3\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.8.10 (default, Nov 22 2023, 10:22:35)  [GCC 9.4.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-182-generic-x86_64-with-glibc2.29\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA L40\r\nGPU 1: NVIDIA A100 80GB PCIe\r\n\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nCPU(s):                             96\r\nOn-line CPU(s) list:                0-95\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 24\r\nSocket(s):                          2\r\nNUMA node(s):                       2\r\nVendor ID:                          GenuineIntel\r\nCPU family:                         6\r\nModel:                              106\r\nModel name:                         Intel(R) Xeon(R) Gold 6336Y CPU @ 2.40GHz\r\nStepping:                           6\r\nFrequency boost:                    enabled\r\nCPU MHz:                            800.000\r\nCPU max MHz:                        2401.0000\r\nCPU min MHz:                        800.0000\r\nBogoMIPS:                           4800.00\r\nVirtualization:                     VT-x\r\nL1d cache:                          2.3 MiB\r\nL1i cache:                          1.5 MiB\r\nL2 cache:                           60 MiB\r\nL3 cache:                           72 MiB\r\nNUMA node0 CPU(s):                  0-23,48-71\r\nNUMA node1 CPU(s):                  24-47,72-95\r\nVulnerability Gather data sampling: Vulnerable\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Vulnerable\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec store bypass:    Vulnerable\r\nVulnerability Spectre v1:           Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:           Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Vulnerable\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.9.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.24.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] triton==2.3.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    NIC0    NIC1    NIC2    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      SYS     PIX     PIX     SYS     0-23,48-71      0               N/A\r\nGPU1    SYS      X      SYS     SYS     PXB     0-23,48-71      0               N/A\r\nNIC0    PIX     SYS      X      PIX     SYS\r\nNIC1    PIX     SYS     PIX      X      SYS\r\nNIC2    SYS     PXB     SYS     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  ```\n\n### \ud83d\udc1b Describe the bug\n\nWhile attempting to build/install the source code using `pip install -e .`, I encountered two issues:\r\n\r\n1. **Dependency Mismatch:**\r\n   There was a dependency mismatch between two packages regarding the `urllib3` version. This issue was resolved by fixing the dependency version of `types-requests==2.31.0` in the `requirements-dev.txt` file.\r\n\r\n2. **Installation Exception:**\r\n   During the `pip install -e .` process, the following exception was raised:\r\n   ```\r\n   raise child_exception_type(errno_num, err_msg, err_filename)\r\n         NotADirectoryError: [Errno 20] Not a directory: 'neuron-ls'\r\n         [end of output]\r\n   ```\r\n   This was resolved by modifying the `_is_neuron()` function in the `setup.py` to the following:\r\n\r\n   ```python\r\n   def _is_neuron() -> bool:\r\n       torch_neuronx_installed = True\r\n       try:\r\n           subprocess.run([\"neuron-ls\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\r\n       except (FileNotFoundError, PermissionError, subprocess.CalledProcessError, NotADirectoryError):\r\n           torch_neuronx_installed = False\r\n       return torch_neuronx_installed or envs.VLLM_BUILD_WITH_NEURON\r\n   ```\r\n\r\nThese changes resolved the build issues, allowing the build to progress successfully.\r\n\r\nIf anyone else has encountered these issues or can confirm them, I can submit a pull request with the proposed fixes.\r\n",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-27T16:34:19+00:00",
    "closed_at": "2024-11-25T02:06:01+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5071/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5071"
  }
]
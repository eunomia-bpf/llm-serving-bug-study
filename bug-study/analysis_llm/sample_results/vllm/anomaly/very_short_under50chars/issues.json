[
  {
    "number": 303,
    "title": "why not support baichuan-7b?",
    "body": null,
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2023-06-29T03:02:32+00:00",
    "closed_at": "2023-07-17T20:50:57+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/303/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/303"
  },
  {
    "number": 3001,
    "title": "Offline Batched Inference with lora?",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-23T02:15:48+00:00",
    "closed_at": "2024-02-27T21:57:14+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3001/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3001"
  },
  {
    "number": 708,
    "title": "Ask the boss: Is there any parameter in vllm that can be passed in history",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-08-09T00:50:00+00:00",
    "closed_at": "2023-08-25T08:33:49+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/708/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/708"
  },
  {
    "number": 3022,
    "title": "8",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-24T15:20:50+00:00",
    "closed_at": "2024-02-24T21:13:38+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3022/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3022"
  },
  {
    "number": 228,
    "title": "When can I support multi graphics cards?",
    "body": "When can I support multi graphics cards?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-06-24T07:51:02+00:00",
    "closed_at": "2023-06-25T17:22:46+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/228/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/228"
  },
  {
    "number": 65,
    "title": "Add documents on how to add new models",
    "body": null,
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2023-05-04T09:05:56+00:00",
    "closed_at": "2023-06-06T03:01:28+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/65/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/65"
  },
  {
    "number": 57,
    "title": "Add code formatting script & Add CI to check code format",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-05-03T02:58:12+00:00",
    "closed_at": "2023-07-03T21:50:58+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/57/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/57"
  },
  {
    "number": 2541,
    "title": "how to usevllm",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-22T06:47:01+00:00",
    "closed_at": "2024-04-04T08:00:31+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2541/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2541"
  },
  {
    "number": 960,
    "title": " how to add history argument\uff1f",
    "body": "when use api_client.py\uff1f",
    "labels": [],
    "state": "closed",
    "created_at": "2023-09-06T06:41:00+00:00",
    "closed_at": "2024-03-13T11:37:52+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/960/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/960"
  },
  {
    "number": 2276,
    "title": "Support `logprobs` for `ChatCompletionRequest` in openai api server?",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-12-27T01:51:50+00:00",
    "closed_at": "2024-02-28T00:42:41+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2276/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2276"
  },
  {
    "number": 9112,
    "title": "[Feature]: Does vLLM support ONNX models?",
    "body": null,
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-07T00:35:40+00:00",
    "closed_at": "2025-03-20T02:04:03+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9112/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/9112"
  },
  {
    "number": 77,
    "title": "Clean up Megatron-LM code",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-05-06T05:03:02+00:00",
    "closed_at": "2023-05-20T15:11:36+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/77/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/77"
  },
  {
    "number": 473,
    "title": "indexSelectLargeIndex: block: [308,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-07-16T04:48:20+00:00",
    "closed_at": "2024-03-08T10:22:53+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/473/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/473"
  },
  {
    "number": 2626,
    "title": "KeyError: 'base_model.model.lm_head.base_layer.weight'",
    "body": "Please help me solve this error",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-01-27T17:03:10+00:00",
    "closed_at": "2024-11-30T02:02:50+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2626/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2626"
  },
  {
    "number": 2047,
    "title": "cuda11.8 for v0.2.4",
    "body": "Hi, there is no cuda11.8 wheel in releases.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-12-12T03:54:54+00:00",
    "closed_at": "2023-12-12T08:51:55+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2047/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2047"
  },
  {
    "number": 1468,
    "title": "[Feature Request] Support to process both prompts and generation in one inference step",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-10-25T08:48:42+00:00",
    "closed_at": "2024-03-13T11:49:42+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1468/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/1468"
  },
  {
    "number": 3141,
    "title": "v100 support int4 \uff08gptq or awq\uff09, Whether it really work?",
    "body": null,
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-01T10:08:34+00:00",
    "closed_at": "2024-12-01T02:15:21+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3141/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3141"
  },
  {
    "number": 889,
    "title": "The AsyncLLMEngine always cache KV",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-08-28T04:43:58+00:00",
    "closed_at": "2023-08-28T06:38:05+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/889/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/889"
  },
  {
    "number": 1827,
    "title": "The default GPU number used is cuda:0, how do I specify the gpu number to use? For example cuda:2, I didn't find anything to change",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-11-29T07:19:07+00:00",
    "closed_at": "2024-03-25T09:57:05+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1827/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1827"
  },
  {
    "number": 910,
    "title": "how to set rope_scaling type to dynamic in vllm?",
    "body": null,
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2023-08-30T09:00:43+00:00",
    "closed_at": "2024-03-09T09:31:12+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/910/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/910"
  },
  {
    "number": 5984,
    "title": "about the RotaryEmbedding",
    "body": null,
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-06-29T09:28:09+00:00",
    "closed_at": "2024-07-01T06:26:31+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5984/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5984"
  },
  {
    "number": 8222,
    "title": "[Installation]: Is there no more \"***.whl\" based on cuda12?",
    "body": "rt",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-06T05:19:58+00:00",
    "closed_at": "2025-01-06T02:02:39+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8222/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8222"
  },
  {
    "number": 2981,
    "title": "\u6c42\u95ee qwen-14b\u5fae\u8c03\u540e\u7684\u6a21\u578b\u7528vllm\u63a8\u7406\u540e\u7ed3\u679c\u90fd\u4e3a\u7a7a ",
    "body": null,
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-02-22T07:49:32+00:00",
    "closed_at": "2024-11-29T02:08:15+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2981/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2981"
  },
  {
    "number": 6518,
    "title": "(not planned)",
    "body": "(not planned)",
    "labels": [
      "RFC"
    ],
    "state": "closed",
    "created_at": "2024-07-17T20:18:52+00:00",
    "closed_at": "2024-07-17T20:21:29+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6518/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/6518"
  },
  {
    "number": 9635,
    "title": "Mamba 2 inference",
    "body": null,
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-10-23T21:51:24+00:00",
    "closed_at": "2024-10-23T21:54:04+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9635/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9635"
  },
  {
    "number": 14121,
    "title": "[Bug]: run v1 engine with cuda graph. raise error.",
    "body": null,
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-03T07:25:18+00:00",
    "closed_at": "2025-03-11T08:15:45+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14121/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14121"
  },
  {
    "number": 1521,
    "title": "How to use vllm to calculate the number of tokens",
    "body": "How to use vllm to calculate the number of tokens",
    "labels": [],
    "state": "closed",
    "created_at": "2023-10-31T11:25:40+00:00",
    "closed_at": "2023-10-31T20:00:28+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1521/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1521"
  },
  {
    "number": 1170,
    "title": "Why don't use flash-attn for prompt's attention? ",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-09-25T02:16:06+00:00",
    "closed_at": "2023-09-27T23:43:58+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1170/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1170"
  },
  {
    "number": 1772,
    "title": "Error from \"Build from source\"",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-11-24T07:32:31+00:00",
    "closed_at": "2023-11-24T07:41:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1772/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1772"
  },
  {
    "number": 2478,
    "title": "How can docker images use the local model in an Intranet environment because I don't have access to HuggingFace ?",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-18T03:27:55+00:00",
    "closed_at": "2024-01-31T07:01:48+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2478/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2478"
  }
]
[
  {
    "number": 20484,
    "title": "[Bug]: Since  #18437 can't serve any Dual Chunked attention model",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\n\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : 14.0.0-1ubuntu1.1\nCMake version                : version 4.0.2\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.9.0.dev20250702+cu128\nIs debug build               : False\nCUDA used to build PyTorch   : 12.8\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0] (64-bit runtime)\nPython platform              : Linux-5.15.0-105-generic-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.8.93\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration :\nGPU 0: GRID A100-40C\nGPU 1: GRID A100-40C\nGPU 2: GRID A100-40C\n\nNvidia driver version        : 570.133.20\ncuDNN version                : Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn.so.8.9.5\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.9.5\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.9.5\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.9.5\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.9.5\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.9.5\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.9.5\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      45 bits physical, 48 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nVendor ID:                          AuthenticAMD\nModel name:                         AMD EPYC 7282 16-Core Processor\nCPU family:                         23\nModel:                              49\nThread(s) per core:                 1\nCore(s) per socket:                 8\nSocket(s):                          4\nStepping:                           0\nBogoMIPS:                           5599.99\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero wbnoinvd arat umip rdpid overflow_recov succor\nHypervisor vendor:                  VMware\nVirtualization type:                full\nL1d cache:                          1 MiB (32 instances)\nL1i cache:                          1 MiB (32 instances)\nL2 cache:                           16 MiB (32 instances)\nL3 cache:                           64 MiB (4 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0-15\nNUMA node1 CPU(s):                  16-31\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Mitigation; untrained return thunk; SMT disabled\nVulnerability Spec rstack overflow: Mitigation; SMT disabled\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu12==12.8.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.8.90\n[pip3] nvidia-cuda-nvrtc-cu12==12.8.93\n[pip3] nvidia-cuda-runtime-cu12==12.8.90\n[pip3] nvidia-cudnn-cu12==9.10.2.21\n[pip3] nvidia-cufft-cu12==11.3.3.83\n[pip3] nvidia-cufile-cu12==1.13.1.3\n[pip3] nvidia-curand-cu12==10.3.9.90\n[pip3] nvidia-cusolver-cu12==11.7.3.90\n[pip3] nvidia-cusparse-cu12==12.5.8.93\n[pip3] nvidia-cusparselt-cu12==0.7.1\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.27.3\n[pip3] nvidia-nvjitlink-cu12==12.8.93\n[pip3] nvidia-nvshmem-cu12==3.2.5\n[pip3] nvidia-nvtx-cu12==12.8.90\n[pip3] onnxruntime==1.22.0\n[pip3] pynvml==12.0.0\n[pip3] pytorch-triton==3.3.1+gitc8757738\n[pip3] pyzmq==27.0.0\n[pip3] sentence-transformers==3.4.1\n[pip3] torch==2.9.0.dev20250702+cu128\n[pip3] torchaudio==2.8.0.dev20250702+cu128\n[pip3] torchvision==0.24.0.dev20250702+cu128\n[pip3] transformers==4.53.0\n[pip3] triton==3.3.0\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.1.dev7393+gb1c1fe3.d20250703 (git sha: b1c1fe3, date: 20250703)\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PIX     PIX     0-31    0-1             N/A\nGPU1    PIX      X      PIX     0-31    0-1             N/A\nGPU2    PIX     PIX      X      0-31    0-1             N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n==============================\n     Environment Variables\n==============================\nLD_LIBRARY_PATH=:/usr/local/cuda-12.8/lib64\nOMP_NUM_THREADS=1\nCUDA_HOME=/usr/local/cuda-12.8/\nCUDA_HOME=/usr/local/cuda-12.8/\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nSince commit 5d6d1adf15aca59cb135853d0f11308af4bbd6e3 and PR #18437 \n\nUsing Dual Chunked Attention with Qwen 1M makes the first prompt request hangs and crashes the `vllm serve` server.\n\n```txt\n\nINFO 07-04 11:32:50 [engine.py:317] Added request chatcmpl-0ff807c9e8744e56b2e69fb3687499a6.\nERROR 07-04 11:33:12 [client.py:307] RuntimeError('Engine process (pid 3607156) died.')\nERROR 07-04 11:33:12 [client.py:307] NoneType: None\nINFO:     Shutting down\nINFO:     Waiting for connections to close. (CTRL+C to force quit)\nERROR 07-04 11:33:15 [serving_chat.py:897] Error in chat completion stream generator.\nERROR 07-04 11:33:15 [serving_chat.py:897] Traceback (most recent call last):\nERROR 07-04 11:33:15 [serving_chat.py:897]   File \"/home/pierre/idextend/vllm_repo/vllm/entrypoints/openai/serving_chat.py\", line 481, in chat_completion_stream_generator\nERROR 07-04 11:33:15 [serving_chat.py:897]     async for res in result_generator:\nERROR 07-04 11:33:15 [serving_chat.py:897]   File \"/home/pierre/idextend/vllm_repo/vllm/engine/multiprocessing/client.py\", line 671, in _process_request\nERROR 07-04 11:33:15 [serving_chat.py:897]     raise request_output\nERROR 07-04 11:33:15 [serving_chat.py:897] vllm.engine.multiprocessing.MQEngineDeadError: Engine loop is not running. Inspect the stacktrace to find the original error: RuntimeError('Engine process (pid 3607156) died.').\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\nINFO:     Finished server process [3607080]\n\n```\n\n`VLLM_ALLOW_LONG_MAX_MODEL_LEN=1 CUDA_VISIBLE_DEVICES=1,2 VLLM_ATTENTION_BACKEND=DUAL_CHUNK_FLASH_ATTN vllm serve  Qwen/Qwen2.5-7B-Instruct-1M   --max-model-len 140000    --max-num-seqs 1 --port 2483 --enforce-eager --gpu-memory-utilization 0.65  --enable-server-load-tracking --enable-prefix-caching`\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-07-04T11:41:42+00:00",
    "closed_at": "2025-07-06T02:38:03+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20484/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20484"
  },
  {
    "number": 21104,
    "title": "[Bug]: Benchmark script is not sending requests to serve with the given request rate",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\nCollecting environment information...                                                                                                                                                   \n==============================                                                                                                                                                          \n        System Info                                                                                                                                                                     \n==============================                                                                                                                                                          \nOS                           : CentOS Stream 9 (x86_64)                                                                                                                                 \nGCC version                  : (GCC) 11.5.0 20240719 (Red Hat 11.5.0-7)                                                                                                                 \nClang version                : Could not collect                                                                                                                                        \nCMake version                : version 4.0.3                                                                                                                                            \nLibc version                 : glibc-2.34                                                                                                                                               \n                                                                                                                                                                                        \n==============================                                                                                                                                                          \n       PyTorch Info                                                                                                                                                                     \n==============================                                                                                                                                                          \nPyTorch version              : 2.7.1+cu128                                                                                                                                              \nIs debug build               : False                                                                                                                                                    \nCUDA used to build PyTorch   : 12.8                                                                                                                                                     \nROCM used to build PyTorch   : N/A                                                                                                                                                      \n                                                                                                                                                                                        \n==============================                                                                                                                                                          \n      Python Environment                                                                                                                                                                \n==============================                                                                                                                                                          \nPython version               : 3.12.10 (main, May  9 2025, 00:00:00) [GCC 11.5.0 20240719 (Red Hat 11.5.0-5)] (64-bit runtime)                                                          \nPython platform              : Linux-6.4.3-0_fbk20_zion_2830_g3e5ab162667d-x86_64-with-glibc2.34                                                                                        \n                                                                                                                                                                                        \n==============================                                                                                                                                                          \n       CUDA / GPU Info                                                                                                                                                                  \n==============================                                                                                                                                                          \nIs CUDA available            : True                                                                                                                                                     \nCUDA runtime version         : 12.4.131                                                                                                                                                 \nCUDA_MODULE_LOADING set to   : LAZY                                                                                                                                                     \nGPU models and configuration :                                                                                                                                                          \nGPU 0: NVIDIA H100                                                                                                                                                                      \nGPU 1: NVIDIA H100                                                                                                                                                                      \nGPU 2: NVIDIA H100                                                                                                                                                                      \nGPU 3: NVIDIA H100                                                                                                                                                                      \nGPU 4: NVIDIA H100                                                                                                                                                                      \nGPU 5: NVIDIA H100                                                                                                                                                                      \nGPU 6: NVIDIA H100                                                                                                                                                                      \nGPU 7: NVIDIA H100                                                                                                                                                                      \n                                                                                                                                                                                        \nNvidia driver version        : 535.154.05                                                                                                                                               \ncuDNN version                : Probably one of the following:                                                                                                                           \n/usr/lib64/libcudnn.so.9.10.1                                                                                                                                                           \n/usr/lib64/libcudnn_adv.so.9.10.1                                                                                                                                                       \n/usr/lib64/libcudnn_cnn.so.9.10.1                                                                                                                                                       \n/usr/lib64/libcudnn_engines_precompiled.so.9.10.1                                                                                                                                       \n/usr/lib64/libcudnn_engines_runtime_compiled.so.9.10.1                                                                                                                                  \n/usr/lib64/libcudnn_graph.so.9.10.1                                                                                                                                                     \n/usr/lib64/libcudnn_heuristic.so.9.10.1                                                                                                                                                 \n/usr/lib64/libcudnn_ops.so.9.10.1                                                                                                                                                       \nHIP runtime version          : N/A                                                                                                                                                      \nMIOpen runtime version       : N/A                                                                                                                                                      \nIs XNNPACK available         : True                                                                                                                                                     \n                \n\n==============================                                                                                                                                                          \n          CPU Info                                                                                                                                                                      \n==============================                                                                                                                                                          \nArchitecture:                       x86_64                                                                                                                                              \nCPU op-mode(s):                     32-bit, 64-bit                                                                                                                                      \nAddress sizes:                      52 bits physical, 57 bits virtual                                                                                                                   \nByte Order:                         Little Endian                                                                                                                                       \nCPU(s):                             384                                                                                                                                                 \nOn-line CPU(s) list:                0-383                                                                                                                                               \nVendor ID:                          AuthenticAMD                                                                                                                                        \nModel name:                         AMD EPYC 9654 96-Core Processor                                                                                                                     \nCPU family:                         25                                                                                                                                                  \nModel:                              17                                                                                                                                                  \nThread(s) per core:                 2                                                                                                                                                   \nCore(s) per socket:                 96                                                                                                                                                  \nSocket(s):                          2                                                                                                                                                   \nStepping:                           1                                                                                                                                                   \nFrequency boost:                    enabled                                                                                                                                             \nCPU(s) scaling MHz:                 83%                                                                                                                                                 \nCPU max MHz:                        3707.8120                                                                                                                                           \nCPU min MHz:                        1500.0000                                                                                                                                           \nBogoMIPS:                           4792.38                                                                                                                                             \nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm co\nnstant_tsc rep_good amd_lbr_v2 nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand \nlahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid\n_single hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt c\nlwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc\n arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx\n512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid overflow_recov succor smca fsrm flush_l1d                                                               \nVirtualization:                     AMD-V                                                                                                                                               \nL1d cache:                          6 MiB (192 instances)                                                                                                                               \nL1i cache:                          6 MiB (192 instances)                                                                                                                               \nL2 cache:                           192 MiB (192 instances)                                                                                                                             \nL3 cache:                           768 MiB (24 instances)                                                                                                                              \nNUMA node(s):                       2                                                                                                                                                   \nNUMA node0 CPU(s):                  0-95,192-287                                                                                                                                        \nNUMA node1 CPU(s):                  96-191,288-383                                                                                                                                      \nVulnerability Gather data sampling: Not affected                                                                                                                                        \nVulnerability Itlb multihit:        Not affected                                                                                                                                        \nVulnerability L1tf:                 Not affected                                                                                                                                        \nVulnerability Mds:                  Not affected                                                                                                                                        \nVulnerability Meltdown:             Not affected                                                                                                                                        \nVulnerability Mmio stale data:      Not affected                                                                                                                                        \nVulnerability Retbleed:             Not affected                                                                                                                                        \nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl                                                                                             \nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization                                                                                \nVulnerability Spectre v2:           Vulnerable: eIBRS with unprivileged eBPF                                                                                                            \nVulnerability Srbds:                Not affected                                                                                                                                        \nVulnerability Tsx async abort:      Not affected                                                                                                                                        \n                                            \n\n==============================                                                                                                                                                          \nVersions of relevant libraries                                                                                                                                                          \n==============================                                                                                                                                                          \n[pip3] mypy-extensions==1.0.0                                                                                                                                                           \n[pip3] numpy==1.26.4                                                                                                                                                                    \n[pip3] nvidia-cublas-cu12==12.8.3.14                                                                                                                                                    \n[pip3] nvidia-cuda-cupti-cu12==12.8.57                                                                                                                                                  \n[pip3] nvidia-cuda-nvrtc-cu12==12.8.61                                                                                                                                                  \n[pip3] nvidia-cuda-runtime-cu12==12.8.57                                                                                                                                                \n[pip3] nvidia-cudnn-cu12==9.7.1.26                                                                                                                                                      \n[pip3] nvidia-cufft-cu12==11.3.3.41                                                                                                                                                     \n[pip3] nvidia-cufile-cu12==1.13.0.11                                                                                                                                                    \n[pip3] nvidia-curand-cu12==10.3.9.55\n[pip3] nvidia-cusolver-cu12==11.7.2.55\n[pip3] nvidia-cusparse-cu12==12.5.7.53\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.8.61\n[pip3] nvidia-nvtx-cu12==12.8.55\n[pip3] pyzmq==27.0.0\n[pip3] sentence-transformers==3.2.1\n[pip3] torch==2.7.1+cu128\n[pip3] torchaudio==2.7.1+cu128\n[pip3] torchvision==0.22.1+cu128\n[pip3] transformers==4.53.2\n[pip3] transformers-stream-generator==0.0.5\n[pip3] triton==3.3.1\n[pip3] tritonclient==2.51.0\n[pip3] vector-quantize-pytorch==1.21.2\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.2rc2.dev308+gc5b8b5953 (git sha: c5b8b5953)\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    PHB     PHB     SYS     SYS     0-95,192-287    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     96-191,288-383  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     96-191,288-383  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     PHB     PHB     96-191,288-383  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     96-191,288-383  1               N/A\nNIC0    SYS     PHB     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     SYS     SYS\nNIC1    SYS     PHB     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      SYS     SYS\nNIC2    SYS     SYS     SYS     SYS     SYS     SYS     PHB     SYS     SYS     SYS      X      PIX\nNIC3    SYS     SYS     SYS     SYS     SYS     SYS     PHB     SYS     SYS     SYS     PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n\n==============================\n     Environment Variables\n==============================\nCUDA_CACHE_PATH=/data/users/jialino/.nv/ComputeCache\nLD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64/:/usr/local/cuda-12.4/lib64/:\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n</details>\n\n### \ud83d\udc1b Describe the bug\n\nBenchmark script sends out requests slower than expected. Roughly 9% slower (183.52 actual QPS while target is set to 200)\n\n# Issue Details\n\nIn the benchmark script, we're leveraging random generated intervals which follow gamma distributions to simulate poisson process with the given sample_rate.\n\nThe random intervals define the time gap among request sends. However, in the current implementation, we do the sleep directly against the random intervals without considering actual elapsed time. This would introduce a small delay for each request and ultimately build up non-negligible gaps to affect server throughput readings.\n\nhttps://github.com/vllm-project/vllm/blob/fdc5b43d2017640a74f89c42ef61e1c79b4ffdd3/vllm/benchmarks/serve.py#L163-L165\n\nBy plotting the actual request sent time vs expected request sent time (with request-rate=200 and num-prompts=10000), we could see that there're delays for actual sent time, which results in smaller request sent rate and ultimately throughput readings.\n\n<img width=\"945\" height=\"637\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/cad015f8-fc01-4d22-83dc-514ac3d82d47\" />\n\nThe impact is significant, we could see that request throughput is 183.52 when we have request-rate set to 200. The bottleneck is actually on benchmark scripts side instead of vllm server side (i.e. benchmark scripts does NOT send requests as expected)\n\n<img width=\"361\" height=\"296\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8fab4d50-f882-4bc4-9945-5fdcc0823ac3\" />\n\n# Repro\n```\nexport VLLM_USE_MODELSCOPE=False;\nvllm serve facebook/opt-125m \\\n    --swap-space 16 \\\n    --disable-log-requests \\\n    --host :: \\\n    --dtype float16\n\nvllm bench serve \\\n    --dataset-name random \\\n    --model facebook/opt-125m \\\n    --served-model-name facebook/opt-125m \\\n    --random-input-len 700 \\\n    --random-output-len 1 \\\n    --endpoint /v1/completions \\\n    --ignore-eos \\\n    --host localhost \\\n    --port 8000 \\\n    --request-rate 200 \\\n    --num-prompts 10000\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-07-17T08:26:29+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/21104/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/21104"
  },
  {
    "number": 20993,
    "title": "[Bug]: Is image embedding supported in llama 4",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI want to run inference of a [Llama-4-Maverick](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8) by directly passing the image embedding, following the instruction [here](https://docs.vllm.ai/en/latest/features/multimodal_inputs.html#embedding-inputs). But I am observing the following error message which appears that the execution is trying to process the image embedding as if it is an image.\n\nAfter some code search I figured that passing image embedding is [not supported](https://github.com/vllm-project/vllm/blob/68d28e37b0d3706601b0d5231178cebaad032605/vllm/model_executor/models/mllama.py#L1395-L1396) whereas the Llava model in the example does have [type](https://github.com/vllm-project/vllm/blob/e7e3e6d2636f6cd012c7ffeff773b20b3c90b958/vllm/model_executor/models/llava.py#L71-L81) for embedding input.\n\nI wonder if passing image embedding to llama 4 is not supported in vLLM. And if not, how can I properly pass it to vLLM?\n\n```\nTraceback (most recent call last):\n  File \"/opt/vllm/vllm/vllm/inputs/registry.py\", line 169, in call_hf_processor\n    output = hf_processor(**data, **merged_kwargs, return_tensors=\"pt\")\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/transformers/models/llama4/processing_llama4.py\", line 192, in __call__\n    image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/transformers/image_processing_utils_fast.py\", line 614, in __call__\n    return self.preprocess(images, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/transformers/models/llama4/image_processing_llama4_fast.py\", line 382, in preprocess\n    return super().preprocess(images, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/transformers/image_processing_utils_fast.py\", line 630, in preprocess\n    images = self._prepare_input_images(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/transformers/image_processing_utils_fast.py\", line 539, in _prepare_input_images\n    processed_images = [process_image_partial(img) for img in images]\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/transformers/image_processing_utils_fast.py\", line 490, in _process_image\n    input_data_format = infer_channel_dimension_format(image)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/transformers/image_utils.py\", line 330, in infer_channel_dimension_format\n    raise ValueError(\"Unable to infer channel dimension format\")\nValueError: Unable to infer channel dimension format\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/dynamo/venv/lib/python3.12/site-packages/dynamo/runtime/__init__.py\", line 85, in wrapper\n    async for item in func(*args_list, **kwargs):\n  File \"/workspace/dynamo/examples/multimodal_v1/components/worker.py\", line 261, in generate\n    async for response in gen:\n  File \"/opt/vllm/vllm/vllm/v1/engine/async_llm.py\", line 309, in generate\n    q = await self.add_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm/vllm/v1/engine/async_llm.py\", line 237, in add_request\n    prompt_str, request = self.processor.process_inputs(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm/vllm/v1/engine/processor.py\", line 241, in process_inputs\n    processed_inputs: ProcessorInputs = self.input_preprocessor.preprocess(\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm/vllm/inputs/preprocess.py\", line 870, in preprocess\n    return self._process_decoder_only_prompt(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm/vllm/inputs/preprocess.py\", line 812, in _process_decoder_only_prompt\n    prompt_comps = self._prompt_to_llm_inputs(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm/vllm/inputs/preprocess.py\", line 489, in _prompt_to_llm_inputs\n    return self._process_tokens(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm/vllm/inputs/preprocess.py\", line 349, in _process_tokens\n    inputs = self._process_multimodal(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm/vllm/inputs/preprocess.py\", line 283, in _process_multimodal\n    return mm_processor.apply(prompt, mm_data, mm_processor_kwargs,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm/vllm/multimodal/processing.py\", line 1808, in apply\n    ) = self._cached_apply_hf_processor(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm/vllm/multimodal/processing.py\", line 1574, in _cached_apply_hf_processor\n    ) = self._apply_hf_processor_main(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm/vllm/multimodal/processing.py\", line 1421, in _apply_hf_processor_main\n    mm_kwargs = self._apply_hf_processor_mm_only(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm/vllm/multimodal/processing.py\", line 1382, in _apply_hf_processor_mm_only\n    _, mm_kwargs, _ = self._apply_hf_processor_text_mm(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm/vllm/multimodal/processing.py\", line 1312, in _apply_hf_processor_text_mm\n    processed_data = self._call_hf_processor(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm/vllm/model_executor/models/mllama4.py\", line 582, in _call_hf_processor\n    processed_outputs = super()._call_hf_processor(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm/vllm/multimodal/processing.py\", line 1275, in _call_hf_processor\n    return self.info.ctx.call_hf_processor(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm/vllm/inputs/registry.py\", line 187, in call_hf_processor\n    raise ValueError(msg) from exc\nValueError: Failed to apply Llama4Processor on data={'text': '<|image|>', 'images': [tensor([[[ 9.6436e-02, -2.5269e-02, -1.1536e-02,  ...,  1.9678e-01,\n           8.5083e-02, -3.2867e-02],\n         [-3.6469e-02, -5.9845e-02, -7.0374e-02,  ..., -1.8591e-01,\n          -5.3596e-03,  2.1561e-02],\n         [-2.5894e-02, -6.5002e-02,  1.7227e-02,  ..., -2.4023e-01,\n           1.6541e-02, -1.3306e-01],\n         ...,\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-07-15T15:30:23+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20993/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20993"
  },
  {
    "number": 20478,
    "title": "[Bug]: Qwen3 Rerank \u6a21\u578b\u7684\u51c6\u786e\u7387\u5b58\u5728\u95ee\u9898",
    "body": "### Your current environment\n\n<details>\n\u6309\u7167\u5b98\u7f51\u4e0b\u8f7d\u6700\u65b0\u7248\u7684vllm pip\u5305\uff0cdaily\u7684\uff0c0.9.2rc\u7248\u672c main\u5206\u652f\nGPU\u5361\u4e3a H20\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n<details>\n\u542f\u52a8\u547d\u4ee4\uff1a\npython3 -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 8181 --served-model-name Qwen3-Rerank --model /mnt/data/.t1/dianjin-0701/Qwen3-Reranker-0.6B --task score --hf_overrides '{\"architectures\": [\"Qwen3ForSequenceClassification\"],\"classifier_from_token\": [\"no\", \"yes\"],\"is_original_qwen3_reranker\": true}' &> start.log &\ncurl\u547d\u4ee4\uff1a\ncurl http://127.0.0.1:8181/v1/rerank \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"model\": \"Qwen3-Rerank\",\n    \"query\": \"\u4ec0\u4e48\u662f\u673a\u5668\u5b66\u4e60\uff1f\",\n    \"documents\": [\n      \"\u673a\u5668\u5b66\u4e60\u662f\u4eba\u5de5\u667a\u80fd\u7684\u4e00\u4e2a\u5206\u652f\uff0c\u901a\u8fc7\u7b97\u6cd5\u8ba9\u8ba1\u7b97\u673a\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u6a21\u5f0f\",\n      \"\u673a\u5668\u5b66\u4e60\u662f\u4e00\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u7528\u4e8e\u5f00\u53d1\u7f51\u7ad9\",\n      \"\u673a\u5668\u5b66\u4e60\u662f\u6570\u636e\u5e93\u7ba1\u7406\u7cfb\u7edf\u7684\u4e00\u79cd\",\n      \"\u673a\u5668\u5b66\u4e60\u662f\u64cd\u4f5c\u7cfb\u7edf\u7684\u4e00\u79cd\u7c7b\u578b\"\n    ],\n    \"top_n\": 2\n  }'\n\u7ed3\u679c\uff1a\n\n![Image](https://github.com/user-attachments/assets/7a269fd1-b3fd-49c4-9d07-a7bff619e3f5)\n\u4f7f\u7528bge-rerank-v2-m3\u7248\u672c\u6a21\u578b\u8fdb\u884c\u540c\u6837\u6d4b\u8bd5\n\u542f\u52a8\u547d\u4ee4\uff1a\npython3 -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 8181 --served-model-name bge-rerank --model /mnt/data/.t1/dianjin-0701/bge-reranker-v2-m3/ --task score  &> start.log &\ncurl\u7ed3\u679c\uff1a\n\n![Image](https://github.com/user-attachments/assets/e864300b-7e96-4256-9523-17d946b47368)\n\n\u5f88\u660e\u663eQwen3-rerank-0.6B\u6a21\u578b\u5728\u76ee\u524d\u7684\u4f7f\u7528\u4e2d\u6709\u95ee\u9898\uff0c\u8c01\u80fd\u5e2e\u6211\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\n\n</details>\n\n\n### Before submitting a new issue...\n\n- [ ] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-07-04T08:28:27+00:00",
    "closed_at": "2025-07-09T01:23:38+00:00",
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20478/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20478"
  },
  {
    "number": 20293,
    "title": "[Feature]: rest api multimodal placeholder prompts",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nDear vllm community, there is an interest at Spotify in vllm supporting a more flexible multi modal rest api. \n\nI am interested in being able to prompt vllm with a textual prompt templated with certain placeholders that are to be populated with embed vectors. Today this seems to be supported through [Multimodal inputs](https://docs.vllm.ai/en/latest/features/multimodal_inputs.html) at the vllm library level. \n\nHowever it is not accessible at the rest api level, at least this is how I'm understanding the code. Is such support something that is on the roadmap for vllm?\n\n### Alternatives\n\nAn alternative I tested out that could work is embed only prompts, which works only on the V0 engine. However ideally it would be good to support text and embeds together.\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-07-01T03:13:28+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20293/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20293"
  },
  {
    "number": 20469,
    "title": "[Performance]: Severe performance drop on 1x A100 80GB with Qwen3-14B-AWQ at >1 concurrency (v0.9.1)",
    "body": "### Report of performance regression\n\n![Image](https://github.com/user-attachments/assets/7dce9a2b-327d-49c2-acfe-9bc12cb2f6a7)\n\nWe observed a significant drop in output tokens per second when serving Qwen/Qwen3-14B-AWQ on a single A100 80GB GPU using vLLM v0.9.1 with --max-model-len 16384.\n\nAt concurrency=1, the model achieves ~52 output tokens per second. However, this drops sharply to ~12 at concurrency=5 and ~3 at concurrency=25. This performance is comparable to or worse than a 2x A30 setup, and significantly below the 2x A100 80GB (TP=2) configuration, which maintains stable output tokens per second (~38) across all concurrency levels.\n\nIn addition, Time-To-First-Token (TTFT) is already high at concurrency=1 (~3345 ms) and increases substantially with concurrency, reaching over 34 seconds at concurrency=25. In contrast, the 2x A100 setup maintains TTFT around ~100 ms across all levels.\n\nvLLM reports a supported max concurrency of 26 for this configuration, so we expected it to handle at least 5 concurrent requests without such severe degradation.\n\nWe tested across vLLM versions 0.8.5 and 0.9.1, with and without --enforce-eager, and using both the v0 engine and FlashInfer backend. The issue persists across all variations.\n\n```\nBenchmark configurations and results:\n\nvllm serve Qwen/Qwen3-14B-AWQ \\\n  --gpu-memory-utilization 0.95 \\\n   --max-model-len 16384 \\\n  --tensor-parallel-size 1 \\\n  --enforce-eager  \\\n  --guided-decoding-backend guidance \\\n  --max-num-seq 30 \n\npython3 vllm/benchmarks/benchmark_serving.py \\\n  --backend vllm \\\n  --model Qwen/Qwen3-14B-AWQ \\\n  --endpoint /v1/completions \\\n  --dataset-name random \\\n  --max-concurrency 1 \\\n  --num-prompts 30 \\\n  --request-rate 1 \\\n  --random-input-len 16000 \\\n  --random-output-len 200 \\\n  --result-dir \"./log/\"\n\n============ Serving Benchmark Result ============\nSuccessful requests:                     30        \nBenchmark duration (s):                  223.53    \nTotal input tokens:                      480000    \nTotal generated tokens:                  6000      \nRequest throughput (req/s):              0.13      \nOutput token throughput (tok/s):         26.84     \nTotal Token throughput (tok/s):          2174.16   \n---------------Time to First Token----------------\nMean TTFT (ms):                          3345.37   \nMedian TTFT (ms):                        3452.00   \nP99 TTFT (ms):                           3696.97   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          20.63     \nMedian TPOT (ms):                        20.94     \nP99 TPOT (ms):                           21.38     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           20.63     \nMedian ITL (ms):                         20.92     \nP99 ITL (ms):                            22.63     \n==================================================\n\npython3 vllm/benchmarks/benchmark_serving.py \\\n  --backend vllm \\\n  --model Qwen/Qwen3-14B-AWQ \\\n  --endpoint /v1/completions \\\n  --dataset-name random \\\n  --max-concurrency 5 \\\n  --num-prompts 30 \\\n  --request-rate 1 \\\n  --random-input-len 16000 \\\n  --random-output-len 200 \\\n  --result-dir \"./log/\" \\\n\n============ Serving Benchmark Result ============\nSuccessful requests:                     30        \nBenchmark duration (s):                  129.77    \nTotal input tokens:                      480000    \nTotal generated tokens:                  6000      \nRequest throughput (req/s):              0.23      \nOutput token throughput (tok/s):         46.24     \nTotal Token throughput (tok/s):          3745.06   \n---------------Time to First Token----------------\nMean TTFT (ms):                          5091.02   \nMedian TTFT (ms):                        5260.32   \nP99 TTFT (ms):                           11491.13  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          79.75     \nMedian TPOT (ms):                        82.64     \nP99 TPOT (ms):                           91.95     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           79.75     \nMedian ITL (ms):                         20.83     \nP99 ITL (ms):                            514.85    \n==================================================\n\npython3 vllm/benchmarks/benchmark_serving.py \\\n  --backend vllm \\\n  --model Qwen/Qwen3-14B-AWQ \\\n  --endpoint /v1/completions \\\n  --dataset-name random \\\n  --max-concurrency 10 \\\n  --num-prompts 30 \\\n  --request-rate 1 \\\n  --random-input-len 16000 \\\n  --random-output-len 200 \\\n  --result-dir \"./log/\"\n\n============ Serving Benchmark Result ============\nSuccessful requests:                     30        \nBenchmark duration (s):                  126.69    \nTotal input tokens:                      480000    \nTotal generated tokens:                  6000      \nRequest throughput (req/s):              0.24      \nOutput token throughput (tok/s):         47.36     \nTotal Token throughput (tok/s):          3836.19   \n---------------Time to First Token----------------\nMean TTFT (ms):                          8517.30   \nMedian TTFT (ms):                        5608.33   \nP99 TTFT (ms):                           26477.54  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          158.78    \nMedian TPOT (ms):                        184.24    \nP99 TPOT (ms):                           193.67    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           158.78    \nMedian ITL (ms):                         28.98     \nP99 ITL (ms):                            561.13    \n==================================================\n\npython3 vllm/benchmarks/benchmark_serving.py \\\n  --backend vllm \\\n  --model Qwen/Qwen3-14B-AWQ \\\n  --endpoint /v1/completions \\\n  --dataset-name random \\\n  --max-concurrency 25 \\\n  --num-prompts 30 \\\n  --request-rate 1 \\\n  --random-input-len 16000 \\\n  --random-output-len 200 \\\n  --result-dir \"./log/\"\n\n============ Serving Benchmark Result ============\nSuccessful requests:                     30        \nBenchmark duration (s):                  134.01    \nTotal input tokens:                      480000    \nTotal generated tokens:                  6000      \nRequest throughput (req/s):              0.22      \nOutput token throughput (tok/s):         44.77     \nTotal Token throughput (tok/s):          3626.72   \n---------------Time to First Token----------------\nMean TTFT (ms):                          34952.05  \nMedian TTFT (ms):                        32292.29  \nP99 TTFT (ms):                           81700.61  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          323.39    \nMedian TPOT (ms):                        342.69    \nP99 TPOT (ms):                           525.98    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           323.39    \nMedian ITL (ms):                         459.58    \nP99 ITL (ms):                            678.63    \n==================================================\n```\n\n### Your current environment (if you think it is necessary)\n\n```text\nThe output of `python collect_env.py`\n\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 24.04.2 LTS (x86_64)\nGCC version                  : (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\nClang version                : Could not collect\nCMake version                : version 3.28.3\nLibc version                 : glibc-2.39\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu126\nIs debug build               : False\nCUDA used to build PyTorch   : 12.6\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0] (64-bit runtime)\nPython platform              : Linux-5.15.0-1074-azure-x86_64-with-glibc2.39\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.8.93\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : \nGPU 0: NVIDIA A100 80GB PCIe\nGPU 1: NVIDIA A100 80GB PCIe\nGPU 2: NVIDIA A100 80GB PCIe\nGPU 3: NVIDIA A100 80GB PCIe\n\nNvidia driver version        : 570.133.20\ncuDNN version                : Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.8.0\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               96\nOn-line CPU(s) list:                  0-95\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 7V13 64-Core Processor\nCPU family:                           25\nModel:                                1\nThread(s) per core:                   1\nCore(s) per socket:                   48\nSocket(s):                            2\nStepping:                             1\nBogoMIPS:                             4890.86\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr rdpru arat umip vaes vpclmulqdq rdpid fsrm\nHypervisor vendor:                    Microsoft\nVirtualization type:                  full\nL1d cache:                            3 MiB (96 instances)\nL1i cache:                            3 MiB (96 instances)\nL2 cache:                             48 MiB (96 instances)\nL3 cache:                             384 MiB (12 instances)\nNUMA node(s):                         4\nNUMA node0 CPU(s):                    0-23\nNUMA node1 CPU(s):                    24-47\nNUMA node2 CPU(s):                    48-71\nNUMA node3 CPU(s):                    72-95\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; safe RET, no microcode\nVulnerability Spec store bypass:      Vulnerable\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.1.2\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-cufile-cu12==1.11.1.6\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-ml-py==12.575.51\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvshmem-cu12==3.3.9\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pynvml==12.0.0\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.0\n[pip3] torchaudio==2.7.0\n[pip3] torchvision==0.22.0\n[pip3] transformers==4.52.4\n[pip3] triton==3.3.0\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.1\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV12    SYS     SYS     0-23    0               N/A\nGPU1    NV12     X      SYS     SYS     24-47   1               N/A\nGPU2    SYS     SYS      X      NV12    48-71   2               N/A\nGPU3    SYS     SYS     NV12     X      72-95   3               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n==============================\n     Environment Variables\n==============================\nNVIDIA_VISIBLE_DEVICES=0,1,2,3\nNVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\nNCCL_VERSION=2.25.1-1\nNVIDIA_DRIVER_CAPABILITIES=all\nNVIDIA_PRODUCT_NAME=CUDA\nCUDA_VERSION=12.8.1\nPYTORCH_VERSION=2.7.0\nLD_LIBRARY_PATH=/usr/local/cuda/lib64\nPYTORCH_INDEX_URL=https://download.pytorch.org/whl/cu128\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance"
    ],
    "state": "open",
    "created_at": "2025-07-04T05:53:42+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20469/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20469"
  },
  {
    "number": 20149,
    "title": "[Feature]: Add Support for Updating Lora Weights",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nWe are using TRL to train an updated version of one of our models which has a modality specific LoRA adapter (i.e., same as granite speech, phi4mm). TRL does have support for integrating into vLLM, but the way it handles adapters doesn't work effectively for this sort of model, because it assumes the lora weights can be merged (e.g., [here](https://github.com/huggingface/trl/blob/79ec242aefedc108de9edbea62be6d95070fde03/trl/trainer/grpo_trainer.py#L924)). \n\nAs far as I know, the way we would currently 'update' the adapter is to save it out and then reload it, e.g., using the [worker lora manager](https://github.com/vllm-project/vllm/blob/main/vllm/lora/worker_manager.py#L86). It would be nice to have a supported way of updating the LoRA tensors being trained without exporting them though.\n\nIf this is possible already, that would be great! Otherwise happy to take a pass at contributing it.  @jeejeelee @avishaiElmakies\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-06-26T20:18:59+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20149/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20149"
  },
  {
    "number": 19991,
    "title": "[Bug]: Error when loading EAGLE3 weight, yuhuili/ EAGLE3-LLaMA3.1-Instruct-8B",
    "body": "### Your current environment\n\nDocker image: rocm/vllm:rocm6.4.1_vllm_0.9.0.1_20250605\n\n\n### \ud83d\udc1b Describe the bug\n\nI'm trying to use vLLM eagle3, using the server launch command as below:\n`vllm serve /models/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/ --trust-remote-code --swap-space 16 --disable-log-requests --tensor-parallel-size 1 --distributed-executor-backend mp --dtype float16 --quantization fp8 --kv-cache-dtype fp8 --no-enable-chunked-prefill --max-num-seqs 300  --max-num-batched-tokens 131072 --gpu-memory-utilization 0.8 --enforce-eager --speculative_config '{\"method\": \"eagle\", \"model\": \"yuhuili/EAGLE3-LLaMA3.1-Instruct-8B\", \"num_speculative_tokens\": 5, \"draft_tensor_parallel_size\": 1, \"dtype\": \"float16\"}'`\n\nFor eagle3 model, I used \"yuhuili/EAGLE3-LLaMA3.1-Instruct-8B\", while there is an error of weight shape misalignment as below:\n\nERROR 06-22 15:12:27 [engine.py:458] Attempted to load weight (torch.Size([4096, 12288])) into parameter (torch.Size([4096, 8192]))\nERROR 06-22 15:12:27 [engine.py:458] Traceback (most recent call last):\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 446, in run_mp_engine\nERROR 06-22 15:12:27 [engine.py:458]     engine = MQLLMEngine.from_vllm_config(\nERROR 06-22 15:12:27 [engine.py:458]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 133, in from_vllm_config\nERROR 06-22 15:12:27 [engine.py:458]     return cls(\nERROR 06-22 15:12:27 [engine.py:458]            ^^^^\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 87, in __init__\nERROR 06-22 15:12:27 [engine.py:458]     self.engine = LLMEngine(*args, **kwargs)\nERROR 06-22 15:12:27 [engine.py:458]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 266, in __init__\nERROR 06-22 15:12:27 [engine.py:458]     self.model_executor = executor_class(vllm_config=vllm_config)\nERROR 06-22 15:12:27 [engine.py:458]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 287, in __init__\nERROR 06-22 15:12:27 [engine.py:458]     super().__init__(*args, **kwargs)\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 53, in __init__\nERROR 06-22 15:12:27 [engine.py:458]     self._init_executor()\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/mp_distributed_executor.py\", line 125, in _init_executor\nERROR 06-22 15:12:27 [engine.py:458]     self._run_workers(\"init_device\")\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/mp_distributed_executor.py\", line 186, in _run_workers\nERROR 06-22 15:12:27 [engine.py:458]     driver_worker_output = run_method(self.driver_worker, sent_method,\nERROR 06-22 15:12:27 [engine.py:458]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2667, in run_method\nERROR 06-22 15:12:27 [engine.py:458]     return func(*args, **kwargs)\nERROR 06-22 15:12:27 [engine.py:458]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 606, in init_device\nERROR 06-22 15:12:27 [engine.py:458]     self.worker.init_device()  # type: ignore\nERROR 06-22 15:12:27 [engine.py:458]     ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/spec_decode/spec_decode_worker.py\", line 355, in init_device\nERROR 06-22 15:12:27 [engine.py:458]     self.proposer_worker.load_model()\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 163, in load_model\nERROR 06-22 15:12:27 [engine.py:458]     self.worker.load_model()\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 210, in load_model\nERROR 06-22 15:12:27 [engine.py:458]     self.model_runner.load_model()\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1174, in load_model\nERROR 06-22 15:12:27 [engine.py:458]     self.model = get_model(vllm_config=self.vllm_config)\nERROR 06-22 15:12:27 [engine.py:458]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 59, in get_model\nERROR 06-22 15:12:27 [engine.py:458]     return loader.load_model(vllm_config=vllm_config,\nERROR 06-22 15:12:27 [engine.py:458]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/base_loader.py\", line 41, in load_model\nERROR 06-22 15:12:27 [engine.py:458]     self.load_weights(model, model_config)\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/default_loader.py\", line 269, in load_weights\nERROR 06-22 15:12:27 [engine.py:458]     loaded_weights = model.load_weights(\nERROR 06-22 15:12:27 [engine.py:458]                      ^^^^^^^^^^^^^^^^^^^\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/eagle.py\", line 222, in load_weights\nERROR 06-22 15:12:27 [engine.py:458]     weight_loader(self.fc.weight, loaded_weight)\nERROR 06-22 15:12:27 [engine.py:458]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/weight_utils.py\", line 614, in default_weight_loader\nERROR 06-22 15:12:27 [engine.py:458]     assert param.size() == loaded_weight.size(), (\nERROR 06-22 15:12:27 [engine.py:458]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-22 15:12:27 [engine.py:458] AssertionError: Attempted to load weight (torch.Size([4096, 12288])) into parameter (torch.Size([4096, 8192]))\n\nThe loaded weight (torch.Size([4096, 12288]) of yuhuili/EAGLE3-LLaMA3.1-Instruct-8B does not align to the EAGLE arch defined in vLLM (torch.Size([4096, 8192]). Wondering if you have met this issue before, and if you know any solutions of this bug.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-06-23T16:30:22+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19991/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19991"
  },
  {
    "number": 20991,
    "title": "[Bug]: Hermes tool call parser: pops empty list",
    "body": "### Your current environment\n\n4xH100-80GiB\n\nCLI Args:\n```\n        # Inference\n        - --model\n        - Qwen/Qwen3-235B-A22B-FP8\n        - --gpu-memory-utilization\n        - \"0.90\"\n        - --disable-custom-all-reduce\n\n        - --rope-scaling.rope_type\n        - \"yarn\"\n        - --rope-scaling.factor\n        - 4\n        - --rope-scaling.original_max_position_embeddings\n        - 32768\n        - --max-model-len\n        - \"131072\"\n        - --tensor-parallel-size\n        - \"4\"\n\n        # Function calling\n        - --enable-auto-tool-choice\n        - --tool-call-parser\n        - hermes\n\n        # Server\n        - --host\n        - \"0.0.0.0\"\n        - --disable-log-requests\n```\n\n\n### \ud83d\udc1b Describe the bug\n\nPops from an empty list in the Hermes tool call parser.\n```\nIndexError: pop from empty list\n  File \"/usr/local/lib/python3.12/dist-packages/partial_json_parser/core/myelin.py\", line 50, in fix_fast\n    _i, _char = stack.pop()\n\n  File \"/usr/local/lib/python3.12/dist-packages/partial_json_parser/core/api.py\", line 22, in ensure_json\n    head, tail = fix_fast(json_string, allow_partial)\n\n  File \"/usr/local/lib/python3.12/dist-packages/partial_json_parser/core/api.py\", line 15, in parse_json\n    return parser(ensure_json(json_string, allow_partial, use_fast_fix))\n\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py\", line 241, in extract_tool_calls\n    current_tool_call = partial_json_parser.loads(...)\n```\n\nUnfortunately, I don't have the client request for you.\n\n<img width=\"1443\" height=\"428\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e570b22a-9a3c-4845-a6ce-0111a9264e05\" />\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-07-15T13:55:52+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20991/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20991"
  },
  {
    "number": 20345,
    "title": "[Usage]: I cannot compile vllm on RTX5090",
    "body": "### Your current environment\n\n\u6211\u7684\u670d\u52a1\u5668\u914d\u7f6e\u662fubuntu20\uff0cconda\u4f7f\u7528\u7684\u73af\u5883\u662fpython 3.12\uff0c\u6211\u7684\u5b89\u88c5\u547d\u4ee4\u5982\u4e0b\uff1a\n```text\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\ngit checkout v0.8.3\npython use_existing_torch.py\npip install -r requirements/build.txt\npip install setuptools_scm\n```\n\u4f46\u662f\u4ed6\u5374\u62a5\u9519\u4e86\uff1a\n```text\n /home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/_distutils/dist.py:1021: _DebuggingTips: Problem in editable installation.\n  !!\n\n          ********************************************************************************\n          An error happened while installing `vllm` in editable mode.\n\n          The following steps are recommended to help debug this problem:\n\n          - Try to install the project normally, without using the editable mode.\n            Does the error still persist?\n            (If it does, try fixing the problem before attempting the editable mode).\n          - If you are using binary extensions, make sure you have all OS-level\n            dependencies installed (e.g. compilers, toolchains, binary libraries, ...).\n          - Try the latest version of setuptools (maybe the error was already fixed).\n          - If you (or your project dependencies) are using any setuptools extension\n            or customization, make sure they support the editable mode.\n\n          After following the steps above, if the problem still persists and\n          you think this is related to how setuptools handles editable installations,\n          please submit a reproducible example\n          (see https://stackoverflow.com/help/minimal-reproducible-example) to:\n\n              https://github.com/pypa/setuptools/issues\n\n          See https://setuptools.pypa.io/en/latest/userguide/development_mode.html for details.\n          ********************************************************************************\n\n  !!\n    cmd_obj.run()\n  Traceback (most recent call last):\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 389, in <module>\n      main()\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 373, in main\n      json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 303, in build_editable\n      return hook(wheel_directory, config_settings, metadata_directory)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/build_meta.py\", line 468, in build_editable\n      return self._build_with_temp_dir(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/build_meta.py\", line 404, in _build_with_temp_dir\n      self.run_setup()\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/build_meta.py\", line 317, in run_setup\n      exec(code, locals())\n    File \"<string>\", line 685, in <module>\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/__init__.py\", line 117, in setup\n      return distutils.core.setup(**attrs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 186, in setup\n      return run_commands(dist)\n             ^^^^^^^^^^^^^^^^^^\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 202, in run_commands\n      dist.run_commands()\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1002, in run_commands\n      self.run_command(cmd)\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/dist.py\", line 1104, in run_command\n      super().run_command(command)\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1021, in run_command\n      cmd_obj.run()\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 139, in run\n      self._create_wheel_file(bdist_wheel)\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 340, in _create_wheel_file\n      files, mapping = self._run_build_commands(dist_name, unpacked, lib, tmp)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 263, in _run_build_commands\n      self._run_build_subcommands()\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 290, in _run_build_subcommands\n      self.run_command(name)\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 357, in run_command\n      self.distribution.run_command(command)\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/dist.py\", line 1104, in run_command\n      super().run_command(command)\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1021, in run_command\n      cmd_obj.run()\n    File \"<string>\", line 268, in run\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/command/build_ext.py\", line 99, in run\n      _build_ext.run(self)\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 368, in run\n      self.build_extensions()\n    File \"<string>\", line 230, in build_extensions\n    File \"<string>\", line 208, in configure\n    File \"/home/pc/data/envs/vllmlast/lib/python3.12/subprocess.py\", line 413, in check_call\n      raise CalledProcessError(retcode, cmd)\n  subprocess.CalledProcessError: Command '['cmake', '/home/pc/data/vllm/vllm', '-G', 'Ninja', '-DCMAKE_BUILD_TYPE=RelWithDebInfo', '-DVLLM_TARGET_DEVICE=cuda', '-DVLLM_PYTHON_EXECUTABLE=/home/pc/data/envs/vllmlast/bin/python3.12', '-DVLLM_PYTHON_PATH=/home/pc/data/envs/vllmlast/lib/python312.zip:/home/pc/data/envs/vllmlast/lib/python3.12:/home/pc/data/envs/vllmlast/lib/python3.12/lib-dynload:/home/pc/data/envs/vllmlast/lib/python3.12/site-packages:/home/pc/data/envs/vllmlast/lib/python3.12/site-packages/setuptools/_vendor', '-DFETCHCONTENT_BASE_DIR=/home/pc/data/vllm/vllm/.deps', '-DNVCC_THREADS=1', '-DCMAKE_JOB_POOL_COMPILE:STRING=compile', '-DCMAKE_JOB_POOLS:STRING=compile=4', '-DCMAKE_CUDA_COMPILER=/usr/bin/nvcc']' returned non-zero exit status 1.\n  error: subprocess-exited-with-error\n  \n  \u00d7 Building editable for vllm (pyproject.toml) did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> See above for output.\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  full command: /home/pc/data/envs/vllmlast/bin/python3.12 /home/pc/data/envs/vllmlast/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py build_editable /tmp/tmp15tbfxdt\n  cwd: /home/pc/data/vllm/vllm\n  Building editable for vllm (pyproject.toml) ... error\n  ERROR: Failed building editable for vllm\nFailed to build vllm\nERROR: Failed to build installable wheels for some pyproject.toml based projects (vllm)\n\n```\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "open",
    "created_at": "2025-07-02T01:10:24+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20345/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20345"
  },
  {
    "number": 20044,
    "title": "[Feature]: Cache EngineCoreOutput for system prompt to prevent repeated calculation",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nMy company is prefixing a common system prompt to all requests we serve through vLLM. Currently, when we call add_request, we send a request with prompt_token_ids including the entire system prompt. Every time we schedule a request, we are forced to schedule these common system prompt tokens over and over again. \n\nThe RFC I propose is, instead of passing the system prompt tokens per-request, we add a function add_system_prompt which schedules a dummy request whose prompt solely consists of the system prompt. We cache the resulting EngineCoreOutputs. Finally, when we start serving requests, we simply set num_computed_tokens to num_tokens_in_sys_prompt // block_size * block_size. To prevent evicting the system prompt KVCacheBlocks, we add a 'permanent' flag to the KVCacheBlock data structure, and set this flag in the add_system_prompt function.\n\nThis feature would significantly help our inference for short context window (max_model_len=2048).\n\nWe are wondering if our reasoning is sound, or if there are issues.\n\n@WoosukKwon @youkaichao @comaniac \n\n### Alternatives\n\n_No response_\n\n### Additional context\n\nWe expect to see a 36% decrease in latency when max_model_len=2048.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-06-24T22:18:36+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20044/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20044"
  },
  {
    "number": 21111,
    "title": "[Bug]: vllm config.py\u62a5\u9519selected_task=next(iter(supported_tasks_lst))",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nvllm 0.6.4.post1\n\u4f7f\u7528cosyvoice2\u62a5\u9519selected_task=next(iter(supported_tasks_lst))\nStopIteration\n\n![Image](https://github.com/user-attachments/assets/3961ca6f-780b-44c2-b920-9f7a30201f89)\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-07-17T09:42:17+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/21111/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/21111"
  },
  {
    "number": 20416,
    "title": "[Usage]: whether torchrun-compatible mode supports DP/EP?",
    "body": "### Your current environment\n\nNone\n\n### How would you like to use vllm\n\nI have read https://github.com/vllm-project/vllm/pull/12071 and it's a wonderful work.\n\nI wonder if this torchrun-compatible executor supports EP? Since the comments in https://github.com/vllm-project/vllm/pull/12071 point out that the input should be same across all ranks (maybe the context is TP).\n\nIn EP scenario, all ranks in the same EP group should have different input to take the advantage of EP MoE. And if DeepEP is enabled, prefill and decode would dispatch to normal kernels and ll kernels separately. This requires schedulers ascross ranks in the same EP group should schedule the same prefill/decode action with different inputs. Are we now ensuring this behavior or this is not necessary in current design?\n\n### Before submitting a new issue...\n\n- [ ] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "open",
    "created_at": "2025-07-03T06:47:37+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20416/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20416"
  },
  {
    "number": 20161,
    "title": "[Feature]: Use LoRA on MoE models",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nHi, I would like to know if the vLLM team has plans to support LoRA on the MoE model?\nI'm currently trying to get this working but don't know if its really necessary.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-06-27T04:00:16+00:00",
    "closed_at": "2025-06-27T06:42:41+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20161/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20161"
  },
  {
    "number": 20304,
    "title": "[Bug]: --max-model-len doesn't work",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nINFO 06-30 23:25:24 [__init__.py:244] Automatically detected platform cuda.\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : Could not collect\nCMake version                : version 4.0.2\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu128\nIs debug build               : False\nCUDA used to build PyTorch   : 12.8\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0] (64-bit runtime)\nPython platform              : Linux-6.5.0-44-generic-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.8.93\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration :\nGPU 0: NVIDIA H100 80GB HBM3\nGPU 1: NVIDIA H100 80GB HBM3\nGPU 2: NVIDIA H100 80GB HBM3\nGPU 3: NVIDIA H100 80GB HBM3\nGPU 4: NVIDIA H100 80GB HBM3\nGPU 5: NVIDIA H100 80GB HBM3\nGPU 6: NVIDIA H100 80GB HBM3\nGPU 7: NVIDIA H100 80GB HBM3\n\nNvidia driver version        : 535.183.01\ncuDNN version                : Could not collect\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      52 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             192\nOn-line CPU(s) list:                0-191\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) Platinum 8468\nCPU family:                         6\nModel:                              143\nThread(s) per core:                 2\nCore(s) per socket:                 48\nSocket(s):                          2\nStepping:                           8\nCPU max MHz:                        3800.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           4200.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                     VT-x\nL1d cache:                          4.5 MiB (96 instances)\nL1i cache:                          3 MiB (96 instances)\nL2 cache:                           192 MiB (96 instances)\nL3 cache:                           210 MiB (2 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0-47,96-143\nNUMA node1 CPU(s):                  48-95,144-191\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu12==12.8.3.14\n[pip3] nvidia-cuda-cupti-cu12==12.8.57\n[pip3] nvidia-cuda-nvrtc-cu12==12.8.61\n[pip3] nvidia-cuda-runtime-cu12==12.8.57\n[pip3] nvidia-cudnn-cu12==9.7.1.26\n[pip3] nvidia-cufft-cu12==11.3.3.41\n[pip3] nvidia-cufile-cu12==1.13.0.11\n[pip3] nvidia-curand-cu12==10.3.9.55\n[pip3] nvidia-cusolver-cu12==11.7.2.55\n[pip3] nvidia-cusparse-cu12==12.5.7.53\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.8.61\n[pip3] nvidia-nvtx-cu12==12.8.55\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.0+cu128\n[pip3] torchaudio==2.7.0+cu128\n[pip3] torchvision==0.22.0+cu128\n[pip3] transformers==4.52.4\n[pip3] triton==3.3.0\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.1\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n  \tGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tNIC4\tNIC5\tNIC6\tNIC7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tPIX\tPXB\tNODE\tNODE\tSYS\tSYSSYS\tSYS\t0-47,96-143\t0\t\tN/A\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tPXB\tPIX\tNODE\tNODE\tSYS\tSYSSYS\tSYS\t0-47,96-143\t0\t\tN/A\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNODE\tNODE\tPIX\tPXB\tSYS\tSYSSYS\tSYS\t0-47,96-143\t0\t\tN/A\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNODE\tNODE\tPXB\tPIX\tSYS\tSYSSYS\tSYS\t0-47,96-143\t0\t\tN/A\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tPIX\tPXBNODE\tNODE\t48-95,144-191\t1\t\tN/A\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tPXB\tPIXNODE\tNODE\t48-95,144-191\t1\t\tN/A\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPIX\tPXB\t48-95,144-191\t1\t\tN/A\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPXB\tPIX\t48-95,144-191\t1\t\tN/A\nNIC0\tPIX\tPXB\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t X \tPXB\tNODE\tNODE\tSYS\tSYSSYS\tSYS\nNIC1\tPXB\tPIX\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tPXB\t X \tNODE\tNODE\tSYS\tSYSSYS\tSYS\nNIC2\tNODE\tNODE\tPIX\tPXB\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\t X \tPXB\tSYS\tSYSSYS\tSYS\nNIC3\tNODE\tNODE\tPXB\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPXB\t X \tSYS\tSYSSYS\tSYS\nNIC4\tSYS\tSYS\tSYS\tSYS\tPIX\tPXB\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t X \tPXBNODE\tNODE\nNIC5\tSYS\tSYS\tSYS\tSYS\tPXB\tPIX\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tPXB\t X NODE\tNODE\nNIC6\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPIX\tPXB\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\t X \tPXB\nNIC7\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPXB\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPXB\t X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n\n==============================\n     Environment Variables\n==============================\nNVIDIA_VISIBLE_DEVICES=all\nNVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\nNCCL_VERSION=2.25.1-1\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nCUDA_VERSION=12.8.1\nLD_LIBRARY_PATH=/usr/local/cuda/lib64\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI am starting the LLM inference service using the `vllm serve` command. When I set `--max-model-len` to 32768, the log shows that `max_model_len` is indeed 32768, but the actual `max_seq_len` of the engine is 16384. When the inference service starts successfully, I am indeed unable to input content longer than 16384, which means my `--max-model-len` setting is not taking effect. This issue did not occur when I was using v9.0.1.\n\nThis is the command that i'm using.\n```bash\nvllm serve /DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tensor-parallel-size 2 --port 9990 --enable-prefix-caching --enable-chunked-prefill --max-model-len 32768\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-07-01T06:57:09+00:00",
    "closed_at": "2025-07-08T07:48:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20304/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20304"
  },
  {
    "number": 20098,
    "title": "[RFC]: Lazy CUDA Graph capture",
    "body": "### Motivation.\n\nCurrently vLLM captures cudagraphs as part of the engine initialization significantly slowing down vLLM startup time. By default, vLLM captures 66 graphs, which depending on model size and GPU type, can take more than 10s. This is not great UX (see #19824 for details).\n\nIn addition, It's most unlikely that all 66 graphs are actually needed, wasting both time and space.  \n\n### Proposed Change.\n\nWe propose to capture cudagraphs lazily. Instead of performing dummy runs during the engine initialization phase, the idea is to do those runs somewhere in the CUDA piecewise backend, and only for the current runtime shape if not cached already.\n\nExact implementation needs to be worked out.\n\n### Feedback Period.\n\none week\n\n### CC List.\n\n@ProExpertProg @aarnphm @charlesfrye  \n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "startup-ux"
    ],
    "state": "open",
    "created_at": "2025-06-25T21:27:51+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20098/reactions",
      "total_count": 4,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 4,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20098"
  },
  {
    "number": 20182,
    "title": "[Feature]: Support for Hunyuan-A13B-Instruct",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nTencent released this new model:\nhttps://huggingface.co/tencent/Hunyuan-A13B-Instruct\n\nIt matches bigger models on benchmarks. It has a decent size to run locally and the MoE architecture should make it pretty fast.\nIt has 256K context too.\n\nThe tencent team released a docker version compatible with vllm 0.8.5 but that image lacks the new improvements. Plus I think it doesn't have the Ampere fp8 marlin support as I can't run the fp8 quant it on a 3090 system\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-06-27T11:36:33+00:00",
    "closed_at": "2025-06-27T15:59:44+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20182/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20182"
  },
  {
    "number": 20458,
    "title": "[Installation]: Solved Bug: CMake `execute_process` fails to read `/proc/cpuinfo` without absolute path",
    "body": "### Your current environment\n\n```text\nUbuntu, the most important is that  all is standard here: \n \nwhich cat\n/bin/cat\n$ file /bin/cat\n/bin/cat: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=70bb40952afe7016b06511e5c96e926f1f4774ba, for GNU/Linux 3.2.0, stripped\n\n```\n\n\nI have quickly searched for `cpuinfo` problem in all issues, including closed ones and it seems to be new one. Gemini CLI solved it and wrote most of below: \n\n### Bug: CMake `execute_process` fails to read `/proc/cpuinfo` without absolute path\n\n**Description:**\nWhen building vLLM with `VLLM_TARGET_DEVICE=cpu`, the CMake configuration fails because the `execute_process` command in `cmake/cpu_extension.cmake` is unable to read `/proc/cpuinfo`. The error message reported is \"Failed to check CPU features via /proc/cpuinfo\".\n\n**Reproduction Steps:**\n1. Clone the vLLM repository.\n2. Run `cmake . -DVLLM_TARGET_DEVICE=cpu -D VLLM_PYTHON_EXECUTABLE=/usr/bin/python3`\n\n**Expected Behavior:**\nThe CMake configuration should complete successfully, detecting CPU features from `/proc/cpuinfo`.\n\n**Observed Behavior:**\nThe CMake configuration fails with the error: \"Failed to check CPU features via /proc/cpuinfo\".\n\n**Root Cause:**\nThe `execute_process(COMMAND cat /proc/cpuinfo ...)` call in `cmake/cpu_extension.cmake` does not use the absolute path for the `cat` command. While `cat /proc/cpuinfo` works correctly in a standard shell environment, CMake's `execute_process` might have a different PATH or environment, preventing it from locating `cat`.\n\n**Solution:**\nModify `cmake/cpu_extension.cmake` to use the absolute path for `cat` (e.g., `/bin/cat`).\n\n**Proposed Fix (already applied and verified):**\n```diff\n--- a/cmake/cpu_extension.cmake\n+++ b/cmake/cpu_extension.cmake\n@@ -45,7 +45,7 @@\n \n if (NOT MACOSX_FOUND)\n     execute_process(COMMAND cat /proc/cpuinfo\n-                    RESULT_VARIABLE CPUINFO_RET\n-                    OUTPUT_VARIABLE CPUINFO)\n+                    RESULT_VARIABLE CPUINFO_RET\n+                    OUTPUT_VARIABLE CPUINFO)\n     if (NOT CPUINFO_RET EQUAL 0)\n         message(FATAL_ERROR \"Failed to check CPU features via /proc/cpuinfo\")\n     endif()\n```\n\n(Note: The diff above is illustrative. The actual fix involved changing `COMMAND cat /proc/cpuinfo` to `COMMAND /bin/cat /proc/cpuinfo`.)\n\n\n### How you are installing vllm\n\n```sh\ngit cloning and struggling with some other bugs too that are more banal\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "open",
    "created_at": "2025-07-03T23:19:58+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20458/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20458"
  },
  {
    "number": 21001,
    "title": "[Usage]: Disable the http request log for /metrics",
    "body": "### Your current environment\n\nvLLM 0.9.2\nDocker\nPrometheus + Grafana\nWindows 11 (Also an Ubuntu Server)\n\n\n### How would you like to use vllm\n\nHello,\n\nI am using the latest release 0.9.2 with Prometheus + Grafana (with Docker). For the dashboard and related configs, I am using the official documentation example. I have two questions:\n\n1. How can I disable the logging of the HTTP requests for the /metrics endpoint only? Because it is cluttering the entire log and making things hard to follow. I still want to see the log of the other http endpoints.\n\n2. I am able to see that Prometheus is running fine, and I can connect to it from Grafana successfully, but using the official Grafana dashboard, it displays 'No Data' even though there are requests made to the server. Is the dashboard JSON template up-to-date?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "open",
    "created_at": "2025-07-15T17:10:47+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/21001/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/21001"
  },
  {
    "number": 21096,
    "title": "[Bug]: The same query yields different results across different vLLM versions under identical reasoning.",
    "body": "### Your current environment\n\nI've tried three different version of vllm: 0.6.5, 0.8.4, 0.9.2. For example, in the environment, I only ran `pip install vllm==0.6.5`.\nThe output after running the `collect_env.py`:\n```\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : Could not collect\nCMake version                : Could not collect\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu126\nIs debug build               : False\nCUDA used to build PyTorch   : 12.6\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:09:17) [GCC 11.2.0] (64-bit runtime)\nPython platform              : Linux-5.15.0-94-generic-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.4.99\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : \nGPU 0: NVIDIA A800-SXM4-40GB\nGPU 1: NVIDIA A800-SXM4-40GB\nGPU 2: NVIDIA A800-SXM4-40GB\nGPU 3: NVIDIA A800-SXM4-40GB\nGPU 4: NVIDIA A800-SXM4-40GB\nGPU 5: NVIDIA A800-SXM4-40GB\nGPU 6: NVIDIA A800-SXM4-40GB\nGPU 7: NVIDIA A800-SXM4-40GB\n\nNvidia driver version        : 535.183.06\ncuDNN version                : Could not collect\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      52 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             128\nOn-line CPU(s) list:                0-127\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) Platinum 8378A CPU @ 3.00GHz\nCPU family:                         6\nModel:                              106\nThread(s) per core:                 2\nCore(s) per socket:                 32\nSocket(s):                          2\nStepping:                           6\nCPU max MHz:                        3500.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           6000.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\nVirtualization:                     VT-x\nL1d cache:                          3 MiB (64 instances)\nL1i cache:                          2 MiB (64 instances)\nL2 cache:                           80 MiB (64 instances)\nL3 cache:                           96 MiB (2 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0-31,64-95\nNUMA node1 CPU(s):                  32-63,96-127\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-cufile-cu12==1.11.1.6\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pyzmq==27.0.0\n[pip3] torch==2.7.0\n[pip3] torchaudio==2.7.0\n[pip3] torchvision==0.22.0\n[pip3] transformers==4.53.2\n[pip3] triton==3.3.0\n[conda] numpy                     2.2.6                    pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.6.4.1                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.6.80                  pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.6.77                  pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.6.77                  pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.5.1.17                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.3.0.4                 pypi_0    pypi\n[conda] nvidia-cufile-cu12        1.11.1.6                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.7.77                pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.7.1.2                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.5.4.2                 pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.3                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.26.2                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.6.85                  pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.6.77                  pypi_0    pypi\n[conda] pyzmq                     27.0.0                   pypi_0    pypi\n[conda] torch                     2.7.0                    pypi_0    pypi\n[conda] torchaudio                2.7.0                    pypi_0    pypi\n[conda] torchvision               0.22.0                   pypi_0    pypi\n[conda] transformers              4.53.2                   pypi_0    pypi\n[conda] triton                    3.3.0                    pypi_0    pypi\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.2\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV8     NV8     NV8     NV8     NV8     NV8     NV8     PXB     PXB     NODE    SYS     SYS     SYS     SYS     NODE    0-31,64-95      0               N/A\nGPU1    NV8      X      NV8     NV8     NV8     NV8     NV8     NV8     PXB     PXB     NODE    SYS     SYS     SYS     SYS     NODE    0-31,64-95      0               N/A\nGPU2    NV8     NV8      X      NV8     NV8     NV8     NV8     NV8     NODE    NODE    PXB     SYS     SYS     SYS     SYS     PXB     0-31,64-95      0               N/A\nGPU3    NV8     NV8     NV8      X      NV8     NV8     NV8     NV8     NODE    NODE    PXB     SYS     SYS     SYS     SYS     PXB     0-31,64-95      0               N/A\nGPU4    NV8     NV8     NV8     NV8      X      NV8     NV8     NV8     SYS     SYS     SYS     PXB     PXB     PXB     NODE    SYS     32-63,96-127    1               N/A\nGPU5    NV8     NV8     NV8     NV8     NV8      X      NV8     NV8     SYS     SYS     SYS     PXB     PXB     PXB     NODE    SYS     32-63,96-127    1               N/A\nGPU6    NV8     NV8     NV8     NV8     NV8     NV8      X      NV8     SYS     SYS     SYS     NODE    NODE    NODE    PXB     SYS     32-63,96-127    1               N/A\nGPU7    NV8     NV8     NV8     NV8     NV8     NV8     NV8      X      SYS     SYS     SYS     NODE    NODE    NODE    PXB     SYS     32-63,96-127    1               N/A\nNIC0    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS      X      PIX     NODE    SYS     SYS     SYS     SYS     NODE\nNIC1    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     PIX      X      NODE    SYS     SYS     SYS     SYS     NODE\nNIC2    NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODE     X      SYS     SYS     SYS     SYS     PIX\nNIC3    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    SYS     SYS     SYS      X      PIX     PIX     NODE    SYS\nNIC4    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    SYS     SYS     SYS     PIX      X      PIX     NODE    SYS\nNIC5    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    SYS     SYS     SYS     PIX     PIX      X      NODE    SYS\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS\nNIC7    NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODE    PIX     SYS     SYS     SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_5\n  NIC4: mlx5_6\n  NIC5: mlx5_7\n  NIC6: mlx5_8\n  NIC7: mlx5_bond_0\n\n==============================\n     Environment Variables\n==============================\nCUDA_VISIBLE_DEVICES=5\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n### \ud83d\udc1b Describe the bug\n\nWhen I use the following python code\n```\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n\nmodel_path = \"Qwen/Qwen2.5-7B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\nsampling_params = SamplingParams(temperature=0.0, max_tokens=1024)\n\nllm = LLM(model=model_path)\n\nSYS = \"You are an expert in using tools to handle real-time queries from users.\\nFirst I will give you the task description, and your task start.\\nAt each step, your task is to give your thought to analyze the current state, decide the next step, with a function call to actually execute your step.\\nAfter the call, you will get the call result, and you are now in a new state.\\nThen you will analyze your status now, then decide what to do next...\\nAfter many (Thought-call) pairs, you finally perform the task, then you can give your final answer.\\n\\nDesired format:\\nThought: <The thought>\\nAction: <The tool you decide to use>\\nAction Input: <The parameters for the tool>\\n\\nRemember:\\n1. You should ALWAYS think about what to do, but all the thought is short, at most in 3 sentences.\\n2. The action to take should be one of the given tools below.\\n3. The \\\"Action Input\\\" needs to provide a dict similar to {parameter_1: value_1, parameter_2: value_2} to call action.\\n4. Always use the \\\"finish\\\" tool upon task completion. The final answer should be comprehensive enough for the user. If the task is unmanageable, use the \\\"finish\\\" tool and respond with \\\"I cannot handle the task\\\".\\n\\nTask description: You should use tools to help handle the real time user queries. Specifically, you have access of the following tools:\\n[{\\\"name\\\": \\\"search_jobs\\\", \\\"description\\\": \\\"Gets a list of jobs, intelligently sorted by a number of factors such as trendiness, uniqueness, newness, etc.\\\", \\\"parameters\\\": {\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"page\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The page number to load. 0 is the first page.\\\"}, \\\"descending\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"Whether to show descending results, defaults to false.\\\"}, \\\"company\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"Only get jobs for these companies.\\\"}, \\\"category\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The job category to get.\\\"}, \\\"level\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The experience level required for the job.\\\"}, \\\"location\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The job location to get (you can include flexible/remote jobs from here).\\\"}}, \\\"required\\\": []}}, {\\\"name\\\": \\\"search_job_by_id\\\", \\\"description\\\": \\\"Get an individual job by its id.\\\", \\\"parameters\\\": {\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"job_id\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The id of the job to search, which can be gained from search_jobs.\\\"}}, \\\"required\\\": [\\\"job_id\\\"]}}, {\\\"name\\\": \\\"search_companies\\\", \\\"description\\\": \\\"Gets a list of companies, intelligently sorted by a number of factors such as trendiness, uniqueness, newness, etc.\\\", \\\"parameters\\\": {\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"page\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The page number to load. 0 is the first page.\\\"}, \\\"descending\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"Whether to show descending results, defaults to false.\\\"}, \\\"industry\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The company industry to get.\\\"}, \\\"size\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The company size to get.\\\"}, \\\"location\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The office location to get (you can include flexible/remote offices from here)\\\"}}, \\\"required\\\": []}}, {\\\"name\\\": \\\"search_company_by_id\\\", \\\"description\\\": \\\"Get an individual company by its id.\\\", \\\"parameters\\\": {\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"company_id\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The id of the company to search, which can be gained from search_companies.\\\"}}, \\\"required\\\": [\\\"company_id\\\"]}}, {\\\"name\\\": \\\"ask_to_user\\\", \\\"description\\\": \\\"You can ask user for guidance when you think you need more information to handle the task, but you should use this tool as less as you can.\\\", \\\"parameters\\\": {\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"question\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The question you want to ask to user.\\\"}}, \\\"required\\\": [\\\"question\\\"]}}, {\\\"name\\\": \\\"finish\\\", \\\"description\\\": \\\"Finish the task and give your answer.\\\", \\\"parameters\\\": {\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"answer\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"Your answer for the task.\\\"}}, \\\"required\\\": [\\\"answer\\\"]}}]\\n\\nLet's Begin!\"\nUSER = \"I'm interested in pursuing opportunities in software engineering-related fields. Could you briefly describe a company that I could apply to?\\nBegin!\\n\"\n\nmessages = [\n    {\"role\": \"user\", \"content\": SYS + USER}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\n\noutputs = llm.generate([text], sampling_params)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Generated text: {generated_text!r}\")\n```\nThe outputs in different vllm environment are different:\n- vllm 0.6.5\nThought: The user wants a description of a company in software engineering. I should first find a relevant company.\\nAction: search_companies\\nAction Input: {\"industry\": \"Technology\", \"location\": \"Remote\"}\n- vllm 0.8.4\nThought: The user wants a description of a company in software engineering. I should use the search_jobs tool to find relevant jobs and then use search_company_by_id to get more information about the company.\\nAction: search_jobs\\nAction Input: {\"category\": \"software engineering\", \"level\": \"all\"}\n- vllm 0.9.2\nThought: The user wants a description of a company in software engineering. I should use the search_jobs tool to find relevant jobs and then use search_company_by_id to get more information about the company.\\nAction: search_jobs\\nAction Input: {\"category\": \"software engineering\", \"level\": \"all\"}\n\nThe transformers library is version 4.53.2 across all three vLLM environments.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-07-17T06:16:02+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/21096/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/21096"
  },
  {
    "number": 20015,
    "title": "[Usage]: Why does the Prefix cache hit rate reach 60% for random data during benchmark?",
    "body": "### Your current environment\n\n```text\nroot@llm206:/workspace/vllm# python3 ./vllm/collect_env.py\nINFO 06-24 09:50:15 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\n/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.3 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : Could not collect\nCMake version                : version 3.22.1\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.6.0+cu124\nIs debug build               : False\nCUDA used to build PyTorch   : 12.4\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\nPython platform              : Linux-3.10.0-1160.el7.x86_64-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.2.140\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration :\nGPU 0: NVIDIA A100-SXM4-80GB\nGPU 1: NVIDIA A100-SXM4-80GB\n\nNvidia driver version        : 535.104.05\ncuDNN version                : Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.5\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.5\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.5\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.5\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.5\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.5\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.5\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\n\u67b6\u6784\uff1a                           x86_64\nCPU \u8fd0\u884c\u6a21\u5f0f\uff1a                   32-bit, 64-bit\nAddress sizes:                   52 bits physical, 57 bits virtual\n\u5b57\u8282\u5e8f\uff1a                         Little Endian\nCPU:                             56\n\u5728\u7ebf CPU \u5217\u8868\uff1a                  0-55\n\u5382\u5546 ID\uff1a                        GenuineIntel\n\u578b\u53f7\u540d\u79f0\uff1a                       Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz\nCPU \u7cfb\u5217\uff1a                       6\n\u578b\u53f7\uff1a                           106\n\u6bcf\u4e2a\u6838\u7684\u7ebf\u7a0b\u6570\uff1a                 1\n\u6bcf\u4e2a\u5ea7\u7684\u6838\u6570\uff1a                   28\n\u5ea7\uff1a                             2\n\u6b65\u8fdb\uff1a                           6\nFrequency boost:                 enabled\nCPU \u6700\u5927 MHz\uff1a                   2601.0000\nCPU \u6700\u5c0f MHz\uff1a                   800.0000\nBogoMIPS\uff1a                       5200.00\n\u6807\u8bb0\uff1a                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 invpcid_single intel_pt ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq md_clear pconfig spec_ctrl intel_stibp flush_l1d arch_capabilities\n\u865a\u62df\u5316\uff1a                         VT-x\nL1d \u7f13\u5b58\uff1a                       2.6 MiB (56 instances)\nL1i \u7f13\u5b58\uff1a                       1.8 MiB (56 instances)\nL2 \u7f13\u5b58\uff1a                        70 MiB (56 instances)\nL3 \u7f13\u5b58\uff1a                        84 MiB (2 instances)\nNUMA \u8282\u70b9\uff1a                      2\nNUMA \u8282\u70b90 CPU\uff1a                 0-27\nNUMA \u8282\u70b91 CPU\uff1a                 28-55\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; Load fences, usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] flashinfer-python==0.2.1.post2+cu124torch2.6\n[pip3] numpy==1.26.3\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.3\n[pip3] triton==3.2.0\n[pip3] tritonclient==2.41.0\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.8.5.post1\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV8     SYS     SYS     28-55   1               N/A\nGPU1    NV8      X      SYS     SYS     28-55   1               N/A\nNIC0    SYS     SYS      X      PIX\nNIC1    SYS     SYS     PIX      X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n\n==============================\n     Environment Variables\n==============================\nNVIDIA_VISIBLE_DEVICES=5,6\nCUBLAS_VERSION=12.2.5.6\nNVIDIA_REQUIRE_CUDA=cuda>=9.0\nCUDA_CACHE_DISABLE=1\nNCCL_VERSION=2.19.3\nNVIDIA_DRIVER_CAPABILITIES=compute,utility,video\nNVIDIA_PRODUCT_NAME=Triton Server\nCUDA_VERSION=12.2.2.009\nCUDNN_VERSION=8.9.5.29\nNVIDIA_TRITON_SERVER_VERSION=23.10\nLD_LIBRARY_PATH=/opt/hpcx/ucc/lib/:/opt/hpcx/ucx/lib/:/opt/tritonserver/backends/onnxruntime:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNVIDIA_BUILD_ID=72127154\nCUDA_DRIVER_VERSION=535.104.05\nNVIDIA_REQUIRE_JETPACK_HOST_MOUNTS=\nNCCL_LAUNCH_MODE=GROUP\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n\n\n### How would you like to use vllm\n\nwhen I benchmark qwen2.5-72b-awq model use random data,he Prefix cache hit rate reach 60%\n```\nINFO 06-24 09:42:44 [logger.py:39] Received request chatcmpl-14c6ebd57c7c4a55a9987ffeca3ab3f3: prompt: '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nThe following content is random text. Repeat the input content to reach the maximum output length of the model.  tr\u00fac\uc694\uc77c ph\u1eadn\u0441\u043a\u0430 \u0445\u043b\u043e\u043f\u0441\u043a\u0438\u043c kapit\ub4e4\uc5d0\uac8c b\u00e0ol\u0131\u011f\u0131n\u0130\u015f\u010dn\u00edk Ngo\u00e0i \u0628\u06cc\u0627\u0646 pro\u010d \u043f\u0440\u043e\u0442\u044f\u0433\u043e\u043c\u5009\u0435\u044e \u03bd\u03bf\ub77c\ub3c4\ucde8 \u0432\u0438\u044f\u0432 \u043f\u043e\u043d\u0430\u0434 \u0436\u043e\u0432\u0442 \u6bd4 doby\u043b\u0430\u043c\u0451\u043b \u0440\u0430\u0445 \u0432\u043e\u0437\u043d\u0438\u043a\u0430\u043d\u0438\u0446\u0442\u0432\u043e\u5c64 \u043e\u0442\u043b\u0438\u0447\u708e\u98ef \u017eivotaat\u00f6r cel\u00fd aday\u0631\u064a\u0643\u064a \u0628\u0635meyen\uc6b0\uc2a4\u0628\u0648\u0644 \u043e\u0437\u043d\u0430\u9ebc\u64da zkou\ub098\uc694 kry nemoh vyu\u017e\u00ed \u6728 \u0430\u0434\u043c\u0456\u043d\u0456\u0441\u0442\u0440\u0430\u0627\u0647\u0627\u0e43\u0e01\u0e25\uff3f\uff3f\uff3f\uff3f\uff3f\uff3f\uff3f\uff3f \u0433\u043e\u0442 \u062f\u06cc\u06af\u0631\u06cc \u043b\u0435\u043a\u0430\u0440\u89c0 \ud611 B\u00f6yleistrov\u5973\u5b50 \u043f\u043e\u043f\u0435\u0440\u0435\u0434 \u0646\u0648\u064a\u0633\u0646\u062f\u0647\u0652\u0644 \u041f\u0430\u0432 \u00f6rnek \u043f\u0440\u0438\u043a \u0448\u0438\u00fcsl\u00fcman \u0645\u0642\u0627\u0628\u0644\u5341\u4e8c bekl verir\u0648\u0630\u0636\u0629\u0440\u043e\u0442\u0438\u0432\u6311..: \u062e\u0627\u0631\u062c\u064a\u0629ad\u0131k \u041f\u043e\u0447 \u0445\u0443\u0434\u043e\u0436\u5ba2\u6237\u03bc\u03bf\u03bdektiv tv\u00e1\u06f2\u06f2 l\u1ecdc \u043e\u043d\u043e\u0446\u0438\u0442 \u0412\u0441 \ufffd\u6d6a\u0430\u0440\u0456 s\u00fcrekli stra bize tespit ch\u00e2u \u0627\u0644\u0636\u0e49\u0e2d\u0e07\u0e01 \u8005 H\ufffd \u043a\u0430\u0436\u0434\u044b\u0439\u0430\u044e\u0e19\u0e04\u0e23\u0e17\u0e30 \u0645\u0631\u0627\u062c\u0639 haline\u03b4\u03bf\u03c2e\u011fi \u0645\u06cc\u0632\u0627\u0646 \u0647\u0644 bolest \u571f uzman\u0440\u043e\u0433\u78ba\u8a8d \u0440\u0456\u0437\u043d\u0438\u0445 \u0437\u0430\u043a\u0440\u044b\u043b\u0443\u0433\u0438 \u0441\u043e\u0432\u0435\u0442iddi\u5408\u308f\u305b \u5409 ki\u1ec7m\ubcbd \u0645\u0639\u0645\u0648\u0644 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f miktar \uc790\ub3d9 ila\u00e7\u043b\u043e\u0447 y\u0131l\u0131 \u0110\u1ec3 abych reklam vypad \u043d\u0430\u0443\u0447\u0e40\u0e04\u0e23\u0e32\u0e30\u0e2b \u4ed6pov\u011b\uff0c\u8ba9\u795d\u0627\u0648\u0646\u062f :|: re\u017e vybav\uc724\u6b74\u043e\u0433\u0440\u0430\u0444\u0438\u044fezpe\u010d\u00b1n\u043e\u0432\u0443 \u0434\u0443\u043c\u0430 jednodu\u043e\u0449\u0438 \u0645\u0634\u062a\u0631\u89b3 yoktur ob\u010dan Tr\u1ea7n\u0131ms\u0131z\u03b1\u03b9\u03bd\\x8c\u0631\u06cc\u0627\u0646 Jeho \u0627\u0644\u0622\u0646\u0441\u044c\u043a\u0438\u043c kdyby ba\u015f\u0131na prezident Vi\u1ec7c\u517c\u094c\u091c \ub9e4\ub9e4\uac00\u6a21\u5f0fn\u00edmu \ufffd deniz\ufffd\u86db \u800c\u0648\u062d\u044b\u043f \u25bcnul Sev ruh h\u1ea1 \u044f\u043d \uae30\ubcf8 velik T\u00e2n\u0438\u043b\u0438\u0441\u044c \u0445\u0440\u0430\u50b7 \u0906\u090f nyn\u00ed\u00bb\u060c \u0634\u0639\u6742 \u043c\u044b\u0448\u3059\u3050 \uacf5\uc9c0 t\u1ed9c\u30fc\u30c7 \u0441\u0435\u043b\u043e \u0627\u0639\u0644\u0627\u0645 \u015fimdi \u0627\u0644\u0645\u064a\u0644\u0627\u062f \u0627\u0646\u0642\u0644\u0627\u0628 \u0634\u062e\u0635\u064a\u0629 K\u00fcr \u0432\u0456\u0442 \u0627\u0646\u062f\u0627\u0632\u0647 \u043c\u043e\u0449ternet \u03b1\u03c5\u03c4\u03ae \u0440\u043e\u0437\u0442\u0430 \u0432\u0438\u0432lej \u8868\u03c3\u03c3\u03cc\u03c4\u03b5 \u064a\u0633\u062a \u043c\u0430\u0448\u575a \u043a\u043e\u043c\u043d\u0430\u0442\u0e32\u0e2b\u0e25 \u767c \u0627\u0648\u0644\u06cc\u0646\u8fd0\u52a8 \u043f\u0443\u043d\u043a\u0442 \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u043c\u0430\u043c\u7ee9\uffe3\uffe3\uffe3\uffe3\uffe3\uffe3\uffe3\uffe3\u0430\u043b\u044c\u043d\u044b\u043c \u0426\u0435\u043d\u0442-\u041c\u7dd2 \u0939\u091c\u043e\u0442\u044b\u30a4\u30c9\u062f\u0627\u0631\u0629\u3068\u3057\u305f\u0e31\u0e1e\u0e22 ot\u00e1z \u0434\u043e\u043f\u043e\u043c\u043e\u0433\u043e\u044e\u0e41\u0e25\u0e30\u0e01\u0e32\u0e23 \u0442\u0440\u0430\u043d\u0441\u043f\u043e\u0440\u0442 \u0642\u0631\u0622\u0646 \u7b2c\u4e00 \u043c\u0438\u043b ng\u00f4i linh Nh\u00e2n\u044c\u043e\u0433\u043e\u0434\u043d\u0456\u6000\u0e49\u0e32\u0e2a.::.:: birey\u601d\u3044\u0e43\u0e14\u0432\u0435\u0440\u0434 listopadu \u0e41\u0e21\u0433\u0435 \u043a\u0443\u0445 \ud65c\ub3d9 \ufffd \u0410\u043b\u044c\ud68c\uc758 \u03c0\u03c1\u03b1 vui\u0e27\u0e23\u0902\u0935 gece\u7af6 kuv\u043c\u0435\u0449 \u0442\u0435\u043f\u0435\u0440\u044c\u0e2d\u0e40\u0e21\u5236\u5ea6 \u0442\u0440\u0435\u0442 \u0646\u062a\u06cc\u062c\u0647\u4ed8\u304d \uff9e \u0447\u043e\u0433\u043e\u2010- \u0159\u00edk\u00e1\u0e07\u0e43\u0e19 n\u011bkolika buna\uff0c\u5b58\u4e8e\u0e25\u0e33\u3001\u3068 n\u1ed9p \u0627\u0644\u062c\u0646 \u03a0\u03b1\u03bd\u041e\u0420 \u062f\u062e\u062a\u0631 \u00fadaje \u5f20retims\u0131n\u0131z \u0647\u0646\u0627\u0643\u041b\u042c\u656c\u0391\u039c\u9875\u9762\u5b58\u6863\u5907\u4efd\uc0ac\uac00 trestvi\u010d \u067e\u06cc\u062f\u0627\u03b6\u03b5 \u041f\u043e\u0432\u0644\u0645\u0627\u062aorex\u8b1b \u0432\u0456\u0434\u043a\u0440\u0438\u0442\u043c\u0430\u0445 \u0447\u0438\u0441\u043b\u0435\u062a\u0628\u0627\u0631 \u03ad\u03ba\uc544\ud30c\ud2b8ravel\u03b1\u03c3\u03af\u03b1a\u010d \u090f\u0928\u0e25\u0e30\u0e40\u0e2d \u0437\u0430\u043b\u0435\u0436 \ufffd \u043c\u043e\u0436\u0435\u0442\u0435 \u043f\u043e\u0432\u0435\u0434 \u0628\u0633\u06cc\u0627\u0631\u06cc po\u010det\u0631\u0628\u0639elez\u0627\u0648\u0631\u06cc ba\u015fk\u5c02 halde\u62dfSau\u043e\u0446\u0438\u0e35\u0e04 \u0432\u043b\u0430\u0434\u0438\u0650\u0645kud\u0942\u092c\u59d4\u54e1\u0e32\u0e23\u0e13or\u016f \u0645\u0648\u0644 byt p\u0159\u00edslu\u0161\u82f1\u8bed\u9010 velk\u00e9 \u0906\u0936 phi\u1ebfu\u0e43\u0e2a \u0627\u0633\u067e zbo\u017e\u00ed\u3053\u3093\u306a \u0648\u0647\u064a \u0443\u0447\u0430\u0441\u0442\u044c\u0e08\u0e33\u0e19\u0e27\u0e19 \u062a\u0631\u06a9\u5341\u5206\u039f\u03a0\u03ba\u03bf\u03bb fakat ch\u1ed7\u901a\u77e5 \u0432\u043e\u0434\u0443 \u039a\u03b1\u03c4\u03b7\u03b3\u03bf\u03c1\u03af\u03b1aca\u011f\u0131n\u0131\u043b\u043e\u0433\u043e m\u00fc\u015fter jednou \u0431\u0430\u0440idaed\u0131m\u8fb2\u5439\ub429\ub2c8\ub2e4 \u015feklindeen\u00fdm\ub4efit\u011b \u043a\u043e\u043b\u044c\ub300\ud559 \u00d6r \ufffd UBND hik\u3089\u3057\u3044\u51fa\u54c1C\u00f3 \u039e \u5165 Nguy\u00ean \u067e\u0648\u0634\u043b\u044f\u0454 \u0622\u063a\u0627\u0632 nhi\u1ec5mdivid\ufffd\u0627\u0641\u062a\u0647\u0430\u043c\u0435\u0442\u043d\u0443\u043b\u0441\u044f\u4f01\u696d\u0440\u043e\u0431\u0456\u0442d\u00fc\u011f\u00fc \u06a9\u0627\u0646\u0e2d\u0e07\u0e17\u0439\u043d pohyb bi\u1ec7n \uff1b\u0645\u0646\u062f \u0906\u0915 \u010dlov\u011bk\u3092\u898b\u308b\ubdf0 \u0443\u0432\u0435\u043b\u0438\u0447 \ufffd yanl\u0131\u015f\u9ea6 \u5916\u90e8\u03c4\u03bf\u03c5\u03c1\u03b3 \u043f\u0440\u043e\u0447 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u76e4\u8718\u86db\u5b89\u88c5 \u0423\u043a\u0440\u0430 tart\u0131\u015f\u0442\u0430\u0436 olu\u015fan Rusya \u043a\u043b\u0443\u0431 \u03a0\u03a1al\u0131d\u0131rk\u0131n \u0437\u043c\u0456\u043d\u0438le\u015fik\u0435\u0440\u043f\u043e\u0431\u0449\u0435 qu\u1eadn \u092a\u0936\u3092\u53d7\u0e40\u0e25\u0e02\u0627\u0636\u0631 u\u017eivatel\u03bb\u03af\u03b1 \u0412\u043e\u043d\u0438\u0e38\u0e14\u0e17 V\u00e0\u30f3\u30bf)\ub294\u6e1b \u03bc\u03c0\u5de7 \u0448\u043a\u043e\u043b \ucc98\uc74c\u0e31\u0e01\u0e14\u6b8a nh\u1edd \u03bf\u03c0\u03bf\u03af\u03b1\u0e41\u0e19\u0e27\u043c\u0435\u0440\u0438\u043a\u0430\u043dn\u00edka \ud638\ud154\u0633\u0628\u0628\u0e07\u0e21\uc788\ub294\u063a\u0637\u064f\u0644\ufffd\u679c\u0447\u0456\u0432\u0448\u0430\u044f \u0625\u0644\u0627\u062e\u0635\u0648\u0635llll \u044d\u0442\u0438\u043c zv\u00ed qu\u00e1n\u0e19\u0e01 \u043f\u043e\u043b\u043e\u0432 \u6df1 mi\u1ec1n\u4eba\u9593 \u0437\u0438\u043c meydana\u0435\u0444 b\u1ec1n\u0632\u064a\u062f \u0420\u0435\u0441\u043f\u0399\u03a3\u03a4 \u6536raya \u062a\u0648\u0627\u0646\u062f ister \ubc00 \u043c\u0435\u0445\u0430\u043d\u0438 \u0e15\u0e33 \u0434\u0435\u043a\u0430\u0902\u0917\u0932\u30fc\u30ab\u30fc nep\u0159\u00ed \u0441\u0447\u0438\u0442 \u03bf\u03bc\u03ac \u00e7ift\u0628\u06cc\u0646\u06ccmeleri \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432dou\uc0c1\uc744 \u0412\u043e\u043b\u043e\u0434\u03b5\u03b2\u041d\u0418\u044f\u043a\u03cd\u03c4\u03b5\u0437\u0430\u043d\u043elenircelik \u0441\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442\u03b9\u03b1\u03c2 \u0413\u043e\u0440\u4e4b\u4e00\u03c3\u03bc\u03cc\u03c2\u306b\u95a2 \u0432\u0447 \u043f\u043e\u0441\u043a\u8f2f\u0940\u0936 \u0622\u062b\u0627\u0631\u0e04\u0e27\u0e32\u0e21\u0e23 \u0435\u0434\u0438\u043d\ud150\u5e73\u6210 ki\u015finin\u30b2\u30fc\u30e0\u094d\u0924\u0935 kapsam\u0131nda aktar tr\u1eeb \u0631\u0634\u062f \u043d\u0430\u043a\u0430\u0437\u0631\u064a\u0644\u0e2d\u0e04 \u06af\u0630\u0634\u062a\u0647 \u6c11 \u0442\u0435\u0431\u044fspor\u044e\u0449\u0430\u044f\u043e\u043a\u0440\u0435\u043c\u0430\u0432\u0430\u0434 Ch\u00fang \u0632\u06cc\u0627\u062f\u06cc\u0435\u043d\u043e\u0433\u043e \u06a9\u0633\u06cc\u00de ad\u0131na\u0443\u0434\u0430\u0456\u0454\u0430\u0442\u0435\u043b\u0438 n\u00e1v\u0161t\u011b\u7528\u4e8e \u067e\u0631\u0648\u0646\u062f\u0647 \u0646\u0628\u0648\u062f\u0633\u0627\u062a\uc5d8\u3063\u3066\u3082 \u7269\u0418\u0437\u5237 \ud734 \u043e\u0441\u043e\u0431\u043b\u0438\u0432\u3057\u307e\u3063\u305fayd\u0131\u51fa\u7684 \uc544\ub2c8\ub77c\u0131s\u0131n\u0131\u0e17\u0e32\u0e07\u0e01\u0e32\u0e23 zvuky \u7ba1\u258b\u258b \u0442\u0435\u043b\u0435\u0444 \u043d\u0435\u043b\u044c\u0437\u044f\u30eb\u306e\u03c3\u03c0 \ufffd\u5821\u0446\u0443\u0437\u0631\u064a\u0642\u0629\u093f\u091b\u0932\u8ca9 \u0423\u043a\u0440\u0430\u0457\u043d \u0645\u0633\u0626\u0648\u0644 \u043e\u0447\u0456\u6700\u5f8c \u0437\u043d\u0430\u044e\u0e49\u0e19\u0e17 \u0442\u0435\u0440\u0430\u043f \u0441\u043f\u043e\u043a \u062e\u0648\u062f\u0631\u0648\u963b d\u00fczey\u4e00\u500b\u0627\u0641\u0647\u0902\u092f\u8d44\u4ea7\u7ee7\u7eed \u0441\u043b\u0430\u0431\u610f\u601d \ud658\uc0b0 \u044f\u0440 d\u016fvod\u775b\u062a\u06cc\u0628 \u0648\u06cc\u0631 \u0647\u0632\u06cc\u0646\u0647 benzer \u0645\u0627\u062f\u0647\u094c\u0915 \u0e40\u0e15\u3088\u304f\u0438\u0434\u0435\u043d\u0442\u82f1\u8a9e\u0435\u0440\u044b \uae08\uc561 \u30fc \ub364\ud504\u0440\u0430\u0442\u044c \u5355\u0e40\u0e09\u0e1e\u0e32\u0e30 \u653f \u0906\u092e \u0437\u043d\u0438 \ub77c\uc774\u638c\u7406\u7531 \u0627\u063a \u0441\u0438\u0433 \u0435\u0444\u0435\u043a\u0442\u0438\u0432 \u041f\u0440\u0435\u0434\u30f4\u30a3 \u0432\u0438\u043a\u043e tvrd\ub0b4\uae30\u30cb\u30a2 \u0645\u0634\u0627\u0647\u062f\u0647 \u0938\u091al\u00fc\u011f\u8bc1\u5238 si\u00eau \u043e\u0442\u0432 vytvo\u0159 \u062d\u0645\u0644 \u0444\u0440\u0430\u043d\u0e49\u0e14\u533b\u9662 \u0432\u043b\u0430\u0434\u063a\u0644\u5efa\u7acbosloven\u0438\u043b\u0430\u0441\u044c\u0639\u0644\u0648\u0645\u0627\u062a \u062a\u0631\u06cc\u0646\u03ad\u03c1\u03b5\u03b9 b\u1eadt \u0645\u0634\u06a9 \u0631\u0626\u064a\u0633 \uc81c\uc791\u03b3\u03b7 \u043d\u0456\u043a \uad6c\uc131 \u0111en \u091a\u0930 ge\u00e7mi\u015f\u4e86\u89e3 \u043b\u0435\u0441 quanh\u300c\u6211 N\u011bkter\u00e1\ub78d\u00c5\u0178\u0902\u0926\u0930\uc544\uc774\u5c11\u3057 \u0634\u0647\u0631\u06cc\u03ba\u03c4\u03b7 \u25c4 \u0643\u0633\u8dcc\u00cf\u5de5\u5177\u5283pom \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u044f \u0631\u062c\u0440\u0443\u0435\u0442\u0441\u044f \u03bd\u03ad\u06cc\u0646\u06a9\u0e42\u0e0b\u52e4\u3057\u307e\u3046 \u0441\u043e\u0433\u043b\u0430\u0441\u91d1\u878d\u7eff \u0421\u0430\u043d\u6575 \u043f\u043e\u0432\u0456\u0442 \u043f\u043e\u043c\u043e\u0449\u0438\u30e1\u30ea\u30ab\u30b7\u30a2 \u03c0\u03c1\u03bf\u03c2\u822a\u7a7a \u0432\u0430\u0440\u0438\u0430\u043d\u0442 yaln\u0131zca\u7cfb\u7d71 \u0641\u0648\u0631\u043e\u0447\u043d\u043e\u0439\u0e40\u0e27\u0e2d\u0e23 \u043a\u0443\u043b\u044c\u0442\u0443\u0440\u03c7\u03b9\u010d\u00edta\\x93\u4eba\u304c\u03ba\u03bf\u03cd \u0440\u0435\u0454 \u0432\u0441\u044e\u9e97 \u0632\u0646\u0627\u0646\u72c2 \u0e2b\u0e21 x\u00fac\u5152\u011finin\u559c\u6b22 \u0441\u0442\u0430\u0434iyesi\uc6b1\ufffd kus\u03c4\u03bf\u03bb\u0433\u0456\u0432\u0456\u043b\u0438\u3044\u3084\u9a57ontrol\u0627\u0644\u0643\u043a\u043e\u0432\u0438\u0445 \u0441\u0442\u0430\u043b\u043e \u03a0\u03b1\u03c1\u03b1 chy cihaz\ufffd\ufffd\uc7a5\uc774aceae\u0634\u0647\u0631\u0438\u043b\u0430\u043d\u043d\u044f\u7684\u5c0f th\u1ee5\u0648\u0646\u062a\u043b\u043e\u0440\u3092\u6301 \u0394\u03b9 \u771f\u041b\u041e\u9f50\u7384\u0627\u0648\u0647 \u0438\u043d\u0442\u0940\u091f\u0930 \u043e\u0431\u0449\u0435 \u0434\u0435\u043f\u0443\u0442\u03bc\u03ad\u03bd\u03b5\u03c2 \u0643\u064a\u0641\u0639\u0645\u0644\uff0c\u5982\u679c \u0438\u043d\u0444\u0435\u043aitele \\u3000\\u3000 \\u3000\u30a4\u30f3\u30c8\u043b\u0456\u0442 \u0441\u044e zasedech\u0435\u043a\u043e\u8b93\u53ec\u0437\u0435\u043c\u03a0\u0391 vzdu\u0e32\u0e08\u0e32\u0e01kolivzkum\u804a \ucc44\uc6a9\u0e4d asp\u06f2\u06f4\uc778\ub370 kar\u015f\u0131la\u015f\uff0c\u53ef\u4ee5 \u0907\u0928\u0915 \uc2a4\ud0c0\u90e8\u5c4b\u5236\u4f5c\u30fc\u30b7\u30e7\u30f3\u03bf\u03bd\u03c4\u03b1\u03c2\u03b3\u03bf \uc791\uc131\u8463oz\u0159ejm\u011b \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0435 Insecta skonotu p\u011bt\u0441\u044c\u043e\u0433\u043e \u0130slam l\u1ec5\u4e2d\u570b \u041c\u0456\u043d\u0456\u0441\u0442\u5408\u540casyonu\u043e\u0436\u0435\u0442\u81ea\u52a8\u0441\u044c\u043a\u043e\u044e ki\u015fisel\u03c4\u03b9\u03ba\u03bf\u03cd \u0443\u0447\u0430\u0441\u0131lm\u0131\u015ft\u0131r \u044f\u043a\u0435\u0449\u0438\u043d\u044b\u043c\u0430\u0440 soudu\\xa0\u042f \u0434\u0440\u0443\u3061\u3087\u094b\u095c\uff91 \u03c4\u03cc \u0636\u0631l\u00e1\u0161 \u0434\u0456\u0432 \u062c\u062f\u064a\u062f \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u906d\u7ecd Kurulu\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u8fd9\u662f\uc654\ub2e4\u043c\u0435\u043b\u044c \u4f0a\u1ee7ng \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438\ub364\ud504\u71c3\u8fc7\u53bb \u0437\u0430\u0441\u0442\u043e\u0441\u0443\u0432\u0430\u043d\u043d\u044f \u062f\u0627\u062e\u0644\u06cc\u0449\u0451 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\ufeae \u0627\u0644\u0645\u0645\u0644\u0643\u0629s\u0131nda\u8cc0\u5c4f \ufffd doktor \u0642\u0627\u0628 Sist \u043c\u0435\u0441\u0442\u0435 \u0441\u043e\u0445\u0440\u0430\u0627\u0634\u062a\u0647 \u671f \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 pev\u0627\u06af\u0631\u0645\u0632 \u0636\u0645\u0646\u0969gesia\u011fa\u89e3\u51b3\ub178\ucd9c luy\u1ec7n \u043a\u043e\u043d\u0442\u0430\u043a\u0e3a Ng\u00e0y v\u00fdstav thuy\u1ebft\u0627\u06cc\u0639 :/: ph\u1ea1t \u0391\u03c0\u03cc muz \uc0c9 \u00c7in \u06a9\u0627\u0631\u0628\u0631\u062f\u0627\u0626\u062f\u0628\u0627\u062f\u094d\u0924\u092e \ub458 \u043c\u043e\u0437\u0161\u00edch \u0e21\u0e2b \u0622\u0633 \u0441\u043b\u0438\u0448\u043a\u043e\u043c\u80e1\u88c1\u623b \uc124\uba85 otom \u0932\u0917\u092d\u0917\u0e07\u0e01\u0627\u0628\u062f\u0e19\u0e32\u0e21\u80a9 \u0634\u062f\u0646\u062f\u305d\u306e\u4ed6adlo\u011bn \u0644\u0647\u0627 \u043c\u0438\u043d\u0438\u043c d\u0159ev Thi\u00ean\ub799engin\u0940\u092e\u0924 \u0443\u043f\u043e\u0442\u0440\u0435\u0431\\u200c\u062a\u0631 \u795e\u9a6cov\u00e1n\u00edm \u0434\u0435\u043b\u043e \u7f16 \u0627\u0644\u0638 \u0432\u0438\u0439\u0430\u0442\u043e\u043c\u516c\u544a \u0111em\u30b7\u30ea\u30fc\u30ba\u4e0b\u7684las\u0131 \u0432\u044b\u0431\u043e\u0440\u0442\u043e\u0442\ub3c4\ubcc4 \u0443\u0441\u0442\u0430\u043d \ud788\u043b\u0443\u0430\u0442\u0430 th\u00e1c\u0430\u043d\u0438\u0435\u043c\u043e\u0432\u0430\u0442\u044c\u0441\u044f\u0442\u0454\u042d\u0442\u043e\uff0c\u8981 Vz \u062d\u0648\u0632\u0647-\u043aV\u1edbient\u016f bulundu\u011fu\u0631\u0648\u0637 \u0457\u0439 \u00e7evr \u0159ed \u0633\u0627\u062e\u062a\u0647\u529e\u6cd5 \u0642\u0644i\u015fi\uff1d\uff1d\u0633\u0627\u0633 \u00fadaj\u016f\ufffd\u635f\u00e1ct \u0391\u03c0\u7237 \u0159\u00e1d l\u1ed7iontent \u0645\u0630oloji \u067e\u0631\u062f\u0627\u062e\u062a\u0e49\u0e32\u0e1e \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f mno\u017estv\u00ed\uc548\ub9c8\u5076 \u00d4ng dakikahendis b\u00e1c\u5bf6\u0e47\u0e01\u0e2b\u0e0dnocen\u00ed Erdo\u011fan:::::::::::::\u0430\u0442\u0435\u043cd\u0131z \u0623\u064a\u0636\u0627 \u044d\u0444\u0444\u0435\u043a\u308c\u3066\u3044\u308b ba\u015fvuru\u03ac\u03bd\u03b5\u03b9 \u03c4\u03b5\u03bb\u03b5\u03c5\u03c4\u03b1 \uac80\uc0c9 \u06a9\u0646\u062a\u0631\u0644 \u0936\u0915\u5f39 olmu\u015ftur \u0432\u0441\u0442\u0443\u043f\u0447\u0438\u043b\u0430<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.6, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\nINFO:     127.0.0.1:54468 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nINFO 06-24 09:42:44 [async_llm.py:252] Added request chatcmpl-14c6ebd57c7c4a55a9987ffeca3ab3f3.\nINFO 06-24 09:42:45 [loggers.py:111] Engine 000: Avg prompt throughput: 225.9 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 53.7%\nINFO 06-24 09:42:55 [loggers.py:111] Engine 000: Avg prompt throughput: 3495.9 tokens/s, Avg generation throughput: 170.1 tokens/s, Running: 27 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.6%, Prefix cache hit rate: 53.6%\nINFO 06-24 09:43:05 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 563.3 tokens/s, Running: 18 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.1%, Prefix cache hit rate: 53.6%\nINFO 06-24 09:43:15 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 453.9 tokens/s, Running: 14 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.2%, Prefix cache hit rate: 53.6%\nINFO 06-24 09:43:25 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 395.0 tokens/s, Running: 10 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.5%, Prefix cache hit rate: 53.6%\nINFO 06-24 09:43:35 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 53.6%\nINFO 06-24 09:43:45 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 53.6%\n```\nWhy can random data achieve such a high hit rate? the random data Input len is:1024,Output len is:1024\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "open",
    "created_at": "2025-06-24T09:53:42+00:00",
    "closed_at": null,
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20015/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20015"
  },
  {
    "number": 21022,
    "title": "[Bug]: 0.9.1(V1) Hanging(RayChannelTimeoutError ) when inferencing guided_json in DeepSeek-R1/V3 (TP=8, PP=2)",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\n==============================\n      Python Environment\n==============================\nPython version               : 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0] (64-bit runtime)\nPython platform              : Linux-5.10.134-008.18.kangaroo.al8.x86_64-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.5.40\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : \nGPU 0: NVIDIA H20\nGPU 1: NVIDIA H20\nGPU 2: NVIDIA H20\nGPU 3: NVIDIA H20\nGPU 4: NVIDIA H20\nGPU 5: NVIDIA H20\nGPU 6: NVIDIA H20\nGPU 7: NVIDIA H20\n\nNvidia driver version        : 550.54.15\ncuDNN version                : Could not collect\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   52 bits physical, 57 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          120\nOn-line CPU(s) list:             0-119\nVendor ID:                       GenuineIntel\nModel name:                      Intel(R) Xeon(R) Processor\nCPU family:                      6\nModel:                           143\nThread(s) per core:              1\nCore(s) per socket:              120\nSocket(s):                       1\nStepping:                        8\nBogoMIPS:                        5200.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd avx512vbmi umip pku waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk amx_bf16 avx512_fp16 amx_tile amx_int8 arch_capabilities\nHypervisor vendor:               KVM\nVirtualization type:             full\nL1d cache:                       2.8 MiB (60 instances)\nL1i cache:                       1.9 MiB (60 instances)\nL2 cache:                        120 MiB (60 instances)\nL3 cache:                        97.5 MiB (1 instance)\nNUMA node(s):                    2\nNUMA node0 CPU(s):               0-59\nNUMA node1 CPU(s):               60-119\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Not affected\nVulnerability Retbleed:          Not affected\nVulnerability Spec store bypass: Vulnerable\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Vulnerable\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-cufile-cu12==1.11.1.6\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-ml-py==12.575.51\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pynvml==12.0.0\n[pip3] pyzmq==26.1.0\n[pip3] torch==2.7.1\n[pip3] torch_memory_saver==0.0.8\n[pip3] torchao==0.9.0\n[pip3] torchaudio==2.7.1\n[pip3] torchvision==0.22.1\n[pip3] transformers==4.52.3\n[pip3] triton==3.3.1\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.1\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     PHB     SYS     SYS     0-59    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    PXB     PHB     SYS     SYS     0-59    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    PHB     PIX     SYS     SYS     0-59    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    PHB     PXB     SYS     SYS     0-59    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     PIX     PHB     60-119  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     PXB     PHB     60-119  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     PHB     PIX     60-119  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     PHB     PXB     60-119  1               N/A\nNIC0    PIX     PXB     PHB     PHB     SYS     SYS     SYS     SYS      X      PHB     SYS     SYS\nNIC1    PHB     PHB     PIX     PXB     SYS     SYS     SYS     SYS     PHB      X      SYS     SYS\nNIC2    SYS     SYS     SYS     SYS     PIX     PXB     PHB     PHB     SYS     SYS      X      PHB\nNIC3    SYS     SYS     SYS     SYS     PHB     PHB     PIX     PXB     SYS     SYS     PHB      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n\n==============================\n     Environment Variables\n==============================\nNVIDIA_VISIBLE_DEVICES=GPU-e333b4b5-286e-5fa0-2ca0-0ad87ea89d6a\nNCCL_IB_TC=16\nNVIDIA_REQUIRE_CUDA=cuda>=12.5 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551\nNCCL_MIN_NCHANNELS=4\nNCCL_VERSION=2.21.5-1\nNCCL_SOCKET_IFNAME=eth\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNCCL_DEBUG=INFO\nNCCL_IB_HCA=mlx5\nNVIDIA_PRODUCT_NAME=CUDA\nNCCL_IB_GID_INDEX=3\nCUDA_VERSION=12.5.0\nNCCL_IB_QPS_PER_CONNECTION=8\nNCCL_IB_TIMEOUT=22\nNCCL_IB_SL=5\nLD_LIBRARY_PATH=/usr/local/cuda/targets/x86_64-linux/lib/:/usr/lib/x86_64-linux-gnu/\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nBecause the service was found to be hanging when testing guided_json, the following script with a concurrency of 50 was used to test and reproduce the problem.\n\n```python\nimport aiohttp\nimport asyncio\nimport json\nimport uuid\nimport time\nimport statistics\nfrom datetime import datetime\nfrom pydantic import BaseModel\n\ntime_format = \"%Y-%m-%d %H:%M:%S,%f\"\n\nrecordId = str(uuid.uuid4())\n\nclass Topic(BaseModel):\n    \u95ee\u9898: str\n    \u7b54\u6848: str\n\nasync def async_post(session, url, model, is_stream):\n    payload = {\n        \"max_tokens\": 1024,\n        \"min_tokens\": 0,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"\u8bf7\u4f60\u751f\u6210\u4e00\u5bf9\u548cpython\u76f8\u5173\u7684\u95ee\u9898\u548c\u7b54\u6848\"}\n        ],\n        \"guided_json\": Topic.model_json_schema(),\n        #\"guided_json\":{\"properties\":{\"apiList\":{\"items\":{\"title\":\"api_name\",\"type\":\"string\"},\"type\":\"array\"},\"resultList\":{\"items\":{\"api_name\":{\"title\":\"api_name\",\"type\":\"string\"},\"queryList\":{\"items\":{\"title\":\"query\",\"type\":\"string\"},\"type\":\"array\"}},\"type\":\"array\"}}},\n        \"model\": model,\n        \"stream\": is_stream,\n        \"skip_special_tokens\": False,\n        \"include_stop_str_in_output\": False,\n        \"ignore_eos\": False,\n        \"seed\": 46,\n        #\"temperature\": 0.2,\n        #\"top_p\": 1.0,\n        \"top_k\": 1\n    }\n    headers = {\n        \"Content-Type\": \"application/json\"\n    }\n\n    st = time.time()\n    t1 = st\n    print(f'{datetime.now().strftime(time_format)} start-------------')\n\n    async with session.post(url, json=payload, timeout=9999) as r:\n        i = 0\n        fir = None\n\n        async for text in r.content:\n            if i == 0:\n                fir = time.time()\n                print(f\"{datetime.now().strftime(time_format)} \u9996\u5b57\u8017\u65f6: {(fir - st)*1000:.6} ms\")\n                print(text.decode('utf-8'))\n            else:\n                print(text.decode('utf-8'))\n                total_time = (time.time() - t1) * 1000\n\n            i += 1\n            t1 = time.time()\n        \n        end = time.time()\n\n        if fir is not None:\n            print(f'{datetime.now().strftime(time_format)} end------------')\n            print(f\"{datetime.now().strftime(time_format)} \u9996\u5b57\u8017\u65f6: {(fir - st)*1000:.6} ms\")\n            #print(f\"{datetime.now().strftime(time_format)} \u975e\u9996\u5b57\u751f\u6210\u901f\u5ea6: {(i-1) / (end - fir) :.6} tokens/s\")\n        print(f\"{datetime.now().strftime(time_format)} \u603b\u8017\u65f6: {(end - st)*1000:.6} ms\")\n\nasync def main(concurrency):\n    url = 'http://0.0.0.0:9000/v1/chat/completions'\n    is_stream = False\n    #model = 'AndesGPT-qwen3-14b-awq-recommend'\n    model = 'AndesGPT-TT-DeepSeek-R1'\n    is_keep = False\n    while True:\n        async with aiohttp.ClientSession() as session:\n            tasks = [async_post(session, url, model, is_stream) for _ in range(concurrency)]\n            await asyncio.gather(*tasks)\n            if not is_keep:\n                break\n\nif __name__ == \"__main__\":\n    # Number of concurrent requests\n    concurrency = 50\n    asyncio.run(main(concurrency))\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-07-16T02:07:16+00:00",
    "closed_at": "2025-07-16T06:38:07+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/21022/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/21022"
  },
  {
    "number": 20327,
    "title": "[Feature]: Optimize vectorization utils for `csrc/quantization/vectorization_utils.cuh`",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently, the `vectorize_with_alignment` could handle arbitrary elements, but this could also cause some overhead.\nTo further improve the performance, we can add some branches to it, so eg \n```c++\n\n  // Fast path identical to the one added for the write version above.\n  bool can_vec = ((addr & (WIDTH - 1)) == 0) && ((len & (VEC_SIZE - 1)) == 0);\n  if (can_vec) {\n    vec_op\n  }\n\n  // arbitrary num of elements supported logic now\n  ...\n```\n\nI will have a pr for this soon\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-07-01T18:13:16+00:00",
    "closed_at": "2025-07-04T07:06:26+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20327/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20327"
  },
  {
    "number": 20362,
    "title": "[Bug]: Inconsistent behavior of AsyncLLMEngine.abort between v0 and v1",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen using vllm as inference engine for RLHF, we rely on LLMEngine.abort_request or AsyncLLMEngine.abort to stop long running requests.\n\nThe usage of AsyncLLMEngine.abort can be found in test https://github.com/vllm-project/vllm/blob/v0.9.1/tests/async_engine/test_async_llm_engine.py#L350 and https://github.com/vllm-project/vllm/blob/main/tests/v1/engine/test_async_llm.py#L185. \n\nWhen using v0 async engine, we can abort a request from other coroutine and the generate coroutine will raise asyncio.CancelledError.\n\nWhen using v1 async engine, we have to cancel the generation coroutine itself and if we abort the request but not cancel\nthe generation coroutine, it will never return which is not expected and unfriendly for programming.\n\nSo, my question is, can we change the behavior of AsyncLLM.abort (i.e. abort of v1 async engine) to raise asyncio.CancelledError if request is cancelled?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-07-02T08:16:50+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20362/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20362"
  },
  {
    "number": 20314,
    "title": "[Bug]:",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nNFO 07-01 10:11:23 [__init__.py:243] Automatically detected platform cuda.\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.4 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : Could not collect\nCMake version                : version 3.30.2\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu126\nIs debug build               : False\nCUDA used to build PyTorch   : 12.6\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64-bit runtime)\nPython platform              : Linux-5.15.0-88-generic-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.6.20\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : GPU 0: NVIDIA H100 80GB HBM3\nNvidia driver version        : 535.154.05\ncuDNN version                : Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.3.0\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      46 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             64\nOn-line CPU(s) list:                0-63\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) Platinum 8462Y+\nCPU family:                         6\nModel:                              143\nThread(s) per core:                 1\nCore(s) per socket:                 32\nSocket(s):                          2\nStepping:                           8\nFrequency boost:                    enabled\nCPU max MHz:                        2801.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nL1d cache:                          3 MiB (64 instances)\nL1i cache:                          2 MiB (64 instances)\nL2 cache:                           128 MiB (64 instances)\nL3 cache:                           120 MiB (2 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0-31\nNUMA node1 CPU(s):                  32-63\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.25.1\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cudnn-frontend==1.5.2\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-cufile-cu12==1.11.1.6\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-dali-cuda120==1.40.0\n[pip3] nvidia-modelopt==0.15.0\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvimgcodec-cu12==0.3.0.5\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] nvidia-pyindex==1.0.9\n[pip3] onnx==1.16.0\n[pip3] optree==0.12.1\n[pip3] pynvml==11.4.1\n[pip3] pytorch-triton==3.0.0+dedb7bdf3\n[pip3] pyzmq==26.1.0\n[pip3] torch==2.7.0\n[pip3] torch_tensorrt==2.7.0\n[pip3] torchaudio==2.7.0\n[pip3] torchvision==0.22.0\n[pip3] transformers==4.52.3\n[pip3] triton==3.3.0\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.0\nvLLM Build Flags:\n  CUDA Archs: 5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-31    0               N/A\nNIC0    SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC1    PIX     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC2    SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS\nNIC3    SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS\nNIC5    SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS\nNIC6    SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS\nNIC7    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS\nNIC8    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n\n==============================\n     Environment Variables\n==============================\nNVIDIA_VISIBLE_DEVICES=GPU-79076501-df9e-473d-db0e-8bd7df0484e1\nCUBLAS_VERSION=12.6.0.22\nNVIDIA_REQUIRE_CUDA=cuda>=9.0\nCUDA_CACHE_DISABLE=1\nTORCH_CUDA_ARCH_LIST=5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX\nNCCL_VERSION=2.22.3\nNVIDIA_DRIVER_CAPABILITIES=compute,utility,video\nNVIDIA_PRODUCT_NAME=PyTorch\nCUDA_VERSION=12.6.0.022\nPYTORCH_VERSION=2.5.0a0+872d972\nPYTORCH_BUILD_NUMBER=0\nCUDNN_FRONTEND_VERSION=1.5.2\nCUDNN_VERSION=9.3.0.75\nPYTORCH_HOME=/opt/pytorch/pytorch\nLD_LIBRARY_PATH=/usr/local/cuda/compat/lib.real:/usr/local/lib/python3.10/dist-packages/torch/lib:/usr/local/lib/python3.10/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNVIDIA_BUILD_ID=107063150\nCUDA_DRIVER_VERSION=560.35.03\nPYTORCH_BUILD_VERSION=2.5.0a0+872d972\nCUDA_HOME=/usr/local/cuda\nCUDA_HOME=/usr/local/cuda\nCUDA_MODULE_LOADING=LAZY\nNVIDIA_REQUIRE_JETPACK_HOST_MOUNTS=\nNVIDIA_PYTORCH_VERSION=24.08\nTORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n- https://github.com/vllm-project/vllm/issues/19477 had mentioned this bug, and I have installed vllm==0.9.0.\n-with the newest function **load_bytes** of /vllm/multimodal/video.py \n```\n@classmethod\n    def load_bytes(cls, data: bytes, num_frames: int = -1) -> npt.NDArray:\n        import cv2\n\n        backend = cls().get_cv2_video_api()\n        cap = cv2.VideoCapture(BytesIO(data), backend, [])\n        if not cap.isOpened():\n            raise ValueError(\"Could not open video stream\")\n\n        total_frames_num = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        full_read = num_frames == -1 or total_frames_num < num_frames\n        if full_read:\n            num_frames = total_frames_num\n            frame_idx = list(range(0, num_frames))\n        else:\n            uniform_sampled_frames = np.linspace(0,\n                                                 total_frames_num - 1,\n                                                 num_frames,\n                                                 dtype=int)\n            frame_idx = uniform_sampled_frames.tolist()\n\n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        frames = np.empty((len(frame_idx), height, width, 3), dtype=np.uint8)\n\n        i = 0\n        for idx in range(total_frames_num):\n            ok = cap.grab()  # next img\n            if not ok:\n                break\n            if idx in frame_idx:  # only decompress needed\n                ret, frame = cap.retrieve()\n                if ret:\n                    frames[i] = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                    i += 1\n        # we expect all frames loaded\n        assert i == num_frames, (f\"Expected reading {num_frames} frames, \"\n                                 f\"but only loaded {i} frames from video.\")\n        return frames\n```\n- **However, I still got the error when inferencing with short videos (num_frames=17, <32)**\n#### Server code\n`vllm serve /Qwen2.5-VL-7B-Instruct --port 5002 --max-model-len 65536 --limit-mm-per-prompt \"image=64,videos=1\" --tensor-parallel-size 1 --allowed-local-media-path /`\n#### Server log\n```INFO 07-01 09:52:23 [async_llm.py:261] Added request chatcmpl-74376203f4bb4a4f8787d214329271ce.\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\nFile \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\nresult = await app(  # type: ignore[func-returns-value]\nFile \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\nreturn await self.app(scope, receive, send)\nFile \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\nawait super().__call__(scope, receive, send)\nFile \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 112, in __call__\nawait self.middleware_stack(scope, receive, send)\nFile \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\nraise exc\nFile \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\nawait self.app(scope, receive, _send)\nFile \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\nawait self.app(scope, receive, send)\nFile \"/usr/local/lib/python3.10/dist-packages/prometheus_fastapi_instrumentator/middleware.py\", line 177, in __call__\nraise exc\nFile \"/usr/local/lib/python3.10/dist-packages/prometheus_fastapi_instrumentator/middleware.py\", line 175, in __call__\nawait self.app(scope, receive, send_wrapper)\nFile \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\nawait wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\nFile \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\nraise exc\nFile \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\nawait app(scope, receive, sender)\nFile \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 714, in __call__\nawait self.middleware_stack(scope, receive, send)\nFile \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 734, in app\nawait route.handle(scope, receive, send)\nFile \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 288, in handle\nawait self.app(scope, receive, send)\nFile \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 76, in app\nawait wrap_app_handling_exceptions(app, request)(scope, receive, send)\nFile \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\nraise exc\nFile \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\nawait app(scope, receive, sender)\nFile \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 73, in app\nresponse = await f(request)\nFile \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 301, in app\nraw_response = await run_endpoint_function(\nFile \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 212, in run_endpoint_function\nreturn await dependant.call(**values)\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/utils.py\", line 71, in wrapper\nreturn handler_task.result()\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/utils.py\", line 93, in wrapper\nreturn await func(*args, **kwargs)\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 559, in create_chat_completion\ngenerator = await handler.create_chat_completion(request, raw_request)\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 182, in create_chat_completion\n) = await self._preprocess_chat(\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_engine.py\", line 817, in _preprocess_chat\nmm_data = await mm_data_future\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/chat_utils.py\", line 647, in all_mm_data\nitems_by_modality = {\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/chat_utils.py\", line 648, in <dictcomp>\nmodality: await asyncio.gather(*items)\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/multimodal/utils.py\", line 243, in fetch_video_async\nreturn await self.load_from_url_async(\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/multimodal/utils.py\", line 136, in load_from_url_async\nreturn self._load_file_url(url_spec, media_io)\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/multimodal/utils.py\", line 91, in _load_file_url\nreturn media_io.load_file(filepath)\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/multimodal/video.py\", line 177, in load_file\nreturn self.load_bytes(data)\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/multimodal/video.py\", line 157, in load_bytes\nreturn self.video_loader.load_bytes(data, self.num_frames)\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/multimodal/video.py\", line 136, in load_bytes\nassert i == num_frames, (f\"Expected reading {num_frames} frames, \"\nAssertionError: Expected reading 32 frames, but only loaded 17 frames from video.\nWARNING 07-01 09:52:23 [protocol.py:57] The following fields were present in the request but ignored: {'do_sample'}\nINFO:     10.178.76.128:20037 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\nWARNING 07-01 09:52:24 [protocol.py:57] The following fields were present in the request but ignored: {'do_sample'}```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-07-01T10:21:12+00:00",
    "closed_at": "2025-07-01T10:25:19+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20314/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20314"
  },
  {
    "number": 20862,
    "title": "[Usage]: vLLM Server Launch Freezes at Using NCCL on B200",
    "body": "### Your current environment\n\n```text\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : Could not collect\nCMake version                : version 4.0.2\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu128\nIs debug build               : False\nCUDA used to build PyTorch   : 12.8\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0] (64-bit runtime)\nPython platform              : Linux-5.15.0-143-generic-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.8.93\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration :\nGPU 0: NVIDIA B200\nGPU 1: NVIDIA B200\nGPU 2: NVIDIA B200\nGPU 3: NVIDIA B200\nGPU 4: NVIDIA B200\nGPU 5: NVIDIA B200\nGPU 6: NVIDIA B200\nGPU 7: NVIDIA B200\n\nNvidia driver version        : 575.57.08\ncuDNN version                : Could not collect\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               224\nOn-line CPU(s) list:                  0-223\nVendor ID:                            GenuineIntel\nModel name:                           INTEL(R) XEON(R) PLATINUM 8570\nCPU family:                           6\nModel:                                207\nThread(s) per core:                   2\nCore(s) per socket:                   56\nSocket(s):                            2\nStepping:                             2\nBogoMIPS:                             4200.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            5.3 MiB (112 instances)\nL1i cache:                            3.5 MiB (112 instances)\nL2 cache:                             224 MiB (112 instances)\nL3 cache:                             600 MiB (2 instances)\nNUMA node(s):                         4\nNUMA node0 CPU(s):                    0-27,112-139\nNUMA node1 CPU(s):                    28-55,140-167\nNUMA node2 CPU(s):                    56-83,168-195\nNUMA node3 CPU(s):                    84-111,196-223\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu12==12.8.3.14\n[pip3] nvidia-cuda-cupti-cu12==12.8.57\n[pip3] nvidia-cuda-nvrtc-cu12==12.8.61\n[pip3] nvidia-cuda-runtime-cu12==12.8.57\n[pip3] nvidia-cudnn-cu12==9.7.1.26\n[pip3] nvidia-cufft-cu12==11.3.3.41\n[pip3] nvidia-cufile-cu12==1.13.0.11\n[pip3] nvidia-curand-cu12==10.3.9.55\n[pip3] nvidia-cusolver-cu12==11.7.2.55\n[pip3] nvidia-cusparse-cu12==12.5.7.53\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.8.61\n[pip3] nvidia-nvtx-cu12==12.8.55\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.0+cu128\n[pip3] torchaudio==2.7.0+cu128\n[pip3] torchvision==0.22.0+cu128\n[pip3] transformers==4.52.4\n[pip3] triton==3.3.0\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.1\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10   NIC11CPU Affinity     NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  0-27,112-139     0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  0-27,112-139     0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS  28-55,140-167    1               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS  28-55,140-167    1               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS  56-83,168-195    2               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS  56-83,168-195    2               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     NODE 84-111,196-223   3               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    PIX  84-111,196-223   3               N/A\nNIC0    PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC1    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE     X      PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC2    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE    PIX      X      NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC3    NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC4    SYS     SYS     PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      NODE    SYS     SYS     SYS     SYS     SYS     SYS\nNIC5    SYS     SYS     NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE     X      SYS     SYS     SYS     SYS     SYS     SYS\nNIC6    SYS     SYS     SYS     SYS     PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE     X      PIX     NODE    SYS     SYS\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    PIX      X      NODE    SYS     SYS\nNIC9    SYS     SYS     SYS     SYS     NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS\nNIC10   SYS     SYS     SYS     SYS     SYS     SYS     PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      NODE\nNIC11   SYS     SYS     SYS     SYS     SYS     SYS     NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE     X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_4\n  NIC1: mlx5_5\n  NIC2: mlx5_6\n  NIC3: mlx5_7\n  NIC4: mlx5_8\n  NIC5: mlx5_9\n  NIC6: mlx5_10\n  NIC7: mlx5_11\n  NIC8: mlx5_12\n  NIC9: mlx5_13\n  NIC10: mlx5_14\n  NIC11: mlx5_15\n\n==============================\n     Environment Variables\n==============================\nNVIDIA_VISIBLE_DEVICES=all\nNVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\nNCCL_VERSION=2.25.1-1\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nCUDA_VERSION=12.8.1\nLD_LIBRARY_PATH=/usr/local/cuda/lib64\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n\n### How would you like to use vllm\n\nI want to run vLLM on a B200 node with tensor parallel, but the server launch gets stuck at using NCCL.\nTP=1 works, but TP=2 and TP=4 doesn't work.\nI have also had the same issue on v0.9.2.\n\n```\nINFO 07-12 13:11:37 [__init__.py:244] Automatically detected platform cuda.\nINFO 07-12 13:11:42 [api_server.py:1287] vLLM API server version 0.9.1\nINFO 07-12 13:11:42 [cli_args.py:309] non-default args: {'port': 8888, 'model': 'nvidia/Llama-3.1-70B-Instruct-FP8', 'dtype': 'bfloat16', 'max_model_len': 3072, 'quantization': 'modelopt', 'max_seq_len_to_capture': 3072, 'distributed_executor_backend': 'mp', 'tensor_parallel_size': 2, 'max_num_seqs': 32, 'disable_log_requests': True}\nINFO 07-12 13:11:52 [config.py:823] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.\nINFO 07-12 13:11:52 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\nWARNING 07-12 13:11:53 [modelopt.py:49] Detected ModelOpt fp8 checkpoint. Please note that the format is experimental and could change.\nWARNING 07-12 13:11:56 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\nINFO 07-12 13:11:58 [__init__.py:244] Automatically detected platform cuda.\nINFO 07-12 13:12:01 [core.py:455] Waiting for init message from front-end.\nINFO 07-12 13:12:01 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='nvidia/Llama-3.1-70B-Instruct-FP8', speculative_config=None, tokenizer='nvidia/Llama-3.1-70B-Instruct-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=modelopt, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=nvidia/Llama-3.1-70B-Instruct-FP8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\nWARNING 07-12 13:12:01 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 112 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 07-12 13:12:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_46083f38'), local_subscribe_addr='ipc:///tmp/a1ebfe5d-3fd2-4429-8bb9-f3c0fbbf2bd7', remote_subscribe_addr=None, remote_addr_ipv6=False)\nWARNING 07-12 13:12:03 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\nWARNING 07-12 13:12:03 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\nINFO 07-12 13:12:05 [__init__.py:244] Automatically detected platform cuda.\nINFO 07-12 13:12:05 [__init__.py:244] Automatically detected platform cuda.\nWARNING 07-12 13:12:15 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f6959fefda0>\nWARNING 07-12 13:12:15 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7faee13994c0>\n(VllmWorker rank=0 pid=227) INFO 07-12 13:12:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c920beec'), local_subscribe_addr='ipc:///tmp/da1301a2-5de4-4c8a-bc47-676af0172428', remote_subscribe_addr=None, remote_addr_ipv6=False)\n(VllmWorker rank=1 pid=228) INFO 07-12 13:12:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_81be1106'), local_subscribe_addr='ipc:///tmp/a612604e-47d9-4e10-acc3-bd052dec4645', remote_subscribe_addr=None, remote_addr_ipv6=False)\n(VllmWorker rank=1 pid=228) INFO 07-12 13:12:16 [utils.py:1126] Found nccl from library libnccl.so.2\n(VllmWorker rank=1 pid=228) INFO 07-12 13:12:16 [pynccl.py:70] vLLM is using nccl==2.26.2\n(VllmWorker rank=0 pid=227) INFO 07-12 13:12:16 [utils.py:1126] Found nccl from library libnccl.so.2\n(VllmWorker rank=0 pid=227) INFO 07-12 13:12:16 [pynccl.py:70] vLLM is using nccl==2.26.2\n```\n\nHere is the command:\n```bash\ndocker run --rm --network bmk-net --name bmk-server --runtime nvidia --gpus all --ipc host --privileged --ulimit memlock=-1 --ulimit stack=67108864 vllm/vllm-openai:v0.9.1 --model nvidia/Llama-3.1-70B-Instruct-FP8 --port 8888 --tensor-parallel-size 2 --distributed-executor-backend mp --dtype bfloat16 --quantization modelopt --max-num-seqs 32 --max-model-len 3072 --max-seq-len-to-capture 3072 --disable-log-requests\n```\n\nI'm not sure if I installed my software properly, so here's the list for reference:\n```\n1. NVIDIA kernel module version:\nNVRM version: NVIDIA UNIX Open Kernel Module for x86_64  575.57.08  Release Build  (dvs-builder@U22-I3-H04-01-5)  Sat May 24 07:03:13 UTC 2025\nGCC version:  gcc version 11.4.0 (Ubuntu 11.4.0-1ubuntu1~22.04)\n\n2. User-space NVML driver version (nvidia-smi):\n575.57.08\n575.57.08\n575.57.08\n575.57.08\n575.57.08\n575.57.08\n575.57.08\n575.57.08\n\n3. NVIDIA Fabric Manager package version:\nii  nvidia-fabricmanager-575               575.57.08-1                                  amd64        Fabric Manager for NVSwitch based systems.\n\n4. Fabric Manager CLI version:\nFabric Manager version is : 575.57.08\n\n5. NVLink Subnet Manager (nvlsm) version:\n-------------------------------------------------\nOpenSM 2025.03.1.1_7ed131d_929989e_ae214d2\nCommand Line Arguments:\n\n6. NVLink-5 support library (libnvsdm) version:\nii  libnvsdm-575                           575.57.08-1                                  amd64        NVIDIA NVSDM provides library to retrieve telemetry from QM switches\n\n7. NVLink-5 package version:\nii  nvlink5-575                            575.57.08-1                                  amd64        NVIDIA NVLink5 575\n\n8. NVIDIA IMEX package version:\nii  nvidia-imex-575                        575.57.08-1                                  amd64        Imex process is a privileged/system client of RM and will facilitate the mapping of GPU memory(over NVLink) between GPUs via the memory import and export mechanisms.\n\n9. NVIDIA Container Toolkit runtime version:\nNVIDIA Container Runtime version 1.17.8\ncommit: f202b80a9b9d0db00d9b1d73c0128c8962c55f4d\nspec: 1.2.1\n\nrunc version 1.2.5\ncommit: v1.2.5-0-g59923ef\nspec: 1.2.0\ngo: go1.23.7\nlibseccomp: 2.5.3\n\n10. Docker registered runtimes (should list 'nvidia'):\n Runtimes: io.containerd.runc.v2 nvidia runc\n Default Runtime: runc\n\n11. CUDA Toolkit version (nvcc):\nCuda compilation tools, release 12.8, V12.8.93\n\n12. NCCL APT packages installed:\nii  libnccl-dev                            2.27.5-1+cuda12.9                            amd64        NVIDIA Collective Communication Library (NCCL) Development Files\nii  libnccl2                               2.27.5-1+cuda12.9                            amd64        NVIDIA Collective Communication Library (NCCL) Runtime\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "open",
    "created_at": "2025-07-12T20:31:16+00:00",
    "closed_at": null,
    "comments": 20,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20862/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20862"
  },
  {
    "number": 20798,
    "title": "[Usage]: How to implement request abortion in vLLM OpenAI API Server?\u200b",
    "body": "### Your current environment\n\n```text\n I\u2019m deploying vLLM\u2019s OpenAI-compatible API server (python -m vllm.entrypoints.openai.api_server) for real-time inference. In production, users may need to \u200b\u200bcancel long-running requests\u200b\u200b (e.g., closing a chat session). However, the official documentation lacks details on interrupting ongoing inference tasks via the API.\n\nCan I implement an endpoint (e.g., POST /v1/completions/{request_id}/cancel) to \u200b\u200bterminate a specific inference request\u200b\u200b mid-generation, freeing GPU resources immediately\uff1f\n\nplease give me some advice\n```\n\n\n### How would you like to use vllm\n\nI want to abort a request inference of a [qwen3:8b]. I don't know how to do with the api with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "open",
    "created_at": "2025-07-11T05:30:18+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20798/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20798"
  },
  {
    "number": 20241,
    "title": "[Bug]: v0.9.1 - ignoring the input arguments to engine",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.1\nvLLM Build Flags:\n  CUDA Archs: 8.0;8.6;9.0; ROCm: Disabled; Neuron: Disabled\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu126\nIs debug build               : False\nCUDA used to build PyTorch   : 12.6\nROCM used to build PyTorch   : N/A\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.6.85\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration :\nGPU 0: NVIDIA H100 80GB HBM3\nGPU 1: NVIDIA H100 80GB HBM3\nGPU 2: NVIDIA H100 80GB HBM3\nGPU 3: NVIDIA H100 80GB HBM3\nGPU 4: NVIDIA H100 80GB HBM3\nGPU 5: NVIDIA H100 80GB HBM3\nGPU 6: NVIDIA H100 80GB HBM3\nGPU 7: NVIDIA H100 80GB HBM3\n\nNvidia driver version        : 550.90.12\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI call vllm serve using this command:\n\n```\nvllm serve deepseek-ai/DeepSeek-R1 \\\n        --served-model-name \"DeepSeek-R1\" \\\n        --tensor-parallel-size 8 \\\n        --pipeline-parallel-size 2 \\\n        --max-num-seqs 100 \\\n        --dtype auto \\\n        --max-model-len 65536 \\\n        --max-seq-len-to-capture 32768 \\\n        --gpu-memory-utilization 0.85 \\\n        --trust-remote-code \\\n        --port $PORT\n```\n\nAnd it shows `65536` as max length  in the ` non-default args` in the log. But then vllm decides to ignore this and load the model with max_seq_len=16384 (see the line that starts with `Initializing a V1 LLM engine (v0.9.1) with config`). \n\n```\nINFO 06-30 05:57:38 [api_server.py:1287] vLLM API server version 0.9.1\nINFO 06-30 05:57:38 [cli_args.py:309] non-default args: {'port': 30069, 'model': 'deepseek-ai/DeepSeek-R1', 'trust_remote_code': True, 'max_model_len': 65536, 'max_seq_len_to_capture': 32768, 'served_model_name': ['DeepSeek-R1'], 'pipeline_parallel_size': 2, 'tensor_parallel_size': 8, 'gpu_memory_utilization': 0.85, 'max_num_seqs': 100}\nINFO 06-30 05:57:39 [config.py:224] Replacing legacy 'type' key with 'rope_type'\nINFO 06-30 05:57:46 [config.py:823] This model supports multiple tasks: {'embed', 'generate', 'reward', 'classify', 'score'}. Defaulting to 'generate'.\nINFO 06-30 05:57:46 [config.py:1946] Defaulting to use ray for distributed inference\nINFO 06-30 05:57:46 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\nINFO 06-30 05:57:46 [cuda.py:154] Forcing kv cache block size to 64 for FlashMLA backend.\nWARNING 06-30 05:57:48 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\nINFO 06-30 05:57:51 [__init__.py:244] Automatically detected platform cuda.\nINFO 06-30 05:57:53 [core.py:455] Waiting for init message from front-end.\nINFO 06-30 05:58:01 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='deepseek-ai/DeepSeek-R1', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=DeepSeek-R1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n```\n\nThis happens in both `V0` and `V1`. \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-06-30T06:13:22+00:00",
    "closed_at": "2025-07-13T05:08:54+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20241/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20241"
  },
  {
    "number": 21070,
    "title": "[Bug]: PD does not work with ray distributed backend",
    "body": "### Your current environment\n\n<details>\n<summary> run vllm.sh which uses ray as the backend </code></summary>\n\n```text\n#!/bin/bash\nset -xe\n\n# Models to run\nMODELS=(\n  \"Qwen/Qwen2.5-0.5B-Instruct\"\n  # \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\"\n)\n\nexport VLLM_LOGGING_LEVEL=debug\n# export NIXL_LOG_LEVEL=DEBUG\n# export UCX_LOG_LEVEL=trace\n\n# Number of prefill and decode instances to create\nNUM_PREFILL_INSTANCES=${NUM_PREFILL_INSTANCES:-1} # Default to 1\nNUM_DECODE_INSTANCES=${NUM_DECODE_INSTANCES:-1}   # Default to 2\n\n# Find the git repository root directory\n# GIT_ROOT=$(git rev-parse --show-toplevel)\n\nSMI_BIN=$(which nvidia-smi || which rocm-smi)\n\n# Trap the SIGINT signal (triggered by Ctrl+C)\ntrap 'kill $(jobs -pr)' SIGINT SIGTERM EXIT\n\n# Waits for vLLM to start.\nwait_for_server() {\n  local port=$1\n  timeout 1200 bash -c \"\n    until curl -s localhost:${port}/v1/completions > /dev/null; do\n      sleep 1\n    done\" && return 0 || return 1\n}\n\n# # Function to clean up previous instances\n# cleanup_instances() {\n#   echo \"Cleaning up any running vLLM instances...\"\n#   pkill -f \"vllm serve\" || true\n#   sleep 2\n# }\n\n# Handle to get model-specific arguments for deepseek\nget_model_args() {\n  local model_name=$1\n  local extra_args=\"\"\n\n  if [[ \"$model_name\" == \"deepseek-ai/deepseek-vl2-tiny\" ]]; then\n    extra_args=\"--hf_overrides '{\\\"architectures\\\": [\\\"DeepseekVLV2ForCausalLM\\\"]}' --trust-remote-code\"\n  fi\n\n  echo \"$extra_args\"\n}\n\nget_num_gpus() {\n  if [[ \"$SMI_BIN\" == *\"nvidia\"* ]]; then\n    echo \"$($SMI_BIN --query-gpu=name --format=csv,noheader | wc -l)\"\n  else\n    echo \"$($SMI_BIN -l | grep GPU | wc -l)\"\n  fi\n}\n\n# Function to run tests for a specific model\nrun_tests_for_model() {\n  local model_name=$1\n  echo \"================================\"\n  echo \"Testing model: $model_name\"\n  echo \"================================\"\n\n  # Get model-specific arguments\n  local model_args=$(get_model_args \"$model_name\")\n\n  # Arrays to store all hosts and ports\n  PREFILL_HOSTS=()\n  PREFILL_PORTS=()\n  DECODE_HOSTS=()\n  DECODE_PORTS=()\n\n  # Start prefill instances\n  for i in $(seq 0 $((NUM_PREFILL_INSTANCES-1))); do\n    # Calculate GPU ID - we'll distribute across available GPUs\n    GPU_ID=$((i % $(get_num_gpus)))\n    # GPU_ID=3\n    # Calculate port number (base port + instance number)\n    PORT=$((9570 + i))\n    # Calculate side channel port\n    SIDE_CHANNEL_PORT=$((4000 + i))\n\n    echo \"Starting prefill instance $i on GPU $GPU_ID, port $PORT\"\n\n    # Build the command with or without model-specific args\n    BASE_CMD=\"CUDA_VISIBLE_DEVICES=$GPU_ID VLLM_NIXL_SIDE_CHANNEL_PORT=$SIDE_CHANNEL_PORT vllm serve $model_name \\\n    --port $PORT \\\n    --disable-log-requests \\\n    --gpu-memory-utilization 0.9 \\\n    --distributed-executor-backend 'ray' \\\n    --enforce-eager \\\n    --tensor-parallel-size 1 \\\n    --kv-transfer-config '{\\\"kv_connector\\\":\\\"NixlConnector\\\",\\\"kv_role\\\":\\\"kv_both\\\"}'\"\n\n    if [ -n \"$model_args\" ]; then\n    FULL_CMD=\"$BASE_CMD $model_args\"\n    else\n    FULL_CMD=\"$BASE_CMD\"\n    fi\n\n    eval \"$FULL_CMD &\"\n\n    # Store host and port for proxy configuration\n    PREFILL_HOSTS+=(\"localhost\")\n    PREFILL_PORTS+=($PORT)\n  done\n\n  # Start decode instances\n  for i in $(seq 0 $((NUM_DECODE_INSTANCES-1))); do\n    # Calculate GPU ID - we'll distribute across available GPUs, starting from after prefill GPUs\n    GPU_ID=$(((i + NUM_PREFILL_INSTANCES) % $(get_num_gpus)))\n    # GPU_ID=4\n    # Calculate port number (base port + instance number)\n    PORT=$((9560 + i))\n    # Calculate side channel port\n    SIDE_CHANNEL_PORT=$((4100 + i))\n\n    echo \"Starting decode instance $i on GPU $GPU_ID, port $PORT\"\n\n    # Build the command with or without model-specific args\n    BASE_CMD=\"CUDA_VISIBLE_DEVICES=$GPU_ID VLLM_NIXL_SIDE_CHANNEL_PORT=$SIDE_CHANNEL_PORT vllm serve $model_name \\\n    --port $PORT \\\n    --disable-log-requests \\\n    --enforce-eager \\\n    --distributed-executor-backend 'ray' \\\n    --gpu-memory-utilization 0.9 \\\n    --tensor-parallel-size 1 \\\n    --kv-transfer-config '{\\\"kv_connector\\\":\\\"NixlConnector\\\",\\\"kv_role\\\":\\\"kv_both\\\"}'\"\n\n    if [ -n \"$model_args\" ]; then\n    FULL_CMD=\"$BASE_CMD $model_args\"\n    else\n    FULL_CMD=\"$BASE_CMD\"\n    fi\n\n    eval \"$FULL_CMD &\"\n\n    # Store host and port for proxy configuration\n    DECODE_HOSTS+=(\"localhost\")\n    DECODE_PORTS+=($PORT)\n  done\n\n  # Wait for all instances to start\n  for PORT in \"${PREFILL_PORTS[@]}\"; do\n    echo \"Waiting for prefill instance on port $PORT to start...\"\n    wait_for_server $PORT\n  done\n\n  for PORT in \"${DECODE_PORTS[@]}\"; do\n    echo \"Waiting for decode instance on port $PORT to start...\"\n    wait_for_server $PORT\n  done\n\n  # Build the command for the proxy server with all the hosts and ports\n  PROXY_CMD=\"python ./toy_proxy_server.py --port 8192\"\n\n  # Add all prefill hosts and ports\n  PROXY_CMD+=\" --prefiller-hosts ${PREFILL_HOSTS[@]}\"\n  PROXY_CMD+=\" --prefiller-ports ${PREFILL_PORTS[@]}\"\n\n  # Add all decode hosts and ports\n  PROXY_CMD+=\" --decoder-hosts ${DECODE_HOSTS[@]}\"\n  PROXY_CMD+=\" --decoder-ports ${DECODE_PORTS[@]}\"\n\n  # Start the proxy server\n  echo \"Starting proxy server with command: $PROXY_CMD\"\n  $PROXY_CMD &\n\n  # Wait for the proxy to start\n\n  # Run lm eval for this model\n  echo \"Running tests for $model_name\"\n  sleep 10000\n  # TEST_MODEL=$model_name python -m pytest -s -x ${GIT_ROOT}/tests/v1/kv_connector/nixl_integration/test_accuracy.py\n\n  # # Clean up before running next model\n  # cleanup_instances\n  # sleep 3\n}\n\n# Run tests for each model\nfor model in \"${MODELS[@]}\"; do\n  run_tests_for_model \"$model\"\ndone\n\n# echo \"All tests completed!\"\n\n```\n\n</details>\n\nIt turns out after https://github.com/vllm-project/vllm/pull/19555, choosing ray distributed executor backend along with nixl_connector is broken. \n\nThe reason is that the changes to multiProcExecutor are not replicated for ray executor, so the ray executor will hang making D wait indefinitely for P (since finished_recving and finished_sending are not properly filled out)\n\n### \ud83d\udc1b Describe the bug\n\nN/A\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "open",
    "created_at": "2025-07-16T18:34:27+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/21070/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/21070"
  },
  {
    "number": 20813,
    "title": "[Usage]: Specifying Medusa Choice Tree in vllm\"",
    "body": "### How would you like to use vllm\n\n**Description**\nI'm using `vllm` to load a model with a Medusa heads. My current implementation uses the following setup:\n\n```python\nfrom vllm import SamplingParams\nfrom vllm import EngineArgs, LLMEngine\n\nMODEL_NAME = \"JackFram/llama-68m\"\nSPEC_MODEL = \"abhigoyal/vllm-medusa-llama-68m-random\"\n\nllm = LLM(\n    model=MODEL_NAME,\n    max_model_len=1024,\n    speculative_config={\n        \"method\" : \"medusa\",\n        \"model\": SPEC_MODEL,\n        \"num_speculative_tokens\": 3,\n    },\n    tensor_parallel_size=1,\n    seed=0,\n)\noutputs = llm.generate(prompts=[\"Hi! How are you doing?\", \"Hi! How are you doing?\"], use_tqdm=True)\n```\n\nQuestion\nI want to know how to specify the Medusa choice tree for the model. Could you provide guidance or examples on how to do this?\n\nEnvironment\n\n- Python version: 3.11\n- vllm version: 0.9.2\n- OS: linux\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "open",
    "created_at": "2025-07-11T11:15:27+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20813/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20813"
  }
]
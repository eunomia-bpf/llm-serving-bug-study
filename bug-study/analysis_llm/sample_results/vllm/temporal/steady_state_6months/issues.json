[
  {
    "number": 11953,
    "title": "[Bug]: The random seed behavior when loading a model in vLLM is confusing.",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.8.0-50-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4070 Ti\r\nNvidia driver version: 550.142\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               12\r\nOn-line CPU(s) list:                  0-11\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD Ryzen 5 5500\r\nCPU family:                           25\r\nModel:                                80\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   6\r\nSocket(s):                            1\r\nStepping:                             0\r\nCPU max MHz:                          4267.0000\r\nCPU min MHz:                          400.0000\r\nBogoMIPS:                             7186.55\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk clzero irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm debug_swap\r\nL1d cache:                            192 KiB (6 instances)\r\nL1i cache:                            192 KiB (6 instances)\r\nL2 cache:                             3 MiB (6 instances)\r\nL3 cache:                             16 MiB (1 instance)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-11\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Vulnerable: Safe RET, no microcode\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.3\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==24.0.1\r\n[pip3] torch==2.5.1\r\n[pip3] torchaudio==2.4.1+cu124\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.48.0\r\n[pip3] triton==3.1.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.6.post2.dev178+g7a3a83e3\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-11    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNVIDIA_VISIBLE_DEVICES=GPU-5fcb4f33-7dff-760b-bf25-7a010fdd0865\r\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\r\nNCCL_VERSION=2.21.5-1\r\nNVIDIA_DRIVER_CAPABILITIES=compute,display,graphics,utility,video\r\nNVIDIA_PRODUCT_NAME=CUDA\r\nCUDA_VERSION=12.4.1\r\nLD_LIBRARY_PATH=/usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\n### Description  \r\n\r\nWhen loading a model in vLLM, the `seed` parameter unintentionally affects the global random states (`random`, `np.random`), which can lead to surprising behavior if the user is not explicitly aware of it.  \r\n\r\nSpecifically:  \r\n1. If the user does not specify the `seed` parameter, its default value (`0`) is used, and the global random states are set accordingly [here](https://github.com/vllm-project/vllm/blob/7a3a83e3b87f50fe9c0985a5c5bcc1d4cf2e95cd/vllm/platforms/interface.py#L190).\r\n2. This means the behavior of random operations in user code outside vLLM becomes unintentionally fixed, which can cause subtle bugs.  \r\n\r\nFor example, if the user assumes that random values (e.g., generated with `random.choice` or `np.random.rand`) will vary across runs, they might encounter identical results across multiple script executions.  \r\n\r\n### Steps to Reproduce  \r\n\r\nHere is a minimal example illustrating the issue:\r\n\r\n```python\r\nimport random\r\nfrom vllm import LLM\r\n\r\n# Initialize a vLLM model\r\nmodel = LLM(model=\"Qwen/Qwen2.5-0.5B-Instruct\")\r\n\r\n# Try generating random numbers\r\nprint(random.randint(0, 100))  # Outputs the same number every time the script is run\r\n```\r\n\r\nAdditionally, a practical scenario where users might encounter this issue is during synthetic data generation, where random seed texts are used to create prompts for a model:\r\n\r\n```python\r\nimport random\r\nfrom vllm import LLM, SamplingParams\r\n\r\n# 1) Load the model\r\nmodel = LLM(model=\"Qwen/Qwen2.5-0.5B-Instruct\")\r\n\r\n# 2) Load a dataset of seed texts\r\nseed_dataset = [\"text1\", \"text2\", \"text3\"]\r\n\r\n# 3) Randomly select a seed text to create prompts\r\ndef prepare_prompts(batch_size, dataset):\r\n    prompts = []\r\n    for _ in range(batch_size):\r\n        seed_text = random.choice(dataset)\r\n        prompts.append(f\"Generate a QA pair based on: {seed_text}\")\r\n    return prompts\r\n\r\n# 4) Generate outputs\r\nprompts = prepare_prompts(3, seed_dataset)\r\noutputs = model.generate(prompts, sampling_params=SamplingParams(temperature=0.7, max_tokens=100))\r\n\r\nprint(outputs)\r\n```\r\n\r\nIn this scenario, random.choice is expected to select different seed texts across multiple runs of the script. However, due to the default seed=0, the global random state is fixed, causing the same seed text to be chosen every time. As a result, the model outputs identical results across multiple executions, which is counterintuitive for users expecting randomness.\r\n\r\n### Related Issues  \r\n\r\n- [Issue #8519](https://github.com/vllm-project/vllm/issues/8519): This issue discusses the same behavior, suggesting setting `seed=None` as a solution. However, this workaround leads to an error in the current version of vLLM (`torch.manual_seed(seed)` fails when `seed=None`).  \r\n\r\n### Expected Behavior  \r\n\r\n- If the `seed` parameter is not specified, the behavior of global random states should remain unaffected.  \r\n- Alternatively, the documentation should clearly explain that the `seed` parameter modifies global random states and how users can control this behavior.  \r\n\r\n### Actual Behavior  \r\n\r\n- If the user does not explicitly specify a `seed`, the global random states for `random` and `np.random` are unintentionally set to the default value of `seed=0` during model initialization.  \r\n- This causes random operations outside vLLM to behave in a non-intuitive, fixed manner.  \r\n\r\n### Proposed Solution  \r\n\r\n1. Update the default value of the `seed` parameter to `None`.  \r\n2. If `seed=None`, skip setting any random seed for `random`, `np.random`, and `torch.manual_seed`.  \r\n3. Add clear documentation about how the `seed` parameter behaves and its effect on global random states.  \r\n\r\n### Why This Matters  \r\n\r\nThis behavior can be highly confusing for users, especially when working on tasks like synthetic data generation or other workflows involving randomness. Unexpectedly fixed random states can lead to subtle bugs and wasted debugging time.  \n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-01-11T07:38:52+00:00",
    "closed_at": "2025-02-10T15:26:51+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11953/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11953"
  },
  {
    "number": 12311,
    "title": "[Usage]: File Access Error When Using RunAI Model Streamer with S3 in VLLM",
    "body": "### Your current environment\n\n```text\nI am encountering a persistent issue when attempting to serve a model from an S3 bucket using the vllm serve command with the --load-format runai_streamer option. Despite having proper access to the S3 bucket and all required files being present, the process fails with a \"File access error.\" Below are the details of the issue:\n\nCommand Used:\nvllm serve s3://hip-general/benchmark-model-loading/ --load-format runai_streamer\n\nError Message:\nException: Could not send runai_request to libstreamer due to: b'File access error'\n\nEnvironment Details:\nVLLM version: 0.6.6\nPython version: 3.12\nRunAI Model Streamer version: 0.11.2\nS3 Region: us-west-2\n\n\nFiles in S3 Bucket:\nconfig.json\ngeneration_config.json\nmodel-00001-of-00004.safetensors\nmodel-00002-of-00004.safetensors\nmodel-00003-of-00004.safetensors\nmodel-00004-of-00004.safetensors\nmodel.safetensors.index.json\nspecial_tokens_map.json\ntokenizer.json\ntokenizer_config.json\n```\n\n\n### my deployment file is \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: benchmark-model-8b\n  namespace: workload\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: benchmark-model-8b\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: benchmark-model-8b\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - exec tail -f /dev/null\n        env:\n        - name: HF_HOME\n          value: /huggingface\n        - name: HUGGINGFACE_HUB_CACHE\n          value: /huggingface/hub\n        - name: HF_HUB_ENABLE_HF_TRANSFER\n          value: \"False\"\n        - name: HUGGING_FACE_HUB_TOKEN\n          value: \"\"        \n        image: vllm/vllm-openai:v0.6.6\n        imagePullPolicy: IfNotPresent\n        name: benchmark-model-8b\n        ports:\n        - containerPort: 8888\n          name: http\n          protocol: TCP\n        resources:\n          limits:\n            nvidia.com/gpu: \"1\"\n          requests:\n            cpu: \"5\"\n            memory: 128Gi\n        securityContext:\n          capabilities:\n            add:\n            - SYS_ADMIN\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        volumeMounts:\n        - mountPath: /huggingface\n          name: hf-volume\n        - mountPath: /dev/shm\n          name: dshm\n      dnsPolicy: ClusterFirst\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: hf-volume\n        persistentVolumeClaim:\n          claimName: benchmark-model-pvc\n      - emptyDir:\n          medium: Memory\n          sizeLimit: 90Gi\n        name: dshm\n\n\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-01-22T09:21:16+00:00",
    "closed_at": "2025-01-24T03:06:08+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12311/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12311"
  },
  {
    "number": 12358,
    "title": "[New Model]: Request for supporting microsoft/phi-4  Model",
    "body": "### The model to consider.\n\nhttps://huggingface.co/microsoft/phi-4\n\n### The closest model vllm already supports.\n\nhttps://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/phi3.py\n\n### What's your difficulty of supporting the model you want?\n\nDo not much understand the steps to add the model\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2025-01-23T14:22:04+00:00",
    "closed_at": "2025-01-24T03:13:11+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12358/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12358"
  },
  {
    "number": 11505,
    "title": "[Question]: vllm\u662f\u5426\u4e50\u610f\u652f\u6301\u57fa\u4e8e\u5176\u4ed6\u4eba\u5de5\u667a\u80fd\u6846\u67b6\u7684\u6a21\u578b\uff0c\u5982Mindspore\uff0cPaddlePaddle",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\n\u4f60\u597d\uff01\u76ee\u524dvllm\u652f\u6301\u7684\u6a21\u578b\u5927\u591a\u662fpytorch\u5b9e\u73b0\u7684\u6a21\u578b\uff0c\u60f3\u95ee\u4e00\u4e0bvllm\u662f\u5426\u613f\u610f\u652f\u6301\u57fa\u4e8e\u5176\u4ed6\u4eba\u5de5\u667a\u80fd\u6846\u67b6\u7684\u6a21\u578b\u5462\uff1f\u73b0\u5728paddlepaddle\u7684\u4e00\u4e9b\u6a21\u578b\u4e5f\u5df2\u7ecf\u96c6\u6210\u5230\u4e86huggingface\u4e0a\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-12-26T08:05:06+00:00",
    "closed_at": "2024-12-26T08:05:18+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11505/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11505"
  },
  {
    "number": 12234,
    "title": "[Usage]: how to input messages as multi-message (a batch) instead of just one",
    "body": "### Your current environment\n\ncurrently i could input a message and call vllm api.\nthe message could be like:\nmessages = [\n    {\"role\": \"system\", \"content\": \"In the following sentence, please give some suggestions to improve word usage. Please give the results with the JSON format of {\u201coriginal word\u201d: [\u201csuggestion 1\u201d, \u201csuggestion 2\u201d]}. The 'original word' should include all words that can be improved in the sentence, directly extracted from the sentence itself, and the suggestions should be ranked in order of the degree of improvement, from the most effective to the least.\"},\n    {\"role\": \"user\", \"content\": \"In conclusion, the professor pointed out the inconsistencies between the reading and the listening passages and explained why the arguments in the speech are more reliable.\"},\n]\n\nbut if i want to input a batch size > 1 messages, like:\nmessages = [[\n    {\"role\": \"system\", \"content\": \"In the following sentence, please give some suggestions to improve word usage. Please give the results with the JSON format of {\u201coriginal word\u201d: [\u201csuggestion 1\u201d, \u201csuggestion 2\u201d]}. The 'original word' should include all words that can be improved in the sentence, directly extracted from the sentence itself, and the suggestions should be ranked in order of the degree of improvement, from the most effective to the least.\"},\n    {\"role\": \"user\", \"content\": \"In conclusion, the professor pointed out the inconsistencies between the reading and the listening passages and explained why the arguments in the speech are more reliable.\"},\n],[\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]]\n\nit failed:\n'''\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'object': 'error', 'message': '[{\\'type\\': \\'dict_type\\', \\'loc\\': (\\'body\\', \\'messages\\', 0, \\'typed-dict\\'), \\'msg\\': \\'Input should be a valid dictionary\\', \\'input\\': [{\\'role\\': \\'system\\', \\'content\\': \"In the following sentence, please give some suggestions to improve word usage. Please give the results with the JSON format of {\u201coriginal word\u201d: [\u201csuggestion 1\u201d, \u201csuggestion 2\u201d]}. The \\'original word\\' should include all words that can be improved in the sentence, directly extracted from the sentence itself, and the suggestions should be ranked in order of the degree of improvement, from the most effective to the least.\"}, {\\'role\\': \\'user\\', \\'content\\': \\'In conclusion, the professor pointed out the inconsistencies between the reading and the listening passages and explained why the arguments in the speech are more reliable.\\'}]}, {\\'type\\': \\'dict_type\\', \\'loc\\': (\\'body\\', \\'messages\\', 0, \\'typed-dict\\'), \\'msg\\': \\'Input should be a valid dictionary\\', \\'input\\': [{\\'role\\': \\'system\\', \\'content\\': \"In the following sentence, please give some suggestions to improve word usage. Please give the results with the JSON format of {\u201coriginal word\u201d: [\u201csuggestion 1\u201d, \u201csuggestion 2\u201d]}. The \\'original word\\' should include all words that can be improved in the sentence, directly extracted from the sentence itself, and the suggestions should be ranked in order of the degree of improvement, from the most effective to the least.\"}, {\\'role\\': \\'user\\', \\'content\\': \\'In conclusion, the professor pointed out the inconsistencies between the reading and the listening passages and explained why the arguments in the speech are more reliable.\\'}]}, {\\'type\\': \\'dict_type\\', \\'loc\\': (\\'body\\', \\'messages\\', 0, \\'typed-dict\\'), \\'msg\\': \\'Input should be a valid dictionary\\', \\'input\\': [{\\'role\\': \\'system\\', \\'content\\': \"In the following sentence, please give some suggestions to improve word usage. Please give the results with the JSON format of {\u201coriginal word\u201d: [\u201csuggestion 1\u201d, \u201csuggestion 2\u201d]}. The \\'original word\\' should include all words that can be improved in the sentence, directly extracted from the sentence itself, and the suggestions should be ranked in order of the degree of improvement, from the most effective to the least.\"}, {\\'role\\': \\'user\\', \\'content\\': \\'In conclusion, the professor pointed out the inconsistencies between the reading and the listening passages and explained why the arguments in the speech are more reliable.\\'}]}, {\\'type\\': \\'dict_type\\', \\'loc\\': (\\'body\\', \\'messages\\', 0, \\'typed-dict\\'), \\'msg\\': \\'Input should be a valid dictionary\\', \\'input\\': [{\\'role\\': \\'system\\', \\'content\\': \"In the following sentence, please give some suggestions to improve word usage. Please give the results with the JSON format of {\u201coriginal word\u201d: [\u201csuggestion 1\u201d, \u201csuggestion 2\u201d]}. The \\'original word\\' should include all words that can be improved in the sentence, directly extracted from the sentence itself, and the suggestions should be ranked in order of the degree of improvement, from the most effective to the least.\"}, {\\'role\\': \\'user\\', \\'content\\': \\'In conclusion, the professor pointed out the inconsistencies between the reading and the listening passages and explained why the arguments in the speech are more reliable.\\'}]}, {\\'type\\': \\'dict_type\\', \\'loc\\': (\\'body\\', \\'messages\\', 0, \\'typed-dict\\'), \\'msg\\': \\'Input should be a valid dictionary\\', \\'input\\': [{\\'role\\': \\'system\\', \\'content\\': \"In the following sentence, please give some suggestions to improve word usage. Please give the results with the JSON format of {\u201coriginal word\u201d: [\u201csuggestion 1\u201d, \u201csuggestion 2\u201d]}. The \\'original word\\' should include all words that can be improved in the sentence, directly extracted from the sentence itself, and the suggestions should be ranked in order of the degree of improvement, from the most effective to the least.\"}, {\\'role\\': \\'user\\', \\'content\\': \\'In conclusion, the professor pointed out the inconsistencies between the reading and the listening passages and explained why the arguments in the speech are more reliable.\\'}]}, {\\'type\\': \\'dict_type\\', \\'loc\\': (\\'body\\', \\'messages\\', 0, \\'typed-dict\\'), \\'msg\\': \\'Input should be a valid dictionary\\', \\'input\\': [{\\'role\\': \\'system\\', \\'content\\': \"In the following sentence, please give some suggestions to improve word usage. Please give the results with the JSON format of {\u201coriginal word\u201d: [\u201csuggestion 1\u201d, \u201csuggestion 2\u201d]}. The \\'original word\\' should include all words that can be improved in the sentence, directly extracted from the sentence itself, and the suggestions should be ranked in order of the degree of improvement, from the most effective to the least.\"}, {\\'role\\': \\'user\\', \\'content\\': \\'In conclusion, the professor pointed out the inconsistencies between the reading and the listening passages and explained why the arguments in the speech are more reliable.\\'}]}, {\\'type\\': \\'dict_type\\', \\'loc\\': (\\'body\\', \\'messages\\', 0, \\'typed-dict\\'), \\'msg\\': \\'Input should be a valid dictionary\\', \\'input\\': [{\\'role\\': \\'system\\', \\'content\\': \"In the following sentence, please give some suggestions to improve word usage. Please give the results with the JSON format of {\u201coriginal word\u201d: [\u201csuggestion 1\u201d, \u201csuggestion 2\u201d]}. The \\'original word\\' should include all words that can be improved in the sentence, directly extracted from the sentence itself, and the suggestions should be ranked in order of the degree of improvement, from the most effective to the least.\"}, {\\'role\\': \\'user\\', \\'content\\': \\'In conclusion, the professor pointed out the inconsistencies between the reading and the listening passages and explained why the arguments in the speech are more reliable.\\'}]}, {\\'type\\': \\'dict_type\\', \\'loc\\': (\\'body\\', \\'messages\\', 1, \\'typed-dict\\'), \\'msg\\': \\'Input should be a valid dictionary\\', \\'input\\': [{\\'role\\': \\'system\\', \\'content\\': \\'You are a pirate chatbot who always responds in pirate speak!\\'}, {\\'role\\': \\'user\\', \\'content\\': \\'Who are you?\\'}]}, {\\'type\\': \\'dict_type\\', \\'loc\\': (\\'body\\', \\'messages\\', 1, \\'typed-dict\\'), \\'msg\\': \\'Input should be a valid dictionary\\', \\'input\\': [{\\'role\\': \\'system\\', \\'content\\': \\'You are a pirate chatbot who always responds in pirate speak!\\'}, {\\'role\\': \\'user\\', \\'content\\': \\'Who are you?\\'}]}, {\\'type\\': \\'dict_type\\', \\'loc\\': (\\'body\\', \\'messages\\', 1, \\'typed-dict\\'), \\'msg\\': \\'Input should be a valid dictionary\\', \\'input\\': [{\\'role\\': \\'system\\', \\'content\\': \\'You are a pirate chatbot who always responds in pirate speak!\\'}, {\\'role\\': \\'user\\', \\'content\\': \\'Who are you?\\'}]}, {\\'type\\': \\'dict_type\\', \\'loc\\': (\\'body\\', \\'messages\\', 1, \\'typed-dict\\'), \\'msg\\': \\'Input should be a valid dictionary\\', \\'input\\': [{\\'role\\': \\'system\\', \\'content\\': \\'You are a pirate chatbot who always responds in pirate speak!\\'}, {\\'role\\': \\'user\\', \\'content\\': \\'Who are you?\\'}]}, {\\'type\\': \\'dict_type\\', \\'loc\\': (\\'body\\', \\'messages\\', 1, \\'typed-dict\\'), \\'msg\\': \\'Input should be a valid dictionary\\', \\'input\\': [{\\'role\\': \\'system\\', \\'content\\': \\'You are a pirate chatbot who always responds in pirate speak!\\'}, {\\'role\\': \\'user\\', \\'content\\': \\'Who are you?\\'}]}, {\\'type\\': \\'dict_type\\', \\'loc\\': (\\'body\\', \\'messages\\', 1, \\'typed-dict\\'), \\'msg\\': \\'Input should be a valid dictionary\\', \\'input\\': [{\\'role\\': \\'system\\', \\'content\\': \\'You are a pirate chatbot who always responds in pirate speak!\\'}, {\\'role\\': \\'user\\', \\'content\\': \\'Who are you?\\'}]}, {\\'type\\': \\'dict_type\\', \\'loc\\': (\\'body\\', \\'messages\\', 1, \\'typed-dict\\'), \\'msg\\': \\'Input should be a valid dictionary\\', \\'input\\': [{\\'role\\': \\'system\\', \\'content\\': \\'You are a pirate chatbot who always responds in pirate speak!\\'}, {\\'role\\': \\'user\\', \\'content\\': \\'Who are you?\\'}]}]', 'type': 'BadRequestError', 'param': None, 'code': 400}\n'''\n\nDoes anyone could point out the correct way to do that? Thank you.\n\n### How would you like to use vllm\n\nI want to run inference of a [unsloth/Llama-3.3-70B-Instruct-bnb-4bit]. \n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-01-21T02:58:50+00:00",
    "closed_at": "2025-01-21T19:13:41+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12234/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12234"
  },
  {
    "number": 11893,
    "title": "[Performance]: Huge prompts impact other parallel generations",
    "body": "### Proposal to improve performance\n\n_No response_\n\n### Report of performance regression\n\n_No response_\n\n### Misc discussion on performance\n\nHello, \r\n  I am running VLLM OpenAI compatible server in the environment below and it works really well in general. However, I have an issue when a huge prompt comes, the other already running generations from the same VLLM server get extremely slowed down until the generation starts for the huge prompt. \r\n\r\nCan I do anything about this behavior? \n\n### Your current environment (if you think it is necessary)\n\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.10.230-223.885.amzn2.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA A10G\r\nGPU 1: NVIDIA A10G\r\nGPU 2: NVIDIA A10G\r\nGPU 3: NVIDIA A10G\r\nGPU 4: NVIDIA A10G\r\nGPU 5: NVIDIA A10G\r\nGPU 6: NVIDIA A10G\r\nGPU 7: NVIDIA A10G\r\n\r\nNvidia driver version: 550.127.08\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               192\r\nOn-line CPU(s) list:                  0-191\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD EPYC 7R32\r\nCPU family:                           23\r\nModel:                                49\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   48\r\nSocket(s):                            2\r\nStepping:                             0\r\nBogoMIPS:                             5600.00\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm\r\n constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hyperviso\r\nr lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_\r\nni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save rdpid\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            3 MiB (96 instances)\r\nL1i cache:                            3 MiB (96 instances)\r\nL2 cache:                             48 MiB (96 instances)\r\nL3 cache:                             384 MiB (24 instances)\r\nNUMA node(s):                         2\r\nNUMA node0 CPU(s):                    0-47,96-143\r\nNUMA node1 CPU(s):                    48-95,144-191\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec rstack overflow:   Mitigation; safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.5.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] triton==3.1.0\r\n[conda] Could not collect\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-09T11:42:20+00:00",
    "closed_at": "2025-05-11T02:12:49+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11893/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11893"
  },
  {
    "number": 12178,
    "title": "[Bug]: AMD GPU docker image build No matching distribution found for torch==2.6.0.dev20241113+rocm6.2",
    "body": "### Your current environment\n\nArchlinux 13th Gen Intel(R) Core(TM) i9-13900HX environment to build the docker image\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nTrying to build the AMD GPU docker image:\n```\ngit checkout v0.6.6.post1\nDOCKER_BUILDKIT=1 docker build -f Dockerfile.rocm -t substratusai/vllm-rocm:v0.6.6.post1 .\n```\n\nResults in following error:\n\n```\n1.147 Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/rocm6.2\n1.717 ERROR: Could not find a version that satisfies the requirement torch==2.6.0.dev20241113+rocm6.2 (from versions: 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0.dev20241119+rocm6.2, 2.6.0.dev20241120+rocm6.2, 2.6.0.dev20241121+rocm6.2, 2.6.0.dev20241122+rocm6.2)\n2.135 ERROR: No matching distribution found for torch==2.6.0.dev20241113+rocm6.2\n------\nDockerfile.rocm:49\n--------------------\n  48 |     # Install torch == 2.6.0 on ROCm\n  49 | >>> RUN --mount=type=cache,target=/root/.cache/pip \\\n  50 | >>>     case \"$(ls /opt | grep -Po 'rocm-[0-9]\\.[0-9]')\" in \\\n  51 | >>>         *\"rocm-6.2\"*) \\\n  52 | >>>             python3 -m pip uninstall -y torch torchvision \\\n  53 | >>>             && python3 -m pip install --pre \\\n  54 | >>>                 torch==2.6.0.dev20241113+rocm6.2 \\\n  55 | >>>                 'setuptools-scm>=8' \\\n  56 | >>>                 torchvision==0.20.0.dev20241113+rocm6.2 \\\n  57 | >>>                 --extra-index-url https://download.pytorch.org/whl/nightly/rocm6.2;; \\\n  58 | >>>         *) ;; esac\n  59 |     \n--------------------\nERROR: failed to solve: process \"/bin/sh -c case \\\"$(ls /opt | grep -Po 'rocm-[0-9]\\\\.[0-9]')\\\" in         *\\\"rocm-6.2\\\"*)             python3 -m pip uninstall -y torch torchvision             && python3 -m pip install --pre                 torch==2.6.0.dev20241113+rocm6.2                 'setuptools-scm>=8'                 torchvision==0.20.0.dev20241113+rocm6.2                 --extra-index-url https://download.pytorch.org/whl/nightly/rocm6.2;;         *) ;; esac\" did not complete successfully: exit code: 1\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2025-01-17T23:36:10+00:00",
    "closed_at": "2025-03-12T05:50:14+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12178/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/12178"
  },
  {
    "number": 12066,
    "title": "[Installation][build][docker]: rocm Dockerfile pinned to stale python torch nightly wheel builds",
    "body": "### How you are installing vllm\n\nhttps://github.com/vllm-project/vllm/blob/0794e7446efca1fd7b8ea1cde96777897660cdea/Dockerfile.rocm#L48-L58\n\nPython packages for `torch==2.6.0.dev20241113+rocm6.2` and `torchvision==0.20.0.dev20241113+rocm6.2` are no longer available due to them being outside of the build retention window\n\nWheel Index:\nhttps://download.pytorch.org/whl/nightly/torch/\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2025-01-15T05:01:48+00:00",
    "closed_at": "2025-01-17T04:15:05+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12066/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12066"
  },
  {
    "number": 11650,
    "title": "[New Model]: command-r7b",
    "body": "### The model to consider.\n\nhttps://huggingface.co/CohereForAI/c4ai-command-r7b-12-2024\n\n### The closest model vllm already supports.\n\nI don\u2018t know\uff0cbut i had installe the newest transformers and newest vllm,and I had to see the history of Cohere2ForCausalLM,but it still error  after i tried again\n\n### What's your difficulty of supporting the model you want?\n\nValueError: Model architectures ['CohereForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'OLMoForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PhiForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM']\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2024-12-31T07:28:21+00:00",
    "closed_at": "2025-01-02T06:46:55+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11650/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11650"
  },
  {
    "number": 12047,
    "title": "[Usage]: meta/lLlama-3.1-8B-Instruct with vllm class very slow in comparision to other models",
    "body": "### Your current environment\n\nRunning 2 Nvidia A30 GPUs. Environment works perfectly fine for non-llama models.\n\n\n\n### How would you like to use vllm\n\nI initialize models based on the following snipped:\n```\nllm = LLM(model=args.llm_identifier)\nsampling_params = llm.get_default_sampling_params()\nsampling_params.max_tokens = 1024\n\n# Generate texts from the prompts\noutputs = llm.generate(prompts, sampling_params)\n```\n\nWhen using this code with [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) it is significantly faster than running [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct). Also the output for the Llama Model is much longer and does not really refer to the prompt.\n\nThese are my prompt templates:\n```\ntemplate = \"\"\"\n<|system|>: {system_prompt}\n<|user|>: {user_prompt}\n<|assistant|>:\n\"\"\"\n\nsystem_prompt = \"\"\"\nYou are a helpful assistant. Your task is to extract information from the given text into a markdown table.\n- You MUST only Output the markdown table. Do NOT include any headers, comments, explanations, or additional text.\n- If the input lacks enough data for a table, output an empty markdown table with placeholder headers.\n\nExample format:\n| Header 1 | Header 2 |\n|----------|----------|\n\"\"\"\n```\n\nThe prompt list is generated for around 4500 input texts. Using the qwen model i need only a few minutes to generate all. Using the llama model i need around 3sec/iteration resulting in 3-4 hours. I am asking myself why is that so? Are there any specifics regarding the llama model that is haven't considered?\n\nWhat i tried:\n- Changing the `template` to use starting and closing tags\n- Changing the `template` to use no tags at all by just concatenating system_prompt and user_prompt\n- define the bfloat16 dtype when initializing `LLM() \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-01-14T16:16:45+00:00",
    "closed_at": "2025-01-16T15:30:08+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12047/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12047"
  },
  {
    "number": 11683,
    "title": "[Bug]: Error while importing vllm since v0.6.6",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\n>>> import vllm\r\nWarning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.\r\nWARNING 01-02 10:53:16 cuda.py:32] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/__init__.py\", line 3, in <module>\r\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/arg_utils.py\", line 11, in <module>\r\n    from vllm.config import (CacheConfig, CompilationConfig, ConfigFormat,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/config.py\", line 22, in <module>\r\n    from vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/__init__.py\", line 1, in <module>\r\n    from vllm.model_executor.parameter import (BasevLLMParameter,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/parameter.py\", line 7, in <module>\r\n    from vllm.distributed import get_tensor_model_parallel_rank\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/__init__.py\", line 1, in <module>\r\n    from .communication_op import *\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/communication_op.py\", line 6, in <module>\r\n    from .parallel_state import get_tp_group\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/parallel_state.py\", line 38, in <module>\r\n    import vllm.distributed.kv_transfer.kv_transfer_agent as kv_transfer\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/kv_transfer/kv_transfer_agent.py\", line 15, in <module>\r\n    from vllm.distributed.kv_transfer.kv_connector.factory import (\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/kv_transfer/kv_connector/factory.py\", line 3, in <module>\r\n    from .base import KVConnectorBase\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/kv_transfer/kv_connector/base.py\", line 14, in <module>\r\n    from vllm.sequence import IntermediateTensors\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/sequence.py\", line 18, in <module>\r\n    from vllm.multimodal import MultiModalDataDict, MultiModalPlaceholderDict\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/multimodal/__init__.py\", line 6, in <module>\r\n    from .registry import MultiModalRegistry\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/multimodal/registry.py\", line 19, in <module>\r\n    from .video import VideoPlugin\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/multimodal/video.py\", line 4, in <module>\r\n    import cv2\r\n  File \"/usr/local/lib/python3.10/dist-packages/cv2/__init__.py\", line 181, in <module>\r\n    bootstrap()\r\n  File \"/usr/local/lib/python3.10/dist-packages/cv2/__init__.py\", line 175, in bootstrap\r\n    if __load_extra_py_code_for_module(\"cv2\", submodule, DEBUG):\r\n  File \"/usr/local/lib/python3.10/dist-packages/cv2/__init__.py\", line 28, in __load_extra_py_code_for_module\r\n    py_module = importlib.import_module(module_name)\r\n  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/usr/local/lib/python3.10/dist-packages/cv2/typing/__init__.py\", line 171, in <module>\r\n    LayerId = cv2.dnn.DictValue\r\nAttributeError: partially initialized module 'cv2' has no attribute 'dnn' (most likely due to a circular import)\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nSince v0.6.6, simply running `import vllm` results in the following error due to an issue with `cv2`.\r\n<details>\r\n<summary>error message</summary>\r\n\r\n```text\r\n>>> import vllm\r\nWarning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.\r\nWARNING 01-02 10:53:16 cuda.py:32] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/__init__.py\", line 3, in <module>\r\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/arg_utils.py\", line 11, in <module>\r\n    from vllm.config import (CacheConfig, CompilationConfig, ConfigFormat,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/config.py\", line 22, in <module>\r\n    from vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/__init__.py\", line 1, in <module>\r\n    from vllm.model_executor.parameter import (BasevLLMParameter,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/parameter.py\", line 7, in <module>\r\n    from vllm.distributed import get_tensor_model_parallel_rank\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/__init__.py\", line 1, in <module>\r\n    from .communication_op import *\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/communication_op.py\", line 6, in <module>\r\n    from .parallel_state import get_tp_group\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/parallel_state.py\", line 38, in <module>\r\n    import vllm.distributed.kv_transfer.kv_transfer_agent as kv_transfer\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/kv_transfer/kv_transfer_agent.py\", line 15, in <module>\r\n    from vllm.distributed.kv_transfer.kv_connector.factory import (\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/kv_transfer/kv_connector/factory.py\", line 3, in <module>\r\n    from .base import KVConnectorBase\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/kv_transfer/kv_connector/base.py\", line 14, in <module>\r\n    from vllm.sequence import IntermediateTensors\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/sequence.py\", line 18, in <module>\r\n    from vllm.multimodal import MultiModalDataDict, MultiModalPlaceholderDict\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/multimodal/__init__.py\", line 6, in <module>\r\n    from .registry import MultiModalRegistry\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/multimodal/registry.py\", line 19, in <module>\r\n    from .video import VideoPlugin\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/multimodal/video.py\", line 4, in <module>\r\n    import cv2\r\n  File \"/usr/local/lib/python3.10/dist-packages/cv2/__init__.py\", line 181, in <module>\r\n    bootstrap()\r\n  File \"/usr/local/lib/python3.10/dist-packages/cv2/__init__.py\", line 175, in bootstrap\r\n    if __load_extra_py_code_for_module(\"cv2\", submodule, DEBUG):\r\n  File \"/usr/local/lib/python3.10/dist-packages/cv2/__init__.py\", line 28, in __load_extra_py_code_for_module\r\n    py_module = importlib.import_module(module_name)\r\n  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/usr/local/lib/python3.10/dist-packages/cv2/typing/__init__.py\", line 171, in <module>\r\n    LayerId = cv2.dnn.DictValue\r\nAttributeError: partially initialized module 'cv2' has no attribute 'dnn' (most likely due to a circular import)\r\n```\r\n\r\n</details>\r\n\r\nvllm was installed using `pip install vllm` inside a Docker container.\r\n```\r\ndocker run --gpus all -it --ipc=host nvcr.io/nvidia/pytorch:23.10-py3\r\n```\r\n\r\nWhile there may be workarounds to make it work, it would be better to ensure that `pip install vllm` works without modifications(for future versions), as it did in previous versions.\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-01-02T10:58:59+00:00",
    "closed_at": "2025-01-06T06:46:43+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11683/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11683"
  },
  {
    "number": 12293,
    "title": "[Usage]: How can I check which operators are used by vLLM-Llama-2-7b-hf?",
    "body": "### Your current environment\n\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.26.0\nLibc version: glibc-2.35\n\nPython version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-125-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090\nNvidia driver version: 550.120\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               96\nOn-line CPU(s) list:                  0-95\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Gold 6248R CPU @ 3.00GHz\nCPU family:                           6\nModel:                                85\nThread(s) per core:                   2\nCore(s) per socket:                   24\nSocket(s):                            2\nStepping:                             7\nCPU max MHz:                          4000.0000\nCPU min MHz:                          1200.0000\nBogoMIPS:                             6000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke avx512_vnni md_clear flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            1.5 MiB (48 instances)\nL1i cache:                            1.5 MiB (48 instances)\nL2 cache:                             48 MiB (48 instances)\nL3 cache:                             71.5 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-23,48-71\nNUMA node1 CPU(s):                    24-47,72-95\nVulnerability Gather data sampling:   Mitigation; Microcode\nVulnerability Itlb multihit:          KVM: Mitigation: VMX disabled\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; Enhanced IBRS\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Mitigation; TSX disabled\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.0\n[pip3] triton==3.1.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\n[conda] torch                     2.5.1                    pypi_0    pypi\n[conda] torchvision               0.20.1                   pypi_0    pypi\n[conda] transformers              4.48.0                   pypi_0    pypi\n[conda] triton                    3.1.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.6.6.post2.dev188+gd14e98d9.d20250113\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NODE    NODE    24-47,72-95     1               N/A\nNIC0    NODE     X      PIX\nNIC1    NODE    PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n\nLD_LIBRARY_PATH=/workspace/mnt/storage/anaconda3/envs/vllm/lib/python3.10/site-packages/cv2/../../lib64:\nCUDA_MODULE_LOADING=LAZY\n\n\n### How would you like to use vllm\n\nMy vllm version is 0.6.6. \nThis is my test.py file,  I have already set up the source environment. How can I debug to see which low-level operators are being used, or is there a profiler method to capture which operators are used?\nThanks you for your reply.\n```python \nfrom vllm import LLM, SamplingParams\n\nprompts = [\n    \"Hello, my name is\",\n    \"The future of AI is\",\n    \"The life is\", \n]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\nllm = LLM(model=\"/workspace/mnt/storage/xiangxin@supremind.com/xiangxin_infer_/vllm/Llama-2-7b-hf\")\n\noutputs = llm.generate(prompts, sampling_params)\n\n# Print the outputs.\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-01-22T02:00:53+00:00",
    "closed_at": "2025-02-03T02:03:01+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12293/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12293"
  },
  {
    "number": 11763,
    "title": "[Bug]: Cutlass 2:4 Sparsity + FP8/Int8 Quant RuntimeError: Error Internal",
    "body": "### Your current environment\r\n```\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Alibaba Group Enterprise Linux Server 7.2 (Paladin) (x86_64)\r\nGCC version: (GCC) 10.2.1 20200825 (Alibaba 10.2.1-3 2.17)\r\nClang version: Could not collect\r\nCMake version: version 3.26.4\r\nLibc version: glibc-2.32\r\n\r\nPython version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.10.134-16.3.al8.x86_64-x86_64-with-glibc2.32\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H20\r\nGPU 1: NVIDIA H20\r\nGPU 2: NVIDIA H20\r\nGPU 3: NVIDIA H20\r\n\r\nNvidia driver version: 535.183.06\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                192\r\nOn-line CPU(s) list:   0-191\r\nThread(s) per core:    2\r\nCore(s) per socket:    48\r\nSocket(s):             2\r\nNUMA node(s):          2\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 143\r\nModel name:            Intel(R) Xeon(R) Platinum 8469C\r\nStepping:              8\r\nCPU MHz:               3100.000\r\nCPU max MHz:           3800.0000\r\nCPU min MHz:           800.0000\r\nBogoMIPS:              5200.00\r\nVirtualization:        VT-x\r\nL1d cache:             48K\r\nL1i cache:             32K\r\nL2 cache:              2048K\r\nL3 cache:              99840K\r\nNUMA node0 CPU(s):     0-47,96-143\r\nNUMA node1 CPU(s):     48-95,144-191\r\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm uintr md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==7.1.1\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1\r\n[pip3] torchaudio==2.5.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.46.2\r\n[pip3] triton==3.1.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.5.1                    pypi_0    pypi\r\n[conda] torchaudio                2.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.20.1                   pypi_0    pypi\r\n[conda] transformers              4.46.2                   pypi_0    pypi\r\n[conda] triton                    3.1.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.1.dev3896+ga491d6f.d20250103 (git sha: a491d6f.d20250103\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV18    NV18    NV18    NODE    NODE    SYS     SYS     0-47,96-143     0               N/A\r\nGPU1    NV18     X      NV18    NV18    NODE    PIX     SYS     SYS     0-47,96-143     0               N/A\r\nGPU2    NV18    NV18     X      NV18    SYS     SYS     PIX     NODE    48-95,144-191   1               N/A\r\nGPU3    NV18    NV18    NV18     X      SYS     SYS     NODE    NODE    48-95,144-191   1               N/A\r\nNIC0    NODE    NODE    SYS     SYS      X      NODE    SYS     SYS\r\nNIC1    NODE    PIX     SYS     SYS     NODE     X      SYS     SYS\r\nNIC2    SYS     SYS     PIX     NODE    SYS     SYS      X      NODE\r\nNIC3    SYS     SYS     NODE    NODE    SYS     SYS     NODE     X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_bond_0\r\n  NIC1: mlx5_bond_1\r\n  NIC2: mlx5_bond_2\r\n  NIC3: mlx5_bond_3\r\n\r\nNVIDIA_VISIBLE_DEVICES=GPU-2e3c3bd8-5671-14b6-05bd-8b2e12a575f7,GPU-9cbca0f2-6fda-5286-a2d2-3a2627f071ad,GPU-a9850d16-a201-98e7-790b-e1fa485aa444,GPU-9367a5d4-02ff-f20f-3781-69b8f3cacb5a\r\nLD_LIBRARY_PATH=/opt/conda/lib/python3.10/site-packages/cv2/../../lib64::/opt/conda/lib/python3.10/site-packages/aistudio_common/reader/libs/:/opt/taobao/java/jre/lib/amd64/server/:/usr/local/cuda/lib64:/opt/conda/lib/python3.10/site-packages/aistudio_common/reader/libs/:/opt/taobao/java/jre/lib/amd64/server/:/usr/local/cuda/lib64:/opt/conda/lib/python3.10/site-packages/aistudio_common/reader/libs/:/opt/taobao/java/jre/lib/amd64/server/:/usr/local/cuda/lib64:/opt/conda/lib/python3.10/site-packages/aistudio_common/reader/libs/:/opt/taobao/java/jre/lib/amd64/server/:/usr/local/cuda/lib64:/opt/conda/lib/python3.10/site-packages/aistudio_common/reader/libs/:/opt/taobao/java/jre/lib/amd64/server/:/usr/local/cuda/lib64:/opt/conda/lib/python3.10/site-packages/aistudio_common/reader/libs/:/opt/taobao/java/jre/lib/amd64/server/:/usr/local/cuda/lib64\r\nNVIDIA_DRIVER_CAPABILITIES=all\r\nNCCL_NVLS_ENABLE=0\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\n```\r\n[root workflow_47400355 /ossfs/workspace] \u4e00 1\u6708 06 15:46:05\r\n$CUDA_LAUNCH_BLOCKING=1 vllm serve /mntfn/yanyi/sparse/mntfn-FP8-Dynamic/\r\nINFO 01-06 15:46:27 api_server.py:647] vLLM API server version 0.1.dev3896+ga491d6f.d20250103\r\nINFO 01-06 15:46:27 api_server.py:648] args: Namespace(subparser='serve', model_tag='/mntfn/yanyi/sparse/mntfn-FP8-Dynamic/', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=[''], allowed_methods=[''], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/mntfn/yanyi/sparse/mntfn-FP8-Dynamic/', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7fd3d1634c10>)\r\nINFO 01-06 15:46:27 api_server.py:195] Started engine process with PID 65910\r\nINFO 01-06 15:46:40 config.py:518] This model supports multiple tasks: {'reward', 'score', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.\r\nINFO 01-06 15:46:50 config.py:518] This model supports multiple tasks: {'reward', 'classify', 'generate', 'embed', 'score'}. Defaulting to 'generate'.\r\nINFO 01-06 15:46:53 llm_engine.py:234] Initializing an LLM engine (v0.1.dev3896+ga491d6f.d20250103) with config: model='/mntfn/yanyi/sparse/mntfn-FP8-Dynamic/', speculative_config=None, tokenizer='/mntfn/yanyi/sparse/mntfn-FP8-Dynamic/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/mntfn/yanyi/sparse/mntfn-FP8-Dynamic/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True,\r\nINFO 01-06 15:46:57 selector.py:120] Using Flash Attention backend.\r\nINFO 01-06 15:46:59 model_runner.py:1094] Starting to load model /mntfn/yanyi/sparse/mntfn-FP8-Dynamic/...\r\nLoading safetensors checkpoint shards: 0% Completed | 0/16 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards: 6% Completed | 1/16 [00:01<00:16, 1.08s/it]\r\nLoading safetensors checkpoint shards: 12% Completed | 2/16 [00:02<00:17, 1.27s/it]\r\nLoading safetensors checkpoint shards: 19% Completed | 3/16 [00:03<00:17, 1.38s/it]\r\nLoading safetensors checkpoint shards: 25% Completed | 4/16 [00:05<00:16, 1.37s/it]\r\nLoading safetensors checkpoint shards: 31% Completed | 5/16 [00:06<00:14, 1.32s/it]\r\nLoading safetensors checkpoint shards: 38% Completed | 6/16 [00:07<00:12, 1.29s/it]\r\nLoading safetensors checkpoint shards: 44% Completed | 7/16 [00:09<00:11, 1.26s/it]\r\nLoading safetensors checkpoint shards: 50% Completed | 8/16 [00:10<00:09, 1.25s/it]\r\nLoading safetensors checkpoint shards: 56% Completed | 9/16 [00:11<00:08, 1.24s/it]\r\nLoading safetensors checkpoint shards: 62% Completed | 10/16 [00:12<00:07, 1.24s/it]\r\nLoading safetensors checkpoint shards: 69% Completed | 11/16 [00:13<00:06, 1.25s/it]\r\nLoading safetensors checkpoint shards: 75% Completed | 12/16 [00:15<00:05, 1.26s/it]\r\nLoading safetensors checkpoint shards: 81% Completed | 13/16 [00:16<00:03, 1.24s/it]\r\nLoading safetensors checkpoint shards: 88% Completed | 14/16 [00:17<00:02, 1.20s/it]\r\nLoading safetensors checkpoint shards: 94% Completed | 15/16 [00:18<00:01, 1.16s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 16/16 [00:19<00:00, 1.03s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 16/16 [00:19<00:00, 1.21s/it]\r\n\r\nINFO 01-06 15:47:21 model_runner.py:1099] Loading model weights took 45.6023 GB\r\nINFO 01-06 15:47:24 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20250106-154724.pkl...\r\nWARNING 01-06 15:47:24 model_runner_base.py:143] Failed to pickle inputs of failed execution: CUDA error: an illegal memory access was encountered\r\nWARNING 01-06 15:47:24 model_runner_base.py:143] Compile with TORCH_USE_CUDA_DSA to enable device-side assertions.\r\nWARNING 01-06 15:47:24 model_runner_base.py:143]\r\nERROR 01-06 15:47:24 engine.py:366] Error in model execution: Error Internal\r\nERROR 01-06 15:47:24 engine.py:366] Traceback (most recent call last):\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/worker/model_runner_base.py\", line 116, in _wrapper\r\nERROR 01-06 15:47:24 engine.py:366] return func(*args, **kwargs)\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/worker/model_runner.py\", line 1691, in execute_model\r\nERROR 01-06 15:47:24 engine.py:366] hidden_or_intermediate_states = model_executable(\r\nERROR 01-06 15:47:24 engine.py:366] File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nERROR 01-06 15:47:24 engine.py:366] return self._call_impl(*args, **kwargs)\r\nERROR 01-06 15:47:24 engine.py:366] File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\nERROR 01-06 15:47:24 engine.py:366] return forward_call(*args, **kwargs)\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/model_executor/models/qwen2.py\", line 477, in forward\r\nERROR 01-06 15:47:24 engine.py:366] hidden_states = self.model(input_ids, positions, kv_caches,\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/compilation/decorators.py\", line 168, in call\r\nERROR 01-06 15:47:24 engine.py:366] return self.forward(*args, **kwargs)\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/model_executor/models/qwen2.py\", line 340, in forward\r\nERROR 01-06 15:47:24 engine.py:366] hidden_states, residual = layer(\r\nERROR 01-06 15:47:24 engine.py:366] File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nERROR 01-06 15:47:24 engine.py:366] return self._call_impl(*args, **kwargs)\r\nERROR 01-06 15:47:24 engine.py:366] File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\nERROR 01-06 15:47:24 engine.py:366] return forward_call(*args, **kwargs)\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/model_executor/models/qwen2.py\", line 247, in forward\r\nERROR 01-06 15:47:24 engine.py:366] hidden_states = self.self_attn(\r\nERROR 01-06 15:47:24 engine.py:366] File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nERROR 01-06 15:47:24 engine.py:366] return self._call_impl(*args, **kwargs)\r\nERROR 01-06 15:47:24 engine.py:366] File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\nERROR 01-06 15:47:24 engine.py:366] return forward_call(*args, **kwargs)\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/model_executor/models/qwen2.py\", line 173, in forward\r\nERROR 01-06 15:47:24 engine.py:366] qkv, _ = self.qkv_proj(hidden_states)\r\nERROR 01-06 15:47:24 engine.py:366] File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nERROR 01-06 15:47:24 engine.py:366] return self._call_impl(*args, **kwargs)\r\nERROR 01-06 15:47:24 engine.py:366] File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in call_impl\r\nERROR 01-06 15:47:24 engine.py:366] return forward_call(*args, **kwargs)\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/model_executor/layers/linear.py\", line 373, in forward\r\nERROR 01-06 15:47:24 engine.py:366] output_parallel = self.quant_method.apply(self, input, bias)\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py\", line 511, in apply\r\nERROR 01-06 15:47:24 engine.py:366] return scheme.apply_weights(layer, x, bias=bias)\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24.py\", line 172, in apply_weights\r\nERROR 01-06 15:47:24 engine.py:366] out = ops.cutlass_scaled_sparse_mm(a=q_input,\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/_custom_ops.py\", line 628, in cutlass_scaled_sparse_mm\r\nERROR 01-06 15:47:24 engine.py:366] torch.ops._C.cutlass_scaled_sparse_mm(out, a, bt_nzs, bt_meta, scale_a,\r\nERROR 01-06 15:47:24 engine.py:366] File \"/opt/conda/lib/python3.10/site-packages/torch/_ops.py\", line 1116, in call\r\nERROR 01-06 15:47:24 engine.py:366] return self._op(*args, **(kwargs or {}))\r\nERROR 01-06 15:47:24 engine.py:366] RuntimeError: Error Internal\r\nERROR 01-06 15:47:24 engine.py:366]\r\nERROR 01-06 15:47:24 engine.py:366] The above exception was the direct cause of the following exception:\r\nERROR 01-06 15:47:24 engine.py:366]\r\nERROR 01-06 15:47:24 engine.py:366] Traceback (most recent call last):\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/engine/multiprocessing/engine.py\", line 357, in run_mp_engine\r\nERROR 01-06 15:47:24 engine.py:366] engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/engine/multiprocessing/engine.py\", line 119, in from_engine_args\r\nERROR 01-06 15:47:24 engine.py:366] return cls(ipc_path=ipc_path,\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/engine/multiprocessing/engine.py\", line 71, in init\r\nERROR 01-06 15:47:24 engine.py:366] self.engine = LLMEngine(*args, **kwargs)\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/engine/llm_engine.py\", line 276, in init\r\nERROR 01-06 15:47:24 engine.py:366] self._initialize_kv_caches()\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/engine/llm_engine.py\", line 416, in _initialize_kv_caches\r\nERROR 01-06 15:47:24 engine.py:366] self.model_executor.determine_num_available_blocks())\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/executor/gpu_executor.py\", line 68, in determine_num_available_blocks\r\nERROR 01-06 15:47:24 engine.py:366] return self.driver_worker.determine_num_available_blocks()\r\nERROR 01-06 15:47:24 engine.py:366] File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nERROR 01-06 15:47:24 engine.py:366] return func(*args, **kwargs)\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/worker/worker.py\", line 202, in determine_num_available_blocks\r\nERROR 01-06 15:47:24 engine.py:366] self.model_runner.profile_run()\r\nERROR 01-06 15:47:24 engine.py:366] File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nERROR 01-06 15:47:24 engine.py:366] return func(*args, **kwargs)\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/worker/model_runner.py\", line 1331, in profile_run\r\nERROR 01-06 15:47:24 engine.py:366] self.execute_model(model_input, kv_caches, intermediate_tensors)\r\nERROR 01-06 15:47:24 engine.py:366] File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nERROR 01-06 15:47:24 engine.py:366] return func(*args, **kwargs)\r\nERROR 01-06 15:47:24 engine.py:366] File \"/ossfs/workspace/ant_vllm/vllm/worker/model_runner_base.py\", line 146, in _wrapper\r\nERROR 01-06 15:47:24 engine.py:366] raise type(err)(f\"Error in model execution: \"\r\nERROR 01-06 15:47:24 engine.py:366] RuntimeError: Error in model execution: Error Internal\r\nProcess SpawnProcess-1:\r\nTraceback (most recent call last):\r\nFile \"/ossfs/workspace/ant_vllm/vllm/worker/model_runner_base.py\", line 116, in _wrapper\r\nreturn func(*args, **kwargs)\r\nFile \"/ossfs/workspace/ant_vllm/vllm/worker/model_runner.py\", line 1691, in execute_model\r\nhidden_or_intermediate_states = model_executable(\r\nFile \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nreturn self._call_impl(*args, **kwargs)\r\nFile \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\nreturn forward_call(*args, **kwargs)\r\nFile \"/ossfs/workspace/ant_vllm/vllm/model_executor/models/qwen2.py\", line 477, in forward\r\nhidden_states = self.model(input_ids, positions, kv_caches,\r\nFile \"/ossfs/workspace/ant_vllm/vllm/compilation/decorators.py\", line 168, in call\r\nreturn self.forward(*args, **kwargs)\r\nFile \"/ossfs/workspace/ant_vllm/vllm/model_executor/models/qwen2.py\", line 340, in forward\r\nhidden_states, residual = layer(\r\nFile \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nreturn self._call_impl(*args, **kwargs)\r\nFile \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\nreturn forward_call(*args, **kwargs)\r\nFile \"/ossfs/workspace/ant_vllm/vllm/model_executor/models/qwen2.py\", line 247, in forward\r\nhidden_states = self.self_attn(\r\nFile \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nreturn self._call_impl(*args, **kwargs)\r\nFile \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\nreturn forward_call(*args, **kwargs)\r\nFile \"/ossfs/workspace/ant_vllm/vllm/model_executor/models/qwen2.py\", line 173, in forward\r\nqkv, _ = self.qkv_proj(hidden_states)\r\nFile \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nreturn self._call_impl(*args, **kwargs)\r\nFile \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in call_impl\r\nreturn forward_call(*args, **kwargs)\r\nFile \"/ossfs/workspace/ant_vllm/vllm/model_executor/layers/linear.py\", line 373, in forward\r\noutput_parallel = self.quant_method.apply(self, input, bias)\r\nFile \"/ossfs/workspace/ant_vllm/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py\", line 511, in apply\r\nreturn scheme.apply_weights(layer, x, bias=bias)\r\nFile \"/ossfs/workspace/ant_vllm/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24.py\", line 172, in apply_weights\r\nout = ops.cutlass_scaled_sparse_mm(a=q_input,\r\nFile \"/ossfs/workspace/ant_vllm/vllm/_custom_ops.py\", line 628, in cutlass_scaled_sparse_mm\r\ntorch.ops._C.cutlass_scaled_sparse_mm(out, a, bt_nzs, bt_meta, scale_a,\r\nFile \"/opt/conda/lib/python3.10/site-packages/torch/_ops.py\", line 1116, in call\r\nreturn self._op(*args, **(kwargs or {}))\r\nRuntimeError: Error Internal\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\nFile \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\r\nself.run()\r\nFile \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\r\nself._target(*self._args, **self._kwargs)\r\nFile \"/ossfs/workspace/ant_vllm/vllm/engine/multiprocessing/engine.py\", line 368, in run_mp_engine\r\nraise e\r\nFile \"/ossfs/workspace/ant_vllm/vllm/engine/multiprocessing/engine.py\", line 357, in run_mp_engine\r\nengine = MQLLMEngine.from_engine_args(engine_args=engine_args,\r\nFile \"/ossfs/workspace/ant_vllm/vllm/engine/multiprocessing/engine.py\", line 119, in from_engine_args\r\nreturn cls(ipc_path=ipc_path,\r\nFile \"/ossfs/workspace/ant_vllm/vllm/engine/multiprocessing/engine.py\", line 71, in init\r\nself.engine = LLMEngine(*args, **kwargs)\r\nFile \"/ossfs/workspace/ant_vllm/vllm/engine/llm_engine.py\", line 276, in init\r\nself._initialize_kv_caches()\r\nFile \"/ossfs/workspace/ant_vllm/vllm/engine/llm_engine.py\", line 416, in _initialize_kv_caches\r\nself.model_executor.determine_num_available_blocks())\r\nFile \"/ossfs/workspace/ant_vllm/vllm/executor/gpu_executor.py\", line 68, in determine_num_available_blocks\r\nreturn self.driver_worker.determine_num_available_blocks()\r\nFile \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nreturn func(*args, **kwargs)\r\nFile \"/ossfs/workspace/ant_vllm/vllm/worker/worker.py\", line 202, in determine_num_available_blocks\r\nself.model_runner.profile_run()\r\nFile \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nreturn func(*args, **kwargs)\r\nFile \"/ossfs/workspace/ant_vllm/vllm/worker/model_runner.py\", line 1331, in profile_run\r\nself.execute_model(model_input, kv_caches, intermediate_tensors)\r\nFile \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nreturn func(*args, **kwargs)\r\nFile \"/ossfs/workspace/ant_vllm/vllm/worker/model_runner_base.py\", line 146, in _wrapper\r\nraise type(err)(f\"Error in model execution: \"\r\nRuntimeError: Error in model execution: Error Internal\r\n[rank0]:[W106 15:47:25.908723004 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present, but this warning has only been added since PyTorch 2.4 (function operator())\r\nTask exception was never retrieved\r\nfuture: <Task finished name='Task-2' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /ossfs/workspace/ant_vllm/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\r\nTraceback (most recent call last):\r\nFile \"/ossfs/workspace/ant_vllm/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\r\nwhile await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\r\nFile \"/opt/conda/lib/python3.10/site-packages/zmq/_future.py\", line 400, in poll\r\nraise _zmq.ZMQError(_zmq.ENOTSUP)\r\nzmq.error.ZMQError: Operation not supported\r\nTask exception was never retrieved\r\nfuture: <Task finished name='Task-3' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /ossfs/workspace/ant_vllm/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\r\nTraceback (most recent call last):\r\nFile \"/ossfs/workspace/ant_vllm/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\r\nwhile await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\r\nFile \"/opt/conda/lib/python3.10/site-packages/zmq/_future.py\", line 400, in poll\r\nraise _zmq.ZMQError(_zmq.ENOTSUP)\r\nzmq.error.ZMQError: Operation not supported\r\nTask exception was never retrieved\r\nfuture: <Task finished name='Task-4' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /ossfs/workspace/ant_vllm/vllm/engine/multiprocessing/client.py:178> exception=ZMQError('Operation not supported')>\r\nTraceback (most recent call last):\r\nFile \"/ossfs/workspace/ant_vllm/vllm/engine/multiprocessing/client.py\", line 184, in run_output_handler_loop\r\nwhile await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\r\nFile \"/opt/conda/lib/python3.10/site-packages/zmq/_future.py\", line 400, in poll\r\nraise _zmq.ZMQError(_zmq.ENOTSUP)\r\nzmq.error.ZMQError: Operation not supported\r\nTraceback (most recent call last):\r\nFile \"/opt/conda/bin/vllm\", line 8, in\r\nsys.exit(main())\r\nFile \"/ossfs/workspace/ant_vllm/vllm/scripts.py\", line 201, in main\r\nargs.dispatch_function(args)\r\nFile \"/ossfs/workspace/ant_vllm/vllm/scripts.py\", line 42, in serve\r\nuvloop.run(run_server(args))\r\nFile \"/opt/conda/lib/python3.10/site-packages/uvloop/init.py\", line 82, in run\r\nreturn loop.run_until_complete(wrapper())\r\nFile \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\r\nFile \"/opt/conda/lib/python3.10/site-packages/uvloop/init.py\", line 61, in wrapper\r\nreturn await main\r\nFile \"/ossfs/workspace/ant_vllm/vllm/entrypoints/openai/api_server.py\", line 671, in run_server\r\nasync with build_async_engine_client(args) as engine_client:\r\nFile \"/opt/conda/lib/python3.10/contextlib.py\", line 199, in aenter\r\nreturn await anext(self.gen)\r\nFile \"/ossfs/workspace/ant_vllm/vllm/entrypoints/openai/api_server.py\", line 114, in build_async_engine_client\r\nasync with build_async_engine_client_from_engine_args(\r\nFile \"/opt/conda/lib/python3.10/contextlib.py\", line 199, in aenter\r\nreturn await anext(self.gen)\r\nFile \"/ossfs/workspace/ant_vllm/vllm/entrypoints/openai/api_server.py\", line 219, in build_async_engine_client_from_engine_args\r\nraise RuntimeError(\r\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\r\n```\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-06T08:02:19+00:00",
    "closed_at": "2025-04-10T02:11:10+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11763/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11763"
  },
  {
    "number": 12245,
    "title": "[Bug]: init_mm_limits_per_prompt not been called when using V1 + TensorSplit + Qwen2VL",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.10.16 (main, Jan  5 2025, 05:32:43) [Clang 19.1.6 ] (64-bit runtime)\nPython platform: Linux-5.10.135.bsk.6-amd64-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: NVIDIA H20\nGPU 1: NVIDIA H20\n\nNvidia driver version: 535.161.08\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   52 bits physical, 57 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          192\nOn-line CPU(s) list:             0-191\nVendor ID:                       GenuineIntel\nModel name:                      Intel(R) Xeon(R) Platinum 8457C\nCPU family:                      6\nModel:                           143\nThread(s) per core:              2\nCore(s) per socket:              48\nSocket(s):                       2\nStepping:                        8\nCPU(s) scaling MHz:              82%\nCPU max MHz:                     3800.0000\nCPU min MHz:                     800.0000\nBogoMIPS:                        5200.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                  VT-x\nL1d cache:                       4.5 MiB (96 instances)\nL1i cache:                       3 MiB (96 instances)\nL2 cache:                        192 MiB (96 instances)\nL3 cache:                        195 MiB (2 instances)\nNUMA node(s):                    2\nNUMA node0 CPU(s):               0-47,96-143\nNUMA node1 CPU(s):               48-95,144-191\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Not affected\nVulnerability Retbleed:          Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.5.1+cu124\n[pip3] torchvision==0.20.1+cu124\n[pip3] transformers==4.47.1\n[pip3] transformers-stream-generator==0.0.5\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.6.6.post2.dev274+g81763c58\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU1    NV18     X      NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nNIC0    NODE    NODE     X      NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC1    NODE    NODE    NODE     X      NODE    NODE    SYS     SYS     SYS     SYS\nNIC2    PIX     NODE    NODE    NODE     X      NODE    SYS     SYS     SYS     SYS\nNIC3    NODE    PIX     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     SYS     SYS      X      NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     SYS     SYS     NODE     X      NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      NODE\nNIC7    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_1\n  NIC1: mlx5_2\n  NIC2: mlx5_3\n  NIC3: mlx5_4\n  NIC4: mlx5_5\n  NIC5: mlx5_6\n  NIC6: mlx5_7\n  NIC7: mlx5_8\n\nLD_LIBRARY_PATH=/mnt/bn/nas-develop-lyc/mlx/users/yuchen.lyc/src/ui_vllm_deploy/.venv/lib/python3.10/site-packages/cv2/../../lib64:/opt/tiger/yarn_deploy/hadoop/lib/native:/opt/tiger/yarn_deploy/hadoop_current/lib/native:/opt/tiger/yarn_deploy/hadoop_current/lzo/lib:\nNVIDIA_VISIBLE_DEVICES=GPU-80bf8a83-cec5-bb7c-3b4f-cd7cbc1dd155,GPU-60c4fea0-31e5-aa88-dfe7-5e570abb533c\nNVIDIA_REQUIRE_CUDA=cuda>=10.1\nCUDA_HOME=/usr/local/cuda\nCUDA_HOME=/usr/local/cuda\nNVIDIA_DRIVER_CAPABILITIES=all\nCUDA_MODULE_LOADING=LAZY\n \n```\n\n</details>\n\n\n### Model Input Dumps\n\nNo input.\n\n### \ud83d\udc1b Describe the bug\nV1 engine works for qwen2-vl only when single gpu, but not tensor-split(multi-gpu).\n\n```log\nMAX_PIXELS=1003520\nMIN_PIXELS=250880\nINFO 01-21 13:45:38 __init__.py:179] Automatically detected platform cuda.\nINFO 01-21 13:45:42 api_server.py:768] vLLM API server version 0.6.6.post2.dev274+g81763c58\nINFO 01-21 13:45:42 api_server.py:769] args: Namespace(subparser='serve', model_tag='/mnt/bn/nas-develop-lyc/mlx/users/yuchen.lyc/src/ui_vllm_deploy/bin/../model/Qwen/Qwen2-VL-7B-Instruct', config='', host='::', port=11111, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/mnt/bn/nas-develop-lyc/mlx/users/yuchen.lyc/src/ui_vllm_deploy/bin/../model/Qwen/Qwen2-VL-7B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=32768, guided_decoding_backend='lm-format-enforcer', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt={'image': 5}, mm_processor_kwargs={'min_pixels': 250880, 'max_pixels': 1003520}, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['qwen2-vl-7b-instruct'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7fd55eb38e50>)\nWARNING 01-21 13:45:42 arg_utils.py:1283] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.\nINFO 01-21 13:45:56 config.py:520] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting to 'generate'.\nINFO 01-21 13:45:56 config.py:1327] Defaulting to use mp for distributed inference\nINFO 01-21 13:45:56 config.py:1482] Chunked prefill is enabled with max_num_batched_tokens=2048.\nINFO 01-21 13:46:04 __init__.py:179] Automatically detected platform cuda.\nINFO 01-21 13:46:08 core.py:45] Initializing an LLM engine (v0.6.6.post2.dev274+g81763c58) with config: model='/mnt/bn/nas-develop-lyc/mlx/users/yuchen.lyc/src/ui_vllm_deploy/bin/../model/Qwen/Qwen2-VL-7B-Instruct', speculative_config=None, tokenizer='/mnt/bn/nas-develop-lyc/mlx/users/yuchen.lyc/src/ui_vllm_deploy/bin/../model/Qwen/Qwen2-VL-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='lm-format-enforcer'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=qwen2-vl-7b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs={'min_pixels': 250880, 'max_pixels': 1003520}, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"candidate_compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"compile_sizes\":[],\"capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\nWARNING 01-21 13:46:08 multiproc_worker_utils.py:298] Reducing Torch parallelism from 96 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 01-21 13:46:08 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\nINFO 01-21 13:46:08 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_abae8bd3'), local_subscribe_port=36709, remote_subscribe_port=None)\nINFO 01-21 13:46:16 __init__.py:179] Automatically detected platform cuda.\n(VllmWorker rank=0 pid=7701) INFO 01-21 13:46:19 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e2892ec4'), local_subscribe_port=63511, remote_subscribe_port=None)\nINFO 01-21 13:46:27 __init__.py:179] Automatically detected platform cuda.\n(VllmWorker rank=1 pid=7789) INFO 01-21 13:46:31 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c1c0dd5b'), local_subscribe_port=47903, remote_subscribe_port=None)\n(VllmWorker rank=0 pid=7701) INFO 01-21 13:46:32 utils.py:938] Found nccl from library libnccl.so.2\n(VllmWorker rank=1 pid=7789) INFO 01-21 13:46:32 utils.py:938] Found nccl from library libnccl.so.2\n(VllmWorker rank=0 pid=7701) INFO 01-21 13:46:32 pynccl.py:67] vLLM is using nccl==2.21.5\n(VllmWorker rank=1 pid=7789) INFO 01-21 13:46:32 pynccl.py:67] vLLM is using nccl==2.21.5\n(VllmWorker rank=0 pid=7701) INFO 01-21 13:46:32 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n(VllmWorker rank=1 pid=7789) INFO 01-21 13:46:32 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n(VllmWorker rank=0 pid=7701) INFO 01-21 13:46:32 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_0220bb80'), local_subscribe_port=50433, remote_subscribe_port=None)\n(VllmWorker rank=1 pid=7789) INFO 01-21 13:46:34 gpu_model_runner.py:815] Starting to load model /mnt/bn/nas-develop-lyc/mlx/users/yuchen.lyc/src/ui_vllm_deploy/bin/../model/Qwen/Qwen2-VL-7B-Instruct...\n(VllmWorker rank=0 pid=7701) INFO 01-21 13:46:34 gpu_model_runner.py:815] Starting to load model /mnt/bn/nas-develop-lyc/mlx/users/yuchen.lyc/src/ui_vllm_deploy/bin/../model/Qwen/Qwen2-VL-7B-Instruct...\n(VllmWorker rank=0 pid=7701) INFO 01-21 13:46:34 cuda.py:157] Using Flash Attention backend on V1 engine.\n(VllmWorker rank=1 pid=7789) INFO 01-21 13:46:34 cuda.py:157] Using Flash Attention backend on V1 engine.\n(VllmWorker rank=1 pid=7789) WARNING 01-21 13:46:35 topk_topp_sampler.py:44] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n(VllmWorker rank=0 pid=7701) WARNING 01-21 13:46:35 topk_topp_sampler.py:44] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:01,  2.06it/s]\nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:01,  1.86it/s]\nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:01<00:01,  1.80it/s]\nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:02<00:00,  1.73it/s]\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:02<00:00,  2.43it/s]\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:02<00:00,  2.11it/s]\n(VllmWorker rank=0 pid=7701)\n(VllmWorker rank=0 pid=7701) INFO 01-21 13:46:37 gpu_model_runner.py:820] Loading model weights took 7.7541 GB\n(VllmWorker rank=1 pid=7789) INFO 01-21 13:46:37 gpu_model_runner.py:820] Loading model weights took 7.7541 GB\n(VllmWorker rank=0 pid=7701) INFO 01-21 13:46:37 gpu_model_runner.py:897] Encoder cache will be initialized with a budget of 9800 tokens, and profiled with 1 video items of the maximum feature size.\n(VllmWorker rank=1 pid=7789) INFO 01-21 13:46:37 gpu_model_runner.py:897] Encoder cache will be initialized with a budget of 9800 tokens, and profiled with 1 video items of the maximum feature size.\n(VllmWorker rank=1 pid=7789) Keyword argument `min_pixels` is not a valid argument for this processor and will be ignored.\n(VllmWorker rank=1 pid=7789) Keyword argument `max_pixels` is not a valid argument for this processor and will be ignored.\n(VllmWorker rank=0 pid=7701) Keyword argument `min_pixels` is not a valid argument for this processor and will be ignored.\n(VllmWorker rank=0 pid=7701) Keyword argument `max_pixels` is not a valid argument for this processor and will be ignored.\n(VllmWorker rank=0 pid=7701) It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n(VllmWorker rank=1 pid=7789) It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n(VllmWorker rank=0 pid=7701) INFO 01-21 13:46:48 backends.py:571] Using cache directory: /root/.cache/vllm/torch_compile_cache/9f30b8927e/rank_0 for vLLM's torch.compile\n(VllmWorker rank=0 pid=7701) INFO 01-21 13:46:48 backends.py:579] Dynamo bytecode transform time: 8.61 s\n(VllmWorker rank=1 pid=7789) INFO 01-21 13:46:48 backends.py:571] Using cache directory: /root/.cache/vllm/torch_compile_cache/9f30b8927e/rank_1 for vLLM's torch.compile\n(VllmWorker rank=1 pid=7789) INFO 01-21 13:46:48 backends.py:579] Dynamo bytecode transform time: 8.61 s\n(VllmWorker rank=0 pid=7701) INFO 01-21 13:46:52 backends.py:309] Cache the graph of shape None for later use\n(VllmWorker rank=1 pid=7789) INFO 01-21 13:46:52 backends.py:309] Cache the graph of shape None for later use\n(VllmWorker rank=1 pid=7789) INFO 01-21 13:47:13 backends.py:321] Compiling a graph for general shape takes 24.17 s\n(VllmWorker rank=0 pid=7701) INFO 01-21 13:47:13 backends.py:321] Compiling a graph for general shape takes 24.34 s\n(VllmWorker rank=1 pid=7789) INFO 01-21 13:47:15 monitor.py:31] torch.compile takes 32.78 s in total\n(VllmWorker rank=0 pid=7701) INFO 01-21 13:47:15 monitor.py:31] torch.compile takes 32.95 s in total\nINFO 01-21 13:47:15 kv_cache_utils.py:395] # GPU blocks: 174004\n(VllmWorker rank=1 pid=7789) INFO 01-21 13:47:34 custom_all_reduce.py:224] Registering 3752 cuda graph addresses\n(VllmWorker rank=0 pid=7701) INFO 01-21 13:47:36 custom_all_reduce.py:224] Registering 3752 cuda graph addresses\n(VllmWorker rank=1 pid=7789) INFO 01-21 13:47:36 gpu_model_runner.py:990] Graph capturing finished in 21 secs, took 1.77 GiB\n(VllmWorker rank=0 pid=7701) INFO 01-21 13:47:36 gpu_model_runner.py:990] Graph capturing finished in 21 secs, took 1.77 GiB\nINFO 01-21 13:47:36 core.py:89] init engine (profile, create kv cache, warmup model) took 58.62 seconds\nERROR 01-21 13:47:36 core.py:205] EngineCore hit an exception: Traceback (most recent call last):\nERROR 01-21 13:47:36 core.py:205]   File \"/mnt/bn/nas-develop-lyc/mlx/users/yuchen.lyc/src/ui_vllm_deploy/.venv/lib/python3.10/site-packages/vllm/multimodal/registry.py\", line 94, in __getitem__\nERROR 01-21 13:47:36 core.py:205]     return super().__getitem__(key)\nERROR 01-21 13:47:36 core.py:205]   File \"/root/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/collections/__init__.py\", line 1106, in __getitem__\nERROR 01-21 13:47:36 core.py:205]     raise KeyError(key)\nERROR 01-21 13:47:36 core.py:205] KeyError: <vllm.config.ModelConfig object at 0x7fec41821900>\nERROR 01-21 13:47:36 core.py:205]\nERROR 01-21 13:47:36 core.py:205] The above exception was the direct cause of the following exception:\nERROR 01-21 13:47:36 core.py:205]\nERROR 01-21 13:47:36 core.py:205] Traceback (most recent call last):\nERROR 01-21 13:47:36 core.py:205]   File \"/mnt/bn/nas-develop-lyc/mlx/users/yuchen.lyc/src/ui_vllm_deploy/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 197, in run_engine_core\nERROR 01-21 13:47:36 core.py:205]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 01-21 13:47:36 core.py:205]   File \"/mnt/bn/nas-develop-lyc/mlx/users/yuchen.lyc/src/ui_vllm_deploy/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 151, in __init__\nERROR 01-21 13:47:36 core.py:205]     super().__init__(vllm_config, executor_class)\nERROR 01-21 13:47:36 core.py:205]   File \"/mnt/bn/nas-develop-lyc/mlx/users/yuchen.lyc/src/ui_vllm_deploy/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 58, in __init__\nERROR 01-21 13:47:36 core.py:205]     self.scheduler = Scheduler(\nERROR 01-21 13:47:36 core.py:205]   File \"/mnt/bn/nas-develop-lyc/mlx/users/yuchen.lyc/src/ui_vllm_deploy/.venv/lib/python3.10/site-packages/vllm/v1/core/scheduler.py\", line 78, in __init__\nERROR 01-21 13:47:36 core.py:205]     encoder_compute_budget, encoder_cache_size = compute_encoder_budget(\nERROR 01-21 13:47:36 core.py:205]   File \"/mnt/bn/nas-develop-lyc/mlx/users/yuchen.lyc/src/ui_vllm_deploy/.venv/lib/python3.10/site-packages/vllm/v1/core/encoder_cache_manager.py\", line 83, in compute_encoder_budget\nERROR 01-21 13:47:36 core.py:205]     ) = _compute_encoder_budget_multimodal(model_config, scheduler_config)\nERROR 01-21 13:47:36 core.py:205]   File \"/mnt/bn/nas-develop-lyc/mlx/users/yuchen.lyc/src/ui_vllm_deploy/.venv/lib/python3.10/site-packages/vllm/v1/core/encoder_cache_manager.py\", line 106, in _compute_encoder_budget_multimodal\nERROR 01-21 13:47:36 core.py:205]     max_tokens_by_modality_dict = MULTIMODAL_REGISTRY.get_max_tokens_per_item_by_nonzero_modality(  # noqa: E501\nERROR 01-21 13:47:36 core.py:205]   File \"/mnt/bn/nas-develop-lyc/mlx/users/yuchen.lyc/src/ui_vllm_deploy/.venv/lib/python3.10/site-packages/vllm/multimodal/registry.py\", line 285, in get_max_tokens_per_item_by_nonzero_modality\nERROR 01-21 13:47:36 core.py:205]     limits_per_plugin = self._limits_by_model[model_config]\nERROR 01-21 13:47:36 core.py:205]   File \"/mnt/bn/nas-develop-lyc/mlx/users/yuchen.lyc/src/ui_vllm_deploy/.venv/lib/python3.10/site-packages/vllm/multimodal/registry.py\", line 98, in __getitem__\nERROR 01-21 13:47:36 core.py:205]     raise KeyError(msg) from exc\nERROR 01-21 13:47:36 core.py:205] KeyError: 'Cannot find `mm_limits` for model=/mnt/bn/nas-develop-lyc/mlx/users/yuchen.lyc/src/ui_vllm_deploy/bin/../model/Qwen/Qwen2-VL-7B-Instruct. Did you forget to call `init_mm_limits_per_prompt`?'\nERROR 01-21 13:47:36 core.py:205]\nCRITICAL 01-21 13:47:36 core_client.py:146] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\nbin/qwen2_vl-deploy_vllm.sh: line 46:  7164 Killed                  vllm serve ${MODEL_DIR}/${MODEL_ID} --served-model-name ${MODEL_TYPE} --host :: --port ${PORT} --guided-decoding-backend lm-format-enforcer --tensor-parallel-size ${NUM_DEVICE} --enable-prefix-caching --gpu-memory-utilization 0.9 --max-model-len ${MAX_MODEL_LEN} --mm-processor-kwargs \"{\\\"min_pixels\\\": $MIN_PIXELS, \\\"max_pixels\\\": $MAX_PIXELS}\" --limit-mm-per-prompt image=${NUM_IMAGES_PER_REQUEST}\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-01-21T05:50:38+00:00",
    "closed_at": "2025-01-21T10:09:40+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12245/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12245"
  },
  {
    "number": 12012,
    "title": "[Doc]: Performance/Optimization Page doesn't mention Pipeline Parallel Size",
    "body": "### \ud83d\udcda The doc issue\n\nIn the Page\r\nhttps://github.com/vllm-project/vllm/blob/main/docs/source/performance/optimization.md\r\n\r\nOne of the recommended options includes the following:\r\n\r\n```\r\nIncrease tensor_parallel_size. This approach shards model weights, so each GPU has more memory available for KV cache.\r\n```\r\n\r\nThis document does not mention increasing `pipeline_parallel_size` which would also result in the model being sharded across more GPUs so their is more memory available for KV cache.\n\n### Suggest a potential alternative/fix\n\nIncrease `tensor_parallel_size` or `pipeline_parallel_size` (if using Multi-Node Multi-GPU). This approach shards model weights, so each GPU has more memory available for KV cache.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2025-01-13T15:36:07+00:00",
    "closed_at": "2025-03-01T05:43:56+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12012/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/12012"
  },
  {
    "number": 11854,
    "title": "[New Model]: Jinja template for  Llama3.3",
    "body": "### The model to consider.\r\n\r\nhttps://huggingface.co/meta-llama/Llama-3.3-70B-Instruct\r\n\r\n### The closest model vllm already supports.\r\n\r\nvllm already supports Llama3.3 deployment but I am wondering on the chat template for it.\r\n\r\n### What's your difficulty of supporting the model you want?\r\n\r\nLooking for chat template examples for Llama3.3 and wondering the closest or best recommended from https://github.com/vllm-project/vllm/tree/main/examples ?\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-08T15:38:53+00:00",
    "closed_at": "2025-05-10T02:06:44+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11854/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11854"
  },
  {
    "number": 11702,
    "title": "[Bug]: vLLM LoRA Crash when using Dynamic Loading",
    "body": "### Your current environment\r\n\r\n```\r\nroot@mistral-7b-lora-7946cc6459-jqx4h:/vllm-workspace# python3 collect_env.py \r\nCollecting environment information...\r\nPyTorch version: 2.5.0+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.12.7 (main, Oct  1 2024, 08:52:12) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.6.41-amd64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3\r\nNvidia driver version: 535.86.10\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        46 bits physical, 57 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               192\r\nOn-line CPU(s) list:                  0-191\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Xeon(R) Platinum 8468\r\nCPU family:                           6\r\nModel:                                143\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   48\r\nSocket(s):                            2\r\nStepping:                             8\r\nCPU max MHz:                          3800.0000\r\nCPU min MHz:                          800.0000\r\nBogoMIPS:                             4200.00\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                       VT-x\r\nL1d cache:                            4.5 MiB (96 instances)\r\nL1i cache:                            3 MiB (96 instances)\r\nL2 cache:                             192 MiB (96 instances)\r\nL3 cache:                             210 MiB (2 instances)\r\nNUMA node(s):                         2\r\nNUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190\r\nNUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159,161,163,165,167,169,171,173,175,177,179,181,183,185,187,189,191\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.6+cu121torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.0\r\n[pip3] torchvision==0.20.0\r\n[pip3] transformers==4.46.1\r\n[pip3] triton==3.1.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.1.dev1+g7b5f655 (git sha: 7b5f655\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0,2,4,6,8,10    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNVIDIA_VISIBLE_DEVICES=GPU-710a76e8-c24c-99c5-d8a8-a734dbc7e932\r\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\r\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\r\nVLLM_USAGE_SOURCE=production-docker-image\r\nCUDA_VERSION=12.4.1\r\nVLLM_ALLOW_RUNTIME_LORA_UPDATING=True\r\nLD_LIBRARY_PATH=/usr/local/lib/python3.12/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nVLLM_NO_USAGE_STATS=1\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nWe encountered a 500 error while testing the dynamic loading of LoRA with vLLM.\r\n## Steps to Reproduce:\r\n\r\n\r\n### Load LoRA dynamically:\r\n```\r\ncurl -X 'POST' \\\r\n  'https://llm-lora-demo.ssdl.only.sap/v1/load_lora_adapter' \\\r\n  -H 'accept: application/json' \\\r\n  -H 'Content-Type: application/json' \\\r\n  -d '{\r\n  \"lora_name\": \"Mistral-7B-Instruct-v0.3-lora\",\r\n  \"lora_path\": \"ssdl-lora/Mistral-7B-Instruct-v0.3-lora/\"\r\n}'\r\n```\r\n\r\n### The LoRA was loaded correctly as confirmed:\r\n```\r\ncurl -X 'GET' \\\r\n  'https://llm-lora-demo.ssdl.only.sap/v1/models' \\\r\n  -H 'accept: application/json'\r\n\r\n{\r\n  \"id\": \"Mistral-7B-Instruct-v0.3-lora\",\r\n  \"object\": \"model\",\r\n  \"created\": 1735799863,\r\n  \"owned_by\": \"vllm\",\r\n  \"root\": \"ssdl-lora/Mistral-7B-Instruct-v0.3-lora/\",\r\n  \"parent\": \"mistralai/Mistral-7B-Instruct-v0.3\",\r\n  \"max_model_len\": null,\r\n  \"permission\": [\r\n    {\r\n      \"id\": \"modelperm-5582e5de3b2c4d7bb4d56af8b10a9ffc\",\r\n      \"object\": \"model_permission\",\r\n      \"created\": 1735799863,\r\n      \"allow_create_engine\": false,\r\n      \"allow_sampling\": true,\r\n      \"allow_logprobs\": true,\r\n      \"allow_search_indices\": false,\r\n      \"allow_view\": true,\r\n      \"allow_fine_tuning\": false,\r\n      \"organization\": \"*\",\r\n      \"group\": null,\r\n      \"is_blocking\": false\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n### Attempt to use the loaded LoRA:\r\n```\r\ncurl -X 'POST' \\\r\n  'https://llm-lora-demo.ssdl.only.sap/v1/chat/completions' \\\r\n  -H 'accept: application/json' \\\r\n  -H 'Content-Type: application/json' \\\r\n  -d '{\r\n  \"model\": \"Mistral-7B-Instruct-v0.3-lora\",\r\n  \"max_tokens\": 200,\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"system\",\r\n      \"content\": \"You are a multilingual expert in Roman history.\"\r\n    },\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"please explain the meaning of \\\"alea iacta est\\\" and its associated story in English.\"\r\n    }\r\n  ]\r\n}'\r\n```\r\n## Observed Error:\r\n```\r\nWARNING 01-01 22:45:09 api_server.py:378] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!\r\nINFO 01-01 22:45:09 api_server.py:531] vLLM API server version 0.1.dev1+g7b5f655\r\nINFO 01-01 22:45:09 api_server.py:532] args: Namespace(subparser='serve', model_tag='mistralai/Mistral-7B-Instruct-v0.3', config='', host='0.0.0.0', port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=[LoRAModulePath(name='e5-mistral-7b', path='/ssdl-lora/e5-mistral-7b-instruct/lora', base_model_name=None)], prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='mistralai/Mistral-7B-Instruct-v0.3', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', chat_template_text_format='string', trust_remote_code=True, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.4, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=True, max_loras=10, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', pooling_type=None, pooling_norm=None, pooling_softmax=None, pooling_step_tag_id=None, pooling_returned_token_ids=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7f4fd2e98040>)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/vllm\", line 8, in <module>\r\n    sys.exit(main())\r\n             ^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/scripts.py\", line 195, in main\r\n    args.dispatch_function(args)\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/scripts.py\", line 41, in serve\r\n    uvloop.run(run_server(args))\r\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\r\n    return __asyncio.run(\r\n           ^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\r\n    return runner.run(main)\r\n           ^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\r\n    return self._loop.run_until_complete(task)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\r\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\r\n    return await main\r\n           ^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 547, in run_server\r\n    sock.bind((args.host, args.port))\r\nOSError: [Errno 98] Address already in use\r\n```\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-01-03T02:49:49+00:00",
    "closed_at": "2025-01-10T07:56:37+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11702/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11702"
  },
  {
    "number": 11792,
    "title": "[Bug]: Run Pixtral-Large-Instruct-2411 raised a error Attempted to assign 1 x 2074 = 2074 multimodal tokens to 2040 placeholders",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 10.5.0-1ubuntu1~20.04) 10.5.0\r\nClang version: Could not collect\r\nCMake version: version 3.31.2\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.12.8 (main, Dec  4 2024, 08:54:13) [GCC 9.4.0] (64-bit runtime)\r\nPython platform: Linux-5.14.0-284.11.1.el9_2.x86_64-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A800 80GB PCIe\r\nGPU 1: NVIDIA A800 80GB PCIe\r\nGPU 2: NVIDIA A800 80GB PCIe\r\nGPU 3: NVIDIA A800 80GB PCIe\r\nGPU 4: NVIDIA A800 80GB PCIe\r\nGPU 5: NVIDIA A800 80GB PCIe\r\nGPU 6: NVIDIA A800 80GB PCIe\r\nGPU 7: NVIDIA A800 80GB PCIe\r\n\r\nNvidia driver version: 535.161.08\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nCPU(s):                          112\r\nOn-line CPU(s) list:             0-111\r\nThread(s) per core:              2\r\nCore(s) per socket:              28\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           106\r\nModel name:                      Intel(R) Xeon(R) Gold 6330 CPU @ 2.00GHz\r\nStepping:                        6\r\nCPU MHz:                         2000.000\r\nBogoMIPS:                        4000.00\r\nVirtualization:                  VT-x\r\nL1d cache:                       2.6 MiB\r\nL1i cache:                       1.8 MiB\r\nL2 cache:                        70 MiB\r\nL3 cache:                        84 MiB\r\nNUMA node0 CPU(s):               0-27,56-83\r\nNUMA node1 CPU(s):               28-55,84-111\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts hwp_epp avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.47.1\r\n[pip3] triton==3.1.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.5\r\nvLLM Build Flags:\r\nCUDA Archs: 7.0 7.5 8.0 8.6 8.9 9.0+PTX; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\r\nGPU1    PIX      X      NODE    NODE    SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\r\nGPU2    NODE    NODE     X      PIX     SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\r\nGPU3    NODE    NODE    PIX      X      SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\r\nGPU4    SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    28-55,84-111    1               N/A\r\nGPU5    SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    28-55,84-111    1               N/A\r\nGPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      PIX     28-55,84-111    1               N/A\r\nGPU7    SYS     SYS     SYS     SYS     NODE    NODE    PIX      X      28-55,84-111    1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNVIDIA_VISIBLE_DEVICES=GPU-9999d3f7-23f2-4c6c-6add-f92af37c9703,GPU-961864f3-89d1-68f7-ace6-84840e45a394,GPU-e5a67348-a86c-d98c-c00f-babc3fef3aa3,GPU-25f8fcc1-73d7-2fa1-9f11-1f4f3efa5d52,GPU-b33ba59d-7e87-a3d8-d52c-2c407601ba34,GPU-5814a818-b70a-dcb7-cfb6-250ada0ca7c7,GPU-1b6e2b3b-f13c-89e5-efc4-ea71fc58e0d5,GPU-8ab2674a-f5dd-92f5-8365-56c210d08910\r\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\r\nTORCH_CUDA_ARCH_LIST=7.0 7.5 8.0 8.6 8.9 9.0+PTX\r\nNCCL_VERSION=2.21.5-1\r\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\r\nNVIDIA_PRODUCT_NAME=CUDA\r\nCUDA_VERSION=12.4.1\r\nNVIDIA_DISABLE_REQUIRE=true\r\nLD_LIBRARY_PATH=/usr/local/lib/python3.12/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n[err_execute_model_input_20250107-133808.zip](https://github.com/user-attachments/files/18328117/err_execute_model_input_20250107-133808.zip)\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\n(VllmWorkerProcess pid=8989) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236] ValueError: Error in model execution (input dumped to /tmp/err_execute_model_input_20250107-133808.pkl): Attempted to assign 1 x 2074 = 2074 multimodal tokens to 2040 placeholders\r\n(VllmWorkerProcess pid=8990) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=8990) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=8990) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner_base.py\", line 152, in _wrapper\r\n(VllmWorkerProcess pid=8990) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]     raise type(err)(\r\n(VllmWorkerProcess pid=8990) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236] ValueError: Error in model execution (input dumped to /tmp/err_execute_model_input_20250107-133808.pkl): Attempted to assign 1 x 2074 = 2074 multimodal tokens to 2040 placeholders\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop.\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236] Traceback (most recent call last):\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner_base.py\", line 116, in _wrapper\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1683, in execute_model\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]     hidden_or_intermediate_states = model_executable(\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]                                     ^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/pixtral.py\", line 289, in forward\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]     inputs_embeds = self.get_input_embeddings(input_ids,\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/pixtral.py\", line 263, in get_input_embeddings\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]     inputs_embeds = merge_multimodal_embeddings(\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 442, in merge_multimodal_embeddings\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]     return _merge_multimodal_embeddings(\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 364, in _merge_multimodal_embeddings\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]     raise ValueError(\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236] ValueError: Attempted to assign 1 x 2074 = 2074 multimodal tokens to 2040 placeholders\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236] \r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236] The above exception was the direct cause of the following exception:\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236] \r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236] Traceback (most recent call last):\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 230, in _run_worker_process\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]              ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 85, in start_worker_execution_loop\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]     output = self.execute_model(execute_model_req=None)\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 343, in execute_model\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]     output = self.model_runner.execute_model(\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner_base.py\", line 152, in _wrapper\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236]     raise type(err)(\r\n(VllmWorkerProcess pid=8992) ERROR 01-07 13:38:08 multiproc_worker_utils.py:236] ValueError: Error in model execution (input dumped to /tmp/err_execute_model_input_20250107-133808.pkl): Attempted to assign 1 x 2074 = 2074 multimodal tokens to 2040 placeholders\r\nERROR 01-07 13:38:11 multiproc_worker_utils.py:123] Worker VllmWorkerProcess pid 8986 died, exit code: -15\r\nINFO 01-07 13:38:11 multiproc_worker_utils.py:127] Killing local vLLM worker processes\r\nProcessed prompts:   0%|                                                                                       | 0/1 [00:11<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\r\n[rank0]:[W107 13:38:18.602010047 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\r\n/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n\r\n###code\r\n```text\r\nimport os.path\r\n\r\nfrom vllm import LLM\r\nfrom vllm.sampling_params import SamplingParams\r\nfrom huggingface_hub import hf_hub_download\r\nfrom datetime import datetime, timedelta\r\nimport os\r\nimport base64\r\nimport mimetypes\r\n\r\n\r\nmodel_name = \"mistralai/Pixtral-Large-Instruct-2411\"\r\nlocal_path = '/dfs/mistralai/Pixtral-Large-Instruct-2411/'\r\n\r\ndef load_system_prompt(repo_id: str, filename: str) -> str:\r\n    # file_path = hf_hub_download(repo_id=repo_id, filename=filename)\r\n    file_path = os.path.join(local_path, filename)\r\n    with open(file_path, 'r') as file:\r\n        system_prompt = file.read()\r\n    today = datetime.today().strftime('%Y-%m-%d')\r\n    yesterday = (datetime.today() - timedelta(days=1)).strftime('%Y-%m-%d')\r\n    model_name = repo_id.split(\"/\")[-1]\r\n    return system_prompt.format(name=model_name, today=today, yesterday=yesterday)\r\n\r\n\r\ndef file_to_data_url(file_path: str):\r\n    \"\"\"\r\n    Convert a local image file to a data URL.\r\n    \"\"\"\r\n    with open(file_path, \"rb\") as image_file:\r\n        encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\r\n\r\n    # _, extension = os.path.splitext(file_path)\r\n    # mime_type = f\"image/{extension[1:].lower()}\"\r\n    mime_type, _ = mimetypes.guess_type(file_path)\r\n\r\n    return f\"data:{mime_type};base64,{encoded_string}\"\r\n\r\nSYSTEM_PROMPT = load_system_prompt(local_path, \"SYSTEM_PROMPT.txt\")\r\n\r\nimage_url = 'https://img-blog.csdnimg.cn/21a0effc21e24d958c38e46820a23049.png'\r\nlocal_image_path = \"/dfs/data/mistral/ss_9.png\"\r\n\r\nmessages = [\r\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\r\n    {\r\n        \"role\": \"user\",\r\n        \"content\": [\r\n            {\r\n                \"type\": \"text\",\r\n                # \"text\": \"Which of the depicted countries has the best food? Which the second and third and fourth? Name the country, its color on the map and one its city that is visible on the map, but is not the capital. Make absolutely sure to only name a city that can be seen on the map.\",\r\n                \"text\": \"describe image\",\r\n            },\r\n            {\"type\": \"image_url\", \"image_url\": {\"url\": file_to_data_url(local_image_path)}},\r\n        ],\r\n    },\r\n]\r\n\r\nsampling_params = SamplingParams(max_tokens=10240)\r\n\r\nllm = LLM(model=local_path, config_format=\"mistral\", load_format=\"mistral\", tokenizer_mode=\"mistral\", tensor_parallel_size=8, limit_mm_per_prompt={\"image\": 4}, device='cuda', allowed_local_media_path='/dfs/data/mistral/', max_model_len=8192, enable_chunked_prefill=False, max_num_batched_tokens=10240)\r\n\r\noutputs = llm.chat(messages, sampling_params=sampling_params)\r\n\r\nprint(outputs[0].outputs[0].text)\r\n\r\n```\r\n\r\n### Before submitting a new issue...\r\n\r\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-07T05:49:21+00:00",
    "closed_at": "2025-07-11T02:16:55+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11792/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11792"
  },
  {
    "number": 12390,
    "title": "[Bug]: UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf8 in position 0: invalid start byte",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\nCollecting environment information...\nPyTorch version: 2.4.0+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 24.04 LTS (x86_64)\nGCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.39\n\nPython version: 3.9.20 (main, Oct  3 2024, 07:27:41)  [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.39\nIs CUDA available: True\nCUDA runtime version: 12.0.140\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 2060\nNvidia driver version: 556.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      48 bits physical, 48 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             8\nOn-line CPU(s) list:                0-7\nVendor ID:                          AuthenticAMD\nModel name:                         AMD Ryzen 7 3750H with Radeon Vega Mobile Gfx\nCPU family:                         23\nModel:                              24\nThread(s) per core:                 2\nCore(s) per socket:                 4\nSocket(s):                          1\nStepping:                           1\nBogoMIPS:                           4591.24\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr virt_ssbd arat npt nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload\nVirtualization:                     AMD-V\nHypervisor vendor:                  Microsoft\nVirtualization type:                full\nL1d cache:                          128 KiB (4 instances)\nL1i cache:                          256 KiB (4 instances)\nL2 cache:                           2 MiB (4 instances)\nL3 cache:                           4 MiB (1 instance)\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Mitigation; untrained return thunk; SMT vulnerable\nVulnerability Spec rstack overflow: Mitigation; safe RET\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.1.3.1\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.0.2.54\n[pip3] nvidia-curand-cu12==10.3.2.106\n[pip3] nvidia-cusolver-cu12==11.4.5.107\n[pip3] nvidia-cusparse-cu12==12.1.0.106\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.20.5\n[pip3] nvidia-nvjitlink-cu12==12.6.77\n[pip3] nvidia-nvtx-cu12==12.1.105\n[pip3] pyzmq==26.2.0\n[pip3] sentence-transformers==3.2.0\n[pip3] torch==2.4.0\n[pip3] torchvision==0.19.0\n[pip3] transformers==4.45.2\n[pip3] triton==3.0.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.6.77                  pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\n[conda] sentence-transformers     3.2.0                    pypi_0    pypi\n[conda] torch                     2.4.0                    pypi_0    pypi\n[conda] torchvision               0.19.0                   pypi_0    pypi\n[conda] transformers              4.45.2                   pypi_0    pypi\n[conda] triton                    3.0.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.6.1.dev238+ge2c6e0a82\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X                              N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nVLLM_USE_MODELSCOPE=True\nCUDA_MODULE_LOADING=LAZY\n\n</details>\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nI can start the server via the vllm serve command,\n\n```\nvllm serve /root/download/Qwen2.5-Coder-0.5B-Instruct \\\n    --host 127.0.0.1 \\\n    --port 6006 \\\n    --served-model-name Qwen2.5-Coder-0.5B-Instruct \\\n    --api-key xxx \\\n    --gpu-memory-utilization 0.95 \\\n    --max_model_len 4600 \\\n    --enforce-eager\n```\n\nerror log:\n```\nINFO 01-24 15:04:26 api_server.py:526] vLLM API server version 0.6.1.dev238+ge2c6e0a82\nINFO 01-24 15:04:26 api_server.py:527] args: Namespace(model_tag='/root/download/Qwen2.5-Coder-0.5B-Instruct', config='', host='127.0.0.1', port=6006, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='xxx', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, model='/root/download/Qwen2.5-Coder-0.5B-Instruct', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=4600, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=False, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['Qwen2.5-Coder-0.5B-Instruct'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7f4755136af0>)\nINFO 01-24 15:04:26 api_server.py:164] Multiprocessing frontend to use ipc:///tmp/91986834-0dc7-4bc7-a22d-9d8cdd784b60 for IPC Path.\nINFO 01-24 15:04:26 api_server.py:177] Started engine process with PID 9357\nWARNING 01-24 15:04:26 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\nWARNING 01-24 15:04:26 config.py:681] Possibly too large swap space. 4.00 GiB out of the 6.74 GiB total CPU memory is allocated for the swap space.\nWARNING 01-24 15:04:31 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\nWARNING 01-24 15:04:31 config.py:681] Possibly too large swap space. 4.00 GiB out of the 6.74 GiB total CPU memory is allocated for the swap space.\nINFO 01-24 15:04:31 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='/root/download/Qwen2.5-Coder-0.5B-Instruct', speculative_config=None, tokenizer='/root/download/Qwen2.5-Coder-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4600, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen2.5-Coder-0.5B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\nWARNING 01-24 15:04:32 utils.py:747] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\nINFO 01-24 15:04:32 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 01-24 15:04:32 selector.py:116] Using XFormers backend.\n/root/miniconda3/envs/qwen/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n/root/miniconda3/envs/qwen/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/vllm/engine/multiprocessing/engine.py\", line 388, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/vllm/engine/multiprocessing/engine.py\", line 138, in from_engine_args\n    return cls(\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/vllm/engine/multiprocessing/engine.py\", line 78, in __init__\n    self.engine = LLMEngine(*args,\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/vllm/engine/llm_engine.py\", line 325, in __init__\n    self.model_executor = executor_class(\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/vllm/executor/executor_base.py\", line 47, in __init__\n    self._init_executor()\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/vllm/executor/gpu_executor.py\", line 39, in _init_executor\n    self.driver_worker.init_device()\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/vllm/worker/worker.py\", line 168, in init_device\n    _check_if_gpu_supports_dtype(self.model_config.dtype)\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/vllm/worker/worker.py\", line 460, in _check_if_gpu_supports_dtype\n    gpu_name = current_platform.get_device_name()\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/vllm/platforms/cuda.py\", line 108, in get_device_name\n    return get_physical_device_name(physical_device_id)\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/vllm/platforms/cuda.py\", line 41, in wrapper\n    return fn(*args, **kwargs)\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/vllm/platforms/cuda.py\", line 59, in get_physical_device_name\n    return pynvml.nvmlDeviceGetName(handle)\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/pynvml.py\", line 2182, in wrapper\n    return res.decode()\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xf8 in position 0: invalid start byte\nTraceback (most recent call last):\n  File \"/root/miniconda3/envs/qwen/bin/vllm\", line 8, in <module>\n    sys.exit(main())\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/vllm/scripts.py\", line 165, in main\n    args.dispatch_function(args)\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/vllm/scripts.py\", line 37, in serve\n    uvloop.run(run_server(args))\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/uvloop/__init__.py\", line 82, in run\n    return loop.run_until_complete(wrapper())\n  File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/vllm/entrypoints/openai/api_server.py\", line 538, in run_server\n    async with build_async_engine_client(args) as engine_client:\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/contextlib.py\", line 181, in __aenter__\n    return await self.gen.__anext__()\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/vllm/entrypoints/openai/api_server.py\", line 105, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/contextlib.py\", line 181, in __aenter__\n    return await self.gen.__anext__()\n  File \"/root/miniconda3/envs/qwen/lib/python3.9/site-packages/vllm/entrypoints/openai/api_server.py\", line 192, in build_async_engine_client_from_engine_args\n    raise RuntimeError(\nRuntimeError: Engine process failed to start\n```\n\n\n\n\nbut ,  the same env,  load gptq model is OK:\n```\nvllm serve /root/download/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4 \\\n    --host 127.0.0.1 \\\n    --port 6006 \\\n    --quantization gptq \\\n    --served-model-name Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4 \\\n    --api-key xxx \\\n    --gpu-memory-utilization 0.95 \\\n    --max_model_len 4600 \\\n    --enforce-eager \n```\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-01-24T07:08:05+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12390/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12390"
  },
  {
    "number": 12124,
    "title": "[Bug]: Phi-3-small-8k cannot be served for vllm >= 0.6.5",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-6.8.0-1021-aws-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA A10G\nNvidia driver version: 550.127.05\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               4\nOn-line CPU(s) list:                  0-3\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 7R32\nCPU family:                           23\nModel:                                49\nThread(s) per core:                   2\nCore(s) per socket:                   2\nSocket(s):                            1\nStepping:                             0\nBogoMIPS:                             5599.99\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save rdpid\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            64 KiB (2 instances)\nL1i cache:                            64 KiB (2 instances)\nL2 cache:                             1 MiB (2 instances)\nL3 cache:                             8 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-3\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.0\n[pip3] triton==3.1.0\n[conda] No relevant packages\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.6.6.post1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-3     0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n```\n\n</details>\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nRunning the following code encounters an exception during model build: \n`Error in model execution (input dumped to ...): too many values to unpack (expected 2)`\n\n```console\npython -m vllm.entrypoints.openai.api_server \\\n    --tensor-parallel-size 1 \\\n    --host 0.0.0.0 --port 8080 \\\n    --model microsoft/Phi-3-small-8k-instruct \\\n    --gpu-memory-utilization 0.90 \\\n    --enforce-eager \\\n    --disable-log-requests \\\n    --disable-custom-all-reduce \\\n    --trust-remote-code\n```\n\nTraceback:\n```\nERROR 01-16 16:42:00 engine.py:366] Traceback (most recent call last):\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/vllm/worker/model_runner_base.py\", line 116, in _wrapper\nERROR 01-16 16:42:00 engine.py:366]     return func(*args, **kwargs)\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1691, in execute_model\nERROR 01-16 16:42:00 engine.py:366]     hidden_or_intermediate_states = model_executable(\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\nERROR 01-16 16:42:00 engine.py:366]     return self._call_impl(*args, **kwargs)\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\nERROR 01-16 16:42:00 engine.py:366]     return forward_call(*args, **kwargs)\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/vllm/model_executor/models/phi3_small.py\", line 443, in forward\nERROR 01-16 16:42:00 engine.py:366]     output_hidden_states = self.model(\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\nERROR 01-16 16:42:00 engine.py:366]     return self._call_impl(*args, **kwargs)\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\nERROR 01-16 16:42:00 engine.py:366]     return forward_call(*args, **kwargs)\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/vllm/model_executor/models/phi3_small.py\", line 356, in forward\nERROR 01-16 16:42:00 engine.py:366]     hidden_states = layer(\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\nERROR 01-16 16:42:00 engine.py:366]     return self._call_impl(*args, **kwargs)\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\nERROR 01-16 16:42:00 engine.py:366]     return forward_call(*args, **kwargs)\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/vllm/model_executor/models/phi3_small.py\", line 288, in forward\nERROR 01-16 16:42:00 engine.py:366]     hidden_states = self.self_attn(\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\nERROR 01-16 16:42:00 engine.py:366]     return self._call_impl(*args, **kwargs)\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\nERROR 01-16 16:42:00 engine.py:366]     return forward_call(*args, **kwargs)\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/vllm/model_executor/models/phi3_small.py\", line 248, in forward\nERROR 01-16 16:42:00 engine.py:366]     attn_output = self.attn(q, k, v, kv_cache, attn_metadata=attn_metadata)\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\nERROR 01-16 16:42:00 engine.py:366]     return self._call_impl(*args, **kwargs)\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\nERROR 01-16 16:42:00 engine.py:366]     return forward_call(*args, **kwargs)\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/vllm/attention/layer.py\", line 154, in forward\nERROR 01-16 16:42:00 engine.py:366]     torch.ops.vllm.unified_attention_with_output(\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/torch/_ops.py\", line 1116, in __call__\nERROR 01-16 16:42:00 engine.py:366]     return self._op(*args, **(kwargs or {}))\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/vllm/attention/layer.py\", line 288, in unified_attention_with_output\nERROR 01-16 16:42:00 engine.py:366]     self.impl.forward(query,\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/vllm/attention/backends/blocksparse_attn.py\", line 384, in forward\nERROR 01-16 16:42:00 engine.py:366]     num_tokens, hidden_size = query.shape\nERROR 01-16 16:42:00 engine.py:366] ValueError: too many values to unpack (expected 2)\nERROR 01-16 16:42:00 engine.py:366] \nERROR 01-16 16:42:00 engine.py:366] The above exception was the direct cause of the following exception:\nERROR 01-16 16:42:00 engine.py:366] \nERROR 01-16 16:42:00 engine.py:366] Traceback (most recent call last):\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 357, in run_mp_engine\nERROR 01-16 16:42:00 engine.py:366]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 119, in from_engine_args\nERROR 01-16 16:42:00 engine.py:366]     return cls(ipc_path=ipc_path,\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 71, in __init__\nERROR 01-16 16:42:00 engine.py:366]     self.engine = LLMEngine(*args, **kwargs)\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 276, in __init__\nERROR 01-16 16:42:00 engine.py:366]     self._initialize_kv_caches()\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 416, in _initialize_kv_caches\nERROR 01-16 16:42:00 engine.py:366]     self.model_executor.determine_num_available_blocks())\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py\", line 68, in determine_num_available_blocks\nERROR 01-16 16:42:00 engine.py:366]     return self.driver_worker.determine_num_available_blocks()\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 01-16 16:42:00 engine.py:366]     return func(*args, **kwargs)\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/vllm/worker/worker.py\", line 202, in determine_num_available_blocks\nERROR 01-16 16:42:00 engine.py:366]     self.model_runner.profile_run()\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 01-16 16:42:00 engine.py:366]     return func(*args, **kwargs)\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1331, in profile_run\nERROR 01-16 16:42:00 engine.py:366]     self.execute_model(model_input, kv_caches, intermediate_tensors)\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 01-16 16:42:00 engine.py:366]     return func(*args, **kwargs)\nERROR 01-16 16:42:00 engine.py:366]   File \".../.venv/lib/python3.10/site-packages/vllm/worker/model_runner_base.py\", line 152, in _wrapper\nERROR 01-16 16:42:00 engine.py:366]     raise type(err)(\nERROR 01-16 16:42:00 engine.py:366] ValueError: Error in model execution (input dumped to ...): too many values to unpack (expected 2)\n```\n\n`vllm==0.6.4.post1 ` does not throw an exception.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-01-16T16:50:23+00:00",
    "closed_at": "2025-02-06T18:01:45+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12124/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/12124"
  },
  {
    "number": 12379,
    "title": "[Usage]: How to run vllm with regression task, just like classify task",
    "body": "### Your current environment\n\nI have trained qwen2.5 with regression task,  below is inference code : \n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"qwen3_score_0123\", trust_remote_code=True)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"qwen3_score_0123\",\n    num_labels=1,\n    problem_type=\"regression\",\n    trust_remote_code=True\n).to(torch.bfloat16)\n\ndevice = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\ntext = \"\"\"\nhello\n\"\"\"\n\ninputs = tokenizer(\n    text,\n    truncation=True,\n    padding='max_length',\n    max_length=512,\n    return_tensors=\"pt\"\n)\n\ninputs = {key: value.to(device) for key, value in inputs.items()}\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\npredicted_score = outputs.logits.item() \n\n\n\n### How would you like to use vllm\n\nI want to inference the qwen llm regression model with vllm, need help ,thanks\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-24T02:19:51+00:00",
    "closed_at": "2025-05-25T02:29:11+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12379/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12379"
  },
  {
    "number": 11863,
    "title": "[Bug]: MQLLMEgine Error on Apple Silicon M4 Pro",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nI receive the following error when calling VLLM which is stop on MacOS on my M4 Pro Mac mini with 64GB RAM. The same error occurs on my M1 Ultra Mac Studio. I have followed the installing steps for macOS correctly, including reinstalling Xcode command line tools. The errors happens instantly when calling the server, and occurs with any model I have tried. \r\n\r\nINFO 01-08 18:58:57 logger-py:37] Received request cmpl-e4776fdb878c4500912374ad23cd2785-0: prompt:\r\n'User: Why is the sky blue?\\nAssistant:',\r\nparams: SamplingParams (n=1,\r\npresence_penalty=0.0,\r\nfrequency_penalty=0.0,\r\nrepetition_penalty=1.0,\r\n=-1-\r\nmin_p=0.0, seed=None,\r\n=None,\r\nstop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False,\r\nmax tokens=100,\r\nguided_decoding=None),\r\n374,\r\n279,\r\nignore\r\neos=False,\r\nmin_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True,'\r\ntemperature=1.0, top_p=1.0, top_k\r\nspaces_between_special_tokens=True,\r\ntruncate_prompt_token\r\nprompt_token_ids: [128000,\r\n1502, 25, 8595,\r\n13180, 6437,\r\n5380, 72803, 25J,\r\nlora_request: None, prompt_adapter_request: None.\r\nINFO 01-08 18:58:57 engine.py:267] Added request cmpl-e4776fdb878c4500912374ad23cd2785-0.\r\nCRITICAL 01-08 18:58:57 launcher.py:99] MQLLMEngine is already dead,\r\nterminating server process\r\nINru.\r\n10.2.4.193:54818 - \"POST /v1/completions HTTP/1.1\" 500 Internal Server Error\r\nERROR 01-08 18:58:57 engine-py:135] AttributeError(\"'SiluAndMul'\r\nobject has no attribute 'op'\")\r\nERROR\r\n01-08 18:58:57 engine.pv:135 Traceback (most recent call last:\r\n01-08 18:58:57 engine-py:135]\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v1lm/engine/multiprocessing/engine.py\", line 133, in start\r\nERHUHI\r\n01-08 18:58:57 engine-py:135]\r\nself.run engine loope\r\nERROR\r\n01-08 18:58:57\r\nengine-py:135]\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v1lm/engine/multiprocessing/engine.py\", line 196, in run_engine_loop\r\nengine. py:135]\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v1lm/engine/multiprocessing/engine.py\", line 214, in engine_step\r\nERROR\r\nERRORI\r\nERRORI\r\nengine.py:135]\r\n18:58:67 engine.py:135J\r\n01-08 18:58:57\r\nengine. py:135]\r\n18:58:57 engine.py:135J\r\n01-08 18:58:57\r\nengine.py:135]\r\n18:58:57 engine-py:135]\r\nengine-py:135]\r\nERROR 01-08 18:58:57 engine-py:135]\r\nERROR\r\n01-08 18:58:57 engine.py:135]\r\nERROR 61-68 18:58:57 engine.py:135J\r\nERROR\r\n01-08 18:58:57\r\nengine. py:135]\r\nERROR 01-08 18:58:57 engine-py:135]\r\nERROR\r\n01-08 18:58:57\r\nengine.py:135]\r\nERROR 01-08 18:58:57 engine.py:135]\r\nERROR\r\n01-08 18:58:57\r\nengine.py:135]\r\nERROR 01-08 18:58:57\r\nERROR\r\nengine.py:135]\r\n01-08 18:58:57\r\nengine.py:135]\r\nERROR 01-08 18:58:57\r\nengine.py:135]\r\nERROR 01-08 18:58:57\r\nERROR\r\nengine.py:135]\r\n01-08 18:58:57\r\nengine.py:135]\r\n01-00 10100101\r\nERROR\r\n01-08 18:58:57\r\nengine.py:135]\r\nEAKUN\r\n0=0010:00402\r\nERROR\r\n0-0610:00402\r\nengine.py:135]\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v1lm/engine/multiprocessing/engine-py\", line 205, in engine_step\r\nrerurn selt.enczne.sceo\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v1lm/engine/1lm_engine-py\", line 1394, in step\r\noutputs = self.model_executor.execute_modell\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v1lm/executor/cpu_executor.py\", line 201, in execute_model\r\noutput = self.driver_method_invoker(self.driver_worker,\r\nFile \"/Users/ai_dev_mac_mini/Documents/vllm/v1lm/executor/cpu_executor.py\", line 298, in _driver_method_invoker return getattr (driver, method) (*args,\r\n**kwargs)\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v11m/worker/worker_base-py\", line 344, in execute_model\r\noutput = self.model_runner.execute_model(\r\nFile \"/Users/ai_dev_mac_mini/Documents/v11m/v1lm/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context return func (*args, **kwargs)\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v11m/worker/cpu_model_runner-py\", line 530, in execute_model\r\nhidden_states = model_executable(\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v11m/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl return self._call_impl(*args,\r\n**kwargs)\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v1lm/lib/python3.9/site-packages/torch/nn/modules/module-py\", line 1747, in _call_impl return forward_call*args,\r\n**kwargs)\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v11m/model_executor/models/1lama.py\", line 569, in forward\r\nmodel_output = self.model(input_ids,\r\npositions,\r\nkv_caches,\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v1lm/compilation/decorators.py\", line 170, in __call.\r\nreturn self. forward(*args,\r\n**kwargs)\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v1lm/model_executor/models/1lama-py\", line 360, in forward\r\nhidden_states, residual = layer(positions,\r\nhidden_states,\r\nFile \"/Users/ai_dev_mac _mini/Documents/vllm/v11m/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nERROR\r\nengine. py:135]\r\nreturn self._call_impl(*args, **kwargs)\r\nERHOR\r\nFile \"/Users/ai_dev_mac_mini/Documents/v11m/vllm/lib/python3.9/site-packages/torch/nn/modules/module-py\", line 1747, in _call_ impl\r\nERROR\r\n01-08 18:58:57\r\nengine. py:135]\r\nreturn forward_call(*args,\r\n**kwargs)\r\nOH\r\n10.b0.r engthe.py:1sbJ\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v11m/model_executor/models/1lama.py\", line 283, in forward\r\n01-08 18:58:57\r\nengine. py:135]\r\nhidden_states = self.mlp(hidden_states)\r\n18:60:0r engine.py:13bJ\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v1lm/lib/python3.9/site-packages/torch/nn/modules/module-py\", line 1736, in _wrapped_call_impl\r\n01-08 18:58:57\r\nengine.py:135]\r\nreturn self._call_impl(*args, **kwargs)\r\nERRORI\r\n61-68 18:58:5/ engine. py: 155J\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v1lm/lib/python3.9/site-packages/torch/nn/modules/module-py\", line 1747, in _call_impl\r\nengine. py:135]\r\nreturn forward_call*args,\r\n**kwargs)\r\nERRORI\r\n61-68 18:58:57 engine. py: 155J\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v1lm/model_executor/models/1lama.py\", line 93, in forward\r\n5-48484684577\r\nengine. py:135]\r\nx = self.act_fn(x)\r\nERRORI\r\n01-08 18:58:57 engine-py:135]\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/vllm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nERROR\r\nengine. py:135]\r\nreturn self._call_impl(*args, **kwargs)\r\nERROR\r\n01-08 18:58:57 engine-py:135]\r\nFile \"/Users/ai_dev_mac_mini/Documents/vllm/v1lm/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\nERROR\r\n61-68 18:58:57\r\nengine. py:135]\r\nreturn forward_call(*args,\r\nERROR\r\n01-08 18:58:57 engine-py:135]\r\n**kwargs)\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v1lm/model_executor/custom_op.py\", line 24, in forward\r\nERROR\r\n01-08 18:58:57\r\nengine.py:135]\r\nreturn self._forward_method(*args, **kwargs)\r\nERROR 01-08 18:58:57\r\nERROR\r\nengine.py:135]\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v1lm/model_executor/custom_op.py\", line 48, in forward_cpu\r\n01-08 18:58:57\r\nengine.py:135]\r\nreturn self.forward_cuda (*args,\r\n**kwargs)\r\nERROR 01-08 18:58:57\r\nengine.py:135]\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v1lm/model_executor/layers/activation.py\", line 79, in forward_cuda\r\nERROR 01-08 18:58:57\r\nengine.py:135]\r\nERROR 01-08 18:58:57\r\nself.op(out,\r\nx)\r\nengine.py:1351\r\nFile \"/Users/ai_dev_mac_mini/Documents/v1lm/v1lm/lib/python3.9/site-packages/torch/nn/modules/module-py\", line 1931, in __getattr\r\nERROR 01-08 18:58:57 engine-py:135J\r\nLUISE ALCLIOUCCCLLOL\r\nERROR\r\n01-08 18:58:57\r\nengine.py:135] AttributeError: 'SiluAndMul' object has no attribute 'op'\r\nINru.\r\nshutting down\r\nINFO:\r\nWaiting for application shutdown.\r\nnooncamon snuccown comorere.\r\nINFO:\r\nFinished server\r\nprocess\r\n\r\nI use the following python code to call the API:\r\n\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(\r\n    base_url=\"http://10.2.5.153:8000/v1\",\r\n    api_key=\"token-abc123\",\r\n)\r\n\r\n# Convert chat input into a plain prompt for completion\r\nprompt = \"User: Why is the sky blue?\\nAssistant:\"\r\n\r\ncompletion = client.completions.create(\r\n    model=\"NousResearch/Llama-3.2-1B\",\r\n    prompt=prompt,\r\n    max_tokens=100,\r\n)\r\n\r\nprint(completion.choices[0].text.strip())\r\n\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-01-08T19:14:23+00:00",
    "closed_at": "2025-01-08T22:16:31+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11863/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11863"
  },
  {
    "number": 12189,
    "title": "[Bug]: CUDA initialization error with vLLM 0.5.4 and PyTorch 2.4.0+cu121",
    "body": "### Your current environment\n```\nPyTorch version: 2.4.0+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.2 LTS (x86_64)\nGCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\nClang version: Could not collect\nCMake version: version 3.31.4\nLibc version: glibc-2.35\n\nPython version: 3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.10.134-008.7.kangaroo.al8.x86_64-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA A100-SXM4-80GB\nGPU 1: NVIDIA A100-SXM4-80GB\nGPU 2: NVIDIA A100-SXM4-80GB\nGPU 3: NVIDIA A100-SXM4-80GB\nGPU 4: NVIDIA A100-SXM4-80GB\nGPU 5: NVIDIA A100-SXM4-80GB\nGPU 6: NVIDIA A100-SXM4-80GB\nGPU 7: NVIDIA A100-SXM4-80GB\n\nNvidia driver version: 535.54.03\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   46 bits physical, 57 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          64\nOn-line CPU(s) list:             0-63\nVendor ID:                       GenuineIntel\nModel name:                      Intel(R) Xeon(R) Processor @ 2.90GHz\nCPU family:                      6\nModel:                           106\nThread(s) per core:              1\nCore(s) per socket:              64\nSocket(s):                       1\nStepping:                        6\nBogoMIPS:                        5800.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd avx512vbmi umip pku avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm md_clear arch_capabilities\nHypervisor vendor:               KVM\nVirtualization type:             full\nL1d cache:                       3 MiB (64 instances)\nL1i cache:                       2 MiB (64 instances)\nL2 cache:                        80 MiB (64 instances)\nL3 cache:                        48 MiB (1 instance)\nNUMA node(s):                    1\nNUMA node0 CPU(s):               0-63\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Vulnerable\nVulnerability Retbleed:          Not affected\nVulnerability Spec store bypass: Vulnerable\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Vulnerable\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.1.3.1\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.0.2.54\n[pip3] nvidia-curand-cu12==10.3.2.106\n[pip3] nvidia-cusolver-cu12==11.4.5.107\n[pip3] nvidia-cusparse-cu12==12.1.0.106\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.20.5\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.1.105\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.4.0+cu121\n[pip3] torchvision==0.19.0\n[pip3] transformers==4.48.0\n[pip3] triton==3.0.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.6.85                  pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\n[conda] torch                     2.4.0+cu121              pypi_0    pypi\n[conda] torchvision               0.19.0                   pypi_0    pypi\n[conda] transformers              4.48.0                   pypi_0    pypi\n[conda] triton                    3.0.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.5.4\nvLLM Build Flags:\nCUDA Archs: 5.2 6.0 6.1 7.0 7.5 8.0 8.6 9.0+PTX; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV12    NV12    NV12    NV12    NV12    NV12    NV12    PHB     PHB     PHB     PHB     0-63            N/A             N/A\nGPU1    NV12     X      NV12    NV12    NV12    NV12    NV12    NV12    PHB     PHB     PHB     PHB     0-63            N/A             N/A\nGPU2    NV12    NV12     X      NV12    NV12    NV12    NV12    NV12    PHB     PHB     PHB     PHB     0-63            N/A             N/A\nGPU3    NV12    NV12    NV12     X      NV12    NV12    NV12    NV12    PHB     PHB     PHB     PHB     0-63            N/A             N/A\nGPU4    NV12    NV12    NV12    NV12     X      NV12    NV12    NV12    PHB     PHB     PHB     PHB     0-63            N/A             N/A\nGPU5    NV12    NV12    NV12    NV12    NV12     X      NV12    NV12    PHB     PHB     PHB     PHB     0-63            N/A             N/A\nGPU6    NV12    NV12    NV12    NV12    NV12    NV12     X      NV12    PHB     PHB     PHB     PHB     0-63            N/A             N/A\nGPU7    NV12    NV12    NV12    NV12    NV12    NV12    NV12     X      PHB     PHB     PHB     PHB     0-63            N/A             N/A\nNIC0    PHB     PHB     PHB     PHB     PHB     PHB     PHB     PHB      X      PHB     PHB     PHB\nNIC1    PHB     PHB     PHB     PHB     PHB     PHB     PHB     PHB     PHB      X      PHB     PHB\nNIC2    PHB     PHB     PHB     PHB     PHB     PHB     PHB     PHB     PHB     PHB      X      PHB\nNIC3    PHB     PHB     PHB     PHB     PHB     PHB     PHB     PHB     PHB     PHB     PHB      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n\nNVIDIA_VISIBLE_DEVICES=all\nNCCL_IB_TC=136\nCUBLAS_VERSION=12.1.3.1\nNVIDIA_REQUIRE_CUDA=cuda>=9.0\nNCCL_MIN_NCHANNELS=4\nCUDA_CACHE_DISABLE=1\nNCCL_NET_PLUGIN=none\nTORCH_CUDA_ARCH_LIST=5.2 6.0 6.1 7.0 7.5 8.0 8.6 9.0+PTX\nNCCL_VERSION=2.18.1\nNCCL_SOCKET_IFNAME=eth\nNVIDIA_DRIVER_CAPABILITIES=compute,utility,video\nNCCL_DEBUG=INFO\nNCCL_IB_HCA=mlx5\nNVIDIA_PRODUCT_NAME=PyTorch\nNCCL_IB_GID_INDEX=3\nCUDA_VERSION=12.1.1.009\nPYTORCH_VERSION=2.0.0\nPYTORCH_BUILD_NUMBER=0\nNCCL_IB_QPS_PER_CONNECTION=8\nNCCL_IB_TIMEOUT=22\nCUDNN_VERSION=8.9.1.23\nNCCL_IB_SL=5\nPYTORCH_HOME=/opt/pytorch/pytorch\nLD_LIBRARY_PATH=/usr/lib64/openmpi/lib:/usr/local/lib/python3.10/dist-packages/torch/lib:/usr/local/lib/python3.10/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNVIDIA_BUILD_ID=60708168\nCUDA_DRIVER_VERSION=530.30.02\nPYTORCH_BUILD_VERSION=2.0.0\nCUDA_HOME=/usr/local/cuda\nCUDA_HOME=/usr/local/cuda\nCUDA_MODULE_LOADING=LAZY\nNVIDIA_REQUIRE_JETPACK_HOST_MOUNTS=\nNVIDIA_PYTORCH_VERSION=23.05\nTORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1\n\n```\n\n### \ud83d\udc1b Describe the bug\n\n\n\nCUDA initialization error in forked subprocesses when using **vLLM 0.5.4 with PyTorch 2.4.0+cu121**. The same code works with vLLM 0.5.0 and PyTorch 2.3.0+cu121, but fails with newer versions (vLLM 0.6.2 with PyTorch 2.5.1+cu121).\n\n**Error Message:**\n```\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n```\n\n**Environment:**\n- vLLM: 0.5.4 (Also tested with 0.6.2)\n- PyTorch: 2.4.0+cu121 (Also tested with 2.5.1+cu121)\n- CUDA: 12.2\n- GPU Driver: 535.54.03\n- OS: [Add your operating system here]\n\n**Steps to Reproduce:**\n1. Install vLLM 0.5.4 and PyTorch 2.4.0+cu121\n2. Set the following environment variables:\n   ```bash\n   export PYTHONMULTIPROCESSING_START_METHOD=spawn\n   export CUDA_VISIBLE_DEVICES=\"0,1,2,3,4,5,6,7\"  \n   export VLLM_USE_SPAWN=1\n   export PYTHONPATH=\"${PYTHONPATH}:/mnt/data/taoshuchang.tsc/IR_RAG/IRM\"\n   export MASTER_PORT=29500\n   export MASTER_ADDR=localhost\n   export WORLD_SIZE=$(echo $CUDA_VISIBLE_DEVICES | tr ',' '\\n' | wc -l)\n   export CUDA_DEVICE_ORDER=PCI_BUS_ID\n   ```\n3. Run a script that uses vLLM to load a large language model (e.g., LLaMA 3.3-70B)\n4. Observe the CUDA initialization error in forked subprocesses\n\n**Additional Context:**\n- This issue doesn't occur with vLLM 0.5.0 and PyTorch 2.3.0+cu121\n- vLLM 0.5.4 is required to load LLaMA 3.3, which necessitates PyTorch 2.4.0+cu121\n\n**Questions:**\n1. Is this a known issue with PyTorch 2.4.0+cu121 and newer versions?\n2. Are there any workarounds or configurations to resolve this issue?\n3. Is there a compatibility matrix for vLLM, PyTorch, and CUDA versions?\n\n**Attempted Solutions:**\n- Tried vLLM 0.6.2 with PyTorch 2.5.1+cu121, but encountered the same error\n- Set `PYTHONMULTIPROCESSING_START_METHOD=spawn` as suggested in error message\n\n**Full error trace:**\n```\nINFO 01-19 03:55:21 config.py:899] Defaulting to use mp for distributed inference\nINFO 01-19 03:55:21 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='/mnt/data/taoshuchang.tsc/IR_RAG/ckpt/hotpot_contriever/analyze_merge//hotpot_1doc_other_Meta-Llama-3-70B-Instruct_lr1e5', speculative_config=None, tokenizer='/mnt/data/taoshuchang.tsc/IR_RAG/ckpt/hotpot_contriever/analyze_merge//hotpot_1doc_other_Meta-Llama-3-70B-Instruct_lr1e5', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/mnt/data/taoshuchang.tsc/IR_RAG/ckpt/hotpot_contriever/analyze_merge//hotpot_1doc_other_Meta-Llama-3-70B-Instruct_lr1e5, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\nWARNING 01-19 03:55:22 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 01-19 03:55:22 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n\u001b[1;36m(VllmWorkerProcess pid=305)\u001b[0;0m INFO 01-19 03:55:24 multiproc_worker_utils.py:218] Worker ready; awaiting tasks\n\u001b[1;36m(VllmWorkerProcess pid=305)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233] Exception in worker VllmWorkerProcess while processing method init_device: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method, Traceback (most recent call last):\n\u001b[1;36m(VllmWorkerProcess pid=305)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 226, in _run_worker_process\n\u001b[1;36m(VllmWorkerProcess pid=305)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     output = executor(*args, **kwargs)\n\u001b[1;36m(VllmWorkerProcess pid=305)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(VllmWorkerProcess pid=305)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/worker/worker.py\", line 166, in init_device\n\u001b[1;36m(VllmWorkerProcess pid=305)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     torch.cuda.set_device(self.device)\n\u001b[1;36m(VllmWorkerProcess pid=305)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n\u001b[1;36m(VllmWorkerProcess pid=305)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     torch._C._cuda_setDevice(device)\n\u001b[1;36m(VllmWorkerProcess pid=305)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n\u001b[1;36m(VllmWorkerProcess pid=305)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     raise RuntimeError(\n\u001b[1;36m(VllmWorkerProcess pid=305)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n\u001b[1;36m(VllmWorkerProcess pid=305)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233] \n\u001b[1;36m(VllmWorkerProcess pid=307)\u001b[0;0m INFO 01-19 03:55:24 multiproc_worker_utils.py:218] Worker ready; awaiting tasks\n\u001b[1;36m(VllmWorkerProcess pid=308)\u001b[0;0m INFO 01-19 03:55:24 multiproc_worker_utils.py:218] Worker ready; awaiting tasks\n\u001b[1;36m(VllmWorkerProcess pid=306)\u001b[0;0m INFO 01-19 03:55:24 multiproc_worker_utils.py:218] Worker ready; awaiting tasks\n\u001b[1;36m(VllmWorkerProcess pid=303)\u001b[0;0m INFO 01-19 03:55:24 multiproc_worker_utils.py:218] Worker ready; awaiting tasks\n\u001b[1;36m(VllmWorkerProcess pid=304)\u001b[0;0m INFO 01-19 03:55:24 multiproc_worker_utils.py:218] Worker ready; awaiting tasks\n\u001b[1;36m(VllmWorkerProcess pid=307)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233] Exception in worker VllmWorkerProcess while processing method init_device: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method, Traceback (most recent call last):\n\u001b[1;36m(VllmWorkerProcess pid=307)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 226, in _run_worker_process\n\u001b[1;36m(VllmWorkerProcess pid=307)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     output = executor(*args, **kwargs)\n\u001b[1;36m(VllmWorkerProcess pid=307)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(VllmWorkerProcess pid=307)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/worker/worker.py\", line 166, in init_device\n\u001b[1;36m(VllmWorkerProcess pid=307)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     torch.cuda.set_device(self.device)\n\u001b[1;36m(VllmWorkerProcess pid=307)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n\u001b[1;36m(VllmWorkerProcess pid=307)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     torch._C._cuda_setDevice(device)\n\u001b[1;36m(VllmWorkerProcess pid=307)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n\u001b[1;36m(VllmWorkerProcess pid=307)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     raise RuntimeError(\n\u001b[1;36m(VllmWorkerProcess pid=307)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n\u001b[1;36m(VllmWorkerProcess pid=307)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233] \n\u001b[1;36m(VllmWorkerProcess pid=308)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233] Exception in worker VllmWorkerProcess while processing method init_device: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method, Traceback (most recent call last):\n\u001b[1;36m(VllmWorkerProcess pid=308)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 226, in _run_worker_process\n\u001b[1;36m(VllmWorkerProcess pid=308)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     output = executor(*args, **kwargs)\n\u001b[1;36m(VllmWorkerProcess pid=308)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(VllmWorkerProcess pid=308)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/worker/worker.py\", line 166, in init_device\n\u001b[1;36m(VllmWorkerProcess pid=308)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     torch.cuda.set_device(self.device)\n\u001b[1;36m(VllmWorkerProcess pid=308)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n\u001b[1;36m(VllmWorkerProcess pid=308)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     torch._C._cuda_setDevice(device)\n\u001b[1;36m(VllmWorkerProcess pid=308)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n\u001b[1;36m(VllmWorkerProcess pid=308)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     raise RuntimeError(\n\u001b[1;36m(VllmWorkerProcess pid=308)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n\u001b[1;36m(VllmWorkerProcess pid=308)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233] \n\u001b[1;36m(VllmWorkerProcess pid=303)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=306)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233] Exception in worker VllmWorkerProcess while processing method init_device: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method, Traceback (most recent call last):\nERROR 01-19 03:55:24 multiproc_worker_utils.py:233] Exception in worker VllmWorkerProcess while processing method init_device: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method, Traceback (most recent call last):\n\u001b[1;36m(VllmWorkerProcess pid=303)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=306)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 226, in _run_worker_process\nERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 226, in _run_worker_process\n\u001b[1;36m(VllmWorkerProcess pid=303)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=306)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     output = executor(*args, **kwargs)\nERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     output = executor(*args, **kwargs)\n\u001b[1;36m(VllmWorkerProcess pid=303)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=306)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]              ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-19 03:55:24 multiproc_worker_utils.py:233]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(VllmWorkerProcess pid=303)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=306)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/worker/worker.py\", line 166, in init_device\nERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/worker/worker.py\", line 166, in init_device\n\u001b[1;36m(VllmWorkerProcess pid=303)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=306)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     torch.cuda.set_device(self.device)\nERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     torch.cuda.set_device(self.device)\n\u001b[1;36m(VllmWorkerProcess pid=303)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n\u001b[1;36m(VllmWorkerProcess pid=306)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=303)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n\u001b[1;36m(VllmWorkerProcess pid=306)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     torch._C._cuda_setDevice(device)\nERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     torch._C._cuda_setDevice(device)\n\u001b[1;36m(VllmWorkerProcess pid=303)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=306)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\nERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n\u001b[1;36m(VllmWorkerProcess pid=303)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=306)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     raise RuntimeError(\nERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     raise RuntimeError(\n\u001b[1;36m(VllmWorkerProcess pid=306)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=303)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\nERROR 01-19 03:55:24 multiproc_worker_utils.py:233] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n\u001b[1;36m(VllmWorkerProcess pid=306)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=303)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233] \nERROR 01-19 03:55:24 multiproc_worker_utils.py:233] \n\u001b[1;36m(VllmWorkerProcess pid=302)\u001b[0;0m INFO 01-19 03:55:24 multiproc_worker_utils.py:218] Worker ready; awaiting tasks\n\u001b[1;36m(VllmWorkerProcess pid=304)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233] Exception in worker VllmWorkerProcess while processing method init_device: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method, Traceback (most recent call last):\n\u001b[1;36m(VllmWorkerProcess pid=304)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 226, in _run_worker_process\n\u001b[1;36m(VllmWorkerProcess pid=304)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     output = executor(*args, **kwargs)\n\u001b[1;36m(VllmWorkerProcess pid=304)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(VllmWorkerProcess pid=304)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/worker/worker.py\", line 166, in init_device\n\u001b[1;36m(VllmWorkerProcess pid=304)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     torch.cuda.set_device(self.device)\n\u001b[1;36m(VllmWorkerProcess pid=304)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n\u001b[1;36m(VllmWorkerProcess pid=304)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     torch._C._cuda_setDevice(device)\n\u001b[1;36m(VllmWorkerProcess pid=304)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n\u001b[1;36m(VllmWorkerProcess pid=304)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     raise RuntimeError(\n\u001b[1;36m(VllmWorkerProcess pid=304)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n\u001b[1;36m(VllmWorkerProcess pid=304)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233] \n\u001b[1;36m(VllmWorkerProcess pid=302)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233] Exception in worker VllmWorkerProcess while processing method init_device: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method, Traceback (most recent call last):\n\u001b[1;36m(VllmWorkerProcess pid=302)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 226, in _run_worker_process\n\u001b[1;36m(VllmWorkerProcess pid=302)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     output = executor(*args, **kwargs)\n\u001b[1;36m(VllmWorkerProcess pid=302)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[1;36m(VllmWorkerProcess pid=302)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/worker/worker.py\", line 166, in init_device\n\u001b[1;36m(VllmWorkerProcess pid=302)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     torch.cuda.set_device(self.device)\n\u001b[1;36m(VllmWorkerProcess pid=302)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n\u001b[1;36m(VllmWorkerProcess pid=302)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     torch._C._cuda_setDevice(device)\n\u001b[1;36m(VllmWorkerProcess pid=302)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]   File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n\u001b[1;36m(VllmWorkerProcess pid=302)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233]     raise RuntimeError(\n\u001b[1;36m(VllmWorkerProcess pid=302)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n\u001b[1;36m(VllmWorkerProcess pid=302)\u001b[0;0m ERROR 01-19 03:55:24 multiproc_worker_utils.py:233] \nERROR 01-19 04:05:25 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 305 died, exit code: -15\nINFO 01-19 04:05:25 multiproc_worker_utils.py:124] Killing local vLLM worker processes\n\n/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/_distutils_hack/__init__.py:54: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/mnt/data/taoshuchang.tsc/IR_RAG/IRM/inference_new.py\", line 299, in <module>\n    main()\n  File \"/mnt/data/taoshuchang.tsc/IR_RAG/IRM/inference_new.py\", line 291, in main\n    evaluate_retrieval(\n  File \"/mnt/data/taoshuchang.tsc/IR_RAG/IRM/inference_new.py\", line 236, in evaluate_retrieval\n    llm = load_model(model_path, enable_lora=enable_lora)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/data/taoshuchang.tsc/IR_RAG/IRM/inference_new.py\", line 49, in load_model\n    llm = LLM(model=model_path, tensor_parallel_size=torch.cuda.device_count(), enable_lora=enable_lora)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/entrypoints/llm.py\", line 214, in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 564, in from_engine_args\n    engine = cls(\n             ^^^^\n  File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 325, in __init__\n    self.model_executor = executor_class(\n                          ^^^^^^^^^^^^^^^\n  File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/executor/distributed_gpu_executor.py\", line 26, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/executor/executor_base.py\", line 47, in __init__\n    self._init_executor()\n  File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 110, in _init_executor\n    self._run_workers(\"init_device\")\n  File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 185, in _run_workers\n    driver_worker_output = driver_worker_method(*args, **kwargs)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/worker/worker.py\", line 176, in init_device\n    init_worker_distributed_environment(self.parallel_config, self.rank,\n  File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/worker/worker.py\", line 448, in init_worker_distributed_environment\n    init_distributed_environment(parallel_config.world_size, rank,\n  File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/vllm/distributed/parallel_state.py\", line 946, in init_distributed_environment\n    torch.distributed.init_process_group(\n  File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/distributed/c10d_logger.py\", line 83, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/distributed/c10d_logger.py\", line 97, in wrapper\n    func_return = func(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\", line 1520, in init_process_group\n    store, rank, world_size = next(rendezvous_iterator)\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/distributed/rendezvous.py\", line 221, in _tcp_rendezvous_handler\n    store = _create_c10d_store(\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/data/taoshuchang.tsc/anaconda3/envs/py311llama33/lib/python3.11/site-packages/torch/distributed/rendezvous.py\", line 189, in _create_c10d_store\n    return TCPStore(\n           ^^^^^^^^^\ntorch.distributed.DistStoreError: Timed out after 601 seconds waiting for clients. 1/8 clients joined.\n\nERROR conda.cli.main_run:execute(49): `conda run python -u /mnt/data/taoshuchang.tsc/IR_RAG/IRM/inference_new.py --test_data_path /mnt/data/taoshuchang.tsc/IR_RAG/IRM/datasets/hotpot_contriever/hotpot-dev.json --batch_size 4 --model_path /mnt/data/taoshuchang.tsc/IR_RAG/ckpt/hotpot_contriever/analyze_merge//hotpot_1doc_other_Meta-Llama-3-70B-Instruct_lr1e5 --result_save_path /mnt/data/taoshuchang.tsc/IR_RAG/IRM/result/hotpot_contriever/analyze/hotpot_1doc_other_Meta-Llama-3-70B-Instruct_lr1e5.json --batch True` failed. (See above for error)\n\n```\n\nThank you for your help in resolving this issue.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-19T12:06:45+00:00",
    "closed_at": "2025-06-22T02:15:00+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12189/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12189"
  },
  {
    "number": 11616,
    "title": "vllm build failure on IBM ppc64le",
    "body": "### Your current environment\r\nIBM powerpc64le.\r\nRHEL 9/ubi9\r\nvllm: Built from source main branch.\r\n\r\n### How you are installing vllm\r\n\r\n```sh\r\ndocker build -t vllm:latest -f Dockerfile.ppc64le .\r\n```\r\nError:\r\n```\r\n\r\n51.54     cargo:rerun-if-env-changed=PKG_CONFIG_SYSROOT_DIR\r\n51.54 \r\n51.54 \r\n51.54     Could not find openssl via pkg-config:\r\n51.54 \r\n51.54     pkg-config exited with status code 1\r\n51.54     > PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 pkg-config --libs --cflags openssl\r\n51.54 \r\n51.54     The system library `openssl` required by crate `openssl-sys` was not found.\r\n51.54     The file `openssl.pc` needs to be installed and the PKG_CONFIG_PATH environment variable must contain its parent directory.\r\n51.54     The PKG_CONFIG_PATH environment variable is not set.\r\n51.54 \r\n51.54     HINT: if you have installed the library, try setting PKG_CONFIG_PATH to the directory containing `openssl.pc`.\r\n51.55 \r\n51.55 \r\n51.55     cargo:warning=Could not find directory of OpenSSL installation, and this `-sys` crate cannot proceed without this knowledge. If OpenSSL is installed and this crate had trouble finding it,  you can set the `OPENSSL_DIR` environment variable for the compilation process. See stderr section below for further information.\r\n51.55 \r\n51.55     --- stderr\r\n51.55 \r\n51.55 \r\n51.55     Could not find directory of OpenSSL installation, and this `-sys` crate cannot\r\n51.55     proceed without this knowledge. If OpenSSL is installed and this crate had\r\n51.55     trouble finding it,  you can set the `OPENSSL_DIR` environment variable for the\r\n51.55     compilation process.\r\n51.55 \r\n51.55     Make sure you also have the development packages of openssl installed.\r\n51.55     For example, `libssl-dev` on Ubuntu or `openssl-devel` on Fedora.\r\n51.56 \r\n51.56     If you're in a situation where you think the directory *should* be found\r\n51.56     automatically, please open a bug at https://github.com/sfackler/rust-openssl\r\n51.56     and include information about your system as well as this message.\r\n51.56 \r\n51.56     $HOST = powerpc64le-unknown-linux-gnu\r\n51.56     $TARGET = powerpc64le-unknown-linux-gnu\r\n51.56     openssl-sys = 0.9.104\r\n51.56 \r\n51.56 \r\n51.56   warning: build failed, waiting for other jobs to finish...\r\n63.11   error: `cargo rustc --lib --message-format=json-render-diagnostics --manifest-path Cargo.toml --release -v --features 'pyo3/extension-module python-bindings' --crate-type cdylib -- --crate-type=cdylib` failed with code 101\r\n63.14   error: subprocess-exited-with-error\r\n63.14   \r\n63.14   \u00d7 Building wheel for outlines_core (pyproject.toml) did not run successfully.\r\n63.14   \u2502 exit code: 1\r\n63.14   \u2570\u2500> See above for output.\r\n\r\n....\r\n....\r\n\r\n70.55   Created wheel for httptools: filename=httptools-0.6.4-cp310-cp310-linux_ppc64le.whl size=130467 sha256=84914d86dae1fee9efa3cd10af06f5594ee7cfe16fb189af7b0acb7522b26a95\r\n70.55   Stored in directory: /root/.cache/pip/wheels/2a/2e/af/680e384ed0eb6f474586969a6552a57bd8d8608477364468b1\r\n70.55 Successfully built httptools\r\n70.55 Failed to build outlines_core\r\n70.55 ERROR: Could not build wheels for outlines_core, which is required to install pyproject.toml-based projects\r\n------\r\nDockerfile.ppc64le:22\r\n--------------------\r\n  21 |     # These packages will be in rocketce eventually\r\n  22 | >>> RUN --mount=type=cache,target=/root/.cache/pip  \\\r\n  23 | >>>     pip install -v --prefer-binary --extra-index-url https://repo.fury.io/mgiessing \\\r\n  24 | >>>         'cmake>=3.26' ninja packaging 'setuptools-scm>=8' wheel jinja2 \\\r\n  25 | >>>         torch==2.4.1 \\\r\n  26 | >>>         -r requirements-cpu.txt \\\r\n  27 | >>>         xformers uvloop==0.20.0\r\n  28 |     \r\n--------------------\r\nERROR: failed to solve: process \"/usr/local/bin/_dockerfile_shell.sh pip install -v --prefer-binary --extra-index-url https://repo.fury.io/mgiessing         'cmake>=3.26' ninja packaging 'setuptools-scm>=8' wheel jinja2         torch==2.4.1         -r requirements-cpu.txt         xformers uvloop==0.20.0\" did not complete successfully: exit code: 1\r\n```\r\n\r\nEven after installing `libssl-dev`, outlines_core fails to locate the libraries with following error -\r\n```\r\n136.0     = note: LC_ALL=\"C\" PATH=\"/opt/conda/lib/rustlib/powerpc64le-unknown-linux-gnu/bin:/tmp/pip-build-env-fw2vfbb3/overlay/bin:/tmp/pip-build-env-fw2vfbb3/normal/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/cargo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/conda/bin/\" VSLANG=\"1033\" \"/opt/conda/bin/powerpc64le-conda-linux-gnu-cc\" \"-Wl,--version-script=/tmp/rustchY3nxX/list\" \"-Wl,--no-undefined-version\" \"-m64\" \"/tmp/rustchY3nxX/symbols.o\" \"/tmp/pip-install-q5sk0wnm/outlines-core_270e564fa5b94bfdac308631b28baba9/target/release/deps/outlines_core.outlines_core.4d42cff6ba60f38d-cgu.0.rcgu.o\" \"-Wl,--as-needed\" \"-L\" \"/tmp/pip-install-q5sk0wnm/outlines-core_270e564fa5b94bfdac308631b28baba9/target/release/deps\" \"-L\" \"/tmp/pip-install-q5sk0wnm/outlines-core_270e564fa5b94bfdac308631b28baba9/target/release/build/ring-e51f19535f8bb199/out\" \"-L\" \"/tmp/pip-install-q5sk0wnm/outlines-core_270e564fa5b94bfdac308631b28baba9/target/release/build/esaxx-rs-ff5db49b271e086d/out\" \"-L\" \"/tmp/pip-install-q5sk0wnm/outlines-core_270e564fa5b94bfdac308631b28baba9/target/release/build/onig_sys-10a029a8b8a442ca/out\" \"-L\" \"/opt/conda/lib/rustlib/powerpc64le-unknown-linux-gnu/lib\" \"-Wl,-Bstatic\" \"/tmp/rustchY3nxX/libesaxx_rs-7f74a07230d29c29.rlib\" \"/tmp/rustchY3nxX/libonig_sys-b5be662fa0ab51dc.rlib\" \"/tmp/rustchY3nxX/libring-8a331d784123b0f3.rlib\" \"/opt/conda/lib/rustlib/powerpc64le-unknown-linux-gnu/lib/libcompiler_builtins-48f1365f3e71a657.rlib\" \"-Wl,-Bdynamic\" \"-lstdc++\" \"-lssl\" \"-lcrypto\" \"-lgcc_s\" \"-lutil\" \"-lrt\" \"-lpthread\" \"-lm\" \"-ldl\" \"-lc\" \"-Wl,--eh-frame-hdr\" \"-Wl,-z,noexecstack\" \"-L\" \"/opt/conda/lib/rustlib/powerpc64le-unknown-linux-gnu/lib\" \"-o\" \"/tmp/pip-install-q5sk0wnm/outlines-core_270e564fa5b94bfdac308631b28baba9/target/release/deps/liboutlines_core.so\" \"-Wl,--gc-sections\" \"-shared\" \"-Wl,-z,relro,-z,now\" \"-Wl,-O1\" \"-Wl,--strip-all\" \"-nodefaultlibs\"\r\n136.0     = note: /opt/conda/bin/../lib/gcc/powerpc64le-conda-linux-gnu/11.2.0/../../../../powerpc64le-conda-linux-gnu/bin/ld: cannot find -lssl: No such file or directory\r\n136.0             /opt/conda/bin/../lib/gcc/powerpc64le-conda-linux-gnu/11.2.0/../../../../powerpc64le-conda-linux-gnu/bin/ld: cannot find -lcrypto: No such file or directory\r\n136.0             collect2: error: ld returned 1 exit status\r\n```\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2024-12-30T07:24:08+00:00",
    "closed_at": "2025-01-08T05:05:39+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11616/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11616"
  },
  {
    "number": 12131,
    "title": "[Bug]: Close feature gaps when using xgrammar for structured output",
    "body": "### \ud83d\udc1b Describe the bug\n\nAs of v0.6.5, we use xgrammar as the default backend for structured output. However, not all ways of expressing output requirements are supported. This issue is for tracking the list of known cases needed to be resolved for making xgrammar the default in all cases.\n\nFallback cases can be found here: https://github.com/vllm-project/vllm/blob/d06e824006d1ba4b92871347738ce1b89f658499/vllm/model_executor/guided_decoding/__init__.py#L40-L76\n\n- [ ] non-x86 architectures\n- [ ] regex \n  - related: https://github.com/mlc-ai/xgrammar/pull/144\n  - https://github.com/mlc-ai/xgrammar/issues/175\n- [ ] choice \\\n  - https://github.com/vllm-project/vllm/pull/12632\n- [ ] jsonschema support is incomplete\n  - https://github.com/mlc-ai/xgrammar/issues/160\n- [ ] lark grammars",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "open",
    "created_at": "2025-01-16T21:10:50+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12131/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/12131"
  },
  {
    "number": 11861,
    "title": "[Usage]: How do I set default temperature for openai compatible server?",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-177-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090\r\nNvidia driver version: 535.113.01\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      43 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             32\r\nOn-line CPU(s) list:                0-31\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 7282 16-Core Processor\r\nCPU family:                         23\r\nModel:                              49\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 16\r\nSocket(s):                          1\r\nStepping:                           0\r\nFrequency boost:                    enabled\r\nCPU max MHz:                        2800.0000\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           5600.14\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip rdpid overflow_recov succor smca sme sev sev_es\r\nVirtualization:                     AMD-V\r\nL1d cache:                          512 KiB (16 instances)\r\nL1i cache:                          512 KiB (16 instances)\r\nL2 cache:                           8 MiB (16 instances)\r\nL3 cache:                           64 MiB (4 instances)\r\nNUMA node(s):                       1\r\nNUMA node0 CPU(s):                  0-31\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Vulnerable\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.6+cu121torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.47.1\r\n[pip3] triton==3.1.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.6.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \t0-31\t\tN/A\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNVIDIA_VISIBLE_DEVICES=all\r\nNVIDIA_REQUIRE_CUDA=cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\r\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\r\nVLLM_USAGE_SOURCE=production-docker-image\r\nCUDA_VERSION=12.1.0\r\nLD_LIBRARY_PATH=/usr/local/lib/python3.12/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nCUDA_MODULE_LOADING=LAZY\r\n\r\n```\r\n\n\n### How would you like to use vllm\n\nHello guys, I am trying to set the default temperature for openai compatible server and unfortunately I am unable to do it. I created a json like the following inside the folder `generation_config`:\r\n```json\r\n{\r\n    \"temperature\": 0.7,\r\n}\r\n```\r\n\r\nAnd I ran this command:\r\n```bash\r\npython3 -m vllm.entrypoints.openai.api_server --model /models/Qwen2.5-14B-Instruct/Qwen2.5-14B-Instruct-Q4_K_M.gguf --tokenizer Qwen/Qwen2.5-14B-Instruct --host \"0.0.0.0\" --port 5000 --gpu-memory-utilization 1 --served-model-name \"VLLMQwen2.5-14B\" --max-num-batched-tokens 32768 --max-num-seqs 256 --max_model_len 32768 --generation-config generation_config\r\n```\r\n\r\nBut I get an error with about incompatibility:\r\n```bash\r\nERROR 01-08 09:10:14 engine.py:366] No supported config format found in generation_config\r\n``` \r\n\r\nWhat's the correct way to set a default value for temperature and other parameters? By default, temperature is set to 1:\r\nhttps://github.com/vllm-project/vllm/blob/571da8fc431ec36427ee1034a7779b23229b015e/vllm/sampling_params.py#L219\r\n\r\nBut I want to modify from the side of the server and not from the side of the client/user.\r\n\r\nAny idea how to do it? \n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-01-08T17:46:11+00:00",
    "closed_at": "2025-01-15T14:52:36+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11861/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11861"
  },
  {
    "number": 11805,
    "title": "[Bug]: preemptmode recompute",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...                                                                                                                                                                                   \r\nPyTorch version: 2.5.1+cu124                                                                                                                                                                                            \r\nIs debug build: False                \r\nCUDA used to build PyTorch: 12.4                     \r\nROCM used to build PyTorch: N/A                      \r\n                                                                                                            \r\nOS: Ubuntu 22.04.4 LTS (x86_64)                                                                             \r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0                                                                                                                                                                      \r\nClang version: Could not collect                                                                            \r\nCMake version: version 3.29.2                                                                               \r\nLibc version: glibc-2.35                                                                                    \r\n                                                                                                            \r\nPython version: 3.10.13 (main, Apr 28 2024, 11:15:22) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-78-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True                                                                                     \r\nCUDA runtime version: 12.1.105               \r\nCUDA_MODULE_LOADING set to: LAZY                                                                            \r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090                                         \r\nNvidia driver version: 535.129.03                                                                                                                                                                                       \r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0  \r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0                                                                                                                                                                   \r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Platinum 8352V CPU @ 2.10GHz\r\nCPU family:                      6\r\nModel:                           106\r\nThread(s) per core:              2\r\nCore(s) per socket:              32\r\nSocket(s):                       2\r\nStepping:                        6\r\nCPU max MHz:                     3500.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        4000.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs\r\n bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx \r\nf16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm\r\n rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinv\r\nd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\r\nVirtualization:                  VT-x\r\nL1d cache:                       3 MiB (64 instances)\r\nL1i cache:                       2 MiB (64 instances)\r\nL2 cache:                        80 MiB (64 instances) \r\nL3 cache:                        108 MiB (2 instances) \r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-31,64-95\r\nNUMA node1 CPU(s):               32-63,96-127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nVersions of relevant libraries:                                                                                                                                                                                  [0/222]\r\n[pip3] numpy==1.26.4                                 \r\n[pip3] nvidia-cublas-cu12==12.4.5.8                  \r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127              \r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127              \r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147                 \r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30                 \r\n[pip3] nvidia-nccl-cu12==2.21.5                                                                             \r\n[pip3] nvidia-nvjitlink-cu12==12.4.127        \r\n[pip3] nvidia-nvtx-cu12==12.4.127   \r\n[pip3] pyzmq==26.0.3                  \r\n[pip3] torch==2.5.1                          \r\n[pip3] torchvision==0.20.1                                                                                  \r\n[pip3] transformers==4.47.1       \r\n[pip3] triton==3.1.0                \r\n[pip3] vllm_nccl_cu12==2.18.1.0.4.0\r\n[conda] Could not collect          \r\nROCM Version: Could not collect   \r\nNeuron SDK Version: N/A           \r\nvLLM Version: 0.6.6.post1                 \r\nvLLM Build Flags:                        \r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:                                                                                                                                                                                                           \r\nGPU0    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID                                                                                                                                                     \r\nGPU0     X      SYS     SYS     0-31,64-95      0               N/A                                                                                                                                                     \r\nNIC0    SYS      X      PIX                                                                                                                                                                                             \r\nNIC1    SYS     PIX      X                                                                                                                                                                                              \r\n                                                      \r\nLegend:                                              \r\n                                                      \r\n  X    = Self                                                                                               \r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node                                                                                                            \r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)                       \r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)              \r\n  PIX  = Connection traversing at most a single PCIe bridge                                                 \r\n  NV#  = Connection traversing a bonded set of # NVLinks                                                    \r\n                                                      \r\nNIC Legend:                                  \r\n                                                                                                            \r\n  NIC0: mlx5_0                               \r\n  NIC1: mlx5_1                                                                                              \r\n                                                                                                            \r\nLD_LIBRARY_PATH=/root/.pyenv/versions/3.10.13/envs/vllm/lib/python3.10/site-packages/cv2/../../lib64:                                                                                                                   \r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nMy task needs structured json output, so I met this problem and I'm not sure whether it's bug or not.\r\nThe scheduler will trigger preemption when the load is high:\r\n1. enable prefix caching, prompt mode == recompute, the output structure is unstable;\r\n2. enable prefix caching, prompt mode == swap , the output structure is stable;\r\n3. disable prefix caching, prompt mode == recompute, the output stucture is stable.\r\nI did serveral experiments and it always happened. I didn't look deep into the code so im not sure whether there is something missing or a confict between prefix caching and preemption?\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-07T07:51:56+00:00",
    "closed_at": "2025-05-08T02:10:47+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11805/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11805"
  },
  {
    "number": 11931,
    "title": "[Bug]: RuntimeError: CUDA error: device-side assert triggered CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 24.04.1 LTS (x86_64)\r\nGCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\r\nClang version: Could not collect\r\nCMake version: version 3.31.2\r\nLibc version: glibc-2.39\r\n\r\nPython version: 3.10.16 (main, Dec  4 2024, 08:53:38) [GCC 13.2.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-204-generic-x86_64-with-glibc2.39\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.91\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA A10\r\nGPU 1: NVIDIA A10\r\nGPU 2: NVIDIA A10\r\nGPU 3: NVIDIA A10\r\n\r\nNvidia driver version: 525.147.05\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             80\r\nOn-line CPU(s) list:                0-79\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Silver 4316 CPU @ 2.30GHz\r\nCPU family:                         6\r\nModel:                              106\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 20\r\nSocket(s):                          2\r\nStepping:                           6\r\nFrequency boost:                    enabled\r\nCPU(s) scaling MHz:                 42%\r\nCPU max MHz:                        2301.0000\r\nCPU min MHz:                        800.0000\r\nBogoMIPS:                           4600.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          1.9 MiB (40 instances)\r\nL1i cache:                          1.3 MiB (40 instances)\r\nL2 cache:                           50 MiB (40 instances)\r\nL3 cache:                           60 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-19,40-59\r\nNUMA node1 CPU(s):                  20-39,60-79\r\nVulnerability Gather data sampling: Mitigation; Microcode\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.1.1\r\n[pip3] sentence-transformers==3.3.1\r\n[pip3] torch==2.5.1\r\n[pip3] torchaudio==2.4.0\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.47.1\r\n[pip3] transformers-stream-generator==0.0.5\r\n[pip3] triton==3.1.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.6\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\r\nGPU0\t X \tPIX\tSYS\tSYS\t0-19,40-59\t0\r\nGPU1\tPIX\t X \tSYS\tSYS\t0-19,40-59\t0\r\nGPU2\tSYS\tSYS\t X \tPXB\t20-39,60-79\t1\r\nGPU3\tSYS\tSYS\tPXB\t X \t20-39,60-79\t1\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNVIDIA_VISIBLE_DEVICES=all\r\nLD_LIBRARY_PATH=/usr/local/lib/python3.10/dist-packages/cv2/../../lib64:/usr/local/lib/python3.10/dist-packages/nvidia/nvjitlink/lib:/usr/local/cuda-12.2/lib64:\r\nCUDA_HOME=/usr/local/cuda-12.2\r\nCUDA_HOME=/usr/local/cuda-12.2\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\nINFO 01-10 21:49:05 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20250110-214905.pkl...\r\nWARNING 01-10 21:49:05 model_runner_base.py:143] Failed to pickle inputs of failed execution: CUDA error: device-side assert triggered\r\nWARNING 01-10 21:49:05 model_runner_base.py:143] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nWARNING 01-10 21:49:05 model_runner_base.py:143] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\nWARNING 01-10 21:49:05 model_runner_base.py:143] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\nWARNING 01-10 21:49:05 model_runner_base.py:143]\r\nERROR 01-10 21:49:05 engine.py:366] Error in model execution: CUDA error: device-side assert triggered\r\nERROR 01-10 21:49:05 engine.py:366] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nProcess SpawnProcess-1:\r\nERROR 01-10 21:49:05 engine.py:366] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\nERROR 01-10 21:49:05 engine.py:366] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\nERROR 01-10 21:49:05 engine.py:366] Traceback (most recent call last):\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner_base.py\", line 116, in _wrapper\r\nERROR 01-10 21:49:05 engine.py:366]     return func(*args, **kwargs)\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1691, in execute_model\r\nERROR 01-10 21:49:05 engine.py:366]     hidden_or_intermediate_states = model_executable(\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nERROR 01-10 21:49:05 engine.py:366]     return self._call_impl(*args, **kwargs)\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\nERROR 01-10 21:49:05 engine.py:366]     return forward_call(*args, **kwargs)\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_vl.py\", line 1171, in forward\r\nERROR 01-10 21:49:05 engine.py:366]     hidden_states = self.language_model.model(\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/compilation/decorators.py\", line 168, in __call__\r\nERROR 01-10 21:49:05 engine.py:366]     return self.forward(*args, **kwargs)\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 340, in forward\r\nERROR 01-10 21:49:05 engine.py:366]     hidden_states, residual = layer(\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nERROR 01-10 21:49:05 engine.py:366]     return self._call_impl(*args, **kwargs)\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\nERROR 01-10 21:49:05 engine.py:366]     return forward_call(*args, **kwargs)\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 247, in forward\r\nERROR 01-10 21:49:05 engine.py:366]     hidden_states = self.self_attn(\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nERROR 01-10 21:49:05 engine.py:366]     return self._call_impl(*args, **kwargs)\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\nERROR 01-10 21:49:05 engine.py:366]     return forward_call(*args, **kwargs)\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 176, in forward\r\nERROR 01-10 21:49:05 engine.py:366]     attn_output = self.attn(q,\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nERROR 01-10 21:49:05 engine.py:366]     return self._call_impl(*args, **kwargs)\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\nERROR 01-10 21:49:05 engine.py:366]     return forward_call(*args, **kwargs)\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/layer.py\", line 154, in forward\r\nTraceback (most recent call last):\r\nERROR 01-10 21:49:05 engine.py:366]     torch.ops.vllm.unified_attention_with_output(\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 1116, in __call__\r\nERROR 01-10 21:49:05 engine.py:366]     return self._op(*args, **(kwargs or {}))\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/layer.py\", line 288, in unified_attention_with_output\r\nERROR 01-10 21:49:05 engine.py:366]     self.impl.forward(query,\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/backends/flash_attn.py\", line 736, in forward\r\nERROR 01-10 21:49:05 engine.py:366]     flash_attn_varlen_func(\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 270, in flash_attn_varlen_func\r\nERROR 01-10 21:49:05 engine.py:366]     out, softmax_lse = _flash_attn_varlen_forward(\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 91, in _flash_attn_varlen_forward\r\nERROR 01-10 21:49:05 engine.py:366]     out, softmax_lse = torch.ops.vllm_flash_attn_c.varlen_fwd(\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 1116, in __call__\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner_base.py\", line 116, in _wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1691, in execute_model\r\n    hidden_or_intermediate_states = model_executable(\r\nERROR 01-10 21:49:05 engine.py:366]     return self._op(*args, **(kwargs or {}))\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nERROR 01-10 21:49:05 engine.py:366] RuntimeError: CUDA error: device-side assert triggered\r\n    return self._call_impl(*args, **kwargs)\r\nERROR 01-10 21:49:05 engine.py:366] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\nERROR 01-10 21:49:05 engine.py:366] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n    return forward_call(*args, **kwargs)\r\nERROR 01-10 21:49:05 engine.py:366] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_vl.py\", line 1171, in forward\r\nERROR 01-10 21:49:05 engine.py:366]\r\n    hidden_states = self.language_model.model(\r\nERROR 01-10 21:49:05 engine.py:366]\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/compilation/decorators.py\", line 168, in __call__\r\nERROR 01-10 21:49:05 engine.py:366] The above exception was the direct cause of the following exception:\r\n    return self.forward(*args, **kwargs)\r\nERROR 01-10 21:49:05 engine.py:366]\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 340, in forward\r\nERROR 01-10 21:49:05 engine.py:366] Traceback (most recent call last):\r\n    hidden_states, residual = layer(\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/engine.py\", line 357, in run_mp_engine\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nERROR 01-10 21:49:05 engine.py:366]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/engine.py\", line 119, in from_engine_args\r\nERROR 01-10 21:49:05 engine.py:366]     return cls(ipc_path=ipc_path,\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/engine.py\", line 71, in __init__\r\nERROR 01-10 21:49:05 engine.py:366]     self.engine = LLMEngine(*args, **kwargs)\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 276, in __init__\r\nERROR 01-10 21:49:05 engine.py:366]     self._initialize_kv_caches()\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 416, in _initialize_kv_caches\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 247, in forward\r\n    hidden_states = self.self_attn(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nERROR 01-10 21:49:05 engine.py:366]     self.model_executor.determine_num_available_blocks())\r\n    return self._call_impl(*args, **kwargs)\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 68, in determine_num_available_blocks\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\nERROR 01-10 21:49:05 engine.py:366]     return self.driver_worker.determine_num_available_blocks()\r\n    return forward_call(*args, **kwargs)\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 176, in forward\r\nERROR 01-10 21:49:05 engine.py:366]     return func(*args, **kwargs)\r\n    attn_output = self.attn(q,\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 202, in determine_num_available_blocks\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\nERROR 01-10 21:49:05 engine.py:366]     self.model_runner.profile_run()\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return forward_call(*args, **kwargs)\r\nERROR 01-10 21:49:05 engine.py:366]     return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/layer.py\", line 154, in forward\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1331, in profile_run\r\n    torch.ops.vllm.unified_attention_with_output(\r\nERROR 01-10 21:49:05 engine.py:366]     self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 1116, in __call__\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return self._op(*args, **(kwargs or {}))\r\nERROR 01-10 21:49:05 engine.py:366]     return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/layer.py\", line 288, in unified_attention_with_output\r\nERROR 01-10 21:49:05 engine.py:366]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner_base.py\", line 146, in _wrapper\r\n    self.impl.forward(query,\r\nERROR 01-10 21:49:05 engine.py:366]     raise type(err)(f\"Error in model execution: \"\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/backends/flash_attn.py\", line 736, in forward\r\nERROR 01-10 21:49:05 engine.py:366] RuntimeError: Error in model execution: CUDA error: device-side assert triggered\r\n    flash_attn_varlen_func(\r\nERROR 01-10 21:49:05 engine.py:366] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 270, in flash_attn_varlen_func\r\nERROR 01-10 21:49:05 engine.py:366] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\n    out, softmax_lse = _flash_attn_varlen_forward(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/vllm_flash_attn/flash_attn_interface.py\", line 91, in _flash_attn_varlen_forward\r\nERROR 01-10 21:49:05 engine.py:366] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n    out, softmax_lse = torch.ops.vllm_flash_attn_c.varlen_fwd(\r\nERROR 01-10 21:49:05 engine.py:366]\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 1116, in __call__\r\n    return self._op(*args, **(kwargs or {}))\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n### \ud83d\udc1b Describe the bug\n\nwhen upgrading VLLM version to 0.6.6, an error occurred when starting Qwen2-VL 7B model.\r\nrunning normally in VLLM 0.6.3 version.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-01-10T14:00:43+00:00",
    "closed_at": "2025-01-11T05:29:28+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11931/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11931"
  },
  {
    "number": 11986,
    "title": "[Bug]: CPU Offloading errors (Worker.__init__() got an unexpected keyword argument 'kv_cache_dtype')",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.22.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.12.8 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:31:09) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.8.0-50-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.6.85\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090\r\nNvidia driver version: 550.120\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        43 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               32\r\nOn-line CPU(s) list:                  0-31\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD EPYC 7282 16-Core Processor\r\nCPU family:                           23\r\nModel:                                49\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   16\r\nSocket(s):                            1\r\nStepping:                             0\r\nFrequency boost:                      enabled\r\nCPU max MHz:                          2800.0000\r\nCPU min MHz:                          1500.0000\r\nBogoMIPS:                             5599.50\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sev sev_es\r\nVirtualization:                       AMD-V\r\nL1d cache:                            512 KiB (16 instances)\r\nL1i cache:                            512 KiB (16 instances)\r\nL2 cache:                             8 MiB (16 instances)\r\nL3 cache:                             64 MiB (4 instances)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-31\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pynvml==12.0.0\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1\r\n[pip3] torchaudio==2.5.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.48.0\r\n[pip3] triton==3.1.0\r\n[conda] cuda-cccl_linux-64        12.6.77                       0    nvidia\r\n[conda] cuda-command-line-tools   12.6.3                        0    nvidia\r\n[conda] cuda-compiler             12.6.3                        0    nvidia\r\n[conda] cuda-crt-dev_linux-64     12.6.85                       0    nvidia\r\n[conda] cuda-crt-tools            12.6.85                       0    nvidia\r\n[conda] cuda-cudart               12.6.77                       0    nvidia\r\n[conda] cuda-cudart-dev           12.6.77                       0    nvidia\r\n[conda] cuda-cudart-dev_linux-64  12.6.77                       0    nvidia\r\n[conda] cuda-cudart-static        12.6.77                       0    nvidia\r\n[conda] cuda-cudart-static_linux-64 12.6.77                       0    nvidia\r\n[conda] cuda-cudart_linux-64      12.6.77                       0    nvidia\r\n[conda] cuda-cuobjdump            12.6.77                       0    nvidia\r\n[conda] cuda-cupti                12.6.80                       0    nvidia\r\n[conda] cuda-cupti-dev            12.6.80                       0    nvidia\r\n[conda] cuda-cuxxfilt             12.6.77                       0    nvidia\r\n[conda] cuda-driver-dev           12.6.77                       0    nvidia\r\n[conda] cuda-driver-dev_linux-64  12.6.77                       0    nvidia\r\n[conda] cuda-gdb                  12.6.77                       0    nvidia\r\n[conda] cuda-libraries            12.6.3                        0    nvidia\r\n[conda] cuda-libraries-dev        12.6.3                        0    nvidia\r\n[conda] cuda-nsight               12.6.77                       0    nvidia\r\n[conda] cuda-nvcc                 12.6.85                       0    nvidia\r\n[conda] cuda-nvcc-dev_linux-64    12.6.85                       0    nvidia\r\n[conda] cuda-nvcc-impl            12.6.85                       0    nvidia\r\n[conda] cuda-nvcc-tools           12.6.85                       0    nvidia\r\n[conda] cuda-nvcc_linux-64        12.6.85                       0    nvidia\r\n[conda] cuda-nvdisasm             12.6.77                       0    nvidia\r\n[conda] cuda-nvml-dev             12.6.77                       2    nvidia\r\n[conda] cuda-nvprof               12.6.80                       0    nvidia\r\n[conda] cuda-nvprune              12.6.77                       0    nvidia\r\n[conda] cuda-nvrtc                12.6.85                       0    nvidia\r\n[conda] cuda-nvrtc-dev            12.6.85                       0    nvidia\r\n[conda] cuda-nvtx                 12.6.77                       0    nvidia\r\n[conda] cuda-nvvm-dev_linux-64    12.6.85                       0    nvidia\r\n[conda] cuda-nvvm-impl            12.6.85                       0    nvidia\r\n[conda] cuda-nvvm-tools           12.6.85                       0    nvidia\r\n[conda] cuda-nvvp                 12.6.80                       0    nvidia\r\n[conda] cuda-opencl               12.6.77                       0    nvidia\r\n[conda] cuda-opencl-dev           12.6.77                       0    nvidia\r\n[conda] cuda-profiler-api         12.6.77                       0    nvidia\r\n[conda] cuda-sanitizer-api        12.6.77                       0    nvidia\r\n[conda] cuda-toolkit              12.6.3                        0    nvidia\r\n[conda] cuda-tools                12.6.3                        0    nvidia\r\n[conda] cuda-version              12.6                          3    nvidia\r\n[conda] cuda-visual-tools         12.6.3                        0    nvidia\r\n[conda] gds-tools                 1.11.1.6                      0    nvidia\r\n[conda] libcublas                 12.6.4.1                      0    nvidia\r\n[conda] libcublas-dev             12.6.4.1                      0    nvidia\r\n[conda] libcufft                  11.3.0.4                      0    nvidia\r\n[conda] libcufft-dev              11.3.0.4                      0    nvidia\r\n[conda] libcufile                 1.11.1.6                      0    nvidia\r\n[conda] libcufile-dev             1.11.1.6                      0    nvidia\r\n[conda] libcurand                 10.3.7.77                     0    nvidia\r\n[conda] libcurand-dev             10.3.7.77                     0    nvidia\r\n[conda] libcusolver               11.7.1.2                      0    nvidia\r\n[conda] libcusolver-dev           11.7.1.2                      0    nvidia\r\n[conda] libcusparse               12.5.4.2                      0    nvidia\r\n[conda] libcusparse-dev           12.5.4.2                      0    nvidia\r\n[conda] libnpp                    12.2.5.30                     0    nvidia\r\n[conda] libnpp-dev                12.2.5.30                     0    nvidia\r\n[conda] libnvfatbin               12.6.77                       0    nvidia\r\n[conda] libnvfatbin-dev           12.6.77                       0    nvidia\r\n[conda] libnvjitlink              12.6.85                       0    nvidia\r\n[conda] libnvjitlink-dev          12.6.85                       0    nvidia\r\n[conda] libnvjpeg                 12.3.1.117                    0    nvidia\r\n[conda] libnvjpeg-dev             12.3.1.117                    0    nvidia\r\n[conda] nsight-compute            2024.3.2.3                    0    nvidia\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\r\n[conda] pynvml                    12.0.0                   pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.5.1                    pypi_0    pypi\r\n[conda] torchaudio                2.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.20.1                   pypi_0    pypi\r\n[conda] transformers              4.48.0                   pypi_0    pypi\r\n[conda] triton                    3.1.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.6.post2.dev189+g619ae268\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \t0-31\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nLD_LIBRARY_PATH=/home/acidhax/miniconda3/lib/python3.12/site-packages/cv2/../../lib64:\r\nCUDA_MODULE_LOADING=LAZY\r\n\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\n```bash\r\n(base) acidhax@acidhax-MZ32-AR0-00:~$ vllm serve \"NovaSky-AI/Sky-T1-32B-Preview\" --cpu-offload-gb 200 --device cpu\r\n```\r\n\r\n```logs\r\nINFO 01-13 00:40:14 api_server.py:768] vLLM API server version 0.6.6.post2.dev189+g619ae268\r\nINFO 01-13 00:40:14 api_server.py:769] args: Namespace(subparser='serve', model_tag='NovaSky-AI/Sky-T1-32B-Preview', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NovaSky-AI/Sky-T1-32B-Preview', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=200.0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='cpu', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7e0fd825d940>)\r\nINFO 01-13 00:40:14 __init__.py:179] Automatically detected platform cuda.\r\nINFO 01-13 00:40:14 api_server.py:195] Started engine process with PID 2004444\r\nINFO 01-13 00:40:18 __init__.py:179] Automatically detected platform cuda.\r\nINFO 01-13 00:40:20 config.py:516] This model supports multiple tasks: {'generate', 'score', 'classify', 'embed', 'reward'}. Defaulting to 'generate'.\r\nINFO 01-13 00:40:24 config.py:516] This model supports multiple tasks: {'classify', 'generate', 'embed', 'reward', 'score'}. Defaulting to 'generate'.\r\nINFO 01-13 00:40:24 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post2.dev189+g619ae268) with config: model='NovaSky-AI/Sky-T1-32B-Preview', speculative_config=None, tokenizer='NovaSky-AI/Sky-T1-32B-Preview', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NovaSky-AI/Sky-T1-32B-Preview, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \r\nERROR 01-13 00:40:26 engine.py:387] Worker.__init__() got an unexpected keyword argument 'kv_cache_dtype'\r\nERROR 01-13 00:40:26 engine.py:387] Traceback (most recent call last):\r\nERROR 01-13 00:40:26 engine.py:387]   File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 378, in run_mp_engine\r\nERROR 01-13 00:40:26 engine.py:387]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\r\nERROR 01-13 00:40:26 engine.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 01-13 00:40:26 engine.py:387]   File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 121, in from_engine_args\r\nERROR 01-13 00:40:26 engine.py:387]     return cls(ipc_path=ipc_path,\r\nERROR 01-13 00:40:26 engine.py:387]            ^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 01-13 00:40:26 engine.py:387]   File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 73, in __init__\r\nERROR 01-13 00:40:26 engine.py:387]     self.engine = LLMEngine(*args, **kwargs)\r\nERROR 01-13 00:40:26 engine.py:387]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 01-13 00:40:26 engine.py:387]   File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py\", line 273, in __init__\r\nERROR 01-13 00:40:26 engine.py:387]     self.model_executor = executor_class(vllm_config=vllm_config, )\r\nERROR 01-13 00:40:26 engine.py:387]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 01-13 00:40:26 engine.py:387]   File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 36, in __init__\r\nERROR 01-13 00:40:26 engine.py:387]     self._init_executor()\r\nERROR 01-13 00:40:26 engine.py:387]   File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/executor/cpu_executor.py\", line 78, in _init_executor\r\nERROR 01-13 00:40:26 engine.py:387]     self.driver_worker = self._create_worker()\r\nERROR 01-13 00:40:26 engine.py:387]                          ^^^^^^^^^^^^^^^^^^^^^\r\nERROR 01-13 00:40:26 engine.py:387]   File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/executor/cpu_executor.py\", line 124, in _create_worker\r\nERROR 01-13 00:40:26 engine.py:387]     wrapper.init_worker(**kwargs)\r\nERROR 01-13 00:40:26 engine.py:387]   File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/worker/worker_base.py\", line 455, in init_worker\r\nERROR 01-13 00:40:26 engine.py:387]     self.worker = worker_class(*args, **kwargs)\r\nERROR 01-13 00:40:26 engine.py:387]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 01-13 00:40:26 engine.py:387] TypeError: Worker.__init__() got an unexpected keyword argument 'kv_cache_dtype'\r\nProcess SpawnProcess-1:\r\nTraceback (most recent call last):\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 389, in run_mp_engine\r\n    raise e\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 378, in run_mp_engine\r\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 121, in from_engine_args\r\n    return cls(ipc_path=ipc_path,\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 73, in __init__\r\n    self.engine = LLMEngine(*args, **kwargs)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/engine/llm_engine.py\", line 273, in __init__\r\n    self.model_executor = executor_class(vllm_config=vllm_config, )\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 36, in __init__\r\n    self._init_executor()\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/executor/cpu_executor.py\", line 78, in _init_executor\r\n    self.driver_worker = self._create_worker()\r\n                         ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/executor/cpu_executor.py\", line 124, in _create_worker\r\n    wrapper.init_worker(**kwargs)\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/worker/worker_base.py\", line 455, in init_worker\r\n    self.worker = worker_class(*args, **kwargs)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: Worker.__init__() got an unexpected keyword argument 'kv_cache_dtype'\r\nTraceback (most recent call last):\r\n  File \"/home/acidhax/miniconda3/bin/vllm\", line 8, in <module>\r\n    sys.exit(main())\r\n             ^^^^^^\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/scripts.py\", line 201, in main\r\n    args.dispatch_function(args)\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/scripts.py\", line 42, in serve\r\n    uvloop.run(run_server(args))\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/uvloop/__init__.py\", line 109, in run\r\n    return __asyncio.run(\r\n           ^^^^^^^^^^^^^^\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/asyncio/runners.py\", line 194, in run\r\n    return runner.run(main)\r\n           ^^^^^^^^^^^^^^^^\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/asyncio/runners.py\", line 118, in run\r\n    return self._loop.run_until_complete(task)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/uvloop/__init__.py\", line 61, in wrapper\r\n    return await main\r\n           ^^^^^^^^^^\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 796, in run_server\r\n    async with build_async_engine_client(args) as engine_client:\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/contextlib.py\", line 210, in __aenter__\r\n    return await anext(self.gen)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 125, in build_async_engine_client\r\n    async with build_async_engine_client_from_engine_args(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/contextlib.py\", line 210, in __aenter__\r\n    return await anext(self.gen)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/acidhax/miniconda3/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 219, in build_async_engine_client_from_engine_args\r\n    raise RuntimeError(\r\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\r\n\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-13T05:41:04+00:00",
    "closed_at": "2025-05-24T02:07:52+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11986/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11986"
  },
  {
    "number": 11649,
    "title": "[Performance]: V1 vs V0 with multi-steps",
    "body": "### Proposal to improve performance\r\n\r\nN/A\r\n\r\n### Report of performance regression\r\n\r\nFor V1:\r\npython -m vllm.entrypoints.openai.api_server --model NousResearch/Meta-Llama-3.1-8B-Instruct --tensor-parallel-size 8 --max-num-seqs 32 --max-model-len 4096 --disable-sliding-window --return-tokens-as-token-ids --port 8080 --enable-prefix-caching --enable-chunked-prefill --disable-log-requests --disable-log-stats\r\n\r\nFor V0:\r\npython -m vllm.entrypoints.openai.api_server --model NousResearch/Meta-Llama-3.1-8B-Instruct --tensor-parallel-size 8 --max-num-seqs 32 --max-model-len 4096 --num-scheduler-steps 32 --multi-step-stream-outputs --disable-sliding-window --return-tokens-as-token-ids --port 8080 --enable-prefix-caching --enable-chunked-prefill --disable-log-requests --disable-log-stats\r\n\r\nrun the following script:\r\ntime curl -XPOST -s http://127.0.0.1:8080/v1/chat/completions -H 'content-type: application/json' -H 'Authorization: Bearer 1234' -d '{\"model\": \"NousResearch/Meta-Llama-3.1-8B-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": \"Explain in detail 5 important events of WW2.\"}], \"stream\": true, \"logprobs\": true, \"temperature\": 0.0, \"seed\": 42}' | grep 'data: ' | wc -l\r\n\r\nV1 result:\r\n1545\r\nreal    0m6.946s\r\nuser    0m0.033s\r\nsys     0m0.063s\r\n\r\nV0 result:\r\n1666\r\nreal    0m5.106s\r\nuser    0m0.036s\r\nsys     0m0.076s\r\n\r\nso V0 with multi step is much faster for single request, right?\r\n\r\n### Misc discussion on performance\r\n\r\n_No response_\r\n\r\n### Your current environment (if you think it is necessary)\r\n\r\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-12-31T06:45:17+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11649/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11649"
  }
]
[
  {
    "number": 48,
    "title": "Improve Weight Loading",
    "body": "Just use Huggingface's weights. Don't do another copy!",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-22T04:16:22+00:00",
    "closed_at": "2023-05-03T07:32:05+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/48/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/48"
  },
  {
    "number": 47,
    "title": "Frontend Improvements",
    "body": "1. Current implementation of the FastAPI+asyncio+ray combination seems slow\r\n2. Merge Hao\u2019s throughput profiling code.\r\n3. Make the frontend looks like OpenAI\u2019s API.\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-22T03:57:50+00:00",
    "closed_at": "2023-05-24T04:39:52+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/47/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/47"
  },
  {
    "number": 46,
    "title": "Debug the optimal upper-bound performance for swapping (0-cost swapping).",
    "body": "Rerun the experiment comparing 0-cost swapping and recomputation. Recomputation should not be faster in any case. If recomputation is consistently faster, we should debug into this.",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-22T03:57:07+00:00",
    "closed_at": "2024-11-30T02:03:37+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/46/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/46"
  },
  {
    "number": 45,
    "title": "Turn shareGPT data into a standard benchmark",
    "body": "1. Extract out the lengths of the conversation rounds, and maybe have that data directly available from github.\r\n2. The current L-shape evaluation with binary search for throughput is hard to run and not scalable. We should find an easier way to benchmark the performance.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-22T03:52:50+00:00",
    "closed_at": "2023-06-15T02:55:39+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/45/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/45"
  },
  {
    "number": 44,
    "title": "Fix the rushed out multi-query kernel",
    "body": "1. Fix the correctness issue in the current FlashAttention-copy-based kernel. Make sure we call the FlashAttention kernel correctly. Evaluate the performance of this kernel.\r\n2. Reduce the memory usage of the current kernel by limiting the buffer size and calling the kernel multiple times.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-22T03:49:18+00:00",
    "closed_at": "2024-03-08T10:19:19+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/44/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/44"
  },
  {
    "number": 43,
    "title": "Add support for Stable-LM and OpenAssistant",
    "body": "The two models are popularly used. As we support LLaMA, it'll not be difficult to support these models.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-22T03:48:25+00:00",
    "closed_at": "2023-04-28T07:32:12+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/43/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/43"
  },
  {
    "number": 42,
    "title": "Modify the current PyTorch model to C++",
    "body": "Expected gain: For 13B models, we should see a 20%-30% latency gain on a single GPU and 2-3x on 4 GPUs. For smaller models, the gain should be even higher.\r\n\r\nHaving a single iteration's computation being completely C++ should be enough for high performance. In this way, we can keep most complicated scheduling logics in Python, including weight loading.\r\n\r\nPotential sources of overheads:\r\n1. Python v.s. C++.\r\n2. PyTorch (even in C++) v.s. FasterTransformer.\r\n\r\nHow to implement a C++ version:\r\n1. (Fake C++) Torch compiler (torch.jit).\r\n2. Libtorch, C++ version of PyTorch (easier to implement and extend, but can only solve overhead 1).\r\n3. Prune out the useful single model code from FasterTransformer to CacheFlow. This solves both overheads but is harder to implement.\r\n",
    "labels": [
      "performance"
    ],
    "state": "closed",
    "created_at": "2023-04-22T03:36:03+00:00",
    "closed_at": "2024-09-20T20:59:21+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/42/reactions",
      "total_count": 7,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/42"
  },
  {
    "number": 23,
    "title": "Add an option to disable Ray when using a single GPU",
    "body": "When working with a single GPU, Ray is not useful. Therefore, it would be beneficial to have an option to disable Ray in such scenarios.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-02T07:32:38+00:00",
    "closed_at": "2023-04-30T07:42:19+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/23/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/23"
  },
  {
    "number": 22,
    "title": "Tensor Parallel profiling result",
    "body": "Will update the profiling results in this PR.\r\n\r\n## BS=8, input_len=32, output_len=128\r\n\r\n```\r\nOPT-13B\r\nTP 1: 3.5404738585154214 seconds\r\nTP 2: 4.742188215255737 seconds\r\nTP 4: 4.907034238179524 seconds\r\n\r\nOPT-30B\r\nTP 1: OOM\r\nTP 2: 5.9848620891571045 seconds\r\nTP 4: 5.943212985992432 seconds\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-02T06:50:04+00:00",
    "closed_at": "2023-06-16T02:38:15+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/22/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/22"
  }
]
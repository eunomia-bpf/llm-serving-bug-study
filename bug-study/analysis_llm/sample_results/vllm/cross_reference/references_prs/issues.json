[
  {
    "number": 6165,
    "title": "[Feature]: Return hidden states (in progress?)",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\r\n\r\nI know this feature request sort of already exists: https://github.com/vllm-project/vllm/issues/5950\r\n(and older, semi related requests) https://github.com/vllm-project/vllm/issues/3594 https://github.com/vllm-project/vllm/issues/1857\r\n\r\nThis is a similar pitch but I am creating a new issue as I noticed newer developments in the codebase. The pitch is to support returning hidden states when generating sequences. This enables many potential behaviors such as output classification, guardrails, etc. Whereas #5950 suggested a different step for embedding, I would suggest building it in as an option to EngineArgs or as an option that can be passed in with each generation request. \r\n\r\nI see that in `v0.5.1` there is already some new code in `ModelDriverBase` to support `return_hidden_states`. However, I don't see that supported yet in the LLM engine yet (not an input to `EngineArgs`). Basically, it seems like this feature is under development. I am mainly wondering what the timeline is for that? And what is the approach being taken so that I and the community can develop accordingly?\r\n\r\n\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\n_No response_",
    "labels": [
      "feature request",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-07-06T01:26:10+00:00",
    "closed_at": null,
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6165/reactions",
      "total_count": 8,
      "+1": 8,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6165"
  },
  {
    "number": 5848,
    "title": "[CI] [Flaky test] distributed/test_shm_broadcast.py is flaky",
    "body": "### Anything you want to discuss about vllm.\n\nDistributed comm ops test failed with below stacktrace. [Buildkite](https://buildkite.com/vllm/ci-aws/builds/2539#01904f5d-0bfd-4a7a-96b2-cdc7f8b60b09)\r\n\r\n```\r\n[2024-06-25T12:58:33Z] distributed/test_shm_broadcast.py:72:\r\n--\r\n\u00a0 | [2024-06-25T12:58:33Z] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\u00a0 | [2024-06-25T12:58:33Z]\r\n\u00a0 | [2024-06-25T12:58:33Z] fn = <function worker_fn_wrapper.<locals>.wrapped_fn at 0x7f8cc92afa30>\r\n\u00a0 | [2024-06-25T12:58:33Z] world_size = 4\r\n\u00a0 | [2024-06-25T12:58:33Z]\r\n\u00a0 | [2024-06-25T12:58:33Z]     def distributed_run(fn, world_size):\r\n\u00a0 | [2024-06-25T12:58:33Z]         number_of_processes = world_size\r\n\u00a0 | [2024-06-25T12:58:33Z]         processes = []\r\n\u00a0 | [2024-06-25T12:58:33Z]         for i in range(number_of_processes):\r\n\u00a0 | [2024-06-25T12:58:33Z]             env = {}\r\n\u00a0 | [2024-06-25T12:58:33Z]             env['RANK'] = str(i)\r\n\u00a0 | [2024-06-25T12:58:33Z]             env['LOCAL_RANK'] = str(i)\r\n\u00a0 | [2024-06-25T12:58:33Z]             env['WORLD_SIZE'] = str(number_of_processes)\r\n\u00a0 | [2024-06-25T12:58:33Z]             env['LOCAL_WORLD_SIZE'] = str(number_of_processes)\r\n\u00a0 | [2024-06-25T12:58:33Z]             env['MASTER_ADDR'] = 'localhost'\r\n\u00a0 | [2024-06-25T12:58:33Z]             env['MASTER_PORT'] = '12345'\r\n\u00a0 | [2024-06-25T12:58:33Z]             p = multiprocessing.Process(target=fn, args=(env, ))\r\n\u00a0 | [2024-06-25T12:58:33Z]             processes.append(p)\r\n\u00a0 | [2024-06-25T12:58:33Z]             p.start()\r\n\u00a0 | [2024-06-25T12:58:33Z]\r\n\u00a0 | [2024-06-25T12:58:33Z]         for p in processes:\r\n\u00a0 | [2024-06-25T12:58:33Z]             p.join()\r\n\u00a0 | [2024-06-25T12:58:33Z]\r\n\u00a0 | [2024-06-25T12:58:33Z]         for p in processes:\r\n\u00a0 | [2024-06-25T12:58:33Z] >           assert p.exitcode == 0\r\n\u00a0 | [2024-06-25T12:58:33Z] E           AssertionError: assert 1 == 0\r\n\u00a0 | [2024-06-25T12:58:33Z] E            +  where 1 = <Process name='Process-1' pid=15885 parent=7 stopped exitcode=1>.exitcode\r\n```",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-06-25T23:22:24+00:00",
    "closed_at": "2024-06-26T04:56:04+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5848/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/5848"
  },
  {
    "number": 13409,
    "title": "[Feature]: Prompt logprobs + APC compatibility",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\n[#9880](https://github.com/vllm-project/vllm/pull/9880) adds sample and prompt logprobs support, however prompt logprobs currently require the server to be instantiated with `--no-enable-prefix-caching`; otherwise, a request with `prompt_logprobs=true` will cause the request to fail with the message \"Prefix caching with prompt logprobs not yet supported on VLLM V1.\"\n\nThe challenge of using prompt logprobs alongside APC is how to recover the topk prompt logprobs from an APC cache hit. The existing APC implementation does not cache prompt logprobs; upon a cache hit, cached blocks are treated as \"computed\" & no prompt logprobs are available for the computed blocks.\n\n### Alternatives\n\nA few possible solutions:\n* **Use APC cached KVs to recompute prompt logprobs if a request with `prompt_logprobs=true` triggers an APC cache hit.** This requires model code and `model_executor` code to support re-running prefill using cached KVs.\n* **Cache prompt logprobs in the APC.** The problem with this solution is that a request which triggers an APC cache hit may require a greater number of topk prompt logprobs than the request which filled the cache, in which case recomputation would be necessary anyway.\n* **Bypass APC for requests with `prompt_logprobs=true`.** Requests with `prompt_logprobs=true` cannot exploit APC cache. This is the simplest solution but incurs a performance penalty.\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-02-17T15:33:22+00:00",
    "closed_at": "2025-02-17T17:27:11+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13409/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/13409"
  },
  {
    "number": 9226,
    "title": "[Bug]: Could not `pip install vllm` inside dockerfile after certain commit in `main` branch",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.26.4\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.25-051525-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA GeForce RTX 4090\r\nGPU 1: NVIDIA GeForce RTX 4090\r\n\r\nNvidia driver version: 545.23.08\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   39 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          16\r\nOn-line CPU(s) list:             0-15\r\nVendor ID:                       GenuineIntel\r\nModel name:                      13th Gen Intel(R) Core(TM) i5-13400\r\nCPU family:                      6\r\nModel:                           191\r\nThread(s) per core:              2\r\nCore(s) per socket:              10\r\nSocket(s):                       1\r\nStepping:                        2\r\nCPU max MHz:                     3425.8350\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        4992.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req umip pku ospke waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize arch_lbr flush_l1d arch_capabilities\r\nVirtualization:                  VT-x\r\nL1d cache:                       416 KiB (10 instances)\r\nL1i cache:                       448 KiB (10 instances)\r\nL2 cache:                        9.5 MiB (7 instances)\r\nL3 cache:                        20 MiB (1 instance)\r\nNUMA node(s):                    1\r\nNUMA node0 CPU(s):               0-15\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] optree==0.12.1\r\n[pip3] torch==2.4.0\r\n[pip3] torchaudio==2.4.0\r\n[pip3] torchelastic==0.2.2\r\n[pip3] torchvision==0.19.0\r\n[pip3] triton==3.0.0\r\n[conda] blas                      1.0                         mkl\r\n[conda] cuda-cudart               12.1.105                      0    nvidia\r\n[conda] cuda-cupti                12.1.105                      0    nvidia\r\n[conda] cuda-libraries            12.1.0                        0    nvidia\r\n[conda] cuda-nvrtc                12.1.105                      0    nvidia\r\n[conda] cuda-nvtx                 12.1.105                      0    nvidia\r\n[conda] cuda-opencl               12.5.39                       0    nvidia\r\n[conda] cuda-runtime              12.1.0                        0    nvidia\r\n[conda] cuda-version              12.5                          3    nvidia\r\n[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\r\n[conda] libcublas                 12.1.0.26                     0    nvidia\r\n[conda] libcufft                  11.0.2.4                      0    nvidia\r\n[conda] libcufile                 1.10.1.7                      0    nvidia\r\n[conda] libcurand                 10.3.6.82                     0    nvidia\r\n[conda] libcusolver               11.4.4.55                     0    nvidia\r\n[conda] libcusparse               12.0.2.55                     0    nvidia\r\n[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\r\n[conda] libnpp                    12.0.2.50                     0    nvidia\r\n[conda] libnvjitlink              12.1.105                      0    nvidia\r\n[conda] libnvjpeg                 12.1.1.14                     0    nvidia\r\n[conda] mkl                       2023.1.0         h213fc3f_46344\r\n[conda] mkl-service               2.4.0           py311h5eee18b_1\r\n[conda] mkl_fft                   1.3.8           py311h5eee18b_0\r\n[conda] mkl_random                1.2.4           py311hdb19cb5_0\r\n[conda] numpy                     1.26.4          py311h08b1b3b_0\r\n[conda] numpy-base                1.26.4          py311hf175353_0\r\n[conda] optree                    0.12.1                   pypi_0    pypi\r\n[conda] pytorch                   2.4.0           py3.11_cuda12.1_cudnn9.1.0_0    pytorch\r\n[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch\r\n[conda] pytorch-mutex             1.0                        cuda    pytorch\r\n[conda] torchaudio                2.4.0               py311_cu121    pytorch\r\n[conda] torchelastic              0.2.2                    pypi_0    pypi\r\n[conda] torchtriton               3.0.0                     py311    pytorch\r\n[conda] torchvision               0.19.0              py311_cu121    pytorch\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PHB     0-15    0               N/A\r\nGPU1    PHB      X      0-15    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nThis is actually a follow up from #9152. After I tested every commit to build docker image using this Dockerfile\r\n\r\n```dockerfile\r\nFROM pytorch/pytorch:2.4.0-cuda12.1-cudnn9-devel AS build\r\n\r\nARG GITHASH\r\n\r\nRUN apt update && apt install gcc g++ git -y && apt clean && rm -rf /var/lib/apt/lists/*\r\n\r\nENV PATH=/workspace-lib:/workspace-lib/bin:$PATH\r\nENV PYTHONUSERBASE=/workspace-lib\r\n\r\nRUN pip install git+https://github.com/vllm-project/vllm.git@${GITHASH} --no-cache-dir --user\r\n\r\nFROM pytorch/pytorch:2.4.0-cuda12.1-cudnn9-runtime AS vllm-openai\r\n\r\nWORKDIR /vllm-workspace\r\n\r\nCOPY --from=build /workspace-lib /workspace-lib\r\n\r\nENV PATH=/workspace-lib:/workspace-lib/bin:$PATH\r\nENV PYTHONUSERBASE=/workspace-lib\r\nENV PYTHONPATH=/workspace-lib:/vllm-workspace\r\n\r\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\r\n```\r\n\r\nI found that this particular commit aeb37c2a725554791ff6f258b1e18830867a3ab9 is the culprit.\r\n\r\nIs there any way to solve this? I already update my Nvidia Driver to the latest but the problem persisted.\r\n\r\nCC: @LucasWilkinson (author of the mentioned commit)\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-10-10T05:47:12+00:00",
    "closed_at": "2024-10-11T19:57:40+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9226/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/9226"
  },
  {
    "number": 20098,
    "title": "[RFC]: Lazy CUDA Graph capture",
    "body": "### Motivation.\n\nCurrently vLLM captures cudagraphs as part of the engine initialization significantly slowing down vLLM startup time. By default, vLLM captures 66 graphs, which depending on model size and GPU type, can take more than 10s. This is not great UX (see #19824 for details).\n\nIn addition, It's most unlikely that all 66 graphs are actually needed, wasting both time and space.  \n\n### Proposed Change.\n\nWe propose to capture cudagraphs lazily. Instead of performing dummy runs during the engine initialization phase, the idea is to do those runs somewhere in the CUDA piecewise backend, and only for the current runtime shape if not cached already.\n\nExact implementation needs to be worked out.\n\n### Feedback Period.\n\none week\n\n### CC List.\n\n@ProExpertProg @aarnphm @charlesfrye  \n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "startup-ux"
    ],
    "state": "open",
    "created_at": "2025-06-25T21:27:51+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20098/reactions",
      "total_count": 4,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 4,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20098"
  },
  {
    "number": 5552,
    "title": "[RFC]: Refactor Worker and ModelRunner to consolidate control plane communication",
    "body": "### Motivation.\r\n\r\nCurrently, both the Worker and the ModelRunner classes contain multi-GPU control plane communication code, i.e. `broadcast_tensor_dict` calls. They look something like this:\r\n\r\n```python\r\nclass Worker:\r\n  def execute_model(self, execute_model_req=None):\r\n    # Do some broadcast here.\r\n    ...\r\n    return self.model_runner.execute_model(execute_model_req)\r\n\r\nclass ModelRunner:\r\n  def execute_model(self, execute_model_req=None):\r\n    # Do some more broadcast here.\r\n    ...\r\n    return model_executable(...)\r\n```\r\n\r\nBecause the ModelRunner class contains both model execution code and multi-GPU control plane communication code, it makes it difficult to improve upon the performance:\r\n- Cannot swap out the control plane mechanism, e.g., using NCCL vs CPU-based serialization to move the inputs from the LLMEngine to the Workers\r\n- Cannot switch to an SPMD design, where the rank 0 worker is moved off of the driver and executes the same code as the rest of the workers\r\n- Difficult to overlap GPU data movement with other compute on Workers, because these are all done ad-hoc in different ModelRunner implementations.\r\n- Difficult to optimize control plane performance, because the broadcast calls are scattered throughout the code.\r\n\r\n### Proposed Change.\r\n\r\nRefactor `Worker` and `ModelRunner` classes to consolidate all control plane communication to the `Worker` class. Both the `Worker` and `ModelRunner` classes should implement new worker-local methods to prepare inputs and execute the model. There should be no control plane communication within these methods; this could be enforced using runtime checks.\r\n\r\nHere is the new proposed interface. The `Worker` and `ModelRunner` create a `WorkerInput` and a `ModelInput` respectively from the `ExecuteModelRequest`. The contract is that `ExecuteModelRequest` contains CPU-only metadata, while any tensors in `WorkerInput` and `ModelInput` should already be on the correct device. Now, the ModelRunnerBase class looks approximately like this:\r\n\r\n```python\r\nclass ModelRunnerBase(ABC, Generic[T]):\r\n    \"\"\"\r\n    Model runner interface that abstracts a particular hardware and/or type of\r\n    model. Model execution may communicate data with model runners in other\r\n    processes, but it should not include control plane metadata communication.\r\n\r\n    Each ModelRunnerBase subclass should define a corresponding ModelInput\r\n    subclass.\r\n    \"\"\"\r\n    @abstractmethod\r\n    def prepare_model_input(\r\n        self,\r\n        seq_group_metadata_list: List[SequenceGroupMetadata],\r\n    ) -> T:\r\n        \"\"\"\r\n        Prepare the inputs to ModelRunnerBase.execute_model from an execution\r\n        request. This method may move data to the worker's local device. It is\r\n        not allowed to communicate with other workers or devices.\r\n        \"\"\"\r\n        raise NotImplementedError\r\n\r\n    @torch.inference_mode()\r\n    def execute_model(\r\n        self,\r\n        model_input: T,\r\n        kv_caches: Optional[List[torch.Tensor]],\r\n    ) -> Optional[SamplerOutput]:\r\n        \"\"\"\r\n        Execute the model on the given input.\r\n        \"\"\"\r\n        raise NotImplementedError\r\n```\r\nThis interface allows for cleaner separation between control plane communication vs. single-GPU logic. Each ModelRunner needs to explicitly state what inputs it requires by defining a ModelInput (subclass). This requires a bit more developer effort but should make it easier to introduce the optimizations discussed above.\r\n\r\nWe also add a new `LocalOrDistributedWorkerBase`. The idea behind this class is that as long as the developer implements this interface plus a ModelRunnerBase, they will get support out-of-the-box for both local and distributed execution. This class has a default implementation for `execute_model` that contains all of the control plane communication needed for distributed execution.\r\n```python\r\nclass LocalOrDistributedWorkerBase:\r\n    \"\"\"\r\n    Partial implementation of WorkerBase that has a default `execute_model`\r\n    definition to perform metadata transfer between workers when in distributed\r\n    mode. Subclasses of this interface should use model runners that inherit\r\n    from ModelRunnerBase, and should only need to implement worker-local logic.\r\n    If custom control plane logic is needed to transfer metadata, or if the\r\n    model runner cannot inherit from ModelRunnerBase, use WorkerBase instead.\r\n    \"\"\"\r\n    @abstractmethod\r\n    def prepare_worker_input(\r\n            self, execute_model_req: ExecuteModelRequest) -> WorkerInput:\r\n        \"\"\"\r\n        Prepare the inputs to WorkerBase.execute_worker from an execution\r\n        request. This method may move data to the worker's local device. It is\r\n        not allowed to communicate with other workers or devices.\r\n        \"\"\"\r\n        raise NotImplementedError\r\n\r\n    @abstractmethod\r\n    def execute_worker(self, worker_input: WorkerInput) -> None:\r\n        \"\"\"\r\n        Process an execution request.\r\n        \"\"\"\r\n        raise NotImplementedError\r\n\r\n    def execute_model(\r\n        self, execute_model_req: Optional[ExecuteModelRequest] = None\r\n    ) -> Optional[List[SamplerOutput]]:\r\n      ...\r\n```\r\n\r\n*Custom model runners*: For workers / model runners that need some custom logic, they can inherit directly from the generic `WorkerBase` and do not need to follow these interfaces. In that case, they are responsible for implementing their own control plane communication too.\r\n\r\n*Speculative decoding*: One complication is that the speculative decoding code goes back and forth between ExecuteModelRequest and ModelInput, whereas other workers only convert from ExecuteModelRequest to ModelInput. Thus, for the speculative decoding path, it's easier for now to keep the per-step broadcast. These extra k broadcasts could also be consolidated in the future, by either supporting ModelInput -> ExecuteModelRequest, or by making it possible to modify a ModelInput. Happily, the latter should be compatible with the solutions proposed in #5561.\r\n\r\n*Pipeline parallelism*: In pipeline parallelism, workers before the last PP rank will return some intermediate tensor(s) instead of a `SamplerOutput`. To support this case, we should define an `IntermediateOutput` type for models that support PP. Then, we extend `ModelRunnerBase.execute_model` to return a `Union[SamplerOutput, IntermediateOutput]` instead of just a `SamplerOutput`.\r\n\r\n### Feedback Period.\r\n\r\nOne week. See #5408 for code.\r\n\r\n### CC List.\r\n\r\n@youkaichao @zhuohan123 @zhisbug @cadedaniel @rkooo567 \r\n\r\n### Any Other Things.\r\n\r\nChecklist:\r\n- [ ] Merge #5408\r\n- [ ] Support speculative decoding\r\n- [ ] Not quite in scope for this RFC, but experiment with other control plane methods, such as Ray DAGs or faster IPC.",
    "labels": [
      "RFC"
    ],
    "state": "closed",
    "created_at": "2024-06-14T18:55:42+00:00",
    "closed_at": "2024-06-27T20:30:39+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5552/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/5552"
  },
  {
    "number": 15901,
    "title": "[SpecDecode] Support EAGLE in V1",
    "body": "- [x] 1. Correctly initializing and loading the EAGLE draft model\n- [x] 2. Consider the lookahead slots in the KV cache manager\n- [x] 3. Cache `draft_probs` inside the model runner and correctly feed it to the rejection sampler in the next step (temporarily workaround: #16899)\n- [x] 4. Handle the edge cases like when the draft model generates beyond `max_pos_embeddings`\n- [ ] 5. Handle the seeds correctly\n- [ ] 6. Do E2E correctness and performance tests\n- [x] 7. Support prefix caching. Eagle requires special handling because Eagle's i-th KV cache is coupled with the i+1-th token ID. (@LiuXiaoxuanPKU)\n- [ ] 8. Properly handle the sampling parameters that are not (currently) compatible with spec decoding (e.g., min_p).\n- [x] 9. Use CUDA graphs for draft model. (@luyuzhe111)\n- [x] 10. Support Eagle 3 (https://github.com/vllm-project/vllm/pull/16937)\n\n_Originally posted by @WoosukKwon in https://github.com/vllm-project/vllm/issues/15729#issuecomment-2765192455_\n            ",
    "labels": [
      "speculative-decoding",
      "v1"
    ],
    "state": "open",
    "created_at": "2025-04-01T19:45:13+00:00",
    "closed_at": null,
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15901/reactions",
      "total_count": 11,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 10,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15901"
  },
  {
    "number": 20468,
    "title": "[Feature]: Support EPLB for More MoE Models, e.g. Qwen 3, Llama 4",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\n\ud83c\udf89 **#18343 introduces dynamic Expert Parallelism Load Balancing (EPLB)** for DeepSeek-V2/V3/R1 models.\n\nAs MoE (Mixture-of-Experts) models become more common, we\u2019d love help extending EPLB support to other MoE models\u2014such as Qwen3, Llama 4, and more.\n\nThis is a great **first good issue** for anyone interested in model internals or systems work. #18343 was built with generality in mind, so extending it to other models or quantization methods should be relatively straightforward.\n\n---\n\n### \u2705 How to add support for a new model\n\nImplement the `MixtureOfExperts` protocol. Specifically, you\u2019ll need to:\n\n- Expose relevant MoE configuration flags.\n- Provide access to expert weights for EPLB to rearrange.\n- Forward EPLB-related arguments into the `FusedMoE` layer.\n\n\ud83d\udccc **Note on weight loading:**  \nFor models with **redundant experts**, you\u2019ll need to carefully adjust the weight loading logic. `FusedMoE` returns an `expert_params_mapping` that reflects expert duplication, but you may need to modify the model class to ensure correct loading behavior.\n\n\ud83d\udd0e Example: See how it\u2019s done in [`deepseek_v2.py`](https://github.com/vllm-project/vllm/pull/18343/files#diff-420f1cd67991a63cb419ca0e00e6f42cbe825864d0541e0662eeed2f9ddbd021).\n\n---\n\n### \u2705 How to add support for quantized models\n\nThis is usually even easier\u2014just make sure EPLB-related arguments are properly forwarded in your quantization path.\n\n\ud83d\udd0e Example: See [`fp8.py`](https://github.com/vllm-project/vllm/pull/18343/files#diff-5511bfcc9c53f7d96517ad43e4087f6777bef21302da983f42cafae40a866644) for a minimal working change.\n\n---\n\n\ud83d\udc4b **Want to contribute?**\n\nWe\u2019d love your help in extending EPLB support! Feel free to comment below or open a draft PR\u2014we\u2019re happy to guide you through the process.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-07-04T05:06:31+00:00",
    "closed_at": null,
    "comments": 20,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20468/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/20468"
  },
  {
    "number": 9778,
    "title": "[Feature]: Image-Modality Throughput Benchmark",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nThis is a subset of #8385. This issue is intended to track the effort of enabling throughput benchmark for image-modal models.\r\n\r\nThis is a reasonably large feature, and will span the work among multiple PRs.\n\n### Alternatives\n\nAd-hoc scripts for each model.\n\n### Additional context\n\nsee #8385 \n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-10-28T23:39:19+00:00",
    "closed_at": "2024-11-05T19:30:04+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9778/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/9778"
  },
  {
    "number": 7045,
    "title": "[RFC]: Deprecation and removal for `--engine-use-ray`",
    "body": "### Motivation.\n\nIn the `async_engine` code path, we have an option to launch the engine in a separate process using Ray\r\n\r\n```python\r\n        parser.add_argument('--engine-use-ray',\r\n                            action='store_true',\r\n                            help='Use Ray to start the LLM engine in a '\r\n                            'separate process as the server process\r\n```\r\n\r\nOriginally, the option make it possible to separate the server's Python overhead with the engine's main scheduler loop. \r\n\r\nHowever, few factors made this unused/less popular\r\n* Ray is an optional component, and typically not used in single node environment.\r\n* The serialization and rpc typically offset the theoretical performance gain\r\n* There are typically other ways to isolate server and engine (through multiprocessing, threading, etc).\r\n* Recently, we are separating this in server using lower overhead approaches #6883\n\n### Proposed Change.\n\nDeprecation of the flag with warning for one release. \r\nRemoval of the flag given no major pushbacks. \n\n### Feedback Period.\n\n1wk\n\n### CC List.\n\n_No response_\n\n### Any Other Things.\n\n_No response_",
    "labels": [
      "RFC"
    ],
    "state": "closed",
    "created_at": "2024-08-01T20:17:35+00:00",
    "closed_at": "2024-08-14T16:44:28+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7045/reactions",
      "total_count": 18,
      "+1": 18,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7045"
  },
  {
    "number": 14603,
    "title": "[Doc]: Does vllm CPU backend support Intel AMX?",
    "body": "### \ud83d\udcda The doc issue\n\nI see pr #4971 has integrated some optimizations into vllm CPU backend, and I want to know whether vllm CPU supports intel AMX, and how to use it.\nThank you.\n\n### Suggest a potential alternative/fix\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "open",
    "created_at": "2025-03-11T08:34:12+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14603/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14603"
  },
  {
    "number": 9180,
    "title": "[Installation]: Pytorch nightly version 2.6 meets error: error: can't copy '/tmp/tmpv5hlsgcm.build-lib/vllm/_core_C.abi3.so': doesn't exist or not a regular file",
    "body": "### Your current environment\n\n<details>\r\n\r\n<summary>click here to view the env</summary>\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 2.6.0.dev20241008+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: 16.0.1\r\nCMake version: version 3.26.0\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-196-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.6.68\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-SXM2-32GB\r\nGPU 1: Tesla V100-SXM2-32GB\r\nGPU 2: Tesla V100-SXM2-32GB\r\nGPU 3: Tesla V100-SXM2-32GB\r\n\r\nNvidia driver version: 560.35.03\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.4.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      40 bits physical, 48 bits virtual\r\nCPU(s):                             20\r\nOn-line CPU(s) list:                0-19\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 20\r\nSocket(s):                          1\r\nNUMA node(s):                       1\r\nVendor ID:                          GenuineIntel\r\nCPU family:                         6\r\nModel:                              85\r\nModel name:                         Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz\r\nStepping:                           7\r\nCPU MHz:                            2499.998\r\nBogoMIPS:                           4999.99\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          640 KiB\r\nL1i cache:                          640 KiB\r\nL2 cache:                           80 MiB\r\nL3 cache:                           16 MiB\r\nNUMA node0 CPU(s):                  0-19\r\nVulnerability Gather data sampling: Unknown: Dependent on hypervisor status\r\nVulnerability Itlb multihit:        KVM: Vulnerable\r\nVulnerability L1tf:                 Mitigation; PTE Inversion\r\nVulnerability Mds:                  Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Meltdown:             Mitigation; PTI\r\nVulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed:             Mitigation; IBRS\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; IBRS; IBPB conditional; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI SW loop, KVM SW loop\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch topoext cpuid_fault invpcid_single pti ssbd ibrs ibpb fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat umip pku ospke avx512_vnni\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] pytorch-triton==3.1.0+cf34004b8a\r\n[pip3] torch==2.6.0.dev20241008+cu124\r\n[pip3] torchaudio==2.5.0.dev20241008+cu124\r\n[pip3] torchvision==0.20.0.dev20241008+cu124\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] pytorch-triton            3.1.0+cf34004b8a          pypi_0    pypi\r\n[conda] torch                     2.6.0.dev20241008+cu124          pypi_0    pypi\r\n[conda] torchaudio                2.5.0.dev20241008+cu124          pypi_0    pypi\r\n[conda] torchvision               0.20.0.dev20241008+cu124          pypi_0    pypi\r\n\r\n```\r\n\r\n</details>\n\n### How you are installing vllm\n\nrelated to #8174\r\nI firstly use the following command to install nightly pytorch\r\n```sh\r\npip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu124\r\n```\r\nThen I follow the instructions here: https://docs.vllm.ai/en/latest/getting_started/installation.html#use-an-existing-pytorch-installation\r\nAfter I exec the command \r\n```sh\r\npip install -e . --no-build-isolation\r\n```\r\nlong time later, I got the following error log:\r\n```sh\r\n  Building editable for vllm (pyproject.toml) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Building editable for vllm (pyproject.toml) did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [290 lines of output]\r\n      running editable_wheel\r\n      creating /tmp/pip-wheel-vrx9vcb_/.tmp-3zex91m_/vllm.egg-info\r\n      writing /tmp/pip-wheel-vrx9vcb_/.tmp-3zex91m_/vllm.egg-info/PKG-INFO\r\n      writing dependency_links to /tmp/pip-wheel-vrx9vcb_/.tmp-3zex91m_/vllm.egg-info/dependency_links.txt\r\n      writing entry points to /tmp/pip-wheel-vrx9vcb_/.tmp-3zex91m_/vllm.egg-info/entry_points.txt\r\n      writing requirements to /tmp/pip-wheel-vrx9vcb_/.tmp-3zex91m_/vllm.egg-info/requires.txt\r\n      writing top-level names to /tmp/pip-wheel-vrx9vcb_/.tmp-3zex91m_/vllm.egg-info/top_level.txt\r\n      writing manifest file '/tmp/pip-wheel-vrx9vcb_/.tmp-3zex91m_/vllm.egg-info/SOURCES.txt'\r\n      reading manifest template 'MANIFEST.in'\r\n      adding license file 'LICENSE'\r\n      writing manifest file '/tmp/pip-wheel-vrx9vcb_/.tmp-3zex91m_/vllm.egg-info/SOURCES.txt'\r\n      creating '/tmp/pip-wheel-vrx9vcb_/.tmp-3zex91m_/vllm-0.6.3.dev138+g480b7f40.d20241009.cu126.dist-info'\r\n      creating /tmp/pip-wheel-vrx9vcb_/.tmp-3zex91m_/vllm-0.6.3.dev138+g480b7f40.d20241009.cu126.dist-info/WHEEL\r\n      running build_py\r\n      running build_ext\r\n      -- The CXX compiler identification is GNU 9.4.0\r\n      -- Detecting CXX compiler ABI info\r\n      -- Detecting CXX compiler ABI info - done\r\n      -- Check for working CXX compiler: /usr/bin/g++ - skipped\r\n      -- Detecting CXX compile features\r\n      -- Detecting CXX compile features - done\r\n      -- Build type: RelWithDebInfo\r\n      -- Target device: cuda\r\n      -- Found Python: /root/anaconda3/envs/llmfuzz/bin/python (found version \"3.12.7\") found components: Interpreter Development.Module Development.SABIModule\r\n      -- Found python matching: /root/anaconda3/envs/llmfuzz/bin/python.\r\n      -- Found CUDA: /usr/local/cuda-12.6 (found version \"12.6\")\r\n      -- The CUDA compiler identification is NVIDIA 12.6.68\r\n      -- Detecting CUDA compiler ABI info\r\n      -- Detecting CUDA compiler ABI info - done\r\n      -- Check for working CUDA compiler: /usr/local/cuda-12.6/bin/nvcc - skipped\r\n      -- Detecting CUDA compile features\r\n      -- Detecting CUDA compile features - done\r\n      -- Found CUDAToolkit: /usr/local/cuda-12.6/include (found version \"12.6.68\")\r\n      -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n      -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\r\n      -- Looking for pthread_create in pthreads\r\n      -- Looking for pthread_create in pthreads - not found\r\n      -- Looking for pthread_create in pthread\r\n      -- Looking for pthread_create in pthread - found\r\n      -- Found Threads: TRUE\r\n      -- Caffe2: CUDA detected: 12.6\r\n      -- Caffe2: CUDA nvcc is: /usr/local/cuda-12.6/bin/nvcc\r\n      -- Caffe2: CUDA toolkit directory: /usr/local/cuda-12.6\r\n      -- Caffe2: Header version is: 12.6\r\n      -- Found Python: /root/anaconda3/envs/llmfuzz/bin/python (found version \"3.12.7\") found components: Interpreter\r\n      CMake Warning at /root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/torch/share/cmake/Caffe2/public/cuda.cmake:140 (message):\r\n        Failed to compute shorthash for libnvrtc.so\r\n      Call Stack (most recent call first):\r\n        /root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\r\n        /root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\r\n        CMakeLists.txt:84 (find_package)\r\n      \r\n      \r\n      CMake Warning (dev) at /usr/local/cmake/share/cmake-3.26/Modules/FindPackageHandleStandardArgs.cmake:438 (message):\r\n        The package name passed to `find_package_handle_standard_args` (nvtx3) does\r\n        not match the name of the calling package (Caffe2).  This can lead to\r\n        problems in calling code that expects `find_package` result variables\r\n        (e.g., `_FOUND`) to follow a certain pattern.\r\n      Call Stack (most recent call first):\r\n        /root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/torch/share/cmake/Caffe2/public/cuda.cmake:174 (find_package_handle_standard_args)\r\n        /root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\r\n        /root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\r\n        CMakeLists.txt:84 (find_package)\r\n      This warning is for project developers.  Use -Wno-dev to suppress it.\r\n      \r\n      -- Could NOT find nvtx3 (missing: nvtx3_dir)\r\n      CMake Warning at /root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/torch/share/cmake/Caffe2/public/cuda.cmake:180 (message):\r\n        Cannot find NVTX3, find old NVTX instead\r\n      Call Stack (most recent call first):\r\n        /root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\r\n        /root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\r\n        CMakeLists.txt:84 (find_package)\r\n      \r\n      \r\n      -- USE_CUDNN is set to 0. Compiling without cuDNN support\r\n      -- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support\r\n      -- USE_CUDSS is set to 0. Compiling without cuDSS support\r\n      -- USE_CUFILE is set to 0. Compiling without cuFile support\r\n      -- Autodetected CUDA architecture(s):  7.0 7.0 7.0 7.0\r\n      -- Added CUDA NVCC flags for: -gencode;arch=compute_70,code=sm_70\r\n      CMake Warning at /root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\r\n        static library kineto_LIBRARY-NOTFOUND not found.\r\n      Call Stack (most recent call first):\r\n        /root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:120 (append_torchlib_if_found)\r\n        CMakeLists.txt:84 (find_package)\r\n      \r\n      \r\n      -- Found Torch: /root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/torch/lib/libtorch.so\r\n      -- Enabling core extension.\r\n      CMake Warning at CMakeLists.txt:125 (message):\r\n        Pytorch version 2.4.0 expected for CUDA build, saw 2.6.0 instead.\r\n      \r\n      \r\n      -- CUDA target architectures: 7.0\r\n      -- CUDA supported arches: 7.0;7.5;8.0;8.6;8.9;9.0\r\n      -- FetchContent base directory: /root/vllm/.deps\r\n      -- CMake Version: 3.26.0\r\n      -- CUTLASS 3.5.1\r\n      -- CUDART: /usr/local/cuda-12.6/lib64/libcudart.so\r\n      -- CUDA Driver: /usr/local/cuda-12.6/lib64/stubs/libcuda.so\r\n      -- NVRTC: /usr/local/cuda-12.6/lib64/libnvrtc.so\r\n      -- Default Install Location: install\r\n      -- Found Python3: /root/anaconda3/envs/llmfuzz/bin/python3.12 (found suitable version \"3.12.7\", minimum required is \"3.5\") found components: Interpreter\r\n      -- Make cute::tuple be the new standard-layout tuple type\r\n      -- CUDA Compilation Architectures: 70;72;75;80;86;87;89;90;90a\r\n      -- Enable caching of reference results in conv unit tests\r\n      -- Enable rigorous conv problem sizes in conv unit tests\r\n      -- Using NVCC flags: --expt-relaxed-constexpr;-DCUTE_USE_PACKED_TUPLE=1;-DCUTLASS_TEST_LEVEL=0;-DCUTLASS_TEST_ENABLE_CACHED_RESULTS=1;-DCUTLASS_CONV_UNIT_TEST_RIGOROUS_SIZE_ENABLED=1;-DCUTLASS_DEBUG_TRACE_LEVEL=0;-Xcompiler=-Wconversion;-Xcompiler=-fno-strict-aliasing;-lineinfo\r\n      -- Configuring cublas ...\r\n      -- cuBLAS Disabled.\r\n      -- Configuring cuBLAS ... done.\r\n      -- Not building Marlin kernels as no compatible archs foundin CUDA target architectures\r\n      -- Not building scaled_mm_c3x as no compatible archs found in CUDA target architectures\r\n      -- Not building scaled_mm_c2x as no compatible archs found in CUDA target architectures\r\n      -- Not building Machete kernels as no compatible archs found in CUDA target architectures\r\n      -- Enabling C extension.\r\n      -- Not building Marlin MOE kernels as no compatible archs foundin CUDA target architectures\r\n      -- Enabling moe extension.\r\n      -- Build type: RelWithDebInfo\r\n      -- Target device: cuda\r\n      -- Found Python: /root/anaconda3/envs/llmfuzz/bin/python (found version \"3.12.7\") found components: Interpreter Development.Module Development.SABIModule\r\n      -- Building vllm-flash-attn inside vLLM. Skipping flag detection and relying on parent build.\r\n      -- vllm-flash-attn is available at /root/vllm/.deps/vllm-flash-attn-src\r\n      -- Configuring done (22.2s)\r\n      -- Generating done (0.0s)\r\n      -- Build files have been written to: /tmp/tmpn7lp8_67.build-temp\r\n      [1/92] Building CXX object CMakeFiles/_core_C.dir/csrc/core/torch_bindings.cpp.o\r\n      [2/92] Linking CXX shared module _core_C.so\r\n      [3/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim192_bf16_causal_sm80.cu.o\r\n      [4/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim128_bf16_causal_sm80.cu.o\r\n      [5/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu.o\r\n      [6/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu.o\r\n      [7/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim160_bf16_causal_sm80.cu.o\r\n      [8/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim160_fp16_causal_sm80.cu.o\r\n      [9/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu.o\r\n      [10/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim192_fp16_causal_sm80.cu.o\r\n      [11/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu.o\r\n      [12/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim128_fp16_causal_sm80.cu.o\r\n      [13/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu.o\r\n      [14/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim224_bf16_causal_sm80.cu.o\r\n      [15/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu.o\r\n      [16/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu.o\r\n      [17/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim224_fp16_causal_sm80.cu.o\r\n      [18/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu.o\r\n      [19/92] Building CUDA object CMakeFiles/_C.dir/csrc/cuda_utils_kernels.cu.o\r\n      [20/92] Building CXX object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/flash_api.cpp.o\r\n      [21/92] Building CXX object CMakeFiles/_moe_C.dir/csrc/moe/torch_bindings.cpp.o\r\n      [22/92] Building CXX object CMakeFiles/_C.dir/csrc/torch_bindings.cpp.o\r\n      [23/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim256_bf16_causal_sm80.cu.o\r\n      [24/92] Building CUDA object CMakeFiles/_moe_C.dir/csrc/moe/topk_softmax_kernels.cu.o\r\n      [25/92] Building CUDA object CMakeFiles/_C.dir/csrc/pos_encoding_kernels.cu.o\r\n      [26/92] Linking CXX shared module _moe_C.so\r\n      [27/92] Building CUDA object CMakeFiles/_C.dir/csrc/moe_align_block_size_kernels.cu.o\r\n      [28/92] Building CUDA object CMakeFiles/_C.dir/csrc/activation_kernels.cu.o\r\n      [29/92] Building CUDA object CMakeFiles/_C.dir/csrc/quantization/compressed_tensors/int8_quant_kernels.cu.o\r\n      [30/92] Building CUDA object CMakeFiles/_C.dir/csrc/quantization/fp8/common.cu.o\r\n      [31/92] Building CUDA object CMakeFiles/_C.dir/csrc/quantization/aqlm/gemm_kernels.cu.o\r\n      [32/92] Building CUDA object CMakeFiles/_C.dir/csrc/prepare_inputs/advance_step.cu.o\r\n      [33/92] Building CUDA object CMakeFiles/_C.dir/csrc/mamba/causal_conv1d/causal_conv1d.cu.o\r\n      [34/92] Building CUDA object CMakeFiles/_C.dir/csrc/cache_kernels.cu.o\r\n      [35/92] Building CUDA object CMakeFiles/_C.dir/csrc/permute_cols.cu.o\r\n      [36/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu.o\r\n      [37/92] Building CUDA object CMakeFiles/_C.dir/csrc/layernorm_kernels.cu.o\r\n      [38/92] Building CUDA object CMakeFiles/_C.dir/csrc/quantization/awq/gemm_kernels.cu.o\r\n      [39/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim32_bf16_causal_sm80.cu.o\r\n      [40/92] Building CUDA object CMakeFiles/_C.dir/csrc/custom_all_reduce.cu.o\r\n      [41/92] Building CUDA object CMakeFiles/_C.dir/csrc/quantization/gguf/gguf_kernel.cu.o\r\n      [42/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu.o\r\n      [43/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim256_fp16_causal_sm80.cu.o\r\n      [44/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu.o\r\n      [45/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim32_fp16_causal_sm80.cu.o\r\n      [46/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu.o\r\n      [47/92] Building CUDA object CMakeFiles/_C.dir/csrc/quantization/gptq/q_gemm.cu.o\r\n      [48/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu.o\r\n      [49/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim64_bf16_causal_sm80.cu.o\r\n      [50/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim64_fp16_causal_sm80.cu.o\r\n      [51/92] Building CUDA object CMakeFiles/_C.dir/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu.o\r\n      [52/92] Building CUDA object CMakeFiles/_C.dir/csrc/mamba/mamba_ssm/selective_scan_fwd.cu.o\r\n      [53/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu.o\r\n      [54/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim96_bf16_causal_sm80.cu.o\r\n      [55/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu.o\r\n      [56/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu.o\r\n      [57/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_hdim96_fp16_causal_sm80.cu.o\r\n      [58/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim128_fp16_causal_sm80.cu.o\r\n      [59/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim160_bf16_causal_sm80.cu.o\r\n      [60/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim128_bf16_causal_sm80.cu.o\r\n      [61/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim128_fp16_sm80.cu.o\r\n      [62/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim128_bf16_sm80.cu.o\r\n      [63/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim160_bf16_sm80.cu.o\r\n      [64/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim160_fp16_causal_sm80.cu.o\r\n      [65/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim160_fp16_sm80.cu.o\r\n      [66/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim192_bf16_causal_sm80.cu.o\r\n      [67/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim192_fp16_causal_sm80.cu.o\r\n      [68/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim192_fp16_sm80.cu.o\r\n      [69/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim224_bf16_sm80.cu.o\r\n      [70/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim192_bf16_sm80.cu.o\r\n      [71/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim224_bf16_causal_sm80.cu.o\r\n      [72/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim224_fp16_causal_sm80.cu.o\r\n      [73/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim256_bf16_causal_sm80.cu.o\r\n      [74/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim224_fp16_sm80.cu.o\r\n      [75/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim256_fp16_causal_sm80.cu.o\r\n      [76/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim256_bf16_sm80.cu.o\r\n      [77/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim32_bf16_sm80.cu.o\r\n      [78/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim32_bf16_causal_sm80.cu.o\r\n      [79/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim256_fp16_sm80.cu.o\r\n      [80/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim32_fp16_causal_sm80.cu.o\r\n      [81/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim64_bf16_causal_sm80.cu.o\r\n      [82/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim64_bf16_sm80.cu.o\r\n      [83/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim32_fp16_sm80.cu.o\r\n      [84/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim64_fp16_causal_sm80.cu.o\r\n      [85/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim64_fp16_sm80.cu.o\r\n      [86/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim96_bf16_causal_sm80.cu.o\r\n      [87/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim96_bf16_sm80.cu.o\r\n      [88/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim96_fp16_causal_sm80.cu.o\r\n      [89/92] Building CUDA object /root/vllm/.deps/vllm-flash-attn-build/CMakeFiles/vllm_flash_attn_c.dir/csrc/flash_attn/src/flash_fwd_split_hdim96_fp16_sm80.cu.o\r\n      [90/92] Linking CXX shared module /root/vllm/.deps/vllm-flash-attn-build/vllm_flash_attn_c.so\r\n      [91/92] Building CUDA object CMakeFiles/_C.dir/csrc/attention/attention_kernels.cu.o\r\n      [92/92] Linking CXX shared module _C.so\r\n      -- Install configuration: \"RelWithDebInfo\"\r\n      -- Installing: /tmp/tmpv5hlsgcm.build-lib/vllm/_core_C.so\r\n      -- Set runtime path of \"/tmp/tmpv5hlsgcm.build-lib/vllm/_core_C.so\" to \"\"\r\n      -- Install configuration: \"RelWithDebInfo\"\r\n      -- Installing: /tmp/tmpv5hlsgcm.build-lib/vllm/_moe_C.so\r\n      -- Set runtime path of \"/tmp/tmpv5hlsgcm.build-lib/vllm/_moe_C.so\" to \"\"\r\n      -- Install configuration: \"RelWithDebInfo\"\r\n      -- Installing: /tmp/tmpv5hlsgcm.build-lib/vllm/vllm_flash_attn/vllm_flash_attn_c.so\r\n      -- Set runtime path of \"/tmp/tmpv5hlsgcm.build-lib/vllm/vllm_flash_attn/vllm_flash_attn_c.so\" to \"\"\r\n      -- Up-to-date: /tmp/tmpv5hlsgcm.build-lib/vllm/vllm_flash_attn\r\n      -- Installing: /tmp/tmpv5hlsgcm.build-lib/vllm/vllm_flash_attn/__init__.py\r\n      -- Installing: /tmp/tmpv5hlsgcm.build-lib/vllm/vllm_flash_attn/flash_attn_interface.py\r\n      -- Install configuration: \"RelWithDebInfo\"\r\n      -- Installing: /tmp/tmpv5hlsgcm.build-lib/vllm/_C.so\r\n      -- Set runtime path of \"/tmp/tmpv5hlsgcm.build-lib/vllm/_C.so\" to \"\"\r\n      Traceback (most recent call last):\r\n        File \"/root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 138, in run\r\n          self._create_wheel_file(bdist_wheel)\r\n        File \"/root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 341, in _create_wheel_file\r\n          files, mapping = self._run_build_commands(dist_name, unpacked, lib, tmp)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 264, in _run_build_commands\r\n          self._run_build_subcommands()\r\n        File \"/root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 291, in _run_build_subcommands\r\n          self.run_command(name)\r\n        File \"/root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\r\n          self.distribution.run_command(command)\r\n        File \"/root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/setuptools/dist.py\", line 950, in run_command\r\n          super().run_command(command)\r\n        File \"/root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\r\n          cmd_obj.run()\r\n        File \"<string>\", line 235, in run\r\n        File \"/root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/setuptools/command/build_ext.py\", line 101, in run\r\n          self.copy_extensions_to_source()\r\n        File \"/root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/setuptools/command/build_ext.py\", line 122, in copy_extensions_to_source\r\n          self.copy_file(regular_file, inplace_file, level=self.verbose)\r\n        File \"/root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 354, in copy_file\r\n          return file_util.copy_file(\r\n                 ^^^^^^^^^^^^^^^^^^^^\r\n        File \"/root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/setuptools/_distutils/file_util.py\", line 108, in copy_file\r\n          raise DistutilsFileError(\r\n      distutils.errors.DistutilsFileError: can't copy '/tmp/tmpv5hlsgcm.build-lib/vllm/_core_C.abi3.so': doesn't exist or not a regular file\r\n      /root/anaconda3/envs/llmfuzz/lib/python3.12/site-packages/setuptools/_distutils/dist.py:973: _DebuggingTips: Problem in editable installation.\r\n      !!\r\n      \r\n              ********************************************************************************\r\n              An error happened while installing `vllm` in editable mode.\r\n      \r\n              The following steps are recommended to help debug this problem:\r\n      \r\n              - Try to install the project normally, without using the editable mode.\r\n                Does the error still persist?\r\n                (If it does, try fixing the problem before attempting the editable mode).\r\n              - If you are using binary extensions, make sure you have all OS-level\r\n                dependencies installed (e.g. compilers, toolchains, binary libraries, ...).\r\n              - Try the latest version of setuptools (maybe the error was already fixed).\r\n              - If you (or your project dependencies) are using any setuptools extension\r\n                or customization, make sure they support the editable mode.\r\n      \r\n              After following the steps above, if the problem still persists and\r\n              you think this is related to how setuptools handles editable installations,\r\n              please submit a reproducible example\r\n              (see https://stackoverflow.com/help/minimal-reproducible-example) to:\r\n      \r\n                  https://github.com/pypa/setuptools/issues\r\n      \r\n              See https://setuptools.pypa.io/en/latest/userguide/development_mode.html for details.\r\n              ********************************************************************************\r\n      \r\n      !!\r\n        cmd_obj.run()\r\n      error: can't copy '/tmp/tmpv5hlsgcm.build-lib/vllm/_core_C.abi3.so': doesn't exist or not a regular file\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building editable for vllm\r\nFailed to build vllm\r\nERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (vllm)\r\n\r\n```\r\n\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2024-10-09T05:23:31+00:00",
    "closed_at": "2024-11-11T01:48:06+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9180/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/9180"
  },
  {
    "number": 11978,
    "title": "[Bug]: The usage of .transpose() and .view() consecutively is not recommended.",
    "body": "### Your current environment\n\n<details>\r\n<summary>Error information (Sorry, I cannot disclose more due to confidentiality reasons)</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.1.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: Could not collect\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (aarch64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.31.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-118-generic-aarch64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: \r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nVersions of relevant libraries:\r\n[pip3] mindietorch==1.0.0+torch2.1.0.abi0\r\n[pip3] numpy==1.26.4\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.1.0\r\n[pip3] torch-npu==2.1.0.post10.dev20241217\r\n[pip3] torchvision==0.16.0\r\n[pip3] transformers==4.46.1\r\n[pip3] tritonclient==2.49.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.3.dev0+g7193774b.d20250103\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n\r\nPYTORCH_INSTALL_PATH=/usr/local/lib/python3.10/dist-packages/torch\r\nPYTORCH_NPU_INSTALL_PATH=/usr/local/lib/python3.10/dist-packages/torch_npu\r\n```\r\n\r\n</details>\\\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\n### Description\r\n\r\nWe encountered the following error while running `internvl`:\r\n\r\n```\r\nview size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\r\n```\r\n\r\nAfter careful analysis, we found that this was a logic bug that had nothing to do with the device we were using.\r\n\r\nThe issue occurs in the following line of code in `vllm/vllm/model_executor/models/intern_vit.py`:\r\n\r\n```python\r\nclass InternSdpaAttention(nn.Module):\r\n  def forward(self, x):\r\n      B, N, C = x.shape\r\n      qkv = self.qkv(x)\r\n      q, k, v = qkv.chunk(3, dim=-1)\r\n  \r\n      q = q.view(B, N, self.num_heads, self.head_dim)\r\n      k = k.view(B, N, self.num_heads, self.head_dim)\r\n      v = v.view(B, N, self.num_heads, self.head_dim)\r\n  \r\n      if self.qk_normalization:\r\n          B_, N_, H_, D_ = q.shape\r\n          q = self.q_norm.forward_native(q.flatten(-2, -1)).view(B_, N_, H_, D_)\r\n          k = self.k_norm.forward_native(k.flatten(-2, -1)).view(B_, N_, H_, D_)\r\n      q = q.transpose(1, 2)\r\n      k = k.transpose(1, 2)\r\n      v = v.transpose(1, 2)\r\n  \r\n      x = F.scaled_dot_product_attention(q, k, v, scale=self.scale)\r\n      x = x.transpose(1, 2).view(B, N, -1)\r\n```\r\n\r\n### Analysis\r\n\r\nAt the beginning of this file, there is a check to determine whether `xformers` is available:\r\n\r\n```python\r\ntry:\r\n    from xformers import ops as xops\r\n    USE_XFORMERS_OPS = True\r\nexcept ImportError:\r\n    USE_XFORMERS_OPS = False\r\n```\r\n\r\nThe execution brunch is different depending on the value of `USE_XFORMERS_OPS`. The error in question occurs only when `USE_XFORMERS_OPS = False`, making it hard to reproduce under normal circumstances (e.g. use Nvidia GPU in most cases).\r\n\r\n**Why the usage of .transpose() and .view() consecutively is not recommended:** In the affected code, the output after the attention operation is a 4D tensor, which we will call `A`. If `A` is a strided tensor, according to the [PyTorch documentation for `transpose`](https://pytorch.org/docs/stable/generated/torch.Tensor.transpose.html), the `.transpose` operation only creates a [view](https://pytorch.org/docs/stable/tensor_view.html) of `A`, modifying its strides. Meanwhile, the [documentation for `.view`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view) specifies that the new view must be compatible with the original size and stride. This means that the new view dimensions must be a subspace of the original dimensions, or the input must satisfy the formula mentioned in the docs. Since the `.transpose` operation changes the strides, the view created by `.view` may not satisfy these conditions and lead to the error. The `.view` documentation also suggests using `.contiguous()` before `.view` to ensure that the tensor is contiguous in memory.\r\n\r\n### Solution\r\n\r\nThe issue arises from the fact that the tensor `A` is strided, and the strides of the tensor after `.transpose` likely don't meet the conditions for `.view` to work correctly. To resolve this issue, we recommend adding a `.contiguous()` call before the `.view` operation to ensure that the tensor is contiguous in memory.\r\n\r\n**P.S.** We also found that .transpose().view() is only valid if:\r\n\r\n1. `A` is a sparse tensor instead of a strided tensor.\r\n2. The stride after `transpose` happens to meet the compatibility conditions for `.view`.\r\n3. PyTorch may have undocumented methods for handling these situations, such as not requiring `.contiguous()` when the changes made by `.view` and `.transpose` are mirrored.\r\n\r\n### Related Issues\r\n\r\nThis issue is similar to the one mentioned in issue #8630, but the key to preventing the problem from reproducing is not found, namely the value of `USE_XFORMERS_OPS`. It was addressed in PR #8880. However, since PR #9560 was believed to have solved this problem, PR #8880 was closed. We found that this issue was not actually resolved in PR #9560, so we are submitting this new issue with our analysis and an explanation based on the official PyTorch documentation.\r\n\r\n### Reproducible Example\r\n\r\nHere is a simple code snippet to verify the issue:\r\n\r\n```python\r\nimport torch\r\n\r\nx = torch.randn(2, 3, 4, 5)\r\ny = x.transpose(1, 2)\r\n# y = y.contiguous()\r\n\r\ntry:\r\n    z = y.view(2, 4, -1)\r\n    print(z.size())\r\nexcept Exception as e:\r\n    print(\"Error:\", e)\r\n```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-01-13T01:41:52+00:00",
    "closed_at": "2025-01-13T06:24:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11978/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11978"
  },
  {
    "number": 9177,
    "title": "[Bug]: quantization does not work with dummy weight format",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nfound in ci, https://buildkite.com/vllm/ci-aws/builds/9653#01926d8d-2b87-4c7e-b5dd-bf56514236f0 , test_cpu_offload_gptq and test_cpu_offload_compressed_tensors do not work with dummy format.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-09T03:20:48+00:00",
    "closed_at": "2025-02-07T01:59:37+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9177/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/9177"
  },
  {
    "number": 612,
    "title": "Multi-GPU error",
    "body": "While using tensor_parallel_size argument to load the vllm model, I was facing the issue in #557 stating something related to network address retrieval. According to [this comment](https://github.com/vllm-project/vllm/issues/570#issuecomment-1650973012) on #570 I trying building vllm from source and running it then.\r\n\r\nThe error goes away, but it does not load the model and gets stuck on that cell.\r\n\r\nIs there any way I can get things working on multiple GPUs? I am able to run llama-2-7b, but in order to run llama-2-13b, I'll need to run it on multiple GPUs.",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2023-07-28T09:48:12+00:00",
    "closed_at": "2023-08-07T22:44:40+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/612/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/612"
  },
  {
    "number": 13058,
    "title": "[V1][Help Wanted] Porting missing sampling parameters to V1",
    "body": "### Anything you want to discuss about vllm.\n\nTo switch the engine from V0 to V1, we need to comprehensively support the sampling parameters in https://github.com/vllm-project/vllm/blob/main/vllm/sampling_params.py\n\nWhile most of the key parameters are already supported, some of them are missing:\n\nTODO (help wanted):\n- [x] `n` (parallel sampling) #10980  @afeldman-nm \n- [x] `guided_decoding` (structured decoding) #12388  @aarnphm \n- [x] `logit_bias` #13079 @houseroad \n- [x] `min_p` #13191 @AoyuQC\n- [ ] `bad_words` (originally implemented via logits processor) #13376 @22quinn \n- [x] `allowed_token_ids` (originally implemented via logits processor) #13210 @houseroad \n\nParameters that will not be supported in V1:\n* best_of\n* logits_processors\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "misc",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-02-10T23:13:42+00:00",
    "closed_at": "2025-03-20T14:15:16+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13058/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/13058"
  },
  {
    "number": 8204,
    "title": "[Bug]: [Errno 98] error while attempting to bind on address ('0.0.0.0', 8000): address already in use",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nThis is a bug we encounter a lot in our ci, e.g. https://buildkite.com/vllm/ci-aws/builds/8098#0191bf43-446d-411d-80c7-3ba10bc392e8/192-1557\r\n\r\nI have been tracking this for months, and try to add more logging information to help debugging.\r\n\r\nfrom the logging information:\r\n\r\n\r\n> [2024-09-05T00:38:34Z] INFO:     Started server process [60858]\r\n> --\r\n> \u00a0 | [2024-09-05T00:38:34Z] INFO:     Waiting for application startup.\r\n> \u00a0 | [2024-09-05T00:38:34Z] INFO:     Application startup complete.\r\n> \u00a0 | [2024-09-05T00:38:34Z] ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 44319): [errno 98] address already in use\r\n> \u00a0 | [2024-09-05T00:38:34Z] INFO:     Waiting for application shutdown.\r\n> \u00a0 | [2024-09-05T00:38:34Z] INFO:     Application shutdown complete.\r\n> \u00a0 | [2024-09-05T00:38:34Z] DEBUG 09-04 17:38:34 launcher.py:64] port 44319 is used by process psutil.Process(pid=60914, name='pt_main_thread', status='sleeping', started='17:37:05') launched with command:\r\n> \u00a0 | [2024-09-05T00:38:34Z] DEBUG 09-04 17:38:34 launcher.py:64] /usr/bin/python3 -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=16, pipe_handle=18) --multiprocessing-fork\r\n> \r\n> \r\n\r\nwe can see that the server process is pid 60858 , and the port 44319 is used by process 60914. scrolling up a little bit, we can find:\r\n\r\n\r\n> [2024-09-05T00:37:05Z] INFO 09-04 17:37:05 api_server.py:160] Multiprocessing frontend to use ipc:///tmp/b6851f4d-4d78-46b8-baba-ae179b0088c2 for RPC Path.\r\n> --\r\n> \u00a0 | [2024-09-05T00:37:05Z] INFO 09-04 17:37:05 api_server.py:176] Started engine process with PID 60914\r\n> \r\n\r\nit becomes clear that this is the engine process.\r\n\r\nI think the problem here, is that we only bind the port after the engine is ready. During engine setup, it might use some ports for ray, or for distributed communication.\r\n\r\nthere are two possible solutions:\r\n1. the api server immediately binds to the port after start, and returns unready status when client queries the `/healthy` endpoint\r\n2. the api server binds the port immediately (via `socket.socket(socket.AF_INET, socket.SOCK_STREAM).bind((\"\", uvicorn_kwargs[\"port\"]))`), and after engine is up, it releases the port, and bind again to serve requests\r\n\r\nI think 1 might be better. 2 would suffer from the fact that client will get 404 not found before the engine is up, because this is just a raw socket without any response.\r\n\r\ncc @robertgshaw2-neuralmagic @njhill @joerunde \r\n\r\nalso cc @richardliaw @rkooo567 how to turn on verbose ray logging, so that we can verify if the port is indeed used by ray.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-09-05T17:35:19+00:00",
    "closed_at": "2024-09-16T20:56:29+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8204/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/8204"
  },
  {
    "number": 20708,
    "title": "[RFC]: Continue on Device agnostic API abstraction to current_platform.XXX",
    "body": "co-author with @jikunshang \n\n### Motivation.\n\nThis RFC is aiming to reuse `GPUWorker` and `GPUModelRunner` for any GPGPU devices, such as CUDA, ROCM and Intel GPU(aka: XPU).\n- By doing so, we can remove redundant duplication by adding a new XXXWorker/XXXModelRunner and derive from GPUWorker/GPUModelRunner\n- Any feature implemented in GPUWorker/GPUModelRunner such as logitsProcessor, samplingOutput optimization, spec_decode can be shared to all GPGPU hardware.\n\n**Status & Challenge**\n\n- Previous RFC from Huawei has made significant work done through - https://github.com/vllm-project/vllm/issues/9268\n\n- Currently, `GPUWorker` and `GPUModelRunner` is assumed that it will only be used by CUDA and RocM, so hard-coded to cuda API will be used in above two files. Ex:torch.cuda.XXX or tensor.to('cuda') \n\n### Proposed Change.\n\n1. Add abstract API into platforms/interface.py and implement in cuda.py, rocm.py, xpu.py.\n2. update any tensor.to('cuda') or tensor.cuda() to use tensor.to(current_platform.device).\n3. Add a skip check in case of API mismatch.\n4. Add static check to PR pre_commit to indicate future contributor to use current_platform instead calling torch.cuda directly.\n\n<img width=\"900\" height=\"974\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fa3f8900-04e3-4912-9e2d-8539f7b8116f\" />\n\n**Plan**\n1. Add abstract API\n\n- [ ] `torch.cuda.empty_cache` -> `current_platform.empty_cache`\n- [ ] `torch.cuda.set_device` -> `current_platform.set_device`\n- [ ] `torch.cuda.reset_peak_memory_stats` -> `current_platform.reset_peak_memory_stats`\n- [ ] `torch.cuda.mem_get_info` -> `current_platform.mem_get_info`\n- [ ] `torch.cuda.memory_stats` -> `current_platform.memory_stats`\n- [ ] `torch.cuda.memory_reserved` -> `current_platform.memory_reserved`\n- [ ] `torch.cuda.synchronize` -> `current_platform.synchronize`\n\n2. abstract device / dist_device\n- [ ] `tensor.to('cuda')` -> `tensor.to(current_platform.device)`\n- [x] use `current_platform.dist_backend` for `init_worker_distributed_environment`. Done in #19410 \n\n3. add skip check\n- [ ] add `current_platform.is_graph_mode_supported()`\n- TBD\n\n4. Add static check\n- TBD\n\n** PR list **\n#19410 => Merged\nhttps://github.com/vllm-project/vllm/pull/20751 => Open\n\n\n** additional **\n\ntorch APIs support list in torch.cuda and torch.xpu - By Q2 25\n\nno.|api | cuda | xpu \n-- | -- | -- | --\n1 | current_device | \u2705| \u2705\n2 | current_stream | \u2705| \u2705\n3 | device | \u2705| \u2705\n4 | device_count | \u2705| \u2705\n5 | get_device_capability | \u2705| \u2705\n6 | get_device_name | \u2705| \u2705\n7 | get_device_properties | \u2705| \u2705\n8 | init | \u2705| \u2705\n9 | is_available | \u2705| \u2705\n10 | is_initialized | \u2705| \u2705\n11 | set_device | \u2705| \u2705\n12 | set_stream | \u2705| \u2705\n13 | stream | \u2705| \u2705\n14 | synchronize | \u2705| \u2705\n15 | manual_seed | \u2705| \u2705\n16 | manual_seed_all | \u2705| \u2705\n17 | Stream | \u2705| \u2705\n18 | Event | \u2705| \u2705\n19 | empty_cache | \u2705| \u2705\n20 | mem_get_info | \u2705| \u2705\n21 | memory_stats, memory_stats_as_nested_dict | \u2705| \u2705\n22 | memory_allocated | \u2705| \u2705\n23 | max_memory_allocated| \u2705| \u2705\n24 | memory_reserved| \u2705| \u2705\n25 | reset_peak_memory_stats| \u2705| \u2705\n\nNot matched APIs\nno.|api|  cuda | xpu \n-- | --  | -- | --\n1 | Cudart | \u2705| enable with alternative API\n2 | is_current_stream_capturing | \u2705 | enable with alternative API\n3 | graph_pool_handle  | \u2705| check and skip\n4 | CUDAGraph| \u2705 | check and skip\n5 | Graph | \u2705| check and skip \n6 | CUDAPluggableAllocator| \u2705| check and skip\n7 | nvtx.range_push | \u2705| check and skip\n8 | nvtx.range_pop | \u2705| check and skip\n9 | nvtx.range | \u2705| check and skip\n10 | _lazy_init| \u2705 | enable with alternative API\n11 | _is_compiled | \u2705| enable with alternative API\n12 | _device_count_amdsmi| \u2705 | enable with alternative API\n13 | _device_count_nvml | \u2705| enable with alternative API\n14 | tunnable | \u2705| enable with alternative API\n\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n@jikunshang @simon-mo @WoosukKwon @youkaichao @gshtras \n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "open",
    "created_at": "2025-07-09T21:00:11+00:00",
    "closed_at": null,
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20708/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20708"
  },
  {
    "number": 14181,
    "title": "[Bug][V1]: Kernel crashed when running  qwen2.5_vl model",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI'm using the vllm 0.7.3 (enable_vllm_v1) to run qwen2_5_vl model. \n\n```shell\nInitializing a V1 LLM engine (v0.7.3) with config: model='/qwen2_5-vl-72b', speculative_config=None, tokenizer='/qwen2_5-vl-72b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32000, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/qwen2_5-vl-72b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}\n```\n\nThen I run it with a trace. It works fine for a while (maybe around 100 requests succeed), but after that, the cuda kernel crashed. It's reproducible when re-run the same trace. \n```\n../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [0,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [1,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374] WorkerProc hit an exception: %s\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374] Traceback (most recent call last):\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]   File \"/usr/local/python-3.10.14/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 370, in worker_busy_loop\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]     output = func(*args, **kwargs)\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]   File \"/usr/local/python-3.10.14/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]     return func(*args, **kwargs)\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]   File \"/usr/local/python-3.10.14/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 227, in execute_model\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]     output = self.model_runner.execute_model(scheduler_output)\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]   File \"/usr/local/python-3.10.14/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]     return func(*args, **kwargs)\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]   File \"/usr/local/python-3.10.14/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 957, in execute_model\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]     sampler_output = self.model.sample(\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]   File \"/usr/local/python-3.10.14/lib/python3.10/site-packages/vllm/model_executor/models/qwen2_5_vl.py\", line 1091, in sample\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]     return self.language_model.sample(logits, sampling_metadata)\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]   File \"/usr/local/python-3.10.14/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 505, in sample\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]     next_tokens = self.sampler(logits, sampling_metadata)\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]   File \"/usr/local/python-3.10.14/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]     return self._call_impl(*args, **kwargs)\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]   File \"/usr/local/python-3.10.14/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]     return forward_call(*args, **kwargs)\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]   File \"/usr/local/python-3.10.14/lib/python3.10/site-packages/vllm/v1/sample/sampler.py\", line 55, in forward\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]     sampled = self.sample(logits, sampling_metadata)\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]   File \"/usr/local/python-3.10.14/lib/python3.10/site-packages/vllm/v1/sample/sampler.py\", line 111, in sample\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]     random_sampled = self.topk_topp_sampler(\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]   File \"/usr/local/python-3.10.14/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]     return self._call_impl(*args, **kwargs)\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]   File \"/usr/local/python-3.10.14/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]     return forward_call(*args, **kwargs)\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]   File \"/usr/local/python-3.10.14/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 62, in forward_native\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]     logits = apply_top_k_top_p(logits, k, p)\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]   File \"/usr/local/python-3.10.14/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 100, in apply_top_k_top_p\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374]     top_k_mask = logits_sort.gather(1, top_k_mask.unsqueeze(dim=1))\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374] RuntimeError: CUDA error: device-side assert triggered\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n(VllmWorker rank=3 pid=271990) ERROR 03-04 01:21:00 multiproc_executor.py:374] \n../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [0,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [1,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [0,0,0], thread: [0,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of \nbounds\"../aten/src/ATen/native/cuda/ScatterGatherKernel.cu` failed.\n:144../aten/src/ATen/native/cuda/ScatterGatherKernel.cu: operator():144: block: [0: operator(),0: block: [0,0,0], thread: [0,0,0], thread: [1,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\",0` failed.\n] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"../aten/src/ATen/native/cuda/ScatterGatherKernel.cu` failed.\n:144: operator(): block: [0,0,0], thread: [1,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\nException raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f551c56c446 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f551c5166e4 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f551d0a5a18 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7f54cdfc7726 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0xa0 (0x7f54cdfcc3f0 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7f54cdfd3b5a in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f54cdfd561d in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x145c0 (0x7f551d4c95c0 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch.so)\nframe #8: <unknown function> + 0x93fb (0x7f551ebd63fb in /usr/lib64/libpthread.so.0)\nframe #9: clone + 0x43 (0x7f551e51be83 in /usr/lib64/libc.so.6)\n\nterminate called after throwing an instance of 'c10::DistBackendError'\n[rank2]:[E304 01:21:01.867394363 ProcessGroupNCCL.cpp:1595] [PG ID 2 PG GUID 3 Rank 2] Process group watchdog thread terminated with exception: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nException raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f9272b6c446 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f9272b166e4 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f92732a5a18 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7f92241c7726 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0xa0 (0x7f92241cc3f0 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7f92241d3b5a in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f92241d561d in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x145c0 (0x7f9274e225c0 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch.so)\nframe #8: <unknown function> + 0x93fb (0x7f9274ebc3fb in /usr/lib64/libpthread.so.0)\nframe #9: clone + 0x43 (0x7f927471be83 in /usr/lib64/libc.so.6)\n\nterminate called after throwing an instance of 'c10::DistBackendError'\n[rank3]:[E304 01:21:01.867443735 ProcessGroupNCCL.cpp:1595] [PG ID 2 PG GUID 3 Rank 3] Process group watchdog thread terminated with exception: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nException raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f782c96c446 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f782c9166e4 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f782d4a5a18 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7f77de3c7726 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0xa0 (0x7f77de3cc3f0 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7f77de3d3b5a in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f77de3d561d in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x145c0 (0x7f782d86f5c0 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch.so)\nframe #8: <unknown function> + 0x93fb (0x7f782ebbb3fb in /usr/lib64/libpthread.so.0)\nframe #9: clone + 0x43 (0x7f782e8c1e83 in /usr/lib64/libc.so.6)\n\nterminate called after throwing an instance of 'c10::DistBackendError'\n  what():  [PG ID 2 PG GUID 3 Rank 0] Process group watchdog thread terminated with exception: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nException raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f551c56c446 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f551c5166e4 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f551d0a5a18 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7f54cdfc7726 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0xa0 (0x7f54cdfcc3f0 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7f54cdfd3b5a in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f54cdfd561d in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x145c0 (0x7f551d4c95c0 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch.so)\nframe #8: <unknown function> + 0x93fb (0x7f551ebd63fb in /usr/lib64/libpthread.so.0)\nframe #9: clone + 0x43 (0x7f551e51be83 in /usr/lib64/libc.so.6)\n\nException raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f551c56c446 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0xe4271b (0x7f54cdc4271b in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #2: <unknown function> + 0x145c0 (0x7f551d4c95c0 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch.so)\nframe #3: <unknown function> + 0x93fb (0x7f551ebd63fb in /usr/lib64/libpthread.so.0)\nframe #4: clone + 0x43 (0x7f551e51be83 in /usr/lib64/libc.so.6)\n\n  what():  [PG ID 2 PG GUID 3 Rank 2] Process group watchdog thread terminated with exception: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nException raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f9272b6c446 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f9272b166e4 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f92732a5a18 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7f92241c7726 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0xa0 (0x7f92241cc3f0 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7f92241d3b5a in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f92241d561d in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x145c0 (0x7f9274e225c0 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch.so)\nframe #8: <unknown function> + 0x93fb (0x7f9274ebc3fb in /usr/lib64/libpthread.so.0)\nframe #9: clone + 0x43 (0x7f927471be83 in /usr/lib64/libc.so.6)\n\nException raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f9272b6c446 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0xe4271b (0x7f9223e4271b in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #2: <unknown function> + 0x145c0 (0x7f9274e225c0 in /usr/local/python-3.10.14/lib/python3.10/site-packages/torch/lib/libtorch.so)\nframe #3: <unknown function> + 0x93fb (0x7f9274ebc3fb in /usr/lib64/libpthread.so.0)\nframe #4: clone + 0x43 (0x7f927471be83 in /usr/lib64/libc.so.6)\n\n  what():  [PG ID 2 PG GUID 3 Rank 3] Process group watchdog thread terminated with exception: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-04T04:49:32+00:00",
    "closed_at": "2025-03-06T03:57:20+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14181/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14181"
  },
  {
    "number": 537,
    "title": "Can't launch OpenAI API server on newly installed vLLM in Docker - fastchat not found",
    "body": "Hi\r\n\r\nI have a Docker container that I created for vLLM. I built it a few days ago and it worked fine.  Today I rebuilt it to get the latest code changes, and now it's failing to launch the OpenAI server.  SSHing in to the docker and running the launch command directly shows the following error:\r\n\r\n```\r\nvllm@36b7089a5957:~/vllm (main \u2714) \u1405 python -m vllm.entrypoints.openai.api_server --model facebook/opt-125m\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/vllm/vllm/vllm/entrypoints/openai/api_server.py\", line 17, in <module>\r\n    from fastchat.model.model_adapter import get_conversation_template\r\nModuleNotFoundError: No module named 'fastchat.model.model_adapter'\r\n```\r\n\r\nHowever I can launch the non-API server fine:\r\n```\r\nvllm@36b7089a5957:~/vllm (main \u2714) \u1405 python -m vllm.entrypoints.api_server\r\nDownloading (\u2026)lve/main/config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 651/651 [00:00<00:00, 1.57MB/s]\r\nINFO 07-20 21:26:11 llm_engine.py:67] Initializing an LLM engine with config: model='facebook/opt-125m', tokenizer='facebook/opt-125m', tokenizer_mode=auto, trust_remote_code=False, dtype=torch.float16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)\r\nDownloading (\u2026)okenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 685/685 [00:00<00:00, 1.83MB/s]\r\n...\r\n```\r\n\r\n<details>\r\n  <summary>Output of `pip freeze`</summary>\r\n\r\n```\r\naccelerate==0.21.0\r\naiofiles==23.1.0\r\naiohttp==3.8.5\r\naiosignal==1.3.1\r\naltair==5.0.1\r\nanyio==3.7.1\r\nappdirs==1.4.4\r\nasync-timeout==4.0.2\r\nattrs==23.1.0\r\ncertifi==2023.5.7\r\ncharset-normalizer==3.2.0\r\nclick==8.1.6\r\ncmake==3.27.0\r\ncontourpy==1.1.0\r\ncycler==0.11.0\r\ndocker-pycreds==0.4.0\r\nexceptiongroup==1.1.2\r\nfastapi==0.100.0\r\nffmpy==0.3.1\r\nfilelock==3.12.2\r\nfonttools==4.41.0\r\nfrozenlist==1.4.0\r\nfschat==0.2.3\r\nfsspec==2023.6.0\r\ngitdb==4.0.10\r\nGitPython==3.1.32\r\ngradio==3.23.0\r\ngrpcio==1.51.3\r\nh11==0.14.0\r\nhttpcore==0.17.3\r\nhttpx==0.24.1\r\nhuggingface-hub==0.16.4\r\nidna==3.4\r\nJinja2==3.1.2\r\njsonschema==4.18.4\r\njsonschema-specifications==2023.7.1\r\nkiwisolver==1.4.4\r\nlinkify-it-py==2.0.2\r\nlit==16.0.6\r\nmarkdown-it-py==2.2.0\r\nmarkdown2==2.4.9\r\nMarkupSafe==2.1.3\r\nmatplotlib==3.7.2\r\nmdit-py-plugins==0.3.3\r\nmdurl==0.1.2\r\nmpmath==1.3.0\r\nmsgpack==1.0.5\r\nmultidict==6.0.4\r\nmypy-extensions==1.0.0\r\nnetworkx==3.1\r\nninja==1.11.1\r\nnumpy==1.25.1\r\nnvidia-cublas-cu11==11.10.3.66\r\nnvidia-cuda-cupti-cu11==11.7.101\r\nnvidia-cuda-nvrtc-cu11==11.7.99\r\nnvidia-cuda-runtime-cu11==11.7.99\r\nnvidia-cudnn-cu11==8.5.0.96\r\nnvidia-cufft-cu11==10.9.0.58\r\nnvidia-curand-cu11==10.2.10.91\r\nnvidia-cusolver-cu11==11.4.0.1\r\nnvidia-cusparse-cu11==11.7.4.91\r\nnvidia-nccl-cu11==2.14.3\r\nnvidia-nvtx-cu11==11.7.91\r\norjson==3.9.2\r\npackaging==23.1\r\npandas==2.0.3\r\npathtools==0.1.2\r\nPillow==10.0.0\r\nprompt-toolkit==3.0.39\r\nprotobuf==4.23.4\r\npsutil==5.9.5\r\npydantic==1.10.11\r\npydub==0.25.1\r\nPygments==2.15.1\r\npyparsing==3.0.9\r\npyre-extensions==0.0.29\r\npython-dateutil==2.8.2\r\npython-multipart==0.0.6\r\npytz==2023.3\r\nPyYAML==6.0.1\r\nray==2.5.1\r\nreferencing==0.30.0\r\nregex==2023.6.3\r\nrequests==2.31.0\r\nrich==13.4.2\r\nrpds-py==0.9.2\r\nsafetensors==0.3.1\r\nsemantic-version==2.10.0\r\nsentencepiece==0.1.99\r\nsentry-sdk==1.28.1\r\nsetproctitle==1.3.2\r\nshortuuid==1.0.11\r\nsix==1.16.0\r\nsmmap==5.0.0\r\nsniffio==1.3.0\r\nstarlette==0.27.0\r\nsvgwrite==1.4.3\r\nsympy==1.12\r\ntokenizers==0.13.3\r\ntoolz==0.12.0\r\ntorch==2.0.1\r\ntqdm==4.65.0\r\ntransformers==4.31.0\r\ntriton==2.0.0\r\ntyping-inspect==0.9.0\r\ntyping_extensions==4.7.1\r\ntzdata==2023.3\r\nuc-micro-py==1.0.2\r\nurllib3==2.0.4\r\nuvicorn==0.23.1\r\n-e git+https://github.com/vllm-project/vllm.git@6fc2a38b110f9ba6037b31ee016f20df32426877#egg=vllm\r\nwandb==0.15.5\r\nwavedrom==2.0.3.post3\r\nwcwidth==0.2.6\r\nwebsockets==11.0.3\r\nxformers==0.0.20\r\nyarl==1.9.2\r\n```\r\n</details>\r\n\r\n<details>\r\n  <summary>My Dockerfile</summary>\r\n\r\n```Dockerfile\r\nARG CUDA_VERSION=\"11.8.0\"\r\nARG CUDNN_VERSION=\"8\"\r\nARG UBUNTU_VERSION=\"22.04\"\r\n\r\n# Base NVidia CUDA Ubuntu image\r\nFROM nvidia/cuda:$CUDA_VERSION-cudnn$CUDNN_VERSION-devel-ubuntu$UBUNTU_VERSION AS base\r\n\r\nEXPOSE 22/tcp\r\nEXPOSE 8000/tcp\r\n\r\nUSER root\r\n# Install Python plus openssh, which is our minimum set of required packages.\r\n# Install useful command line utility software\r\nARG APTPKGS=\"zsh sudo wget tmux nvtop vim neovim curl rsync less\"\r\nRUN apt-get update -y && \\\r\n    apt-get install -y python3 python3-pip python3-venv && \\\r\n    apt-get install -y --no-install-recommends openssh-server openssh-client git git-lfs && \\\r\n    python3 -m pip install --upgrade pip && \\\r\n    apt-get install -y --no-install-recommends $APTPKGS && \\\r\n    apt-get clean && \\\r\n    rm -rf /var/lib/apt/lists/*\r\n\r\nENV PATH=\"/usr/local/cuda/bin:${PATH}\"\r\n\r\nARG USERNAME=vllm\r\nENV USERNAME=$USERNAME\r\nARG VOLUME=/workspace\r\nENV VOLUME=$VOLUME\r\n\r\n# Create user, change shell to ZSH, make a volume which they own\r\nRUN useradd -m -u 1000 $USERNAME && \\\r\n    chsh -s /usr/bin/zsh $USERNAME && \\\r\n    mkdir -p \"$VOLUME\" && \\\r\n    chown $USERNAME:$USERNAME \"$VOLUME\" && \\\r\n    usermod -aG sudo $USERNAME && \\\r\n    echo \"$USERNAME ALL=(ALL) NOPASSWD:ALL\" > /etc/sudoers.d/90-docker-users\r\n\r\nUSER $USERNAME\r\nENV HOME=/home/$USERNAME\r\nENV PATH=$HOME/.local/bin:$PATH\r\nWORKDIR $HOME\r\n\r\nENV TORCH_CUDA_ARCH_LIST=\"8.0;8.6+PTX;8.9;9.0\"\r\n\r\nRUN git clone https://github.com/vllm-project/vllm.git && \\\r\n    cd vllm && \\\r\n    pip3 install -e . && \\\r\n    pip3 cache purge\r\n```\r\n\r\n</details>\r\n\r\nAnd finally here is the log from the creation of the Docker container:\r\n\r\n<details>\r\n  <summary>docker build log</summary>\r\n\r\n#1 [internal] load build definition from Dockerfile\r\n#1 transferring dockerfile: 2.67kB done\r\n#1 DONE 0.0s\r\n\r\n#2 [internal] load .dockerignore\r\n#2 transferring context: 2B done\r\n#2 DONE 0.0s\r\n\r\n#3 [internal] load metadata for docker.io/nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04\r\n#3 ...\r\n\r\n#4 [auth] nvidia/cuda:pull token for registry-1.docker.io\r\n#4 DONE 0.0s\r\n\r\n#3 [internal] load metadata for docker.io/nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04\r\n#3 DONE 0.7s\r\n\r\n#5 [ 1/17] FROM docker.io/nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04@sha256:b856c89aa26c1dc1b56c834a66b44c527a298325173c87291486fc100ceedb6e\r\n#5 DONE 0.0s\r\n\r\n#6 [ 2/17] RUN apt-get update -y &&     apt-get install -y python3 python3-pip python3-venv &&     apt-get install -y --no-install-recommends openssh-server openssh-client git git-lfs &&     python3 -m pip install --upgrade pip &&     apt-get install -y --no-install-recommends zsh sudo wget tmux nvtop vim neovim curl rsync less &&     apt-get clean &&     rm -rf /var/lib/apt/lists/*\r\n#6 CACHED\r\n\r\n#7 [ 3/17] RUN useradd -m -u 1000 vllm &&     chsh -s /usr/bin/zsh vllm &&     mkdir -p \"/workspace\" &&     chown vllm:vllm \"/workspace\" &&     usermod -aG sudo vllm &&     echo \"vllm ALL=(ALL) NOPASSWD:ALL\" > /etc/sudoers.d/90-docker-users\r\n#7 CACHED\r\n\r\n#8 [ 4/17] WORKDIR /home/vllm\r\n#8 CACHED\r\n\r\n#9 [internal] load build context\r\n#9 transferring context: 444B 0.0s done\r\n#9 DONE 0.0s\r\n\r\n#10 [ 5/17] RUN git clone https://github.com/vllm-project/vllm.git &&     cd vllm &&     pip3 install -e . &&     pip3 cache purge\r\n#10 0.321 Cloning into 'vllm'...\r\n#10 2.377 Defaulting to user installation because normal site-packages is not writeable\r\n#10 2.419 Obtaining file:///home/vllm/vllm\r\n#10 2.423   Installing build dependencies: started\r\n#10 90.29   Installing build dependencies: still running...\r\n#10 90.75   Installing build dependencies: finished with status 'done'\r\n#10 90.75   Checking if build backend supports build_editable: started\r\n#10 90.97   Checking if build backend supports build_editable: finished with status 'done'\r\n#10 90.97   Getting requirements to build editable: started\r\n#10 93.48   Getting requirements to build editable: finished with status 'done'\r\n#10 93.49   Preparing editable metadata (pyproject.toml): started\r\n#10 96.08   Preparing editable metadata (pyproject.toml): finished with status 'done'\r\n#10 96.47 Collecting ninja (from vllm==0.1.2)\r\n#10 96.47   Using cached ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\r\n#10 96.80 Collecting psutil (from vllm==0.1.2)\r\n#10 96.99   Downloading psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\r\n#10 97.20      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 282.1/282.1 kB 1.4 MB/s eta 0:00:00\r\n#10 97.50 Collecting ray>=2.5.1 (from vllm==0.1.2)\r\n#10 97.50   Obtaining dependency information for ray>=2.5.1 from https://files.pythonhosted.org/packages/9c/42/ef94d5cbd492d05999ee6f77fa7de6a16c18b634241085203919029fef8d/ray-2.5.1-cp310-cp310-manylinux2014_x86_64.whl.metadata\r\n#10 97.53   Downloading ray-2.5.1-cp310-cp310-manylinux2014_x86_64.whl.metadata (12 kB)\r\n#10 97.77 Collecting sentencepiece (from vllm==0.1.2)\r\n#10 97.80   Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\r\n#10 97.87      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.3/1.3 MB 21.5 MB/s eta 0:00:00\r\n#10 98.44 Collecting numpy (from vllm==0.1.2)\r\n#10 98.44   Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/d0/55/559e6f455a066e12058330377259a106b7fefa41c15dbdb1b71070cec429/numpy-1.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n#10 98.47   Downloading numpy-1.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\r\n#10 98.59 Collecting torch>=2.0.0 (from vllm==0.1.2)\r\n#10 100.9   Using cached torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\r\n#10 103.1 Collecting transformers>=4.31.0 (from vllm==0.1.2)\r\n#10 103.1   Obtaining dependency information for transformers>=4.31.0 from https://files.pythonhosted.org/packages/21/02/ae8e595f45b6c8edee07913892b3b41f5f5f273962ad98851dc6a564bbb9/transformers-4.31.0-py3-none-any.whl.metadata\r\n#10 103.1   Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\r\n#10 103.1      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 116.9/116.9 kB 20.7 MB/s eta 0:00:00\r\n#10 103.2 Collecting xformers>=0.0.19 (from vllm==0.1.2)\r\n#10 103.2   Obtaining dependency information for xformers>=0.0.19 from https://files.pythonhosted.org/packages/4b/b0/dfbb3b0ceafdb73cd1b2bbe33f65dc1c5c47dcb0d4b03ba6f95da6557306/xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl.metadata\r\n#10 103.2   Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.1 kB)\r\n#10 103.4 Collecting fastapi (from vllm==0.1.2)\r\n#10 103.4   Obtaining dependency information for fastapi from https://files.pythonhosted.org/packages/49/f5/048206823aae9b3a4a61ba6b7a1dd1de36bd4c0a0283f2efb1f1f2289c8a/fastapi-0.100.0-py3-none-any.whl.metadata\r\n#10 103.4   Downloading fastapi-0.100.0-py3-none-any.whl.metadata (23 kB)\r\n#10 103.5 Collecting uvicorn (from vllm==0.1.2)\r\n#10 103.5   Obtaining dependency information for uvicorn from https://files.pythonhosted.org/packages/5d/07/b9eac057f7efa56900640a233c1ed63db83568322c6bcbabe98f741d5289/uvicorn-0.23.1-py3-none-any.whl.metadata\r\n#10 103.5   Downloading uvicorn-0.23.1-py3-none-any.whl.metadata (6.2 kB)\r\n#10 103.9 Collecting pydantic<2 (from vllm==0.1.2)\r\n#10 103.9   Obtaining dependency information for pydantic<2 from https://files.pythonhosted.org/packages/b6/8e/7dd215f91528487535e7aa048e4092c20ecd0168df958e58809e2235cece/pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n#10 103.9   Downloading pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (148 kB)\r\n#10 103.9      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 149.0/149.0 kB 30.1 MB/s eta 0:00:00\r\n#10 104.0 Collecting fschat (from vllm==0.1.2)\r\n#10 104.0   Obtaining dependency information for fschat from https://files.pythonhosted.org/packages/d2/29/ab7ae254ab4b73f29fc1a0b9dd5f95cbcaacfb28f836719033458337d9cf/fschat-0.2.18-py3-none-any.whl.metadata\r\n#10 104.0   Downloading fschat-0.2.18-py3-none-any.whl.metadata (15 kB)\r\n#10 104.1 Collecting typing-extensions>=4.2.0 (from pydantic<2->vllm==0.1.2)\r\n#10 104.1   Obtaining dependency information for typing-extensions>=4.2.0 from https://files.pythonhosted.org/packages/ec/6b/63cc3df74987c36fe26157ee12e09e8f9db4de771e0f3404263117e75b95/typing_extensions-4.7.1-py3-none-any.whl.metadata\r\n#10 104.1   Using cached typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\r\n#10 104.3 Collecting attrs (from ray>=2.5.1->vllm==0.1.2)\r\n#10 104.3   Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\r\n#10 104.4      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 61.2/61.2 kB 11.2 MB/s eta 0:00:00\r\n#10 104.4 Collecting click>=7.0 (from ray>=2.5.1->vllm==0.1.2)\r\n#10 104.4   Obtaining dependency information for click>=7.0 from https://files.pythonhosted.org/packages/1a/70/e63223f8116931d365993d4a6b7ef653a4d920b41d03de7c59499962821f/click-8.1.6-py3-none-any.whl.metadata\r\n#10 104.5   Downloading click-8.1.6-py3-none-any.whl.metadata (3.0 kB)\r\n#10 104.6 Collecting filelock (from ray>=2.5.1->vllm==0.1.2)\r\n#10 104.6   Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/00/45/ec3407adf6f6b5bf867a4462b2b0af27597a26bd3cd6e2534cb6ab029938/filelock-3.12.2-py3-none-any.whl.metadata\r\n#10 104.6   Using cached filelock-3.12.2-py3-none-any.whl.metadata (2.7 kB)\r\n#10 104.7 Collecting jsonschema (from ray>=2.5.1->vllm==0.1.2)\r\n#10 104.7   Obtaining dependency information for jsonschema from https://files.pythonhosted.org/packages/a1/ba/28ce987450c6afa8336373761193ddaadc1ba2004fbf23a6407db036f558/jsonschema-4.18.4-py3-none-any.whl.metadata\r\n#10 104.7   Downloading jsonschema-4.18.4-py3-none-any.whl.metadata (7.8 kB)\r\n#10 104.9 Collecting msgpack<2.0.0,>=1.0.0 (from ray>=2.5.1->vllm==0.1.2)\r\n#10 104.9   Downloading msgpack-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\r\n#10 104.9      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 316.8/316.8 kB 38.5 MB/s eta 0:00:00\r\n#10 105.0 Collecting packaging (from ray>=2.5.1->vllm==0.1.2)\r\n#10 105.0   Using cached packaging-23.1-py3-none-any.whl (48 kB)\r\n#10 105.5 Collecting protobuf!=3.19.5,>=3.15.3 (from ray>=2.5.1->vllm==0.1.2)\r\n#10 105.5   Obtaining dependency information for protobuf!=3.19.5,>=3.15.3 from https://files.pythonhosted.org/packages/01/cb/445b3e465abdb8042a41957dc8f60c54620dc7540dbcf9b458a921531ca2/protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl.metadata\r\n#10 105.5   Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\r\n#10 105.6 Collecting pyyaml (from ray>=2.5.1->vllm==0.1.2)\r\n#10 105.6   Obtaining dependency information for pyyaml from https://files.pythonhosted.org/packages/29/61/bf33c6c85c55bc45a29eee3195848ff2d518d84735eb0e2d8cb42e0d285e/PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n#10 105.7   Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\r\n#10 105.7 Collecting aiosignal (from ray>=2.5.1->vllm==0.1.2)\r\n#10 105.7   Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\r\n#10 105.9 Collecting frozenlist (from ray>=2.5.1->vllm==0.1.2)\r\n#10 105.9   Obtaining dependency information for frozenlist from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n#10 105.9   Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\r\n#10 106.1 Collecting requests (from ray>=2.5.1->vllm==0.1.2)\r\n#10 106.1   Obtaining dependency information for requests from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\r\n#10 106.1   Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\r\n#10 107.4 Collecting grpcio<=1.51.3,>=1.42.0 (from ray>=2.5.1->vllm==0.1.2)\r\n#10 107.5   Downloading grpcio-1.51.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\r\n#10 107.6      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4.8/4.8 MB 51.3 MB/s eta 0:00:00\r\n#10 107.7 Collecting sympy (from torch>=2.0.0->vllm==0.1.2)\r\n#10 107.7   Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\r\n#10 107.8 Collecting networkx (from torch>=2.0.0->vllm==0.1.2)\r\n#10 107.8   Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\r\n#10 107.9 Collecting jinja2 (from torch>=2.0.0->vllm==0.1.2)\r\n#10 107.9   Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\r\n#10 108.0 Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch>=2.0.0->vllm==0.1.2)\r\n#10 108.0   Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\r\n#10 108.3 Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch>=2.0.0->vllm==0.1.2)\r\n#10 108.3   Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\r\n#10 108.4 Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch>=2.0.0->vllm==0.1.2)\r\n#10 108.4   Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\r\n#10 108.5 Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch>=2.0.0->vllm==0.1.2)\r\n#10 110.6   Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\r\n#10 112.3 Collecting nvidia-cublas-cu11==11.10.3.66 (from torch>=2.0.0->vllm==0.1.2)\r\n#10 113.5   Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\r\n#10 114.7 Collecting nvidia-cufft-cu11==10.9.0.58 (from torch>=2.0.0->vllm==0.1.2)\r\n#10 115.3   Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\r\n#10 115.9 Collecting nvidia-curand-cu11==10.2.10.91 (from torch>=2.0.0->vllm==0.1.2)\r\n#10 116.1   Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\r\n#10 116.3 Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch>=2.0.0->vllm==0.1.2)\r\n#10 116.7   Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\r\n#10 117.0 Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch>=2.0.0->vllm==0.1.2)\r\n#10 117.7   Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\r\n#10 118.3 Collecting nvidia-nccl-cu11==2.14.3 (from torch>=2.0.0->vllm==0.1.2)\r\n#10 118.8   Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\r\n#10 119.2 Collecting nvidia-nvtx-cu11==11.7.91 (from torch>=2.0.0->vllm==0.1.2)\r\n#10 119.2   Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\r\n#10 119.2 Collecting triton==2.0.0 (from torch>=2.0.0->vllm==0.1.2)\r\n#10 119.4   Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\r\n#10 119.5 Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=2.0.0->vllm==0.1.2) (59.6.0)\r\n#10 119.5 Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=2.0.0->vllm==0.1.2) (0.37.1)\r\n#10 119.8 Collecting cmake (from triton==2.0.0->torch>=2.0.0->vllm==0.1.2)\r\n#10 119.8   Obtaining dependency information for cmake from https://files.pythonhosted.org/packages/14/b8/06f8fdc4687af3d3d8d95461d97737df2f144acd28eff65a3c47c29d0152/cmake-3.27.0-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\r\n#10 119.8   Using cached cmake-3.27.0-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.7 kB)\r\n#10 119.8 Collecting lit (from triton==2.0.0->torch>=2.0.0->vllm==0.1.2)\r\n#10 119.8   Using cached lit-16.0.6-py3-none-any.whl\r\n#10 120.2 Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=4.31.0->vllm==0.1.2)\r\n#10 120.2   Obtaining dependency information for huggingface-hub<1.0,>=0.14.1 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\r\n#10 120.3   Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\r\n#10 121.0 Collecting regex!=2019.12.17 (from transformers>=4.31.0->vllm==0.1.2)\r\n#10 121.0   Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/a4/06/85618f80ae552ac309ead9702c6826edda27884e26e07fdc8fa93f283546/regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n#10 121.0   Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\r\n#10 121.0      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 40.9/40.9 kB 7.3 MB/s eta 0:00:00\r\n#10 121.3 Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.31.0->vllm==0.1.2)\r\n#10 121.3   Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\r\n#10 121.5      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.8/7.8 MB 42.7 MB/s eta 0:00:00\r\n#10 121.6 Collecting safetensors>=0.3.1 (from transformers>=4.31.0->vllm==0.1.2)\r\n#10 121.7   Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\r\n#10 121.7      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.3/1.3 MB 68.6 MB/s eta 0:00:00\r\n#10 121.8 Collecting tqdm>=4.27 (from transformers>=4.31.0->vllm==0.1.2)\r\n#10 121.9   Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\r\n#10 121.9      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 77.1/77.1 kB 12.5 MB/s eta 0:00:00\r\n#10 122.0 Collecting pyre-extensions==0.0.29 (from xformers>=0.0.19->vllm==0.1.2)\r\n#10 122.0   Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\r\n#10 122.1 Collecting typing-inspect (from pyre-extensions==0.0.29->xformers>=0.0.19->vllm==0.1.2)\r\n#10 122.1   Obtaining dependency information for typing-inspect from https://files.pythonhosted.org/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl.metadata\r\n#10 122.1   Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\r\n#10 122.3 Collecting starlette<0.28.0,>=0.27.0 (from fastapi->vllm==0.1.2)\r\n#10 122.3   Obtaining dependency information for starlette<0.28.0,>=0.27.0 from https://files.pythonhosted.org/packages/58/f8/e2cca22387965584a409795913b774235752be4176d276714e15e1a58884/starlette-0.27.0-py3-none-any.whl.metadata\r\n#10 122.4   Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\r\n#10 122.5 Collecting accelerate (from fschat->vllm==0.1.2)\r\n#10 122.5   Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/70/f9/c381bcdd0c3829d723aa14eec8e75c6c377b4ca61ec68b8093d9f35fc7a7/accelerate-0.21.0-py3-none-any.whl.metadata\r\n#10 122.6   Downloading accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\r\n#10 122.8 Collecting gradio==3.35.2 (from fschat->vllm==0.1.2)\r\n#10 122.8   Obtaining dependency information for gradio==3.35.2 from https://files.pythonhosted.org/packages/50/70/ed0ba0fb5c3b1cb2e481717ad190056a4c9a0ef2f296b871e10375b2ab83/gradio-3.35.2-py3-none-any.whl.metadata\r\n#10 122.8   Downloading gradio-3.35.2-py3-none-any.whl.metadata (15 kB)\r\n#10 122.9 Collecting httpx (from fschat->vllm==0.1.2)\r\n#10 122.9   Obtaining dependency information for httpx from https://files.pythonhosted.org/packages/ec/91/e41f64f03d2a13aee7e8c819d82ee3aa7cdc484d18c0ae859742597d5aa0/httpx-0.24.1-py3-none-any.whl.metadata\r\n#10 122.9   Downloading httpx-0.24.1-py3-none-any.whl.metadata (7.4 kB)\r\n#10 123.0 Collecting markdown2[all] (from fschat->vllm==0.1.2)\r\n#10 123.0   Obtaining dependency information for markdown2[all] from https://files.pythonhosted.org/packages/8f/b5/93495ced07fb66c8b8a0fbc5edf07bf9fefefc1135d6e2d66e0ce5689b7d/markdown2-2.4.9-py2.py3-none-any.whl.metadata\r\n#10 123.0   Downloading markdown2-2.4.9-py2.py3-none-any.whl.metadata (2.0 kB)\r\n#10 123.2 Collecting nh3 (from fschat->vllm==0.1.2)\r\n#10 123.2   Obtaining dependency information for nh3 from https://files.pythonhosted.org/packages/b7/cd/7f64121ec731255265867e0d7d782962f2bd1f15fce83f523c8f6b69463b/nh3-0.2.14-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n#10 123.2   Downloading nh3-0.2.14-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n#10 123.2 Collecting peft (from fschat->vllm==0.1.2)\r\n#10 123.2   Obtaining dependency information for peft from https://files.pythonhosted.org/packages/88/a0/6e1c23293a922a9c9e9bd8d56a60cd78ecf531fdabe45ac975e142bfbe86/peft-0.4.0-py3-none-any.whl.metadata\r\n#10 123.3   Downloading peft-0.4.0-py3-none-any.whl.metadata (21 kB)\r\n#10 123.4 Collecting prompt-toolkit>=3.0.0 (from fschat->vllm==0.1.2)\r\n#10 123.4   Obtaining dependency information for prompt-toolkit>=3.0.0 from https://files.pythonhosted.org/packages/a9/b4/ba77c84edf499877317225d7b7bc047a81f7c2eed9628eeb6bab0ac2e6c9/prompt_toolkit-3.0.39-py3-none-any.whl.metadata\r\n#10 123.4   Downloading prompt_toolkit-3.0.39-py3-none-any.whl.metadata (6.4 kB)\r\n#10 123.6 Collecting rich>=10.0.0 (from fschat->vllm==0.1.2)\r\n#10 123.6   Obtaining dependency information for rich>=10.0.0 from https://files.pythonhosted.org/packages/fc/1e/482e5eec0b89b593e81d78f819a9412849814e22225842b598908e7ac560/rich-13.4.2-py3-none-any.whl.metadata\r\n#10 123.6   Downloading rich-13.4.2-py3-none-any.whl.metadata (18 kB)\r\n#10 123.7 Collecting shortuuid (from fschat->vllm==0.1.2)\r\n#10 123.7   Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\r\n#10 123.8 Collecting tiktoken (from fschat->vllm==0.1.2)\r\n#10 123.8   Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\r\n#10 123.9      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.7/1.7 MB 56.6 MB/s eta 0:00:00\r\n#10 123.9 INFO: pip is looking at multiple versions of fschat to determine which version is compatible with other requirements. This could take a while.\r\n#10 123.9 Collecting fschat (from vllm==0.1.2)\r\n#10 123.9   Obtaining dependency information for fschat from https://files.pythonhosted.org/packages/8e/4e/58a0929f57afaf92f2a04758982b93d484a31b77392d03adfaec2c9dab06/fschat-0.2.17-py3-none-any.whl.metadata\r\n#10 123.9   Downloading fschat-0.2.17-py3-none-any.whl.metadata (15 kB)\r\n#10 124.0   Obtaining dependency information for fschat from https://files.pythonhosted.org/packages/49/03/c9eb3b7ea94a7e0cc200cbc99b8c8b2856100d5895a2dff790cdcb721f89/fschat-0.2.16-py3-none-any.whl.metadata\r\n#10 124.0   Downloading fschat-0.2.16-py3-none-any.whl.metadata (16 kB)\r\n#10 124.0   Obtaining dependency information for fschat from https://files.pythonhosted.org/packages/bd/31/8f916a482674de9ce02bd5433073a7c2c441ef3dd387441e7675bce23662/fschat-0.2.15-py3-none-any.whl.metadata\r\n#10 124.1   Downloading fschat-0.2.15-py3-none-any.whl.metadata (17 kB)\r\n#10 124.1 Collecting gradio==3.23 (from fschat->vllm==0.1.2)\r\n#10 124.2   Downloading gradio-3.23.0-py3-none-any.whl (15.8 MB)\r\n#10 124.4      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 15.8/15.8 MB 55.7 MB/s eta 0:00:00\r\n#10 124.5 Collecting fschat (from vllm==0.1.2)\r\n#10 124.5   Obtaining dependency information for fschat from https://files.pythonhosted.org/packages/0d/ff/1da77fc8f7c08d00822944cbb8bc6563b98f4c2d1bdd784947dc72bf6bd9/fschat-0.2.14-py3-none-any.whl.metadata\r\n#10 124.5   Downloading fschat-0.2.14-py3-none-any.whl.metadata (19 kB)\r\n#10 124.6   Obtaining dependency information for fschat from https://files.pythonhosted.org/packages/07/ae/3f1e88b46daad7230b93341b12f851db2c4ce590a4282d3fee5b674bfb5e/fschat-0.2.13-py3-none-any.whl.metadata\r\n#10 124.6   Downloading fschat-0.2.13-py3-none-any.whl.metadata (18 kB)\r\n#10 124.6   Obtaining dependency information for fschat from https://files.pythonhosted.org/packages/af/a5/dfe1bcc540995911c36f67b23eecee5fd5054dab006bac42d5a64e79b215/fschat-0.2.12-py3-none-any.whl.metadata\r\n#10 124.7   Downloading fschat-0.2.12-py3-none-any.whl.metadata (18 kB)\r\n#10 124.7   Obtaining dependency information for fschat from https://files.pythonhosted.org/packages/87/41/84972fca6c05407eddd079e3861f0e53b1181136e01934373f6292fbe1f0/fschat-0.2.11-py3-none-any.whl.metadata\r\n#10 124.8   Downloading fschat-0.2.11-py3-none-any.whl.metadata (18 kB)\r\n#10 124.8 INFO: pip is still looking at multiple versions of fschat to determine which version is compatible with other requirements. This could take a while.\r\n#10 124.8   Obtaining dependency information for fschat from https://files.pythonhosted.org/packages/e1/29/e299813dd5b0035637d06ce1f79cdd754f71325aea3affe35db8e20f9f67/fschat-0.2.10-py3-none-any.whl.metadata\r\n#10 124.8   Downloading fschat-0.2.10-py3-none-any.whl.metadata (16 kB)\r\n#10 124.9   Obtaining dependency information for fschat from https://files.pythonhosted.org/packages/1d/f5/2ef73a157f15bcbd579a2e31ccec4440cc29fce5e34d1361e1337b34cdfd/fschat-0.2.9-py3-none-any.whl.metadata\r\n#10 124.9   Downloading fschat-0.2.9-py3-none-any.whl.metadata (16 kB)\r\n#10 124.9   Obtaining dependency information for fschat from https://files.pythonhosted.org/packages/f7/31/d16a1f71efb0d3681c22579f07a898ce02f2c018689231ffdd7550768887/fschat-0.2.8-py3-none-any.whl.metadata\r\n#10 125.0   Downloading fschat-0.2.8-py3-none-any.whl.metadata (16 kB)\r\n#10 125.0   Downloading fschat-0.2.7-py3-none-any.whl (109 kB)\r\n#10 125.0      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 109.4/109.4 kB 15.5 MB/s eta 0:00:00\r\n#10 125.1   Downloading fschat-0.2.6-py3-none-any.whl (108 kB)\r\n#10 125.1      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 108.5/108.5 kB 18.1 MB/s eta 0:00:00\r\n#10 125.2 INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\r\n#10 125.2   Downloading fschat-0.2.5-py3-none-any.whl (164 kB)\r\n#10 125.2      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 164.6/164.6 kB 23.2 MB/s eta 0:00:00\r\n#10 125.3   Downloading fschat-0.2.4-py3-none-any.whl (161 kB)\r\n#10 125.3      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.4/161.4 kB 27.2 MB/s eta 0:00:00\r\n#10 125.4   Downloading fschat-0.2.3-py3-none-any.whl (79 kB)\r\n#10 125.4      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 80.0/80.0 kB 12.5 MB/s eta 0:00:00\r\n#10 125.6 Collecting wandb (from fschat->vllm==0.1.2)\r\n#10 125.6   Obtaining dependency information for wandb from https://files.pythonhosted.org/packages/87/26/66e944b17aa06a4c9df9850a6a5c56378cb4f9b3acf3452ace7dfd895c13/wandb-0.15.5-py3-none-any.whl.metadata\r\n#10 125.7   Downloading wandb-0.15.5-py3-none-any.whl.metadata (8.2 kB)\r\n#10 125.8 Collecting aiofiles (from gradio==3.23->fschat->vllm==0.1.2)\r\n#10 125.8   Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\r\n#10 126.5 Collecting aiohttp (from gradio==3.23->fschat->vllm==0.1.2)\r\n#10 126.5   Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n#10 126.5   Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\r\n#10 126.6 Collecting altair>=4.2.0 (from gradio==3.23->fschat->vllm==0.1.2)\r\n#10 126.6   Obtaining dependency information for altair>=4.2.0 from https://files.pythonhosted.org/packages/b2/20/5c3b89d6f8d9938325a9330793438389e0dc94c34d921f6da35ec62095f3/altair-5.0.1-py3-none-any.whl.metadata\r\n#10 126.6   Downloading altair-5.0.1-py3-none-any.whl.metadata (8.5 kB)\r\n#10 126.7 Collecting ffmpy (from gradio==3.23->fschat->vllm==0.1.2)\r\n#10 126.7   Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\r\n#10 126.7   Preparing metadata (setup.py): started\r\n#10 127.0   Preparing metadata (setup.py): finished with status 'done'\r\n#10 127.1 Collecting fsspec (from gradio==3.23->fschat->vllm==0.1.2)\r\n#10 127.1   Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/e3/bd/4c0a4619494188a9db5d77e2100ab7d544a42e76b2447869d8e124e981d8/fsspec-2023.6.0-py3-none-any.whl.metadata\r\n#10 127.1   Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\r\n#10 127.3 Collecting markdown-it-py[linkify]>=2.0.0 (from gradio==3.23->fschat->vllm==0.1.2)\r\n#10 127.3   Obtaining dependency information for markdown-it-py[linkify]>=2.0.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata\r\n#10 127.3   Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\r\n#10 127.5 Collecting markupsafe (from gradio==3.23->fschat->vllm==0.1.2)\r\n#10 127.5   Obtaining dependency information for markupsafe from https://files.pythonhosted.org/packages/12/b3/d9ed2c0971e1435b8a62354b18d3060b66c8cb1d368399ec0b9baa7c0ee5/MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n#10 127.5   Using cached MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\r\n#10 127.9 Collecting matplotlib (from gradio==3.23->fschat->vllm==0.1.2)\r\n#10 127.9   Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/c2/da/a5622266952ab05dc3995d77689cba600e49ea9d6c51d469c077695cb719/matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n#10 127.9   Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\r\n#10 128.0 Collecting mdit-py-plugins<=0.3.3 (from gradio==3.23->fschat->vllm==0.1.2)\r\n#10 128.0   Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\r\n#10 128.0      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 50.5/50.5 kB 8.1 MB/s eta 0:00:00\r\n#10 128.6 Collecting orjson (from gradio==3.23->fschat->vllm==0.1.2)\r\n#10 128.6   Obtaining dependency information for orjson from https://files.pythonhosted.org/packages/a3/13/959dbe9e6cc77a0e50f617b79d49e21d0ac80a16838d4f2d2a172f76f363/orjson-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n#10 128.6   Downloading orjson-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\r\n#10 128.6      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 49.2/49.2 kB 8.9 MB/s eta 0:00:00\r\n#10 129.0 Collecting pandas (from gradio==3.23->fschat->vllm==0.1.2)\r\n#10 129.0   Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/e3/59/35a2892bf09ded9c1bf3804461efe772836a5261ef5dfb4e264ce813ff99/pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n#10 129.0   Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\r\n#10 129.6 Collecting pillow (from gradio==3.23->fschat->vllm==0.1.2)\r\n#10 129.6   Obtaining dependency information for pillow from https://files.pythonhosted.org/packages/3d/36/e78f09d510354977e10102dd811e928666021d9c451e05df962d56477772/Pillow-10.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata\r\n#10 129.6   Downloading Pillow-10.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\r\n#10 129.7 Collecting pydub (from gradio==3.23->fschat->vllm==0.1.2)\r\n#10 129.7   Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\r\n#10 129.8 Collecting python-multipart (from gradio==3.23->fschat->vllm==0.1.2)\r\n#10 129.8   Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\r\n#10 129.8      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 45.7/45.7 kB 6.0 MB/s eta 0:00:00\r\n#10 129.9 Collecting semantic-version (from gradio==3.23->fschat->vllm==0.1.2)\r\n#10 129.9   Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\r\n#10 130.2 Collecting websockets>=10.0 (from gradio==3.23->fschat->vllm==0.1.2)\r\n#10 130.2   Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\r\n#10 130.2      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.9/129.9 kB 22.8 MB/s eta 0:00:00\r\n#10 130.3 Collecting h11>=0.8 (from uvicorn->vllm==0.1.2)\r\n#10 130.3   Downloading h11-0.14.0-py3-none-any.whl (58 kB)\r\n#10 130.4      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 58.3/58.3 kB 11.2 MB/s eta 0:00:00\r\n#10 130.6 Collecting wcwidth (from prompt-toolkit>=3.0.0->fschat->vllm==0.1.2)\r\n#10 130.6   Downloading wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\r\n#10 130.7 Collecting pygments<3.0.0,>=2.13.0 (from rich>=10.0.0->fschat->vllm==0.1.2)\r\n#10 130.8   Downloading Pygments-2.15.1-py3-none-any.whl (1.1 MB)\r\n#10 130.8      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1/1.1 MB 69.4 MB/s eta 0:00:00\r\n#10 131.0 Collecting anyio<5,>=3.4.0 (from starlette<0.28.0,>=0.27.0->fastapi->vllm==0.1.2)\r\n#10 131.0   Obtaining dependency information for anyio<5,>=3.4.0 from https://files.pythonhosted.org/packages/19/24/44299477fe7dcc9cb58d0a57d5a7588d6af2ff403fdd2d47a246c91a3246/anyio-3.7.1-py3-none-any.whl.metadata\r\n#10 131.0   Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\r\n#10 131.3 Collecting certifi (from httpx->fschat->vllm==0.1.2)\r\n#10 131.3   Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\r\n#10 131.4      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 157.0/157.0 kB 28.2 MB/s eta 0:00:00\r\n#10 131.4 Collecting httpcore<0.18.0,>=0.15.0 (from httpx->fschat->vllm==0.1.2)\r\n#10 131.4   Obtaining dependency information for httpcore<0.18.0,>=0.15.0 from https://files.pythonhosted.org/packages/94/2c/2bde7ff8dd2064395555220cbf7cba79991172bf5315a07eb3ac7688d9f1/httpcore-0.17.3-py3-none-any.whl.metadata\r\n#10 131.4   Downloading httpcore-0.17.3-py3-none-any.whl.metadata (18 kB)\r\n#10 131.5 Collecting idna (from httpx->fschat->vllm==0.1.2)\r\n#10 131.5   Downloading idna-3.4-py3-none-any.whl (61 kB)\r\n#10 131.5      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 61.5/61.5 kB 11.0 MB/s eta 0:00:00\r\n#10 131.6 Collecting sniffio (from httpx->fschat->vllm==0.1.2)\r\n#10 131.6   Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\r\n#10 131.8 Collecting jsonschema-specifications>=2023.03.6 (from jsonschema->ray>=2.5.1->vllm==0.1.2)\r\n#10 131.8   Obtaining dependency information for jsonschema-specifications>=2023.03.6 from https://files.pythonhosted.org/packages/1c/24/83349ac2189cc2435e84da3f69ba3c97314d3c0622628e55171c6798ed80/jsonschema_specifications-2023.7.1-py3-none-any.whl.metadata\r\n#10 131.8   Downloading jsonschema_specifications-2023.7.1-py3-none-any.whl.metadata (2.8 kB)\r\n#10 131.9 Collecting referencing>=0.28.4 (from jsonschema->ray>=2.5.1->vllm==0.1.2)\r\n#10 131.9   Obtaining dependency information for referencing>=0.28.4 from https://files.pythonhosted.org/packages/ea/c3/f75f0ce2cdacca3d68a70b1756635092a1add1002e34afb4895b9fb62598/referencing-0.30.0-py3-none-any.whl.metadata\r\n#10 132.0   Downloading referencing-0.30.0-py3-none-any.whl.metadata (2.7 kB)\r\n#10 132.3 Collecting rpds-py>=0.7.1 (from jsonschema->ray>=2.5.1->vllm==0.1.2)\r\n#10 132.3   Obtaining dependency information for rpds-py>=0.7.1 from https://files.pythonhosted.org/packages/e2/26/69fd9b7e0ec9c2d710eae3eac5db157f5384b7717f2342596948c14cb6a3/rpds_py-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n#10 132.3   Downloading rpds_py-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\r\n#10 132.4 Collecting wavedrom (from markdown2[all]->fschat->vllm==0.1.2)\r\n#10 132.5   Downloading wavedrom-2.0.3.post3.tar.gz (137 kB)\r\n#10 132.5      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 137.7/137.7 kB 17.7 MB/s eta 0:00:00\r\n#10 132.5   Preparing metadata (setup.py): started\r\n#10 136.7   Preparing metadata (setup.py): finished with status 'done'\r\n#10 137.0 Collecting charset-normalizer<4,>=2 (from requests->ray>=2.5.1->vllm==0.1.2)\r\n#10 137.0   Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/a4/65/057bf29660aae6ade0816457f8db4e749e5c0bfa2366eb5f67db9912fa4c/charset_normalizer-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n#10 137.1   Downloading charset_normalizer-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\r\n#10 137.2 Collecting urllib3<3,>=1.21.1 (from requests->ray>=2.5.1->vllm==0.1.2)\r\n#10 137.2   Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/9b/81/62fd61001fa4b9d0df6e31d47ff49cfa9de4af03adecf339c7bc30656b37/urllib3-2.0.4-py3-none-any.whl.metadata\r\n#10 137.2   Downloading urllib3-2.0.4-py3-none-any.whl.metadata (6.6 kB)\r\n#10 137.3 Collecting mpmath>=0.19 (from sympy->torch>=2.0.0->vllm==0.1.2)\r\n#10 137.3   Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n#10 137.6 Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->fschat->vllm==0.1.2)\r\n#10 137.6   Obtaining dependency information for GitPython!=3.1.29,>=1.0.0 from https://files.pythonhosted.org/packages/67/50/742c2fb60989b76ccf7302c7b1d9e26505d7054c24f08cc7ec187faaaea7/GitPython-3.1.32-py3-none-any.whl.metadata\r\n#10 137.6   Downloading GitPython-3.1.32-py3-none-any.whl.metadata (10.0 kB)\r\n#10 137.8 Collecting sentry-sdk>=1.0.0 (from wandb->fschat->vllm==0.1.2)\r\n#10 137.8   Obtaining dependency information for sentry-sdk>=1.0.0 from https://files.pythonhosted.org/packages/8b/ef/cee575cda78f419a76ac9be4830f136c16bc2d90f00720f03b70bf7d8a6d/sentry_sdk-1.28.1-py2.py3-none-any.whl.metadata\r\n#10 137.8   Downloading sentry_sdk-1.28.1-py2.py3-none-any.whl.metadata (8.8 kB)\r\n#10 137.9 Collecting docker-pycreds>=0.4.0 (from wandb->fschat->vllm==0.1.2)\r\n#10 137.9   Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\r\n#10 138.0 Collecting pathtools (from wandb->fschat->vllm==0.1.2)\r\n#10 138.0   Downloading pathtools-0.1.2.tar.gz (11 kB)\r\n#10 138.0   Preparing metadata (setup.py): started\r\n#10 138.3   Preparing metadata (setup.py): finished with status 'done'\r\n#10 138.4 Collecting setproctitle (from wandb->fschat->vllm==0.1.2)\r\n#10 138.5   Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\r\n#10 138.5 Collecting appdirs>=1.4.3 (from wandb->fschat->vllm==0.1.2)\r\n#10 138.6   Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\n#10 138.8 Collecting toolz (from altair>=4.2.0->gradio==3.23->fschat->vllm==0.1.2)\r\n#10 138.8   Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\r\n#10 138.8      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 55.8/55.8 kB 9.4 MB/s eta 0:00:00\r\n#10 138.9 Collecting exceptiongroup (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->vllm==0.1.2)\r\n#10 138.9   Obtaining dependency information for exceptiongroup from https://files.pythonhosted.org/packages/fe/17/f43b7c9ccf399d72038042ee72785c305f6c6fdc6231942f8ab99d995742/exceptiongroup-1.1.2-py3-none-any.whl.metadata\r\n#10 139.0   Downloading exceptiongroup-1.1.2-py3-none-any.whl.metadata (6.1 kB)\r\n#10 139.1 Collecting six>=1.4.0 (from docker-pycreds>=0.4.0->wandb->fschat->vllm==0.1.2)\r\n#10 139.1   Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\r\n#10 139.2 Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->fschat->vllm==0.1.2)\r\n#10 139.2   Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\r\n#10 139.2      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 62.7/62.7 kB 9.3 MB/s eta 0:00:00\r\n#10 139.4 Collecting mdurl~=0.1 (from markdown-it-py[linkify]>=2.0.0->gradio==3.23->fschat->vllm==0.1.2)\r\n#10 139.5   Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\r\n#10 139.5 Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio==3.23->fschat->vllm==0.1.2)\r\n#10 139.6   Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\r\n#10 139.6 INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\r\n#10 139.6 Collecting mdit-py-plugins<=0.3.3 (from gradio==3.23->fschat->vllm==0.1.2)\r\n#10 139.6   Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\r\n#10 139.7      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 50.4/50.4 kB 10.4 MB/s eta 0:00:00\r\n#10 139.7   Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\r\n#10 139.7      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 46.5/46.5 kB 11.1 MB/s eta 0:00:00\r\n#10 139.8   Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\r\n#10 139.8      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 43.7/43.7 kB 7.4 MB/s eta 0:00:00\r\n#10 139.8   Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\r\n#10 139.8      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 41.0/41.0 kB 9.4 MB/s eta 0:00:00\r\n#10 139.9   Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\r\n#10 139.9      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 41.0/41.0 kB 9.0 MB/s eta 0:00:00\r\n#10 139.9   Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\r\n#10 140.0   Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\r\n#10 140.0 INFO: pip is still looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\r\n#10 140.0   Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\r\n#10 140.1   Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\r\n#10 140.1   Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\r\n#10 140.2   Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\r\n#10 140.3   Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\r\n#10 140.3 INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\r\n#10 140.3   Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\r\n#10 140.4 Collecting markdown-it-py[linkify]>=2.0.0 (from gradio==3.23->fschat->vllm==0.1.2)\r\n#10 140.4   Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\r\n#10 140.4      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 84.5/84.5 kB 11.5 MB/s eta 0:00:00\r\n#10 140.7 Collecting python-dateutil>=2.8.2 (from pandas->gradio==3.23->fschat->vllm==0.1.2)\r\n#10 140.8   Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\r\n#10 140.8      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 247.7/247.7 kB 36.1 MB/s eta 0:00:00\r\n#10 140.9 Collecting pytz>=2020.1 (from pandas->gradio==3.23->fschat->vllm==0.1.2)\r\n#10 140.9   Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\r\n#10 140.9      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 502.3/502.3 kB 49.5 MB/s eta 0:00:00\r\n#10 141.0 Collecting tzdata>=2022.1 (from pandas->gradio==3.23->fschat->vllm==0.1.2)\r\n#10 141.0   Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\r\n#10 141.1      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 341.8/341.8 kB 40.2 MB/s eta 0:00:00\r\n#10 142.0 Collecting multidict<7.0,>=4.5 (from aiohttp->gradio==3.23->fschat->vllm==0.1.2)\r\n#10 142.0   Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\r\n#10 142.0      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 114.5/114.5 kB 19.8 MB/s eta 0:00:00\r\n#10 142.1 Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->gradio==3.23->fschat->vllm==0.1.2)\r\n#10 142.1   Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\r\n#10 142.5 Collecting yarl<2.0,>=1.0 (from aiohttp->gradio==3.23->fschat->vllm==0.1.2)\r\n#10 142.5   Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\r\n#10 142.5      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 268.8/268.8 kB 37.4 MB/s eta 0:00:00\r\n#10 142.9 Collecting contourpy>=1.0.1 (from matplotlib->gradio==3.23->fschat->vllm==0.1.2)\r\n#10 142.9   Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/aa/55/02c6d24804592b862b38a85c9b3283edc245081390a520ccd11697b6b24f/contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n#10 142.9   Downloading contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\r\n#10 143.0 Collecting cycler>=0.10 (from matplotlib->gradio==3.23->fschat->vllm==0.1.2)\r\n#10 143.1   Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\r\n#10 143.2 Collecting fonttools>=4.22.0 (from matplotlib->gradio==3.23->fschat->vllm==0.1.2)\r\n#10 143.2   Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/e5/3d/000faec66c11733a0bf9f9a3a7b69290329cc8b3799228fe33eb0707dc7b/fonttools-4.41.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n#10 143.3   Downloading fonttools-4.41.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\r\n#10 143.3      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 149.4/149.4 kB 23.8 MB/s eta 0:00:00\r\n#10 143.4 Collecting kiwisolver>=1.0.1 (from matplotlib->gradio==3.23->fschat->vllm==0.1.2)\r\n#10 143.5   Downloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\r\n#10 143.5      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.6/1.6 MB 51.0 MB/s eta 0:00:00\r\n#10 143.7 Collecting pyparsing<3.1,>=2.3.1 (from matplotlib->gradio==3.23->fschat->vllm==0.1.2)\r\n#10 143.7   Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\r\n#10 143.7      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 98.3/98.3 kB 14.4 MB/s eta 0:00:00\r\n#10 144.0 Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers>=0.0.19->vllm==0.1.2)\r\n#10 144.0   Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\r\n#10 144.1 Collecting svgwrite (from wavedrom->markdown2[all]->fschat->vllm==0.1.2)\r\n#10 144.1   Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\r\n#10 144.1      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 67.1/67.1 kB 11.6 MB/s eta 0:00:00\r\n#10 144.4 Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->fschat->vllm==0.1.2)\r\n#10 144.4   Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\r\n#10 144.6 Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.23->fschat->vllm==0.1.2)\r\n#10 144.6   Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\r\n#10 145.0 Downloading pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\r\n#10 145.1    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.1/3.1 MB 48.8 MB/s eta 0:00:00\r\n#10 145.1 Downloading ray-2.5.1-cp310-cp310-manylinux2014_x86_64.whl (56.2 MB)\r\n#10 146.1    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.2/56.2 MB 28.4 MB/s eta 0:00:00\r\n#10 146.1 Downloading numpy-1.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\r\n#10 146.4    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 17.6/17.6 MB 55.7 MB/s eta 0:00:00\r\n#10 146.4 Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\r\n#10 146.6    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.4/7.4 MB 63.6 MB/s eta 0:00:00\r\n#10 146.6 Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\r\n#10 148.4    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 109.1/109.1 MB 21.3 MB/s eta 0:00:00\r\n#10 148.4 Downloading fastapi-0.100.0-py3-none-any.whl (65 kB)\r\n#10 148.4    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 65.7/65.7 kB 10.3 MB/s eta 0:00:00\r\n#10 148.4 Downloading uvicorn-0.23.1-py3-none-any.whl (59 kB)\r\n#10 148.5    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 59.5/59.5 kB 9.2 MB/s eta 0:00:00\r\n#10 148.5 Downloading click-8.1.6-py3-none-any.whl (97 kB)\r\n#10 148.5    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 97.9/97.9 kB 17.4 MB/s eta 0:00:00\r\n#10 148.5 Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\r\n#10 148.5    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 268.8/268.8 kB 35.8 MB/s eta 0:00:00\r\n#10 148.6 Downloading prompt_toolkit-3.0.39-py3-none-any.whl (385 kB)\r\n#10 148.6    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 385.2/385.2 kB 46.1 MB/s eta 0:00:00\r\n#10 148.8 Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\r\n#10 148.8    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 304.5/304.5 kB 27.2 MB/s eta 0:00:00\r\n#10 148.8 Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\r\n#10 148.8    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 705.5/705.5 kB 43.3 MB/s eta 0:00:00\r\n#10 148.9 Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\r\n#10 148.9    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 770.4/770.4 kB 43.2 MB/s eta 0:00:00\r\n#10 148.9 Downloading rich-13.4.2-py3-none-any.whl (239 kB)\r\n#10 148.9    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 239.4/239.4 kB 24.7 MB/s eta 0:00:00\r\n#10 149.0 Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\r\n#10 149.0    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 67.0/67.0 kB 8.5 MB/s eta 0:00:00\r\n#10 149.0 Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\r\n#10 149.1 Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\r\n#10 149.1    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 244.2/244.2 kB 24.4 MB/s eta 0:00:00\r\n#10 149.1 Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\r\n#10 149.1    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 225.7/225.7 kB 24.4 MB/s eta 0:00:00\r\n#10 149.1 Using cached filelock-3.12.2-py3-none-any.whl (10 kB)\r\n#10 149.2 Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\r\n#10 149.2    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 75.4/75.4 kB 9.6 MB/s eta 0:00:00\r\n#10 149.2 Downloading jsonschema-4.18.4-py3-none-any.whl (80 kB)\r\n#10 149.2    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 81.0/81.0 kB 11.0 MB/s eta 0:00:00\r\n#10 149.2 Downloading requests-2.31.0-py3-none-any.whl (62 kB)\r\n#10 149.3    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 62.6/62.6 kB 8.8 MB/s eta 0:00:00\r\n#10 149.3 Downloading wandb-0.15.5-py3-none-any.whl (2.1 MB)\r\n#10 149.3    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.1/2.1 MB 53.6 MB/s eta 0:00:00\r\n#10 149.4 Downloading altair-5.0.1-py3-none-any.whl (471 kB)\r\n#10 149.4    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 471.5/471.5 kB 37.3 MB/s eta 0:00:00\r\n#10 149.4 Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\r\n#10 149.4    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 80.9/80.9 kB 11.6 MB/s eta 0:00:00\r\n#10 149.5 Downloading charset_normalizer-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (201 kB)\r\n#10 149.5    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 201.8/201.8 kB 22.0 MB/s eta 0:00:00\r\n#10 149.5 Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\r\n#10 149.5    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 188.5/188.5 kB 20.3 MB/s eta 0:00:00\r\n#10 149.6 Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\r\n#10 149.6    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 74.5/74.5 kB 9.5 MB/s eta 0:00:00\r\n#10 149.6 Downloading jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)\r\n#10 149.6 Using cached MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\r\n#10 149.7 Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\r\n#10 149.9    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.3/12.3 MB 65.4 MB/s eta 0:00:00\r\n#10 149.9 Downloading referencing-0.30.0-py3-none-any.whl (25 kB)\r\n#10 149.9 Downloading rpds_py-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\r\n#10 149.9    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.2/1.2 MB 66.4 MB/s eta 0:00:00\r\n#10 150.0 Downloading sentry_sdk-1.28.1-py2.py3-none-any.whl (214 kB)\r\n#10 150.0    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 214.7/214.7 kB 27.9 MB/s eta 0:00:00\r\n#10 150.0 Downloading urllib3-2.0.4-py3-none-any.whl (123 kB)\r\n#10 150.0    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 123.9/123.9 kB 19.4 MB/s eta 0:00:00\r\n#10 150.1 Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\r\n#10 150.1    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.0/1.0 MB 64.9 MB/s eta 0:00:00\r\n#10 150.2 Using cached cmake-3.27.0-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\r\n#10 150.3 Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\r\n#10 150.3    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 163.8/163.8 kB 33.2 MB/s eta 0:00:00\r\n#10 150.3 Downloading markdown2-2.4.9-py2.py3-none-any.whl (39 kB)\r\n#10 150.3 Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\r\n#10 150.5    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11.6/11.6 MB 61.7 MB/s eta 0:00:00\r\n#10 150.5 Downloading Pillow-10.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\r\n#10 150.6    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 55.2 MB/s eta 0:00:00\r\n#10 150.6 Downloading orjson-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\r\n#10 150.6    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 138.7/138.7 kB 20.0 MB/s eta 0:00:00\r\n#10 150.7 Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\r\n#10 150.7 Downloading contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\r\n#10 150.7    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 300.7/300.7 kB 34.1 MB/s eta 0:00:00\r\n#10 150.8 Downloading fonttools-4.41.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\r\n#10 150.8    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4.3/4.3 MB 72.7 MB/s eta 0:00:00\r\n#10 150.9 Downloading exceptiongroup-1.1.2-py3-none-any.whl (14 kB)\r\n#10 151.4 Building wheels for collected packages: vllm, ffmpy, pathtools, wavedrom\r\n#10 151.4   Building editable for vllm (pyproject.toml): started\r\n#10 217.8   Building editable for vllm (pyproject.toml): still running...\r\n#10 299.4   Building editable for vllm (pyproject.toml): still running...\r\n#10 360.2   Building editable for vllm (pyproject.toml): still running...\r\n#10 422.2   Building editable for vllm (pyproject.toml): still running...\r\n#10 484.0   Building editable for vllm (pyproject.toml): still running...\r\n#10 485.2   Building editable for vllm (pyproject.toml): finished with status 'done'\r\n#10 485.2   Created wheel for vllm: filename=vllm-0.1.2-0.editable-cp310-cp310-linux_x86_64.whl size=8447 sha256=bb9cb10f442e5c705e8e675ae3a42a75c0f7e21c5bfc235739098dc4750bcd76\r\n#10 485.2   Stored in directory: /tmp/pip-ephem-wheel-cache-21f6p5js/wheels/61/90/8e/df32f4c5b947476bbfa504d343595e2cd0c99019c48e878a86\r\n#10 485.2   Building wheel for ffmpy (setup.py): started\r\n#10 485.5   Building wheel for ffmpy (setup.py): finished with status 'done'\r\n#10 485.5   Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5596 sha256=7778ed64cc7cb26d23f1279c00efcf1a383239f9d1e3658f1701c4d1452b53eb\r\n#10 485.5   Stored in directory: /home/vllm/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\r\n#10 485.5   Building wheel for pathtools (setup.py): started\r\n#10 485.8   Building wheel for pathtools (setup.py): finished with status 'done'\r\n#10 485.8   Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=89a04102b6eb416abc0ddbe3063db4db5c75082c9705673643bd960fe2ca3c68\r\n#10 485.8   Stored in directory: /home/vllm/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\r\n#10 485.8   Building wheel for wavedrom (setup.py): started\r\n#10 486.2   Building wheel for wavedrom (setup.py): finished with status 'done'\r\n#10 486.2   Created wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=29952 sha256=25e7b144b1a9c01ee035a6b62130e94a0fcd212628968c5b1d5b2e0f8509ab93\r\n#10 486.2   Stored in directory: /home/vllm/.cache/pip/wheels/9c/52/8c/38b454b42f712f325e26f633287484c7dc1ad469e1580c5954\r\n#10 486.2 Successfully built vllm ffmpy pathtools wavedrom\r\n#10 487.5 Installing collected packages: wcwidth, tokenizers, sentencepiece, safetensors, pytz, pydub, pathtools, ninja, msgpack, mpmath, lit, ffmpy, cmake, appdirs, websockets, urllib3, uc-micro-py, tzdata, typing-extensions, tqdm, toolz, sympy, svgwrite, sniffio, smmap, six, shortuuid, setproctitle, semantic-version, rpds-py, regex, pyyaml, python-multipart, pyparsing, pygments, psutil, protobuf, prompt-toolkit, pillow, packaging, orjson, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, networkx, mypy-extensions, multidict, mdurl, markupsafe, markdown2, kiwisolver, idna, h11, grpcio, fsspec, frozenlist, fonttools, filelock, exceptiongroup, cycler, click, charset-normalizer, certifi, attrs, async-timeout, aiofiles, yarl, wavedrom, uvicorn, typing-inspect, sentry-sdk, requests, referencing, python-dateutil, pydantic, nvidia-cusolver-cu11, nvidia-cudnn-cu11, markdown-it-py, linkify-it-py, jinja2, gitdb, docker-pycreds, contourpy, anyio, aiosignal, starlette, rich, pyre-extensions, pandas, mdit-py-plugins, matplotlib, jsonschema-specifications, huggingface-hub, httpcore, GitPython, aiohttp, wandb, transformers, jsonschema, httpx, fastapi, ray, altair, gradio, triton, torch, accelerate, xformers, fschat, vllm\r\n#10 542.4 Successfully installed GitPython-3.1.32 accelerate-0.21.0 aiofiles-23.1.0 aiohttp-3.8.5 aiosignal-1.3.1 altair-5.0.1 anyio-3.7.1 appdirs-1.4.4 async-timeout-4.0.2 attrs-23.1.0 certifi-2023.5.7 charset-normalizer-3.2.0 click-8.1.6 cmake-3.27.0 contourpy-1.1.0 cycler-0.11.0 docker-pycreds-0.4.0 exceptiongroup-1.1.2 fastapi-0.100.0 ffmpy-0.3.1 filelock-3.12.2 fonttools-4.41.0 frozenlist-1.4.0 fschat-0.2.3 fsspec-2023.6.0 gitdb-4.0.10 gradio-3.23.0 grpcio-1.51.3 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 huggingface-hub-0.16.4 idna-3.4 jinja2-3.1.2 jsonschema-4.18.4 jsonschema-specifications-2023.7.1 kiwisolver-1.4.4 linkify-it-py-2.0.2 lit-16.0.6 markdown-it-py-2.2.0 markdown2-2.4.9 markupsafe-2.1.3 matplotlib-3.7.2 mdit-py-plugins-0.3.3 mdurl-0.1.2 mpmath-1.3.0 msgpack-1.0.5 multidict-6.0.4 mypy-extensions-1.0.0 networkx-3.1 ninja-1.11.1 numpy-1.25.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 orjson-3.9.2 packaging-23.1 pandas-2.0.3 pathtools-0.1.2 pillow-10.0.0 prompt-toolkit-3.0.39 protobuf-4.23.4 psutil-5.9.5 pydantic-1.10.11 pydub-0.25.1 pygments-2.15.1 pyparsing-3.0.9 pyre-extensions-0.0.29 python-dateutil-2.8.2 python-multipart-0.0.6 pytz-2023.3 pyyaml-6.0.1 ray-2.5.1 referencing-0.30.0 regex-2023.6.3 requests-2.31.0 rich-13.4.2 rpds-py-0.9.2 safetensors-0.3.1 semantic-version-2.10.0 sentencepiece-0.1.99 sentry-sdk-1.28.1 setproctitle-1.3.2 shortuuid-1.0.11 six-1.16.0 smmap-5.0.0 sniffio-1.3.0 starlette-0.27.0 svgwrite-1.4.3 sympy-1.12 tokenizers-0.13.3 toolz-0.12.0 torch-2.0.1 tqdm-4.65.0 transformers-4.31.0 triton-2.0.0 typing-extensions-4.7.1 typing-inspect-0.9.0 tzdata-2023.3 uc-micro-py-1.0.2 urllib3-2.0.4 uvicorn-0.23.1 vllm-0.1.2 wandb-0.15.5 wavedrom-2.0.3.post3 wcwidth-0.2.6 websockets-11.0.3 xformers-0.0.20 yarl-1.9.2\r\n#10 544.6 Files removed: 328\r\n#10 DONE 546.1s\r\n\r\n</details>\r\n\r\nThis has really confused me because this was working fine until recently, and I can't see any commits in vLLM that look like they should affect this.  Unless it's a change in FastChat that's broken it?\r\n\r\nThanks in advance for any help.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-07-20T21:35:25+00:00",
    "closed_at": "2023-08-02T18:05:31+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/537/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/537"
  },
  {
    "number": 14397,
    "title": "[Bug]: `triton_scaled_mm` never used on ROCm",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 03-07 02:02:58 [__init__.py:207] Automatically detected platform rocm.\nCollecting environment information...\nPyTorch version: 2.5.1+rocm6.2\nIs debug build: False\nCUDA used to build PyTorch: N/A\nROCM used to build PyTorch: 6.2.41133-dd7f95766\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 10.5.0-1ubuntu1~22.04) 10.5.0\nClang version: Could not collect\nCMake version: version 3.31.6\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-131-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: AMD Instinct MI300X (gfx942:sramecc+:xnack-)\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\nHIP runtime version: 6.2.41133\nMIOpen runtime version: 3.2.0\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               104\nOn-line CPU(s) list:                  0-103\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8470\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   1\nCore(s) per socket:                   52\nSocket(s):                            2\nStepping:                             8\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nL1d cache:                            4.9 MiB (104 instances)\nL1i cache:                            3.3 MiB (104 instances)\nL2 cache:                             208 MiB (104 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-51\nNUMA node1 CPU(s):                    52-103\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] pytorch-triton-rocm==3.1.0\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1+rocm6.2\n[pip3] torchaudio==2.5.1+rocm6.2\n[pip3] torchvision==0.20.1+rocm6.2\n[pip3] transformers==4.49.0\n[conda] Could not collect\nROCM Version: 6.2.41134-65d174c3e\nNeuron SDK Version: N/A\nvLLM Version: 0.7.4.dev189+gae122b1cb\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n============================ ROCm System Management Interface ============================\n================================ Weight between two GPUs =================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \nGPU0   0            15           15           15           15           15           15           15           \nGPU1   15           0            15           15           15           15           15           15           \nGPU2   15           15           0            15           15           15           15           15           \nGPU3   15           15           15           0            15           15           15           15           \nGPU4   15           15           15           15           0            15           15           15           \nGPU5   15           15           15           15           15           0            15           15           \nGPU6   15           15           15           15           15           15           0            15           \nGPU7   15           15           15           15           15           15           15           0            \n\n================================= Hops between two GPUs ==================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \nGPU0   0            1            1            1            1            1            1            1            \nGPU1   1            0            1            1            1            1            1            1            \nGPU2   1            1            0            1            1            1            1            1            \nGPU3   1            1            1            0            1            1            1            1            \nGPU4   1            1            1            1            0            1            1            1            \nGPU5   1            1            1            1            1            0            1            1            \nGPU6   1            1            1            1            1            1            0            1            \nGPU7   1            1            1            1            1            1            1            0            \n\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         \nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         \nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         \nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         \nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         \nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            \n\n======================================= Numa Nodes =======================================\nGPU[0]          : (Topology) Numa Node: 0\nGPU[0]          : (Topology) Numa Affinity: 0\nGPU[1]          : (Topology) Numa Node: 0\nGPU[1]          : (Topology) Numa Affinity: 0\nGPU[2]          : (Topology) Numa Node: 0\nGPU[2]          : (Topology) Numa Affinity: 0\nGPU[3]          : (Topology) Numa Node: 0\nGPU[3]          : (Topology) Numa Affinity: 0\nGPU[4]          : (Topology) Numa Node: 1\nGPU[4]          : (Topology) Numa Affinity: 1\nGPU[5]          : (Topology) Numa Node: 1\nGPU[5]          : (Topology) Numa Affinity: 1\nGPU[6]          : (Topology) Numa Node: 1\nGPU[6]          : (Topology) Numa Affinity: 1\nGPU[7]          : (Topology) Numa Node: 1\nGPU[7]          : (Topology) Numa Affinity: 1\n================================== End of ROCm SMI Log ===================================\n\nLD_LIBRARY_PATH=/home/luka/git/vllm/.venv/lib/python3.10/site-packages/cv2/../../lib64:\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI found an issue with vLLM and block fp8 linear, where the ROCm platform is incorrectly using a cutlass execution path. Because the cutlass path is always disabled on ROCm, this kernel is never reached, and instead we fall back on either `w8a8_block_fp8_matmul` or `torch.scaled_mm`.\n\nThe way we got there:\n- @rasmith added the triton kernel `triton_scaled_mm` into `custom_ops.cutlass_scaled_mm` (not the right place for it in my opinion) in [127c074](https://github.com/vllm-project/vllm/commit/127c07480ecea15e4c2990820c457807ff78a057)\n- @hongxiayang added DeepSeek support, using the cutlass path where cutlass_block_fp8_supported was True by default in [c36ac98](https://github.com/vllm-project/vllm/commit/c36ac98d0118537ec5f3f405a68311a10f9b59a5)\n- @LucasWilkinson fixed the default of `cutlass_block_fp8_supported` param to `cutlass_block_fp8_supported()` which always returns False on ROCm in [76abd0c](https://github.com/vllm-project/vllm/commit/76abd0c88143419826bfc13d2cd29669d0fdfa1b).\n\nThe effect of this is that triton_scaled_mm is currently never used.\n\nI think the path forward is to move `triton_scaled_mm` out of the `custom_ops.cutlass_scaled_mm`. This should likely be done as part of larger refactoring of the FP8 code, including the new `Fp8LinearOp` added in #14390. Additionally, it would be good so (at least somewhat) unify the `triton_scaled_mm` with `w8a8_block_fp8_matmul`, which is the fallback for `apply_block_fp8_linear`.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "good first issue",
      "unstale"
    ],
    "state": "open",
    "created_at": "2025-03-07T02:04:44+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14397/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14397"
  },
  {
    "number": 13429,
    "title": "[Bug]: [V1][Core] Structured decoding - Decouple Json from Json Object",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen using json_object = True for guided decoding (as per V0 guidelines), V1 (based on #12388 ) expects us to have a json_schema present for it as well. In nature both json_object and json need to be decoupled as per this [code snippet](https://github.com/vllm-project/vllm/pull/12388/files#diff-35f85e99eae8897d78a45f6a8d21bb69f9d8fe4a51e072bf299118dadac612f3R160) since json_object would inherently use the JsonGrammar compiled by xgrammar backend and would not require a Json Schema for it. \n\n```\nfrom vllm import LLM, SamplingParams\n\nprompts = [\n    \"Hello, my name is\",\n    \"The president of the United States is\",\n    \"The capital of France is\",\n    \"The future of AI is\",\n]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95, guided_decoding=GuidedDecodingParams(\n                                        json_object=True,\n                                         backend=\"xgrammar\"))\n\nllm = LLM(model=\"facebook/opt-125m\")\n\noutputs = llm.generate(prompts, sampling_params)\n\n# Print the outputs.\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```\n\nError : \n```ERROR 02-17 22:27:52 core.py:235]   File \"/host/vllm/vllm/v1/request.py\", line 179, in guided_decoding_key\nERROR 02-17 22:27:52 core.py:235]     raise ValueError(\"No valid guided decoding parameter found\")\nERROR 02-17 22:27:52 core.py:235] ValueError: No valid guided decoding parameter found\nERROR 02-17 22:27:52 core.py:235] ```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-17T22:38:47+00:00",
    "closed_at": "2025-05-22T14:15:39+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13429/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13429"
  },
  {
    "number": 20804,
    "title": "[Bug]: JinaVLForRanking 400 BadRequest",
    "body": "### Your current environment\n\n<details>\n\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nThis merger adds support for the JinaVL Reranker model ([[#20260](https://github.com/vllm-project/vllm/pull/20260)]). However, when I started the service and ran test calls, it returned a 400 Bad Request error.\n### Online Serving\n```text\nvllm serve jinaai/jina-reranker-m0\n```\n### Request\n```text\ncurl -X 'POST' \\\n  'http://127.0.0.1:8000/v1/rerank' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"model\": \"jinaai/jina-reranker-m0\",\n  \"query\": \"slm markdown\",\n  \"documents\": {\n    \"content\": [\n      {\n        \"type\": \"image_url\",\n        \"image_url\": {\n          \"url\": \"https://raw.githubusercontent.com/jina-ai/multimodal-reranker-test/main/handelsblatt-preview.png\"\n        }\n      },\n      {\n        \"type\": \"image_url\",\n        \"image_url\": {\n          \"url\": \"https://raw.githubusercontent.com/jina-ai/multimodal-reranker-test/main/paper-11.png\"\n        }\n      }\n    ]\n  }\n}'\n```\n### Response\n```text\n{\"object\":\"error\",\"message\":\"1 validation error for RerankDocument\\nmulti_modal.content\\n  Field required [type=missing, input_value={'image_url': {'url': 'ht...'}, 'type': 'image_url'}, input_type=dict]\\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\",\"type\":\"BadRequestError\",\"param\":null,\"code\":400}\n```\n\n### Problems and Solutions\nI reviewed the relevant code and identified the following issues:\nIn vllm/entrypoints/openai/serving_score.py:485, we use documents[\"content\"][idx]) to initialize RerankDocument\n```python\n...\n            result = RerankResult(\n                index=idx,\n                document=RerankDocument(text=documents[idx]) if isinstance(\n                    documents, list) else RerankDocument(\n                        multi_modal=documents[\"content\"][idx]),\n                relevance_score=classify_res.outputs.score,\n            )\n...\n```\nFor RerankDocument, the multi_modal attribute should be an Optional[ScoreMultiModalParam] per its definition(vllm/vllm/entrypoints/openai/protocol.py:1351). However, during the function call, the documents variable is a ScoreMultiModalParam type, and documents[\"content\"][idx] is actually a ScoreContentPartParam(vllm/vllm/entrypoints/score_utils.py:35).\n\nAfter modifying the definition of RerankDocument.multi_modal to ScoreContentPartParam, the service invocation now functions correctly.\n\nI'd like to submit a PR to fix this type definition issue. Thank you.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-07-11T08:04:59+00:00",
    "closed_at": "2025-07-13T14:32:41+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20804/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20804"
  },
  {
    "number": 12625,
    "title": "[Bug]: stack trace for \"Watchdog caught collective operation timeout\"",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 01-31 10:13:08 __init__.py:183] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-124-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA H100 80GB HBM3\nGPU 1: NVIDIA H100 80GB HBM3\nGPU 2: NVIDIA H100 80GB HBM3\nGPU 3: NVIDIA H100 80GB HBM3\nGPU 4: NVIDIA H100 80GB HBM3\nGPU 5: NVIDIA H100 80GB HBM3\nGPU 6: NVIDIA H100 80GB HBM3\nGPU 7: NVIDIA H100 80GB HBM3\n\nNvidia driver version: 550.90.07\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        43 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               128\nOn-line CPU(s) list:                  0-127\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8468\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   32\nSocket(s):                            2\nStepping:                             8\nBogoMIPS:                             4200.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk avx512_fp16 arch_capabilities\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            4 MiB (128 instances)\nL1i cache:                            4 MiB (128 instances)\nL2 cache:                             256 MiB (64 instances)\nL3 cache:                             32 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-63\nNUMA node1 CPU(s):                    64-127\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Unknown: No mitigations\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Mitigation; TSX disabled\n\nVersions of relevant libraries:\n[pip3] flashinfer==0.1.6+cu121torch2.4\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.1\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.0\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t0-63\t0\t\tN/A\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t0-63\t0\t\tN/A\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\t0-63\t0\t\tN/A\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\t0-63\t0\t\tN/A\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\t64-127\t1\t\tN/A\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\t64-127\t1\t\tN/A\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\t64-127\t1\t\tN/A\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \t64-127\t1\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNVIDIA_VISIBLE_DEVICES=all\nNVIDIA_REQUIRE_CUDA=cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nVLLM_USAGE_SOURCE=production-docker-image\nCUDA_VERSION=12.1.0\nCUDA_VISIBLE_DEVICES=0,1,2,3\nCUDA_VISIBLE_DEVICES=0,1,2,3\nVLLM_FLASH_ATTN_VERSION=3\nLD_LIBRARY_PATH=/usr/local/lib/python3.12/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### Model Input Dumps\n\nUnfortunately, the problematic inputs cannot be pickled for some reason, here is what happens after `Error in model execution`:\n```\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240] RuntimeError: Error in model execution: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)\n(VllmWorkerProcess pid=351) INFO 01-29 20:00:47 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20250129-200047.pkl...\n(VllmWorkerProcess pid=351) WARNING 01-29 20:00:47 model_runner_base.py:143] Failed to pickle inputs of failed execution: CUDA error: an illegal memory access was encountered\n(VllmWorkerProcess pid=351) WARNING 01-29 20:00:47 model_runner_base.py:143] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n(VllmWorkerProcess pid=351) WARNING 01-29 20:00:47 model_runner_base.py:143] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n(VllmWorkerProcess pid=351) WARNING 01-29 20:00:47 model_runner_base.py:143] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n```\n\n\n<details>\n<summary><strong>And here is an example of a full trace</strong></summary>\n\n```\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop.\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240] Traceback (most recent call last):\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner_base.py\", line 116, in _wrapper\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     return func(*args, **kwargs)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1716, in execute_model\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     hidden_or_intermediate_states = model_executable(\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]                                     ^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     return self._call_impl(*args, **kwargs)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     return forward_call(*args, **kwargs)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 538, in forward\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     model_output = self.model(input_ids, positions, kv_caches,\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 170, in __call__\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     return self.forward(*args, **kwargs)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 363, in forward\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     hidden_states, residual = layer(positions, hidden_states,\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     return self._call_impl(*args, **kwargs)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     return forward_call(*args, **kwargs)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 277, in forward\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     hidden_states = self.self_attn(positions=positions,\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     return self._call_impl(*args, **kwargs)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     return forward_call(*args, **kwargs)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 202, in forward\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     output, _ = self.o_proj(attn_output)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]                 ^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     return self._call_impl(*args, **kwargs)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     return forward_call(*args, **kwargs)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py\", line 1143, in forward\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     output = tensor_model_parallel_all_reduce(output_parallel)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/communication_op.py\", line 11, in tensor_model_parallel_all_reduce\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     return get_tp_group().all_reduce(input_)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 347, in all_reduce\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     return torch.ops.vllm.all_reduce(input_, group_name=self.unique_name)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/torch/_ops.py\", line 1116, in __call__\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     return self._op(*args, **(kwargs or {}))\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 109, in all_reduce\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     return group._all_reduce_out_place(tensor)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 360, in _all_reduce_out_place\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     out = pynccl_comm.all_reduce(input_)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl.py\", line 124, in all_reduce\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     self.nccl.ncclAllReduce(buffer_type(in_tensor.data_ptr()),\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 288, in ncclAllReduce\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     self.NCCL_CHECK(self._funcs[\"ncclAllReduce\"](sendbuff, recvbuff, count,\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 254, in NCCL_CHECK\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     raise RuntimeError(f\"NCCL error: {error_str}\")\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240] RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240] \n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240] The above exception was the direct cause of the following exception:\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240] \n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240] Traceback (most recent call last):\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 234, in _run_worker_process\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     output = run_method(worker, method, args, kwargs)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2208, in run_method\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     return func(*args, **kwargs)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 90, in start_worker_execution_loop\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     output = self.execute_model(execute_model_req=None)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 410, in execute_model\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     output = self.model_runner.execute_model(\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     return func(*args, **kwargs)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/multi_step_model_runner.py\", line 538, in execute_model\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     output = self._base_model_runner.execute_model(frozen_model_input,\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     return func(*args, **kwargs)\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner_base.py\", line 146, in _wrapper\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240]     raise type(err)(f\"Error in model execution: \"\n(VllmWorkerProcess pid=350) ERROR 01-29 20:00:47 multiproc_worker_utils.py:240] RuntimeError: Error in model execution: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)\n(VllmWorkerProcess pid=351) INFO 01-29 20:00:47 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20250129-200047.pkl...\n(VllmWorkerProcess pid=351) WARNING 01-29 20:00:47 model_runner_base.py:143] Failed to pickle inputs of failed execution: CUDA error: an illegal memory access was encountered\n(VllmWorkerProcess pid=351) WARNING 01-29 20:00:47 model_runner_base.py:143] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n(VllmWorkerProcess pid=351) WARNING 01-29 20:00:47 model_runner_base.py:143] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n(VllmWorkerProcess pid=351) WARNING 01-29 20:00:47 model_runner_base.py:143] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n(VllmWorkerProcess pid=351) WARNING 01-29 20:00:47 model_runner_base.py:143] \n[rank3]:[E129 20:00:47.374645555 ProcessGroupNCCL.cpp:1595] [PG ID 2 PG GUID 3 Rank 3] Process group watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nException raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f31b24b9446 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f31b24636e4 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f31b25a5a18 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7f316841e726 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0xa0 (0x7f31684233f0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7f316842ab5a in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f316842c61d in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x145c0 (0x7f31b294e5c0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch.so)\nframe #8: <unknown function> + 0x94ac3 (0x7f31b3193ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #9: clone + 0x44 (0x7f31b3224a04 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n\nterminate called after throwing an instance of 'c10::DistBackendError'\n  what():  [PG ID 2 PG GUID 3 Rank 3] Process group watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nException raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f31b24b9446 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f31b24636e4 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f31b25a5a18 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7f316841e726 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0xa0 (0x7f31684233f0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7f316842ab5a in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f316842c61d in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x145c0 (0x7f31b294e5c0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch.so)\nframe #8: <unknown function> + 0x94ac3 (0x7f31b3193ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #9: clone + 0x44 (0x7f31b3224a04 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n\nException raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f31b24b9446 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0xe4271b (0x7f316809971b in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #2: <unknown function> + 0x145c0 (0x7f31b294e5c0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch.so)\nframe #3: <unknown function> + 0x94ac3 (0x7f31b3193ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #4: clone + 0x44 (0x7f31b3224a04 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n\nCRITICAL 01-29 20:00:47 launcher.py:99] MQLLMEngine is already dead, terminating server process\nINFO:     172.19.0.4:53840 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\nINFO:     Shutting down\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\nINFO:     Finished server process [7]\n```\n\n</details>\n\n### \ud83d\udc1b Describe the bug\n\nThis is a follow-up to this issue https://github.com/vllm-project/vllm/issues/6042 which was closed because of inactivity. The issue still exists as of v0.7.0 release and we would be able to provide stack-traces as initially requested in the issue (I pasted one in the \"Model Input Dump\" section of the issue; unfortunately the pickling of problematic inputs fails and the files produced are all empty).\n\nThe issue is hard to reproduce because it tends to happen after a while (sometimes a few hours, after vLLM has been running fine).\n\nThe issue happens with the following launch config:\n```sh\n      --host=0.0.0.0\n      --port=8080\n      --model=Llama-3.3-70B-Instruct-FP8-Dynamic\n      --disable-log-requests\n      --dtype=auto\n      --gpu-memory-utilization=0.92\n      --kv-cache-dtype=auto\n      --load-format=safetensors\n      --max-num-seqs=32\n      --num_scheduler_steps=16\n      --seed=0\n      --tensor-parallel-size=4\n      --tokenizer-mode=auto\n```\n\nOn Docker image: `vllm/vllm-openai:v0.7.0@sha256:a43963ed149a7b8b6c8c9dd028d4ab2be9fe804761d41b11cc07043a1edb61a8`\nModel is `Llama-3.3-70B-Instruct` quantized to dynamic FP8 (as produced by this script: https://github.com/vllm-project/llm-compressor/blob/main/examples/quantization_w8a8_fp8/llama3_example.py)\n\nWe have other models running with no tensor parallelism and they do not seem to exhibit this issue (it is unclear if any tensor parallelism has the issue, or specifically 4).\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-01-31T18:23:56+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12625/reactions",
      "total_count": 7,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12625"
  },
  {
    "number": 15116,
    "title": "[Feature]: Improve GPTQ implementation",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nAs we all know, quantizing some layers in the model will cause a large loss of accuracy. When using the AWQ algorithm, you can use the **modules_to_not_convert** attribute to avoid quantizing some layers. If the same function is also available in the GPTQ algorithm, I think it will be very convenient.\n\n### Alternatives\n\nI have pull a request [#12103](https://github.com/vllm-project/vllm/pull/12103)\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-19T09:06:52+00:00",
    "closed_at": "2025-07-18T02:28:06+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15116/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15116"
  },
  {
    "number": 10727,
    "title": "[RFC]: Implement disaggregated prefilling using Mooncake",
    "body": "### Motivation.\n\nDisaggregated prefilling/decoding is expected to achieve better performance (e.g., long documents) in LLM inference. [#5557](https://github.com/vllm-project/vllm/issues/5557) proposes a good paradigm. \r\n\r\nIn addition, the Transfer Engine of [Mooncake](https://github.com/kvcache-ai/mooncake), which is a KVCache-centric disaggregated architecture for LLM serving, is open-sourced. \r\n\r\nCompared with NCCL, Mooncake Transfer Engine has the following features:\r\n- a unified programming interface for data transfers between DRAM-to-DRAM (both local and remote), DRAM-to-GPU VRAM (both local and remote), and DRAM-to-remote NVMe devices\r\n- support for TCP, RDMA, and NVMe-of protocols\r\n- topology-aware path selection (link to our english doc, transfer_engine.md), aggregating bandwidth from multiple NICs\n\n### Proposed Change.\n\nThe plan is to integrate vLLM with Mooncake. Initially we have implemented a prototype that replaces nccl with Transfer Engine in the data plane. In the future, we are planning to develop Mooncake Store to fully support disaggregated prefilling (M prefill & N decode) and make it ready for production. Mooncake's architecture is [here](https://github.com/kvcache-ai/Mooncake/blob/main/doc/en/architecture.md).\r\n\r\nFeel free to use our prototype and comment about our design!\n\n### Feedback Period.\n\nSeveral weeks\r\n\r\n\n\n### CC List.\n\n@ShangmingCai @stmatengss  @james0zan \n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-28T00:27:22+00:00",
    "closed_at": "2025-03-28T02:04:55+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10727/reactions",
      "total_count": 6,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 5,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10727"
  },
  {
    "number": 16354,
    "title": "[Feature]: Benchmarks for audio models",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\n- Add audio datasets to `benchmarks/benchmark_dataset.py` to so we can run performance benchmarks on audio models as well.\n- Add a benchmark similar to MMMU (#11196) but for audio models to evaluate their correctness.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\ncc @mgoin @ywang96 \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "help wanted",
      "feature request",
      "multi-modality"
    ],
    "state": "closed",
    "created_at": "2025-04-09T16:55:19+00:00",
    "closed_at": "2025-04-19T09:24:15+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16354/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/16354"
  },
  {
    "number": 15735,
    "title": "[Roadmap] vLLM Roadmap Q2 2025",
    "body": "This page is accessible via [roadmap.vllm.ai](https://roadmap.vllm.ai/)\n\nThis is a living document! For each item here, we intend to link the RFC as well as discussion Slack channel in the [vLLM Slack](https://slack.vllm.ai)\n\n---\n\n#### Core Themes\n\n**Path to vLLM v1.0.0**  \n*We want to fully remove the V0 engine and clean up the codebase for unpopular and unsupported features. The v1.0.0 version of vLLM will be performant and easy to maintain, as well as modular and extensible, with backward compatibility.*\n\n- [ ] V1 core feature set  \n    - [x] Hybrid memory allocators  \n    - [ ] ~Jump decoding~\n    - [x] Redesigned native support for pipeline parallelism   \n    - [x] Redesigned spec decode  \n    - [ ] Redesigned sampler with modularity support\n- [ ] Close the feature gaps and fully remove V0  \n    - [x] Attention backends\n    - [ ] Pooling models  \n    - [ ] Mamba/Hybrid models  \n    - [ ] (TBD) encoder and encoder decoder  \n    - [x] Hardware support  \n- [ ] Performance  \n    - [ ] Further lower scheduler overhead   \n    - [x] Further enhance LoRA performance  \n    - [x] API Server Scale-out\n\n**Cluster Scale Serving**  \n*As the model expands in size, serving them in multi-node scale-out and disaggregating prefill and decode becomes the way to go. We are fully committed to making vLLM the best engine for cluster scale serving.* \n\n- [x] Data Parallelism  \n    - [x] Single node DP\n    - [x] API Server and Engine decoupling (any to any communication)   \n- [x] Expert Parallelism  \n    - [x] DeepEP and pplx integrations  \n    - [x] Transition from fused\\_moe to cutlass based grouped gemm.   \n- [ ] Online Reconfiguration (e.g. EPLB)  \n    - [ ] Online reconfiguration  \n    - [ ] Zero-overhead expert movement  \n- [ ] Prefill Decode Disaggregation  \n    - [x] 1P1D in V1: both symmetric TP/PP and asymmetric TP/PP  \n    - [x] XPYD  \n    - [x] Data Parallel Compatibility  \n    - [x] NIXL integration  \n    - [ ] Overhead Reduction & Performance Enhancements  \n- [ ] KV Cache Storage   \n    - [ ] Offload KV cache to CPU  \n    - [ ] Offload KV cache to disk  \n    - [x] Integration with Mooncake and LMCache  \n- [ ] DeepSeek Specific Enhancements  \n    - [x] MLA enhancements: TP, FlashAttention, FlashInfer, Blackwell Kernels.   \n    - [x] MTP enhancements: V1 support, further lower overhead.   \n- [ ] Others  \n    - [ ] Investigate communication and compute pipelining\n\n**vLLM for Production**  \n*vLLM is designed for production. We will continue to enhance stability and tune the systems around vLLM for optimal performance.* \n\n- [ ] Testing:   \n    - [ ] Comprehensive performance suite  \n    - [ ] Enhance accuracy testing coverage  \n    - [ ] Large-scale deployment \\+ testing  \n    - [ ] Stress and longevity testing  \n- [ ] Offer tuned recipes and analysis for different models and hardware combinations.   \n- [ ] Multi-platform wheels and containers for production use cases.\n\n#### Features\n\n**Models**\n\n- [ ] Scaling Omni Modality  \n- [ ] Long Context   \n- [ ] Stable OOT model registration interface   \n- [ ] Attention Sparsity: support the sparse mechanism for new models. \n\n**Use Case**\n\n- [ ] Enhance testing and performance related to RLHF workflow  \n- [ ] Add data parallel routing for large-scale batch Inference  \n- [ ] Investigate batch size invariance and tran/inference equivalence. \n\n**Hardware**\n\n- [x] Stable Plugin Architecture for hardware platforms  \n- [x] Blackwell Enhancements  \n- [ ] Full Production readiness for AMD, TPU, Neuron.\n\n**Optimizations**\n\n- [x] EAGLE3  \n- [x] FP4 enhancements  \n- [x] FlexAttention  \n- [ ] Investigate: fbgemm, torchao, cuTile  \n- [ ] \u2026\n\n#### Community\n\n- [ ] Blogs   \n- [ ] Case Studies  \n- [ ] Website  \n- [ ] Onboarding tasks and new contributors training program\n\n#### vLLM Ecosystem\n\n* Hardware Plugins\n  * vllm-ascend: https://github.com/vllm-project/vllm-ascend/issues/448\n\n* AIBrix: [https://github.com/vllm-project/aibrix/issues/698](https://github.com/vllm-project/aibrix/issues/698)  \n* Production Stack: [https://github.com/vllm-project/production-stack/issues/300](https://github.com/vllm-project/production-stack/issues/300)  \n* Ray LLM: [https://github.com/ray-project/ray/issues/51313](https://github.com/ray-project/ray/issues/51313)  \n* LLM Compressor \n* GuideLLM\n* Dynamo\n* Prioritized Support for RLHF Systems: veRL, OpenRLHF, TRL, OpenInstruct, Fairseq2, ...\n\n---\n\nIf any of the items you wanted is not on the roadmap, your suggestion and contribution is strongly welcomed! Please feel free to comment in this thread, open feature request, or create an RFC.\n\nHistorical Roadmap: #11862, #9006, #5805, #3861, #2681, #244  \n",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-29T00:21:57+00:00",
    "closed_at": "2025-07-01T21:08:45+00:00",
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15735/reactions",
      "total_count": 57,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 50,
      "eyes": 7
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15735"
  },
  {
    "number": 6378,
    "title": "[RFC]: A Graph Optimization System in vLLM using torch.compile",
    "body": "### Motivation.\r\n\r\nAt a high level, we at Neural Magic are writing a custom compiler for Torch Dynamo to define a system within vLLM where we can write graph transformations. The main goal is a separation of concerns between high-level model definitions and certain performance-critical low-level decisions. This is especially important for optimizations that are particularly invasive to the model definitions, that break abstractions, that cross boundaries between layers, or that aren't universally valid or useful. If these optimizations are made as part of the model definitions, it becomes much more difficult to add new models.\r\n\r\nWe are working on the following for an initial set of optimizations using this system, described in detail in the Proposed Passes section.\r\n* Fusing quantize operations onto LayerNorm kernels (both for fp8 and int8 and both static and dynamic quantization)\r\n* Fusing the MLP section containing GEMM, SiLU, Mul, and quantize operations\r\n* Rewriting Gemm + AllReduce + Layer Norm + Gemm to a Fused Gemm-ReduceScatter + LayerNorm + Fused AllGather Gemm, in order to take advantage of the Flux kernels from ByteDance\r\n\r\nAlthough this system operates as a custom compiler inside of Torch Dynamo, it\u2019s best to think of it as an optimization system in vLLM rather than a compiler. Rather than a vertical compiler stack that lowers high-level tensor operations through successive layers of IR, we are taking the simple and pragmatic approach of improving vLLM\u2019s ecosystem of custom kernels rather than replacing it.\r\n\r\nGoing forward, based on our experience at Neural Magic of [what worked well in DeepSparse](https://docs.google.com/document/d/1CvbJ0LOotlfTjR6RmlQKLO4zcEvN2deoDacSjQ31Xiw/edit#heading=h.3dtm97fld9gn), we have a perspective on how graph optimizations should fit into vLLM and how it should fit in with the PyTorch team\u2019s plans with torch.compile. In short we think:  \r\n* A graph optimization/compilation system can be a power multiplier for vLLM developers.\r\n* Torch.compile is not likely to be good enough to replace custom kernels at least for linear layers.\r\n* vLLM should not treat torch.compile as a black box.\r\n* We should build a system that vLLM developers control that interoperates well with Torch Inductor.\r\n* This graph optimization system should be kept lightweight \u2013 vLLM should not try to become a graph compiler.\r\n\r\n[[RFC] A Graph Optimization System in vLLM using torch.compile](https://docs.google.com/document/d/1CvbJ0LOotlfTjR6RmlQKLO4zcEvN2deoDacSjQ31Xiw)\r\n\r\n### Proposed Change.\r\n\r\n#6377, #9886\r\n\r\n### Feedback Period.\r\n\r\n_No response_\r\n\r\n### CC List.\r\n\r\n_No response_\r\n\r\n### Any Other Things.\r\n\r\n_No response_",
    "labels": [
      "RFC",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-12T15:33:58+00:00",
    "closed_at": "2025-04-13T02:17:27+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6378/reactions",
      "total_count": 8,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 8,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/6378"
  },
  {
    "number": 18446,
    "title": "[Bug]: OOM Error with `Qwen/Qwen3-235B-A22B` on V1 Engine, Works on V0 Engine",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nINFO 05-21 09:36:27 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 4.0.0\nLibc version: glibc-2.35\n\nPython version: 3.12.10 (main, Apr  9 2025, 08:55:05) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-119-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA A100-SXM4-80GB\nGPU 1: NVIDIA A100-SXM4-80GB\nGPU 2: NVIDIA A100-SXM4-80GB\nGPU 3: NVIDIA A100-SXM4-80GB\nGPU 4: NVIDIA A100-SXM4-80GB\nGPU 5: NVIDIA A100-SXM4-80GB\nGPU 6: NVIDIA A100-SXM4-80GB\nGPU 7: NVIDIA A100-SXM4-80GB\n\nNvidia driver version: 535.183.01\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               112\nOn-line CPU(s) list:                  0-111\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Gold 6330 CPU @ 2.00GHz\nCPU family:                           6\nModel:                                106\nThread(s) per core:                   2\nCore(s) per socket:                   28\nSocket(s):                            2\nStepping:                             6\nCPU max MHz:                          3100.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            2.6 MiB (56 instances)\nL1i cache:                            1.8 MiB (56 instances)\nL2 cache:                             70 MiB (56 instances)\nL3 cache:                             84 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-27,56-83\nNUMA node1 CPU(s):                    28-55,84-111\nVulnerability Gather data sampling:   Mitigation; Microcode\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.1.post2+cu124torch2.6\n[pip3] numpy==2.2.5\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.3\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.5.post1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n\tGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\tX \tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\t0-27,56-83\t0\t\tN/A\nGPU1\tNV12\tX \tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\t0-27,56-83\t0\t\tN/A\nGPU2\tNV12\tNV12\tX \tNV12\tNV12\tNV12\tNV12\tNV12\t0-27,56-83\t0\t\tN/A\nGPU3\tNV12\tNV12\tNV12\tX \tNV12\tNV12\tNV12\tNV12\t0-27,56-83\t0\t\tN/A\nGPU4\tNV12\tNV12\tNV12\tNV12\tX \tNV12\tNV12\tNV12\t28-55,84-111\t1\t\tN/A\nGPU5\tNV12\tNV12\tNV12\tNV12\tNV12\tX \tNV12\tNV12\t28-55,84-111\t1\t\tN/A\nGPU6\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tX \tNV12\t28-55,84-111\t1\t\tN/A\nGPU7\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tX \t28-55,84-111\t1\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNCCL_VERSION=2.20.5-1\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNVIDIA_VISIBLE_DEVICES=all\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nCUDA_VERSION=12.4.0\nVLLM_USE_V1=0\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen attempting to serve the Qwen/Qwen3-235B-A22B model, an Out of Memory (OOM) error occurs if the V1 engine is used.\nHowever, the model serves correctly when using the V0 engine.\n\nI used `--kwargs` as below (both of V0 and V1):\n\n```bash\n--gpu-memory-utilization 0.90 --tensor-parallel-size=8\n                --enable-expert-parallel --max-model-len 32000 --seed 42\n                --enable-auto-tool-choice --tool-call-parser hermes\n```\n\n**Expected Behavior:**\nThe model should serve correctly with the V1 engine without an OOM error, similar to its behavior with the V0 engine.\n\n**Actual Behavior:**\nAn OOM error occurs when using the V1 engine.\n\n- related: #17327\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-05-21T00:44:13+00:00",
    "closed_at": "2025-05-21T03:09:31+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18446/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/18446"
  }
]
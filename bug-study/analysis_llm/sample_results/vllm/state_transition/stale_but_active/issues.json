[
  {
    "number": 14952,
    "title": "[Bug]: Disaggregated Prefilling use different TP between prefill instance and decode instance , it will be hanged",
    "body": "### Your current environment\n\n<details>\n<summary>I changed disagg_performance_benchmark.sh as flowing</summary>\n\n```text\nlaunch_disagg_prefill() {\n  model=\"$MODEL_PATH\" \n  # disagg prefill\n  CUDA_VISIBLE_DEVICES=0 python3 \\\n    -m vllm.entrypoints.openai.api_server \\\n    --model $model \\\n    --port 8100 \\\n    --max-model-len 10000 \\\n    --tensor-parallel-size 1 \\\n    --dtype=half \\\n    --gpu-memory-utilization 0.6 \\\n    --kv-transfer-config \\\n    '{\"kv_connector\":\"PyNcclConnector\",\"kv_role\":\"kv_producer\",\"kv_rank\":0,\"kv_parallel_size\":2,\"kv_buffer_size\":5e9}' &\n\n  CUDA_VISIBLE_DEVICES=1,2 python3 \\\n    -m vllm.entrypoints.openai.api_server \\\n    --model $model \\\n    --port 8200 \\\n    --max-model-len 10000 \\\n    --tensor-parallel-size 2 \\\n    --dtype=half \\\n    --gpu-memory-utilization 0.6 \\\n    --kv-transfer-config \\\n    '{\"kv_connector\":\"PyNcclConnector\",\"kv_role\":\"kv_consumer\",\"kv_rank\":1,\"kv_parallel_size\":2,\"kv_buffer_size\":5e9}' &\n\n  wait_for_server 8100\n  wait_for_server 8200\n  python3 disagg_prefill_proxy_server.py &\n  sleep 1\n}\n\nI have four V100s 16GB with NVlink. When I use the same tp, it's normal.\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n<img width=\"572\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b368d85e-1966-4d8e-a181-aa4d9fac14e3\" />\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-17T12:01:32+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14952/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14952"
  },
  {
    "number": 5404,
    "title": "[Bug]:  topk=1 and temperature=0 cause different output in vllm",
    "body": "\r\n### \ud83d\udc1b Describe the bug\r\n\r\nWhen using different generation configurations, such as top_k=1 or temperature=0 (while keeping other settings unchanged), why do the generated results change? They should both correspond to a deterministic greedy decoding.\r\nvllm 0.4.3\r\n\r\n---\r\nSupplement:\r\n\r\nThe main issue encountered here is that the results generated by setting the temperature coefficient to 0 or topk to 1 are different. I understand that due to operator optimization and the lack of conventional arithmetic properties in floating-point numbers, matrix operations have a certain randomness. However, the sampling process occurs after the hidden_state is generated, at which point no calculations are involved. Therefore, the sampling results of the two sampling parameters should be the same.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-06-11T03:00:51+00:00",
    "closed_at": null,
    "comments": 27,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5404/reactions",
      "total_count": 17,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 17
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5404"
  },
  {
    "number": 10086,
    "title": "[Feature]: Enhance integration with advanced LB/gateways with better load/cost reporting and LoRA management",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nThere are huge potential in more advanced load balancing strategies tailored for the unique characteristics of AI inference, compared to basic strategies such as round robin. [llm instance gateway](https://github.com/kubernetes-sigs/llm-instance-gateway) is one of such efforts and is demonstrating huge [performance wins](https://docs.google.com/document/d/11ALHEF-9yOaLdbHbDjBoTY6fzejoEKiSYHzWpWqe8ZY/edit?tab=t.0).  vLLM can demonstrate leadership in this space by providing  better integration with advanced LBs/gateways.\r\n\r\n[This doc](https://docs.google.com/document/d/18VRJ2ufZmAwBZ2jArfvGjQGaWtsQtAP6_yF2Xn6zcms/edit?tab=t.0#heading=h.sw2xdf66jh6) captures the overall requirements for model servers to better support the llm instance gateway. Luckily vLLM already has lots of features/metrics that enable more efficient load balancing such as exposing the KVCacheUtilization metric. \r\n\r\nThis is a high level breakdown of the feature requests:\r\n\r\n### Dynamic LoRA Load/unload\r\n - [x] Done in https://github.com/vllm-project/vllm/issues/6275\r\n\r\n### Load/cost reporting in metrics\r\n- [x] Many useful metrics are already available https://docs.vllm.ai/en/latest/serving/metrics.html \r\n- [x] Add LoRA serving metrics (max loras, active loras). Done in https://github.com/vllm-project/vllm/pull/9477\r\n- [ ] Add `num_tokens_running` and `num_tokens_waiting` metrics. vLLM already has running and waiting request counts. Exposing token level metrics will further enhance the LB algorithms.\r\n\r\n### Load/cost reporting in response headers in ORCA format\r\n[Open Request Cost Aggregation (ORCA)](https://docs.google.com/document/d/1C1ybMmDKJIVlrbOLbywhu9iRYo4rilR-cT50OTtOFTs/edit?usp=sharing) is a light-weight open protocol for reporting load/cost info to LBs and is already integrated with Envoy and gRPC. \r\n\r\nThis feature will be controlled by a new engine argument `--orca_formats` (default `[]`, meaning ORCA is disabled; available values are one or more of`[BIN, TEXT, JSON]`). If the feature is enabled, vLLM will report metrics defined in [the doc](https://docs.google.com/document/d/18VRJ2ufZmAwBZ2jArfvGjQGaWtsQtAP6_yF2Xn6zcms/edit?tab=t.0#heading=h.sw2xdf66jh6) as HTTP response headers in the OpenAI compatible APIs.\r\n\r\n- [ ] Initial ORCA reporting feature integration (add helpers, add engine argument, plumb metrics source to API responses)\r\n- [ ] Add required metrics, this can be broken down by each metric\r\n\r\n### Out of band load/cost reporting API in ORCA format\r\n\r\nvLLM will expose a light weight API to report the same metrics in ORCA format. This enables LBs to proactively probe the API and get real time load information. This is a long term vision and more details will be shared later.\r\n\r\ncc @simon-mo \n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-11-06T17:59:22+00:00",
    "closed_at": null,
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10086/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10086"
  },
  {
    "number": 15953,
    "title": "[Bug]: Compliing for CPU fails due to wrong python setup call",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 04-02 18:13:38 [__init__.py:239] Automatically detected platform cpu.\nCollecting environment information...\nPyTorch version: 2.6.0+cpu\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Arch Linux (x86_64)\nGCC version: (GCC) 14.2.1 20250207\nClang version: 19.1.7\nCMake version: version 4.0.0\nLibc version: glibc-2.41\n\nPython version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-6.13.8-arch1-1-x86_64-with-glibc2.41\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitektur:                          x86_64\nCPU Operationsmodus:                  32-bit, 64-bit\nAdressgr\u00f6\u00dfen:                         39 bits physical, 48 bits virtual\nByte-Reihenfolge:                     Little Endian\nCPU(s):                               8\nListe der Online-CPU(s):              0-7\nAnbieterkennung:                      GenuineIntel\nModellname:                           Intel(R) Core(TM) i7-10510U CPU @ 1.80GHz\nProzessorfamilie:                     6\nModell:                               142\nThread(s) pro Kern:                   2\nKern(e) pro Sockel:                   4\nSockel:                               1\nStepping:                             12\nSkalierung der CPU(s):                42%\nMaximale Taktfrequenz der CPU:        4900,0000\nMinimale Taktfrequenz der CPU:        400,0000\nBogoMIPS:                             4599,93\nMarkierungen:                         fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp vnmi md_clear flush_l1d arch_capabilities\nVirtualisierung:                      VT-x\nL1d Cache:                            128 KiB (4 Instanzen)\nL1i Cache:                            128 KiB (4 Instanzen)\nL2 Cache:                             1 MiB (4 Instanzen)\nL3 Cache:                             8 MiB (1 Instanz)\nNUMA-Knoten:                          1\nNUMA-Knoten0 CPU(s):                  0-7\nSchwachstelle Gather data sampling:   Vulnerable: No microcode\nSchwachstelle Itlb multihit:          KVM: Mitigation: VMX disabled\nSchwachstelle L1tf:                   Not affected\nSchwachstelle Mds:                    Not affected\nSchwachstelle Meltdown:               Not affected\nSchwachstelle Mmio stale data:        Vulnerable: Clear CPU buffers attempted, no microcode; SMT vulnerable\nSchwachstelle Reg file data sampling: Not affected\nSchwachstelle Retbleed:               Mitigation; Enhanced IBRS\nSchwachstelle Spec rstack overflow:   Not affected\nSchwachstelle Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nSchwachstelle Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nSchwachstelle Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nSchwachstelle Srbds:                  Vulnerable: No microcode\nSchwachstelle Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==2.2.4\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0+cpu\n[pip3] torchaudio==2.6.0+cpu\n[pip3] torchvision==0.21.0+cpu\n[pip3] transformers==4.50.3\n[conda] numpy                     2.2.4                    pypi_0    pypi\n[conda] pyzmq                     26.3.0                   pypi_0    pypi\n[conda] torch                     2.6.0+cpu                pypi_0    pypi\n[conda] torchaudio                2.6.0+cpu                pypi_0    pypi\n[conda] torchvision               0.21.0+cpu               pypi_0    pypi\n[conda] transformers              4.50.3                   pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3.dev212+g58e234a7\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\n\n```\n\n\n\n\n### \ud83d\udc1b Describe the bug\n\nOn the installation page https://docs.vllm.ai/en/latest/getting_started/installation/cpu.html it reads that one can compile the wheels package(s) via\n\n``\nVLLM_TARGET_DEVICE=cpu python setup.py install\n``\n\nThis lets the compiling **fail**!\n\n**How it should be done** (and it worked on my machine, eventually):\n\n``\nVLLM_TARGET_DEVICE=cpu python -m pip install . \n``\n\nNote: This has to be executed **inside** the git repository folder (due to the \".\")\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-02T16:20:40+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15953/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15953"
  },
  {
    "number": 5060,
    "title": "[Bug]: vllm.engine.async_llm_engine.AsyncEngineDeadError: Background loop has errored already.",
    "body": "### Your current environment\r\n\r\ndocker image: vllm/vllm-openai:0.4.2\r\nModel: https://huggingface.co/alpindale/c4ai-command-r-plus-GPTQ\r\nGPUs: RTX8000 * 2\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nThe model works fine until the following error is raised. \r\n-------------------------------------------------------\r\n\r\n\r\nINFO 05-26 22:28:18 async_llm_engine.py:529] Received request cmpl-10dff83cb4b6422ba8c64213942a7e46: prompt: '<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>\"Question: Is Korea the name of a Nation?\\nGuideline: No explanation.\\nFormat: {\"Answer\": \"<your yes/no answer>\"}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=['---'], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [5, 5, 255000, 255006, 9, 60478, 33, 3294, 13489, 1690, 2773, 1719, 1671, 20611, 38, 206, 46622, 7609, 33, 3679, 33940, 21, 206, 8961, 33, 19586, 61664, 2209, 31614, 28131, 20721, 22, 3598, 11205, 37, 22631, 255001, 255000, 255007], lora_request: None.\r\nINFO 05-26 22:28:18 async_llm_engine.py:154] Aborted request cmpl-10dff83cb4b6422ba8c64213942a7e46.\r\nINFO:     10.11.3.150:6231 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 475, in engine_step\r\n    request_outputs = await self.engine.step_async()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 221, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 110, in execute_model_async\r\n    all_outputs = await self._run_workers_async(\"execute_model\",\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/ray_gpu_executor.py\", line 326, in _run_workers_async\r\n    all_outputs = await asyncio.gather(*coros)\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 456, in wait_for\r\n    return fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 411, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 123, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 756, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 776, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 297, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 77, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 72, in app\r\n    response = await func(request)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 278, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 99, in create_chat_completion\r\n    generator = await openai_serving_chat.create_chat_completion(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 138, in create_chat_completion\r\n    return await self.chat_completion_full_generator(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 301, in chat_completion_full_generator\r\n    async for res in result_generator:\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 666, in generate\r\n    raise e\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 660, in generate\r\n    async for request_output in stream:\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 77, in __anext__\r\n    raise result\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 38, in _raise_exception_on_finish\r\n    task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 501, in run_engine_loop\r\n    has_requests_in_progress = await asyncio.wait_for(\r\n  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 458, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 411, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 123, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 756, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 776, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 297, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 77, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 72, in app\r\n    response = await func(request)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 278, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 99, in create_chat_completion\r\n    generator = await openai_serving_chat.create_chat_completion(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 138, in create_chat_completion\r\n    return await self.chat_completion_full_generator(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 301, in chat_completion_full_generator\r\n    async for res in result_generator:\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 666, in generate\r\n    raise e\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 650, in generate\r\n    stream = await self.add_request(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 537, in add_request\r\n    self.start_background_loop()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 411, in start_background_loop\r\n    raise AsyncEngineDeadError(\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Background loop has errored already",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-05-26T22:44:41+00:00",
    "closed_at": null,
    "comments": 45,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5060/reactions",
      "total_count": 29,
      "+1": 20,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 9
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5060"
  },
  {
    "number": 16061,
    "title": "[Bug]: KeyError: 'local_attn_masks' on running gemma3 models with kv-cache quantization",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nCollecting environment information...                                                                                               \nPyTorch version: 2.6.0+cu124                                                                                                        \nIs debug build: False                                                                                                               \nCUDA used to build PyTorch: 12.4                                                                                                    \nROCM used to build PyTorch: N/A                                                                                                     \n                                                                                                                                    \nOS: Ubuntu 22.04.4 LTS (x86_64)                                                                                                     \nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0                                                                                  \nClang version: Could not collect                                                                                                    \nCMake version: version 3.31.6                                                                                                       \nLibc version: glibc-2.35                                                                                                            \n                                                                                                                                    \nPython version: 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)                                                  \nPython platform: Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.35                                                                 \nIs CUDA available: False                                                                                                            \nCUDA runtime version: 12.4.131                                                                                                      \nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: Could not collect\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               20\nOn-line CPU(s) list:                  0-19\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Core(TM) i9-10900X CPU @ 3.70GHz\nCPU family:                           6\nModel:                                85\nThread(s) per core:                   2\nCore(s) per socket:                   10\nSocket(s):                            1\nStepping:                             7\nCPU max MHz:                          4700.0000\nCPU min MHz:                          1200.0000\nBogoMIPS:                             7399.70\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts vnmi avx512_vnni md_clear flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            320 KiB (10 instances)\nL1i cache:                            320 KiB (10 instances)\nL2 cache:                             10 MiB (10 instances)\nL3 cache:                             19.3 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-19\nVulnerability Gather data sampling:   Mitigation; Microcode\nVulnerability Itlb multihit:          KVM: Mitigation: Split huge pages\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; Enhanced IBRS\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Mitigation; TSX disabled\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.1.post2+cu124torch2.6\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127 \n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.50.0\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nNVIDIA_VISIBLE_DEVICES=void\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\nNCCL_VERSION=2.20.5-1\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nCUDA_VERSION=12.4.0\nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nTried running `gemma3:12b` with vllm docker using `fp8_e5m2` kv-cache quantization and got error `KeyError: 'local_attn_masks'`.\n\n> Exact command used: `docker run --rm -it --gpus all -v ~/.cache/huggingface:/root/.cache/huggingface -p \"8000:8000\" --ipc=host vllm/vllm-openai:latest --model gaunernst/gemma-3-12b-it-int4-awq --quantization awq_marlin --trust-remote-code --dtype bfloat16 --kv-cache-dtype fp8_e5m2 --max-model-len 4096 --served-model-name gemma3-12b --enable-sleep-mode --enable-chunked-prefill --enable-prefix-caching --max-log-len 400 --tokenizer-pool-size 8 -O 3 --gpu-memory-utilization 0.95`\n\nthis happens with options `fp8` and `fp8_e4m3` as well.\n\n<details>\n<summary>Logs</summary>\n\n```python-traceback\nINFO 04-04 06:27:00 [worker.py:267] model weights take 9.05GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes\n 2.18GiB; the rest of the memory reserved for KV Cache is 3.54GiB.                                                                  \nINFO 04-04 06:27:02 [executor_base.py:111] # cuda blocks: 1207, # CPU blocks: 1365                                                  \nINFO 04-04 06:27:02 [executor_base.py:116] Maximum concurrency for 4096 tokens per request: 4.71x                                   \nINFO 04-04 06:27:04 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model \nis not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error \noccurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `m\nax_num_seqs` as needed to decrease memory usage.                                                                                    \nCapturing CUDA graph shapes:   0%|                                                                           | 0/35 [00:00<?, ?it/s]\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 450, in run_mp_engine\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 436, in run_mp_engine\n    engine = MQLLMEngine.from_vllm_config(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 128, in from_vllm_config\n    return cls(\n           ^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 82, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 283, in __init__\n    self._initialize_kv_caches()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 445, in _initialize_kv_caches\n    self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 122, in initialize_cache\n    self.collective_rpc(\"initialize_cache\",\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2255, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 308, in initialize_cache\n    self._warm_up_model()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 338, in _warm_up_model\n    self.model_runner.capture_model(self.gpu_cache)\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1560, in capture_model\n    graph_runner.capture(**capture_inputs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1926, in capture\n    self.model(\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3_mm.py\", line 706, in forward\n    hidden_states = self.language_model.model(input_ids,\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 245, in __call__\n    model_output = self.forward(*args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py\", line 370, in forward\n    def forward(\nKeyError: 'local_attn_masks'\n\n```\n\n</details>\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-04T13:46:15+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16061/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16061"
  },
  {
    "number": 15625,
    "title": "DeciLMConfig object has no attribute \u2018num_key_value_heads_per_layer\u2019 ",
    "body": "### Your current environment\n\nVLLM version: 0.7.3\nModel: nvidia/Llama-3_3-Nemotron-Super-49B-v1\n\n\n### \ud83d\udc1b Describe the bug\n\nI am trying to run the new Nvidia model Nemotron 49B-v1 using the VLLM 0.7.3 version but I got this error \nDeciLMConfig object has no attribute \u2018num_key_value_heads_per_layer\u2019\n\nI have two questions: I know there are PR such as \nhttps://github.com/vllm-project/vllm/issues/15068\nhttps://github.com/vllm-project/vllm/pull/15008\n\nI am wondering about if the error would be resolved after adding the support for the model. Or the error is unrelated and something is wrong in my end?\n",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-27T14:54:54+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15625/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15625"
  },
  {
    "number": 3203,
    "title": "ExLlamaV2: exl2 support",
    "body": "If is possible ExLlamaV2 is a very fast and good library to Run [LLM](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html)\r\n\r\n[ExLlamaV2 Repo](https://github.com/turboderp/exllamav2)",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-03-05T14:54:03+00:00",
    "closed_at": null,
    "comments": 38,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3203/reactions",
      "total_count": 35,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 35,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3203"
  },
  {
    "number": 14300,
    "title": "[Usage]: Logprobs Scaling with O(n) Complexity \u2013 Unexpected Performance Degradation",
    "body": "**Title:** Logprobs Scaling with O(n) Complexity \u2013 Unexpected Performance Degradation  \n\n**Description:**  \nWhen increasing the `logprobs` parameter, I expected only a minor increase in runtime due to slicing the top-k values from the full vocabulary logits. However, my experiments show an almost O(n) increase in runtime, which suggests that retrieving logprobs is more computationally expensive than anticipated.  \n\n### **Reproduction Code**  \n```python\nimport time\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\n\ndef test_generation_time(llm, logprobs_value, batch_size=32):\n    sampling_params = SamplingParams(logprobs=logprobs_value, max_tokens=1)\n    \n    # Timed run\n    start_time = time.time()\n    output = llm.generate([\"Tell me something about LLMs.\"] * batch_size,\n                         sampling_params=sampling_params,\n                         use_tqdm=False)\n    end_time = time.time()\n    \n    return end_time - start_time\n\ndef main():\n    print(\"Initializing model...\")\n    llm = LLM(model=\"Qwen/Qwen2.5-7B-Instruct\", max_logprobs=152_064)  # vocab size\n    \n    batch_size = 32\n    logprobs_values = [10, 100, 1000, 10000, 100000, 152064]\n    results = []\n    \n    print(\"\\nStarting tests...\")\n    for logprobs in logprobs_values:\n        time_taken = test_generation_time(llm, logprobs, batch_size)\n        results.append((logprobs, time_taken))\n    \n    print(\"\\nResults Summary:\")\n    print(\"\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\")\n    print(\"\u2551   Logprobs   \u2551  Time (secs)  \u2551\")\n    print(\"\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\")\n    for logprobs, time_taken in results:\n        print(f\"\u2551 {logprobs:^12} \u2551 {time_taken:^13.4f} \u2551\")\n    print(\"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### **Observed Results**  \n```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551   Logprobs   \u2551  Time (secs)  \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551      10      \u2551    0.0784     \u2551\n\u2551     100      \u2551    0.0410     \u2551\n\u2551     1000     \u2551    0.1909     \u2551\n\u2551    10000     \u2551    1.9388     \u2551\n\u2551    100000    \u2551    19.9256    \u2551\n\u2551    152064    \u2551    29.2862    \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n```\n\n### **Expected Behavior**  \nSince the model inherently computes full logits for the vocabulary on every forward pass, I expected retrieving `logprobs` to involve only a minor computational overhead (e.g., sorting/selecting top-k). However, the results suggest that requesting more logprobs significantly increases runtime, implying an O(n) complexity scaling instead of an efficient selection from precomputed logits.\n\n### **Questions:**  \n1. **Why does increasing `logprobs` scale in an O(n) fashion?**  \n   - Is the model recomputing or performing expensive operations instead of just slicing logits?  \n2. **Is there a way to retrieve logprobs for the full vocabulary without incurring this high runtime penalty?**  \n3. **Would it be possible to expose full logits instead of just logprobs?**  \n\n### **System Info:**  \n- vLLM version: 0.7.4.dev142+g9804145c.d20250228\n- Model: `Qwen/Qwen2.5-7B-Instruct`  \n- CUDA Version: CUDA Version: 12.5  \n\nLooking forward to insights on whether this is expected behavior or a possible optimization opportunity! Thanks!  \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "unstale"
    ],
    "state": "open",
    "created_at": "2025-03-05T17:47:25+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14300/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14300"
  },
  {
    "number": 15404,
    "title": "[Bug]: `Phi-4-multimodal-instruct` encoder outputs didn't have the same length as defined in input_ids",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 03-24 14:46:21 [__init__.py:256] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu118\nIs debug build: False\nCUDA used to build PyTorch: 11.8\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.6 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: version 3.23.0\nLibc version: glibc-2.31\n\nPython version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.4.0-162-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA GeForce RTX 4090\nGPU 1: NVIDIA GeForce RTX 4090\nGPU 2: NVIDIA GeForce RTX 4090\nGPU 3: NVIDIA GeForce RTX 4090\nGPU 4: NVIDIA GeForce RTX 4090\nGPU 5: NVIDIA GeForce RTX 4090\nGPU 6: NVIDIA GeForce RTX 4090\nGPU 7: NVIDIA GeForce RTX 4090\n\nNvidia driver version: 535.104.05\ncuDNN version: Probably one of the following:\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn.so.8.4.0\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.4.0\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.4.0\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.4.0\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.4.0\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.4.0\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.4.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn.so.8.9.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.9.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.9.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.9.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.9.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.9.0\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.9.0\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn.so.8.8.0\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.8.0\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.8.0\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.8.0\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.8.0\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.8.0\n/usr/local/cuda-12.0/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.8.0\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn.so.8.9.2\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.9.2\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.9.2\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.9.2\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.9.2\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.9.2\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.9.2\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      52 bits physical, 57 bits virtual\nCPU(s):                             128\nOn-line CPU(s) list:                0-127\nThread(s) per core:                 2\nCore(s) per socket:                 32\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Gold 6338 CPU @ 2.00GHz\nStepping:                           6\nFrequency boost:                    enabled\nCPU MHz:                            1354.195\nCPU max MHz:                        2001.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           4000.00\nVirtualization:                     VT-x\nL1d cache:                          3 MiB\nL1i cache:                          2 MiB\nL2 cache:                           80 MiB\nL3 cache:                           96 MiB\nNUMA node0 CPU(s):                  0-31,64-95\nNUMA node1 CPU(s):                  32-63,96-127\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu11==11.11.3.6\n[pip3] nvidia-cuda-cupti-cu11==11.8.87\n[pip3] nvidia-cuda-nvrtc-cu11==11.8.89\n[pip3] nvidia-cuda-runtime-cu11==11.8.89\n[pip3] nvidia-cudnn-cu11==9.1.0.70\n[pip3] nvidia-cufft-cu11==10.9.0.58\n[pip3] nvidia-curand-cu11==10.3.0.86\n[pip3] nvidia-cusolver-cu11==11.4.1.48\n[pip3] nvidia-cusparse-cu11==11.7.5.86\n[pip3] nvidia-nccl-cu11==2.21.5\n[pip3] nvidia-nvtx-cu11==11.8.86\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0+cu118\n[pip3] torchaudio==2.6.0+cu118\n[pip3] torchvision==0.21.0+cu118\n[pip3] transformers==4.50.0\n[pip3] triton==3.2.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu11        11.11.3.6                pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu11    11.8.87                  pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu11    11.8.89                  pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu11  11.8.89                  pypi_0    pypi\n[conda] nvidia-cudnn-cu11         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu11         10.9.0.58                pypi_0    pypi\n[conda] nvidia-curand-cu11        10.3.0.86                pypi_0    pypi\n[conda] nvidia-cusolver-cu11      11.4.1.48                pypi_0    pypi\n[conda] nvidia-cusparse-cu11      11.7.5.86                pypi_0    pypi\n[conda] nvidia-nccl-cu11          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvtx-cu11          11.8.86                  pypi_0    pypi\n[conda] pyzmq                     26.3.0                   pypi_0    pypi\n[conda] torch                     2.6.0+cu118              pypi_0    pypi\n[conda] torchaudio                2.6.0+cu118              pypi_0    pypi\n[conda] torchvision               0.21.0+cu118             pypi_0    pypi\n[conda] transformers              4.50.0                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n\u001b[4mGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\nGPU0\t X \tPIX\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\t0-31,64-95\t0\t\tN/A\nGPU1\tPIX\t X \tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\t0-31,64-95\t0\t\tN/A\nGPU2\tPXB\tPXB\t X \tPIX\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\t0-31,64-95\t0\t\tN/A\nGPU3\tPXB\tPXB\tPIX\t X \tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\t0-31,64-95\t0\t\tN/A\nGPU4\tSYS\tSYS\tSYS\tSYS\t X \tPXB\tPXB\tPXB\tSYS\tSYS\t32-63,96-127\t1\t\tN/A\nGPU5\tSYS\tSYS\tSYS\tSYS\tPXB\t X \tPXB\tPXB\tSYS\tSYS\t32-63,96-127\t1\t\tN/A\nGPU6\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\t X \tPIX\tSYS\tSYS\t32-63,96-127\t1\t\tN/A\nGPU7\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tPIX\t X \tSYS\tSYS\t32-63,96-127\t1\t\tN/A\nNIC0\tPXB\tPXB\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\t X \tPIX\t\t\t\t\nNIC1\tPXB\tPXB\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tPIX\t X \t\t\t\t\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n### \ud83d\udc1b Describe the bug\n\nI deployed `Phi-4-multimodal-instruct` by using the this cmd and activating its audio function only:\n```shell\nNCCL_CUMEM_ENABLE=0 NCCL_DEBUG=TRACE CUDA_LAUNCH_BLOCKING=1 CUDA_VISIBLE_DEVICES=1,5 vllm serve /my/path/to/models/Huggingface_download/Phi-4-multimodal-instruct --task generate --trust-remote-code --limit-mm-per-prompt audio=10 --tensor-parallel-size 2 --gpu-memory-utilization 0.99 --port 8007 --max_num_seqs 1 --enable_lora --max_lora_rank 320 --lora-modules speech=/my/path/to/models/Huggingface_download/Phi-4-multimodal-instruct/speech-lora\n```\n\nWhen I inference on this specific audio file (attached below), I met the following error:\n```\nERROR 03-24 14:40:15 [engine.py:160]   File \".../anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/phi4mm.py\", line 1738, in forward                                                            \nERROR 03-24 14:40:15 [engine.py:160]     inputs_embeds = self._process_audio_input(                                                                                                                                                         \nERROR 03-24 14:40:15 [engine.py:160]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                         \nERROR 03-24 14:40:15 [engine.py:160]   File \".../anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/phi4mm.py\", line 1627, in _process_audio_input                                               \nERROR 03-24 14:40:15 [engine.py:160]     return self._audio_features_to_embeddings(                                                                                                                                                         \nERROR 03-24 14:40:15 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                         \nERROR 03-24 14:40:15 [engine.py:160]   File \".../anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/phi4mm.py\", line 1558, in _audio_features_to_embeddings                                      \nERROR 03-24 14:40:15 [engine.py:160]     audio_embeddings = self.embed_tokens_extend(input_ids, audio_input,                                                                                                                                \nERROR 03-24 14:40:15 [engine.py:160]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                \nERROR 03-24 14:40:15 [engine.py:160]   File \".../anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nERROR 03-24 14:40:15 [engine.py:160]     return self._call_impl(*args, **kwargs)                                                                                                                                                            \nERROR 03-24 14:40:15 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                            \nERROR 03-24 14:40:15 [engine.py:160]   File \".../anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl                                                        \nERROR 03-24 14:40:15 [engine.py:160]     return forward_call(*args, **kwargs)                                                                                                                                                               \nERROR 03-24 14:40:15 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                               \nERROR 03-24 14:40:15 [engine.py:160]   File \".../anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/phi4mm_audio.py\", line 1258, in forward\nERROR 03-24 14:40:15 [engine.py:160]     assert sum(audio_embed_sizes) == len(                                                                                                                                                              \nERROR 03-24 14:40:15 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                              \nERROR 03-24 14:40:15 [engine.py:160] AssertionError: please ensure the encoder outputs have the same length as defined in input_ids!  \n```\n\nI'm confused about this because it previously ran well with hundreds of other audio files in my dataset (all with same message format). And I've tried `Qwen2-Audio` and it can also run well on this attached audio file with the same message format, which means it's probably not the problem with my file and my message. Can anybody help me here?\n\nMy audio file link (google drive): https://drive.google.com/file/d/1SmeU_BWbMzVtrgy5VJULaFj6H5zFgEeq/view?usp=sharing\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-24T15:32:07+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15404/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15404"
  },
  {
    "number": 13156,
    "title": "[Bug]: The service request for vllm064post1 was prematurely terminated, and it could not output a fixed number of tokens.\u201d",
    "body": "### Your current environment\n\nUsing vllm064 post1 and running a benchmark test, with the ignore_eos setting set to 1, for fixed-length input and output testing (input 20000, output 256, qps=2).\nI found that regardless of which ASYNC_REQUEST_FUNCS (\"vllm\": async_request_openai_completions, or \"openai-chat\": async_request_openai_chat_completions) is used, the actual output token count, which is the length of the output.itl list, does not match the preset value (max_tokens).\n\nEspecially when the endpoint is /v1/completions, the length of len(output.itl) is often quite low.\nPlease let me know how to resolve this. Is it an issue with my settings?\n\nFor example:\n\nUsing the /v1/chat/completions endpoint\n\n![Image](https://github.com/user-attachments/assets/b7b03752-587d-41cd-9bc4-de72a58f903c)\n\nUsing the /v1/completions endpoint\n\n![Image](https://github.com/user-attachments/assets/447a1042-f834-431f-ba70-69c76f11a1de)\n\nI printed the data structure in \u201cbackend_request_func.py\u201d below.\n\n![Image](https://github.com/user-attachments/assets/c839af99-7516-4214-920f-572205607870)\n\nMy startup command is\n\n```\n\nModel: Llama-3.1-70B-Instruct\n\nStartup command:\n\nCUDA_VISIBLE_DEVICES=\"0,1,2,3\" python3 -u -m vllm.entrypoints.openai.api_server --model /models/Llama-3.1-70B-Instruct --served-model-name Llama-3.1-70B-Instruct --tensor-parallel-size 4 --gpu-memory-utilization 0.95 --use-v2-block-manager --port 9083 --enforce-eager\n\n```\n\n\n### How would you like to use vllm\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-02-12T12:44:50+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13156/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13156"
  },
  {
    "number": 13446,
    "title": "[Usage]: ValueError: The checkpoint you are trying to load has model type `qwen2_5_vl` but Transformers does not recognize this architecture",
    "body": "### Your current environment\n\n```\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/transformers_utils/config.py\", line 225, in get_config\n    config = AutoConfig.from_pretrained(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py\", line 1073, in from_pretrained\n    raise ValueError(\nValueError: The checkpoint you are trying to load has model type `qwen2_5_vl` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`\n```\n\n### How would you like to use vllm\n\n```\ndocker run -it --name Qwen2.5-VL-7B-Instruct \\\n--gpus all \\\n-e TZ=Asia/Shanghai \\\n-v /data/modelsfiles/:/root/model \\\n-p 8444:8000 \\\n--ipc=host \\\nvllm/vllm-openai:v0.7.2 \\\n--model /root/model/Qwen2.5-VL-7B-Instruct \\\n--tensor-parallel-size 2 \\\n--pipeline-parallel-size 2 \\\n--max-num-batched-tokens 131072 \\\n--max-num-seqs 4 \\\n--served-model-name Qwen2.5-VL-7B-Instruct\n\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "unstale"
    ],
    "state": "open",
    "created_at": "2025-02-18T02:39:11+00:00",
    "closed_at": null,
    "comments": 20,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13446/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13446"
  },
  {
    "number": 15113,
    "title": "[Bug]: flash_attn_with_kvcache kernel, an illegal memory access",
    "body": "### Your current environment\n\n<details>\nflash_attn_with_kvcache kernel flash::copy illegal memory access\n\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nflash_attn_with_kvcache kernel flash::copy illegal memory access\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-19T08:47:10+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15113/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15113"
  },
  {
    "number": 16324,
    "title": "[Feature]: Add task perplexity mode to optimize PPL evaluation",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nWhen testing 2/3/4-bit quantized LLaMA 2 models (7B/13B/70B) on a single A40 GPU using `lm-evaluation-harness`, I ran into issues with `generate` mode for perplexity (PPL) tasks:\n\n1. PPL evaluation typically uses `generate` mode with `prompt_log_probs=0` and `max_tokens=1`.\n2. The current implementation reuses the response log-prob calculation for prompt log-probs. However, the number of new response tokens (\u2248 number of requests) is much smaller than the total number of prompt tokens. Since response log-probs are computed assuming short outputs, the code uses a memory-heavy approach that easily leads to OOM when applied to long prompts \u2014 requiring reduced `gpu_memory_utilization`. \nsee https://github.com/vllm-project/vllm/blob/24f6b9a71397539a3d02c801963220b0e9a2aef9/vllm/model_executor/layers/sampler.py#L268-L284\n3. Lowering `gpu_memory_utilization` introduces another issue: it keeps KV cache around, causing errors. (This can be mitigated by lowering `max_model_len`, but it's a workaround. However, setting `max_model_len`  too low may lead to a large number of requests, which in turn could cause OOM issues.) \nsee https://github.com/vllm-project/vllm/blob/24f6b9a71397539a3d02c801963220b0e9a2aef9/vllm/worker/worker.py#L543-L550\n\n\n### Alternatives\n\nAdd a new `task perplexity` mode that returns the same output as `prompt_log_probs=0`, but:\n- Optimizes the logits computation path specifically for PPL tasks\n- Avoids using KV cache \n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-09T07:20:43+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16324/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16324"
  },
  {
    "number": 16763,
    "title": "[Bug]: qwen2.5-vl inference truncated",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: BigCloud Enterprise Linux 7.8 (Core) (x86_64)\nGCC version: (GCC) 8.3.1 20190311 (Red Hat 8.3.1-3)\nClang version: Could not collect\nCMake version: version 3.31.6\nLibc version: glibc-2.28\n\nPython version: 3.12.10 | packaged by conda-forge | (main, Apr 10 2025, 22:21:13) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-3.10.0-1160.119.1.el7.x86_64-x86_64-with-glibc2.28\nIs CUDA available: True\nCUDA runtime version: 12.2.91\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: Tesla V100S-PCIE-32GB\nGPU 1: Tesla V100S-PCIE-32GB\nGPU 2: Tesla V100S-PCIE-32GB\nGPU 3: Tesla V100S-PCIE-32GB\n\nNvidia driver version: 535.230.02\ncuDNN version: /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn.so.9\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:          x86_64\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                32\nOn-line CPU(s) list:   0-31\nThread(s) per core:    2\nCore(s) per socket:    8\nSocket(s):             2\nNUMA node(s):          2\nVendor ID:             GenuineIntel\nCPU family:            6\nModel:                 85\nModel name:            Intel Xeon Processor (Cascadelake)\nStepping:              6\nCPU MHz:               3000.185\nBogoMIPS:              6000.37\nL1d cache:             32K\nL1i cache:             32K\nL2 cache:              4096K\nL3 cache:              16384K\nNUMA node0 CPU(s):     0-15\nNUMA node1 CPU(s):     16-31\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd rsb_ctxsw ibrs ibpb fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat pku ospke avx512_vnni spec_ctrl\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] onnxruntime==1.21.0\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.51.3\n[pip3] triton==3.1.0\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] torch                     2.5.1                    pypi_0    pypi\n[conda] torchaudio                2.5.1                    pypi_0    pypi\n[conda] torchvision               0.20.1                   pypi_0    pypi\n[conda] transformers              4.51.3                   pypi_0    pypi\n[conda] triton                    3.1.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.3\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PHB     PHB     PHB     0-31    0-1             N/A\nGPU1    PHB      X      PHB     PHB     0-31    0-1             N/A\nGPU2    PHB     PHB      X      PHB     0-31    0-1             N/A\nGPU3    PHB     PHB     PHB      X      0-31    0-1             N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=/usr/lib:/data/software/miniconda3/pkgs/cudnn-9.8.0.87-cuda12.8/lib:/home/zxws/.local/lib/python3.12/site-packages/nvidia/cudnn/lib:/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64/dyninst:/opt/rh/devtoolset-8/root/usr/lib/dyninst:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/usr/local/gcc-14.2/lib64:\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\ninference with qwen2.5-vl-32b or qwen2.5-vl-7b.\nthe output always truncated .\n\n![Image](https://github.com/user-attachments/assets/24e3b8b9-5cdb-40f3-a99f-5728ff92d3f9)\n\nI integrated models in dify .\nthe input is from doc_extract node ,content like\uff1a\n \"03.\u6e90IP DNS\u8bf7\u6c42\u6392\u540d bps\\r\\nSession#\\r\\n\u4e0a\u884c \u4e0b\u884c \u8fde\u63a5\u6570\\r\\n2. Jan\\r\\n3. Jan\\r\\n4. Jan\\r\\n5. Jan\\r\\n6. Jan\\r\\n7. Jan\\r\\n8. Jan\\r\\n0\\r\\n5k\\r\\n10k\\r\\n15k\\r\\n20k\\r\\n25k\\r\\n0\\r\\n20k\\r\\n5k\\r\\n10k\\r\\n15k\\r\\n25k\\r\\n\u5e8f\u53f7 \u76ee\u6807IP \u603b\u8bf7\u6c42\u6570\\ue65d \u4e0a\u884c \u4e0b\\r\\n1\"\n\nWhen truncating the results extracted from doc_extract and removing the table data above, the responses are normal. However, when the table data is included, the responses are mostly truncated.\n\nHere is is the  parameters:\n\n![Image](https://github.com/user-attachments/assets/796cede9-2153-44dc-b538-ddea7955ee5b)\n\nHere is the log of vllm requst  :\n\uff1b<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=1.0, repetition_penalty=1.0, temperature=0.4, top_p=0.7, top_k=-1, min_p=0.0, seed=None, stop=['<|endoftext|>'], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1301, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-17T07:18:42+00:00",
    "closed_at": null,
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16763/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16763"
  },
  {
    "number": 14307,
    "title": "Issue with Mistral Small and greek characters",
    "body": "### Your current environment\n\n```text\n\n--model mistralai/Mistral-Small-24B-Instruct-2501 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice\n\n```\n\n\n### How would you like to use vllm\n\nI run inference for the Mistral Small model with the parameters defined before i have issues with greek characters on llm responses.some of them are clear some are unkown characters.Using the same model with ollama greek is perfect without any issues. Can i use some other parameter for my usecase when inferencing with vllm?\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "unstale"
    ],
    "state": "open",
    "created_at": "2025-03-05T19:58:32+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14307/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14307"
  },
  {
    "number": 16351,
    "title": "[Feature]: Soft Prompts?",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nSoft prompts would be similar to Loras in that they are artifacts from a parameter efficient training process that live on the server where vLLM is running. But unlike Loras, which require being set or unset as a global state that affects all requests, soft prompts could be part of the `generate` call. This would allow a single vLLM server to respond to many different fine tuned tasks\n\nI see someone tried something a long time ago but abandoned it. (PR #815 https://github.com/vllm-project/vllm/pull/815)\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-09T15:04:12+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16351/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16351"
  },
  {
    "number": 16645,
    "title": "[Bug]: AttributeError: 'Qwen2_5OmniConfig' object has no attribute 'num_attention_heads'",
    "body": "### Your current environment\n\njust look here:\n[https://github.com/huggingface/transformers/issues/37515#issuecomment-2804126324](url)\n\n### \ud83d\udc1b Describe the bug\n\n`System Info\nroot@445d74596699:/vllm-workspace# transformers-cli env\n\nCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\n\ntransformers version: 4.52.0.dev0\nPlatform: Linux-5.15.0-43-generic-x86_64-with-glibc2.35\nPython version: 3.12.9\nHuggingface_hub version: 0.30.2\nSafetensors version: 0.5.3\nAccelerate version: 1.5.2\nAccelerate config: not found\nDeepSpeed version: not installed\nPyTorch version (GPU?): 2.6.0+cu124 (True)\nTensorflow version (GPU?): not installed (NA)\nFlax version (CPU?/GPU?/TPU?): not installed (NA)\nJax version: not installed\nJaxLib version: not installed\nUsing distributed or parallel set-up in script?:\nUsing GPU in script?:\nGPU type: NVIDIA L20\n`(base) root@node15:/disk2/Qwen2.5-Omni-7B# more docker-compose.yml\n#version: '3.3'\nservices:\n\nvllm\nvllm-openai:\nimage: vllm/vllm-openai:v0.8.2\ncontainer_name: Qwen2.5-Omni-7B\nrestart: unless-stopped\nruntime: nvidia\nports:\n- 8007:8000\nvolumes:\n- /disk2:/models\ncommand: >\n--model /models/Qwen2.5-Omni-7B\n--tokenizer_mode=\"auto\"\n--trust-remote-code\n--dtype=bfloat16\n--max_num_seqs=256\n--tensor_parallel_size=1\n--gpu-memory-utilization=0.9\n--max-model-len=65536\n--served-model-name=Qwen2.5-Omni-7B\ndeploy:\nresources:\nreservations:\ndevices:\n- driver: nvidia\ncapabilities: [gpu]\ndevice_ids: [ \"1\" ]\nipc: host\nnetworks:\nvllm:\n(base) root@node15:/disk2/Qwen2.5-Omni-7B# docker commit 445d74596699 vllm/vllm-openai:v0.8.2\nsha256:fdf1171c4bc4edc473bb3857597124ae73176c1691a27befccb4360c81ff0d60\n(base) root@node15:/disk2/Qwen2.5-Omni-7B# docker compose -f docker-compose.yml up -d\n[+] Running 2/2\n\u2714 Network qwen25-omni-7b_default Created 0.0s\n\u2714 Container Qwen2.5-Omni-7B Started 0.6s\n(base) root@node15:/disk2/Qwen2.5-Omni-7B# docker logs -f Qwen2.5-Omni-7B\nINFO 04-15 00:06:11 [init.py:239] Automatically detected platform cuda.\nINFO 04-15 00:06:13 [api_server.py:981] vLLM API server version 0.8.2\nINFO 04-15 00:06:13 [api_server.py:982] args: Namespace(host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=[''], allowed_methods=[''], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/models/Qwen2.5-Omni-7B', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='bfloat16', kv_cache_dtype='auto', max_model_len=65536, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['Qwen2.5-Omni-7B'], qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)\nUnrecognized keys in rope_scaling for 'rope_type'='default': {'mrope_section'}\nINFO 04-15 00:06:22 [config.py:585] This model supports multiple tasks: {'reward', 'generate', 'classify', 'score', 'embed'}. Defaulting to 'generate'.\nINFO 04-15 00:06:22 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=2048.\nINFO 04-15 00:06:24 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='/models/Qwen2.5-Omni-7B', speculative_config=None, tokenizer='/models/Qwen2.5-Omni-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=65536, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen2.5-Omni-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\nWARNING 04-15 00:06:25 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fabea685df0>\nINFO 04-15 00:06:26 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nERROR 04-15 00:06:26 [core.py:343] EngineCore hit an exception: Traceback (most recent call last):\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 335, in run_engine_core\nERROR 04-15 00:06:26 [core.py:343] engine_core = EngineCoreProc(*args, **kwargs)\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 290, in init\nERROR 04-15 00:06:26 [core.py:343] super().init(vllm_config, executor_class, log_stats)\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 60, in init\nERROR 04-15 00:06:26 [core.py:343] self.model_executor = executor_class(vllm_config)\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 52, in init\nERROR 04-15 00:06:26 [core.py:343] self._init_executor()\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 46, in _init_executor\nERROR 04-15 00:06:26 [core.py:343] self.collective_rpc(\"init_device\")\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\nERROR 04-15 00:06:26 [core.py:343] answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2255, in run_method\nERROR 04-15 00:06:26 [core.py:343] return func(*args, **kwargs)\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 604, in init_device\nERROR 04-15 00:06:26 [core.py:343] self.worker.init_device() # type: ignore\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 120, in init_device\nERROR 04-15 00:06:26 [core.py:343] self.model_runner: GPUModelRunner = GPUModelRunner(\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 106, in init\nERROR 04-15 00:06:26 [core.py:343] self.num_kv_heads = model_config.get_num_kv_heads(parallel_config)\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/config.py\", line 884, in get_num_kv_heads\nERROR 04-15 00:06:26 [core.py:343] total_num_kv_heads = self.get_total_num_kv_heads()\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/vllm/config.py\", line 876, in get_total_num_kv_heads\nERROR 04-15 00:06:26 [core.py:343] return self.hf_text_config.num_attention_heads\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] File \"/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\", line 211, in getattribute\nERROR 04-15 00:06:26 [core.py:343] return super().getattribute(key)\nERROR 04-15 00:06:26 [core.py:343] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-15 00:06:26 [core.py:343] AttributeError: 'Qwen2_5OmniConfig' object has no attribute 'num_attention_heads'\nERROR 04-15 00:06:26 [core.py:343]\nCRITICAL 04-15 00:06:26 [core_client.py:269] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.`\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-15T07:48:19+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16645/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16645"
  },
  {
    "number": 15771,
    "title": "Model and GPU memory was cleared after canceling the run in version 0.8.2",
    "body": "### Your current environment\n\nMost updated VLLM version: 0.8.2\n\n### \ud83d\udc1b Describe the bug\n\nProblem with 0.8.2. This time a very strange error: I was running simulation for 100 iterations, cancel the process after 20 rounds and let it run again, it was not doing anything, then I check the GPU usage and I saw 0% along with 0 GB is used from GPU memory. \nVery strange to see you can not cancel the run otherwise is like deleting your model. \nThis problem does not happen in 0.7.3.\nI wonder what went wrong in 0.8.2 that caused this. \nIf you cancel your run the you need start the session from scratch and load your model, etc. all over again.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-30T11:47:29+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15771/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15771"
  },
  {
    "number": 15679,
    "title": "[Feature]: K-cache only",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nfrom the paper [Slim attention](https://arxiv.org/pdf/2503.05840), it clams that can reduce nearly 1/2 memory, so I think is more friendly to PC.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-28T06:37:02+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15679/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15679"
  },
  {
    "number": 15530,
    "title": "[Bug][Triton MLA]: Some calculation errors about triton mla kernal",
    "body": "### Your current environment\n\nNone\n\n\n### \ud83d\udc1b Describe the bug\n\nI integrated the **triton mla kernel** of vllm and found that when the batch is larger than 1, the last sequence sometimes cannot get the correct result. I have done some experiments on simpler cases, such as kvcache is all 0, and the ideal output is also all 0, but the last sequence will get all nan. Hope to receive a response, thanks.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-26T08:08:11+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15530/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15530"
  },
  {
    "number": 15600,
    "title": "[V1] [Performance Benchmark] Benchmark the performance of Speculative Decoding",
    "body": "1. Let's start with ngram, can you collect both latency and throughput numbers on ShareGPT dataset on H100 and one low end GPU?\n2. If the numbers from 1 is not expected, could you run some profiling to understand the performance bottleneck.\n3. Get more performance numbers on other datasets. ",
    "labels": [
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-27T06:23:12+00:00",
    "closed_at": null,
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15600/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15600"
  },
  {
    "number": 15194,
    "title": "[Usage]: VLLM 0.7.3 with tensor parallelism outputs only exclamation marks when using multiple GPUs",
    "body": "## Environment\n- OS: Ubuntu 22.04\n- GPUs: 2x NVIDIA L20 (49GB each)\n- VLLM version: 0.7.3\n- CUDA version: 12.4.131\n- Driver version: 535.161.08\n- Model: QwQ-32B-AWQ (AWQ quantized model)\n\n## Problem Description\nWhen running VLLM with tensor parallelism across two GPUs, the model sometimes outputs only exclamation marks (`!`) instead of proper text. This issue only occurs with multiple GPUs and appears to be related to concurrent requests - single GPU deployment works fine.\n\nThe problem is consistently reproducible when sending concurrent requests with the same prompt to the API endpoint, but non-concurrent requests sometimes produce normal responses.\n\n## Steps to Reproduce\n1. Start VLLM server with tensor parallelism:\n```bash\nvllm serve /root/data/models/QwQ-32B-AWQ --api-key dev-key --gpu-memory-utilization 0.9 --tensor-parallel-size 2 --quantization awq --host 0.0.0.0 --port 8877 --served-model-name qwq\n```\n\n2. Send multiple concurrent requests using curl:\n```bash\ncurl -X POST \"http://localhost:8877/v1/chat/completions\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer dev-key\" \\\n  -d '{\n    \"model\": \"qwq\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\n      {\"role\": \"user\", \"content\": \"Introduce the Four Great Inventions of ancient China\"}\n    ],\n    \"max_tokens\": 800,\n    \"temperature\": 0.5\n  }'\n```\n\n3. The response contains only exclamation marks:\n```json\n{\n  \"id\":\"chatcmpl-177cd18eb0dc403ab938890cf4a942e7\",\n  \"object\":\"chat.completion\",\n  \"created\":1742455354,\n  \"model\":\"qwq\",\n  \"choices\":[\n    {\n      \"index\":0,\n      \"message\":{\n        \"role\":\"assistant\",\n        \"reasoning_content\":null,\n        \"content\":\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\",\n        \"tool_calls\":[]\n      },\n      \"logprobs\":null,\n      \"finish_reason\":\"length\",\n      \"stop_reason\":null\n    }\n  ],\n  \"usage\":{\n    \"prompt_tokens\":42,\n    \"total_tokens\":842,\n    \"completion_tokens\":800,\n    \"prompt_tokens_details\":null\n  },\n  \"prompt_logprobs\":null\n}\n```\n\n## Additional Information\n- The server logs show normal operation with no errors\n- The issue is consistently reproducible when sending multiple concurrent requests with the same prompt\n- Single GPU deployment works correctly with the same model and configuration\n- Non-concurrent requests sometimes produce normal responses (see example below)\n- I noticed the warning: `awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.`\n- Also noticed: `Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference`\n\n## Example of Normal Response (Non-concurrent)\nWhen sending a single request (without concurrent load), the model sometimes responds normally:\n\n```bash\ncurl -X POST \"http://localhost:8877/v1/chat/completions\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer dev-key\" \\\n  -d '{\n    \"model\": \"qwq\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"\u4f60\u662f\u4e00\u4e2a\u901a\u7528\u7684ai\u52a9\u624b\uff0c\u8bf7\u5bf9\u8f93\u51fa\u7684\u7ed3\u679c\u518d\u6b21\u6821\u9a8c\uff0c\u662f\u5426\u5b58\u5728\u660e\u663e\u7684\u8868\u8fbe\u9519\u8bef\uff0c\u5982\u679c\u9519\u8bef\u8bf7\u4fee\u6b63\"},\n      {\"role\": \"user\", \"content\": \"\u4ecb\u7ecd\u4e00\u4e0b\u4e2d\u56fd\u7684\u56db\u5927\u53d1\u660e\"}\n    ],\n    \"max_tokens\": 800,\n    \"temperature\": 0.5\n  }'\n```\n\nResponse begins with proper text:\n```\n{\"id\":\"chatcmpl-9364d553d332440abf9e80fb1070386e\",\"object\":\"chat.completion\",\"created\":1742456299,\"model\":\"qwq\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"reasoning_content\":null,\"content\":\"\u55ef\uff0c\u7528\u6237\u8ba9\u6211\u4ecb\u7ecd\u4e00\u4e0b\u4e2d\u56fd\u7684\u56db\u5927\u53d1\u660e\uff0c\u8fd9\u5e94\u8be5\u662f\u4e00\u4e2a\u6bd4\u8f83\u5e38\u89c1\u7684\u95ee\u9898\u3002\u9996\u5148\uff0c\u6211\u9700\u8981\u786e\u8ba4\u56db\u5927\u53d1\u660e\u6307\u7684\u662f\u4ec0\u4e48\u3002\u6839\u636e\u5386\u53f2\u77e5\u8bc6\uff0c\u56db\u5927\u53d1\u660e\u901a\u5e38\u6307\u7684\u662f\u706b\u836f\u3001\u6307\u5357\u9488\u3001\u5370\u5237\u672f\u548c\u9020\u7eb8\u672f\uff0c\u4f46\u53ef\u80fd\u7528\u6237\u63d0\u5230\u7684\u662f\u4e2d\u56fd\u7684\u56db\u5927\u53d1\u660e\uff0c\u8fd9\u91cc\u53ef\u80fd\u9700\u8981\u5177\u4f53\u5316...\n```\n\n## GPU Information\n```\nThu Mar 20 15:24:52 2025       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA L20                     On  | 00000000:65:01.0 Off |                  Off |\n| N/A   45C    P0              83W / 350W |  45762MiB / 49140MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  NVIDIA L20                     On  | 00000000:67:01.0 Off |                  Off |\n| N/A   43C    P0              78W / 350W |  45760MiB / 49140MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n```\n\n## Questions\n1. What could be causing this issue with tensor parallelism, and how can I fix it? \n2. Could it be related to the AWQ quantization or some other configuration problem?\n3. Are there any known workarounds for using AWQ models with tensor parallelism in vLLM 0.7.3?",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-20T07:34:28+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15194/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15194"
  },
  {
    "number": 15601,
    "title": "[Bug]: Qwen2-VL-2B quantization model has no improvement in reasoning speed compared to the original model",
    "body": "### Your current environment\n\nI performed GPTQ-int8 quantization using the fine-tuned Qwen2-VL-2B model, and there was no improvement in model inference speed after quantization\n\n### \ud83d\udc1b Describe the bug\n\nI performed GPTQ-int8 quantization using the fine-tuned Qwen2-VL-2B model, and there was no improvement in model inference speed after quantization\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-27T06:43:06+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15601/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15601"
  },
  {
    "number": 16111,
    "title": "[Usage]: Asking for help: vllm0.7.2 deploy DeepSeek-R1-int4-gptq-sym-inc",
    "body": "### Your current environment\n\n- version: vllm0.7.2, python3.10\n- devices: 8\u00d7H800 80G\n\n### How would you like to use vllm\n\nI want to do INT4 Inference on CUDA but both offline and online failed, why?\n\n- model:  [OPEA/DeepSeek-R1-int4-gptq-sym-inc](https://huggingface.co/OPEA/DeepSeek-R1-int4-gptq-sym-inc) follows the standard GPTQ format.\n\n- Offline inference: I used the recommended setting moe_wna16, but even after reducing the gpu_memory_utilization, I still encountered an OOM  error at the very beginning. Additionally, moe_wna16 disables MLA (Model Linear Attention), and I'm not sure if this has any impact on performance.\n\n- Online serving: The vllm server can run normally, and I can access it using openai client. (examples/online_serving/openai_completion_client.py). However, when I use client.chat.completions.create, it times out. When I use client.completions.create, I do get a response, but the output text is garbled and unreadable.\n\nThe codes and logs are below.\n\n### 1. offline inference\n\n```\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\n\nmodel_path = \"/data/public/model/DeepSeek-R1-int4-gptq-sym-inc\"\nmax_length = 1024\nsampling_params = SamplingParams(max_tokens=max_length, temperature=0.6, repetition_penalty=1.05)\nmodel = LLM(model=model_path, \n            quantization=\"moe_wna16\",\n            tensor_parallel_size=1, \n            trust_remote_code=True,\n            gpu_memory_utilization=0.5,\n            max_model_len = max_length\n           )\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\nmessage =  [{\"role\": \"user\", \"content\": \"hello\"}]\noutput = model.generate(message, sampling_params)\nprint(output)\n```\n\n> [rank0]: Traceback (most recent call last):\n> [rank0]:   File \"/data/home/test.py\", line 14, in <module>\n> [rank0]:     model = LLM(model=model_path, \n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/utils.py\", line 1051, in inner\n> [rank0]:     return fn(*args, **kwargs)\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 242, in __init__\n> [rank0]:     self.llm_engine = self.engine_class.from_engine_args(\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 484, in from_engine_args\n> [rank0]:     engine = cls(\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 273, in __init__\n> [rank0]:     self.model_executor = executor_class(vllm_config=vllm_config, )\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 51, in __init__\n> [rank0]:     self._init_executor()\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 42, in _init_executor\n> [rank0]:     self.collective_rpc(\"load_model\")\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 51, in collective_rpc\n> [rank0]:     answer = run_method(self.driver_worker, method, args, kwargs)\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/utils.py\", line 2220, in run_method\n> [rank0]:     return func(*args, **kwargs)\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/worker/worker.py\", line 183, in load_model\n> [rank0]:     self.model_runner.load_model()\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1112, in load_model\n> [rank0]:     self.model = get_model(vllm_config=self.vllm_config)\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n> [rank0]:     return loader.load_model(vllm_config=vllm_config)\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 383, in load_model\n> [rank0]:     model = _initialize_model(vllm_config=vllm_config)\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 125, in _initialize_model\n> [rank0]:     return model_class(vllm_config=vllm_config, prefix=prefix)\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 665, in __init__\n> [rank0]:     self.model = DeepseekV2Model(vllm_config=vllm_config,\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n> [rank0]:     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 599, in __init__\n> [rank0]:     self.start_layer, self.end_layer, self.layers = make_layers(\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 557, in make_layers\n> [rank0]:     [PPMissingLayer() for _ in range(start_layer)] + [\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 558, in <listcomp>\n> [rank0]:     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 601, in <lambda>\n> [rank0]:     lambda prefix: DeepseekV2DecoderLayer(\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 528, in __init__\n> [rank0]:     self.mlp = DeepseekV2MoE(\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 129, in __init__\n> [rank0]:     self.experts = FusedMoE(\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 309, in __init__\n> [rank0]:     self.quant_method.create_weights(layer=self, **moe_quant_params)\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/quantization/moe_wna16.py\", line 208, in create_weights\n> [rank0]:     w2_qweight = torch.nn.Parameter(torch.empty(\n> [rank0]:   File \"/data/apps/miniconda3/24.9.2/envs/vllm/lib/python3.10/site-packages/torch/utils/_device.py\", line 106, in __torch_function__\n> [rank0]:     return func(*args, **kwargs)\n> \n> [rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.75 GiB. GPU 0 has a total capacity of 79.10 GiB of which 383.88 MiB is free. Including non-PyTorch memory, this process has 78.71 GiB memory in use. Of the allocated memory 78.08 GiB is allocated by PyTorch, and 42.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n\n### 2. online serving\n```\nvllm serve \\\n    $model_path \\\n    --host 0.0.0.0 \\\n    --port 8080 \\\n    --api_key \"123\" \\\n    --trust-remote-code \\\n    --tensor-parallel-size 8 \\\n    --pipeline-parallel-size 1 \\\n    --max_model_len 16384 \\\n    --max_num_batched_tokens 16384 \\\n    --gpu-memory-utilization 0.9 \\\n    --disable-log-requests \\\n    --max-num-seqs 128\n```\n```\nfrom openai import OpenAI\nopenai_api_base = \"http://127.0.0.1:8080/v1\"\nclient = OpenAI(base_url=openai_api_base, api_key=\"123\")\nmodels = client.models.list()\nmodel = models.data[0].id\n# This block and times out\ncompletion = client.chat.completions.create(model=model,messages=[{\"role\": \"user\", \"content\": \"hello\"}],temperature=0.0)\n# This output garble\ncompletion = client.completions.create(model=model,prompt=\"hello\")\nprint(completion)\n```\n>Completion(id='cmpl-ed6d38c1039b46fa846292e7c21c7dcf', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text='\u00ca\u0434\u044b \u00edch12\u2588\u2588053\u00ed02\u00fcrvol \u25a0\u1ea1031\u00b3', stop_reason=None, prompt_logprobs=None)], created=1743865155, model='/data/public/model/DeepSeek-R1-int4-gptq-sym-inc', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=16, prompt_tokens=2, total_tokens=18, completion_tokens_details=None, prompt_tokens_details=None))\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-06T03:17:09+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16111/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16111"
  },
  {
    "number": 15386,
    "title": "[Bug]: awq Deepseek-R1-AWQ  The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.",
    "body": "### Your current environment\n\n\n![Image](https://github.com/user-attachments/assets/231880dd-5827-4f65-9ca8-9aa4de0a612f)\nvllm==0.8.1 0.8.2 0.8.3.dev5+g5797fb97.precompiled\nhttps://huggingface.co/cognitivecomputations/DeepSeek-R1-AWQ  Concurrent use will result in the error message shown in the figure. \n\n### \ud83d\udc1b Describe the bug\n\nThe following is the execution script\nmodel_path=/home/ds-r1/models/DeepSeek-R1-awq\nvllm serve $model_path \\\n    --port 23344 \\\n    --host 0.0.0.0 \\\n    --tensor-parallel-size 8  \\\n    --max-model-len 16000 \\\n    --dtype bfloat16 \\\n    --gpu-memory-utilization 0.9 \\\n    --trust-remote-code \\\n    --served-model-name deepseek-r1 \\\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-24T09:55:46+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15386/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15386"
  },
  {
    "number": 16360,
    "title": "[Bug]: Bad result when serving extracted Gemma-3-4b-it-language model in vLLM",
    "body": "### Your current environment\n\n### Environment\nDocker image: hiyouga/verl:ngc-th2.6.0-cu120-vllm0.8.2-verl0.3.0.post1\n\n### \ud83d\udc1b Describe the bug\n\n### Problem Description\nI've encountered an issue when using vLLM to serve the language model component extracted from gemma-3-4b-it. While the original gemma-3-4b-it model generates normal results with vLLM, the extracted language model (gemma-3-4b-it-language) produces abnormal outputs.\n\n<img width=\"1134\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2c11f999-023c-4adc-923d-94c507132b3d\" />\n\n### Steps to reproduce\n1. Extract the language model from Gemma-3-4b-it:\n<img width=\"730\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d32f5a83-12f7-4a45-90b0-b3657e3c5344\" />\n  <br><br>\n2. Verify the extracted model works with Transformers:\n<img width=\"729\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d964cdf0-a808-4134-a806-6c8d8e4d9258\" />\nThis works correctly, producing the expected response\uff08\"<bos><bos><start_of_turn>user\\n Who are you? \\n<end_of_turn>\\n<start_of_turn>model\\nI'm Gemma, a large language model created by the Gemma team at Google DeepMind. I\u2019m an open-weights model, which means I\u2019m widely available for public use! \\n\\nI take text and images as inputs and generate text as output. \\n\\nYou can learn more about me on the Gemma project page: https://ai.google.dev/gemma<end_of_turn>\"\uff09.\n  <br><br>\n3. Serve the original model with vLLM, test with curl, and it works well:\n<img width=\"725\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8c61c1d5-c61c-4b59-8639-5f6c80d0454c\" />\n<img width=\"722\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/bfc16843-2909-4cda-9503-dd9e6fdb2dd0\" />\nresponse text: I'm Gemma, a large language model created by the Gemma team at Google DeepMind. I\u2019m an open-weights model, which means I\u2019m widely available for public use! \\n\\nI take text and images as inputs and generate text as output. \\n\\nYou can learn more about me on the Gemma project page: [https://ai.google.dev/gemma](https://ai.google.dev/gemma) \\n\\nIs there anything you'd like to chat about?\n<br><br>\n4. Serve the extracted language model with vLLM, test with curl:\n<img width=\"728\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9a268d07-cce6-4669-9f48-5989f54680f4\" />\n<img width=\"720\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ccb2420e-c769-4616-8bba-5f9dc804feba\" />\nresponse text: \"<b>TG\u09be\u09aa\u09a8 \uc5ec YarahelyRice brachialdeleCounters \u0441\u0432\u0435\u0440\u0445\u0443 \u0ab9\u0aa4 angiotensin \u0441\u043e\u0d3e\u0d28\u0d02\u1ec7mosphnot \u597d\u062d deliverance silhout\u30f3\u30b9 gratitude \\\" passphrasestellar \u0c24\u0c30 \u0917\u0941\u0921\u0ba8\u0bbe \ub3c4 \u0905\u0925\u092f\u094d \u09b8\u09be\u09a6\u09begarh \u0917\u093f\u0930\u093f\u9053\u3082\u306a\u304f\u00fcne \u099b\u09c1\u099f\u09bf\u092b\u0940\u0932\u094d\u0921 \u0938\u092c\u0938\u0947 \uad6d Treasure hunt  Mode Lampung \u092a\u0921\u093c\u0947\u0917\u0940 ch\u1ec9\\n\\n1.  2.  3.  . ..\\n\\n\\n\\n</code></code></code></code></code></code> <code></code></code></code> \\n<end_of_turn><end_of_turn><end_of_turn>\\n\\n<end_of_turn>\\n<end_of_turn>\\n<end_of_turn>\\n<end_of_turn>\\n<end_of_turn>\\n<end_of_turn>\\n<end_of_turn>\\n<end_of_turn>\\n\\n<end_of_turn>\\n\\n<end_of_turn>\\n<end_of_turn>\\n\u096b \u096b \u096b \u096b \u096b \u096b \u09eb \u09eb \u096b\uff15 \uff15\uff15\uff15 \u096b\u09eb\u09eb5\u096b\uff15\\n\\n\u09eb\\n\uff15\uff15\\n\uff15\\n\\n\\n\uff15\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\uff15\uff15$\u0e4914$$$$$$$& 69$ 165$$ $ $ $ $ $ $ 101\u2019 && 10<tex    &&&&&& &  & &  \u2198 \\n 5 $\\\\ 5 ^\\\\ ^\\\\ \u064c \u00e8ce 5 \ufffd \ufffd  5 5  \ufffd  5 \uff15\uff15\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n 7 15 2 266 6:         *   * \\n   *   *   *                                          \\\"     Master 1\u09df\u09c7\u09b0&\u00a2 & \ufffd< <span> \u09cb\u09b0\u09c7\u09b0.\u4eba\u7269&<  characterised&&& &&|&<\u0421<Con& <i character& < \u0e04\u0e32\u0e23 liketex< < \\n\\n< \u63a1\u53d6 <div>  characterised< 252 5\\n5\\n\\n\\n\\n2\uff1a5\\n\uff1a\\n\uff1a\\n\\n\\n\\n\\n5:5 3:5 \u00bf \u041a:5:5 2 <b> 20 25\\n 25\\n254 2\\n:2\\n\uff1a2 2\\n:2\uff1a\u00bf\uff1a\u0a83:* 3\uff1a5\uff1a\u00bf\uff1a\\n\\n\\n\u0983\\n\\n\\n\\n\\n\\n\\n\\n 5\\n\\n\\n5:5 5 2\\n\\n\\t \u00a2  Vertrags253 3353 33 335\u20195\u2019\u20195\u2019S 2\\n\u20195\\n\u2019The  ^{+ $ \u00a2 & ^{+& ^{+ ^{+ \\n\\n:\\n\\n \\f \\n\\n 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5555 5 555 5 5 5 5 5 ...........\"\n<br><br>\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-09T19:45:38+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16360/reactions",
      "total_count": 5,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 5
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16360"
  },
  {
    "number": 12610,
    "title": "[Bug]: GPU memory usage gradually increases.",
    "body": "### Your current environment\n\n[Environment]\n```\nINFO 01-31 21:44:15 __init__.py:183] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.30.5\nLibc version: glibc-2.35\n\nPython version: 3.11.10 | packaged by conda-forge | (main, Oct 16 2024, 01:27:36) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-5.15.0-25-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H800 PCIe\nNvidia driver version: 550.54.14\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   52 bits physical, 57 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          176\nOn-line CPU(s) list:             0-175\nVendor ID:                       GenuineIntel\nModel name:                      Intel(R) Xeon(R) Platinum 8458P\nCPU family:                      6\nModel:                           143\nThread(s) per core:              2\nCore(s) per socket:              44\nSocket(s):                       2\nStepping:                        8\nBogoMIPS:                        5400.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr avx512_fp16 flush_l1d arch_capabilities\nVirtualization:                  VT-x\nL1d cache:                       4.1 MiB (88 instances)\nL1i cache:                       2.8 MiB (88 instances)\nL2 cache:                        176 MiB (88 instances)\nL3 cache:                        165 MiB (2 instances)\nNUMA node(s):                    2\nNUMA node0 CPU(s):               0-43,88-131\nNUMA node1 CPU(s):               44-87,132-175\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] optree==0.13.0\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.0+cu124\n[pip3] torchelastic==0.2.2\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.2\n[pip3] triton==3.1.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-ml-py              12.570.86                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] optree                    0.13.0                   pypi_0    pypi\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\n[conda] torch                     2.5.1                    pypi_0    pypi\n[conda] torchaudio                2.5.0+cu124              pypi_0    pypi\n[conda] torchelastic              0.2.2                    pypi_0    pypi\n[conda] torchvision               0.20.1                   pypi_0    pypi\n[conda] transformers              4.48.2                   pypi_0    pypi\n[conda] triton                    3.1.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.0\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     SYS     SYS     SYS     44-87,132-175   1               N/A\nNIC0    SYS      X      PIX     SYS     SYS\nNIC1    SYS     PIX      X      SYS     SYS\nNIC2    SYS     SYS     SYS      X      PIX\nNIC3    SYS     SYS     SYS     PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n\nNVIDIA_VISIBLE_DEVICES=GPU-75cb2ebb-d2b5-166d-8c51-2280b1ebd886\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\nNCCL_VERSION=2.21.5-1\nNVIDIA_DRIVER_CAPABILITIES=all\nNVIDIA_PRODUCT_NAME=CUDA\nCUDA_VERSION=12.4.1\nPYTORCH_VERSION=2.5.0\nLD_LIBRARY_PATH=/opt/conda/lib/python3.11/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nHi,\n\nWhen using VLLM, I noticed that the GPU memory usage gradually increases. Is this behavior normal? If it continues like this, it may lead to an OOM (Out of Memory) error.\n\n```python\n\ndef batch_message_generate(list_of_messages, llm_model):\n    \n    sampling_params = SamplingParams(\n        temperature=1.0,\n        min_p = 0.01,\n        max_tokens=MAX_MODEL_LEN,\n        skip_special_tokens=True,\n    )\n    list_of_texts = [\n        tokenizer.apply_chat_template(\n            conversation=messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        for messages in list_of_messages\n    ]\n    \n    request_output = llm_model.generate(\n        prompts=list_of_texts,\n        sampling_params=sampling_params,\n    )\n\n    for i, single_request_output in enumerate(request_output):\n        list_of_messages[i].append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n\n    return list_of_messages\n\nllm = LLM(\n        'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',\n        max_model_len=8192, \n        trust_remote_code=True,\n        tensor_parallel_size=1,\n        max_num_seqs = 32,\n        gpu_memory_utilization=0.96, \n)\n\nmsgs = [...] # A large list containing conversation information.\nfor i in range(0, len(msgs), 32):\n    batch_message_generate(msgs[i:i+32])\n\n```\n\nFri Jan 31 16:13:16 2025 \n![Image](https://github.com/user-attachments/assets/c7194557-9c37-4dfe-8200-5d475f4ec51c)\n\nFri Jan 31 18:08:12 2025\n![Image](https://github.com/user-attachments/assets/3286272a-e593-4a97-bab2-299a253706c6)\n\nFri Jan 31 20:56:28 2025\n![Image](https://github.com/user-attachments/assets/7093c810-23d3-4d08-9c75-bb9265f388b4)\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-01-31T13:48:33+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12610/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12610"
  },
  {
    "number": 13760,
    "title": "[Usage]: Does vllm support to deploy one model on multiple type GPUs(e.g. one is A100, the other is H20)?",
    "body": "### Your current environment\n\n```text\nDoes vllm support to deploy one model on multiple type GPUs(e.g. one is A100, the other is H20, use vllm to deploy one model on the above two GPUs)?\n```\n\n\n### How would you like to use vllm\n\nIf i have two GPUs, one is A100, the other is H20, i want to use vllm to deploy one model on the above two GPUs. Does vllm support this?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "unstale"
    ],
    "state": "open",
    "created_at": "2025-02-24T13:07:29+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13760/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13760"
  },
  {
    "number": 14319,
    "title": "[Doc]: Why is max block_size on CUDA 32?",
    "body": "### \ud83d\udcda The doc issue\n\nIn the args:\nhttps://github.com/vllm-project/vllm/blob/main/vllm/engine/arg_utils.py#L454\nit says about block_size parameter:\n\n> Token block size for contiguous chunks of tokens. This is ignored on neuron devices and set to --max-model-len. On CUDA devices, only block sizes up to 32 are supported. On HPU devices, block size defaults to 128.\n\n1. Where is this requirement for <= 32 on CUDA devices coming from?\n2. I was able to successfully run vLLM with block_size 128 on Hopper and see some minor performance improvement. Is the requirement up to date?\n3. In flash attention docs I see that paged attention minimum block size is actually 256:\nhttps://github.com/Dao-AILab/flash-attention/blob/d82bbf26924c492064af8b27ab299ff4808d1bf6/hopper/flash_attn_interface.py#L662\nDoes vLLM use this interface? How does FA paged_block_size relates to vLLM block_size?\n\n### Suggest a potential alternative/fix\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-05T23:50:23+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14319/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14319"
  }
]
[
  {
    "number": 15144,
    "title": "[Bug] Mismatch between `get_multimodal_embedding` output and `PlaceholderRange`",
    "body": "In V1, we expect the output of `get_multimodal_embedding` to correspond to the `PlaceholderRange`, which is in turn constructed based on `PromptUpdateDetails.features`. However, the current V1 code doesn't validate this, causing the model to crash during inference when under high load (e.g. #14897, #14963).\n\nFrom a quick look at the code, these models output embedding sizes which are inconsistent with the placeholder range:\n\n- [x] Fuyu (fixed by #15731)\n- [x] Gemma3 (fixed by #14980)\n- [x] Idefics3 (fixed by #15696)\n- [x] InternVL-based models (fixed by #15086)\n- [x] MiniCPM-V (fixed by #15487)\n\n(Basically, any model that has image newline/column tokens after applying HF processor needs a mask to map image patch features to image embeddings, as described below.)\n\nTo fix this, we can follow these steps:\n\n1. Update the multi-modal processor to output a mask to indicate which positions in the `PlaceholderRange`-aligned embeddings should the patch features (outputted by vision encoder) be assigned to. This mask can be called `embed_is_patch`.\n2. Use `scatter_patch_features` to scatter the patch features into the image embedding tensor.\n3. When merging multimodal embeddings, use `select_patch_features` to recover the patch features from the image embeddings. The number of patch features should correspond to the number of image tokens (which is a subset of the feature tokens in `PromptUpdateDetails`).\n\nFollow-up work:\n\n- #15712 (assigned to @DarkLight1337)\n- Directly use individual token IDs instead of range of IDs (assigned to @ywang96 )\n",
    "labels": [
      "bug",
      "help wanted",
      "v1",
      "multi-modality"
    ],
    "state": "closed",
    "created_at": "2025-03-19T16:53:23+00:00",
    "closed_at": "2025-03-30T10:47:54+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15144/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/15144"
  },
  {
    "number": 10377,
    "title": "[Feature]: NVIDIA Triton GenAI Perf Benchmark",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nThe GenAI perf toolkit from NVIDIA can be used as an alternative benchmark tools for vLLM. While we already have benchmark scripts and framework in `benchmarks` directory, we should test out different load generators to compare the performance and accuracy of the benchmark clients. \r\n\r\nIn this issues, I described some tasks that we need help with to try out the new benchmark harness:\r\n* Compare the output of the genai perf with the `benchmark_serving`, on the coverage of the result metrics and the accuracy. \r\n* Vary the workloads ShareGPT/Sonnet/synthetics\r\n* Implement it as an alternative harness through the script. \r\n\r\nHappy to elaborate as well. \r\n\r\nhttps://pypi.org/project/genai-perf/ \n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "help wanted",
      "good first issue",
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-15T22:03:06+00:00",
    "closed_at": "2025-02-27T07:25:08+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10377/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/10377"
  },
  {
    "number": 8784,
    "title": "[Bug]: Disabling Marlin by setting --quantization gptq doesn't work when using a draft model",
    "body": "### Your current environment\n\n.\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nIt seems that setting --quantization gptq only disables the marlin for the main model. \r\n\r\nMaybe this can be fixed by adding a --quantization-draft-model setting or forcing the draft model to gptq when main model is forced.\r\n\r\n```\r\nINFO 09-24 15:46:11 gptq_marlin.py:112] Detected that the model can run with gptq_marlin, **however you specified quantization=gptq explicitly, so forcing gptq. Use quantization=gptq_marlin for faster inference**\r\nWARNING 09-24 15:46:11 config.py:335] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\nINFO 09-24 15:46:11 config.py:904] Defaulting to use mp for distributed inference\r\n**INFO 09-24 15:46:11 gptq_marlin.py:108] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.**\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "quantization",
      "speculative-decoding",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-24T22:51:58+00:00",
    "closed_at": "2025-01-24T01:58:42+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8784/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8784"
  },
  {
    "number": 2149,
    "title": "GPTQ does not support bfloat16",
    "body": "Currently, our GPTQ kernels only support the float16 precision.",
    "labels": [
      "help wanted",
      "feature request",
      "quantization",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-17T06:06:30+00:00",
    "closed_at": "2024-11-30T02:03:24+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2149/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2149"
  },
  {
    "number": 16353,
    "title": "[Feature]: Run performance benchmarks for multi-modal models in CI",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nWe currently only have benchmarks for text-only models such as Llama. With the increasing importance of multi-modality and related optimizations such as processor cache, we should add performance benchmarks for multi-modal models to avoid regressions (e.g. memory leaks, slow batching).\n\nWe can measure the peak memory usage based on this code:\n\n```python\nimport resource\n\nmax_self_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / (1 << 20)\nmax_children_usage = resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss / (1 << 20)\nprint(f\"Peak memory usage: {max_self_usage} (self) + {max_children_usage} (children) GiB\")\n```\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\ncc @mgoin @ywang96 \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "help wanted",
      "feature request",
      "stale",
      "multi-modality"
    ],
    "state": "open",
    "created_at": "2025-04-09T16:48:25+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16353/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/16353"
  },
  {
    "number": 16054,
    "title": "[Bug]: CI flake - v1/engine/test_async_llm.py::test_abort - assert has_unfinished_requests()",
    "body": "### Your current environment\n\n...\n\n### \ud83d\udc1b Describe the bug\n\nmain commit 51d7c6a2b23e100cd9e7d85b8e7c0eea656b331e\n\nSeen in https://github.com/vllm-project/vllm/pull/15894\n\nhttps://buildkite.com/organizations/vllm/pipelines/ci/builds/16742/jobs/0195f24d-e81a-46a3-ad08-6a51983d65d6/log\n\n\n```\n=================================== FAILURES ===================================\n[2025-04-01T17:38:12Z] _ test_abort[engine_args0-Hello my name is Robert and-RequestOutputKind.DELTA] _\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z] monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fd1fa052e70>\n[2025-04-01T17:38:12Z] output_kind = <RequestOutputKind.DELTA: 1>\n[2025-04-01T17:38:12Z] engine_args = AsyncEngineArgs(model='meta-llama/Llama-3.2-1B-Instruct', served_model_name=None, tokenizer='meta-llama/Llama-3.2-1B-I...additional_config=None, enable_reasoning=None, reasoning_parser=None, use_tqdm_on_load=True, disable_log_requests=True)\n[2025-04-01T17:38:12Z] prompt = 'Hello my name is Robert and'\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]     @pytest.mark.parametrize(\n[2025-04-01T17:38:12Z]         \"output_kind\", [RequestOutputKind.DELTA, RequestOutputKind.FINAL_ONLY])\n[2025-04-01T17:38:12Z]     @pytest.mark.parametrize(\"engine_args,prompt\",\n[2025-04-01T17:38:12Z]                              [(TEXT_ENGINE_ARGS, TEXT_PROMPT),\n[2025-04-01T17:38:12Z]                               (VISION_ENGINE_ARGS, VISION_PROMPT)])\n[2025-04-01T17:38:12Z]     @pytest.mark.asyncio\n[2025-04-01T17:38:12Z]     async def test_abort(monkeypatch: pytest.MonkeyPatch,\n[2025-04-01T17:38:12Z]                          output_kind: RequestOutputKind,\n[2025-04-01T17:38:12Z]                          engine_args: AsyncEngineArgs, prompt: PromptType):\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]         with monkeypatch.context() as m, ExitStack() as after:\n[2025-04-01T17:38:12Z]             m.setenv(\"VLLM_USE_V1\", \"1\")\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             engine = AsyncLLM.from_engine_args(engine_args)\n[2025-04-01T17:38:12Z]             after.callback(engine.shutdown)\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             NUM_REQUESTS = 100\n[2025-04-01T17:38:12Z]             NUM_EXPECTED_TOKENS = 100\n[2025-04-01T17:38:12Z]             NUM_EXPECTED_TOKENS_LONG = 50000\n[2025-04-01T17:38:12Z]             REQUEST_IDS_TO_ABORT = range(1, 100, 10)\n[2025-04-01T17:38:12Z]             PARALLEL_SAMPLE_REQ_IDS = range(1, 100, 15)\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             request_ids = [f\"request-{i}\" for i in range(NUM_REQUESTS)]\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             # Create concurrent requests.\n[2025-04-01T17:38:12Z]             tasks: list[asyncio.Task] = []\n[2025-04-01T17:38:12Z]             for idx, request_id in enumerate(request_ids):\n[2025-04-01T17:38:12Z]                 max_tokens = NUM_EXPECTED_TOKENS_LONG if (\n[2025-04-01T17:38:12Z]                     idx in REQUEST_IDS_TO_ABORT) else NUM_EXPECTED_TOKENS\n[2025-04-01T17:38:12Z]                 n = 3 if idx in PARALLEL_SAMPLE_REQ_IDS else 1\n[2025-04-01T17:38:12Z]                 tasks.append(\n[2025-04-01T17:38:12Z]                     asyncio.create_task(\n[2025-04-01T17:38:12Z]                         generate(engine, request_id, prompt, output_kind,\n[2025-04-01T17:38:12Z]                                  max_tokens, n)))\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             # API server cancels requests when they disconnect.\n[2025-04-01T17:38:12Z]             for idx in REQUEST_IDS_TO_ABORT:\n[2025-04-01T17:38:12Z]                 tasks[idx].cancel()\n[2025-04-01T17:38:12Z]                 await asyncio.sleep(0.1)\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             # Confirm the other requests are okay.\n[2025-04-01T17:38:12Z]             for idx, task in enumerate(tasks):\n[2025-04-01T17:38:12Z]                 # Confirm that it was actually canceled.\n[2025-04-01T17:38:12Z]                 if idx in REQUEST_IDS_TO_ABORT:\n[2025-04-01T17:38:12Z]                     with pytest.raises(asyncio.CancelledError):\n[2025-04-01T17:38:12Z]                         await task\n[2025-04-01T17:38:12Z]                 else:\n[2025-04-01T17:38:12Z]                     # Otherwise, make sure the request was not impacted.\n[2025-04-01T17:38:12Z]                     num_generated_tokens, request_id = await task\n[2025-04-01T17:38:12Z]                     n = 3 if idx in PARALLEL_SAMPLE_REQ_IDS else 1\n[2025-04-01T17:38:12Z]                     expected_tokens = NUM_EXPECTED_TOKENS * n\n[2025-04-01T17:38:12Z]                     assert num_generated_tokens == expected_tokens, (\n[2025-04-01T17:38:12Z]                         f\"{request_id} generated {num_generated_tokens} but \"\n[2025-04-01T17:38:12Z]                         f\"expected {expected_tokens}\")\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             # Make sure all aborted requests were really aborted.\n[2025-04-01T17:38:12Z] >           assert not engine.output_processor.has_unfinished_requests()\n[2025-04-01T17:38:12Z] E           assert not True\n[2025-04-01T17:38:12Z] E            +  where True = has_unfinished_requests()\n[2025-04-01T17:38:12Z] E            +    where has_unfinished_requests = <vllm.v1.engine.output_processor.OutputProcessor object at 0x7fd1ef1614c0>.has_unfinished_requests\n[2025-04-01T17:38:12Z] E            +      where <vllm.v1.engine.output_processor.OutputProcessor object at 0x7fd1ef1614c0> = <vllm.v1.engine.async_llm.AsyncLLM object at 0x7fd1ef132750>.output_processor\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z] v1/engine/test_async_llm.py:178: AssertionError\n[2025-04-01T17:38:12Z] =============================== warnings summary ===============================\n[2025-04-01T17:38:12Z] tests/v1/engine/test_async_llm.py: 12 warnings\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core_client.py: 1 warning\n[2025-04-01T17:38:12Z] tests/v1/engine/test_llm_engine.py: 2 warnings\n[2025-04-01T17:38:12Z]   /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1700) is multi-threaded, use of fork() may lead to deadlocks in the child.\n[2025-04-01T17:38:12Z]     self.pid = os.fork()\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core.py::test_engine_core\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core.py::test_engine_core_advanced_sampling\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core.py::test_engine_core_concurrent_batches\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core_client.py::test_engine_core_client[True]\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core_client.py::test_engine_core_client[False]\n[2025-04-01T17:38:12Z]   /vllm-workspace/tests/utils.py:720: DeprecationWarning: This process (pid=1700) is multi-threaded, use of fork() may lead to deadlocks in the child.\n[2025-04-01T17:38:12Z]     pid = os.fork()\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z] -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n[2025-04-01T17:38:12Z] =========================== short test summary info ============================\n[2025-04-01T17:38:12Z] FAILED v1/engine/test_async_llm.py::test_abort[engine_args0-Hello my name is Robert and-RequestOutputKind.DELTA] - assert not True\n[2025-04-01T17:38:12Z]  +  where True = has_unfinished_requests()\n[2025-04-01T17:38:12Z]  +    where has_unfinished_requests = <vllm.v1.engine.output_processor.OutputProcessor object at 0x7fd1ef1614c0>.has_unfinished_requests\n[2025-04-01T17:38:12Z]  +      where <vllm.v1.engine.output_processor.OutputProcessor object at 0x7fd1ef1614c0> = <vllm.v1.engine.async_llm.AsyncLLM object at 0x7fd1ef132750>.output_processor\n[2025-04-01T17:38:12Z] ============ 1 failed, 44 passed, 20 warnings in 1059.59s (0:17:39) ============\n[2025-04-01T17:38:14Z] \ud83d\udea8 Error: The command exited with status 1\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci/build",
      "stale",
      "v1"
    ],
    "state": "open",
    "created_at": "2025-04-04T09:48:13+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16054/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/16054"
  },
  {
    "number": 16053,
    "title": "[Bug]: CI flake - v1/entrypoints/llm/test_struct_output_generate.py::test_structured_output - JSONDecodeError: Expecting value: line 1 column 1 (char 0)",
    "body": "### Your current environment\n\n...\n\n\n### \ud83d\udc1b Describe the bug\n\nmain commit 51d7c6a2b23e100cd9e7d85b8e7c0eea656b331e\n\nSeen in https://github.com/vllm-project/vllm/pull/15894\n\nhttps://buildkite.com/organizations/vllm/pipelines/ci/builds/16742/jobs/0195fc58-3d11-45b5-b76f-8e962cbda765/log\n\n```\nFAILED v1/entrypoints/llm/test_struct_output_generate.py::test_structured_output[Qwen/Qwen2.5-1.5B-Instruct-guidance:disable-any-whitespace-auto] - json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n[2025-04-03T16:08:35Z] _ test_structured_output[Qwen/Qwen2.5-1.5B-Instruct-guidance:disable-any-whitespace-auto] _\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z] monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f318d89eb40>\n[2025-04-03T16:08:35Z] sample_json_schema = {'properties': {'age': {'type': 'integer'}, 'name': {'type': 'string'}, 'skills': {'items': {'type': 'string'}, 'type'...ition'], 'type': 'object'}, 'type': 'array'}}, 'required': ['name', 'age', 'skills', 'work_history'], 'type': 'object'}\n[2025-04-03T16:08:35Z] unsupported_json_schema = {'properties': {'email': {'pattern': '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$', 'type': 'string'}, 'grade': ...[a-z]{1,10}$', 'type': 'string'}, 'type': 'array'}}, 'required': ['score', 'grade', 'email', 'tags'], 'type': 'object'}\n[2025-04-03T16:08:35Z] sample_sql_ebnf = '\\nroot ::= select_statement\\nselect_statement ::= \"SELECT\" column \"from\" table \"where\" condition\\ncolumn ::= \"col_1\" | \"col_2\"\\ntable ::= \"table_1\" | \"table_2\"\\ncondition ::= column \"=\" number\\nnumber ::= \"1\" | \"2\"\\n'\n[2025-04-03T16:08:35Z] sample_sql_lark = '\\nstart: select_statement\\nselect_statement: \"SELECT\" column \"from\" table \"where\" condition\\ncolumn: \"col_1\" | \"col_2\"\\ntable: \"table_1\" | \"table_2\"\\ncondition: column \"=\" number\\nnumber: \"1\" | \"2\"\\n'\n[2025-04-03T16:08:35Z] sample_regex = '((25[0-5]|(2[0-4]|1\\\\d|[1-9]|)\\\\d)\\\\.){3}(25[0-5]|(2[0-4]|1\\\\d|[1-9]|)\\\\d)'\n[2025-04-03T16:08:35Z] sample_guided_choice = ['Python', 'Java', 'JavaScript', 'C++', 'C#', 'PHP', ...]\n[2025-04-03T16:08:35Z] guided_decoding_backend = 'guidance:disable-any-whitespace'\n[2025-04-03T16:08:35Z] tokenizer_mode = 'auto', model_name = 'Qwen/Qwen2.5-1.5B-Instruct'\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]     @pytest.mark.skip_global_cleanup\n[2025-04-03T16:08:35Z]     @pytest.mark.parametrize(\"model_name, guided_decoding_backend, tokenizer_mode\",\n[2025-04-03T16:08:35Z]                              PARAMS_MODELS_BACKENDS_TOKENIZER_MODE)\n[2025-04-03T16:08:35Z]     def test_structured_output(\n[2025-04-03T16:08:35Z]         monkeypatch: pytest.MonkeyPatch,\n[2025-04-03T16:08:35Z]         sample_json_schema: dict[str, Any],\n[2025-04-03T16:08:35Z]         unsupported_json_schema: dict[str, Any],\n[2025-04-03T16:08:35Z]         sample_sql_ebnf: str,\n[2025-04-03T16:08:35Z]         sample_sql_lark: str,\n[2025-04-03T16:08:35Z]         sample_regex: str,\n[2025-04-03T16:08:35Z]         sample_guided_choice: str,\n[2025-04-03T16:08:35Z]         guided_decoding_backend: str,\n[2025-04-03T16:08:35Z]         tokenizer_mode: str,\n[2025-04-03T16:08:35Z]         model_name: str,\n[2025-04-03T16:08:35Z]     ):\n[2025-04-03T16:08:35Z]         monkeypatch.setenv(\"VLLM_USE_V1\", \"1\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         # Use a single LLM instance for several scenarios to\n[2025-04-03T16:08:35Z]         # speed up the test suite.\n[2025-04-03T16:08:35Z]         llm = LLM(model=model_name,\n[2025-04-03T16:08:35Z]                   enforce_eager=True,\n[2025-04-03T16:08:35Z]                   max_model_len=1024,\n[2025-04-03T16:08:35Z]                   guided_decoding_backend=guided_decoding_backend,\n[2025-04-03T16:08:35Z]                   tokenizer_mode=tokenizer_mode)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 1: Generate JSON output based on a provided schema\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=1.0,\n[2025-04-03T16:08:35Z]             max_tokens=1000,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(json=sample_json_schema))\n[2025-04-03T16:08:35Z]         outputs = llm.generate(prompts=[\n[2025-04-03T16:08:35Z]             f\"Give an example JSON for an employee profile \"\n[2025-04-03T16:08:35Z]             f\"that fits this schema: {sample_json_schema}\"\n[2025-04-03T16:08:35Z]         ] * 2,\n[2025-04-03T16:08:35Z]                                sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]                                use_tqdm=True)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]             prompt = output.prompt\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]             assert generated_text is not None\n[2025-04-03T16:08:35Z]             if 'disable-any-whitespace' in guided_decoding_backend:\n[2025-04-03T16:08:35Z]                 assert \"\\n\" not in generated_text\n[2025-04-03T16:08:35Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n[2025-04-03T16:08:35Z]             output_json = json.loads(generated_text)\n[2025-04-03T16:08:35Z]             jsonschema.validate(instance=output_json, schema=sample_json_schema)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 2: Generate JSON object without a schema\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=1.0,\n[2025-04-03T16:08:35Z]             max_tokens=100,\n[2025-04-03T16:08:35Z]             n=2,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(json_object=True))\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         outputs = llm.generate(\n[2025-04-03T16:08:35Z]             prompts=(\"Generate a JSON object with curly braces for a person with \"\n[2025-04-03T16:08:35Z]                      \"name and age fields for John Smith who is 31 years old.\"),\n[2025-04-03T16:08:35Z]             sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]             use_tqdm=True)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             for i in range(2):\n[2025-04-03T16:08:35Z]                 generated_text = output.outputs[i].text\n[2025-04-03T16:08:35Z]                 print(generated_text)\n[2025-04-03T16:08:35Z]                 assert generated_text is not None\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]                 # Parse to verify it is valid JSON\n[2025-04-03T16:08:35Z]                 parsed_json = json.loads(generated_text)\n[2025-04-03T16:08:35Z]                 allowed_types: tuple[type, ...] = (dict, )\n[2025-04-03T16:08:35Z]                 if guided_decoding_backend.startswith(\"xgrammar\"):\n[2025-04-03T16:08:35Z]                     # TODO - we are currently too permissive with xgrammar and\n[2025-04-03T16:08:35Z]                     # allow # any valid json (typically comes back as a list or\n[2025-04-03T16:08:35Z]                     # object).  We can fix this by specifying a jsonschema of\n[2025-04-03T16:08:35Z]                     # {\"type\": \"object\"}, # but we need this fix in a release\n[2025-04-03T16:08:35Z]                     # first: https://github.com/mlc-ai/xgrammar/pull/264\n[2025-04-03T16:08:35Z]                     allowed_types = (dict, list)\n[2025-04-03T16:08:35Z]                 assert isinstance(parsed_json, allowed_types)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 3: test a jsonschema incompatible with xgrammar\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=1.0,\n[2025-04-03T16:08:35Z]             max_tokens=1000,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(json=unsupported_json_schema))\n[2025-04-03T16:08:35Z]         if guided_decoding_backend.startswith(\"xgrammar\"):\n[2025-04-03T16:08:35Z]             with pytest.raises(ValueError,\n[2025-04-03T16:08:35Z]                                match=\"The provided JSON schema contains features \"\n[2025-04-03T16:08:35Z]                                \"not supported by xgrammar.\"):\n[2025-04-03T16:08:35Z]                 llm.generate(prompts=[\n[2025-04-03T16:08:35Z]                     f\"Give an example JSON for an employee profile \"\n[2025-04-03T16:08:35Z]                     f\"that fits this schema: {unsupported_json_schema}\"\n[2025-04-03T16:08:35Z]                 ] * 2,\n[2025-04-03T16:08:35Z]                              sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]                              use_tqdm=True)\n[2025-04-03T16:08:35Z]         else:\n[2025-04-03T16:08:35Z]             outputs = llm.generate(\n[2025-04-03T16:08:35Z]                 prompts=(\"Give an example JSON object for a grade \"\n[2025-04-03T16:08:35Z]                          \"that fits this schema: \"\n[2025-04-03T16:08:35Z]                          f\"{unsupported_json_schema}\"),\n[2025-04-03T16:08:35Z]                 sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]                 use_tqdm=True)\n[2025-04-03T16:08:35Z]             assert outputs is not None\n[2025-04-03T16:08:35Z]             for output in outputs:\n[2025-04-03T16:08:35Z]                 assert output is not None\n[2025-04-03T16:08:35Z]                 assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]                 generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]                 assert generated_text is not None\n[2025-04-03T16:08:35Z]                 print(generated_text)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]                 # Parse to verify it is valid JSON\n[2025-04-03T16:08:35Z]                 parsed_json = json.loads(generated_text)\n[2025-04-03T16:08:35Z]                 assert isinstance(parsed_json, dict)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 4: Generate SQL statement using EBNF grammar\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=0.8,\n[2025-04-03T16:08:35Z]             top_p=0.95,\n[2025-04-03T16:08:35Z]             max_tokens=1000,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(grammar=sample_sql_ebnf))\n[2025-04-03T16:08:35Z]         outputs = llm.generate(\n[2025-04-03T16:08:35Z]             prompts=(\"Generate a sql statement that selects col_1 from \"\n[2025-04-03T16:08:35Z]                      \"table_1 where it is equal to 1\"),\n[2025-04-03T16:08:35Z]             sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]             use_tqdm=True,\n[2025-04-03T16:08:35Z]         )\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]             prompt = output.prompt\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]             assert generated_text is not None\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             # remove spaces for comparison b/c we removed them in the grammar\n[2025-04-03T16:08:35Z]             ground_truth = \"SELECT col_1 from table_1 where col_1 = 1\".replace(\n[2025-04-03T16:08:35Z]                 \" \", \"\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             assert generated_text.strip() == ground_truth\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 5: Generate SQL statement using Lark grammar\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=0.8,\n[2025-04-03T16:08:35Z]             top_p=0.95,\n[2025-04-03T16:08:35Z]             max_tokens=1000,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(grammar=sample_sql_lark))\n[2025-04-03T16:08:35Z]         outputs = llm.generate(\n[2025-04-03T16:08:35Z]             prompts=(\"Generate a sql statement that selects col_1 from \"\n[2025-04-03T16:08:35Z]                      \"table_1 where it is equal to 1\"),\n[2025-04-03T16:08:35Z]             sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]             use_tqdm=True,\n[2025-04-03T16:08:35Z]         )\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]             prompt = output.prompt\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]             assert generated_text is not None\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             # use Lark to parse the output, and make sure it's a valid parse tree\n[2025-04-03T16:08:35Z]             from lark import Lark\n[2025-04-03T16:08:35Z]             parser = Lark(sample_sql_lark)\n[2025-04-03T16:08:35Z]             parser.parse(generated_text)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             # remove spaces for comparison b/c we removed them in the grammar\n[2025-04-03T16:08:35Z]             ground_truth = \"SELECT col_1 from table_1 where col_1 = 1\".replace(\n[2025-04-03T16:08:35Z]                 \" \", \"\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             assert generated_text.strip() == ground_truth\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 6: Test invalid grammar input\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=0.8,\n[2025-04-03T16:08:35Z]             top_p=0.95,\n[2025-04-03T16:08:35Z]             max_tokens=1000,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(grammar=\"not a grammar\"))\n[2025-04-03T16:08:35Z]         with pytest.raises(ValueError, match=\"Failed to convert the grammar \"):\n[2025-04-03T16:08:35Z]             llm.generate(\n[2025-04-03T16:08:35Z]                 prompts=(\"Generate a sql statement that selects col_1 from \"\n[2025-04-03T16:08:35Z]                          \"table_1 where it is equal to 1\"),\n[2025-04-03T16:08:35Z]                 sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]                 use_tqdm=True,\n[2025-04-03T16:08:35Z]             )\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 7: Generate text based on a regex pattern\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=0.8,\n[2025-04-03T16:08:35Z]             top_p=0.95,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(regex=sample_regex))\n[2025-04-03T16:08:35Z]         outputs = llm.generate(\n[2025-04-03T16:08:35Z]             prompts=[\n[2025-04-03T16:08:35Z]                 f\"Give an example IPv4 address with this regex: {sample_regex}\"\n[2025-04-03T16:08:35Z]             ] * 2,\n[2025-04-03T16:08:35Z]             sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]             use_tqdm=True,\n[2025-04-03T16:08:35Z]         )\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]             prompt = output.prompt\n[2025-04-03T16:08:35Z]             generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]             print(generated_text)\n[2025-04-03T16:08:35Z]             assert generated_text is not None\n[2025-04-03T16:08:35Z]             assert re.fullmatch(sample_regex, generated_text) is not None\n[2025-04-03T16:08:35Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 8: Generate text based on a choices\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=0.8,\n[2025-04-03T16:08:35Z]             top_p=0.95,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(choice=sample_guided_choice))\n[2025-04-03T16:08:35Z]         outputs = llm.generate(\n[2025-04-03T16:08:35Z]             prompts=\"The best language for type-safe systems programming is \",\n[2025-04-03T16:08:35Z]             sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]             use_tqdm=True)\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]             prompt = output.prompt\n[2025-04-03T16:08:35Z]             generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]             print(generated_text)\n[2025-04-03T16:08:35Z]             assert generated_text is not None\n[2025-04-03T16:08:35Z]             assert generated_text in sample_guided_choice\n[2025-04-03T16:08:35Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 9: Generate structured output using a Pydantic model with an enum\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         json_schema = CarDescription.model_json_schema()\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=1.0,\n[2025-04-03T16:08:35Z]             max_tokens=1000,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(json=json_schema))\n[2025-04-03T16:08:35Z]         outputs = llm.generate(\n[2025-04-03T16:08:35Z]             prompts=\"Generate a JSON with the brand, model and car_type of\"\n[2025-04-03T16:08:35Z]             \"the most iconic car from the 90's\",\n[2025-04-03T16:08:35Z]             sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]             use_tqdm=True)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]             prompt = output.prompt\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]             assert generated_text is not None\n[2025-04-03T16:08:35Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n[2025-04-03T16:08:35Z] >           output_json = json.loads(generated_text)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z] v1/entrypoints/llm/test_struct_output_generate.py:332:\n[2025-04-03T16:08:35Z] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n[2025-04-03T16:08:35Z] /usr/lib/python3.12/json/__init__.py:346: in loads\n[2025-04-03T16:08:35Z]     return _default_decoder.decode(s)\n[2025-04-03T16:08:35Z] /usr/lib/python3.12/json/decoder.py:338: in decode\n[2025-04-03T16:08:35Z]     obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n[2025-04-03T16:08:35Z] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z] self = <json.decoder.JSONDecoder object at 0x7f32ee3035c0>, s = '', idx = 0\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]     def raw_decode(self, s, idx=0):\n[2025-04-03T16:08:35Z]         \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n[2025-04-03T16:08:35Z]         a JSON document) and return a 2-tuple of the Python\n[2025-04-03T16:08:35Z]         representation and the index in ``s`` where the document ended.\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         This can be used to decode a JSON document from a string that may\n[2025-04-03T16:08:35Z]         have extraneous data at the end.\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         \"\"\"\n[2025-04-03T16:08:35Z]         try:\n[2025-04-03T16:08:35Z]             obj, end = self.scan_once(s, idx)\n[2025-04-03T16:08:35Z]         except StopIteration as err:\n[2025-04-03T16:08:35Z] >           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n[2025-04-03T16:08:35Z] E           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z] /usr/lib/python3.12/json/decoder.py:356: JSONDecodeError\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci/build",
      "stale",
      "v1"
    ],
    "state": "open",
    "created_at": "2025-04-04T09:46:16+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16053/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/16053"
  },
  {
    "number": 15855,
    "title": "[Bug]: CI flake - v1/engine/test_llm_engine.py::test_parallel_sampling[True]",
    "body": "### Your current environment\n\n...\n\n\n### \ud83d\udc1b Describe the bug\n\nSaw V1 test failing with this yesterday, went away with recheck:\n\n```\n[2025-03-31T17:33:47Z] _________________________ test_parallel_sampling[True] _________________________\n[2025-03-31T17:33:47Z]\n[2025-03-31T17:33:47Z] vllm_model = <tests.conftest.VllmRunner object at 0x7f0d875e06e0>\n[2025-03-31T17:33:47Z] example_prompts = ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the majo...me.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', ...]\n[2025-03-31T17:33:47Z]\n[2025-03-31T17:33:47Z]     def test_parallel_sampling(vllm_model, example_prompts) -> None:\n[2025-03-31T17:33:47Z]         \"\"\"Test passes if parallel sampling `n>1` yields `n` unique completions.\n[2025-03-31T17:33:47Z]\n[2025-03-31T17:33:47Z]         Args:\n[2025-03-31T17:33:47Z]           vllm_model: VllmRunner instance under test.\n[2025-03-31T17:33:47Z]           example_prompt: test fixture providing prompts for testing.\n[2025-03-31T17:33:47Z]         \"\"\"\n[2025-03-31T17:33:47Z]         sampling_params_list, n_list = _get_test_sampling_params(example_prompts)\n[2025-03-31T17:33:47Z]         model: LLM = vllm_model.model\n[2025-03-31T17:33:47Z]         outputs = model.generate(example_prompts, sampling_params_list)\n[2025-03-31T17:33:47Z]\n[2025-03-31T17:33:47Z]         # Validate each request response\n[2025-03-31T17:33:47Z]         for out, n in zip(outputs, n_list):\n[2025-03-31T17:33:47Z]             completion_counts: dict[str, int] = {}\n[2025-03-31T17:33:47Z]             # Assert correct number of completions\n[2025-03-31T17:33:47Z]             assert len(out.outputs) == n, (\n[2025-03-31T17:33:47Z]                 f\"{len(out.outputs)} completions; {n} expected.\")\n[2025-03-31T17:33:47Z]             for idx in range(n):\n[2025-03-31T17:33:47Z]                 comp = out.outputs[idx]\n[2025-03-31T17:33:47Z]                 # Assert correct completion indices\n[2025-03-31T17:33:47Z]                 assert comp.index == idx, (f\"Index {comp.index}; expected {idx}.\")\n[2025-03-31T17:33:47Z]                 text = comp.text\n[2025-03-31T17:33:47Z]                 completion_counts[text] = completion_counts.get(text, 0) + 1\n[2025-03-31T17:33:47Z]             # Assert unique completions\n[2025-03-31T17:33:47Z]             if len(completion_counts) != n:\n[2025-03-31T17:33:47Z]                 repeats = {\n[2025-03-31T17:33:47Z]                     txt: num\n[2025-03-31T17:33:47Z]                     for (txt, num) in completion_counts.items() if num > 1\n[2025-03-31T17:33:47Z]                 }\n[2025-03-31T17:33:47Z] >               raise AssertionError(\n[2025-03-31T17:33:47Z]                     f\"{len(completion_counts)} unique completions; expected\"\n[2025-03-31T17:33:47Z]                     f\" {n}. Repeats: {repeats}\")\n[2025-03-31T17:33:47Z] E               AssertionError: 19 unique completions; expected 20. Repeats: {'\\nWrite a short story about a robot that dreams for the first time.\\n': 2}\n[2025-03-31T17:33:47Z]\n[2025-03-31T17:33:47Z] v1/engine/test_llm_engine.py:97: AssertionError\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci/build",
      "stale",
      "v1"
    ],
    "state": "open",
    "created_at": "2025-04-01T06:55:01+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15855/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/15855"
  },
  {
    "number": 15710,
    "title": "[Feature]: Support Cascade Attention for Sliding Window Attention",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently, vLLM does not support cascade attention for sliding window attention:\n\nhttps://github.com/vllm-project/vllm/blob/3b00ff91380044fa409612401309b9cb6a82685f/vllm/v1/attention/backends/flash_attn.py#L352-L354\n\nHowever, it is technically possible to use it in specific cases. For instance, when the context lengths of all requests in the batch do not exceed the sliding window size, it functions the same as global attention, making it suitable for cascade attention.\n\nAs such, we should expand the coverage of cascade attention with sliding window.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "help wanted",
      "good first issue",
      "feature request",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-28T14:55:29+00:00",
    "closed_at": null,
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15710/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15710"
  },
  {
    "number": 15394,
    "title": "[Bug]: RequestMetrics object (accessed through output[0].metrics) is None",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 03-24 12:02:48 [__init__.py:256] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.12.9 (main, Feb 24 2025, 10:05:14) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-131-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H200\nNvidia driver version: 550.127.08\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        43 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               16\nOn-line CPU(s) list:                  0-15\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8468\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   8\nSocket(s):                            1\nStepping:                             8\nBogoMIPS:                             4200.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk avx512_fp16 arch_capabilities\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            512 KiB (16 instances)\nL1i cache:                            512 KiB (16 instances)\nL2 cache:                             32 MiB (8 instances)\nL3 cache:                             16 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-15\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Unknown: No mitigations\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Mitigation; TSX disabled\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.50.0\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-15    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64:/usr/mpi/gcc/openmpi-4.1.7a1/lib\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen I try to access the `RequestMetrics` object (through e.g, `output[0].metrics`), it is `None`.  I can only access it when I try a Speculative Decoding configuration.\n\nExample code to reproduce it:\n\n```python\nfrom vllm import LLM\n\nllm = LLM(\n    model=\"facebook/opt-125m\",\n)\n\noutputs = llm.generate(\"Hello, world!\")\n\nassert outputs[0].metrics is not None\nprint(outputs[0].metrics)\n```\n\nOutput:\n\n```\nDEBUG 03-24 12:09:29 [__init__.py:28] No plugins for group vllm.platform_plugins found.\nDEBUG 03-24 12:09:29 [__init__.py:35] Checking if TPU platform is available.\nDEBUG 03-24 12:09:29 [__init__.py:45] TPU platform is not available because: No module named 'libtpu'\nDEBUG 03-24 12:09:29 [__init__.py:53] Checking if CUDA platform is available.\nDEBUG 03-24 12:09:29 [__init__.py:73] Confirmed CUDA platform is available.\nDEBUG 03-24 12:09:29 [__init__.py:101] Checking if ROCm platform is available.\nDEBUG 03-24 12:09:29 [__init__.py:115] ROCm platform is not available because: No module named 'amdsmi'\nDEBUG 03-24 12:09:29 [__init__.py:123] Checking if HPU platform is available.\nDEBUG 03-24 12:09:29 [__init__.py:130] HPU platform is not available because habana_frameworks is not found.\nDEBUG 03-24 12:09:29 [__init__.py:141] Checking if XPU platform is available.\nDEBUG 03-24 12:09:29 [__init__.py:151] XPU platform is not available because: No module named 'intel_extension_for_pytorch'\nDEBUG 03-24 12:09:29 [__init__.py:159] Checking if CPU platform is available.\nDEBUG 03-24 12:09:29 [__init__.py:181] Checking if Neuron platform is available.\nDEBUG 03-24 12:09:29 [__init__.py:188] Neuron platform is not available because: No module named 'transformers_neuronx'\nDEBUG 03-24 12:09:29 [__init__.py:196] Checking if OpenVINO platform is available.\nDEBUG 03-24 12:09:29 [__init__.py:203] OpenVINO platform is not available because vLLM is not built with OpenVINO.\nDEBUG 03-24 12:09:29 [__init__.py:53] Checking if CUDA platform is available.\nDEBUG 03-24 12:09:29 [__init__.py:73] Confirmed CUDA platform is available.\nINFO 03-24 12:09:29 [__init__.py:256] Automatically detected platform cuda.\nDEBUG 03-24 12:09:30 [__init__.py:28] No plugins for group vllm.general_plugins found.\nINFO 03-24 12:09:37 [config.py:583] This model supports multiple tasks: {'classify', 'score', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.\nDEBUG 03-24 12:09:37 [arg_utils.py:1722] Setting max_num_batched_tokens to 16384 for LLM_CLASS usage context.\nDEBUG 03-24 12:09:37 [arg_utils.py:1730] Setting max_num_seqs to 1024 for LLM_CLASS usage context.\nINFO 03-24 12:09:37 [config.py:1693] Chunked prefill is enabled with max_num_batched_tokens=16384.\nINFO 03-24 12:09:38 [core.py:53] Initializing a V1 LLM engine (v0.8.1) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=facebook/opt-125m, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\nWARNING 03-24 12:09:38 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f03ffabb0e0>\nDEBUG 03-24 12:09:38 [config.py:3658] enabled custom ops: Counter()\nDEBUG 03-24 12:09:38 [config.py:3660] disabled custom ops: Counter()\nDEBUG 03-24 12:09:39 [parallel_state.py:817] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.0:50567 backend=nccl\nINFO 03-24 12:09:39 [parallel_state.py:967] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 03-24 12:09:39 [cuda.py:215] Using Flash Attention backend on V1 engine.\nDEBUG 03-24 12:09:39 [config.py:3658] enabled custom ops: Counter()\nDEBUG 03-24 12:09:39 [config.py:3660] disabled custom ops: Counter()\nINFO 03-24 12:09:39 [gpu_model_runner.py:1164] Starting to load model facebook/opt-125m...\nDEBUG 03-24 12:09:39 [decorators.py:109] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.opt.OPTModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']\nWARNING 03-24 12:09:39 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\nDEBUG 03-24 12:09:39 [config.py:3658] enabled custom ops: Counter()\nDEBUG 03-24 12:09:39 [config.py:3660] disabled custom ops: Counter()\nINFO 03-24 12:09:39 [weight_utils.py:257] Using model weights format ['*.bin']\nLoading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.00it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.00it/s]\n\nINFO 03-24 12:09:40 [loader.py:429] Loading weights took 0.17 seconds\nINFO 03-24 12:09:40 [gpu_model_runner.py:1176] Model loading took 0.2389 GB and 0.917760 seconds\nDEBUG 03-24 12:09:40 [decorators.py:203] Start compiling function <code object forward at 0x7f03f3f59e30, file \"/home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/vllm/model_executor/models/opt.py\", line 304>\nDEBUG 03-24 12:09:41 [backends.py:370] Traced files (to be considered for compilation cache):\nDEBUG 03-24 12:09:41 [backends.py:370] /home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/_dynamo/polyfills/builtins.py\nDEBUG 03-24 12:09:41 [backends.py:370] /home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/nn/modules/activation.py\nDEBUG 03-24 12:09:41 [backends.py:370] /home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/nn/modules/container.py\nDEBUG 03-24 12:09:41 [backends.py:370] /home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\nDEBUG 03-24 12:09:41 [backends.py:370] /home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/nn/modules/normalization.py\nDEBUG 03-24 12:09:41 [backends.py:370] /home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/nn/modules/sparse.py\nDEBUG 03-24 12:09:41 [backends.py:370] /home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/vllm/attention/layer.py\nDEBUG 03-24 12:09:41 [backends.py:370] /home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/vllm/distributed/communication_op.py\nDEBUG 03-24 12:09:41 [backends.py:370] /home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/vllm/distributed/parallel_state.py\nDEBUG 03-24 12:09:41 [backends.py:370] /home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py\nDEBUG 03-24 12:09:41 [backends.py:370] /home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\nDEBUG 03-24 12:09:41 [backends.py:370] /home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/vllm/model_executor/models/opt.py\nINFO 03-24 12:09:42 [backends.py:409] Using cache directory: /home/search/.cache/vllm/torch_compile_cache/0766410721/rank_0_0 for vLLM's torch.compile\nINFO 03-24 12:09:42 [backends.py:419] Dynamo bytecode transform time: 1.61 s\nDEBUG 03-24 12:09:42 [backends.py:86] Directly load the 0-th graph for shape None from inductor via handle ('fg4yjpexwibuoc4czsrij5e6ijwntxxrnkqkukuxs7ogttixolpj', '/home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/_inductor/utils.py')\nINFO 03-24 12:09:42 [backends.py:115] Directly load the compiled graph for shape None from the cache\nDEBUG 03-24 12:09:42 [backends.py:86] Directly load the 1-th graph for shape None from inductor via handle ('fsgvh3fgazwmztcgpeggl43axs5agy5wwcjhp7vqopraerya5vq3', '/home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/_inductor/utils.py')\nDEBUG 03-24 12:09:42 [backends.py:86] Directly load the 2-th graph for shape None from inductor via handle ('fsgvh3fgazwmztcgpeggl43axs5agy5wwcjhp7vqopraerya5vq3', '/home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/_inductor/utils.py')\nDEBUG 03-24 12:09:42 [backends.py:86] Directly load the 3-th graph for shape None from inductor via handle ('fsgvh3fgazwmztcgpeggl43axs5agy5wwcjhp7vqopraerya5vq3', '/home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/_inductor/utils.py')\nDEBUG 03-24 12:09:42 [backends.py:86] Directly load the 4-th graph for shape None from inductor via handle ('fsgvh3fgazwmztcgpeggl43axs5agy5wwcjhp7vqopraerya5vq3', '/home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/_inductor/utils.py')\nDEBUG 03-24 12:09:42 [backends.py:86] Directly load the 5-th graph for shape None from inductor via handle ('fsgvh3fgazwmztcgpeggl43axs5agy5wwcjhp7vqopraerya5vq3', '/home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/_inductor/utils.py')\nDEBUG 03-24 12:09:42 [backends.py:86] Directly load the 6-th graph for shape None from inductor via handle ('fsgvh3fgazwmztcgpeggl43axs5agy5wwcjhp7vqopraerya5vq3', '/home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/_inductor/utils.py')\nDEBUG 03-24 12:09:42 [backends.py:86] Directly load the 7-th graph for shape None from inductor via handle ('fsgvh3fgazwmztcgpeggl43axs5agy5wwcjhp7vqopraerya5vq3', '/home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/_inductor/utils.py')\nDEBUG 03-24 12:09:42 [backends.py:86] Directly load the 8-th graph for shape None from inductor via handle ('fsgvh3fgazwmztcgpeggl43axs5agy5wwcjhp7vqopraerya5vq3', '/home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/_inductor/utils.py')\nDEBUG 03-24 12:09:42 [backends.py:86] Directly load the 9-th graph for shape None from inductor via handle ('fsgvh3fgazwmztcgpeggl43axs5agy5wwcjhp7vqopraerya5vq3', '/home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/_inductor/utils.py')\nDEBUG 03-24 12:09:42 [backends.py:86] Directly load the 10-th graph for shape None from inductor via handle ('fsgvh3fgazwmztcgpeggl43axs5agy5wwcjhp7vqopraerya5vq3', '/home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/_inductor/utils.py')\nDEBUG 03-24 12:09:43 [backends.py:86] Directly load the 11-th graph for shape None from inductor via handle ('fsgvh3fgazwmztcgpeggl43axs5agy5wwcjhp7vqopraerya5vq3', '/home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/_inductor/utils.py')\nDEBUG 03-24 12:09:43 [backends.py:86] Directly load the 12-th graph for shape None from inductor via handle ('fonaeayky6oxas7npm24nhqujabfsdcx76urzfpn2qpzhpcv7ogn', '/home/search/minos/vllm-benchmarks/vllm/lib/python3.12/site-packages/torch/_inductor/utils.py')\nINFO 03-24 12:09:43 [monitor.py:33] torch.compile takes 1.61 s in total\nINFO 03-24 12:09:44 [kv_cache_utils.py:537] GPU KV cache size: 3,581,232 tokens\nINFO 03-24 12:09:44 [kv_cache_utils.py:540] Maximum concurrency for 2,048 tokens per request: 1748.65x\nDEBUG 03-24 12:09:44 [backends.py:637] Warming up 1/1 for shape 512\nDEBUG 03-24 12:09:44 [backends.py:648] Capturing a cudagraph for shape 512\nDEBUG 03-24 12:09:44 [backends.py:637] Warming up 1/1 for shape 504\nDEBUG 03-24 12:09:44 [backends.py:648] Capturing a cudagraph for shape 504\nDEBUG 03-24 12:09:44 [backends.py:637] Warming up 1/1 for shape 496\nDEBUG 03-24 12:09:44 [backends.py:648] Capturing a cudagraph for shape 496\nDEBUG 03-24 12:09:44 [backends.py:637] Warming up 1/1 for shape 488\nDEBUG 03-24 12:09:44 [backends.py:648] Capturing a cudagraph for shape 488\nDEBUG 03-24 12:09:44 [backends.py:637] Warming up 1/1 for shape 480\nDEBUG 03-24 12:09:44 [backends.py:648] Capturing a cudagraph for shape 480\nDEBUG 03-24 12:09:44 [backends.py:637] Warming up 1/1 for shape 472\nDEBUG 03-24 12:09:44 [backends.py:648] Capturing a cudagraph for shape 472\nDEBUG 03-24 12:09:44 [backends.py:637] Warming up 1/1 for shape 464\nDEBUG 03-24 12:09:44 [backends.py:648] Capturing a cudagraph for shape 464\nDEBUG 03-24 12:09:45 [backends.py:637] Warming up 1/1 for shape 456\nDEBUG 03-24 12:09:45 [backends.py:648] Capturing a cudagraph for shape 456\nDEBUG 03-24 12:09:45 [backends.py:637] Warming up 1/1 for shape 448\nDEBUG 03-24 12:09:45 [backends.py:648] Capturing a cudagraph for shape 448\nDEBUG 03-24 12:09:45 [backends.py:637] Warming up 1/1 for shape 440\nDEBUG 03-24 12:09:45 [backends.py:648] Capturing a cudagraph for shape 440\nDEBUG 03-24 12:09:45 [backends.py:637] Warming up 1/1 for shape 432\nDEBUG 03-24 12:09:45 [backends.py:648] Capturing a cudagraph for shape 432\nDEBUG 03-24 12:09:45 [backends.py:637] Warming up 1/1 for shape 424\nDEBUG 03-24 12:09:45 [backends.py:648] Capturing a cudagraph for shape 424\nDEBUG 03-24 12:09:45 [backends.py:637] Warming up 1/1 for shape 416\nDEBUG 03-24 12:09:45 [backends.py:648] Capturing a cudagraph for shape 416\nDEBUG 03-24 12:09:45 [backends.py:637] Warming up 1/1 for shape 408\nDEBUG 03-24 12:09:45 [backends.py:648] Capturing a cudagraph for shape 408\nDEBUG 03-24 12:09:45 [backends.py:637] Warming up 1/1 for shape 400\nDEBUG 03-24 12:09:45 [backends.py:648] Capturing a cudagraph for shape 400\nDEBUG 03-24 12:09:46 [backends.py:637] Warming up 1/1 for shape 392\nDEBUG 03-24 12:09:46 [backends.py:648] Capturing a cudagraph for shape 392\nDEBUG 03-24 12:09:46 [backends.py:637] Warming up 1/1 for shape 384\nDEBUG 03-24 12:09:46 [backends.py:648] Capturing a cudagraph for shape 384\nDEBUG 03-24 12:09:46 [backends.py:637] Warming up 1/1 for shape 376\nDEBUG 03-24 12:09:46 [backends.py:648] Capturing a cudagraph for shape 376\nDEBUG 03-24 12:09:46 [backends.py:637] Warming up 1/1 for shape 368\nDEBUG 03-24 12:09:46 [backends.py:648] Capturing a cudagraph for shape 368\nDEBUG 03-24 12:09:46 [backends.py:637] Warming up 1/1 for shape 360\nDEBUG 03-24 12:09:46 [backends.py:648] Capturing a cudagraph for shape 360\nDEBUG 03-24 12:09:46 [backends.py:637] Warming up 1/1 for shape 352\nDEBUG 03-24 12:09:46 [backends.py:648] Capturing a cudagraph for shape 352\nDEBUG 03-24 12:09:46 [backends.py:637] Warming up 1/1 for shape 344\nDEBUG 03-24 12:09:46 [backends.py:648] Capturing a cudagraph for shape 344\nDEBUG 03-24 12:09:47 [backends.py:637] Warming up 1/1 for shape 336\nDEBUG 03-24 12:09:47 [backends.py:648] Capturing a cudagraph for shape 336\nDEBUG 03-24 12:09:47 [backends.py:637] Warming up 1/1 for shape 328\nDEBUG 03-24 12:09:47 [backends.py:648] Capturing a cudagraph for shape 328\nDEBUG 03-24 12:09:47 [backends.py:637] Warming up 1/1 for shape 320\nDEBUG 03-24 12:09:47 [backends.py:648] Capturing a cudagraph for shape 320\nDEBUG 03-24 12:09:47 [backends.py:637] Warming up 1/1 for shape 312\nDEBUG 03-24 12:09:47 [backends.py:648] Capturing a cudagraph for shape 312\nDEBUG 03-24 12:09:47 [backends.py:637] Warming up 1/1 for shape 304\nDEBUG 03-24 12:09:47 [backends.py:648] Capturing a cudagraph for shape 304\nDEBUG 03-24 12:09:47 [backends.py:637] Warming up 1/1 for shape 296\nDEBUG 03-24 12:09:47 [backends.py:648] Capturing a cudagraph for shape 296\nDEBUG 03-24 12:09:47 [backends.py:637] Warming up 1/1 for shape 288\nDEBUG 03-24 12:09:47 [backends.py:648] Capturing a cudagraph for shape 288\nDEBUG 03-24 12:09:47 [backends.py:637] Warming up 1/1 for shape 280\nDEBUG 03-24 12:09:47 [backends.py:648] Capturing a cudagraph for shape 280\nDEBUG 03-24 12:09:48 [backends.py:637] Warming up 1/1 for shape 272\nDEBUG 03-24 12:09:48 [backends.py:648] Capturing a cudagraph for shape 272\nDEBUG 03-24 12:09:48 [backends.py:637] Warming up 1/1 for shape 264\nDEBUG 03-24 12:09:48 [backends.py:648] Capturing a cudagraph for shape 264\nDEBUG 03-24 12:09:48 [backends.py:637] Warming up 1/1 for shape 256\nDEBUG 03-24 12:09:48 [backends.py:648] Capturing a cudagraph for shape 256\nDEBUG 03-24 12:09:48 [backends.py:637] Warming up 1/1 for shape 248\nDEBUG 03-24 12:09:48 [backends.py:648] Capturing a cudagraph for shape 248\nDEBUG 03-24 12:09:48 [backends.py:637] Warming up 1/1 for shape 240\nDEBUG 03-24 12:09:48 [backends.py:648] Capturing a cudagraph for shape 240\nDEBUG 03-24 12:09:48 [backends.py:637] Warming up 1/1 for shape 232\nDEBUG 03-24 12:09:48 [backends.py:648] Capturing a cudagraph for shape 232\nDEBUG 03-24 12:09:48 [backends.py:637] Warming up 1/1 for shape 224\nDEBUG 03-24 12:09:48 [backends.py:648] Capturing a cudagraph for shape 224\nDEBUG 03-24 12:09:49 [backends.py:637] Warming up 1/1 for shape 216\nDEBUG 03-24 12:09:49 [backends.py:648] Capturing a cudagraph for shape 216\nDEBUG 03-24 12:09:49 [backends.py:637] Warming up 1/1 for shape 208\nDEBUG 03-24 12:09:49 [backends.py:648] Capturing a cudagraph for shape 208\nDEBUG 03-24 12:09:49 [backends.py:637] Warming up 1/1 for shape 200\nDEBUG 03-24 12:09:49 [backends.py:648] Capturing a cudagraph for shape 200\nDEBUG 03-24 12:09:49 [backends.py:637] Warming up 1/1 for shape 192\nDEBUG 03-24 12:09:49 [backends.py:648] Capturing a cudagraph for shape 192\nDEBUG 03-24 12:09:49 [backends.py:637] Warming up 1/1 for shape 184\nDEBUG 03-24 12:09:49 [backends.py:648] Capturing a cudagraph for shape 184\nDEBUG 03-24 12:09:49 [backends.py:637] Warming up 1/1 for shape 176\nDEBUG 03-24 12:09:49 [backends.py:648] Capturing a cudagraph for shape 176\nDEBUG 03-24 12:09:49 [backends.py:637] Warming up 1/1 for shape 168\nDEBUG 03-24 12:09:49 [backends.py:648] Capturing a cudagraph for shape 168\nDEBUG 03-24 12:09:49 [backends.py:637] Warming up 1/1 for shape 160\nDEBUG 03-24 12:09:49 [backends.py:648] Capturing a cudagraph for shape 160\nDEBUG 03-24 12:09:50 [backends.py:637] Warming up 1/1 for shape 152\nDEBUG 03-24 12:09:50 [backends.py:648] Capturing a cudagraph for shape 152\nDEBUG 03-24 12:09:50 [backends.py:637] Warming up 1/1 for shape 144\nDEBUG 03-24 12:09:50 [backends.py:648] Capturing a cudagraph for shape 144\nDEBUG 03-24 12:09:50 [backends.py:637] Warming up 1/1 for shape 136\nDEBUG 03-24 12:09:50 [backends.py:648] Capturing a cudagraph for shape 136\nDEBUG 03-24 12:09:50 [backends.py:637] Warming up 1/1 for shape 128\nDEBUG 03-24 12:09:50 [backends.py:648] Capturing a cudagraph for shape 128\nDEBUG 03-24 12:09:50 [backends.py:637] Warming up 1/1 for shape 120\nDEBUG 03-24 12:09:50 [backends.py:648] Capturing a cudagraph for shape 120\nDEBUG 03-24 12:09:50 [backends.py:637] Warming up 1/1 for shape 112\nDEBUG 03-24 12:09:50 [backends.py:648] Capturing a cudagraph for shape 112\nDEBUG 03-24 12:09:50 [backends.py:637] Warming up 1/1 for shape 104\nDEBUG 03-24 12:09:50 [backends.py:648] Capturing a cudagraph for shape 104\nDEBUG 03-24 12:09:51 [backends.py:637] Warming up 1/1 for shape 96\nDEBUG 03-24 12:09:51 [backends.py:648] Capturing a cudagraph for shape 96\nDEBUG 03-24 12:09:51 [backends.py:637] Warming up 1/1 for shape 88\nDEBUG 03-24 12:09:51 [backends.py:648] Capturing a cudagraph for shape 88\nDEBUG 03-24 12:09:51 [backends.py:637] Warming up 1/1 for shape 80\nDEBUG 03-24 12:09:51 [backends.py:648] Capturing a cudagraph for shape 80\nDEBUG 03-24 12:09:51 [backends.py:637] Warming up 1/1 for shape 72\nDEBUG 03-24 12:09:51 [backends.py:648] Capturing a cudagraph for shape 72\nDEBUG 03-24 12:09:51 [backends.py:637] Warming up 1/1 for shape 64\nDEBUG 03-24 12:09:51 [backends.py:648] Capturing a cudagraph for shape 64\nDEBUG 03-24 12:09:51 [backends.py:637] Warming up 1/1 for shape 56\nDEBUG 03-24 12:09:51 [backends.py:648] Capturing a cudagraph for shape 56\nDEBUG 03-24 12:09:51 [backends.py:637] Warming up 1/1 for shape 48\nDEBUG 03-24 12:09:51 [backends.py:648] Capturing a cudagraph for shape 48\nDEBUG 03-24 12:09:51 [backends.py:637] Warming up 1/1 for shape 40\nDEBUG 03-24 12:09:51 [backends.py:648] Capturing a cudagraph for shape 40\nDEBUG 03-24 12:09:52 [backends.py:637] Warming up 1/1 for shape 32\nDEBUG 03-24 12:09:52 [backends.py:648] Capturing a cudagraph for shape 32\nDEBUG 03-24 12:09:52 [backends.py:637] Warming up 1/1 for shape 24\nDEBUG 03-24 12:09:52 [backends.py:648] Capturing a cudagraph for shape 24\nDEBUG 03-24 12:09:52 [backends.py:637] Warming up 1/1 for shape 16\nDEBUG 03-24 12:09:52 [backends.py:648] Capturing a cudagraph for shape 16\nDEBUG 03-24 12:09:52 [backends.py:637] Warming up 1/1 for shape 8\nDEBUG 03-24 12:09:52 [backends.py:648] Capturing a cudagraph for shape 8\nDEBUG 03-24 12:09:52 [backends.py:637] Warming up 1/1 for shape 4\nDEBUG 03-24 12:09:52 [backends.py:648] Capturing a cudagraph for shape 4\nDEBUG 03-24 12:09:52 [backends.py:637] Warming up 1/1 for shape 2\nDEBUG 03-24 12:09:52 [backends.py:648] Capturing a cudagraph for shape 2\nDEBUG 03-24 12:09:52 [backends.py:637] Warming up 1/1 for shape 1\nDEBUG 03-24 12:09:52 [backends.py:648] Capturing a cudagraph for shape 1\nINFO 03-24 12:09:53 [gpu_model_runner.py:1499] Graph capturing finished in 9 secs, took 0.31 GiB\nINFO 03-24 12:09:53 [core.py:138] init engine (profile, create kv cache, warmup model) took 12.45 seconds\nDEBUG 03-24 12:09:53 [core.py:357] EngineCore busy loop waiting.\nProcessed prompts:   0%|                                                                                                                                                                                                                                              | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]DEBUG 03-24 12:09:53 [core.py:357] EngineCore busy loop waiting.\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 27.32it/s, est. speed input: 136.96 toks/s, output: 437.99 toks/s]\nTraceback (most recent call last):\n  File \"/home/search/minos/vllm-benchmarks/test.py\", line 14, in <module>\n    assert outputs[0].metrics is not None\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\nDEBUG 03-24 12:09:53 [core.py:336] EngineCore interrupted.\n```\n\nSame example but with `speculative_model=\"[ngram]\"` works:\n\n```python\nfrom vllm import LLM\n\nllm = LLM(\n    model=\"facebook/opt-125m\",\n    speculative_model=\"[ngram]\",\n    num_speculative_tokens=10,\n    ngram_prompt_lookup_max=4,\n)\n\noutputs = llm.generate(\"Hello, world!\")\n\nassert outputs[0].metrics is not None\nprint(outputs[0].metrics)\n```\n\nOutput:\n```\nDEBUG 03-24 12:12:51 [__init__.py:28] No plugins for group vllm.platform_plugins found.\nDEBUG 03-24 12:12:51 [__init__.py:35] Checking if TPU platform is available.\nDEBUG 03-24 12:12:51 [__init__.py:45] TPU platform is not available because: No module named 'libtpu'\nDEBUG 03-24 12:12:51 [__init__.py:53] Checking if CUDA platform is available.\nDEBUG 03-24 12:12:51 [__init__.py:73] Confirmed CUDA platform is available.\nDEBUG 03-24 12:12:51 [__init__.py:101] Checking if ROCm platform is available.\nDEBUG 03-24 12:12:51 [__init__.py:115] ROCm platform is not available because: No module named 'amdsmi'\nDEBUG 03-24 12:12:51 [__init__.py:123] Checking if HPU platform is available.\nDEBUG 03-24 12:12:51 [__init__.py:130] HPU platform is not available because habana_frameworks is not found.\nDEBUG 03-24 12:12:51 [__init__.py:141] Checking if XPU platform is available.\nDEBUG 03-24 12:12:51 [__init__.py:151] XPU platform is not available because: No module named 'intel_extension_for_pytorch'\nDEBUG 03-24 12:12:51 [__init__.py:159] Checking if CPU platform is available.\nDEBUG 03-24 12:12:51 [__init__.py:181] Checking if Neuron platform is available.\nDEBUG 03-24 12:12:51 [__init__.py:188] Neuron platform is not available because: No module named 'transformers_neuronx'\nDEBUG 03-24 12:12:51 [__init__.py:196] Checking if OpenVINO platform is available.\nDEBUG 03-24 12:12:51 [__init__.py:203] OpenVINO platform is not available because vLLM is not built with OpenVINO.\nDEBUG 03-24 12:12:51 [__init__.py:53] Checking if CUDA platform is available.\nDEBUG 03-24 12:12:51 [__init__.py:73] Confirmed CUDA platform is available.\nINFO 03-24 12:12:51 [__init__.py:256] Automatically detected platform cuda.\nDEBUG 03-24 12:12:52 [__init__.py:28] No plugins for group vllm.general_plugins found.\nINFO 03-24 12:12:58 [config.py:583] This model supports multiple tasks: {'reward', 'classify', 'score', 'generate', 'embed'}. Defaulting to 'generate'.\nINFO 03-24 12:12:58 [arg_utils.py:1776] ngram is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.\nINFO 03-24 12:12:58 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.1) with config: model='facebook/opt-125m', speculative_config=SpeculativeConfig(draft_model='[ngram]', num_spec_tokens=10), tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=facebook/opt-125m, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \nINFO 03-24 12:12:59 [cuda.py:285] Using Flash Attention backend.\nWARNING 03-24 12:12:59 [utils.py:2282] Methods determine_num_available_blocks,device_config not implemented in <vllm.spec_decode.ngram_worker.NGramWorker object at 0x7f23b887eab0>\nINFO 03-24 12:12:59 [spec_decode_worker.py:211] Configuring SpecDecodeWorker with proposer=<class 'vllm.spec_decode.ngram_worker.NGramWorker'>\nINFO 03-24 12:12:59 [rejection_sampler.py:60] Use pytorch for rejection sampling.\nINFO 03-24 12:12:59 [spec_decode_worker.py:223] [Speculative Decoding] Configuring SpecDecodeWorker with sampler=<class 'vllm.model_executor.layers.rejection_sampler.RejectionSampler'>\nINFO 03-24 12:12:59 [spec_decode_worker.py:246] [Speculative Decoding] Disabling MQA scorer as the target model is not running in eager mode.\nDEBUG 03-24 12:13:00 [config.py:3658] enabled custom ops: Counter()\nDEBUG 03-24 12:13:00 [config.py:3660] disabled custom ops: Counter()\nDEBUG 03-24 12:13:00 [parallel_state.py:817] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.0:47417 backend=nccl\nINFO 03-24 12:13:00 [parallel_state.py:967] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 03-24 12:13:00 [model_runner.py:1110] Starting to load model facebook/opt-125m...\nDEBUG 03-24 12:13:00 [decorators.py:109] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.opt.OPTModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']\nDEBUG 03-24 12:13:00 [config.py:3658] enabled custom ops: Counter()\nDEBUG 03-24 12:13:00 [config.py:3660] disabled custom ops: Counter()\nINFO 03-24 12:13:00 [weight_utils.py:257] Using model weights format ['*.bin']\nLoading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.65it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.64it/s]\n\nINFO 03-24 12:13:00 [loader.py:429] Loading weights took 0.18 seconds\nINFO 03-24 12:13:00 [model_runner.py:1146] Model loading took 0.2389 GB and 0.518447 seconds\nINFO 03-24 12:13:00 [spec_decode_worker.py:380] [Speculative Decoding] Use batch expansion for scoring proposals.\nDEBUG 03-24 12:13:00 [config.py:3658] enabled custom ops: Counter()\nDEBUG 03-24 12:13:00 [config.py:3660] disabled custom ops: Counter()\nINFO 03-24 12:13:01 [worker.py:267] Memory profiling takes 0.59 seconds\nINFO 03-24 12:13:01 [worker.py:267] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.90) = 125.75GiB\nINFO 03-24 12:13:01 [worker.py:267] model weights take 0.24GiB; non_torch_memory takes 0.22GiB; PyTorch activation peak memory takes 0.49GiB; the rest of the memory reserved for KV Cache is 124.79GiB.\nINFO 03-24 12:13:01 [executor_base.py:111] # cuda blocks: 227182, # CPU blocks: 7281\nINFO 03-24 12:13:01 [executor_base.py:116] Maximum concurrency for 2048 tokens per request: 1774.86x\nINFO 03-24 12:13:03 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nCapturing CUDA graph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:07<00:00,  4.55it/s]\nINFO 03-24 12:13:11 [model_runner.py:1570] Graph capturing finished in 8 secs, took 0.13 GiB\nINFO 03-24 12:13:11 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 10.30 seconds\nProcessed prompts:   0%|                                                                                                                                                                                                                                              | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]DEBUG 03-24 12:13:14 [llm_engine.py:1520] Stopping remote worker execution loop.\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.04s/it, est. speed input: 4.81 toks/s, output: 14.42 toks/s]\nRequestMetrics(arrival_time=1742818393.482077, last_token_time=1742818394.5239623, first_scheduled_time=1742818393.4845228, first_token_time=1742818393.490636, time_in_queue=0.0024459362030029297, finished_time=1742818394.524077, scheduler_time=0.0013894308358430862, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n[rank0]:[W324 12:13:15.000622332 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n```\n\nTested in both `VLLM_USE_V1`  0 and 1, and using different models.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "good first issue",
      "feature request",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-24T12:15:17+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15394/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15394"
  },
  {
    "number": 14764,
    "title": "[RFC]: Schema for checking input shapes for multi-modal models",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently, we use `_parse_and_validate_*_input` to validate the multi-modal inputs. However, only minimal checks are being made, with some models only checking the type of the inputs. It is easy for the actual shape of the inputs to not match what is being documented in classes like `*ImagePixelInputs`, confusing model developers and maintainers.\n\nTo avoid this, I propose adding a base class `TensorSchema` to validate the model inputs. For example:\n\nOriginal code:\n```py\nclass Phi3VImagePixelInputs(TypedDict):\n    type: Literal[\"pixel_values\"]\n    data: Union[torch.Tensor, List[torch.Tensor]]\n    \"\"\"Shape: `(batch_size * num_images, 1 + num_patches, num_channels, height, width)`\"\"\"\n\n    image_sizes: torch.Tensor\n    \"\"\"Shape: `(batch_size * num_images, 2)`\"\"\"\n```\n\nThe idea:\n```py\nclass Phi3VImagePixelInputs(TensorSchema):\n    \"\"\"\n    Dimensions:\n        - b: Batch size (number of prompts)\n        - n: Number of images\n        - p: Number of patches\n        - h: Height of each patch\n        - w: Width of each patch\n    \"\"\"\n    type: Literal[\"pixel_values\"] = \"pixel_values\"\n    data: Annotated[Union[torch.Tensor, List[torch.Tensor]], TensorShape(\"bn\", \"p\", 3, \"h\", \"w\")]\n    image_sizes: Annotated[Union[torch.Tensor, List[torch.Tensor]], TensorShape(\"bn\", 2)]\n```\n\n- Validation is done automatically, similar to Pydantic models\n  - To avoid performance issues, we should be able to disable validation using a flag\n- We tag each tensor field with `typing_extensions.Annotated` and use the additional metadata to perform validation.\n  - Can switch to `typing.Annotated` once we drop support for Python 3.9\n- Dimensions that are constants can be checked directly\n  - Example: We validate that `data.shape[2] == 3`\n- Dimensions with the same name should be consistent between fields, e.g.\n  - Example: Since `data.shape[0]` and `image_sizes.shape[0]` share the name `bn`, we validate that `data.shape[0] == image_sizes[0]`\n- If a field is a list/tuple instead of a tensor, we use `len` instead of `shape` to check the leading dimension, then recurse into each element of the list to check the remaining dimensions.\n\n### Notes\n\nThis idea can benefit projects outside of vLLM as well, so we should consider developing this as a separate package.\n\n### CC List\n\n@ywang96 @Isotr0py @mgoin \n@hmellor in case this is already a project on HF\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "feature request",
      "RFC",
      "multi-modality"
    ],
    "state": "open",
    "created_at": "2025-03-13T14:57:56+00:00",
    "closed_at": null,
    "comments": 29,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14764/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/14764"
  },
  {
    "number": 12179,
    "title": "[Bug]: Multi-Node Online Inference on TPUs Failing",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nroot@t1v-n-4d36f9a1-w-0:/workspace/vllm# python collect_env.py\nINFO 01-17 23:21:42 __init__.py:179] Automatically detected platform tpu.\nCollecting environment information...\nPyTorch version: 2.6.0.dev20241126+cpu\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\nClang version: Could not collect\nCMake version: version 3.31.4\nLibc version: glibc-2.31\n\nPython version: 3.10.15 (main, Oct 17 2024, 02:58:23) [GCC 10.2.1 20210110] (64-bit runtime)\nPython platform: Linux-5.19.0-1022-gcp-x86_64-with-glibc2.31\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nByte Order:                      Little Endian\nAddress sizes:                   48 bits physical, 48 bits virtual\nCPU(s):                          240\nOn-line CPU(s) list:             0-239\nThread(s) per core:              2\nCore(s) per socket:              60\nSocket(s):                       2\nNUMA node(s):                    2\nVendor ID:                       AuthenticAMD\nCPU family:                      23\nModel:                           49\nModel name:                      AMD EPYC 7B12\nStepping:                        0\nCPU MHz:                         2249.998\nBogoMIPS:                        4499.99\nHypervisor vendor:               KVM\nVirtualization type:             full\nL1d cache:                       3.8 MiB\nL1i cache:                       3.8 MiB\nL2 cache:                        60 MiB\nL3 cache:                        480 MiB\nNUMA node0 CPU(s):               0-59,120-179\nNUMA node1 CPU(s):               60-119,180-239\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Not affected\nVulnerability Retbleed:          Mitigation; untrained return thunk; SMT enabled with STIBP protection\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.6.0.dev20241126+cpu\n[pip3] torch-xla==2.6.0+git39e67b5\n[pip3] torchvision==0.20.0.dev20241126+cpu\n[pip3] transformers==4.48.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.6.6.post2.dev250+gd1adb9b40\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nVLLM_TARGET_DEVICE=tpu\nLD_LIBRARY_PATH=/usr/local/lib/python3.10/site-packages/cv2/../../lib64:\n```\n\n</details>\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nI'm trying to run multi-node inference on TPUs, so that I'd be able to fit large models like Llama-3.1-70B-Instruct which will require more than one pod of v4-8.\n\nHere are some of my settings:\n- TPU Type: v4-32\n- TPU Software Version: tpu-ubuntu2204-base\n- Docker Version: vllm/vllm-tpu:nightly\n\nI followed the examples shown in distributed inference and serving in the [docs](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). I modified it to fit the TPU case. The gist was to add `--privileged` flag and also add `TPU` as a resource to the RAY START COMMANDS. Check out my repo for exact information: [link](https://github.com/BabyChouSr/vllm/blob/befc0727ac1e4704e1a2c7f41c180205e046b873/examples/online_serving/run_cluster.sh)\n\nI first tested out running with just two of the four hosts in a v4-32 setting. Here are the commands I used.\n\n```\n# ssh into head and worker and setup containers\ngcloud compute tpus tpu-vm ssh --zone \"us-central2-b\" \"chris-vllm-labelling\" --project \"hai-gcp-models\" --worker 0\ngit clone https://github.com/BabyChouSr/vllm.git\ncd vllm \ngit checkout chris/vllm-tpu-multi\n\nsudo bash examples/online_serving/run_cluster.sh \\\n                  vllm/vllm-tpu:nightly \\\n                  10.130.0.9 \\\n                  --head \\\n                  /home/chrischou/.cache/huggingface\n\n# Repeat above for worker node\n```\n\nThen I ran `ray status` inside the container of the head node. I get the following as expected. I see 8 TPUs registered since I have a head and a worker.\n```\nroot@t1v-n-4d36f9a1-w-0:/workspace/vllm# ray status\n======== Autoscaler status: 2025-01-17 23:33:55.419278 ========\nNode status\n---------------------------------------------------------------\nActive:\n 1 node_6ef276d8e9c645cf2b3ef633fa3e40daa64c635cc5f4accb7fa05659\n 1 node_3e3224930b5214adc9afebf72acea691d97f608dbe26e222875d9af2\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/240.0 CPU\n 0.0/8.0 TPU\n 0.0/1.0 TPU-v4-32-head\n 0.0/2.0 chris-vllm-labelling\n 0B/760.02GiB memory\n 0B/30.40GiB object_store_memory\n\nDemands:\n (no resource demands)\n```\n\nThen I try to serve a small model just to see if tensor parallelism works. I run `vllm serve Qwen/Qwen2.5-7B-Instruct --tensor-parallel-size 4`. I get the following error:\n```\nroot@t1v-n-4d36f9a1-w-0:/workspace/vllm# vllm serve Qwen/Qwen2.5-7B-Instruct --tensor-parallel-size 4\nINFO 01-17 23:34:35 __init__.py:179] Automatically detected platform tpu.\nINFO 01-17 23:34:36 api_server.py:768] vLLM API server version 0.6.6.post2.dev250+gd1adb9b40\nINFO 01-17 23:34:36 api_server.py:769] args: Namespace(subparser='serve', model_tag='Qwen/Qwen2.5-7B-Instruct', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2.5-7B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7fb2e47c4af0>)\nINFO 01-17 23:34:36 api_server.py:195] Started engine process with PID 2002\nINFO 01-17 23:34:40 __init__.py:179] Automatically detected platform tpu.\nINFO 01-17 23:34:44 config.py:520] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed', 'score'}. Defaulting to 'generate'.\nINFO 01-17 23:34:48 config.py:520] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify', 'score'}. Defaulting to 'generate'.\nINFO 01-17 23:34:48 llm_engine.py:232] Initializing an LLM engine (v0.6.6.post2.dev250+gd1adb9b40) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":2,\"backend\":\"openxla\",\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \n2025-01-17 23:34:48,948\tINFO worker.py:1636 -- Connecting to existing Ray cluster at address: 10.130.0.9:6379...\n2025-01-17 23:34:48,961\tINFO worker.py:1812 -- Connected to Ray cluster. View the dashboard at 127.0.0.1:8265 \nINFO 01-17 23:34:50 ray_distributed_executor.py:152] use_ray_spmd_worker: False\n(pid=2257) INFO 01-17 23:34:53 __init__.py:179] Automatically detected platform tpu.\n(RayWorkerWrapper pid=2253) WARNING 01-17 23:34:54 utils.py:546] Overwriting environment variable TPU_VISIBLE_CHIPS from '3' to '0,1,2,3'\nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n(RayWorkerWrapper pid=2251) WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\nINFO 01-17 23:34:55 tpu.py:34] Cannot use None backend on TPU.\nINFO 01-17 23:34:55 tpu.py:35] Using Pallas backend.\nWARNING 01-17 23:34:55 tpu_model_runner.py:127] The max_model_len (32768) is too large. This may degrade the performance due to the insufficient smem size. Consider setting --max-model-len to a smaller value, like 8192.\n(RayWorkerWrapper pid=2253) INFO 01-17 23:34:55 tpu.py:34] Cannot use None backend on TPU.\n(RayWorkerWrapper pid=2253) INFO 01-17 23:34:55 tpu.py:35] Using Pallas backend.\n(RayWorkerWrapper pid=2253) WARNING 01-17 23:34:55 tpu_model_runner.py:127] The max_model_len (32768) is too large. This may degrade the performance due to the insufficient smem size. Consider setting --max-model-len to a smaller value, like 8192.\nERROR 01-17 23:34:56 worker_base.py:554] Error executing method init_device. This might cause deadlock in distributed execution.\nERROR 01-17 23:34:56 worker_base.py:554] Traceback (most recent call last):\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/worker/worker_base.py\", line 546, in execute_method\nERROR 01-17 23:34:56 worker_base.py:554]     return executor(*args, **kwargs)\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\nERROR 01-17 23:34:56 worker_base.py:554]     ensure_model_parallel_initialized(\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 1093, in ensure_model_parallel_initialized\nERROR 01-17 23:34:56 worker_base.py:554]     initialize_model_parallel(tensor_model_parallel_size,\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 1037, in initialize_model_parallel\nERROR 01-17 23:34:56 worker_base.py:554]     _TP = init_model_parallel_group(group_ranks,\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 866, in init_model_parallel_group\nERROR 01-17 23:34:56 worker_base.py:554]     return GroupCoordinator(\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\nERROR 01-17 23:34:56 worker_base.py:554]     self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 53, in __init__\nERROR 01-17 23:34:56 worker_base.py:554]     pjrt.initialize_multiprocess(local_rank, local_world_size)\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 121, in initialize_multiprocess\nERROR 01-17 23:34:56 worker_base.py:554]     devices = xm.get_xla_supported_devices()\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/core/xla_model.py\", line 93, in get_xla_supported_devices\nERROR 01-17 23:34:56 worker_base.py:554]     devices = torch_xla._XLAC._xla_get_devices()\nERROR 01-17 23:34:56 worker_base.py:554] RuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: Invalid --2a886c8_slice_builder_worker_addresses specified. Expected 16 worker addresses, got 8.\nERROR 01-17 23:34:56 engine.py:380] Bad StatusOr access: UNKNOWN: TPU initialization failed: Invalid --2a886c8_slice_builder_worker_addresses specified. Expected 16 worker addresses, got 8.\nERROR 01-17 23:34:56 engine.py:380] Traceback (most recent call last):\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 371, in run_mp_engine\nERROR 01-17 23:34:56 engine.py:380]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 120, in from_engine_args\nERROR 01-17 23:34:56 engine.py:380]     return cls(ipc_path=ipc_path,\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 72, in __init__\nERROR 01-17 23:34:56 engine.py:380]     self.engine = LLMEngine(*args, **kwargs)\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/engine/llm_engine.py\", line 271, in __init__\nERROR 01-17 23:34:56 engine.py:380]     self.model_executor = executor_class(vllm_config=vllm_config, )\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/executor/executor_base.py\", line 222, in __init__\nERROR 01-17 23:34:56 engine.py:380]     super().__init__(*args, **kwargs)\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/executor/executor_base.py\", line 42, in __init__\nERROR 01-17 23:34:56 engine.py:380]     self._init_executor()\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/executor/ray_distributed_executor.py\", line 87, in _init_executor\nERROR 01-17 23:34:56 engine.py:380]     self._init_workers_ray(placement_group)\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/executor/ray_distributed_executor.py\", line 342, in _init_workers_ray\nERROR 01-17 23:34:56 engine.py:380]     self._run_workers(\"init_device\")\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/executor/ray_distributed_executor.py\", line 458, in _run_workers\nERROR 01-17 23:34:56 engine.py:380]     self.driver_worker.execute_method(method, *args, **kwargs)\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/worker/worker_base.py\", line 555, in execute_method\nERROR 01-17 23:34:56 engine.py:380]     raise e\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/worker/worker_base.py\", line 546, in execute_method\nERROR 01-17 23:34:56 engine.py:380]     return executor(*args, **kwargs)\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\nERROR 01-17 23:34:56 engine.py:380]     ensure_model_parallel_initialized(\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 1093, in ensure_model_parallel_initialized\nERROR 01-17 23:34:56 engine.py:380]     initialize_model_parallel(tensor_model_parallel_size,\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 1037, in initialize_model_parallel\nERROR 01-17 23:34:56 engine.py:380]     _TP = init_model_parallel_group(group_ranks,\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 866, in init_model_parallel_group\nERROR 01-17 23:34:56 engine.py:380]     return GroupCoordinator(\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\nERROR 01-17 23:34:56 engine.py:380]     self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 53, in __init__\nERROR 01-17 23:34:56 engine.py:380]     pjrt.initialize_multiprocess(local_rank, local_world_size)\nERROR 01-17 23:34:56 engine.py:380]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 121, in initialize_multiprocess\nERROR 01-17 23:34:56 engine.py:380]     devices = xm.get_xla_supported_devices()\nERROR 01-17 23:34:56 engine.py:380]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/core/xla_model.py\", line 93, in get_xla_supported_devices\nERROR 01-17 23:34:56 engine.py:380]     devices = torch_xla._XLAC._xla_get_devices()\nERROR 01-17 23:34:56 engine.py:380] RuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: Invalid --2a886c8_slice_builder_worker_addresses specified. Expected 16 worker addresses, got 8.\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/local/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 382, in run_mp_engine\n    raise e\n  File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 371, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n  File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 120, in from_engine_args\n    return cls(ipc_path=ipc_path,\n  File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 72, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n  File \"/workspace/vllm/vllm/engine/llm_engine.py\", line 271, in __init__\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n  File \"/workspace/vllm/vllm/executor/executor_base.py\", line 222, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/workspace/vllm/vllm/executor/executor_base.py\", line 42, in __init__\n    self._init_executor()\n  File \"/workspace/vllm/vllm/executor/ray_distributed_executor.py\", line 87, in _init_executor\n    self._init_workers_ray(placement_group)\n  File \"/workspace/vllm/vllm/executor/ray_distributed_executor.py\", line 342, in _init_workers_ray\n    self._run_workers(\"init_device\")\n  File \"/workspace/vllm/vllm/executor/ray_distributed_executor.py\", line 458, in _run_workers\n    self.driver_worker.execute_method(method, *args, **kwargs)\n  File \"/workspace/vllm/vllm/worker/worker_base.py\", line 555, in execute_method\n    raise e\n  File \"/workspace/vllm/vllm/worker/worker_base.py\", line 546, in execute_method\n    return executor(*args, **kwargs)\n  File \"/workspace/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\n    ensure_model_parallel_initialized(\n  File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 1093, in ensure_model_parallel_initialized\n    initialize_model_parallel(tensor_model_parallel_size,\n  File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 1037, in initialize_model_parallel\n    _TP = init_model_parallel_group(group_ranks,\n  File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 866, in init_model_parallel_group\n    return GroupCoordinator(\n  File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\n    self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\n  File \"/workspace/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 53, in __init__\n    pjrt.initialize_multiprocess(local_rank, local_world_size)\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 121, in initialize_multiprocess\n    devices = xm.get_xla_supported_devices()\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/core/xla_model.py\", line 93, in get_xla_supported_devices\n    devices = torch_xla._XLAC._xla_get_devices()\nRuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: Invalid --2a886c8_slice_builder_worker_addresses specified. Expected 16 worker addresses, got 8.\n(pid=2251) INFO 01-17 23:34:53 __init__.py:179] Automatically detected platform tpu. [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\n(RayWorkerWrapper pid=2251) WARNING 01-17 23:34:54 utils.py:546] Overwriting environment variable TPU_VISIBLE_CHIPS from '2' to '0,1,2,3' [repeated 2x across cluster]\n(RayWorkerWrapper pid=2251) INFO 01-17 23:34:55 tpu.py:34] Cannot use None backend on TPU. [repeated 2x across cluster]\n(RayWorkerWrapper pid=2251) INFO 01-17 23:34:55 tpu.py:35] Using Pallas backend. [repeated 2x across cluster]\n(RayWorkerWrapper pid=2251) WARNING 01-17 23:34:55 tpu_model_runner.py:127] The max_model_len (32768) is too large. This may degrade the performance due to the insufficient smem size. Consider setting --max-model-len to a smaller value, like 8192. [repeated 2x across cluster]\n(RayWorkerWrapper pid=2248) WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU. [repeated 2x across cluster]\nTask exception was never retrieved\nfuture: <Task finished name='Task-2' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /workspace/vllm/vllm/engine/multiprocessing/client.py:180> exception=ZMQError('Operation not supported')>\nTraceback (most recent call last):\n  File \"/workspace/vllm/vllm/engine/multiprocessing/client.py\", line 186, in run_output_handler_loop\n    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n  File \"/usr/local/lib/python3.10/site-packages/zmq/_future.py\", line 400, in poll\n    raise _zmq.ZMQError(_zmq.ENOTSUP)\nzmq.error.ZMQError: Operation not supported\nTraceback (most recent call last):\n  File \"/usr/local/bin/vllm\", line 33, in <module>\n    sys.exit(load_entry_point('vllm', 'console_scripts', 'vllm')())\n  File \"/workspace/vllm/vllm/scripts.py\", line 201, in main\n    args.dispatch_function(args)\n  File \"/workspace/vllm/vllm/scripts.py\", line 42, in serve\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.10/site-packages/uvloop/__init__.py\", line 82, in run\n    return loop.run_until_complete(wrapper())\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.10/site-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 796, in run_server\n    async with build_async_engine_client(args) as engine_client:\n  File \"/usr/local/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 125, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/usr/local/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 219, in build_async_engine_client_from_engine_args\n    raise RuntimeError(\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\n```\n\nIt seems like there is some issues with the spawning of the multiple processes needed to run distributed inference. I also tested the case without tensor parallelism since Qwen7B fits on a single node. It ends up hanging:\n```\nroot@t1v-n-4d36f9a1-w-0:/workspace/vllm# vllm serve Qwen/Qwen2.5-7B-Instruct\nINFO 01-17 23:36:53 __init__.py:179] Automatically detected platform tpu.\nINFO 01-17 23:36:55 api_server.py:768] vLLM API server version 0.6.6.post2.dev250+gd1adb9b40\nINFO 01-17 23:36:55 api_server.py:769] args: Namespace(subparser='serve', model_tag='Qwen/Qwen2.5-7B-Instruct', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2.5-7B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7f6874240af0>)\nINFO 01-17 23:36:55 api_server.py:195] Started engine process with PID 18709\nINFO 01-17 23:36:58 __init__.py:179] Automatically detected platform tpu.\nINFO 01-17 23:37:02 config.py:520] This model supports multiple tasks: {'score', 'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\nINFO 01-17 23:37:06 config.py:520] This model supports multiple tasks: {'generate', 'reward', 'classify', 'embed', 'score'}. Defaulting to 'generate'.\nINFO 01-17 23:37:06 llm_engine.py:232] Initializing an LLM engine (v0.6.6.post2.dev250+gd1adb9b40) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":2,\"backend\":\"openxla\",\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\nINFO 01-17 23:37:08 tpu.py:34] Cannot use None backend on TPU.\nINFO 01-17 23:37:08 tpu.py:35] Using Pallas backend.\nWARNING 01-17 23:37:08 tpu_model_runner.py:127] The max_model_len (32768) is too large. This may degrade the performance due to the insufficient smem size. Consider setting --max-model-len to a smaller value, like 8192.\n```\n^ It stops right here and does not output anything anymore. Running ray status on a separate terminal window, it says that there are no demanded resources, so nothing is being used.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tpu",
      "ray",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-01-17T23:38:35+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12179/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12179"
  },
  {
    "number": 11260,
    "title": "[Bug]: vLLM on TPU does not support --pipeline-parallel-size with Ray",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.6.0.dev20241126+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.31.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-1013-gcp-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             112\r\nOn-line CPU(s) list:                0-111\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 7B13\r\nCPU family:                         25\r\nModel:                              1\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 56\r\nSocket(s):                          1\r\nStepping:                           0\r\nBogoMIPS:                           4899.99\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip vaes vpclmulqdq rdpid fsrm\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          1.8 MiB (56 instances)\r\nL1i cache:                          1.8 MiB (56 instances)\r\nL2 cache:                           28 MiB (56 instances)\r\nL3 cache:                           224 MiB (7 instances)\r\nNUMA node(s):                       1\r\nNUMA node0 CPU(s):                  0-111\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; safe RET\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.6.0.dev20241126+cpu\r\n[pip3] torch-xla==2.6.0+git39e67b5\r\n[pip3] torchvision==0.20.0.dev20241126+cpu\r\n[pip3] transformers==4.47.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.6.0.dev20241126+cpu          pypi_0    pypi\r\n[conda] torch-xla                 2.6.0+git39e67b5          pypi_0    pypi\r\n[conda] torchvision               0.20.0.dev20241126+cpu          pypi_0    pypi\r\n[conda] transformers              4.47.0                   pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.4.post2.dev378+g69ba344d\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n\r\nLD_LIBRARY_PATH=/home/ext_hzchen_google_com/miniconda3/envs/tpu/lib/python3.10/site-packages/cv2/../../lib64:\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nI setup v5e-32(8 hosts/4 chips each) and started a ray cluster.\r\n\r\noutput of **ray status**\r\n```\r\n======== Autoscaler status: 2024-12-17 11:18:22.317629 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 node_719fd8c930dcd8b932914ebb34d70d16323b468e1f93094a78d50f75\r\n 1 node_ac79abf69175ce6f927b5317fe292d22aea9ac4e3c224a7ba42ca6d3\r\n 1 node_23a15d4de28063c4a04865b963c54c0a8f29d8e928c298e4021a146b\r\n 1 node_153c21365890656c54742efe22e675b89127db7998084e482c8260c6\r\n 1 node_cedcf4265be52d297b3d95bab832675688d8241360c819bd9bd63de7\r\n 1 node_8993f15ab992801a04a70b5b7c4c691158b949bd7da98c35154aa372\r\n 1 node_3723c73cee4bc754265308f5a9f384b493e06b7d76860a739e9ddfb4\r\n 1 node_e2823d348a9370bfe108d635330b339b48e4847ce0608af311cd75e2\r\nPending:\r\n (no pending nodes)\r\nRecent failures:\r\n (no failures)\r\n\r\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 0.0/880.0 CPU\r\n 0.0/32.0 TPU\r\n 0.0/1.0 TPU-v5litepod-32-head\r\n 0B/1.01TiB memory\r\n 0B/449.11GiB object_store_memory\r\n 0.0/8.0 tpuvm-01\r\n\r\nDemands:\r\n (no resource demands)\r\n```\r\n\r\n\r\nI start serving with below command\r\n```\r\nvllm serve mistralai/Pixtral-Large-Instruct-2411 --config-format mistral --load-format mistral --tokenizer-mode mistral --num-scheduler-steps 2 --swap-space 4 --max-model-len=1024 --limit_mm_per_prompt 'image=10' --tensor-parallel-size 4 --pipeline-parallel-size 8 --disable-log-requests --dtype=bfloat16\r\n```\r\n\r\nget below error messages\r\n\r\n```text\r\nINFO 12-17 09:37:22 api_server.py:643] vLLM API server version 0.6.4.post2.dev378+g69ba344d\r\nINFO 12-17 09:37:22 api_server.py:644] args: Namespace(subparser='serve', model_tag='mistralai/Pixtral-Large-Instruct-2411', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='mistralai/Pixtral-Large-Instruct-2411', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='mistral', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='mistral', config_format='mistral', dtype='bfloat16', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=1024, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=8, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt={'image': 10}, mm_processor_kwargs=None, mm_cache_preprocessor=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=2, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7fa9d7e42320>)\r\nINFO 12-17 09:37:24 config.py:1938] Downcasting torch.float32 to torch.bfloat16.\r\nINFO 12-17 09:37:31 config.py:451] This model supports multiple tasks: {'embed', 'reward', 'score', 'classify', 'generate'}. Defaulting to 'generate'.\r\nWARNING 12-17 09:37:31 config.py:569] Async output processing can not be enabled with pipeline parallel\r\n2024-12-17 09:37:31,835\tINFO worker.py:1636 -- Connecting to existing Ray cluster at address: 10.164.0.34:6379...\r\n2024-12-17 09:37:31,898\tINFO worker.py:1812 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\r\nINFO 12-17 09:37:32 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post2.dev378+g69ba344d) with config: model='mistralai/Pixtral-Large-Instruct-2411', speculative_config=None, tokenizer='mistralai/Pixtral-Large-Instruct-2411', skip_tokenizer_init=False, tokenizer_mode=mistral, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=mistral, tensor_parallel_size=4, pipeline_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Pixtral-Large-Instruct-2411, num_scheduler_steps=2, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":2,\"backend\":\"openxla\",\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n\u001b[36m(RayWorkerWrapper pid=150332, ip=10.164.0.35)\u001b[0m WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\r\nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m INFO 12-17 09:40:07 tpu.py:27] Cannot use _Backend.FLASH_ATTN backend on TPU.\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m INFO 12-17 09:40:07 selector.py:163] Using Pallas backend.\r\nINFO 12-17 09:40:08 tpu.py:27] Cannot use _Backend.FLASH_ATTN backend on TPU.\r\nINFO 12-17 09:40:08 selector.py:163] Using Pallas backend.\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467] Error executing method init_device. This might cause deadlock in distributed execution.\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467] Traceback (most recent call last):\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/worker_base.py\", line 459, in execute_method\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     return executor(*args, **kwargs)\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     ensure_model_parallel_initialized(\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1101, in ensure_model_parallel_initialized\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     initialize_model_parallel(tensor_model_parallel_size,\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1045, in initialize_model_parallel\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     _TP = init_model_parallel_group(group_ranks,\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 876, in init_model_parallel_group\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     return GroupCoordinator(\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 43, in __init__\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     local_rank = global_rank % local_world_size\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467] ZeroDivisionError: integer division or modulo by zero\r\nERROR 12-17 09:40:09 worker_base.py:467] Error executing method init_device. This might cause deadlock in distributed execution.\r\nERROR 12-17 09:40:09 worker_base.py:467] Traceback (most recent call last):\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/worker_base.py\", line 459, in execute_method\r\nERROR 12-17 09:40:09 worker_base.py:467]     return executor(*args, **kwargs)\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\r\nERROR 12-17 09:40:09 worker_base.py:467]     ensure_model_parallel_initialized(\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1101, in ensure_model_parallel_initialized\r\nERROR 12-17 09:40:09 worker_base.py:467]     initialize_model_parallel(tensor_model_parallel_size,\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1045, in initialize_model_parallel\r\nERROR 12-17 09:40:09 worker_base.py:467]     _TP = init_model_parallel_group(group_ranks,\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 876, in init_model_parallel_group\r\nERROR 12-17 09:40:09 worker_base.py:467]     return GroupCoordinator(\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\r\nERROR 12-17 09:40:09 worker_base.py:467]     self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 43, in __init__\r\nERROR 12-17 09:40:09 worker_base.py:467]     local_rank = global_rank % local_world_size\r\nERROR 12-17 09:40:09 worker_base.py:467] ZeroDivisionError: integer division or modulo by zero\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/home/ext_hzchen_google_com/miniconda3/envs/tpu/bin/vllm\", line 33, in <module>\r\n[rank0]:     sys.exit(load_entry_point('vllm', 'console_scripts', 'vllm')())\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/scripts.py\", line 201, in main\r\n[rank0]:     args.dispatch_function(args)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/scripts.py\", line 42, in serve\r\n[rank0]:     uvloop.run(run_server(args))\r\n[rank0]:   File \"/home/ext_hzchen_google_com/miniconda3/envs/tpu/lib/python3.10/site-packages/uvloop/__init__.py\", line 82, in run\r\n[rank0]:     return loop.run_until_complete(wrapper())\r\n[rank0]:   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\r\n[rank0]:   File \"/home/ext_hzchen_google_com/miniconda3/envs/tpu/lib/python3.10/site-packages/uvloop/__init__.py\", line 61, in wrapper\r\n[rank0]:     return await main\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/entrypoints/openai/api_server.py\", line 667, in run_server\r\n[rank0]:     async with build_async_engine_client(args) as engine_client:\r\n[rank0]:   File \"/home/ext_hzchen_google_com/miniconda3/envs/tpu/lib/python3.10/contextlib.py\", line 199, in __aenter__\r\n[rank0]:     return await anext(self.gen)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/entrypoints/openai/api_server.py\", line 117, in build_async_engine_client\r\n[rank0]:     async with build_async_engine_client_from_engine_args(\r\n[rank0]:   File \"/home/ext_hzchen_google_com/miniconda3/envs/tpu/lib/python3.10/contextlib.py\", line 199, in __aenter__\r\n[rank0]:     return await anext(self.gen)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/entrypoints/openai/api_server.py\", line 150, in build_async_engine_client_from_engine_args\r\n[rank0]:     engine_client = build_engine()\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/engine/async_llm_engine.py\", line 707, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/engine/async_llm_engine.py\", line 594, in __init__\r\n[rank0]:     self.engine = self._engine_class(*args, **kwargs)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/engine/async_llm_engine.py\", line 267, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/engine/llm_engine.py\", line 288, in __init__\r\n[rank0]:     self.model_executor = executor_class(vllm_config=vllm_config, )\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/executor/ray_tpu_executor.py\", line 306, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/executor/ray_tpu_executor.py\", line 39, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/executor/executor_base.py\", line 36, in __init__\r\n[rank0]:     self._init_executor()\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/executor/ray_tpu_executor.py\", line 51, in _init_executor\r\n[rank0]:     self._init_workers_ray(placement_group)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/executor/ray_tpu_executor.py\", line 184, in _init_workers_ray\r\n[rank0]:     self._run_workers(\"init_device\")\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/executor/ray_tpu_executor.py\", line 249, in _run_workers\r\n[rank0]:     driver_worker_output = self.driver_worker.execute_method(\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/worker_base.py\", line 468, in execute_method\r\n[rank0]:     raise e\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/worker_base.py\", line 459, in execute_method\r\n[rank0]:     return executor(*args, **kwargs)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\r\n[rank0]:     ensure_model_parallel_initialized(\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1101, in ensure_model_parallel_initialized\r\n[rank0]:     initialize_model_parallel(tensor_model_parallel_size,\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1045, in initialize_model_parallel\r\n[rank0]:     _TP = init_model_parallel_group(group_ranks,\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 876, in init_model_parallel_group\r\n[rank0]:     return GroupCoordinator(\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\r\n[rank0]:     self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 43, in __init__\r\n[rank0]:     local_rank = global_rank % local_world_size\r\n[rank0]: ZeroDivisionError: integer division or modulo by zero\r\n\u001b[36m(RayWorkerWrapper pid=140502, ip=10.164.0.40)\u001b[0m WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=144474, ip=10.164.0.36)\u001b[0m INFO 12-17 09:40:09 tpu.py:27] Cannot use _Backend.FLASH_ATTN backend on TPU.\u001b[32m [repeated 30x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=144474, ip=10.164.0.36)\u001b[0m INFO 12-17 09:40:09 selector.py:163] Using Pallas backend.\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467] Error executing method init_device. This might cause deadlock in distributed execution.\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467] Traceback (most recent call last):\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/worker_base.py\", line 459, in execute_method\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     return executor(*args, **kwargs)\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     ensure_model_parallel_initialized(\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1101, in ensure_model_parallel_initialized\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     initialize_model_parallel(tensor_model_parallel_size,\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1045, in initialize_model_parallel\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     _TP = init_model_parallel_group(group_ranks,\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 876, in init_model_parallel_group\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     return GroupCoordinator(\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 43, in __init__\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     local_rank = global_rank % local_world_size\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467] ZeroDivisionError: integer division or modulo by zero\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n```\r\n</details>\r\n\r\nWhat's more, I can start the serving with ```--tensor-parallel-size 32``` only without error, which may have performance impact.\r\nWould like to know if this is work as intended or not.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tpu",
      "ray",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-12-17T13:04:46+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11260/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11260"
  }
]
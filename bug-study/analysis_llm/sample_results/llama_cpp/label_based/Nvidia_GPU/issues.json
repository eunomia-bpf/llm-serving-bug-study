[
  {
    "number": 10764,
    "title": "vulkan: rounding differences on Turing",
    "body": "### Name and Version\n\nfails at commit 26a8406ba9198eb6fdd8329fa717555b4f77f05f\r\n\r\nNot a recent regression, to my knowledge\n\n### Operating systems\n\n_No response_\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Problem description & steps to reproduce\n\nThere are failures in im2col and rope tests that look like rounding differences. I believe Turing is using round-to-zero, which is allowed by the Vulkan spec but doesn't match other implementations or the CPU reference.\r\n\r\n```\r\nIM2COL(type_input=f32,type_kernel=f16,dst_type=f16,ne_input=[10,10,3,1],ne_kernel=[3,3,3,1],s0=1,s1=1,p0=1,p1=1,d0=1,d1=1,is_2D=1): [IM2COL] NMSE = 0.000000203 > 0.000000100 \ufffd[1;31mFAIL\ufffd[0m\r\n\r\nROPE(type=f16,ne_a=[128,32,2,1],n_dims=128,mode=0,n_ctx=512,fs=1.000000,ef=0.000000,af=1.000000,ff=0,v=0): [ROPE] NMSE = 0.000000240 > 0.000000100 \ufffd[1;31mFAIL\ufffd[0m\r\n```\r\n\r\n(more failures at https://github.com/ggml-org/ci/tree/results/llama.cpp/26/a8406ba9198eb6fdd8329fa717555b4f77f05f/ggml-6-x86-vulkan-t4, but the mul_mat failures are unrelated).\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "Nvidia GPU",
      "bug-unconfirmed",
      "Vulkan"
    ],
    "state": "closed",
    "created_at": "2024-12-10T15:18:51+00:00",
    "closed_at": "2024-12-10T20:23:19+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10764/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10764"
  },
  {
    "number": 9881,
    "title": "llama.cpp is slow on GPU",
    "body": "### What happened?\r\n\r\nllama.cpp is running slow on NVIDIA A100 80GB GPU\r\n\r\nSteps to reproduce:\r\n1. git clone https://github.com/ggerganov/llama.cpp && cd llama.cpp\r\n2. mkdir build && cd build\r\n3. cmake .. -DGGML_CUDA=ON\r\n4. make GGML_CUDA=1\r\n3. command:  ./build/bin/llama-cli -m ../gguf_files/llama-3-8B.gguf -t 6912 --ctx-size 50 --n_predict 50 --prompt \"There are two persons named ram and krishna\"\r\n Here threads are set to 6912 since GPU has 6912 CUDA cores.\r\n\r\nIt is slow on gpu compared to cpu.\r\nOn gpu  eval time is around 0.07 tokens per second.\r\nIs this expected behaviour or any tweak should be done while building llama.cpp?\r\n\r\n### Name and Version\r\n\r\nversion: 3902 (c81f3bbb)\r\nbuilt with cc (GCC) 11.4.1 20231218 (Red Hat 11.4.1-3) for aarch64-redhat-linux\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nLinux\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n```\r\n",
    "labels": [
      "Nvidia GPU",
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-10-14T11:19:53+00:00",
    "closed_at": "2024-12-01T01:08:05+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9881/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9881"
  },
  {
    "number": 7492,
    "title": "CUDA graphs break quantized K cache",
    "body": "As of right now it is already possible on master to quantize the K cache via e.g. `-ctk q8_0`. However, this is currently broken on master for batch size 1. Disabling CUDA graphs via the environment variable `GGML_CUDA_DISABLE_GRAPHS=1` fixes the issue.\r\n\r\ncc: @agray3 ",
    "labels": [
      "bug",
      "Nvidia GPU"
    ],
    "state": "closed",
    "created_at": "2024-05-23T12:11:15+00:00",
    "closed_at": "2024-05-27T17:33:43+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7492/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7492"
  },
  {
    "number": 7048,
    "title": "Significantly different results (and WRONG) inference when GPU is enabled.",
    "body": "I am running llama_cpp version 0.2.68 on Ubuntu 22.04LTS under conda environment. Attached are two Jupyter notebooks with ONLY one line changed (use CPU vs GPU).  As you can see for exact same environmental conditions switching between CPU/GPU gives vastly different answers where the GPU is completely wrong.  Some pointers on how to debug this I would appreciate it.\r\n\r\nThe only significant difference between the two files is this one liner\r\n      `#n_gpu_layers=-1, # Uncomment to use GPU acceleration`\r\n\r\nThe model used was **openhermes-2.5-mistral-7b.Q5_K_M.gguf**\r\n\r\n[mistral_llama_large-gpu.pdf](https://github.com/ggerganov/llama.cpp/files/15192723/mistral_llama_large-gpu.pdf)\r\n[mistral_llama_large-cpu.pdf](https://github.com/ggerganov/llama.cpp/files/15192725/mistral_llama_large-cpu.pdf)\r\n\r\n",
    "labels": [
      "bug",
      "Nvidia GPU"
    ],
    "state": "closed",
    "created_at": "2024-05-02T18:51:50+00:00",
    "closed_at": "2024-05-17T18:49:39+00:00",
    "comments": 40,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7048/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7048"
  },
  {
    "number": 6758,
    "title": "ggml : add GPU support for Mamba models",
    "body": "Recently, initial Mamba support (CPU-only) has been introduced in #5328 by @compilade \r\n\r\nIn order to support running these models efficiently on the GPU, we seem to be lacking kernel implementations for the following 2 ops:\r\n\r\n- `GGML_OP_SSM_CONV`\r\n- `GGML_OP_SSM_SCAN`\r\n\r\nCreating this issue to keep track of this and give more visibility of this feature. Help with implementing the missing kernels for CUDA and Metal (and other backends potentially) is welcome. We can also discuss if anything else is required to better support this architecture in `llama.cpp`",
    "labels": [
      "enhancement",
      "help wanted",
      "Nvidia GPU",
      "roadmap"
    ],
    "state": "open",
    "created_at": "2024-04-19T06:47:35+00:00",
    "closed_at": null,
    "comments": 32,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6758/reactions",
      "total_count": 32,
      "+1": 23,
      "-1": 0,
      "laugh": 0,
      "hooray": 3,
      "confused": 0,
      "heart": 3,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6758"
  },
  {
    "number": 3820,
    "title": "Broken generation with specific ngl values",
    "body": "While playing with implementing compression for copy/save state, I found a bug, which turned out to be reproducible in current `main` (41aee4d)\r\n\r\nIt seems to be model independent, and no parameters other than `-ngl` seem to make a difference either.\r\n\r\nThe first symptom happens for `save-load-state`, `main` and `server`, when `-ngl` equal to exactly N-1 is specified, basically this happens (generated output):\r\n\r\n```\r\n Hello there!###############################\r\n```\r\n\r\nSecond symptom was found by accident, when fiddling with `save-load-state` for the purpose of implementing compression. Basically, if `-ngl` is N or bigger (all layers loaded),\r\nThe problem above, seems to disappear, however:\r\nNot only `save-load-state` fails because generated text is different for both runs,\r\nbut also, **after** some tokens were sampled `llama_copy_state_data` outputs mostly empty array, which I only noticed because I tried to dump the state post generation, and suddenly started to get 99% compression ratio on that array. Because it turned out to be mostly zeroes.\r\n\r\nAll `-ngl` values between 0 - (N-2) work properly.\r\n\r\nI have no way of testing on AMD so I do not know if it's Nvidia specific.\r\n\r\n[main.output.txt](https://github.com/ggerganov/llama.cpp/files/13193695/main.output.txt)\r\n[main.log](https://github.com/ggerganov/llama.cpp/files/13193696/main.log)\r\n\r\nAs a sanity check, here are results for `-ngl` from 0 to N with the same model and parameters (except `-ngl`):\r\n\r\n[out.txt](https://github.com/ggerganov/llama.cpp/files/13193775/out.txt)\r\n\r\n\r\nEdit: Interestingly enough, perplexity looks fine ?\r\n```\r\n-ngl N-2 (27/29)\r\n[1]5.2069,[2]5.1932,[3]5.1802,[4]5.2837,[5]5.2742,[6]5.0776,\r\nFinal estimate: PPL = 5.0776 +/- 0.25768\r\n-ngl N-1 (28/29)\r\n[1]5.2069,[2]5.1932,[3]5.1802,[4]5.2837,[5]5.2742,[6]5.0776,\r\nFinal estimate: PPL = 5.0776 +/- 0.25768\r\n-ngl N (29/29)\r\n[1]5.2077,[2]5.1813,[3]5.1687,[4]5.2820,[5]5.2682,[6]5.0756,\r\nFinal estimate: PPL = 5.0756 +/- 0.25766\r\n```",
    "labels": [
      "bug",
      "generation quality",
      "Nvidia GPU"
    ],
    "state": "closed",
    "created_at": "2023-10-27T22:49:53+00:00",
    "closed_at": "2023-11-09T14:08:31+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3820/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3820"
  },
  {
    "number": 3479,
    "title": "llama : improve batched decoding performance",
    "body": "Based on info from the following post, [vLLM](https://github.com/vllm-project/vllm) can achieve the following speeds for parallel decoding on A100 GPU:\r\n\r\nhttps://docs.mystic.ai/docs/mistral-ai-7b-vllm-fast-inference-guide\r\n\r\nBatch size | Tokens/s\r\n-- | --\r\n1 | 46\r\n10 | 400\r\n60 | 1.8k\r\n\r\n(thanks to @wsxiaoys for bringing my attention to this)\r\n\r\nEven though `llama.cpp`'s single batch inference is faster ([~72 t/s](https://github.com/ggerganov/llama.cpp/discussions/3359)) we currently don't seem to scale well with batch size. At batch size 60 for example, the performance is roughly x5 slower than what is reported in the post above.\r\n\r\nWe should understand where is the bottleneck and try to optimize the performance.\r\n\r\n```bash\r\n# batch size 1\r\n./parallel -m ~/f16.gguf -t 1 -ngl 100 -c 8192 -b 512 -s 1 -np 1 -ns 128 -n 100 -cb\r\n\r\n# batch size 10\r\n./parallel -m ~/f16.gguf -t 1 -ngl 100 -c 8192 -b 512 -s 1 -np 10 -ns 128 -n 100 -cb\r\n\r\n# batch size 60\r\n./parallel -m ~/f16.gguf -t 1 -ngl 100 -c 8192 -b 512 -s 1 -np 60 -ns 128 -n 100 -cb\r\n```\r\n\r\nAs discussed with @slaren, the discrepancy is likely due to lack of Flash Attention and CUDA tensor core utilization in `llama.cpp`. Still, I wouldn't be surprised if there is some low-hanging fruit that would improve the performance similar to #3412.\r\n\r\nAt the very least, we should profile things and have a better understanding where to focus in the future.\r\n\r\n---\r\n\r\nHere are some results with `llama.cpp` on A100 (48edda30ee545fdac2e7a33d505382888f748bbf) using OpenLLaMA 7B F16\r\n\r\nTo measure this, I've remove the system prompt from the `parallel` example to match better the vllm test above.\r\nWe count both the prompt and the generated tokens.\r\n\r\nBatch size | Tokens/s\r\n-- | --\r\n1 | 108.29\r\n8 | 247.30\r\n10 | 296.58\r\n16 | 368.59\r\n32 | 422.33\r\n60 | 489.99\r\n64 | 481.83\r\n\r\n```bash\r\n# single batch\r\nLLAMA_CUBLAS=1 make -j && CUDA_VISIBLE_DEVICES=5 ./parallel -m models/openllama-7b/ggml-model-f16.gguf -t 1 -ngl 100 -c 8192 -b 512 -s 1 -np 1 -ns 128 -n 100 -cb\r\n\r\nTotal prompt tokens:   2011, speed: 53.51 t/s\r\nTotal gen tokens:      2059, speed: 54.79 t/s\r\nTotal speed (AVG):           speed: 108.29 t/s\r\n```\r\n\r\n<details>\r\n\r\n```java\r\nmain: clearing the KV cache\r\nClient   0, seq  126, started decoding ...\r\nClient   0, seq  126, prompt   18 t, response   13 t, time  0.25 s, speed 126.04 t/s, cache miss 0  \r\n\r\nInput:    If you could have any superpower, what would it be?\r\nResponse: If you could have any superpower, what would it be?\r\n\r\nmain: clearing the KV cache\r\nClient   0, seq  127, started decoding ...\r\nClient   0, seq  127, prompt   23 t, response   23 t, time  0.40 s, speed 113.95 t/s, cache miss 0  \r\n\r\nInput:    Are you familiar with the Special Theory of Relativity and can you explain it to me?\r\nResponse: I have a question. Are you familiar with the Special Theory of Relativity and can you explain it to me?\r\n\r\nmain: clearing the KV cache\r\n\r\n\r\nTotal prompt tokens:   2011, speed: 53.51 t/s\r\nTotal gen tokens:      2059, speed: 54.79 t/s\r\nTotal speed (AVG):           speed: 108.29 t/s\r\nCache misses:             0\r\n\r\n\r\n\r\nllama_print_timings:        load time =  3377.87 ms\r\nllama_print_timings:      sample time =  1735.54 ms /  2187 runs   (    0.79 ms per token,  1260.13 tokens per second)\r\nllama_print_timings: prompt eval time =  5227.17 ms /  2011 tokens (    2.60 ms per token,   384.72 tokens per second)\r\nllama_print_timings:        eval time = 29932.81 ms /  2060 runs   (   14.53 ms per token,    68.82 tokens per second)\r\nllama_print_timings:       total time = 37582.41 ms\r\n```\r\n</details>\r\n\r\n```bash\r\n# n_parallel = 8\r\nLLAMA_CUBLAS=1 make -j && CUDA_VISIBLE_DEVICES=5 ./parallel -m models/openllama-7b/ggml-model-f16.gguf -t 1 -ngl 100 -c 8192 -b 512 -s 1 -np 8 -ns 128 -n 100 -cb\r\n\r\nTotal prompt tokens:   2011, speed: 124.95 t/s\r\nTotal gen tokens:      1969, speed: 122.34 t/s\r\nTotal speed (AVG):           speed: 247.30 t/s\r\n```\r\n\r\n<details>\r\n\r\n```\r\nClient   7, seq  119, prompt   12 t, response   38 t, time  2.34 s, speed 21.33 t/s, cache miss 0  \r\n\r\nInput:    What is the meaning of life?\r\nResponse: Hello. This is the United States Army, and we need your help! You\u2019ve been drafted to fight in a war against an army of zombies that have taken over the world.\r\n\r\nClient   3, seq  117, prompt   15 t, response   46 t, time  2.82 s, speed 21.66 t/s, cache miss 0  \r\n\r\nInput:    Tell me an interesting fact about llamas.\r\nResponse: I don't know of any interesting facts about llamas, so I searched for \"interesting facts about llama\" on the internet. (Search engine). I found a couple of websites and read some of them.\r\n\r\nClient   6, seq  120, prompt   13 t, response   44 t, time  2.47 s, speed 23.06 t/s, cache miss 0  \r\n\r\nInput:    How to get a job at Google?\r\nResponse: The job is to make sure that Google search works as intended by organizing and maintaining the database. They are also responsible for making sure that everything is running smoothly, updating the website and keeping it up-to-date.\r\n\r\nmain: clearing the KV cache\r\n\r\n\r\nTotal prompt tokens:   2011, speed: 124.95 t/s\r\nTotal gen tokens:      1969, speed: 122.34 t/s\r\nTotal speed (AVG):           speed: 247.30 t/s\r\nCache misses:             0\r\n\r\n\r\n\r\nllama_print_timings:        load time =  3436.27 ms\r\nllama_print_timings:      sample time =  1684.62 ms /  2097 runs   (    0.80 ms per token,  1244.79 tokens per second)\r\nllama_print_timings: prompt eval time = 13690.16 ms /  3975 tokens (    3.44 ms per token,   290.35 tokens per second)\r\nllama_print_timings:        eval time =    94.53 ms /     6 runs   (   15.75 ms per token,    63.47 tokens per second)\r\nllama_print_timings:       total time = 16093.98 ms\r\n```\r\n</details>\r\n\r\n\r\n```bash\r\n# n_parallel = 10\r\nLLAMA_CUBLAS=1 make -j && CUDA_VISIBLE_DEVICES=5 ./parallel -m models/openllama-7b/ggml-model-f16.gguf -t 1 -ngl 100 -c 8192 -b 512 -s 1 -np 10 -ns 128 -n 100 -cb\r\n\r\nTotal prompt tokens:   2011, speed: 153.91 t/s\r\nTotal gen tokens:      1864, speed: 142.66 t/s\r\nTotal speed (AVG):           speed: 296.58 t/s\r\n```\r\n\r\n<details>\r\n\r\n```\r\nClient   7, seq  127, prompt   23 t, response   19 t, time  1.06 s, speed 39.77 t/s, cache miss 0  \r\n\r\nInput:    Are you familiar with the Special Theory of Relativity and can you explain it to me?\r\nResponse: We can try! If we go back in time, everything will be the same, right?\r\n\r\nClient   5, seq  112, prompt   13 t, response   59 t, time  3.26 s, speed 22.08 t/s, cache miss 0  \r\n\r\nInput:    How to get a job at Google?\r\nResponse: \u201cI\u2019ve been with Google for seven years. I started as a summer intern and have worked in a variety of roles, including Search Ads Product Marketing Manager and now Senior Manager of Product Management, Search Ads Strategy. For me, the most memorable aspect of working at Google is the people.\r\n\r\nmain: clearing the KV cache\r\n\r\n\r\nTotal prompt tokens:   2011, speed: 153.91 t/s\r\nTotal gen tokens:      1864, speed: 142.66 t/s\r\nTotal speed (AVG):           speed: 296.58 t/s\r\nCache misses:             0\r\n\r\n\r\n\r\nllama_print_timings:        load time =  3420.25 ms\r\nllama_print_timings:      sample time =  1693.70 ms /  1992 runs   (    0.85 ms per token,  1176.12 tokens per second)\r\nllama_print_timings: prompt eval time = 10678.86 ms /  3870 tokens (    2.76 ms per token,   362.40 tokens per second)\r\nllama_print_timings:        eval time =    96.14 ms /     6 runs   (   16.02 ms per token,    62.41 tokens per second)\r\nllama_print_timings:       total time = 13064.91 ms\r\n```\r\n</details>\r\n\r\n\r\n```bash\r\n# n_parallel = 16\r\nLLAMA_CUBLAS=1 make -j && CUDA_VISIBLE_DEVICES=5 ./parallel -m models/openllama-7b/ggml-model-f16.gguf -t 1 -ngl 100 -c 8192 -b 512 -s 1 -np 16 -ns 128 -n 100 -cb\r\n\r\nTotal prompt tokens:   2011, speed: 181.94 t/s\r\nTotal gen tokens:      2063, speed: 186.65 t/s\r\nTotal speed (AVG):           speed: 368.59 t/s\r\n```\r\n\r\n<details>\r\n\r\n```\r\n\r\nInput:    What is the best way to learn a new language?\r\nResponse: The easiest way to learn any language is to live with someone that speaks that language. However, if that isn\u2019t an option, the best way to learn any language is to use a program that uses a combination of verbal learning and verbal reinforcement to help you learn. When I first started studying Russian, I used programs like Rosetta Stone (which is great for beginners), but what worked best for me was a method\r\n\r\nClient   9, seq   90, prompt   15 t, response   71 t, time  4.76 s, speed 18.08 t/s, cache miss 0  \r\n\r\nInput:    What is the best way to cook a steak?\r\nResponse: The best way to cook a steak is to first preheat your oven to 425 degrees. Then, lightly season both sides of the steak with salt and pepper. Put it on a baking sheet lined with aluminum foil, drizzle with olive oil, and bake it in the oven for 10 minutes, or until medium-rare.\r\n\r\nClient  13, seq  111, prompt   15 t, response   58 t, time  3.22 s, speed 22.69 t/s, cache miss 0  \r\n\r\nInput:    I want to learn how to play the piano.\r\nResponse: I think you are a good piano player and I can teach you all about the piano. You will learn how to play all the songs that you like on the piano in no time. I can teach you how to improve your piano playing so that you can become an even better piano player.\r\n\r\nmain: clearing the KV cache\r\n\r\n\r\nTotal prompt tokens:   2011, speed: 181.94 t/s\r\nTotal gen tokens:      2063, speed: 186.65 t/s\r\nTotal speed (AVG):           speed: 368.59 t/s\r\nCache misses:             0\r\n\r\n\r\n\r\nllama_print_timings:        load time =  3391.46 ms\r\nllama_print_timings:      sample time =  1843.20 ms /  2191 runs   (    0.84 ms per token,  1188.69 tokens per second)\r\nllama_print_timings: prompt eval time =  8358.01 ms /  4063 tokens (    2.06 ms per token,   486.12 tokens per second)\r\nllama_print_timings:        eval time =   200.03 ms /    12 runs   (   16.67 ms per token,    59.99 tokens per second)\r\nllama_print_timings:       total time = 11052.24 ms\r\n```\r\n</details>\r\n\r\n\r\n```bash\r\n# n_parallel = 32\r\nLLAMA_CUBLAS=1 make -j && CUDA_VISIBLE_DEVICES=5 ./parallel -m models/openllama-7b/ggml-model-f16.gguf -t 1 -ngl 100 -c 8192 -b 512 -s 1 -np 32 -ns 128 -n 100 -cb\r\n\r\nTotal prompt tokens:   2011, speed: 186.50 t/s\r\nTotal gen tokens:      2543, speed: 235.83 t/s\r\nTotal speed (AVG):           speed: 422.33 t/s\r\n```\r\n\r\n<details>\r\n\r\n```\r\nInput:    How to get a job at Google?\r\nResponse: Job Description. As an assistant, you will support the people who work at Google and our partners. This includes supporting some of the most senior leaders as they run their teams. You will have a wide variety of responsibilities, including scheduling meetings, booking travel and supporting senior leadership in planning events.\r\n\r\nClient  19, seq   87, prompt   13 t, response   87 t, time  7.09 s, speed 14.11 t/s, cache miss 0  \r\n\r\nInput:    How to get a job at Google?\r\nResponse: Google is a search engine for the Internet and one of the most visited sites on the Internet. However, it has not been easy to work at Google since its creation, as it has taken more than ten years to find it. At the beginning, Larry Page and Sergey Brin were looking for employees who were as intelligent as possible. They did not really understand how to work well or where to search for good workers. They simply thought\r\n\r\nClient  25, seq  127, prompt   23 t, response   75 t, time  4.29 s, speed 22.83 t/s, cache miss 0  \r\n\r\nInput:    Are you familiar with the Special Theory of Relativity and can you explain it to me?\r\nResponse: Yes. The Special Theory of Relativity (SR) is a theory that, in essence, says that the speed of light is constant for all observers. For example, if you have three observers at rest with respect to one another, who are moving towards each other and who have different speeds, the three will measure the same speed of light for any object that they view.\r\n\r\nmain: clearing the KV cache\r\n\r\n\r\nTotal prompt tokens:   2011, speed: 186.50 t/s\r\nTotal gen tokens:      2543, speed: 235.83 t/s\r\nTotal speed (AVG):           speed: 422.33 t/s\r\nCache misses:             0\r\n\r\n\r\n\r\nllama_print_timings:        load time =  3420.38 ms\r\nllama_print_timings:      sample time =  2267.36 ms /  2671 runs   (    0.85 ms per token,  1178.02 tokens per second)\r\nllama_print_timings: prompt eval time =  7318.15 ms /  4535 tokens (    1.61 ms per token,   619.69 tokens per second)\r\nllama_print_timings:        eval time =   412.22 ms /    20 runs   (   20.61 ms per token,    48.52 tokens per second)\r\nllama_print_timings:       total time = 10782.65 ms\r\n```\r\n</details>\r\n\r\n\r\n```bash\r\n# n_parallel = 60\r\nLLAMA_CUBLAS=1 make -j && CUDA_VISIBLE_DEVICES=5 ./parallel -m models/openllama-7b/ggml-model-f16.gguf -t 1 -ngl 100 -c 8192 -b 512 -s 1 -np 60 -ns 128 -n 100 -cb\r\n\r\nTotal prompt tokens:   2011, speed: 235.90 t/s\r\nTotal gen tokens:      2166, speed: 254.09 t/s\r\nTotal speed (AVG):           speed: 489.99 t/s\r\n```\r\n\r\n<details>\r\n\r\n```\r\nClient  33, seq   78, prompt   13 t, response   72 t, time  6.70 s, speed 12.69 t/s, cache miss 0  \r\n\r\nInput:    How to get a job at Google?\r\nResponse: Assistant role at Google is one of the most important jobs in the organization. The job requires candidates who are passionate, enthusiastic and well-versed with the latest technology in the market. The candidates must be passionate and able to understand and solve problems on their own. They should also be able to collaborate with others, communicate effectively, and have a strong work ethic\r\n\r\nClient  26, seq   77, prompt   23 t, response   77 t, time  7.00 s, speed 14.29 t/s, cache miss 0  \r\n\r\nInput:    Are you familiar with the Special Theory of Relativity and can you explain it to me?\r\nResponse: \u201cNo, sir. It is not my specialty. I only know that the theory was first put forward by Einstein, it was quite an influential theory of his and it has been used in a lot of scientific experiments and measurements. There is a whole bunch of experiments that have been done to prove it, but I cannot explain them to you. You should speak to one of my colleagues.\u201d\r\n\r\nClient  29, seq  102, prompt   16 t, response   79 t, time  6.41 s, speed 14.83 t/s, cache miss 0  \r\n\r\nInput:    What is the best way to learn a new language?\r\nResponse: Well I do know that you have to know the grammar, you have to know vocabulary, and you have to get a feel for the sounds and the way it is pronounced. You also have to know the culture of where the language is spoken. And you also have to have friends that are natives of the country to practice with, and that\u2019s really the best way to do it.\r\n\r\nmain: clearing the KV cache\r\n\r\n\r\nTotal prompt tokens:   2011, speed: 235.90 t/s\r\nTotal gen tokens:      2166, speed: 254.09 t/s\r\nTotal speed (AVG):           speed: 489.99 t/s\r\nCache misses:             0\r\n\r\n\r\n\r\nllama_print_timings:        load time =  3407.33 ms\r\nllama_print_timings:      sample time =  1923.99 ms /  2294 runs   (    0.84 ms per token,  1192.31 tokens per second)\r\nllama_print_timings: prompt eval time =  5760.76 ms /  4170 tokens (    1.38 ms per token,   723.86 tokens per second)\r\nllama_print_timings:        eval time =   159.77 ms /     8 runs   (   19.97 ms per token,    50.07 tokens per second)\r\nllama_print_timings:       total time =  8524.06 ms\r\n```\r\n</details>\r\n\r\n\r\n```bash\r\n# n_parallel = 64\r\nLLAMA_CUBLAS=1 make -j && CUDA_VISIBLE_DEVICES=5 ./parallel -m models/openllama-7b/ggml-model-f16.gguf -t 1 -ngl 100 -c 8192 -b 512 -s 1 -np 64 -ns 128 -n 100 -cb\r\n\r\nTotal prompt tokens:   2011, speed: 228.04 t/s\r\nTotal gen tokens:      2238, speed: 253.78 t/s\r\nTotal speed (AVG):           speed: 481.83 t/s\r\n```\r\n\r\n<details>\r\n\r\n```\r\nClient  61, seq   61, prompt   23 t, response   77 t, time  8.09 s, speed 12.36 t/s, cache miss 0  \r\n\r\nInput:    Are you familiar with the Special Theory of Relativity and can you explain it to me?\r\nResponse: Sure. The Special Theory of Relativity is very simply understood by the layman. It concerns the speed of light and how to measure distance. You can imagine a room with a large light bulb at one end, a meter stick on the floor and a tape measure, a ruler, etc. at the other end of the room. When we go to that far end of the room\r\n\r\nClient  15, seq   82, prompt   23 t, response   74 t, time  7.03 s, speed 13.79 t/s, cache miss 0  \r\n\r\nInput:    Are you familiar with the Special Theory of Relativity and can you explain it to me?\r\nResponse: Yes, you can ask me about the Special Theory of Relativity. This theory states that the speed of light in vacuum is constant and independent of the source or the observer in a coordinate system moving relative to the source. Einstein's relativity theory also states that gravity is not a force but that it can be described as the curvature of space-time.\r\n\r\nClient  47, seq  127, prompt   23 t, response   77 t, time  5.48 s, speed 18.24 t/s, cache miss 0  \r\n\r\nInput:    Are you familiar with the Special Theory of Relativity and can you explain it to me?\r\nResponse: I\u2019m sure you have heard about the Special Theory of Relativity by now, although it is not very often brought up in the classroom. It is a theory developed by the famous physicist Albert Einstein that explains how space and time are interrelated. For example, if you travel fast enough across space, you would experience time as speeding up. On the other hand, in general rel\r\n\r\nmain: clearing the KV cache\r\n\r\n\r\nTotal prompt tokens:   2011, speed: 228.04 t/s\r\nTotal gen tokens:      2238, speed: 253.78 t/s\r\nTotal speed (AVG):           speed: 481.83 t/s\r\nCache misses:             0\r\n\r\n\r\n\r\nllama_print_timings:        load time =  3401.75 ms\r\nllama_print_timings:      sample time =  1976.50 ms /  2366 runs   (    0.84 ms per token,  1197.06 tokens per second)\r\nllama_print_timings: prompt eval time =  5806.75 ms /  4234 tokens (    1.37 ms per token,   729.15 tokens per second)\r\nllama_print_timings:        eval time =   335.70 ms /    16 runs   (   20.98 ms per token,    47.66 tokens per second)\r\nllama_print_timings:       total time =  8817.67 ms\r\n```\r\n</details>",
    "labels": [
      "performance",
      "Nvidia GPU"
    ],
    "state": "closed",
    "created_at": "2023-10-04T20:20:55+00:00",
    "closed_at": "2023-10-24T13:48:38+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3479/reactions",
      "total_count": 13,
      "+1": 13,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3479"
  }
]
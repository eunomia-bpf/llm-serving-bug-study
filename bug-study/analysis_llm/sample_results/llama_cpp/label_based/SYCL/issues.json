[
  {
    "number": 9106,
    "title": "Bug: Intel Arc - not working at all",
    "body": "### What happened?\n\nGoing through the manual - SYCL I mean. Everything compiles okay. Running it always thows an error. Can't make it work. OS used: Linux Gentoo. P.S. docker doesn't work either. P.P.S. device IS listed in the list.\n\n### Name and Version\n\n# ./build/bin/llama-cli --version\r\nversion: 3609 (2f3c1466)\r\nbuilt with Intel(R) oneAPI DPC++/C++ Compiler 2024.2.1 (2024.2.1.20240711) for x86_64-unknown-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\n# ZES_ENABLE_SYSMAN=1 ./build/bin/llama-cli -m models/llama-2-7b.Q4_0.gguf -p \"Building a website can be done in 10 simple steps:\" -n 400 -e -ngl 33 -sm none -mg 0\r\nLog start\r\nmain: build = 3609 (2f3c1466)\r\nmain: built with Intel(R) oneAPI DPC++/C++ Compiler 2024.2.1 (2024.2.1.20240711) for x86_64-unknown-linux-gnu\r\nmain: seed  = 1724182694\r\nllama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from models/llama-2-7b.Q4_0.gguf (version GGUF V2)\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = LLaMA v2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 4096\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  18:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_0:  225 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special tokens cache size = 3\r\nllm_load_vocab: token to piece cache size = 0.1684 MB\r\nllm_load_print_meta: format           = GGUF V2\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 4096\r\nllm_load_print_meta: n_embd_v_gqa     = 4096\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 11008\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 4096\r\nllm_load_print_meta: n_embd_v_gqa     = 4096\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 11008\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0                                                  llm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_0                                               llm_load_print_meta: model params     = 6.74 B\r\nllm_load_print_meta: model size       = 3.56 GiB (4.54 BPW)                                llm_load_print_meta: general.name     = LLaMA v2\r\nllm_load_print_meta: BOS token        = 1 '<s>'                                            llm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'                                          llm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_print_meta: max token length = 48                                                 ggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes                                                          ggml_sycl_init: found 2 SYCL devices:\r\nllm_load_tensors: ggml ctx size =    0.27 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU                                    llm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  3577.56 MiB\r\nllm_load_tensors:        CPU buffer size =    70.31 MiB\r\n..................................................................................................\r\nllama_new_context_with_model: n_ctx      = 4096\r\nllama_new_context_with_model: n_batch    = 2048                                            llama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1                                               [SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: yes\r\nfound 2 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Arc A380 Graphics|    1.3|    128|    1024|   32|  6064M|            1.3.29735|\r\n| 1| [level_zero:gpu:1]|                 Intel UHD Graphics 630|    1.3|     24|     256|   32| 46333M|            1.3.29735|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =  2048.00 MiB\r\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.12 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =   296.00 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    16.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 2\r\nUnexpected pattern!\r\nUNREACHABLE executed at /var/tmp/portage/dev-util/spirv-llvm-translator-15.0.0-r1/work/SPIRV-LLVM-Translator-15.0.0/lib/SPIRV/SPIRVUtil.cpp:2037!\r\nThe program was built for 1 devices\r\nBuild program log for 'Intel(R) Arc(TM) A380 Graphics':\r\n -11 (PI_ERROR_BUILD_PROGRAM_FAILURE)Exception caught at file:/home/username/llama/ggml/src/ggml-sycl.cpp, line:2722\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "SYCL",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-08-20T19:45:26+00:00",
    "closed_at": "2024-12-17T01:07:43+00:00",
    "comments": 28,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9106/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9106"
  },
  {
    "number": 8551,
    "title": "Bug: InvalidModule: Invalid SPIR-V module: input SPIR-V module uses extension 'SPV_INTEL_memory_access_aliasing' which were disabled by --spirv-ext option",
    "body": "### What happened?\n\nCurrently on Fedora 40 with Intel Arc A750.\r\n\r\nRunning the following:\r\n```txt\r\nZES_ENABLE_SYSMAN=1 ./build/bin/llama-server \\\r\n-t 10 \\\r\n-ngl 20 \\\r\n-b 512 \\\r\n--ctx-size 16384 \\\r\n-m ~/llama-models/llama-2-7b.Q4_0.gguf \\\r\n--color -c 3400 \\\r\n--seed 42 \\\r\n--temp 0.8 \\\r\n--top_k 5 \\\r\n--repeat_penalty 1.1 \\\r\n--host :: \\\r\n--port 8080 \\ \r\n-n -1 \\\r\n-sm none -mg 0\r\n```\r\n\r\ngives the following output:\r\n```txt\r\nINFO [                    main] build info | tid=\"140466031364096\" timestamp=1721277895 build=3411 commit=\"e02b597b\"\r\nINFO [                    main] system info | tid=\"140466031364096\" timestamp=1721277895 n_threads=10 n_threads_batch=-1 total_threads=28 system_info=\"AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \"\r\nllama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /home/jacob/llama-models/llama-2-7b.Q4_0.gguf (version GGUF V2)\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = LLaMA v2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 4096\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  18:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_0:  225 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special tokens cache size = 3\r\nllm_load_vocab: token to piece cache size = 0.1684 MB\r\nllm_load_print_meta: format           = GGUF V2\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 4096\r\nllm_load_print_meta: n_embd_v_gqa     = 4096\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 11008\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 6.74 B\r\nllm_load_print_meta: model size       = 3.56 GiB (4.54 BPW)\r\nllm_load_print_meta: general.name     = LLaMA v2\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_print_meta: max token length = 48\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 5 SYCL devices:\r\nllm_load_tensors: ggml ctx size =    0.27 MiB\r\nllm_load_tensors: offloading 20 repeating layers to GPU\r\nllm_load_tensors: offloaded 20/33 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  2171.88 MiB\r\nllm_load_tensors:        CPU buffer size =  3647.87 MiB\r\n.................................................................................................\r\nllama_new_context_with_model: n_ctx      = 3424\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 5 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Arc A750 Graphics|    1.3|    448|    1024|   32|  8096M|            1.3.28717|\r\n| 1| [level_zero:gpu:1]|                 Intel UHD Graphics 770|    1.3|     32|     512|   32| 62662M|            1.3.28717|\r\n| 2|     [opencl:gpu:0]|                Intel Arc A750 Graphics|    3.0|    448|    1024|   32|  8096M|       24.09.28717.17|\r\n| 3|     [opencl:gpu:1]|                 Intel UHD Graphics 770|    3.0|     32|     512|   32| 62662M|       24.09.28717.17|\r\n| 4|     [opencl:cpu:0]|                   Intel Core i7-14700K|    3.0|     28|    8192|   64| 67164M|2024.18.6.0.02_160000|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =  1070.00 MiB\r\nllama_kv_cache_init:  SYCL_Host KV buffer size =   642.00 MiB\r\nllama_new_context_with_model: KV self size  = 1712.00 MiB, K (f16):  856.00 MiB, V (f16):  856.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.24 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =   300.50 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    22.69 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 136\r\nInvalidModule: Invalid SPIR-V module: input SPIR-V module uses extension 'SPV_INTEL_memory_access_aliasing' which were disabled by --spirv-ext option\r\n```\r\n\r\nI'm not sure where the `--spirv-ext` option is set, but it seems like a compiler flag. What can I do to fix this?\r\n\r\nI ran the following to set this up, and the build did not fail:\r\n```txt\r\n# Export relevant ENV variables\r\nsource /opt/intel/oneapi/setvars.sh\r\n\r\n# Build LLAMA with MKL BLAS acceleration for intel GPU\r\n\r\n# Option 1: Use FP32 (recommended for better performance in most cases)\r\ncmake -B build -DGGML_SYCL=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx\r\n\r\n# Option 2: Use FP16\r\ncmake -B build -DGGML_SYCL=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_SYCL_F16=ON\r\n\r\n# build all binary\r\ncmake --build build --config Release -j -v\r\n```\r\n\r\nThis is my result for `clinfo`:\r\n```txt\r\nPlatform #0: Intel(R) OpenCL\r\n `-- Device #0: Intel(R) Core(TM) i7-14700K\r\nPlatform #1: Intel(R) OpenCL Graphics\r\n `-- Device #0: Intel(R) Arc(TM) A750 Graphics\r\nPlatform #2: Intel(R) OpenCL Graphics\r\n `-- Device #0: Intel(R) UHD Graphics 770\r\n```\r\n\r\nThis is my result for `./build/bin/llama-ls-sycl-device`:\r\n```txt\r\nfound 5 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Arc A750 Graphics|    1.3|    448|    1024|   32|  8096M|            1.3.28717|\r\n| 1| [level_zero:gpu:1]|                 Intel UHD Graphics 770|    1.3|     32|     512|   32| 62662M|            1.3.28717|\r\n| 2|     [opencl:gpu:0]|                Intel Arc A750 Graphics|    3.0|    448|    1024|   32|  8096M|       24.09.28717.17|\r\n| 3|     [opencl:gpu:1]|                 Intel UHD Graphics 770|    3.0|     32|     512|   32| 62662M|       24.09.28717.17|\r\n| 4|     [opencl:cpu:0]|                   Intel Core i7-14700K|    3.0|     28|    8192|   64| 67164M|2024.18.6.0.02_160000|\r\n```\n\n### Name and Version\n\n~/llama.cpp$ ./build/bin/llama-server --version\r\n\r\nversion: 3411 (e02b597b)\r\nbuilt with Intel(R) oneAPI DPC++/C++ Compiler 2024.2.0 (2024.2.0.20240602) for x86_64-unknown-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "SYCL",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-07-18T04:49:20+00:00",
    "closed_at": "2024-09-06T01:07:04+00:00",
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8551/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8551"
  },
  {
    "number": 8294,
    "title": "Bug: [SYCL] Inference not working correctly on multiple GPUs",
    "body": "### What happened?\n\nI am using Llama.cpp + SYCL to perform inference on a multiple GPU server. However, I get a Segmentation Fault when using multiple GPUs. The same model can produce inference output correctly with single GPU mode.\r\n\r\n```shell\r\ngit clone https://github.com/ggerganov/llama.cpp.git\r\nsource /opt/intel/oneapi/setvars.sh\r\ncmake -B build -DGGML_SYCL=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx\r\ncmake --build build --config Release -j -v\r\n\r\ncd ~/llama.cpp/\r\n./build/bin/llama-ls-sycl-device\r\n\r\n## single gpu, ok\r\n./build/bin/llama-cli -m ~/mistral-7b-v0.1.Q4_0.gguf -p \"Building a website can be done in 10 simple steps:\\nStep 1:\" -n 400 -e -ngl 33 -s 0 -sm none -mg 0\r\n\r\n## multiple gpus, Segmentation Fault, core dumped\r\n./build/bin/llama-cli -m ~/mistral-7b-v0.1.Q4_0.gguf -p \"Building a website can be done in 10 simple steps:\\nStep 1:\" -n 400 -e -ngl 33 -s 0 -sm layer\r\n``` \r\n\r\n![image](https://github.com/ggerganov/llama.cpp/assets/16000946/f2ef0a11-6b54-43b2-acba-23624e1c3b85)\r\n![image](https://github.com/ggerganov/llama.cpp/assets/16000946/40a9d95b-29d8-4372-8c1f-48bebfd90534)\r\n\r\nOutput of `./build/bin/llama-ls-sycl-device`:\r\n```\r\nfound 8 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|    1.3|    512|    1024|   32| 16225M|            1.3.26241|\r\n| 1| [level_zero:gpu:1]|                Intel Arc A770 Graphics|    1.3|    512|    1024|   32| 16225M|            1.3.26241|\r\n| 2| [level_zero:gpu:2]|                 Intel UHD Graphics 770|    1.3|     32|     512|   32| 53751M|            1.3.26241|\r\n| 3|     [opencl:gpu:0]|                Intel Arc A770 Graphics|    3.0|    512|    1024|   32| 16225M|       23.17.26241.33|\r\n| 4|     [opencl:gpu:1]|                Intel Arc A770 Graphics|    3.0|    512|    1024|   32| 16225M|       23.17.26241.33|\r\n| 5|     [opencl:gpu:2]|                 Intel UHD Graphics 770|    3.0|     32|     512|   32| 53751M|       23.17.26241.33|\r\n| 6|     [opencl:cpu:0]|                   Intel Core i9-14900K|    3.0|     32|    8192|   64| 67189M|2023.16.11.0.22_160000|\r\n| 7|     [opencl:acc:0]|            Intel FPGA Emulation Device|    1.2|     32|67108864|   64| 67189M|2023.16.11.0.22_160000|\r\n```\n\n### Name and Version\n\n```bash\r\n./llama-cli --version\r\nversion: 3292 (20fc3804)\r\nbuilt with Intel(R) oneAPI DPC++/C++ Compiler 2024.0.1 (2024.0.1.20231122) for x86_64-unknown-linux-gnu\r\n\r\n```\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\nLog start\r\nmain: build = 3292 (20fc3804)\r\nmain: built with Intel(R) oneAPI DPC++/C++ Compiler 2024.0.1 (2024.0.1.20231122) for x86_64-unknown-linux-gnu\r\nmain: seed  = 0\r\nllama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /home/arda/qiyue/mistral-7b-v0.1.Q4_0.gguf (version GGUF V2)\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-v0.1\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_0:  225 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special tokens cache size = 259\r\nllm_load_vocab: token to piece cache size = 0.1637 MB\r\nllm_load_print_meta: format           = GGUF V2\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 7.24 B\r\nllm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \r\nllm_load_print_meta: general.name     = mistralai_mistral-7b-v0.1\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_print_meta: max token length = 48\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 8 SYCL devices:\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\nllm_load_tensors: ggml ctx size =    1.23 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =   234.06 MiB\r\nllm_load_tensors:      SYCL1 buffer size =   234.06 MiB\r\nllm_load_tensors:      SYCL2 buffer size =   702.19 MiB\r\nllm_load_tensors:      SYCL3 buffer size =   234.06 MiB\r\nllm_load_tensors:      SYCL4 buffer size =   117.03 MiB\r\nllm_load_tensors:      SYCL5 buffer size =   702.19 MiB\r\nllm_load_tensors:      SYCL6 buffer size =   819.22 MiB\r\nllm_load_tensors:      SYCL7 buffer size =   804.74 MiB\r\nllm_load_tensors:        CPU buffer size =    70.31 MiB\r\n.................................................................................................\r\nllama_new_context_with_model: n_ctx      = 32768\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 8 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|    1.3|    512|    1024|   32| 16225M|            1.3.26241|\r\n| 1| [level_zero:gpu:1]|                Intel Arc A770 Graphics|    1.3|    512|    1024|   32| 16225M|            1.3.26241|\r\n| 2| [level_zero:gpu:2]|                 Intel UHD Graphics 770|    1.3|     32|     512|   32| 53751M|            1.3.26241|\r\n| 3|     [opencl:gpu:0]|                Intel Arc A770 Graphics|    3.0|    512|    1024|   32| 16225M|       23.17.26241.33|\r\n| 4|     [opencl:gpu:1]|                Intel Arc A770 Graphics|    3.0|    512|    1024|   32| 16225M|       23.17.26241.33|\r\n| 5|     [opencl:gpu:2]|                 Intel UHD Graphics 770|    3.0|     32|     512|   32| 53751M|       23.17.26241.33|\r\n| 6|     [opencl:cpu:0]|                   Intel Core i9-14900K|    3.0|     32|    8192|   64| 67189M|2023.16.11.0.22_160000|\r\n| 7|     [opencl:acc:0]|            Intel FPGA Emulation Device|    1.2|     32|67108864|   64| 67189M|2023.16.11.0.22_160000|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =   256.00 MiB\r\nllama_kv_cache_init:      SYCL1 KV buffer size =   256.00 MiB\r\nllama_kv_cache_init:      SYCL2 KV buffer size =   768.00 MiB\r\nllama_kv_cache_init:      SYCL3 KV buffer size =   256.00 MiB\r\nllama_kv_cache_init:      SYCL4 KV buffer size =   128.00 MiB\r\nllama_kv_cache_init:      SYCL5 KV buffer size =   768.00 MiB\r\nllama_kv_cache_init:      SYCL6 KV buffer size =   896.00 MiB\r\nllama_kv_cache_init:      SYCL7 KV buffer size =   768.00 MiB\r\nllama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.12 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =  2144.00 MiB\r\nllama_new_context_with_model:      SYCL1 compute buffer size =  2144.00 MiB\r\nllama_new_context_with_model:      SYCL2 compute buffer size =  2144.00 MiB\r\nllama_new_context_with_model:      SYCL3 compute buffer size =  2144.00 MiB\r\nllama_new_context_with_model:      SYCL4 compute buffer size =  2144.00 MiB\r\nllama_new_context_with_model:      SYCL5 compute buffer size =  2144.00 MiB\r\nllama_new_context_with_model:      SYCL6 compute buffer size =  2144.00 MiB\r\nllama_new_context_with_model:      SYCL7 compute buffer size =  2144.00 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    72.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 9\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "SYCL",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-07-04T11:46:57+00:00",
    "closed_at": "2024-09-07T01:07:06+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8294/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8294"
  }
]
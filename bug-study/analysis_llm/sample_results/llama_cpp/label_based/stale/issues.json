[
  {
    "number": 1869,
    "title": "train-text-from-scratch.exe stop after \"begin training\" (tensor->src0 is null)",
    "body": "I'm running the latest release (master-254a7a7) like that:\r\n\r\n`bin\\train-text-from-scratch.exe --vocab-model models\\ggml-vocab.bin --checkpoint-in chk-lamartine-256x16.bin --checkpoint-out chk-lamartine-256x16.bin --model-out ggml-lamartine-265x16-f32.bin --train-data \"shakespeare.txt\"           `\r\nI tried with several models.\r\n\r\n# Expected Behavior\r\n\r\nTraining shoud run for a long time\r\n\r\n# Current Behavior\r\n\r\nTraining stop immediatly without error:\r\n\r\n```\r\nD:\\git\\llama.cpp>bin\\train-text-from-scratch.exe --vocab-model models\\ggml-vocab.bin --ctx 64 --embd 256 --head 8 --layer 16 --checkpoint-in chk-lamartine-256x16.bin --checkpoint-out chk-lamartine-256x16.bin --model-out ggml-lamartine-265x16-f32.bin --train-data \"alphonsedelamartine.txt\" -t 6 -b 1 -n 32 --seed 2 --adam-iter 16 --print-details-interval 0 --predict 16 --use-flash\r\nmain: seed: 2\r\nllama.cpp: loading model from models\\ggml-vocab.bin\r\nllama_model_load_internal: format     = ggjt v1 (pre #1405)\r\nllama_model_load_internal: n_vocab    = 32000\r\nllama_model_load_internal: n_ctx      = 512\r\nllama_model_load_internal: n_embd     = 4096\r\nllama_model_load_internal: n_mult     = 256\r\nllama_model_load_internal: n_head     = 32\r\nllama_model_load_internal: n_layer    = 32\r\nllama_model_load_internal: n_rot      = 128\r\nllama_model_load_internal: ftype      = 1 (mostly F16)\r\nllama_model_load_internal: n_ff       = 11008\r\nllama_model_load_internal: n_parts    = 1\r\nllama_model_load_internal: model size = 7B\r\nmain: tokenize training data\r\nmain: number of training tokens: 474\r\nprint_params: n_vocab: 32000\r\nprint_params: n_ctx:   64\r\nprint_params: n_embd:  256\r\nprint_params: n_mult:  256\r\nprint_params: n_head:  8\r\nprint_params: n_ff:    768\r\nprint_params: n_layer: 16\r\nprint_params: n_rot:   32\r\nmain: number of unique tokens: 253\r\nggml_init_cublas: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 3080\r\nmain: init model\r\nload_checkpoint: Training iterations: 0.\r\nload_checkpoint: Training samples:    0.\r\nload_checkpoint: Training tokens:     0.\r\nmain: opt iter 0\r\nused_mem model+cache: 242364416 bytes\r\nmain: begin training\r\n```\r\n\r\n# Environment and Context\r\n\r\nWindows 11\r\nNVidia RTX 3080\r\nRyzen 7 2700\r\nRam 32GB",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-06-15T07:26:00+00:00",
    "closed_at": "2024-04-10T01:07:05+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1869/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1869"
  },
  {
    "number": 4429,
    "title": "Add `completion` server parameters to `v1/chat/completions` ",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\n\r\nThe same set of parameters should be available when calling from either `completion` or `v1/chat/completions` endpoints. Most notably `min_p` and `grammar` are useful to have.\r\n\r\nA call like this should be possible for example:\r\n\r\n```bash\r\ncurl http://localhost:3077/v1/chat/completions \\\r\n-H \"Content-Type: application/json\" \\\r\n-H \"Authorization: Bearer no-key\" \\\r\n-d '{\r\n\"temperature\": 1.0,\r\n\"min_p\": 0.01,\r\n\"top_k\": 0,\r\n\"top_p\": 1,\r\n\"repeat_penalty\": 1,\r\n\"grammar\":  \"root ::= (\\\"Hello!\\\" | \\\"Hi!\\\")\",\r\n\"messages\": [\r\n{\r\n    \"role\": \"system\",\r\n    \"content\": \"You are ChatGPT, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests.\"\r\n},\r\n{\r\n    \"role\": \"user\",\r\n    \"content\": \"Hi\"\r\n}\r\n]\r\n}'\r\n```\r\n\r\n# Motivation\r\n\r\nTo be able to fully make use the llama.cpp backend, when replacing another LLM call that uses openai sdk for example, its useful to have access to the full set of parameters to tune the output for the task. It's possible to add those parameters as a dictionary using the `extra_body` input parameter when making a call using the python openai library.\r\n\r\nIf the parameters aren't available when making the switch, the dev will have to consider changing the code to use the `completion` endpoint instead, or even have separate versions of the same code to be able to compare different LLMs.\r\n\r\n# Possible Implementation\r\n\r\nI'm guessing `oaicompat_completion_params_parse` function in `examples/server/server.cpp` can be used to add more parameters. \r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-12T17:32:11+00:00",
    "closed_at": "2024-04-03T01:14:16+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4429/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4429"
  },
  {
    "number": 9097,
    "title": "Bug: Throughput (tokens/sec) does not scale with increasing batch sizes in Intel GPUs",
    "body": "### What happened?\n\nFor Intel dGPU like ARC770, the tokens per second doesn't scale with increasing batch size. For example if tps for batch size 1 is ~x tps, for batch size 8 also throughput is ~x tps. \n\n### Name and Version\n\nllama build: 2663 (7e54166)\r\nOS ubuntu 22.04\r\ncommand line used: ZES_ENABLE_SYSMAN=1 ./build/bin/main -m models/llama-2-7b.Q8_0.gguf -ngl 33 -mg 0 -b 1 -p \"solve the 3 ants and triangel puzzle\"\r\nbatch size changed for different execution.\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-08-20T04:34:26+00:00",
    "closed_at": "2024-10-06T01:07:32+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9097/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9097"
  },
  {
    "number": 1494,
    "title": "[Enhancement] Simultaneous CLBLAS/CUBLAS instances. ",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ ]x I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Enhancement\r\n\r\nIf not already possible through a config I missed, would offloading some layers to CLBLAS and other layers to CUBLAS be viable? Or maybe offloading layers to multiple CLBLAS devices?\r\n\r\nA common hardware config is a CPU with an IGP + discrete gpu, and this would allow the IGP to be utilized on systems with weak CPUs and low-vram dGPUs. And much more powerful, 4 channel IGPs are rumored to be in development at Intel/AMD.\r\n\r\nWith the extra transfers and possible CPU bandwidth starvation, this may or may not even improve performance much... I'm not sure.",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-05-17T03:14:18+00:00",
    "closed_at": "2024-04-09T01:09:02+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1494/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1494"
  },
  {
    "number": 7076,
    "title": "Using #pragma once makes it difficult",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\n\r\nIt would be helpful to use\r\n ```\r\n#ifndef GGML_H\r\n#define GGML_H\r\n...\r\n#endif ```\r\n\r\nInstead of \"#pragma once\"\r\n\r\nI'm noticing if I include say llama.cpp and whisper.cpp as git submodules in my project then these #pragma once directive do not correctly avoid including the file twice...  Using a GGML_H handles all cases... i'm using latest mac environment too... it's a little more typing in each file but would help make the code easier to use in more environments... thanks!",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-04T15:30:23+00:00",
    "closed_at": "2024-06-19T01:06:39+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7076/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7076"
  },
  {
    "number": 13854,
    "title": "Eval bug: Embeddings Always returned as non",
    "body": "### Name and Version\nMac os:\nllama-cli --version\nversion: 5390 (aa48e373)\nbuilt with Apple clang version 16.0.0 (clang-1600.0.26.6) for arm64-apple-darwin23.6.0\n\nUbuntu os:\n./llama-cli --version\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla T4, compute capability 7.5, VMM: yes\nload_backend: loaded CUDA backend from /app/libggml-cuda.so\nload_backend: loaded CPU backend from /app/libggml-cpu-haswell.so\nversion: 5332 (7c28a74e)\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n\n### Operating systems\n\nMac,Ubuntu\n\n### GGML backends\n\nMetal,Cuda\n\n### Hardware\n\nGPUs\n\n### Models\n\nall models\n\n### Problem description & steps to reproduce\n\nwhen i run llama-server with embeddings enabled i got null for all embeddings vectors, and when try to use the cli i got the same result\n\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 8 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \nmain: last token in the prompt is not SEP\nmain: 'tokenizer.ggml.add_eos_token' should be set to 'true' in the GGUF header\nbatch_decode: n_tokens = 9, n_seq = 1\n\nembedding 0:       nan       nan       nan  ...       nan       nan       nan \nembedding 1:       nan       nan       nan  ...       nan       nan       nan \nembedding 2:       nan       nan       nan  ...       nan       nan       nan \nembedding 3:       nan       nan       nan  ...       nan       nan       nan \nembedding 4:       nan       nan       nan  ...       nan       nan       nan \nembedding 5:       nan       nan       nan  ...       nan       nan       nan \nembedding 6:       nan       nan       nan  ...       nan       nan       nan \nembedding 7:       nan       nan       nan  ...       nan       nan       nan \nembedding 8:       nan       nan       nan  ...       nan       nan       nan \n\nApi response when using /embedding end point: \n(.venv) (\u2388|redz-gpu:redz) \u279c  flink python3 llm_embeddings.py\n/Users/homyt-devops/Sync/flink/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn(\n\u2705 Status Code: 200\n\ud83d\udce6 Raw JSON Response:\n[\n  {\n    \"index\": 0,\n    \"embedding\": [\n      [\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n      ]\n    ]\n  }\n]\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-05-28T11:48:26+00:00",
    "closed_at": "2025-07-12T01:08:20+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13854/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13854"
  },
  {
    "number": 3051,
    "title": "Multi-GPU support for AMD?",
    "body": "Do you have multi-GPU support for AMD, if not, do you see it as something you might add in the future?",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-09-07T00:15:22+00:00",
    "closed_at": "2024-06-12T01:06:50+00:00",
    "comments": 28,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3051/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3051"
  },
  {
    "number": 9666,
    "title": "Bug: Issue building hipBLAS error: call to undeclared function '_mm256_dpbusd_epi32'",
    "body": "### What happened?\r\n\r\nHi, \r\n\r\nI'm trying to compile llama with the hipBLAS backed on \r\nCPU: 12th Gen Intel(R) Core(TM) i9-12900K\r\nGPU: AMD Radeon PRO W7800 (gfx1100)\r\nOS: Windows 11 23H2\r\nWith AMD HIP SDK 6.1.2 for Windows Installed:\r\nhttps://www.amd.com/en/developer/resources/rocm-hub/eula/licenses.html?filename=AMD-Software-PRO-Edition-24.Q3-Win10-Win11-For-HIP.exe\r\n\r\nllama.cpp version: https://github.com/ggerganov/llama.cpp/releases/tag/b3828\r\n\r\nWhen I run these commands\r\n```\r\nset PATH=%HIP_PATH%\\bin;%PATH%\r\ncmake -S . -B build -G Ninja -DAMDGPU_TARGETS=gfx1100 -DGGML_HIPBLAS=ON -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DCMAKE_BUILD_TYPE=Release\r\ncmake --build build\r\n```\r\n\r\nI get this error message, and the build is unable to continue:\r\n```\r\nllama.cpp-b3828/ggml/src/ggml-quants.c:107:34: error: call to undeclared function '_mm256_dpbusd_epi32'; ISO C99 and later do not support implicit function declarations [-Wimplicit-function-declaration]\r\n    const __m256i summed_pairs = _mm256_dpbusd_epi32(zero, ax, sy);\r\n                                 ^\r\nllama.cpp-b3828/ggml/src/ggml-quants.c:107:19: error: initializing 'const __m256i' (vector of 4 'long long' values) with an expre\r\n```\r\n\r\nCan someone tell me what I'm doing wrong?\r\n\r\n### Name and Version\r\n\r\nCPU: 12th Gen Intel(R) Core(TM) i9-12900K\r\nGPU: AMD Radeon PRO W7800 (gfx1100)\r\nOS: Windows 11 23H2\r\nWith AMD HIP SDK 6.1.2 for Windows Installed:\r\nhttps://www.amd.com/en/developer/resources/rocm-hub/eula/licenses.html?filename=AMD-Software-PRO-Edition-24.Q3-Win10-Win11-For-HIP.exe\r\n\r\nllama.cpp version: https://github.com/ggerganov/llama.cpp/releases/tag/b3828\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nWindows\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n[1/245] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.obj\r\nFAILED: ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.obj \r\nccache C:\\PROGRA~1\\AMD\\ROCm\\5.5\\bin\\clang.exe -DGGML_BUILD -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CUDA -DGGML_USE_HIPBLAS -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -DK_QUANTS_PER_ITERATION=2 -D_CRT_SECURE_NO_WARNINGS -D_XOPEN_SOURCE=600 -D__HIP_PLATFORM_AMD__=1 -D__HIP_PLATFORM_HCC__=1 -Dggml_EXPORTS -IC:/Users/owen/Desktop/Owen/llama.cpp-b3828/ggml/src/../include -IC:/Users/owen/Desktop/Owen/llama.cpp-b3828/ggml/src/. -isystem \"C:/Program Files/AMD/ROCm/5.5/include\" -O3 -DNDEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrt -std=gnu11 -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -march=native -fopenmp=libomp -MD -MT ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.obj -MF ggml\\src\\CMakeFiles\\ggml.dir\\ggml-quants.c.obj.d -o ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.obj -c C:/Users/owen/Desktop/Owen/llama.cpp-b3828/ggml/src/ggml-quants.c\r\nIn file included from C:/Users/owen/Desktop/Owen/llama.cpp-b3828/ggml/src/ggml-quants.c:4:\r\nIn file included from C:/Users/owen/Desktop/Owen/llama.cpp-b3828/ggml/src/./ggml-quants.h:4:\r\nC:/Users/owen/Desktop/Owen/llama.cpp-b3828/ggml/src/./ggml-common.h:62:9: warning: keyword is hidden by macro definition [-Wkeyword-macro]\r\n#define static_assert(cond, msg) _Static_assert(cond, msg)\r\n        ^\r\nC:/Users/owen/Desktop/Owen/llama.cpp-b3828/ggml/src/ggml-quants.c:107:34: error: call to undeclared function '_mm256_dpbusd_epi32'; ISO C99 and later do not support implicit function declarations [-Wimplicit-function-declaration]\r\n    const __m256i summed_pairs = _mm256_dpbusd_epi32(zero, ax, sy);\r\n                                 ^\r\nC:/Users/owen/Desktop/Owen/llama.cpp-b3828/ggml/src/ggml-quants.c:107:19: error: initializing 'const __m256i' (vector of 4 'long long' values) with an expression of incompatible type 'int'\r\n    const __m256i summed_pairs = _mm256_dpbusd_epi32(zero, ax, sy);\r\n                  ^              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n1 warning and 2 errors generated.\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-09-27T16:37:54+00:00",
    "closed_at": "2024-11-12T01:08:45+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9666/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9666"
  },
  {
    "number": 9630,
    "title": "Do llama.cpp support input_embeds?",
    "body": "Do llama.cpp support input_embeds? Just like `transformers` support `input_embeds` in `model.generate` function.",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-09-24T14:53:16+00:00",
    "closed_at": "2024-11-09T01:07:02+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9630/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9630"
  },
  {
    "number": 9086,
    "title": "Feature Request: Tensor Parallelism support",
    "body": "### Prerequisites\r\n\r\n- [X] I am running the latest code. Mention the version if possible as well.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\r\n\r\n### Feature Description\r\n\r\nTensor parallelism is a a critical technique employed to train and inference from very large language models by splitting the actual computations/tensors across multiple compute devices. \r\n\r\n### Motivation\r\n\r\nIn our previous implementation on Xeon CPU, tensor parallelism(TP) can significantly reduce the latency on inference. <html xmlns:v=\"urn:schemas-microsoft-com:vml\"\r\nxmlns:o=\"urn:schemas-microsoft-com:office:office\"\r\nxmlns:x=\"urn:schemas-microsoft-com:office:excel\"\r\nxmlns=\"http://www.w3.org/TR/REC-html40\">\r\n\r\n<head>\r\n\r\n<meta name=ProgId content=Excel.Sheet>\r\n<meta name=Generator content=\"Microsoft Excel 15\">\r\n<link id=Main-File rel=Main-File\r\nhref=\"file:///C:/Users/chen13/AppData/Local/Temp/msohtmlclip1/01/clip.htm\">\r\n<link rel=File-List\r\nhref=\"file:///C:/Users/chen13/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml\">\r\n<style>\r\n<!--table\r\n\t{mso-displayed-decimal-separator:\"\\.\";\r\n\tmso-displayed-thousand-separator:\"\\,\";}\r\n@page\r\n\t{margin:.75in .7in .75in .7in;\r\n\tmso-header-margin:.3in;\r\n\tmso-footer-margin:.3in;}\r\ntr\r\n\t{mso-height-source:auto;}\r\ncol\r\n\t{mso-width-source:auto;}\r\nbr\r\n\t{mso-data-placement:same-cell;}\r\ntd\r\n\t{padding-top:1px;\r\n\tpadding-right:1px;\r\n\tpadding-left:1px;\r\n\tmso-ignore:padding;\r\n\tcolor:black;\r\n\tfont-size:11.0pt;\r\n\tfont-weight:400;\r\n\tfont-style:normal;\r\n\ttext-decoration:none;\r\n\tfont-family:Calibri, sans-serif;\r\n\tmso-font-charset:0;\r\n\tmso-number-format:General;\r\n\ttext-align:general;\r\n\tvertical-align:bottom;\r\n\tborder:none;\r\n\tmso-background-source:auto;\r\n\tmso-pattern:auto;\r\n\tmso-protection:locked visible;\r\n\twhite-space:nowrap;\r\n\tmso-rotate:0;}\r\n-->\r\n</style>\r\n</head>\r\n\r\n<body link=\"#0563C1\" vlink=\"#954F72\">\r\n\r\n\r\nmodel | precision | TP size | input_size | nex_token_time/ms\r\n-- | -- | -- | -- | --\r\nllama2-70b | q4_j | 1 | 32 | 191.91\r\nllama2-70b | q4_j | 2 | 32 | 120.87\r\nllama2-70b | q4_j | 4 | 32 | 86.15\r\nllama2-70b | q4_j | 1 | 1024 | 197.18\r\nllama2-70b | q4_j | 2 | 1024 | 129.25\r\nllama2-70b | q4_j | 4 | 1024 | 91.76\r\nllama2-70b | q4_j | 1 | 2012 | 204.85\r\nllama2-70b | q4_j | 2 | 2012 | 127.31\r\nllama2-70b | q4_j | 4 | 2012 | 100.44\r\n\r\n\r\n\r\n</body>\r\n\r\n</html>\r\n\r\nNotice: TP size= 1 means not use TP.\r\n\r\n### Possible Implementation\r\n\r\nIn our TP implementation, we adopt the method of pre-splitting the corresponding weights, so the time consumed for this part is one-time and does not affect inference performance. Meanwhile, another major factor impacting performance is 'all reduce'. Since each node computes partial and incomplete results, it is necessary to perform 'all reduce' on the output data. But all reduce is relatively time-consuming, interestingly, by using a reasonable splitting and combining method, primitives can be operated independently across nodes, which is very helpful for performance optimization. Thus, a rational splitting method becomes extremely important.\r\n\r\nTaking the FFN module as an example, if the first matmul splits by column and computes the matmul with input, it will result in two unrelated sub-matrices on each node. These two sub-matrices, when performing the second matmul operation, can proceed directly without having to perform 'all reduce' if splitting by rows. Thus, the entire FFN module only requires one 'all reduce', meaning that with properly tailored split implementation, even with multiple matmul operations, only one 'all reduce' operation may be needed. We ignored the element-wise operations between matmul as they would not influence the results.\r\n![image](https://github.com/user-attachments/assets/8a9d6c4a-45ca-4fa7-9930-1660936fda90)\r\nThe scenario for the attention module is more complex. As shown in the following figure, a rational split can make it so that the entire attention module only requires one 'all reduce' operation, thus greatly saving synchronization time.\r\n![image](https://github.com/user-attachments/assets/19d77152-4dff-4b8e-a3e2-34d582ce3b53)\r\n",
    "labels": [
      "enhancement",
      "threading",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-19T01:38:13+00:00",
    "closed_at": "2024-12-13T01:07:40+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9086/reactions",
      "total_count": 11,
      "+1": 11,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9086"
  },
  {
    "number": 12290,
    "title": "Misc. bug: Intel docker containers are running out of disk space during build",
    "body": "### Name and Version\n\nSee: https://github.com/ggml-org/llama.cpp/actions/runs/13744758544/job/38438332993\n\nIt has the error message: \n\n> You are running out of disk space. The runner will stop working when the machine runs out of disk space. Free space left: 24 MB\n\nIt looks like there is already a [free disk space](https://github.com/ggml-org/llama.cpp/blob/master/.github/workflows/docker.yml#L103-L118) but it is currently [disabled for all builds](https://github.com/ggml-org/llama.cpp/blob/master/.github/workflows/docker.yml#L42). \n\n### Problem description & steps to reproduce\n\nIntel docker container builds are failing due to running out of disk space.\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-09T20:19:04+00:00",
    "closed_at": "2025-04-23T01:07:41+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12290/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12290"
  },
  {
    "number": 5916,
    "title": "\u3010help\u3011why  function  llama_build_graph  is internal function  llama_decode\uff1f",
    "body": "  I read the llama.cpp source code\u3002\r\n  I am confused as to why the function llama_build_graph needs to be called every time the function llama_decode is called.\r\n The function llama_build_graph cannot be called during program initialization, which will reduce the inference time.\r\n\r\nstatic int llama_decode_internal(\r\n         llama_context & lctx,\r\n           llama_batch   batch) {\r\n    ....\r\n   ggml_cgraph * gf = llama_build_graph(lctx, batch, false);\r\n.....\r\n}\r\n\r\nThanks",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-07T06:56:01+00:00",
    "closed_at": "2024-04-21T01:06:37+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5916/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5916"
  },
  {
    "number": 1188,
    "title": "[Suggestion] Add parameter for setting openblas threads",
    "body": "Openblas deafults to some maximum available threads, but would probably not be the most optimal.\r\nIn Openblas there is a function to set the number of threads, why not use this?\r\n\r\n```void openblas_set_num_threads(int num_threads);```\r\n\r\nCurrent workaround is to set an openblas environment variable.",
    "labels": [
      "enhancement",
      "threading",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-26T13:24:17+00:00",
    "closed_at": "2024-04-09T01:09:52+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1188/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1188"
  },
  {
    "number": 2185,
    "title": "[requirement] Support baichuan 13B model.",
    "body": "Baichuan 13B model use ALiBi , any plan to support this model?",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-07-12T03:15:38+00:00",
    "closed_at": "2024-04-09T01:08:09+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2185/reactions",
      "total_count": 11,
      "+1": 11,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2185"
  },
  {
    "number": 3701,
    "title": "Prompt evaluation performance regression in llama.cpp on RDNA3 with HSA_OVERRIDE_GFX_VERSION=11.0.1 vs 11.0.0",
    "body": "(Probably a ROCm issue - see https://github.com/RadeonOpenCompute/ROCm/issues/2590 -, but maybe llama.cpp devs can also weigh in.)\r\n\r\nWe're evaluating llama.cpp with ROCm offloading on various RDNA3 GPUs (primarily RX 7800 XT and RX 7900 XT).\r\n\r\nOn initial testing, we found that a 13b model with Q6_K quantization fully offloaded to GPU (-ngl 43) showed significantly slower prompt evaluation times on the 7800 compared to the 7900, far beyond what would be expected from the relative performance difference between these 2 GPUs. On the 7800, prompt evaluation would take more than a minute on our example prompt, while it was near instantaneous (less than a second) on the 7900.\r\n\r\nUpon further investigation, the RX 7800 shows a 3 second penalty for every 64 tokens of prompt (0-64 tokens 3 seconds, 65-128 tokens 6s, 129-192 tokens 9s, and so on) over the 7900.\r\n\r\nSince the 7800 is recognized by ROCm as gfx1101, vs gfx1100 on the 7900, we tried setting HSA_OVERRIDE_GFX_VERSION=11.0.0, which led to massive performance improvement, with the 7800 only proportionally slower than the 7900, as expected based on the specifications. We then tested HSA_OVERRIDE_GFX_VERSION=11.0.1 (and 11.0.2) on the 7900, and saw the exact same performance issue as on the 7800.\r\n\r\nThe testing was performed on Ubuntu 22.04.3 Server, using ROCm v5.7.0, apparently the latest version available for this version of Ubuntu. The issue was reproducible using the very latest llama.cpp code and a newly quantized model.",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-10-20T17:47:41+00:00",
    "closed_at": "2024-04-04T01:07:46+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3701/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3701"
  },
  {
    "number": 4103,
    "title": "server: system prompt makes generated text incoherent",
    "body": "# Current Behavior\r\n\r\nPassing a system prompt to the `server` makes the generated text incoherent after the first request.\r\n\r\n\r\n# Environment and Context\r\n\r\n**Commit:** 8da46278e1a57107591653275f8e03a281de94f0\r\n\r\n**OS:** Kubuntu 23.10\r\n\r\n<blockquote>\r\n\u276f lscpu | grep -P 'Model name|Flags'\r\n\r\nModel name:                         AMD Ryzen 9 7900 12-Core Processor\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid overflow_recov succor smca fsrm flush_l1d\r\n</blockquote>\r\n\r\n```\r\n\u276f uname -a\r\nLinux comp 6.5.0-10-generic #10-Ubuntu SMP PREEMPT_DYNAMIC Fri Oct 13 13:49:38 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\n```\r\n\u276f make --version | head -1\r\nGNU Make 4.3\r\n```\r\n\r\n```\r\n\u276f g++ --version | head -1\r\ng++ (Ubuntu 13.2.0-4ubuntu3) 13.2.0\r\n```\r\n\r\n\r\n# Steps to Reproduce\r\n\r\n1. I used this model:\r\nhttps://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/blob/main/llama-2-13b-chat.Q4_K_M.gguf\r\n\r\n2. The server is built with just `make`, no other params.\r\n\r\n3. Start the server:\r\n```\r\n./server -m /opt/models/text/llama-2-13b-chat.Q4_K_M.gguf\r\n```\r\n\r\n<details>\r\n<summary>startup log</summary>\r\n\r\n```\r\n{\"timestamp\":1700156091,\"level\":\"INFO\",\"function\":\"main\",\"line\":2268,\"message\":\"build info\",\"build\":1519,\"commit\":\"8da4627\"}\r\n{\"timestamp\":1700156091,\"level\":\"INFO\",\"function\":\"main\",\"line\":2271,\"message\":\"system info\",\"n_threads\":12,\"n_threads_batch\":-1,\"total_threads\":24,\"system_info\":\"AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \"}\r\nllama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /opt/models/text/llama-2-13b-chat.Q4_K_M.gguf (version GGUF V2)\r\nllama_model_loader: - tensor    0:                token_embd.weight q4_K     [  5120, 32000,     1,     1 ]\r\nllama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor    2:            blk.0.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor    4:              blk.0.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor    6:              blk.0.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor    7:         blk.0.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor    8:              blk.0.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor    9:              blk.0.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   11:            blk.1.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   13:              blk.1.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   15:              blk.1.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   16:         blk.1.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   17:              blk.1.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   18:              blk.1.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   20:           blk.10.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   22:             blk.10.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   24:             blk.10.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   25:        blk.10.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   26:             blk.10.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   27:             blk.10.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   29:           blk.11.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   31:             blk.11.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   33:             blk.11.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   34:        blk.11.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   35:             blk.11.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   36:             blk.11.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   38:           blk.12.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   40:             blk.12.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   42:             blk.12.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   43:        blk.12.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   44:             blk.12.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   45:             blk.12.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   47:           blk.13.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   49:             blk.13.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   51:             blk.13.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   52:        blk.13.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   53:             blk.13.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   54:             blk.13.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   56:           blk.14.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   58:             blk.14.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   60:             blk.14.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   61:        blk.14.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   62:             blk.14.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   63:             blk.14.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   64:             blk.15.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   65:             blk.15.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   66:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   67:            blk.2.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   68:            blk.2.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   69:              blk.2.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   70:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   71:              blk.2.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   72:         blk.2.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   73:              blk.2.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   74:              blk.2.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   75:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   76:            blk.3.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   77:            blk.3.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   78:              blk.3.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   79:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   80:              blk.3.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   81:         blk.3.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   82:              blk.3.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   83:              blk.3.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   84:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   85:            blk.4.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   86:            blk.4.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   87:              blk.4.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   88:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   89:              blk.4.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   90:         blk.4.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   91:              blk.4.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   92:              blk.4.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   93:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   94:            blk.5.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   95:            blk.5.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   96:              blk.5.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   97:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   98:              blk.5.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   99:         blk.5.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  100:              blk.5.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  101:              blk.5.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  102:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  103:            blk.6.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  104:            blk.6.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  105:              blk.6.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  106:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  107:              blk.6.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  108:         blk.6.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  109:              blk.6.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  110:              blk.6.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  111:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  112:            blk.7.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  113:            blk.7.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  114:              blk.7.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  115:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  116:              blk.7.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  117:         blk.7.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  118:              blk.7.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  119:              blk.7.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  120:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  121:            blk.8.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  122:            blk.8.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  123:              blk.8.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  124:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  125:              blk.8.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  126:         blk.8.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  127:              blk.8.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  128:              blk.8.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  129:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  130:            blk.9.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  131:            blk.9.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  132:              blk.9.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  133:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  134:              blk.9.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  135:         blk.9.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  136:              blk.9.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  137:              blk.9.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  138:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  139:           blk.15.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  141:             blk.15.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  142:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  143:        blk.15.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  144:             blk.15.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  145:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  146:           blk.16.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  147:           blk.16.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  148:             blk.16.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  149:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  150:             blk.16.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  151:        blk.16.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  152:             blk.16.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  153:             blk.16.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  154:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  155:           blk.17.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  156:           blk.17.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  157:             blk.17.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  158:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  159:             blk.17.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  160:        blk.17.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  161:             blk.17.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  162:             blk.17.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  163:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  164:           blk.18.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  165:           blk.18.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  166:             blk.18.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  167:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  168:             blk.18.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  169:        blk.18.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  170:             blk.18.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  171:             blk.18.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  172:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  173:           blk.19.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  174:           blk.19.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  175:             blk.19.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  176:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  177:             blk.19.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  178:        blk.19.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  179:             blk.19.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  180:             blk.19.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  181:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  182:           blk.20.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  183:           blk.20.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  184:             blk.20.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  185:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  186:             blk.20.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  187:        blk.20.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  188:             blk.20.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  189:             blk.20.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  190:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  191:           blk.21.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  192:           blk.21.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  193:             blk.21.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  194:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  195:             blk.21.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  196:        blk.21.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  197:             blk.21.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  198:             blk.21.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  199:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  200:           blk.22.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  201:           blk.22.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  202:             blk.22.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  203:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  204:             blk.22.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  205:        blk.22.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  206:             blk.22.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  207:             blk.22.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  208:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  209:           blk.23.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  210:           blk.23.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  211:             blk.23.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  212:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  213:             blk.23.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  214:        blk.23.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  215:             blk.23.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  216:             blk.23.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  217:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  218:           blk.24.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  219:           blk.24.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  220:             blk.24.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  221:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  222:             blk.24.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  223:        blk.24.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  224:             blk.24.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  225:             blk.24.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  226:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  227:           blk.25.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  228:           blk.25.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  229:             blk.25.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  230:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  231:             blk.25.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  232:        blk.25.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  233:             blk.25.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  234:             blk.25.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  235:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  236:           blk.26.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  237:           blk.26.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  238:             blk.26.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  239:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  240:             blk.26.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  241:        blk.26.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  242:             blk.26.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  243:             blk.26.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  244:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  245:           blk.27.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  246:           blk.27.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  247:             blk.27.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  248:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  249:             blk.27.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  250:        blk.27.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  251:             blk.27.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  252:             blk.27.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  253:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  254:           blk.28.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  255:           blk.28.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  256:             blk.28.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  257:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  258:             blk.28.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  259:        blk.28.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  260:             blk.28.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  261:             blk.28.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  262:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  263:           blk.29.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  264:           blk.29.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  265:             blk.29.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  266:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  267:             blk.29.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  268:        blk.29.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  269:             blk.29.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  270:             blk.29.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  271:           blk.30.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  272:             blk.30.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  273:             blk.30.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  274:        blk.30.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  275:             blk.30.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  276:             blk.30.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  277:                    output.weight q6_K     [  5120, 32000,     1,     1 ]\r\nllama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  279:           blk.30.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  280:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  282:           blk.31.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  284:             blk.31.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  286:             blk.31.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  287:        blk.31.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  288:             blk.31.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  289:             blk.31.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  290:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  291:           blk.32.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  292:           blk.32.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  293:             blk.32.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  294:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  295:             blk.32.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  296:        blk.32.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  297:             blk.32.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  298:             blk.32.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  299:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  300:           blk.33.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  301:           blk.33.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  302:             blk.33.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  303:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  304:             blk.33.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  305:        blk.33.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  306:             blk.33.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  307:             blk.33.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  308:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  309:           blk.34.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  310:           blk.34.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  311:             blk.34.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  312:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  313:             blk.34.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  314:        blk.34.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  315:             blk.34.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  316:             blk.34.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  317:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  318:           blk.35.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  319:           blk.35.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  320:             blk.35.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  321:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  322:             blk.35.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  323:        blk.35.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  324:             blk.35.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  325:             blk.35.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  326:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  327:           blk.36.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  328:           blk.36.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  329:             blk.36.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  330:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  331:             blk.36.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  332:        blk.36.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  333:             blk.36.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  334:             blk.36.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  335:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  336:           blk.37.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  337:           blk.37.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  338:             blk.37.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  339:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  340:             blk.37.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  341:        blk.37.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  342:             blk.37.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  343:             blk.37.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  344:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  345:           blk.38.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  346:           blk.38.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  347:             blk.38.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  348:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  349:             blk.38.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  350:        blk.38.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  351:             blk.38.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  352:             blk.38.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  353:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  354:           blk.39.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  355:           blk.39.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  356:             blk.39.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  357:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  358:             blk.39.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  359:        blk.39.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  360:             blk.39.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  361:             blk.39.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  362:               output_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - kv   0:                       general.architecture str\r\nllama_model_loader: - kv   1:                               general.name str\r\nllama_model_loader: - kv   2:                       llama.context_length u32\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32\r\nllama_model_loader: - kv   4:                          llama.block_count u32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32\r\nllama_model_loader: - kv  10:                          general.file_type u32\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str\r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr\r\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32\r\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32\r\nllama_model_loader: - kv  18:               general.quantization_version u32\r\nllama_model_loader: - type  f32:   81 tensors\r\nllama_model_loader: - type q4_K:  241 tensors\r\nllama_model_loader: - type q6_K:   41 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V2\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 5120\r\nllm_load_print_meta: n_head           = 40\r\nllm_load_print_meta: n_head_kv        = 40\r\nllm_load_print_meta: n_layer          = 40\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 13824\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: model type       = 13B\r\nllm_load_print_meta: model ftype      = mostly Q4_K - Medium\r\nllm_load_print_meta: model params     = 13.02 B\r\nllm_load_print_meta: model size       = 7.33 GiB (4.83 BPW)\r\nllm_load_print_meta: general.name   = LLaMA v2\r\nllm_load_print_meta: BOS token = 1 '<s>'\r\nllm_load_print_meta: EOS token = 2 '</s>'\r\nllm_load_print_meta: UNK token = 0 '<unk>'\r\nllm_load_print_meta: LF token  = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.13 MB\r\nllm_load_tensors: mem required  = 7500.98 MB\r\n....................................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_new_context_with_model: kv self size  =  400.00 MB\r\nllama_build_graph: non-view tensors processed: 924/924\r\nllama_new_context_with_model: compute buffer total size = 76.57 MB\r\nAvailable slots:\r\n -> Slot 0 - max context: 512\r\n\r\nllama server listening at http://127.0.0.1:8080\r\n\r\n{\"timestamp\":1700156092,\"level\":\"INFO\",\"function\":\"main\",\"line\":2548,\"message\":\"HTTP server listening\",\"hostname\":\"127.0.0.1\",\"port\":8080}\r\nall slots are idle and system prompt is empty, clear the KV cache\r\n```\r\n</details>\r\n\r\n4. Call the API and set the system prompt in it, e.g:\r\n```\r\ncurl -sS -H 'Content-Type: application/json' --data '{\"n_predict\":8, \"prompt\":\"When he looked in the mirror, he saw that he\", \"system_prompt\": {\"prompt\": \"This is a story about a mysterious man.\"}, \"cache_prompt\": true}' http://127.0.0.1:8080/completion | jq .content\r\n```\r\n\r\n\r\n# Failure Logs\r\n\r\nFirst, these are the results, if you never set the system prompt:\r\n```\r\n\" had become a monkey.\\n\\nHe\"\r\n\" was still dressed as a woman.\\n\\\"\"\r\n\" had aged. He had grown old and fra\"\r\n\" was no longer a handsome man. His\"\r\n\" had grown a goatee.\\nWhen\"\r\n```\r\n\r\nSeems OK. Now the results with the system prompt:\r\n\r\n```\r\n\" was no longer young and handsome. He\"\r\n\" he he. He was a man of mystery\"\r\n\". He was a mysterious man with a\"\r\n\" he\\nThis is a story about a myster\"\r\n\" ha ha ha ha ha ha ha ha.\"\r\n```\r\n\r\nThe first result is always OK, but the rest are just either end abruptly with a full stop or contain some nonsense. It also doesn't matter if you specify the system prompt in the second and later requests or not.\r\n\r\nAlso, if you reset the system prompt (set it to an empty string), the output seems weird and disconnected as well, but it also almost never starts with a space, e.g. this query:\r\n```\r\ncurl -sS -H 'Content-Type: application/json' --data '{\"n_predict\":8, \"prompt\":\"When he looked in the mirror, he saw that he\", \"system_prompt\": {\"prompt\": \"\"}, \"cache_prompt\": true}' http://127.0.0.1:8080/completion | jq .content\r\n```\r\n\r\nGives these results:\r\n```\r\n\"ated debate about immigration and border security.\"\r\n\"aling process.\\n2. Diet: E\"\r\n\"ist film genre.\\nThe story follows a\"\r\n\"brew alphabet song lyrics and the he\"\r\n\"ir of God\u2019s promise to Abraham.\"\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-11-16T17:48:59+00:00",
    "closed_at": "2024-04-02T01:11:19+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4103/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4103"
  },
  {
    "number": 9765,
    "title": "Bug:  Rocm extreme slow down on GFX1100 with release binary",
    "body": "### What happened?\n\nThere are large slow down on gfx1100\n\n### Name and Version\n\n.\\llama-cli.exe --version\r\nversion: 1 (b6d6c52)\r\nbuilt with  for x86_64-pc-windows-msvc\r\n\r\n.\\llama-cli.exe --version\r\nversion: 3235 (88540445)\r\nbuilt with  for x86_64-pc-windows-msvc\n\n### What operating system are you seeing the problem on?\n\nWindows\n\n### Relevant log output\n\n```shell\nlatest release binary\r\n.\\llama-bench.exe -m W:\\model\\qwen2-7b-instruct-q5_k_m.gguf -ngl 99 -fa 1,0\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 ROCm devices:\r\n  Device 0: AMD Radeon RX 7900 XTX, compute capability 11.0, VMM: no\r\n| model                          |       size |     params | backend    | ngl | fa |          test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ------------: | -------------------: |\r\n| qwen2 ?B Q5_K - Medium         |   5.07 GiB |     7.62 B | CUDA       |  99 |  1 |         pp512 |       1443.02 \u00b1 4.16 |\r\n| qwen2 ?B Q5_K - Medium         |   5.07 GiB |     7.62 B | CUDA       |  99 |  1 |         tg128 |          5.17 \u00b1 0.03 |\r\n| qwen2 ?B Q5_K - Medium         |   5.07 GiB |     7.62 B | CUDA       |  99 |  0 |         pp512 |       1588.26 \u00b1 4.46 |\r\n| qwen2 ?B Q5_K - Medium         |   5.07 GiB |     7.62 B | CUDA       |  99 |  0 |         tg128 |          5.16 \u00b1 0.04 |\r\n\r\nbuild: b6d6c52 (1)\r\n\r\n\r\nbefore\r\n\r\n.\\llama-bench.exe -m W:\\model\\qwen2-7b-instruct-q5_k_m.gguf -ngl 99 -fa 1,0\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 ROCm devices:\r\n  Device 0: AMD Radeon RX 7900 XTX, compute capability 11.0, VMM: no\r\n| model                          |       size |     params | backend    | ngl | fa |          test |              t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ------------: | ---------------: |\r\n| qwen2 ?B Q5_K - Medium         |   5.07 GiB |     7.62 B | ROCm       |  99 |  1 |         pp512 |  2775.82 \u00b1 13.51 |\r\n| qwen2 ?B Q5_K - Medium         |   5.07 GiB |     7.62 B | ROCm       |  99 |  1 |         tg128 |     90.52 \u00b1 0.20 |\r\n| qwen2 ?B Q5_K - Medium         |   5.07 GiB |     7.62 B | ROCm       |  99 |  0 |         pp512 |  3108.33 \u00b1 26.07 |\r\n| qwen2 ?B Q5_K - Medium         |   5.07 GiB |     7.62 B | ROCm       |  99 |  0 |         tg128 |     89.28 \u00b1 0.21 |\r\n\r\nbuild: 88540445 (3235)\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-10-06T17:16:14+00:00",
    "closed_at": "2024-11-20T01:07:29+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9765/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9765"
  },
  {
    "number": 692,
    "title": "How to make llama.cpp return control to add additional context?",
    "body": "I want to be able to tell the model that if it can't reply something useful to return control so I can give more information.\r\n\r\nSimilarly, how do I add more context so that it can reason about a full conversation or say a specific set of documents?\r\n\r\nFor example, I ask it something and it should say I don't know can you provide me more information? And then I give it a document. Then I can add another document to the prompt, so it can understand from that and so on.\r\n\r\nI've heard this is some sort of chaining, but I don't understand.",
    "labels": [
      "enhancement",
      "generation quality",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-01T22:20:36+00:00",
    "closed_at": "2024-04-11T01:07:16+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/692/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/692"
  },
  {
    "number": 11493,
    "title": "Misc. bug: Vulkan is not optional at runtime",
    "body": "### Name and Version\n\nWith llama.cpp version from git tag b4549, if I compile Vulkan support in and then run in an environment where Vulkan is not supported (for example this would happen if a Linux distribution provides llama.cpp with Vulkan enabled but the user doesn't have a GPU with Vulkan), it will fail with the following exception:\n\n    terminate called after throwing an instance of 'vk::IncompatibleDriverError'\n      what():  vk::createInstance: ErrorIncompatibleDriver\n\nIt would be better to just disable Vulkan in this case but run on CPU.\n\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nlibllama (core library)\n\n### Command line\n\n```shell\nAny llama-cli command (yes, even `llama-cli -dev none`).\n```\n\n### Problem description & steps to reproduce\n\n1. Compile with GGML_VULKAN=ON\n2. Run without GPU\n3. It crashes with exception\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-29T17:45:16+00:00",
    "closed_at": "2025-03-16T01:07:49+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11493/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11493"
  },
  {
    "number": 3630,
    "title": "[Question] WARP_SIZE as 64 for MI GPUs?",
    "body": "MI GPUs based on the gfx9xx architecture such as MI100 and MI200 natively run on wave64 (64 threads per warp/wave). That means by not setting `WARP_SIZE` as `64`, we're not able to fully utilize those GPUs. RDNA1 onwards support wave32 as well as wave64, so it can still run well when WARP_SIZE is `32`.\r\n\r\nWhat would it take to get `WARP_SIZE` of `64` to work? I tried it and ran llama2 7b q4_0 but regardless of a prompt I give, the result is always `######...`\r\n\r\nI suspect the reduction kernels that use `__shfl_*_sync` have the last parameter width set to `32`. So I tried setting those to `WARP_SIZE` as well, as well as setting the `s_sum` size to `WARP_SIZE`, but that didn't make any difference. Am I missing something? I guess there's still many parts of the code that rely on `WARP_SIZE` set to `32`, so I was wondering which parts I should look at.",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-10-15T09:14:05+00:00",
    "closed_at": "2024-04-04T01:08:18+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3630/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3630"
  },
  {
    "number": 13298,
    "title": "Eval bug: Can't run Qwen3-32B Q4_K_XL",
    "body": "### Name and Version\n\nbuild: 5273 (8ae5ebcf) with gcc-14 (Homebrew GCC 14.2.0_1) 14.2.0 for x86_64-pc-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\n2x T4\n\n### Models\n\nhttps://huggingface.co/unsloth/Qwen3-32B-GGUF/blob/main/Qwen3-32B-UD-Q4_K_XL.gguf\n\n### Problem description & steps to reproduce\n\nNaN perplexity and completely trashed output while using [this model](https://huggingface.co/unsloth/Qwen3-32B-GGUF/blob/main/Qwen3-32B-UD-Q4_K_XL.gguf)\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 2 CUDA devices:\n  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n  Device 1: Tesla T4, compute capability 7.5, VMM: yes\nbuild: 5273 (8ae5ebcf) with gcc-14 (Homebrew GCC 14.2.0_1) 14.2.0 for x86_64-pc-linux-gnu\nllama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\nllama_model_load_from_file_impl: using device CUDA1 (Tesla T4) - 14992 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 707 tensors from /root/Qwen3-32B-UD-Q4_K_XL.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3-32B\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3-32B\nllama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth\nllama_model_loader: - kv   5:                         general.size_label str              = 32B\nllama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth\nllama_model_loader: - kv   7:                          qwen3.block_count u32              = 64\nllama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960\nllama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 5120\nllama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 25600\nllama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 64\nllama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\nllama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654\nllama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  26:               general.quantization_version u32              = 2\nllama_model_loader: - kv  27:                          general.file_type u32              = 15\nllama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-32B-GGUF/imatrix_unsloth.dat\nllama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-32B.txt\nllama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 448\nllama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 32\nllama_model_loader: - type  f32:  257 tensors\nllama_model_loader: - type q4_K:  293 tensors\nllama_model_loader: - type q5_K:   35 tensors\nllama_model_loader: - type q6_K:   94 tensors\nllama_model_loader: - type iq4_xs:   28 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 18.64 GiB (4.89 BPW) \nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 40960\nprint_info: n_embd           = 5120\nprint_info: n_layer          = 64\nprint_info: n_head           = 64\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 8\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 25600\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 40960\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 32B\nprint_info: model params     = 32.76 B\nprint_info: general.name     = Qwen3-32B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 11 ','\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151654 '<|vision_pad|>'\nprint_info: LF token         = 198 '\u010a'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 64 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 65/65 layers to GPU\nload_tensors:  CUDA0_Split model buffer size =  9285.39 MiB\nload_tensors:  CUDA1_Split model buffer size =  9383.22 MiB\nload_tensors:        CUDA0 model buffer size =     1.32 MiB\nload_tensors:        CUDA1 model buffer size =     1.26 MiB\nload_tensors:   CPU_Mapped model buffer size =   417.30 MiB\n................................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 4\nllama_context: n_ctx         = 2048\nllama_context: n_ctx_per_seq = 512\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\nllama_context:  CUDA_Host  output buffer size =     2.32 MiB\nllama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1, padding = 32\nllama_kv_cache_unified:      CUDA0 KV buffer size =   264.00 MiB\nllama_kv_cache_unified:      CUDA1 KV buffer size =   248.00 MiB\nllama_kv_cache_unified: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\nllama_context:      CUDA0 compute buffer size =   312.00 MiB\nllama_context:      CUDA1 compute buffer size =   312.00 MiB\nllama_context:  CUDA_Host compute buffer size =    14.01 MiB\nllama_context: graph nodes  = 2438\nllama_context: graph splits = 3\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 2048\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n\nsystem_info: n_threads = 2 (n_threads_batch = 2) / 4 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \nperplexity: tokenizing the input ..\nperplexity: tokenization took 786.684 ms\nperplexity: calculating perplexity over 528 chunks, n_ctx=512, batch_size=2048, n_seq=4\nperplexity: 7.88 seconds per pass - ETA 17.32 minutes\n[1]nan,[2]nan,[3]nan,[4]nan,[5]nan,[6]nan,[7]nan,[8]nan,^C\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-05-04T11:24:21+00:00",
    "closed_at": null,
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13298/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13298"
  },
  {
    "number": 5134,
    "title": "Run on GPU",
    "body": "I compiled the main file according to the instructions on the official website below\r\n`mkdir build\r\ncd build\r\ncmake .. -DLLAMA_CUBLAS=ON\r\ncmake --build . --config Release`\r\n\r\nBut I found that the inference speed is 40t/s when using the following instructions\r\n`./build/bin/main -m /data/nwj/models/microsoft/phi-2/ggml-model-f32_q4_0.gguf -p \"Question: Write a python function to print the first n numbers in the fibonacci series\" `\r\n\r\nWhen I add -ngl 33, the speed is 10t/s\r\n`./build/bin/main -m /data/nwj/models/microsoft/phi-2/ggml-model-f32_q4_0.gguf -p \"Question: Write a python function to print the first n numbers in the fibonacci series\" -ngl 33`\r\n\r\nthe output is as follows:\r\n./build/bin/main -m /data/nwj/models/microsoft/phi-2/ggml-model-f32_q4_0.gguf -p \"Question: Write a python function to print the first n numbers in the fibonacci series\" -ngl 33\r\nLog start\r\nmain: build = 1761 (cb1e281)\r\nmain: built with cc (Ubuntu 10.5.0-1ubuntu1~22.04) 10.5.0 for x86_64-linux-gnu\r\nmain: seed  = 1706240310\r\nggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\r\nggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\r\nggml_init_cublas: found 8 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\r\n  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\r\n  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\r\n  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\r\n  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\r\n  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\r\n  Device 6: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\r\n  Device 7: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\r\nllama_model_loader: loaded meta data with 20 key-value pairs and 325 tensors from /data/nwj/models/microsoft/phi-2/ggml-model-f32_q4_0.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = phi2\r\nllama_model_loader: - kv   1:                               general.name str              = Phi2\r\nllama_model_loader: - kv   2:                        phi2.context_length u32              = 2048\r\nllama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2560\r\nllama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 10240\r\nllama_model_loader: - kv   5:                           phi2.block_count u32              = 32\r\nllama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32\r\nllama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010\r\nllama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = [\"\u0120 t\", \"\u0120 a\", \"h e\", \"i n\", \"r e\",...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256\r\nllama_model_loader: - kv  19:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  195 tensors\r\nllama_model_loader: - type q4_0:  129 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: mismatch in special tokens definition ( 910/51200 vs 944/51200 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = phi2\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 51200\r\nllm_load_print_meta: n_merges         = 50000\r\nllm_load_print_meta: n_ctx_train      = 2048\r\nllm_load_print_meta: n_embd           = 2560\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 32\r\nllm_load_print_meta: n_embd_head_k    = 80\r\nllm_load_print_meta: n_embd_head_v    = 80\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 2560\r\nllm_load_print_meta: n_embd_v_gqa     = 2560\r\nllm_load_print_meta: f_norm_eps       = 1.0e-05\r\nllm_load_print_meta: f_norm_rms_eps   = 0.0e+00\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 10240\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 2048\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: model type       = 3B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 2.78 B\r\nllm_load_print_meta: model size       = 1.49 GiB (4.61 BPW) \r\nllm_load_print_meta: general.name     = Phi2\r\nllm_load_print_meta: BOS token        = 50256 '<|endoftext|>'\r\nllm_load_print_meta: EOS token        = 50256 '<|endoftext|>'\r\nllm_load_print_meta: UNK token        = 50256 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 128 '\u00c4'\r\nllm_load_tensors: ggml ctx size       =    0.12 MiB\r\nllm_load_tensors: using CUDA for GPU acceleration\r\nllm_load_tensors: system memory used  =   70.44 MiB\r\nllm_load_tensors: VRAM used           = 1456.19 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\n...........................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init: VRAM kv self = 160.00 MB\r\nllama_new_context_with_model: KV self size  =  160.00 MiB, K (f16):   80.00 MiB, V (f16):   80.00 MiB\r\nllama_build_graph: non-view tensors processed: 774/774\r\nllama_new_context_with_model: compute buffer total size = 113.19 MiB\r\nllama_new_context_with_model: VRAM scratch buffer: 110.00 MiB\r\nllama_new_context_with_model: total VRAM used: 1726.19 MiB (model: 1456.19 MiB, context: 270.00 MiB)\r\n\r\nsystem_info: n_threads = 32 / 64 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \r\nsampling: \r\n        repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\r\n        top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\r\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\r\nsampling order: \r\nCFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \r\ngenerate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0\r\n\r\n\r\nQuestion: Write a python function to print the first n numbers in the fibonacci series using recursion. \r\nOutput: def fib(n):\r\n  if n < 2:\r\n    return 1\r\n  else:\r\n    return fib(n-1) + fib(n-2)\r\n [end of text]\r\n\r\nllama_print_timings:        load time =     498.70 ms\r\nllama_print_timings:      sample time =      17.50 ms /    50 runs   (    0.35 ms per token,  2857.80 tokens per second)\r\nllama_print_timings: prompt eval time =     116.97 ms /    18 tokens (    6.50 ms per token,   153.89 tokens per second)\r\nllama_print_timings:        eval time =    4804.61 ms /    49 runs   (   98.05 ms per token,    10.20 tokens per second)\r\nllama_print_timings:       total time =    4965.25 ms",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-01-26T03:39:07+00:00",
    "closed_at": "2024-04-02T01:08:20+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5134/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5134"
  },
  {
    "number": 6706,
    "title": "Idefics2 VLM Support",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x ] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x ] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\n\r\nRequesting support for HuggingFace's new [Idefics2 VLM](https://huggingface.co/blog/idefics2).\r\n\r\n# Motivation\r\n\r\n- First true open source VLM (Apache 2.0)\r\n- This 8B model offers comparable performance to Llava-1.6-34b and Apple's unreleased 30B MM1.\r\n- The HuggingFace team included a [fine-tuning notebook ](https://colab.research.google.com/drive/1NtcTgRbSBKN7pYD3Vdx1j9m8pt3fhFDB?usp=sharing#scrollTo=j-zKnRTZKZmI) which allows users to make specialized VLMs\r\n\r\n## Possible Implementation:\r\nHopefully this isn't exceedingly difficult to do. The base LLM is Mistral-7B-v0.1 and the image encoder is Google's [siglip-so400m-patch14-384](https://huggingface.co/google/siglip-so400m-patch14-384). Their projector is also a simple MLP similar to Llava.\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-16T16:29:38+00:00",
    "closed_at": "2024-06-18T01:07:01+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6706/reactions",
      "total_count": 31,
      "+1": 16,
      "-1": 0,
      "laugh": 0,
      "hooray": 8,
      "confused": 0,
      "heart": 7,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6706"
  },
  {
    "number": 6396,
    "title": "common: Gibberish results and/or crashes due to incorrect character encodings",
    "body": "As of ~~b2579~~ b2646, prompts (among other parameters) are internally stored as `std::string`s, which is basically glorified `std::vector<char>` and do not care or handle character encodings. This will not cause any problem since (as far as I can tell) llama.cpp treats all strings as in UTF-8, but care must be taken when taking strings from external sources.\r\n\r\nFor example, when parsing command-line arguments, `--prompt` (and maybe other arguments) gets stored directly as `params.prompt`:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/blob/c342d070c64a1ffe35d22c1b16b672e684a30297/common/common.cpp#L215-L222\r\n\r\nThis (somehow) works on Linux, but thanks to Windows' infinite wisdom `argv` is in ANSI codepage encoding, and will cause gibberish results or a crash to happen soon after since all other parts are expecting a UTF-8 string:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/blob/c342d070c64a1ffe35d22c1b16b672e684a30297/llama.cpp#L10974\r\n\r\nhttps://github.com/ggerganov/llama.cpp/blob/c342d070c64a1ffe35d22c1b16b672e684a30297/llama.cpp#L11161-L11162\r\n\r\nSample gibberish/crash log, both running `main.exe` from [llama-b2579-bin-win-avx2-x64.zip](https://github.com/ggerganov/llama.cpp/releases/tag/b2579):\r\n\r\n<details>\r\n\r\n```\r\nC:\\>llama-b2579-bin-win-avx2-x64\\main.exe -m \"C:\\mistral-7b-instruct-v0.2-q4_k_m.gguf\" -n 1024 -p \"\u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u5341\u5927\u5c71\u5cf0\uff1a\"\r\nLog start\r\nmain: build = 2579 (f7fc5f6c)\r\nmain: built with MSVC 19.38.33135.0 for x64\r\nmain: seed  = 1711790286\r\nllama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from C:\\mistral-7b-instruct-v0.2-q4_k_m.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\r\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 7.24 B\r\nllm_load_print_meta: model size       = 4.07 GiB (4.83 BPW)\r\nllm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.11 MiB\r\nllm_load_tensors:        CPU buffer size =  4165.37 MiB\r\n.................................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\r\nllama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 1\r\n\r\nsystem_info: n_threads = 8 / 16 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 |\r\nsampling:\r\n        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\r\n        top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\r\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\r\nsampling order:\r\nCFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature\r\n\r\ngenerate: n_ctx = 512, n_batch = 2048, n_predict = 1024, n_keep = 1\r\n\r\n\r\n \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\u07f5\ufffd\ufffd\ufffd\ufffd\ufffd\u027d\ufffd\u58fa\u4cab\u70d5\u79a5\ua1e4\uaaa4\ua864\ua8e0\ua86b\ua9b5\ua9b8\ua9b6\ua9cc\ua945\ua9d4\ua94c\ua927\ua9b1\ua9d4\ua957\ua92d\ua945\ua931\ua936\ua9d4\ua95b\ua933\ua93c\ua9cc\ua936\ua9b8\ua9b8\ua93c\ua936\ua938\ua920\ua9bc\ua933\ua92e\ua9ba\ua936\ua9d4\ua957\ua937\ua922\ua936\ua9d4\ua934\ua931\ua937\ua92e\ua9d4\ua94c\ua92a\ua920\ua937\ua92e\ua9d4\ua94c\ua92f\ua936\ua926\ua92d\ua93c\ua936\ua934\ua931\ua933\ua936\ua935\ua938\ua92f\ua933\ua926\ua92e\ua92f\ua936\ua933\ua932\ua936\ua934\ua92d\ua921\ua92c\ua91e\ua933\ua922\ua92e\ua93c\ua934\ua929\ua936\ua932\ua936\ua92e\ua936\ua935\ua92f\ua932\ua936\ua933\ua934\ua933\ua922\ua927\ua931\ua933\ua926\ua936\ua929\ua936\ua933\ua936\ua935\ua931\ua933\ua931\ua933\ua936\ua934\ua933\ua929\ua936\ua933\ua936\ua935\ua932\ua936\ua934\ua92e\ua933\ua936\ua933\ua936\ua935\ua931\ua933\ua931\ua933\ua936\ua934\ua933\ua929\ua936\ua933\ua936\ua935\ua932\ua936\ua934\ua92e\ua933\ua936\ua933\ua936\ua935\ua931\ua933\ua931\ua933\ua936\ua934\ua933\ua929\ua936\ua933\ua936\ua935\ua932\ua936\ua934\ua92e\ua933\ua936\ua933\ua936\ua935\ua931\ua933\ua931\ua933\ua936\ua934\ua933\ua929\ua936\ua933\ua936\ua935\ua932\ua936\ua934\ua92e\ua933\ua936\ua933\ua936\ua935\ua931\ua933\ua931\ua933\ua936\ua934\ua933\ua929\ua936\ua933\ua936\ua935\ua932\ua936\ua934\ua92e\ua933\ua936\ua933\ua936\ua935\ua931\ua933\ua931\ua933\ua936\ua934\ua933\ua929\ua936\ua933\ua936\ua935\ua932\ua936\ua934\ua92e\ua933\ua936\ua933\ua936\ua935\ua931\ua933\ua931\ua933\ua936\ua934\ua933\ua929\ua936\ua933\ua936\ua935\ua932\ua936\ua934\ua92e\ua933\ua936\ua933\ua936\ua935\ua931\ua933\ua931\ua933\ua936\ua934\ua933\ua929\ua936\ua933\ua936\ua935\ua932\ua936\ua934\ua92e\ua933\ua936\ua933\ua936\ua935\ua931\ua933\ua931\ua933\ua936\ua934\ua933\ua929\ua936\ua933\ua936\ua935\ua932\ua936\ua934\ua92e\ua933\ua936\ua933\ua936\ua935\ua931\ua933\ua931\ua933\ua936\ua934\ua933\ua929\ua936\ua933\ua936\ua935\r\n\r\nllama_print_timings:        load time =     715.64 ms\r\nllama_print_timings:      sample time =      27.37 ms /   966 runs   (    0.03 ms per token, 35288.96 tokens per second)\r\nllama_print_timings: prompt eval time =     991.13 ms /    24 tokens (   41.30 ms per token,    24.21 tokens per second)\r\nllama_print_timings:        eval time =   90356.08 ms /   965 runs   (   93.63 ms per token,    10.68 tokens per second)\r\nllama_print_timings:       total time =   91573.76 ms /   989 tokens\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n\r\n```\r\nMicrosoft (R) Windows Debugger Version 10.0.25921.1001 AMD64\r\nCopyright (c) Microsoft Corporation. All rights reserved.\r\n\r\nCommandLine: C:\\llama-b2579-bin-win-avx2-x64\\main.exe -m \"C:\\openbuddy-stablelm-3b-v13-q4_k_m.gguf\" -n 1024 -p \"\u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u5341\u5927\u5c71\u5cf0\uff1a\"\r\n\r\n************* Path validation summary **************\r\nResponse                         Time (ms)     Location\r\nDeferred                                       srv*C:\\Symbols*http://msdl.microsoft.com/download/symbols\r\nSymbol search path is: srv*C:\\Symbols*http://msdl.microsoft.com/download/symbols\r\nExecutable search path is: \r\n\r\n+------------------------------------------------------------------------+\r\n| This target supports Hardware-enforced Stack Protection. A HW based    |\r\n| \"Shadow Stack\" may be available to assist in debugging and analysis.   |\r\n| See aka.ms/userhsp for more info.                                      |\r\n|                                                                        |\r\n| dps @ssp                                                               |\r\n|                                                                        |\r\n+------------------------------------------------------------------------+\r\n\r\nModLoad: 00007ff7`b7e60000 00007ff7`b7eb6000   image00007ff7`b7e60000\r\nModLoad: 00007fff`b9b10000 00007fff`b9d26000   ntdll.dll\r\nModLoad: 00007fff`b9070000 00007fff`b9134000   C:\\Windows\\System32\\KERNEL32.DLL\r\nModLoad: 00007fff`b7370000 00007fff`b7717000   C:\\Windows\\System32\\KERNELBASE.dll\r\nModLoad: 00007fff`b7720000 00007fff`b7831000   C:\\Windows\\System32\\ucrtbase.dll\r\nModLoad: 00007fff`a6bb0000 00007fff`a6c3d000   C:\\Windows\\SYSTEM32\\MSVCP140.dll\r\nModLoad: 00007fff`a80c0000 00007fff`a80dd000   C:\\Windows\\SYSTEM32\\VCRUNTIME140.dll\r\nModLoad: 00007fff`1bd00000 00007fff`1bf36000   C:\\llama-b2579-bin-win-avx2-x64\\llama.dll\r\nModLoad: 00007fff`a6ba0000 00007fff`a6bac000   C:\\Windows\\SYSTEM32\\VCRUNTIME140_1.dll\r\n(daec.111f4): Break instruction exception - code 80000003 (first chance)\r\nntdll!LdrpDoDebuggerBreak+0x30:\r\n00007fff`b9beb744 cc              int     3\r\n0:000> g\r\n(daec.111f4): C++ EH exception - code e06d7363 (first chance)\r\n(daec.111f4): C++ EH exception - code e06d7363 (first chance)\r\n...\r\n(daec.111f4): C++ EH exception - code e06d7363 (first chance)\r\n(daec.111f4): C++ EH exception - code e06d7363 (!!! second chance !!!)\r\nKERNELBASE!RaiseException+0x6c:\r\n00007fff`b73d53ac 0f1f440000      nop     dword ptr [rax+rax]\r\n0:000> k\r\n # Child-SP          RetAddr               Call Site\r\n00 000000df`c80fb900 00007fff`a80c6ba7     KERNELBASE!RaiseException+0x6c\r\n01 000000df`c80fb9e0 00007fff`1bd936fe     VCRUNTIME140!_CxxThrowException+0x97 [D:\\a\\_work\\1\\s\\src\\vctools\\crt\\vcruntime\\src\\eh\\throw.cpp @ 82] \r\n02 000000df`c80fba40 00007fff`1bd93fcd     llama!unicode_byte_to_utf8+0x9fe\r\n03 000000df`c80fba90 00007fff`1bd3e5ca     llama!unicode_cpts_from_utf8+0x4d\r\n04 000000df`c80fbad0 00007fff`1bd858e9     llama!llm_tokenizer_bpe::bpe_gpt2_preprocess+0x12a\r\n05 000000df`c80fbd30 00007fff`1bd6a709     llama!llm_tokenizer_bpe::tokenize+0x59\r\n06 000000df`c80fc040 00007fff`1bd920ac     llama!llama_internal_get_tensor_map+0x9a09\r\n07 000000df`c80fc1e0 00007ff7`b7e8eeb2     llama!llama_tokenize+0x5c\r\n08 000000df`c80fc260 00007ff7`b7e8edec     main+0x2eeb2\r\n09 000000df`c80fc2e0 00007ff7`b7e6ac7e     main+0x2edec\r\n0a 000000df`c80fc330 00007ff7`b7e971fc     main+0xac7e\r\n0b 000000df`c80ffb20 00007fff`b908257d     main+0x371fc\r\n0c 000000df`c80ffb60 00007fff`b9b6aa48     KERNEL32!BaseThreadInitThunk+0x1d\r\n0d 000000df`c80ffb90 00000000`00000000     ntdll!RtlUserThreadStart+0x28\r\n```\r\n\r\n</details>\r\n\r\nThe same prompt works correctly with Linux builds:\r\n\r\n<details>\r\n\r\n```\r\n$ ./main -m ./openbuddy-stablelm-3b-v13-q4_k_m.gguf -n 1024 -p \"\u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u5341\u5927\u5c71\u5cf0\uff1a\"\r\nLog start\r\nmain: build = 2551 (e5b89a4)\r\nmain: built with cc (Debian 12.2.0-14) 12.2.0 for x86_64-linux-gnu\r\nmain: seed  = 1711790555\r\nllama_model_loader: loaded meta data with 19 key-value pairs and 356 tensors from ./openbuddy-stablelm-3b-v13-q4_k_m.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = stablelm\r\nllama_model_loader: - kv   1:                               general.name str              = openbuddy-stablelm-3b-v13\r\nllama_model_loader: - kv   2:                    stablelm.context_length u32              = 4096\r\nllama_model_loader: - kv   3:                  stablelm.embedding_length u32              = 2560\r\nllama_model_loader: - kv   4:                       stablelm.block_count u32              = 32\r\nllama_model_loader: - kv   5:               stablelm.feed_forward_length u32              = 6912\r\nllama_model_loader: - kv   6:              stablelm.rope.dimension_count u32              = 20\r\nllama_model_loader: - kv   7:              stablelm.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:             stablelm.use_parallel_residual bool             = true\r\nllama_model_loader: - kv   9:      stablelm.attention.layer_norm_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,52736]   = [\"<|endoftext|>\", \"<|padding|>\", \"!\",...\r\nllama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,52736]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,50009]   = [\"\u0120 \u0120\", \"\u0120 t\", \"\u0120 a\", \"h e\", \"i n...\r\nllama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 0\r\nllama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 0\r\nllama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  17:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  18:                          general.file_type u32              = 15\r\nllama_model_loader: - type  f32:  130 tensors\r\nllama_model_loader: - type q4_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: mismatch in special tokens definition ( 68/52736 vs 2484/52736 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = stablelm\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 52736\r\nllm_load_print_meta: n_merges         = 50009\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 2560\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 20\r\nllm_load_print_meta: n_embd_head_k    = 80\r\nllm_load_print_meta: n_embd_head_v    = 80\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 2560\r\nllm_load_print_meta: n_embd_v_gqa     = 2560\r\nllm_load_print_meta: f_norm_eps       = 1.0e-05\r\nllm_load_print_meta: f_norm_rms_eps   = 0.0e+00\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 6912\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 3B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 2.81 B\r\nllm_load_print_meta: model size       = 1.60 GiB (4.89 BPW)\r\nllm_load_print_meta: general.name     = openbuddy-stablelm-3b-v13\r\nllm_load_print_meta: BOS token        = 0 '<|endoftext|>'\r\nllm_load_print_meta: EOS token        = 0 '<|endoftext|>'\r\nllm_load_print_meta: UNK token        = 0 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 128 '\u00c4'\r\nllm_load_tensors: ggml ctx size =    0.14 MiB\r\nllm_load_tensors:        CPU buffer size =  1635.95 MiB\r\n............................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:        CPU KV buffer size =   160.00 MiB\r\nllama_new_context_with_model: KV self size  =  160.00 MiB, K (f16):   80.00 MiB, V (f16):   80.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.20 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =   113.00 MiB\r\nllama_new_context_with_model: graph nodes  = 1127\r\nllama_new_context_with_model: graph splits = 1\r\n\r\nsystem_info: n_threads = 8 / 16 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 |\r\nsampling:\r\n        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\r\n        top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\r\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\r\nsampling order:\r\nCFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature\r\ngenerate: n_ctx = 512, n_batch = 2048, n_predict = 1024, n_keep = 0\r\n\r\n\r\n\u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u5341\u5927\u5c71\u5cf0\uff1a\r\n1. \u963f\u5c14\u5351\u65af\u5c71\u8109\r\n2. \u559c\u9a6c\u62c9\u96c5\u5c71\u8109\r\n3. \u5927\u897f\u6d0b\u5c71\u8109\r\n4. \u65af\u582a\u7684\u7eb3\u7ef4\u4e9a\u5c71\u8109\r\n5. \u963f\u62c9\u65af\u52a0\u5c71\u8109\r\n6. \u963f\u5854\u5361\u5c71\u8109\r\n7. \u897f\u4f2f\u5229\u4e9a\u5c71\u8109\r\n8. \u559c\u9a6c\u62c9\u96c5\u5c71\u8109\r\n9. \u963f\u5c14\u5351\u65af\u5c71\u8109\r\n10. \u65af\u582a\u7684\u7eb3\u7ef4\u4e9a\u5c71\u8109\r\n [end of text]\r\n\r\nllama_print_timings:        load time =     957.15 ms\r\nllama_print_timings:      sample time =       3.72 ms /   140 runs   (    0.03 ms per token, 37604.08 tokens per second)\r\nllama_print_timings: prompt eval time =     226.91 ms /    14 tokens (   16.21 ms per token,    61.70 tokens per second)\r\nllama_print_timings:        eval time =    5656.50 ms /   139 runs   (   40.69 ms per token,    24.57 tokens per second)\r\nllama_print_timings:       total time =    5945.27 ms /   153 tokens\r\nLog end\r\n```\r\n\r\n</details>\r\n\r\n`chcp 65001` or compile with [`/utf-8`](https://learn.microsoft.com/en-us/cpp/build/reference/utf-8-set-source-and-executable-character-sets-to-utf-8?view=msvc-170) does not fix this issue.\r\n\r\nFor reference, I'm running Windows 11 x64 (10.0.22631.3374), system language is Simplified Chinese, and tested on codepages 437 (ANSI United States), 936 (ANSI Simplified Chinese) and 65001 (UTF-8).",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-30T09:32:12+00:00",
    "closed_at": "2024-05-28T02:13:07+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6396/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6396"
  },
  {
    "number": 7106,
    "title": "Server completion streaming returns special tokens as empty strings in chunks",
    "body": "Version: b2794.\r\nModel: Meta-Llama-3-8B-Instruct-Q8_0.gguf (updated)\r\nPrompt: \"<|start_header_id|>user<|end_header_id|>How much is 12 plus 19?<|eot_id|>\"\r\n\r\nWhen I run the server and send a completion request with streaming, in the verbose logs I see that the server generates the \"<|start_header_id|>\", \"assistant\" and \"<|end_header_id|>\", followed by \"\\n\\n12 + 19 = 31\".\r\n\r\nHowever, the streaming chunks sent by server for <|start_header_id|> and <|end_header_id|> have empty strings as `content` in `data`.\r\n\r\nI couldn't find a config parameter either in the server or in the request that could change this behavior.",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-06T18:27:10+00:00",
    "closed_at": "2024-07-24T01:06:53+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7106/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7106"
  },
  {
    "number": 12692,
    "title": "Eval bug: Qwerky 72B (rwkv6qwen2) failed to load with `--split-mode row` option",
    "body": "### Name and Version\n\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: yes\nggml_cuda_init: found 6 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\n  Device 1: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\n  Device 2: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\n  Device 3: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\n  Device 4: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\n  Device 5: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\nversion: 5008 (4172aea2)\nbuilt with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\nThreadripper 3960X + 6x RTX 4060 Ti 16GB\n\n### Models\n\nOriginal model: [featherless-ai/Qwerky-72B](https://huggingface.co/featherless-ai/Qwerky-72B)\nQuantized model using latest llama.cpp: https://huggingface.co/exxocism/featherless-ai_Qwerky-72B-GGUF\nQ4_K_M\n\n### Problem description & steps to reproduce\n\n- without `--split-mode row` it runs fine. (but slow)\n- with `--split-mode row`, we have the following error:\n\n```\nllama-server --model \"./Qwerky-72B-Q4_K_M.gguf\" -ngl 81 --split-mode row\n\nggml-backend.cpp:748: pre-allocated tensor (blk.0.time_mix_w2.weight (reshaped)) in a buffer (CUDA0_Split) that cannot run the operation (RESHAPE)\n```\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n$ llama-server --model \"./Qwerky-72B-Q4_K_M.gguf\" -ngl 81 --split-mode row\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: yes\nggml_cuda_init: found 6 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\n  Device 1: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\n  Device 2: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\n  Device 3: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\n  Device 4: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\n  Device 5: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\nbuild: 5008 (4172aea2) with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\nsystem info: n_threads = 24, n_threads_batch = 24, total_threads = 48\n\nsystem_info: n_threads = 24 (n_threads_batch = 24) / 48 | CUDA : ARCHS = 890 | FORCE_CUBLAS = 1 | F16 = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 512 | FA_ALL_QUANTS = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |\n\nmain: binding port with default address family\nmain: HTTP server is listening, hostname: 127.0.0.1, port: 8080, http threads: 47\nmain: loading model\nsrv    load_model: loading model './Qwerky-72B-Q4_K_M.gguf'\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4060 Ti) - 15818 MiB free\nllama_model_load_from_file_impl: using device CUDA1 (NVIDIA GeForce RTX 4060 Ti) - 15818 MiB free\nllama_model_load_from_file_impl: using device CUDA2 (NVIDIA GeForce RTX 4060 Ti) - 15748 MiB free\nllama_model_load_from_file_impl: using device CUDA3 (NVIDIA GeForce RTX 4060 Ti) - 15818 MiB free\nllama_model_load_from_file_impl: using device CUDA4 (NVIDIA GeForce RTX 4060 Ti) - 15818 MiB free\nllama_model_load_from_file_impl: using device CUDA5 (NVIDIA GeForce RTX 4060 Ti) - 15818 MiB free\nllama_model_loader: loaded meta data with 30 key-value pairs and 1603 tensors from ./Qwerky-72B-Q4_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = rwkv6qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwerky 72B\nllama_model_loader: - kv   3:                           general.basename str              = Qwerky\nllama_model_loader: - kv   4:                         general.size_label str              = 72B\nllama_model_loader: - kv   5:                            general.license str              = other\nllama_model_loader: - kv   6:                       general.license.name str              = tongyi-qianwen\nllama_model_loader: - kv   7:                  rwkv6qwen2.context_length u32              = 1048576\nllama_model_loader: - kv   8:                rwkv6qwen2.embedding_length u32              = 8192\nllama_model_loader: - kv   9:                     rwkv6qwen2.block_count u32              = 80\nllama_model_loader: - kv  10:                   rwkv6qwen2.wkv.head_size u32              = 128\nllama_model_loader: - kv  11:              rwkv6qwen2.time_mix_extra_dim u32              = 160\nllama_model_loader: - kv  12:            rwkv6qwen2.time_decay_extra_dim u32              = 128\nllama_model_loader: - kv  13:             rwkv6qwen2.feed_forward_length u32              = 29568\nllama_model_loader: - kv  14: rwkv6qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  15:               rwkv6qwen2.token_shift_count u32              = 1\nllama_model_loader: - kv  16:         rwkv6qwen2.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  17:            rwkv6qwen2.attention.head_count u32              = 0\nllama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\nllama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  27:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\nllama_model_loader: - kv  29:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  961 tensors\nllama_model_loader: - type q5_0:   40 tensors\nllama_model_loader: - type q8_0:   40 tensors\nllama_model_loader: - type q4_K:  561 tensors\nllama_model_loader: - type q6_K:    1 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 51.39 GiB (5.57 BPW)\nload: special tokens cache size = 22\nload: token to piece cache size = 0.9310 MB\nprint_info: arch             = rwkv6qwen2\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 1048576\nprint_info: n_embd           = 8192\nprint_info: n_layer          = 80\nprint_info: n_head           = 0\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 0\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 0\nprint_info: n_embd_head_v    = 0\nprint_info: n_gqa            = 0\nprint_info: n_embd_k_gqa     = 0\nprint_info: n_embd_v_gqa     = 0\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 29568\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = -1\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 1048576\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = ?B\nprint_info: model params     = 79.30 B\nprint_info: general.name     = Qwerky 72B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 152064\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 '\u010a'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 80 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 81/81 layers to GPU\nload_tensors:  CUDA0_Split model buffer size =  9208.75 MiB\nload_tensors:  CUDA1_Split model buffer size =  8689.00 MiB\nload_tensors:  CUDA2_Split model buffer size =  8012.67 MiB\nload_tensors:  CUDA3_Split model buffer size =  8012.67 MiB\nload_tensors:  CUDA4_Split model buffer size =  8689.00 MiB\nload_tensors:  CUDA5_Split model buffer size =  9051.87 MiB\nload_tensors:        CUDA0 model buffer size =    60.48 MiB\nload_tensors:        CUDA1 model buffer size =    60.48 MiB\nload_tensors:        CUDA2 model buffer size =    56.16 MiB\nload_tensors:        CUDA3 model buffer size =    56.16 MiB\nload_tensors:        CUDA4 model buffer size =    60.48 MiB\nload_tensors:        CUDA5 model buffer size =     3.88 MiB\nload_tensors:   CPU_Mapped model buffer size =   668.25 MiB\n..................................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 10000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (1048576) -- the full capacity of the model will not be utilized\nllama_context:  CUDA_Host  output buffer size =     0.58 MiB\ninit: kv_size = 1, offload = 1, type_k = 'f32', type_v = 'f32', n_layer = 80, can_shift = 0\ninit:      CUDA0 KV buffer size =    56.44 MiB\ninit:      CUDA1 KV buffer size =    56.44 MiB\ninit:      CUDA2 KV buffer size =    52.41 MiB\ninit:      CUDA3 KV buffer size =    52.41 MiB\ninit:      CUDA4 KV buffer size =    56.44 MiB\ninit:      CUDA5 KV buffer size =    48.38 MiB\nllama_context: KV self size  =  322.50 MiB, K (f32):    2.50 MiB, V (f32):  320.00 MiB\nllama.cpp/ggml/src/ggml-backend.cpp:748: pre-allocated tensor (blk.0.time_mix_w2.weight (reshaped)) in a buffer (CUDA0_Split) that cannot run the operation (RESHAPE)\nCould not attach to process.  If your uid matches the uid of the target\nprocess, check the setting of /proc/sys/kernel/yama/ptrace_scope, or try\nagain as the root user.  For more details, see /etc/sysctl.d/10-ptrace.conf\nptrace: Operation not permitted.\nNo stack.\nThe program is not being run.\nAborted (core dumped)\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-04-01T13:04:59+00:00",
    "closed_at": "2025-05-16T01:07:49+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12692/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12692"
  },
  {
    "number": 5822,
    "title": "How to write a chat template for llama.cpp server?",
    "body": "Which variables do i have to use in jina? How to define user-name and system-name?\r\n\r\nhttps://github.com/ggerganov/llama.cpp/issues/5766#issuecomment-1973708563",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-01T19:04:28+00:00",
    "closed_at": "2024-04-16T01:06:29+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5822/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5822"
  },
  {
    "number": 9748,
    "title": "Feature Request: Anti-slop / fine tuning of a model output in realtime / on the fly for output quality enhancement.",
    "body": "### Prerequisites\r\n\r\n- [X] I am running the latest code. Mention the version if possible as well.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\r\n\r\n### Feature Description\r\n\r\nBasically this enhancement fixes model generation on the fly so to speak, and drastically improves the performance of any model\r\nfor specific tasks.\r\n\r\nAlthough this is more involved than the \"XTC\" enhancement, this one is far stronger and will allow users to control the quality of generation of any model at the root level customized to their use case(s).\r\n\r\nThe occurs on the word, phrase level rather than per token level... roughly it forces the model to regenerate token(s) when generated token(s) match word(s)/phrase(s) in a json file. Like a live fire \"regen\" forcing the model to generate better/stronger output during generation.\r\n\r\nRoughly is a live \"proof\" reading / editing during generation... but it seems to do more than that based on examples at the project website.\r\n\r\nThis may reduce T/S but increase quality output.\r\nOn smaller models this would allow them to perform well outside their \"size\" so to speak.\r\n\r\nIn some ways enhancement almost acts like \"live fine tuning\" of a model.\r\n\r\nThis is the project online for this enhancements:\r\nhttps://github.com/sam-paech/antislop-sampler\r\n\r\nHere is a detailed test at EQBENCH using this enhancement on a Gemma 9B top rated model:\r\nhttps://eqbench.com/results/creative-writing-v2/Gemma-2-Ataraxy-v2-9B%20[antislop].txt\r\n\r\n### Motivation\r\n\r\nDrastic improvement of output quality on the fly in real time, tunable by the use / use case.\r\nSmaller and mid size model improvement across the board.\r\nLarger models would also benefit - could result in drastic leap in coherence / generation and maybe logic solving improvement.\r\n\r\nThis could match or exceed closed source model performance depending on implementation.\r\nThe quality enhancement at small / mid-sized models can not be overstated. \r\n\r\nRoughly allows fine tuning of a model at the user / case level.\r\n\r\nOn the user level: Far fewer (if any) \"regens\" to get good output quality.\r\nThat all by itself is a game changer.\r\nThis would drastically improve user / end user experience across the board.\r\n\r\n### Possible Implementation\r\n\r\nBased on the project online and noted ; one or more \"user defined\" text files / json for specific use case(s).\r\nThis could be a \"config.json\" at the \"source level\" before quanting and/or post use / post quant.\r\n\r\nOption to download hugging face, one or more of these files to augment usage of model(s) much like a dataset is used to fine tune a model.\r\n\r\nI would suggest implementing this as a minimum to allow one or more files to be selected at the quantize (similar to embedding \"config.json\") in the GGUF step and/or inference step(s).\r\n\r\nIE:\r\n--optimize creative.json\r\n\r\nOR when you \"run\" the model:\r\n\r\n--enhance creative.json  \r\n\r\nWhich would then run \"creative.json\" live as per project noted here during output generation:\r\nhttps://github.com/sam-paech/antislop-sampler",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-05T02:44:16+00:00",
    "closed_at": "2024-11-19T01:17:33+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9748/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9748"
  },
  {
    "number": 12623,
    "title": "Misc. bug:",
    "body": "### Name and Version\n\ngit clone https://github.com/ggerganov/llama.cpp\nmake GGML_CUDA=1\n\nI llama.cpp build info: \nI UNAME_S:   Linux\nI UNAME_P:   x86_64\nI UNAME_M:   x86_64\nI CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -pthread -march=native -mtune=native \nI CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -march=native -mtune=native \nI NVCCFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread    -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation -march=native -mtune=native \"\nI LDFLAGS:    \nI CC:        cc (GCC) 7.3.1 20180303 (Red Hat 7.3.1-5)\nI CXX:       g++ (GCC) 7.3.1 20180303 (Red Hat 7.3.1-5)\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nDocumentation/Github\n\n### Command line\n\n```shell\ncd llama.app\npython  convert-hf-to-gguf.py  /data/qwen2.5/saves/export  --outtype f16 --outfile /data/qwen2.5/saves/gguf/qwen2.5-1.5b-zpert.gguf\n```\n\n### Problem description & steps to reproduce\n\nexecute the following commands, it raised an exception:\ncd llama.app\npython  convert-hf-to-gguf.py  /data/qwen2.5/saves/export  --outtype f16 --outfile /data/qwen2.5/saves/gguf/qwen2.5-1.5b-zpert.gguf\n\nexceptiion:\n\nLoading model: export\ngguf: This GGUF file is for Little Endian only\nSet model parameters\nSet model tokenizer\ngguf: Adding 151387 merge(s).\ngguf: Setting special token type eos to 151645\ngguf: Setting special token type pad to 151643\ngguf: Setting special token type bos to 151643\ngguf: Setting add_bos_token to False\ngguf: Setting chat_template to {%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n\nExporting model to '/data/qwen2.5/saves/gguf/qwen2.5-1.5b-zpert.gguf'\ngguf: loading model part 'model.safetensors'\ntoken_embd.weight, n_dims = 2, torch.bfloat16 --> float16\nblk.0.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\nblk.0.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\nblk.0.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\nblk.0.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\nblk.0.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\nCan not map tensor 'model.layers.0.self_attn.k_proj.bias'\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-28T09:57:55+00:00",
    "closed_at": "2025-05-12T01:07:58+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12623/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12623"
  },
  {
    "number": 11819,
    "title": "Misc. bug: llama-server does not print model loading errors by default (log level misconfigured?)",
    "body": "### Name and Version\n\n```\nbuild: 4205 (c6bc7395) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin23.6.0\n```\n\n### Operating systems\n\nMac\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Problem description & steps to reproduce\n\nWhen using a model with e.g. an incompatible pre-tokenizer, the loading error isn't shown by default; `llama-server` seems to just quit.\n\n```\n$ ./llama-server -m model-CoT-Q4_K_M.gguf\nbuild: 4205 (c6bc7395) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin23.6.0\n[...]\nmain: HTTP server is listening, hostname: 127.0.0.1, port: 8080, http threads: 11\nmain: loading model\nsrv    load_model: loading model 'model-CoT-Q4_K_M.gguf'\nllama_load_model_from_file: using device Metal (Apple M2 Max) - 49151 MiB free\nllama_model_loader: loaded meta data with 25 key-value pairs and 579 tensors \n[...]\nllama_model_loader: - type  f32:  241 tensors\nllama_model_loader: - type q4_K:  289 tensors\nllama_model_loader: - type q6_K:   49 tensors\n$\n```\n(The exit code is 1.)\nAdding `-v` seems to bump up the logging level enough for errors to be shown:\n```\n$ ./llama-server -v -m model-CoT-Q4_K_M.gguf\n[...]\nllama_model_loader: - type q4_K:  289 tensors\nllama_model_loader: - type q6_K:   49 tensors\nllama_model_load: error loading model: error loading model vocabulary: unknown pre-tokenizer type: 'deepseek-r1-qwen'\nllama_load_model_from_file: failed to load model\ncommon_init_from_params: failed to load model 'model-CoT-Q4_K_M.gguf'\nsrv    load_model: failed to load model, 'model-CoT-Q4_K_M.gguf'\nmain: exiting due to model loading error\n```\n\nI think errors should be shown by default.\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-12T09:18:11+00:00",
    "closed_at": "2025-03-29T01:07:39+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11819/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11819"
  }
]
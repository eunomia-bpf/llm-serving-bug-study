[
  {
    "number": 13565,
    "title": "Misc. bug: HIP backend performs poorly on AMD Ryzen AI MAX 395 (Strix Halo gfx1151)",
    "body": "### Name and Version\n\n```\n\u276f build/bin/llama-cli --version\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Radeon Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32\nversion: 5392 (c753d7be)\nbuilt with cc (GCC) 15.0.1 20250418 (Red Hat 15.0.1-0) for x86_64-redhat-linux\n```\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-bench\n\n### Command line\n\n```shell\nllama.cpp-cpu/build/bin/llama-bench -m ~/models/llama-2-7b.Q4_0.gguf\nllama.cpp-vulkan/build/bin/llama-bench -m ~/models/llama-2-7b.Q4_0.gguf\nllama.cpp-hip/build/bin/llama-bench -m ~/models/llama-2-7b.Q4_0.gguf\n```\n\n### Problem description & steps to reproduce\n\nRecently I've been testing a Strix Halo (gfx1151) system and was a bit surprised by how poorly the HIP backend ran. All tests were run with `llama-bench` built on HEAD (b5392) with the standard [TheBloke/Llama-2-7B-GGUF](https://huggingface.co/TheBloke/Llama-2-7B-GGUF) (Q4_0):\n\n| Backend|pp512 (t/s)|tg128 (t/s)|\n|:-|:-|:-|\n|CPU|304.42 \u00b1 2.05|28.65 \u00b1 0.03|\n|HIP|348.62 \u00b1 0.35|48.70 \u00b1 0.02|\n|Vulkan|881.38 \u00b1 2.11|52.82 \u00b1 0.04|\n\nThe HIP version performs far below what you'd expect in terms of tok/TFLOPS efficiency for prompt processing vs other RDNA3 architectures:\n\n- `gfx1103` Radeon 780M iGPU gets 14.51 tok/TFLOP. At that efficiency you'd expect the about 850 tok/s that the Vulkan backend delivers.\n- `gfx1100` Radeon 7900 XTX gets 25.12 tok/TFLOP. At that efficiency you'd expect almost 1500 tok/s, almost double what the Vulkan backend delivers, and >4X what the current HIP backend delivers.\n- HIP pp512 barely beats out CPU backend numbers. I don't have an explanation for this.\n- Just for a reference of how bad the HIP performance is, an 18CU M3 Pro has \\~12.8 FP16 TFLOPS (4.6X less compute than Strix Halo) and delivers about the same pp512. Lunar Lake Arc 140V has 32 FP16 TFLOPS (almost 1/2 Strix Halo) and has a pp512 of 657 tok/s (1.9X faster)\n- With the Vulkan backend pp512 is about the same as an M4 Max and tg128 is about equivalent to an M4 Pro\n\nWith monitoring, I've confirmed that both HIP and Vulkan reach the max graphics clock. This is a system running Linux 6.15.0-0.rc3, so should be up to date with the latest AMDGPU drivers.\n\nThese results are from a standard `llama-bench` run. I've tried `-fa 1` and a rocWMMA build but they don't make much difference so excluded for clarity.\n\nOne interesting observation that may help track a potential regression, when I compile w/ gfx1100 support and run with HSA_OVERRIDE_GFX_VERSION=11.0.0 , the pp512 basically doubles to 598.84 \u00b1 1.41 (this eventually leads to MES/kernel errors so obviously is not recommended for use, just an interesting observation that might help in tracking down the issue).\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "performance",
      "AMD GPU"
    ],
    "state": "closed",
    "created_at": "2025-05-15T14:12:58+00:00",
    "closed_at": "2025-05-18T16:49:49+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13565/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13565"
  },
  {
    "number": 11949,
    "title": "Misc. bug: hipGraph causes a crash in hipGraphDestroy",
    "body": "First encountered when testing https://github.com/ggml-org/llama.cpp/pull/11867, but this is a problem in master too. Debugged to a bug in rocm-clr: https://github.com/ROCm/clr/issues/138\n\nThis issue tracks that currently non-defaults builds with GGML_HIP_GRAPHS=On are unreliable and will be closed when the corresponding upstream bug is addressed.",
    "labels": [
      "AMD GPU"
    ],
    "state": "closed",
    "created_at": "2025-02-18T22:54:45+00:00",
    "closed_at": "2025-03-02T20:49:39+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11949/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11949"
  },
  {
    "number": 8207,
    "title": "Bug: Failed to allocate memory on the 2nd GPU for loading large model",
    "body": "### What happened?\n\nI am running:\r\n* 256 GB RAM\r\n* 2GPUs: AMD RX 7900 XTX x 2\r\n* ROCm 6.1.3\r\n\r\nI compiled from source with:\r\n\r\n> make GGML_HIPBLAS=1 AMDGPU_TARGETS=gfx1100 -j$(lscpu | grep '^Core(s)' | awk '{print $NF}')\r\n\r\nWhen I run large files like, mixtral_8x22b.gguf or command-r-plus_104b.gguf, I encountered errors:\r\n\r\n> ./llama-cli -m ../../ollama_gguf/gguf/command-r-plus.gguf -p \"What is machine learning?\" -ngl 999\r\n\r\n```\r\nggml_cuda_init: found 2 ROCm devices:\r\n  Device 0: Radeon RX 7900 XTX, compute capability 11.0, VMM: no\r\n  Device 1: Radeon RX 7900 XTX, compute capability 11.0, VMM: no\r\nllm_load_tensors: ggml ctx size =    0.88 MiB\r\nggml_backend_cuda_buffer_type_alloc_buffer: allocating 27846.97 MiB on device 0: cudaMalloc failed: out of memory\r\nllama_model_load: error loading model: unable to allocate backend buffer\r\nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params: error: failed to load model '../../ollama_gguf/gguf/command-r-plus.gguf'\r\nmain: error: unable to load model\r\n```\r\n\r\nIt looks like memory is only allocated to the first GPU, the second is ignored.\r\n\r\nI can load and run both `mixtral_8x22b.gguf` and `command-r-plus_104b.gguf` with `ollama` on the same machine. \r\n\n\n### Name and Version\n\nversion: 3265 (72272b83)\r\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\nggml_cuda_init: found 2 ROCm devices:\r\n  Device 0: Radeon RX 7900 XTX, compute capability 11.0, VMM: no\r\n  Device 1: Radeon RX 7900 XTX, compute capability 11.0, VMM: no\r\nllm_load_tensors: ggml ctx size =    0.88 MiB\r\nggml_backend_cuda_buffer_type_alloc_buffer: allocating 27846.97 MiB on device 0: cudaMalloc failed: out of memory\r\nllama_model_load: error loading model: unable to allocate backend buffer\r\nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params: error: failed to load model '../../ollama_gguf/gguf/command-r-plus.gguf'\r\nmain: error: unable to load model\r\n```\n```\n",
    "labels": [
      "AMD GPU",
      "bug-unconfirmed",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-06-29T12:05:27+00:00",
    "closed_at": "2024-06-29T18:06:28+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8207/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8207"
  },
  {
    "number": 7829,
    "title": "iGPU offloading Bug: Memory access fault by GPU node-1 (appeared once only)",
    "body": "### What happened?\n\nI am comparing inference with and without AMD iGPU offloading with ROCm.\r\n\r\nThe setup is documented at https://github.com/eliranwong/MultiAMDGPU_AIDev_Ubuntu/blob/main/igpu_only.md#compare-cpu-vs-openblas-vs-rocm-vs-rocmigpu-offloading\r\n\r\nThe result shows that AMD iGPU offloading with ROCm runs roughly 1.5x faster.\r\n\r\nIt is interesting to note that the first time I tried to run the following command:\r\n\r\n> ./main -t $(lscpu | grep '^Core(s)' | awk '{print $NF}') --temp 0 -m '/home/eliran/freegenius/LLMs/gguf/mistral.gguf' -p \"What is machine learning?\" -ngl 33\r\n\r\nI got the following error:\r\n\r\n```\r\nMemory access fault by GPU node-1 (Agent handle: 0x613061b881f0) on address 0x9000. Reason: Page not present or supervisor privilege.\r\nAborted (core dumped)\r\n```\r\n\r\nHowever, it appeared once only.  Further inference with the same command runs smoothly.  It is not a practical problem, as it happened once only.  All later inferences runs without an issue.\n\n### Name and Version\n\nversion: 3108 (da799b41)\r\n\r\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\r\nggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\r\nggml_cuda_init: found 1 ROCm devices:\r\n  Device 0: AMD Radeon Graphics, compute capability 10.3, VMM: no\r\nllm_load_tensors: ggml ctx size =    0.30 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:      ROCm0 buffer size =  3847.55 MiB\r\nllm_load_tensors:        CPU buffer size =    70.31 MiB\r\n..................................................................................................\r\nllama_new_context_with_model: n_ctx      = 32768\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:      ROCm0 KV buffer size =  4096.00 MiB\r\nllama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\r\nllama_new_context_with_model:  ROCm_Host  output buffer size =     0.12 MiB\r\nllama_new_context_with_model:      ROCm0 compute buffer size =  2144.00 MiB\r\nllama_new_context_with_model:  ROCm_Host compute buffer size =    72.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 2\r\nMemory access fault by GPU node-1 (Agent handle: 0x613061b881f0) on address 0x9000. Reason: Page not present or supervisor privilege.\r\nAborted (core dumped)\r\n```\n```\n",
    "labels": [
      "AMD GPU",
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-06-08T06:04:54+00:00",
    "closed_at": "2024-07-23T01:06:44+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7829/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7829"
  },
  {
    "number": 3705,
    "title": "special token handling sometimes produces garbage output with AMD ROCM/HIP",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nRunning models with special tokens (e.g. ChatML) with GPU offload via HIPBLAS should produce output similar to running pure CPU\r\n\r\n# Current Behavior\r\n\r\nInstead running with -ngl 35 and -ngl 32 causes the model to fill the context with hashes \"#\"\r\n\r\n# Environment and Context\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n`$ lscpu`\r\n\r\n```\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         48 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  16\r\n  On-line CPU(s) list:   0-15\r\nVendor ID:               AuthenticAMD\r\n  Model name:            AMD Ryzen 7 5700X 8-Core Processor\r\n    CPU family:          25\r\n    Model:               33\r\n    Thread(s) per core:  2\r\n    Core(s) per socket:  8\r\n    Socket(s):           1\r\n    Stepping:            2\r\n    Frequency boost:     enabled\r\n    CPU(s) scaling MHz:  55%\r\n    CPU max MHz:         4661.7178\r\n    CPU min MHz:         2200.0000\r\n    BogoMIPS:            6790.71\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall \r\n                         nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl\r\n                          pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp\r\n                         _legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_co\r\n                         re perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase \r\n                         bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsa\r\n                         ves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv s\r\n                         vm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload\r\n                          vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\nVirtualization features: \r\n  Virtualization:        AMD-V\r\nCaches (sum of all):     \r\n  L1d:                   256 KiB (8 instances)\r\n  L1i:                   256 KiB (8 instances)\r\n  L2:                    4 MiB (8 instances)\r\n  L3:                    32 MiB (1 instance)\r\nNUMA:                    \r\n  NUMA node(s):          1\r\n  NUMA node0 CPU(s):     0-15\r\nVulnerabilities:         \r\n  Gather data sampling:  Not affected\r\n  Itlb multihit:         Not affected\r\n  L1tf:                  Not affected\r\n  Mds:                   Not affected\r\n  Meltdown:              Not affected\r\n  Mmio stale data:       Not affected\r\n  Retbleed:              Not affected\r\n  Spec rstack overflow:  Mitigation; safe RET, no microcode\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\n  Spectre v2:            Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\n  Srbds:                 Not affected\r\n  Tsx async abort:       Not affected\r\n\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`$ uname -a`\r\n\r\nArtix Linux (Arch-based):\r\n\r\n```\r\nLinux art 6.5.7-artix1-1 #1 SMP PREEMPT_DYNAMIC Sun, 15 Oct 2023 22:13:26 +0000 x86_64 GNU/Linux\r\n```\r\n\r\n`$ pacman -Qi rocm-hip-sdk`\r\n\r\n```\r\nName            : rocm-hip-sdk\r\nVersion         : 5.6.1-1\r\nDescription     : Develop applications using HIP and libraries for AMD platforms\r\nArchitecture    : x86_64\r\nURL             : https://rocm.docs.amd.com/\r\nLicenses        : custom:None\r\nGroups          : None\r\nProvides        : None\r\nDepends On      : rocm-core  rocm-hip-libraries  rocm-llvm  rocm-hip-runtime  hipblas  hipcub  hipfft  hipsparse  hipsolver\r\n                  miopen-hip  rccl  rocalution  rocblas  rocfft  rocprim  rocrand  rocsolver  rocsparse  rocthrust\r\nOptional Deps   : None\r\nRequired By     : rocm-ml-sdk\r\nOptional For    : None\r\nConflicts With  : None\r\nReplaces        : None\r\nInstalled Size  : 0.00 B\r\nPackager        : Torsten Ke\u00dfler <tpkessler@archlinux.org>\r\nBuild Date      : Tue 05 Sep 2023 22:59:50\r\nInstall Date    : Sun 24 Sep 2023 09:24:16\r\nInstall Reason  : Explicitly installed\r\nInstall Script  : No\r\nValidated By    : Signature\r\n```\r\n\r\n```\r\n$ python3 --version\r\nPython 3.11.5\r\n\r\n$ make --version\r\nGNU Make 4.4.1\r\nBuilt for x86_64-pc-linux-gnu\r\n\r\n$ g++ --version\r\ng++ (GCC) 13.2.1 20230801\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nBuilding with AMD HIPBlas, and enabling gpu offload (-ngl 32 and -ngl 35 tested) and using models with special tokenizers will cause the following\r\n\r\nCurrent model's I've tested that this affects:\r\n\r\n- dolphin-2.1-mistral-7b.Q5_K_M.gguf\r\n- openhermes-2-mistral-7b.Q5_K_M.gguf\r\n\r\n# Failure Logs\r\n\r\nExample running openhermes-2-mistral-7b.Q5_K_M.gguf, but happens with dolphin 2.1 as well:\r\n\r\n```\r\n./main -e -m mistral/openhermes-2-mistral-7b.Q5_K_M.gguf --temp 0.7 -c 4096 --repeat_penalty 1.1 --color -p \"<|im_start|>user\\nExplain how Linux can win in the desktop space Apple and Microsoft invest more money into their desktop systems.<|im_end|>\\n<|im_start|>assistant\\n\"\r\n```\r\n\r\noutput:\r\n\r\n```\r\nuser\r\nExplain how Linux can win in the desktop space Apple and Microsoft invest more money into their desktop systems.\r\nassistant\r\n######################################################################################################################################\r\n```\r\n\r\nExample environment info:\r\n```\r\nllama.cpp$ git log | head -1\r\ncommit 465219b9143ac01db0990bbcb0a081ef72ec2008\r\n\r\n$ sha256sum\r\n11b6d5eff77485fe39f54e1612cc42f82b5fd4d9d5473be683e7a5c09ccfdbc1  openhermes-2-mistral-7b.Q5_K_M.gguf\r\n786b79cf8fb54ed125ee17bfcf66cb3b3e81fbbccd770406bdc17b1ab8752a2b  dolphin-2.1-mistral-7b.Q5_K_M.gguf\r\n```",
    "labels": [
      "generation quality",
      "AMD GPU",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-10-21T02:19:36+00:00",
    "closed_at": "2024-04-04T01:07:44+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3705/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3705"
  },
  {
    "number": 3451,
    "title": "[bug] ROCm segfault when running multi-gpu inference.",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nExpected Tensor split to leverage multi gpus.\r\n\r\n# Current Behavior\r\n\r\nSegfault after model loading when using multi-gpu. Correct inference when using either GPU(two vega-56s  installed) and HIP_VISIBLE_DEVICES to force single GPU inference.\r\n\r\n# Environment and Context\r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\nRyzen 1700x\r\nVega-56 8G*2\r\n\r\n* Operating System (Ubuntu LTS):\r\n\r\nLinux jerryxu-Inspiron-5675 6.2.0-33-generic #33~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Sep  7 10:33:52 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version\r\nPython 3.10.13\r\n$ make --version\r\nGNU Make 4.3\r\n$ g++ --version\r\ng++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\n```\r\n\r\n# Failure Information (for bugs)\r\nSee logs.\r\n\r\n# Steps to Reproduce.\r\n\r\n1. Compile llama.cpp with ROCm\r\n2. run any model with tensor split (tried 2 quantizations of 7B and 13B)\r\n3. get segfault\r\n\r\n# Failure Logs\r\n\r\nllama.cpp log:\r\n```\r\nLog start\r\nmain: build = 1310 (1c84003)\r\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\r\nmain: seed  = 1696299120\r\nggml_init_cublas: found 2 ROCm devices:\r\n  Device 0: Radeon RX Vega, compute capability 9.0\r\n  Device 1: Radeon RX Vega, compute capability 9.0\r\n...................................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init: offloading v cache to GPU\r\nllama_kv_cache_init: offloading k cache to GPU\r\nllama_kv_cache_init: VRAM kv self = 256.00 MB\r\nllama_new_context_with_model: kv self size  =  256.00 MB\r\nllama_new_context_with_model: compute buffer total size = 76.38 MB\r\nllama_new_context_with_model: VRAM scratch buffer: 70.50 MB\r\nllama_new_context_with_model: total VRAM used: 4801.43 MB (model: 4474.93 MB, context: 326.50 MB)\r\n\u6bb5\u9519\u8bef (\u6838\u5fc3\u5df2\u8f6c\u50a8)\r\n```\r\nGDB stacktrace on segfault:\r\n```\r\n#0  0x00007ffff672582e in ?? () from /opt/rocm/lib/libamdhip64.so.5\r\n#1  0x00007ffff672dba0 in ?? () from /opt/rocm/lib/libamdhip64.so.5\r\n#2  0x00007ffff672fc6d in ?? () from /opt/rocm/lib/libamdhip64.so.5\r\n#3  0x00007ffff66f8a44 in ?? () from /opt/rocm/lib/libamdhip64.so.5\r\n#4  0x00007ffff65688e7 in ?? () from /opt/rocm/lib/libamdhip64.so.5\r\n#5  0x00007ffff65689e5 in ?? () from /opt/rocm/lib/libamdhip64.so.5\r\n#6  0x00007ffff6568ae0 in ?? () from /opt/rocm/lib/libamdhip64.so.5\r\n#7  0x00007ffff65ac7a2 in hipMemcpy2DAsync () from /opt/rocm/lib/libamdhip64.so.5\r\n#8  0x00005555556917e6 in ggml_cuda_op_mul_mat (src0=0x7ffd240e06b0, src1=0x7f8ab9ea0860, dst=0x7f8ab9ea09b0, \r\n    op=0x55555569f330 <ggml_cuda_op_mul_mat_q(ggml_tensor const*, ggml_tensor const*, ggml_tensor*, char const*, float const*, char const*, float*, long, long, long, long, ihipStream_t* const&)>, convert_src1_to_q8_1=true)\r\n    at ggml-cuda.cu:6706\r\n#9  0x000055555568cc45 in ggml_cuda_mul_mat (src0=0x7ffd240e06b0, src1=0x7f8ab9ea0860, dst=0x7f8ab9ea09b0) at ggml-cuda.cu:6895\r\n#10 0x000055555568c754 in ggml_cuda_compute_forward (params=0x7ffffffebbb0, tensor=0x7f8ab9ea09b0) at ggml-cuda.cu:7388\r\n#11 0x00005555555b4d1d in ggml_compute_forward (params=0x7ffffffebbb0, tensor=0x7f8ab9ea09b0) at ggml.c:16214\r\n#12 0x00005555555b9a94 in ggml_graph_compute_thread (data=0x7ffffffebc00) at ggml.c:17911\r\n#13 0x00005555555bb123 in ggml_graph_compute (cgraph=0x7f8ab9e00020, cplan=0x7ffffffebd00) at ggml.c:18440\r\n#14 0x00005555555c72aa in ggml_graph_compute_helper (buf=std::vector of length 25112, capacity 25112 = {...}, graph=0x7f8ab9e00020, n_threads=1) at llama.cpp:478\r\n#15 0x00005555555da79f in llama_decode_internal (lctx=..., batch=...) at llama.cpp:4144\r\n#16 0x00005555555e6d41 in llama_decode (ctx=0x5555628ba020, batch=...) at llama.cpp:7454\r\n#17 0x0000555555665dcf in llama_init_from_gpt_params (params=...) at common/common.cpp:845\r\n#18 0x0000555555567b32 in main (argc=8, argv=0x7fffffffde08) at examples/main/main.cpp:181\r\n```",
    "labels": [
      "AMD GPU"
    ],
    "state": "closed",
    "created_at": "2023-10-03T02:26:38+00:00",
    "closed_at": "2023-10-05T04:33:47+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3451/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3451"
  },
  {
    "number": 3422,
    "title": "[User] AMD GPU slower than CPU",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\nGPU inference should be faster than CPU.\r\n\r\n\r\n# Current Behavior\r\n\r\nI have 13900K CPU & 7900XTX 24G hardware. I built llama.cpp using the [hipBLAS](https://github.com/ggerganov/llama.cpp#hipblas) and it builds. However, I noticed that when I offload all layers to GPU, it is noticably slower \r\n\r\n## GPU\r\n```\r\n./main -m ../model/llama-2-13b-chat/ggml-model-q4.gguf -n 128 -ngl 50\r\n----\r\nLog start\r\nmain: build = 1299 (f5ef5cf)\r\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\r\nmain: seed  = 1696212406\r\nggml_init_cublas: found 1 ROCm devices:\r\n  Device 0: Radeon RX 7900 XTX, compute capability 11.0\r\nllama_model_loader: loaded meta data with 16 key-value pairs and 363 tensors from ../model/llama-2-13b-chat/ggml-model-q4.gguf (version GGUF V2 (latest))\r\nllama_model_loader: - tensor    0:                token_embd.weight q4_0     [  5120, 32000,     1,     1 ]\r\nllama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor    2:                    output.weight q6_K     [  5120, 32000,     1,     1 ]\r\n...\r\nllama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - kv   0:                       general.architecture str\r\nllama_model_loader: - kv   1:                               general.name str\r\nllama_model_loader: - kv   2:                       llama.context_length u32\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32\r\nllama_model_loader: - kv   4:                          llama.block_count u32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32\r\nllama_model_loader: - kv  10:                          general.file_type u32\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str\r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr\r\nllama_model_loader: - kv  15:               general.quantization_version u32\r\nllama_model_loader: - type  f32:   81 tensors\r\nllama_model_loader: - type q4_0:  281 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_print_meta: format           = GGUF V2 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 5120\r\nllm_load_print_meta: n_head           = 40\r\nllm_load_print_meta: n_head_kv        = 40\r\nllm_load_print_meta: n_layer          = 40\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: n_ff             = 13824\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: model type       = 13B\r\nllm_load_print_meta: model ftype      = mostly Q4_0\r\nllm_load_print_meta: model params     = 13.02 B\r\nllm_load_print_meta: model size       = 6.86 GiB (4.53 BPW)\r\nllm_load_print_meta: general.name   = LLaMA v2\r\nllm_load_print_meta: BOS token = 1 '<s>'\r\nllm_load_print_meta: EOS token = 2 '</s>'\r\nllm_load_print_meta: UNK token = 0 '<unk>'\r\nllm_load_print_meta: LF token  = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.12 MB\r\nllm_load_tensors: using ROCm for GPU acceleration\r\nllm_load_tensors: mem required  =   88.01 MB\r\nllm_load_tensors: offloading 40 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 43/43 layers to GPU\r\nllm_load_tensors: VRAM used: 6936.01 MB\r\n...................................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init: offloading v cache to GPU\r\nllama_kv_cache_init: offloading k cache to GPU\r\nllama_kv_cache_init: VRAM kv self = 400.00 MB\r\nllama_new_context_with_model: kv self size  =  400.00 MB\r\nllama_new_context_with_model: compute buffer total size = 80.88 MB\r\nllama_new_context_with_model: VRAM scratch buffer: 75.00 MB\r\nllama_new_context_with_model: total VRAM used: 7411.01 MB (model: 6936.01 MB, context: 475.00 MB)\r\n\r\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\r\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\r\ngenerate: n_ctx = 512, n_batch = 512, n_predict = 128, n_keep = 0\r\n\r\n\r\npgfplotstablecolumntypes\r\n\r\nIn addition to the built-in types provided by `pgfplots`, you can also use your own custom column types. Here are some examples of how to define and use custom column types:\r\n\r\n1. `boolean` type:\r\n\\documentclass{article}\r\n\\usepackage{pgfplotstable}\r\n\\begin{document}\r\n\\pgfplotstabletypeset[\r\n    columns/my_column/type={boolean},\r\n    data=mydata,\r\n    every head row/.style={before row={\\hline}}\r\n]{%\r\n    my_column & other_column\r\nllama_print_timings:        load time =  6432.57 ms\r\nllama_print_timings:      sample time =    32.92 ms /   128 runs   (    0.26 ms per token,  3888.10 tokens per second)\r\nllama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\r\nllama_print_timings:        eval time = 22756.97 ms /   128 runs   (  177.79 ms per token,     5.62 tokens per second)\r\nllama_print_timings:       total time = 22857.59 ms\r\nLog end\r\n```\r\n\r\n## CPU\r\n```\r\n./main -m ../model/llama-2-13b-chat/ggml-model-q4.gguf -n 128\r\n\r\n----\r\nLog start\r\nmain: build = 1299 (f5ef5cf)\r\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\r\nmain: seed  = 1696212490\r\nggml_init_cublas: found 1 ROCm devices:\r\n  Device 0: Radeon RX 7900 XTX, compute capability 11.0\r\nllama_model_loader: loaded meta data with 16 key-value pairs and 363 tensors from ../model/llama-2-13b-chat/ggml-model-q4.gguf (version GGUF V2 (latest))\r\nllama_model_loader: - tensor    0:                token_embd.weight q4_0     [  5120, 32000,     1,     1 ]\r\nllama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor    2:                    output.weight q6_K     [  5120, 32000,     1,     1 ]\r\n...\r\nllama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - kv   0:                       general.architecture str\r\nllama_model_loader: - kv   1:                               general.name str\r\nllama_model_loader: - kv   2:                       llama.context_length u32\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32\r\nllama_model_loader: - kv   4:                          llama.block_count u32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32\r\nllama_model_loader: - kv  10:                          general.file_type u32\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str\r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr\r\nllama_model_loader: - kv  15:               general.quantization_version u32\r\nllama_model_loader: - type  f32:   81 tensors\r\nllama_model_loader: - type q4_0:  281 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_print_meta: format           = GGUF V2 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 5120\r\nllm_load_print_meta: n_head           = 40\r\nllm_load_print_meta: n_head_kv        = 40\r\nllm_load_print_meta: n_layer          = 40\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: n_ff             = 13824\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: model type       = 13B\r\nllm_load_print_meta: model ftype      = mostly Q4_0\r\nllm_load_print_meta: model params     = 13.02 B\r\nllm_load_print_meta: model size       = 6.86 GiB (4.53 BPW)\r\nllm_load_print_meta: general.name   = LLaMA v2\r\nllm_load_print_meta: BOS token = 1 '<s>'\r\nllm_load_print_meta: EOS token = 2 '</s>'\r\nllm_load_print_meta: UNK token = 0 '<unk>'\r\nllm_load_print_meta: LF token  = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.12 MB\r\nllm_load_tensors: using ROCm for GPU acceleration\r\nllm_load_tensors: mem required  = 7024.01 MB\r\nllm_load_tensors: offloading 0 repeating layers to GPU\r\nllm_load_tensors: offloaded 0/43 layers to GPU\r\nllm_load_tensors: VRAM used: 0.00 MB\r\n...................................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_new_context_with_model: kv self size  =  400.00 MB\r\nllama_new_context_with_model: compute buffer total size = 80.88 MB\r\nllama_new_context_with_model: VRAM scratch buffer: 75.00 MB\r\nllama_new_context_with_model: total VRAM used: 75.00 MB (model: 0.00 MB, context: 75.00 MB)\r\n\r\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\r\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\r\ngenerate: n_ctx = 512, n_batch = 512, n_predict = 128, n_keep = 0\r\n\r\n\r\ntikz\\draw[fill=blue!50] (0,0) rectangle (1.5,1.5);\r\n\\tikz\\draw[fill=red!50] (1.5,0) rectangle (3,1.5);\r\n\\tikz\\draw[fill=green!50] (3,0) rectangle (4.5,1.5);\r\n\\end{tikzpicture}\r\n\r\nIn this example, the rectangles are drawn with different colors: blue, red and green.\r\n\r\nYou can also use other shapes like circles, triangles, etc. by changing the\r\nllama_print_timings:        load time =   363.76 ms\r\nllama_print_timings:      sample time =    36.15 ms /   128 runs   (    0.28 ms per token,  3541.29 tokens per second)\r\nllama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\r\nllama_print_timings:        eval time = 19588.62 ms /   128 runs   (  153.04 ms per token,     6.53 tokens per second)\r\nllama_print_timings:       total time = 19695.27 ms\r\nLog end\r\n```\r\n\r\n# Environment and Context\r\n\r\n\r\nCPU: i9-13900KF\r\nOS: Linux pia 6.2.0-33-generic #33~22.04.1-Ubuntu\r\nGPU: 7900XTX\r\nPython: 3.10\r\ng++: 11.4.0\r\nMake: 4.3\r\n\r\n\r\nBuild command\r\n\r\n```\r\nmake LLAMA_HIPBLAS=1\r\n```\r\n\r\nrocminfo\r\n\r\n```\r\n\r\n\u276f rocminfo\r\nROCk module is loaded\r\n=====================\r\nHSA System Attributes\r\n=====================\r\nRuntime Version:         1.1\r\nSystem Timestamp Freq.:  1000.000000MHz\r\nSig. Max Wait Duration:  18446744073709551615 (0xFFFFFFFFFFFFFFFF) (timestamp count)\r\nMachine Model:           LARGE\r\nSystem Endianness:       LITTLE\r\n\r\n==========\r\nHSA Agents\r\n==========\r\n*******\r\nAgent 1\r\n*******\r\n  Name:                    13th Gen Intel(R) Core(TM) i9-13900KF\r\n  Uuid:                    CPU-XX\r\n  Marketing Name:          13th Gen Intel(R) Core(TM) i9-13900KF\r\n  Vendor Name:             CPU\r\n  Feature:                 None specified\r\n  Profile:                 FULL_PROFILE\r\n  Float Round Mode:        NEAR\r\n  Max Queue Number:        0(0x0)\r\n  Queue Min Size:          0(0x0)\r\n  Queue Max Size:          0(0x0)\r\n  Queue Type:              MULTI\r\n  Node:                    0\r\n  Device Type:             CPU\r\n  Cache Info:\r\n    L1:                      32768(0x8000) KB\r\n  Chip ID:                 0(0x0)\r\n  ASIC Revision:           0(0x0)\r\n  Cacheline Size:          64(0x40)\r\n  Max Clock Freq. (MHz):   5500\r\n  BDFID:                   0\r\n  Internal Node ID:        0\r\n  Compute Unit:            32\r\n  SIMDs per CU:            0\r\n  Shader Engines:          0\r\n  Shader Arrs. per Eng.:   0\r\n  WatchPts on Addr. Ranges:1\r\n  Features:                None\r\n  Pool Info:\r\n    Pool 1\r\n      Segment:                 GLOBAL; FLAGS: FINE GRAINED\r\n      Size:                    98692092(0x5e1ebfc) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       TRUE\r\n    Pool 2\r\n      Segment:                 GLOBAL; FLAGS: KERNARG, FINE GRAINED\r\n      Size:                    98692092(0x5e1ebfc) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       TRUE\r\n    Pool 3\r\n      Segment:                 GLOBAL; FLAGS: COARSE GRAINED\r\n      Size:                    98692092(0x5e1ebfc) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       TRUE\r\n  ISA Info:\r\n*******\r\nAgent 2\r\n*******\r\n  Name:                    gfx1100\r\n  Uuid:                    GPU-754358d3215edcd7\r\n  Marketing Name:          Radeon RX 7900 XTX\r\n  Vendor Name:             AMD\r\n  Feature:                 KERNEL_DISPATCH\r\n  Profile:                 BASE_PROFILE\r\n  Float Round Mode:        NEAR\r\n  Max Queue Number:        128(0x80)\r\n  Queue Min Size:          64(0x40)\r\n  Queue Max Size:          131072(0x20000)\r\n  Queue Type:              MULTI\r\n  Node:                    1\r\n  Device Type:             GPU\r\n  Cache Info:\r\n    L1:                      32(0x20) KB\r\n    L2:                      6144(0x1800) KB\r\n    L3:                      98304(0x18000) KB\r\n  Chip ID:                 29772(0x744c)\r\n  ASIC Revision:           0(0x0)\r\n  Cacheline Size:          64(0x40)\r\n  Max Clock Freq. (MHz):   2304\r\n  BDFID:                   768\r\n  Internal Node ID:        1\r\n  Compute Unit:            96\r\n  SIMDs per CU:            2\r\n  Shader Engines:          6\r\n  Shader Arrs. per Eng.:   2\r\n  WatchPts on Addr. Ranges:4\r\n  Features:                KERNEL_DISPATCH\r\n  Fast F16 Operation:      TRUE\r\n  Wavefront Size:          32(0x20)\r\n  Workgroup Max Size:      1024(0x400)\r\n  Workgroup Max Size per Dimension:\r\n    x                        1024(0x400)\r\n    y                        1024(0x400)\r\n    z                        1024(0x400)\r\n  Max Waves Per CU:        32(0x20)\r\n  Max Work-item Per CU:    1024(0x400)\r\n  Grid Max Size:           4294967295(0xffffffff)\r\n  Grid Max Size per Dimension:\r\n    x                        4294967295(0xffffffff)\r\n    y                        4294967295(0xffffffff)\r\n    z                        4294967295(0xffffffff)\r\n  Max fbarriers/Workgrp:   32\r\n  Pool Info:\r\n    Pool 1\r\n      Segment:                 GLOBAL; FLAGS: COARSE GRAINED\r\n      Size:                    25149440(0x17fc000) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       FALSE\r\n    Pool 2\r\n      Segment:                 GROUP\r\n      Size:                    64(0x40) KB\r\n      Allocatable:             FALSE\r\n      Alloc Granule:           0KB\r\n      Alloc Alignment:         0KB\r\n      Accessible by all:       FALSE\r\n  ISA Info:\r\n    ISA 1\r\n      Name:                    amdgcn-amd-amdhsa--gfx1100\r\n      Machine Models:          HSA_MACHINE_MODEL_LARGE\r\n      Profiles:                HSA_PROFILE_BASE\r\n      Default Rounding Mode:   NEAR\r\n      Default Rounding Mode:   NEAR\r\n      Fast f16:                TRUE\r\n      Workgroup Max Size:      1024(0x400)\r\n      Workgroup Max Size per Dimension:\r\n        x                        1024(0x400)\r\n        y                        1024(0x400)\r\n        z                        1024(0x400)\r\n      Grid Max Size:           4294967295(0xffffffff)\r\n      Grid Max Size per Dimension:\r\n        x                        4294967295(0xffffffff)\r\n        y                        4294967295(0xffffffff)\r\n        z                        4294967295(0xffffffff)\r\n      FBarrier Max Size:       32\r\n*** Done ***\r\n```\r\n\r\n\r\n## Additional comparison between Nvidia RTX 4700 ti vs RX7900XTX\r\n\r\nI further tested RTX 4700 TI... it is probably 10x faster than RX7900XTX...\r\n\r\n### Nvidia GPU (4700TI)\r\n\r\n**4700ti 56.23 tokens**\r\n```\r\nllama_print_timings:        load time =   824.29 ms\r\nllama_print_timings:      sample time =    52.74 ms /   128 runs   (    0.41 ms per token,  2427.18 tokens per second)\r\nllama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\r\nllama_print_timings:        eval time =  2276.23 ms /   128 runs   (   17.78 ms per token,    56.23 tokens per second)\r\nllama_print_timings:       total time =  2357.70 ms\r\nLog end\r\n```\r\n\r\n### 7900XTX 5.62 tokens per second\r\n```\r\nllama_print_timings:        load time =  6432.57 ms\r\nllama_print_timings:      sample time =    32.92 ms /   128 runs   (    0.26 ms per token,  3888.10 tokens per second)\r\nllama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\r\nllama_print_timings:        eval time = 22756.97 ms /   128 runs   (  177.79 ms per token,     5.62 tokens per second)\r\nllama_print_timings:       total time = 22857.59 ms\r\n```\r\n",
    "labels": [
      "performance",
      "AMD GPU",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-10-01T05:57:21+00:00",
    "closed_at": "2024-05-12T01:35:23+00:00",
    "comments": 40,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3422/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3422"
  }
]
[
  {
    "number": 6571,
    "title": "b2447 (c47cf41) decreased output quality",
    "body": "With identical seeds and options, b2447 (https://github.com/ggerganov/llama.cpp/commit/c47cf414efafb8f60596edc7edb5a2d68065e992) produces different output that seems lower in quality compared to b2446. Is it possible to preserve old output quality in new builds?\r\n\r\nSystem: MacBook Pro w/ i5-1038NG7",
    "labels": [
      "need more info",
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-04-09T18:50:49+00:00",
    "closed_at": "2024-05-24T13:29:29+00:00",
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6571/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6571"
  },
  {
    "number": 4575,
    "title": "make process hangs if LLAMA_CUBLAS=1, at the line that includes the file scripts/get-flags.mk for the second time",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI run make LLAMA_CUBLAS=1 and that process hangs. I used make --debug=f to figure out that make gets stuck at the line that includes get-flags.mk for the second time (it is already included a few lines before). \r\n\r\n# Environment and Context\r\n\r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\n|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:00:05.0 Off |                    0 |\r\n| N/A   42C    P0              86W / 400W |    591MiB / 81920MiB |      0%      Default |\r\n|                                         |                      |             Disabled |\r\n+-----------------------------------------+----------------------+----------------------+\r\n\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n`$ lscpu`\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         46 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  12\r\n  On-line CPU(s) list:   0-11\r\nVendor ID:               GenuineIntel\r\n  Model name:            Intel(R) Xeon(R) Gold 6342 CPU @ 2.80GHz\r\n    CPU family:          6\r\n    Model:               106\r\n    Thread(s) per core:  1\r\n    Core(s) per socket:  12\r\n    Socket(s):           1\r\n    Stepping:            6\r\n    BogoMIPS:            5600.21\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush acpi mmx fxsr sse sse2 s\r\n                         s ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl cpuid tsc_known_freq pni pclmulqdq ssse3 fm\r\n                         a cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor la\r\n                         hf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjus\r\n                         t bmi1 avx2 smep bmi2 erms invpcid rtm rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xs\r\n                         aves umip pku ospke gfni vaes vpclmulqdq rdpid md_clear flush_l1d arch_capabilities\r\nVirtualization features:\r\n  Hypervisor vendor:     Xen\r\n  Virtualization type:   full\r\n\r\nCaches (sum of all):\r\n  L1d:                   576 KiB (12 instances)\r\n  L1i:                   384 KiB (12 instances)\r\n  L2:                    15 MiB (12 instances)\r\n  L3:                    432 MiB (12 instances)\r\nNUMA:\r\n  NUMA node(s):          1\r\n  NUMA node0 CPU(s):     0-11\r\nVulnerabilities:\r\n  Gather data sampling:  Not affected\r\n  Itlb multihit:         Not affected\r\n  L1tf:                  Not affected\r\n  Mds:                   Not affected\r\n  Meltdown:              Not affected\r\n  Mmio stale data:       Mitigation; Clear CPU buffers; SMT Host state unknown\r\n  Retbleed:              Not affected\r\n  Spec rstack overflow:  Not affected\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\n  Spectre v2:            Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\n  Srbds:                 Not affected\r\n  Tsx async abort:       Not affected\r\n\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`$ uname -a`\r\n\r\nLinux psbngks26deu 6.2.0-39-generic #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2 x86_64 x86_64 x86_64 GNU/Linux \r\n\r\n* SDK version, e.g. for Linux:\r\nPython 3.11.7\r\nGNU Make 4.3\r\nBuilt for x86_64-pc-linux-gnu\r\ng++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\n\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-21T21:18:17+00:00",
    "closed_at": "2024-04-02T01:10:16+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4575/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4575"
  },
  {
    "number": 8612,
    "title": "Bug:  Recent changes break Rocm compile on windows",
    "body": "### What happened?\n\nIt cannot compile after CUDA: MMQ code deduplication + iquant support on windows.\r\nthat's my guess that pr break compile.\n\n### Name and Version\n\nb3428 and Windows 11 rocm5.7.1\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n```shell\ncmake --build . -j 99 --parallel 32 --config Release\r\n[1/61] Linking CXX shared library bin\\ggml.dll\r\nFAILED: bin/ggml.dll ggml/src/ggml.lib\r\ncmd.exe /C \"cmd.exe /C \"C:\\Strawberry\\c\\bin\\cmake.exe -E __create_def W:\\git\\llama.cpp\\rocm_1100\\ggml\\src\\CMakeFiles\\ggml.dir\\.\\exports.def W:\\git\\llama.cpp\\rocm_1100\\ggml\\src\\CMakeFiles\\ggml.dir\\.\\exports.def.objs --nm=C:\\Strawberry\\c\\bin\\nm.exe && cd W:\\git\\llama.cpp\\rocm_1100\" && C:\\PROGRA~1\\AMD\\ROCm\\5.7\\bin\\CLANG_~1.EXE -fuse-ld=lld-link -nostartfiles -nostdlib -O3 -DNDEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrt  -Xlinker /DEF:ggml\\src\\CMakeFiles\\ggml.dir\\.\\exports.def -shared -o bin\\ggml.dll  -Xlinker /MANIFEST:EMBED -Xlinker /implib:ggml\\src\\ggml.lib -Xlinker /pdb:bin\\ggml.pdb -Xlinker /version:0.0 ggml/src/CMakeFiles/ggml.dir/ggml.c.obj ggml/src/CMakeFiles/ggml.dir/ggml-alloc.c.obj ggml/src/CMakeFiles/ggml.dir/ggml-backend.c.obj ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/conv-transpose-1d.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/fattn-tile-f16.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/fattn-tile-f32.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb16.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb32.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb16.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb32.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb8.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-iq4_nl.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-iq4_xs.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q2_k.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q3_k.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q4_0.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q4_1.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q4_k.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q5_0.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q5_1.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q5_k.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q6_k.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q8_0.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.obj ggml/src/CMakeFiles/ggml.dir/llamafile/sgemm.cpp.obj ggml/src/CMakeFiles/ggml.dir/ggml-aarch64.c.obj  \"C:/Program Files/AMD/ROCm/5.7/lib/hipblas.lib\"  --hip-link  --offload-arch=gfx1100  \"C:/Program Files/AMD/ROCm/5.7/lib/rocblas.lib\"  \"C:/Program Files/AMD/ROCm/5.7/lib/clang/17.0.0/lib/windows/clang_rt.builtins-x86_64.lib\"  \"C:/Program Files/AMD/ROCm/5.7/lib/amdhip64.lib\"  -lkernel32 -luser32 -lgdi32 -lwinspool -lshell32 -lole32 -loleaut32 -luuid -lcomdlg32 -ladvapi32 -loldnames  && cd .\"\r\nlld-link: error: undefined symbol: void __cdecl mul_mat_q_case<16>(struct ggml_backend_cuda_context &, struct mmq_args const &, struct ihipStream_t *)\r\n>>> referenced by ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.obj:(void __cdecl ggml_cuda_op_mul_mat_q(struct ggml_backend_cuda_context &, struct ggml_tensor const *, struct ggml_tensor const *, struct ggml_tensor *, char const *, float const *, char const *, float *, __int64, __int64, __int64, __int64, struct ihipStream_t *))\r\n\r\nlld-link: error: undefined symbol: void __cdecl mul_mat_q_case<17>(struct ggml_backend_cuda_context &, struct mmq_args const &, struct ihipStream_t *)\r\n>>> referenced by ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.obj:(void __cdecl ggml_cuda_op_mul_mat_q(struct ggml_backend_cuda_context &, struct ggml_tensor const *, struct ggml_tensor const *, struct ggml_tensor *, char const *, float const *, char const *, float *, __int64, __int64, __int64, __int64, struct ihipStream_t *))\r\n\r\nlld-link: error: undefined symbol: void __cdecl mul_mat_q_case<18>(struct ggml_backend_cuda_context &, struct mmq_args const &, struct ihipStream_t *)\r\n>>> referenced by ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.obj:(void __cdecl ggml_cuda_op_mul_mat_q(struct ggml_backend_cuda_context &, struct ggml_tensor const *, struct ggml_tensor const *, struct ggml_tensor *, char const *, float const *, char const *, float *, __int64, __int64, __int64, __int64, struct ihipStream_t *))\r\n\r\nlld-link: error: undefined symbol: void __cdecl mul_mat_q_case<19>(struct ggml_backend_cuda_context &, struct mmq_args const &, struct ihipStream_t *)\r\n>>> referenced by ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.obj:(void __cdecl ggml_cuda_op_mul_mat_q(struct ggml_backend_cuda_context &, struct ggml_tensor const *, struct ggml_tensor const *, struct ggml_tensor *, char const *, float const *, char const *, float *, __int64, __int64, __int64, __int64, struct ihipStream_t *))\r\n\r\nlld-link: error: undefined symbol: void __cdecl mul_mat_q_case<21>(struct ggml_backend_cuda_context &, struct mmq_args const &, struct ihipStream_t *)\r\n>>> referenced by ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.obj:(void __cdecl ggml_cuda_op_mul_mat_q(struct ggml_backend_cuda_context &, struct ggml_tensor const *, struct ggml_tensor const *, struct ggml_tensor *, char const *, float const *, char const *, float *, __int64, __int64, __int64, __int64, struct ihipStream_t *))\r\n\r\nlld-link: error: undefined symbol: void __cdecl mul_mat_q_case<22>(struct ggml_backend_cuda_context &, struct mmq_args const &, struct ihipStream_t *)\r\n>>> referenced by ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.obj:(void __cdecl ggml_cuda_op_mul_mat_q(struct ggml_backend_cuda_context &, struct ggml_tensor const *, struct ggml_tensor const *, struct ggml_tensor *, char const *, float const *, char const *, float *, __int64, __int64, __int64, __int64, struct ihipStream_t *))\r\nCLANG_~1: error: linker command failed with exit code 1 (use -v to see invocation)\r\nninja: build stopped: subcommand failed.\n```\n",
    "labels": [
      "bug-unconfirmed",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-07-21T08:53:56+00:00",
    "closed_at": "2024-07-21T14:39:23+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8612/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8612"
  },
  {
    "number": 7112,
    "title": "Server: Multimodal Model Input Parameter No longer Exists",
    "body": "I have noticed that when using the server that the --mmproj parameter for multimodal models has been disabled. Although it still remains in the README. Is there an alternative to --mmproj , I cannot seem to find one in the code. \r\n\r\nAny help on this would be great. \r\n\r\nCode to reproduce:\r\n`./server -m ./ggml-model-q4_k.gguf --mmproj ./mmproj-model-f16.gguf -ngl 1`\r\n\r\nError:\r\n`\r\nerror: unknown argument: --mmproj\r\nusage: ./server [options]\r\n`",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-07T04:11:00+00:00",
    "closed_at": "2024-07-18T01:06:49+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7112/reactions",
      "total_count": 5,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 1,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7112"
  },
  {
    "number": 13549,
    "title": "Misc. bug: Potential out of bound in rerank",
    "body": "### Name and Version\n\nversion: 5387 (3198405e)\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n\n### Operating systems\n\n_No response_\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Command line\n\n```shell\n\n```\n\n### Problem description & steps to reproduce\n\n[llama_context](https://github.com/ggml-org/llama.cpp/blob/f5170c1d7a66222ca7c75d2022fec3ed87257e0b/src/llama-context.cpp#L807) resize the rerank output to size 1 while [here](https://github.com/ggml-org/llama.cpp/blob/017f10b5fa630a013ec4f9936e410a60d4f460d5/examples/embedding/embedding.cpp#L69) we still normalize it as if we have full embedding vector. I found this problem happened randomly in python binding but cannot reproduce it in cpp. Not sure if it is a bug in cpp side.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-05-14T19:50:38+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13549/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13549"
  },
  {
    "number": 13102,
    "title": "Misc. bug: Retrieval sample not decoding token successfully",
    "body": "### Name and Version\n\nversion: 5184 (87616f06)\nbuilt with MSVC 19.41.34120.0 for x64\n\n### Operating systems\n\nMac, Windows\n\n### Which llama.cpp modules do you know to be affected?\n\nOther (Please specify in the next section)\n\n### Command line\n\n```shell\nllama-retrieval.exe --context-file <any_text_file> --chunk-size 1 -c 512 -t 8 -m bge-large-en-v1.5-f32.gguf\n```\n\n### Problem description & steps to reproduce\n\nThe sample failed to decode any tokens created from the text embeddings.\n\nIt looks like  we need to skip the kv-cache logic to look for an unused slot when pooling is active (which is true for the above model).\n\nThe following IF in llama-context.cpp is removed, causing us to go into this logic to search for an unused slot and hit the decoding spew.\n\n        // non-causal masks do not use the KV cache\n        if (hparams.causal_attn) {\n            kv_self_update();\n\nJust adding \"if (!embd_pooling)\" appears to fix the issue but I am not sure what it does to the original logic for the non-causal mask with gemma-3.\n\n### First Bad Commit\n\nhttps://github.com/ggml-org/llama.cpp/pull/12615/commits/bed4c73a51f0ddc0c0c95c7710749f674c9c204c\n\n### Relevant log output\n\n```shell\nllama-retrieval.exe --context-file <any_text_file> --chunk-size 1 -c 512 -t 8 -m bge-large-en-v1.5-f32.gguf\n...\ninit:        CPU KV buffer size =    48.00 MiB\nllama_context: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB\nllama_context:        CPU compute buffer size =    27.01 MiB\nllama_context: graph nodes  = 825\nllama_context: graph splits = 1\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 512\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n\nsystem_info: n_threads = 8 (n_threads_batch = 8) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |\nbatch_decode: n_tokens = 2043, n_seq = 118\nfind_slot: n_tokens = 2043 > size = 512\ndecode: failed to find KV cache slot for ubatch of size 2043\nllama_decode: failed to decode, ret = 1\nget_embeddings_ith: invalid embeddings id 0, reason: no embeddings\nbatch_decode: failed to get embeddings for token 0\nget_embeddings_ith: invalid embeddings id 1, reason: no embeddings\nbatch_decode: failed to get embeddings for token 1\nget_embeddings_ith: invalid embeddings id 2, reason: no embeddings\nbatch_decode: failed to get embeddings for token 2\nget_embeddings_ith: invalid embeddings id 3, reason: no embeddings\nbatch_decode: failed to get embeddings for token 3\n...\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-04-24T22:26:15+00:00",
    "closed_at": "2025-06-08T01:08:06+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13102/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13102"
  },
  {
    "number": 7450,
    "title": "Since last update Mistral models doesn't works anymore",
    "body": "since this https://github.com/ggerganov/llama.cpp/tree/b2961\r\nphi3-128k works better (if ctx <32k)\r\nbut mistral models are crazy, I tried 7bQ2 7bQ8, 70BQ2XS, none of them works anymore\r\n\r\n```\r\nLog start\r\nmain\r\nmain: build = 2961 (201cc11a)\r\nmain: built with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\r\nmain: seed  = 1716349593\r\nllama_model_loader\r\nllama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from models/mistral-7b-instruct-v0.2.Q2_K.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 10\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\r\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q2_K:   65 tensors\r\nllama_model_loader: - type q3_K:  160 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q2_K - Medium\r\nllm_load_print_meta: model params     = 7.24 B\r\nllm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) \r\nllm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nggml_cuda_init\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\r\nggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\r\nggml_cuda_init: found 1 CUDA devices:\r\nDevice 0\r\n  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\r\nllm_load_tensors\r\nllm_load_tensors: ggml ctx size =    0.30 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:        CPU buffer size =    41.02 MiB\r\nllm_load_tensors:      CUDA0 buffer size =  2898.55 MiB\r\n..................................................................................................\r\nllama_new_context_with_model\r\nllama_new_context_with_model: n_ctx      = 32000\r\nllama_new_context_with_model: n_batch    = 1024\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init\r\nllama_kv_cache_init:      CUDA0 KV buffer size =  4000.00 MiB\r\nllama_new_context_with_model\r\nllama_new_context_with_model: KV self size  = 4000.00 MiB, K (f16): 2000.00 MiB, V (f16): 2000.00 MiB\r\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =  2094.50 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =    70.51 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 2\r\n\r\nsystem_info\r\nsystem_info: n_threads = 32 / 32 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \r\nllama_tokenize_internal\r\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\r\nsampling: \r\n\trepeat_last_n = 256, repeat_penalty = 1.176, frequency_penalty = 0.000, presence_penalty = 0.000\r\n\ttop_k = 40, tfs_z = 1.000, top_p = 0.500, min_p = 0.050, typical_p = 1.000, temp = 0.500\r\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\r\nsampling order: \r\nCFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \r\ngenerate\r\ngenerate: n_ctx = 32000, n_batch = 1024, n_predict = 8192, n_keep = 1\r\n\r\n\r\n\r\nllama_print_timings\r\nllama_print_timings:        load time =     622.18 ms\r\nllama_print_timings:      sample time =     907.92 ms /  8192 runs   (    0.11 ms per token,  9022.83 tokens per second)\r\nllama_print_timings: prompt eval time =      37.07 ms /    25 tokens (    1.48 ms per token,   674.40 tokens per second)\r\nllama_print_timings:        eval time =   53408.73 ms /  8191 runs   (    6.52 ms per token,   153.36 tokens per second)\r\nllama_print_timings:       total time =   55902.60 ms /  8216 tokens\r\nLog end\r\n```\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-05-22T03:53:03+00:00",
    "closed_at": "2024-05-22T10:03:22+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7450/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 1,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7450"
  },
  {
    "number": 7311,
    "title": "ggml_validate_row_data finding nan value for IQ4_NL",
    "body": "Using b2854\r\n\r\nConverted Hermes-2-Theta-Llama-3-8B to F32, then measured imatrix with https://gist.github.com/bartowski1182/b6ac44691e994344625687afe3263b3a\r\n\r\nUpon quanting, all sizes work fine, except for IQ4_NL which produces this output:\r\n\r\n```\r\nload_imatrix: imatrix dataset='/training_data/calibration_data.txt'\r\nload_imatrix: loaded 224 importance matrix entries from /models/Hermes-2-Theta-Llama-3-8B-GGUF/Hermes-2-Theta-Llama-3-8B.imatrix computed on 189 chunks\r\nprepare_imatrix: have 224 importance matrix entries\r\nmain: build = 2854 (72c177c1)\r\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\r\nmain: quantizing '/models/Hermes-2-Theta-Llama-3-8B-GGUF/Hermes-2-Theta-Llama-3-8B-f32.gguf' to '/models/Hermes-2-Theta-Llama-3-8B-GGUF/Hermes-2-Theta-Llama-3-8B-IQ4_NL.gguf' as IQ4_NL\r\nllama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /models/Hermes-2-Theta-Llama-3-8B-GGUF/Hermes-2-Theta-Llama-3-8B-f32.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = Hermes-2-Theta-Llama-3-8B\r\nllama_model_loader: - kv   2:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   3:                       llama.context_length u32              = 8192\r\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 0\r\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\r\nllama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"\u0120 \u0120\", \"\u0120 \u0120\u0120\u0120\", \"\u0120\u0120 \u0120\u0120\", \"...\r\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\r\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128003\r\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\r\nllama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{bos_token}}{% for message in messag...\r\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  291 tensors\r\n================================ Have weights data with 224 entries\r\n[   1/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f32,\r\n====== llama_model_quantize_internal: did not find weights for token_embd.weight\r\nconverting to iq4_nl .. ggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 128\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 384\r\nggml_validate_row_data: found nan value at block 128\r\nggml_validate_row_data: found nan value at block 128\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 128\r\nggml_validate_row_data: found nan value at block 384\r\nggml_validate_row_data: found nan value at block 256\r\nggml_validate_row_data: found nan value at block 256\r\nggml_validate_row_data: found nan value at block 384\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 256\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 384\r\nggml_validate_row_data: found nan value at block 128\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 128\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nllama_model_quantize: failed to quantize: quantized data validation failed\r\nmain: failed to quantize model from '/models/Hermes-2-Theta-Llama-3-8B-GGUF/Hermes-2-Theta-Llama-3-8B-f32.gguf'\r\n```\r\n\r\nWhen I refer to \"all quants\" I mean these all work fine:\r\n\r\nIQ1_S, IQ1_M, IQ2_XXS, IQ2_XS, IQ2_S, IQ2_M, Q2_K, IQ3_XXS, IQ3_XS, IQ3_S, IQ3_M, Q3_K_S, Q3_K_M, Q3_K_L, IQ4_XS, Q4_K_S, Q4_K_M, Q5_K_S, Q5_K_M, Q6_K, Q8_0",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-05-15T19:40:10+00:00",
    "closed_at": "2024-05-18T00:39:55+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7311/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7311"
  },
  {
    "number": 13614,
    "title": "Compile bug: tools build failing",
    "body": "### Git commit\n\nCommit 6a2bc8b\n\n\n### Operating systems\n\nWindows\n\n### GGML backends\n\nCUDA\n\n### Problem description & steps to reproduce\n\nEnvironment: Windows 11 with CUDA toolkit installed.\n\nSorry, new to this. I tried searching if there was already a solution but couldn't find anything with my limited domain of knowledge.\n\nI followed the guide to build llama.cpp with CUDA support which seems to worked as it built a few binaries that I can see in the bin/Release folder, but I noticed none of the tools were built. I.g. cli, server etc...\n\nAlso, my environment was missing CURL libraries, so I had to look it up and install a windows version. And issued the following to build this:\n\n```\ncmake -B build -DGGML_CUDA=ON -DCURL_LIBRARY=c:\\Curl\\lib\\libcurl.a -DCURL_INCLUDE_DIR=c:\\Curl\\include\n```\n\nReading up on the llama-server docs, I saw there was a way to build it so I tried it but I got this error:\n```\ncommon.lib(arg.obj) : error LNK2019: unresolved external symbol __imp_curl_slist_appe\nnd referenced in function \"bool __cdecl common_download_file_single(class std::basic_\nstring<char,struct std::char_traits<char>,class std::allocator<char> > const &,class\nstd::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > con\nst &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<\nchar> > const &)\" (?common_download_file_single@@YA_NAEBV?$basic_string@DU?$char_trai\nts@D@std@@V?$allocator@D@2@@std@@00@Z)\n```\nAnd a bunch related to curl. \n\n### First Bad Commit\n\n_No response_\n\n### Compile command\n\n```shell\ncmake -B build -DGGML_CUDA=ON -DCURL_LIBRARY=c:\\Curl\\lib\\libcurl.a -DCURL_INCLUDE_DIR=c:\\Curl\\include\n```\n\n### Relevant log output\n\n```shell\ncommon.lib(arg.obj) : error LNK2019: unresolved external symbol __imp_curl_slist_appe\nnd referenced in function \"bool __cdecl common_download_file_single(class std::basic_\nstring<char,struct std::char_traits<char>,class std::allocator<char> > const &,class\nstd::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > con\nst &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<\nchar> > const &)\" (?common_download_file_single@@YA_NAEBV?$basic_string@DU?$char_trai\nts@D@std@@V?$allocator@D@2@@std@@00@Z)\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-05-18T14:26:43+00:00",
    "closed_at": "2025-07-02T01:07:53+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13614/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13614"
  },
  {
    "number": 6031,
    "title": "Segmentation fault during inference on AMD gfx900 with codebooga-34b-v0.1.Q5_K_M.gguf",
    "body": "Hi,\r\n\r\nI compiled `llama.cpp` from git, todays master HEAD `commit 8030da7afea2d89f997aeadbd14183d399a017b9` on Fedora Rawhide (ROCm 6.0.x) like this:\r\n```\r\nCC=/usr/bin/clang CXX=/usr/bin/clang++ cmake .. -DLLAMA_HIPBLAS=ON -DAMDGPU_TARGETS=gfx900 -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=\"--rocm-device-lib-path=/usr/lib/clang/17/amdgcn/bitcode\"\r\nmake -j 16\r\n```\r\n\r\nThen I tried to run a prompt using the `codebooga-34b-v0.1.Q5_K_M.gguf` model which I got from here: https://huggingface.co/TheBloke/CodeBooga-34B-v0.1-GGUF\r\n\r\nI kept the prompt simple and used the following command:\r\n./main -t 10 -ngl 16 -m ~/models/codebooga-34b-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"### Instruction: How do I get the length of a Vec in Rust?\\n### Response:\"\r\n\r\nI have an AMD Instinct MI25 card with 16GB VRAM, according to `nvtop` with `-ngl 16` about half of it is used `8.219Gi/15.984`, so this does not seem to be an OOM issue.\r\n\r\nThe console output looks like this:\r\n```\r\nLog start                                                                       \r\nmain: build = 2408 (8030da7a)\r\nmain: built with clang version 18.1.0 (Fedora 18.1.0~rc4-2.fc41) for x86_64-redhat-linux-gnu\r\nmain: seed  = 1710292844\r\n[New Thread 0x7fff074006c0 (LWP 11038)]\r\n[New Thread 0x7ffe068006c0 (LWP 11039)]\r\n[Thread 0x7ffe068006c0 (LWP 11039) exited]\r\nggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\r\nggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\r\nggml_init_cublas: found 1 ROCm devices:\r\n  Device 0: AMD Radeon Instinct MI25, compute capability 9.0, VMM: no\r\nllama_model_loader: loaded meta data with 21 key-value pairs and 435 tensors from /home/jin/Work/text-generation-webui/models/codebooga-34b-v0.1.Q5_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = oobabooga_codebooga-34b-v0.1\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 16384\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 48\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 17\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\r\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   97 tensors\r\nllama_model_loader: - type q5_K:  289 tensors\r\nllama_model_loader: - type q6_K:   49 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 16384\r\nllm_load_print_meta: n_embd           = 8192\r\nllm_load_print_meta: n_head           = 64\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 48\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 8\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 22016\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attm      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 16384\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 34B\r\nllm_load_print_meta: model ftype      = Q5_K - Medium\r\nllm_load_print_meta: model params     = 33.74 B\r\nllm_load_print_meta: model size       = 22.20 GiB (5.65 BPW) \r\nllm_load_print_meta: general.name     = oobabooga_codebooga-34b-v0.1\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 2 '</s>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.33 MiB\r\nllm_load_tensors: offloading 16 repeating layers to GPU\r\nllm_load_tensors: offloaded 16/49 layers to GPU\r\nllm_load_tensors:      ROCm0 buffer size =  7500.06 MiB\r\nllm_load_tensors:        CPU buffer size = 22733.73 MiB\r\n....................................................................................................\r\nllama_new_context_with_model: n_ctx      = 2048\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:      ROCm0 KV buffer size =   128.00 MiB\r\nllama_kv_cache_init:  ROCm_Host KV buffer size =   256.00 MiB\r\nllama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB\r\nllama_new_context_with_model:  ROCm_Host input buffer size   =    21.02 MiB\r\nggml_gallocr_reserve_n: reallocating ROCm0 buffer from size 0.00 MiB to 324.00 MiB\r\nggml_gallocr_reserve_n: reallocating ROCm_Host buffer from size 0.00 MiB to 336.00 MiB\r\nllama_new_context_with_model:      ROCm0 compute buffer size =   324.00 MiB\r\nllama_new_context_with_model:  ROCm_Host compute buffer size =   336.00 MiB\r\nllama_new_context_with_model: graph splits (measure): 3\r\n```\r\n\r\nShortly after I get a segfault, although sometimes it starts responding and crashes a few seconds into the response:\r\n\r\n```\r\n(gdb) bt\r\n#0  amd::KernelParameters::set (this=0x1d9cb10, index=11, size=4, \r\n    value=0x100000020, svmBound=false)\r\n    at /usr/src/debug/rocclr-6.0.2-1.fc41.x86_64/rocclr/platform/kernel.cpp:127\r\n#1  0x00007fffb9822b7c in ihipLaunchKernel_validate (f=f@entry=0x3281e20, \r\n    globalWorkSizeX=globalWorkSizeX@entry=4096, \r\n    globalWorkSizeY=globalWorkSizeY@entry=1, \r\n    globalWorkSizeZ=globalWorkSizeZ@entry=1, blockDimX=blockDimX@entry=32, \r\n    blockDimY=blockDimY@entry=1, blockDimZ=1, sharedMemBytes=256, \r\n    kernelParams=0x7fffffff7430, extra=0x0, deviceId=0, params=0)\r\n    at /usr/src/debug/rocclr-6.0.2-1.fc41.x86_64/hipamd/src/hip_module.cpp:301\r\n#2  0x00007fffb98273fd in ihipModuleLaunchKernel (f=0x3281e20, \r\n    globalWorkSizeX=4096, globalWorkSizeY=1, globalWorkSizeZ=1, blockDimX=32, \r\n    blockDimY=1, blockDimZ=1, sharedMemBytes=256, hStream=0x195d320, \r\n    kernelParams=0x7fffffff7430, extra=0x0, startEvent=0x0, stopEvent=0x0, \r\n    flags=0, params=0, gridId=0, numGrids=0, prevGridSum=0, allGridSum=0, \r\n    firstDevice=0)\r\n    at /usr/src/debug/rocclr-6.0.2-1.fc41.x86_64/hipamd/src/hip_module.cpp:371\r\n#3  0x00007fffb98492a2 in ihipLaunchKernel (\r\n    hostFunction=0x679308 <void soft_max_f32<true, 32, 32>(float const*, float const*, float const*, float*, int, int, float, float, float, float, unsigned int)--Type <RET> for more, q to quit, c to continue without paging--\r\n>, gridDim=..., blockDim=..., args=0x7fffffff7430, sharedMemBytes=256, \r\n    stream=0x195d320, startEvent=0x0, stopEvent=0x0, flags=0)\r\n    at /usr/src/debug/rocclr-6.0.2-1.fc41.x86_64/hipamd/src/hip_platform.cpp:584\r\n#4  0x00007fffb9822519 in hipLaunchKernel_common (\r\n    hostFunction=hostFunction@entry=0x679308 <void soft_max_f32<true, 32, 32>(float const*, float const*, float const*, float*, int, int, float, float, float, float, unsigned int)>, gridDim=..., blockDim=..., \r\n    args=args@entry=0x7fffffff7430, sharedMemBytes=256, stream=<optimized out>)\r\n    at /usr/src/debug/rocclr-6.0.2-1.fc41.x86_64/hipamd/src/hip_module.cpp:662\r\n#5  0x00007fffb9824b83 in hipLaunchKernel (hostFunction=<optimized out>, \r\n    gridDim=..., blockDim=..., args=0x7fffffff7430, \r\n    sharedMemBytes=<optimized out>, stream=<optimized out>)\r\n    at /usr/src/debug/rocclr-6.0.2-1.fc41.x86_64/hipamd/src/hip_module.cpp:669\r\n#6  0x000000000062ea50 in void __device_stub__soft_max_f32<true, 32, 32>(float const*, float const*, float const*, float*, int, int, float, float, float, float, unsigned int) ()\r\n#7  0x000000000062e1f9 in soft_max_f32_cuda (x=0x7ff65e400800, \r\n    mask=0x7ff65c000800, pos=0x0, dst=0x7ff65e400800, ncols_x=32, nrows_x=128, \r\n    nrows_y=2, scale=0.0883883461, max_bias=0, stream=0x195d320)\r\n    at /llama.cpp/ggml-cuda.cu:7505\r\n#8  0x000000000062ded6 in ggml_cuda_op_soft_max (src0=0x7ff66eca9450, \r\n    src1=0x7ff66e80ee50, dst=0x7ff66eca95e0, src0_dd=0x7ff65e400800, \r\n    src1_dd=0x7ff65c000800, dst_dd=0x7ff65e400800, main_stream=0x195d320)\r\n    at /llama.cpp/ggml-cuda.cu:9053\r\n#9  0x00000000005f98f7 in ggml_cuda_op_flatten (src0=0x7ff66eca9450, \r\n    src1=0x7ff66e80ee50, dst=0x7ff66eca95e0, \r\n    op=0x62db50 <ggml_cuda_op_soft_max(ggml_tensor const*, ggml_tensor const*, ggml_tensor*, float const*, float const*, float*, ihipStream_t*)>)\r\n    at /llama.cpp/ggml-cuda.cu:9145\r\n#10 0x00000000005f856f in ggml_cuda_soft_max (src0=0x7ff66eca9450, \r\n    src1=0x7ff66e80ee50, dst=0x7ff66eca95e0)\r\n    at /llama.cpp/ggml-cuda.cu:10393\r\n#11 0x00000000005f5cb8 in ggml_cuda_compute_forward (params=0x7fffffff7b78, \r\n    tensor=0x7ff66eca95e0) at /llama.cpp/ggml-cuda.cu:10619\r\n#12 0x0000000000635106 in ggml_backend_cuda_graph_compute (backend=0x19e1420, \r\n    cgraph=0x7ff66e8002d8) at /llama.cpp/ggml-cuda.cu:11310\r\n#13 0x00000000005c1d42 in ggml_backend_graph_compute (backend=0x19e1420, \r\n    cgraph=0x7ff66e8002d8) at /llama.cpp/ggml-backend.c:270\r\n#14 0x00000000005c55c3 in ggml_backend_sched_compute_splits (\r\n--Type <RET> for more, q to quit, c to continue without paging--\r\n    sched=0x7ff66e800010) at /llama.cpp/ggml-backend.c:1474\r\n#15 0x00000000005c5237 in ggml_backend_sched_graph_compute (\r\n    sched=0x7ff66e800010, graph=0x7ff66ec00030)\r\n    at /llama.cpp/ggml-backend.c:1597\r\n#16 0x00000000004f85e9 in llama_graph_compute (lctx=..., gf=0x7ff66ec00030, \r\n    n_threads=10) at /llama.cpp/llama.cpp:8733\r\n#17 0x00000000004b7926 in llama_decode_internal (lctx=..., batch=...)\r\n    at /llama.cpp/llama.cpp:8887\r\n#18 0x00000000004b6fc3 in llama_decode (ctx=0x19f7b60, batch=...)\r\n    at /llama.cpp/llama.cpp:13837\r\n#19 0x0000000000452e95 in llama_init_from_gpt_params (params=...)\r\n    at /llama.cpp/common/common.cpp:1380\r\n#20 0x000000000042c0a5 in main (argc=18, argv=0x7fffffffdac8)\r\n    at /llama.cpp/examples/main/main.cpp:199\r\n```\r\n\r\nI saw some issues about partial offloading and also tried a smaller model which should completely fit on my GPU, but the segfault was still there, the smaller model is this one:\r\n\r\n```\r\nllm_load_print_meta: model type       = 13B\r\nllm_load_print_meta: model ftype      = Q8_0\r\nllm_load_print_meta: model params     = 13.02 B\r\nllm_load_print_meta: model size       = 12.88 GiB (8.50 BPW) \r\nllm_load_print_meta: general.name     = newhope.ggmlv3.q8_0.bin\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.28 MiB\r\nllm_load_tensors: offloading 40 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 41/41 layers to GPU\r\nllm_load_tensors:      ROCm0 buffer size = 13023.85 MiB\r\nllm_load_tensors:        CPU buffer size =   166.02 MiB\r\n```\r\n\r\nCrashed as well with a very similar backtrace.\r\n\r\nSince this is nicely reproducible, I can provide more more info or add some debug logs as needed, please let me know what you need.",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-03-13T01:47:52+00:00",
    "closed_at": "2024-03-14T18:46:32+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6031/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6031"
  },
  {
    "number": 6030,
    "title": "convert.py incompatible with most new models, including salesforce/codegen models",
    "body": "Most newer models on huggingface are unable to be converted with convert.py, including all the Salesforce/codegen, Salesforce/codegen2, and Salesforce/codegen25 models. Examples of output of different salesforce models below\r\n\r\n### codegen25-7b-multi\r\nhttps://huggingface.co/Salesforce/codegen25-7b-multi\r\n```\r\n(pythonenv) raptor85@raptor1 /var/storage/llama/llama.cpp $ python convert.py huggingface/Salesforce/codegen25-7b-multi --outfile models/salesforce-codegen25-7b-multi.gguf --outtype f16\r\nLoading model file huggingface/Salesforce/codegen25-7b-multi/pytorch_model-00001-of-00003.bin\r\nLoading model file huggingface/Salesforce/codegen25-7b-multi/pytorch_model-00001-of-00003.bin\r\nLoading model file huggingface/Salesforce/codegen25-7b-multi/pytorch_model-00002-of-00003.bin\r\nLoading model file huggingface/Salesforce/codegen25-7b-multi/pytorch_model-00003-of-00003.bin\r\nparams = Params(n_vocab=51200, n_embd=4096, n_layer=32, n_ctx=2048, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('huggingface/Salesforce/codegen25-7b-multi'))\r\nFound vocab files: {'spm': None, 'bpe': None, 'hfft': None}\r\nTraceback (most recent call last):\r\n  File \"/var/storage/llama/llama.cpp/convert.py\", line 1466, in <module>\r\n    main()\r\n  File \"/var/storage/llama/llama.cpp/convert.py\", line 1434, in main\r\n    vocab, special_vocab = vocab_factory.load_vocab(args.vocab_type.split(\",\"), model_parent_path)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/var/storage/llama/llama.cpp/convert.py\", line 1323, in load_vocab\r\n    vocab_type, path = self._select_file(vocab_types)\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/var/storage/llama/llama.cpp/convert.py\", line 1310, in _select_file\r\n    raise FileNotFoundError(f\"Could not find any of {[self._FILES[vt] for vt in vocab_types]}\")\r\nFileNotFoundError: Could not find any of ['tokenizer.model', 'tokenizer.json']\r\n```\r\n\r\n### codegen2-7b\r\nhttps://huggingface.co/Salesforce/codegen2-7B\r\n```\r\n(pythonenv) raptor85@raptor1 /var/storage/llama/llama.cpp $ python convert.py huggingface/Salesforce/codegen2-7B --outfile models/salesforce-codegen2-7b.gguf --outtype f16\r\nLoading model file huggingface/Salesforce/codegen2-7B/pytorch_model-00001-of-00003.bin\r\nTraceback (most recent call last):\r\n  File \"/var/storage/llama/llama.cpp/convert.py\", line 1466, in <module>\r\n    main()\r\n  File \"/var/storage/llama/llama.cpp/convert.py\", line 1402, in main\r\n    model_plus = load_some_model(args.model)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/var/storage/llama/llama.cpp/convert.py\", line 1278, in load_some_model\r\n    models_plus.append(lazy_load_file(path))\r\n                       ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/var/storage/llama/llama.cpp/convert.py\", line 887, in lazy_load_file\r\n    return lazy_load_torch_file(fp, path)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/var/storage/llama/llama.cpp/convert.py\", line 843, in lazy_load_torch_file\r\n    model = unpickler.load()\r\n            ^^^^^^^^^^^^^^^^\r\n  File \"/var/storage/llama/llama.cpp/convert.py\", line 832, in find_class\r\n    return self.CLASSES[(module, name)]\r\n           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^\r\nKeyError: ('torch', 'ByteStorage')\r\n```\r\n\r\n### Installed python packages\r\n```\r\nPackage                  Version\r\n------------------------ ----------\r\ncertifi                  2024.2.2\r\ncharset-normalizer       3.3.2\r\neinops                   0.7.0\r\nfilelock                 3.13.1\r\nfsspec                   2024.2.0\r\ngguf                     0.6.0\r\nhuggingface-hub          0.21.4\r\nidna                     3.6\r\nJinja2                   3.1.3\r\nMarkupSafe               2.1.5\r\nmpmath                   1.3.0\r\nnetworkx                 3.2.1\r\nnumpy                    1.24.4\r\nnvidia-cublas-cu12       12.1.3.1\r\nnvidia-cuda-cupti-cu12   12.1.105\r\nnvidia-cuda-nvrtc-cu12   12.1.105\r\nnvidia-cuda-runtime-cu12 12.1.105\r\nnvidia-cudnn-cu12        8.9.2.26\r\nnvidia-cufft-cu12        11.0.2.54\r\nnvidia-curand-cu12       10.3.2.106\r\nnvidia-cusolver-cu12     11.4.5.107\r\nnvidia-cusparse-cu12     12.1.0.106\r\nnvidia-nccl-cu12         2.18.1\r\nnvidia-nvjitlink-cu12    12.4.99\r\nnvidia-nvtx-cu12         12.1.105\r\npackaging                24.0\r\npip                      24.0\r\nprotobuf                 4.25.3\r\nPyYAML                   6.0.1\r\nregex                    2023.12.25\r\nrequests                 2.31.0\r\nsafetensors              0.4.2\r\nsentencepiece            0.1.99\r\nsetuptools               69.0.3\r\nsympy                    1.12\r\ntiktoken                 0.4.0\r\ntokenizers               0.15.2\r\ntorch                    2.1.2\r\ntqdm                     4.66.2\r\ntransformers             4.38.2\r\ntriton                   2.1.0\r\ntyping_extensions        4.10.0\r\nurllib3                  2.2.1\r\n```\r\n\r\n\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-13T00:57:03+00:00",
    "closed_at": "2024-04-27T01:06:32+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6030/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6030"
  },
  {
    "number": 5335,
    "title": "response dont see underscore ",
    "body": "Hello, when I use Llama.cpp as inference server, I found the result will have the following problem.\r\n\r\nfor example, if the expected result is a_b_c  but the result from Llama.cpp will be a_bc.   anyone know the reason? ",
    "labels": [
      "need more info",
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-02-05T08:43:11+00:00",
    "closed_at": "2024-05-04T01:06:43+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5335/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5335"
  },
  {
    "number": 6776,
    "title": "quantize.exe Bug(s) --token-embedding-type / --output-tensor-type and  - Docu? Advanced Usage Context ?",
    "body": "Windows 11. Use of quantize.exe - missing documentation?\r\n\r\nI am trying to locate information on:\r\n--include-weights tensor_name: use importance matrix for this/these tensor(s)\r\n--exclude-weights tensor_name: use importance matrix for this/these tensor(s)\r\n\r\nSpecifically the format of \"tensor_name(s)\" to be used and/or file to be provided and used with these options.\r\nIs it looking for a imatrix.dat or a file with \"tensor name(s) : Q6_K\" for example ?\r\n\r\nI can see the output and names during execution - just need to know what format(s) that \"--include-weights\" is expecting/valid.\r\nNot sure if this is a bug or not.\r\n\r\nSame for this ( BUG? ) :\r\n  --token-embedding-type ggml_type:\r\n  --output-tensor-type ggml_type:\r\n\r\nThese do not seem work when using \"Q8_0\", \"Q6_0\" etc etc as in:\r\n--token-embedding-type Q8_0\r\n--token-embedding-type ggml_type_Q8_0\r\n--token-embedding-type ggml_type:Q8_0\r\n\r\nExample:\r\n\r\n./quantize --output-tensor-type Q8_0 --token-embedding-type Q8_0 --imatrix imatrix.dat models/TinyLlama/ggml-model-f32.gguf models/TinyLlama/TinyLlama-IQ4_XS-we5.gguf IQ4_XS\r\n\r\nThis is what shows when I try to quant with these two flags set:\r\n\r\n[   1/ 201]                        output.weight - [ 2048, 32000,     1,     1], type =    f32,\r\n====== llama_model_quantize_internal: did not find weights for output.weight\r\nconverting to q6_K .. size =   250.00 MiB ->    51.27 MiB\r\n[   2/ 201]                    token_embd.weight - [ 2048, 32000,     1,     1], type =    f32,\r\n====== llama_model_quantize_internal: did not find weights for token_embd.weight\r\nconverting to iq4_xs .. size =   250.00 MiB ->    33.20 MiB\r\n\r\n(REF: https://github.com/ggerganov/llama.cpp/pull/6239 )\r\n\r\nNOTE:\r\n--leave-output-tensor   Works 100% (stays in FP 32/16)\r\n--pure  Works 100%.\r\n\r\nAnd example for:\r\n --override-kv KEY=TYPE:VALUE\r\n\r\nIf you can point me to the documentation and/or show a brief example that would be great.\r\nI have reviewed the programming code directly ; but it also does not reveal format(s) supported.\r\n\r\nFor \" --override-kv KEY=TYPE:VALUE \" ; a brief example or point to documentation would be great.\r\n\r\nThank you\r\n\r\nusage: F:\\llamacpp\\quantize.exe [--help] [--allow-requantize] [--leave-output-tensor] [--pure] [--imatrix] [--include-weights] [--exclude-weights] [--output-tensor-type] [--token-embedding-type] [--override-kv] model-f32.gguf [model-quant.gguf] type [nthreads]\r\n\r\n  --allow-requantize: Allows requantizing tensors that have already been quantized. Warning: This can severely reduce quality compared to quantizing from 16bit or 32bit\r\n  --leave-output-tensor: Will leave output.weight un(re)quantized. Increases model size but may also increase quality, especially when requantizing\r\n  --pure: Disable k-quant mixtures and quantize all tensors to the same type\r\n  --imatrix file_name: use data in file_name as importance matrix for quant optimizations\r\n  --include-weights tensor_name: use importance matrix for this/these tensor(s)\r\n  --exclude-weights tensor_name: use importance matrix for this/these tensor(s)\r\n  --output-tensor-type ggml_type: use this ggml_type for the output.weight tensor\r\n  --token-embedding-type ggml_type: use this ggml_type for the token embeddings tensor\r\n  --override-kv KEY=TYPE:VALUE\r\n      Advanced option to override model metadata by key in the quantized model. May be specified multiple times.\r\nNote: --include-weights and --exclude-weights cannot be used together\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-04-20T01:43:18+00:00",
    "closed_at": "2024-04-30T02:05:16+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6776/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6776"
  },
  {
    "number": 12729,
    "title": "Eval bug: Jinja not replacing `date_string`",
    "body": "### Name and Version\n\n```\n$ ~/llama.cpp/build/bin/llama-cli --version\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB MIG 7g.80gb, compute capability 8.0, VMM: yes\nversion: 5002 (2c3f8b85)\nbuilt with x86_64-conda-linux-gnu-cc (conda-forge gcc 11.4.0-13) 11.4.0 for x86_64-conda-linux-gnu\n```\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\nAMD EPYC 7742 64-Core Processor + A800-SXM4-80GB\n\n### Models\n\n_No response_\n\n### Problem description & steps to reproduce\n\nCompile llama.cpp from source and run it with `~/llama.cpp/build/bin/llama-server -m /models/Llama-3.3-70B-Instruct-Q8_0.gguf --port 8000 -t 8 -ngl 81 -c 15360 --jinja`\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n$ ~/llama.cpp/build/bin/llama-server -m /models/Llama-3.3-70B-Instruct-Q8_0.gguf --port 8000 -t 8 -ngl 81 -c 15360 --jinja\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA A800-SXM4-80GB MIG 7g.80gb, compute capability 8.0, VMM: yes\nbuild: 5002 (2c3f8b85) with x86_64-conda-linux-gnu-cc (conda-forge gcc 11.4.0-13) 11.4.0 for x86_64-conda-linux-gnu\nsystem info: n_threads = 8, n_threads_batch = 8, total_threads = 256\n\nsystem_info: n_threads = 8 (n_threads_batch = 8) / 256 | CUDA : ARCHS = 500,610,700,750,800 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n\nmain: binding port with default address family\nmain: HTTP server is listening, hostname: 127.0.0.1, port: 8000, http threads: 255\nmain: loading model\nsrv    load_model: loading model '/models/Llama-3.3-70B-Instruct-Q8_0.gguf'\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA A800-SXM4-80GB MIG 7g.80gb) - 80839 MiB free\nllama_model_loader: loaded meta data with 33 key-value pairs and 724 tensors from /models/Llama-3.3-70B-Instruct-Q8_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                         general.size_label str              = 71B\nllama_model_loader: - kv   3:                            general.license str              = llama3.3\nllama_model_loader: - kv   4:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   5:                  general.base_model.0.name str              = Llama 3.1 70B\nllama_model_loader: - kv   6:          general.base_model.0.organization str              = Meta Llama\nllama_model_loader: - kv   7:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...\nllama_model_loader: - kv   8:                               general.tags arr[str,5]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   9:                          general.languages arr[str,8]       = [\"en\", \"fr\", \"it\", \"pt\", \"hi\", \"es\", ...\nllama_model_loader: - kv  10:                          llama.block_count u32              = 80\nllama_model_loader: - kv  11:                       llama.context_length u32              = 131072\nllama_model_loader: - kv  12:                     llama.embedding_length u32              = 8192\nllama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 28672\nllama_model_loader: - kv  14:                 llama.attention.head_count u32              = 64\nllama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  16:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  18:                 llama.attention.key_length u32              = 128\nllama_model_loader: - kv  19:               llama.attention.value_length u32              = 128\nllama_model_loader: - kv  20:                          general.file_type u32              = 7\nllama_model_loader: - kv  21:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  22:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,280147]  = [\"\u0120 \u0120\", \"\u0120 \u0120\u0120\u0120\", \"\u0120\u0120 \u0120\u0120\", \"...\nllama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 128004\nllama_model_loader: - kv  31:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\nllama_model_loader: - kv  32:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  162 tensors\nllama_model_loader: - type q8_0:  562 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q8_0\nprint_info: file size   = 69.82 GiB (8.50 BPW) \nload: special tokens cache size = 256\nload: token to piece cache size = 0.7999 MB\nprint_info: arch             = llama\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 8192\nprint_info: n_layer          = 80\nprint_info: n_head           = 64\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 8\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 28672\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 500000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 70B\nprint_info: model params     = 70.55 B\nprint_info: general.name     = n/a\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 128256\nprint_info: n_merges         = 280147\nprint_info: BOS token        = 128000 '<|begin_of_text|>'\nprint_info: EOS token        = 128009 '<|eot_id|>'\nprint_info: EOT token        = 128009 '<|eot_id|>'\nprint_info: EOM token        = 128008 '<|eom_id|>'\nprint_info: PAD token        = 128004 '<|finetune_right_pad_id|>'\nprint_info: LF token         = 198 '\u010a'\nprint_info: EOG token        = 128008 '<|eom_id|>'\nprint_info: EOG token        = 128009 '<|eot_id|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 80 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 81/81 layers to GPU\nload_tensors:        CUDA0 model buffer size = 70429.66 MiB\nload_tensors:   CPU_Mapped model buffer size =  1064.62 MiB\n...................................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 15360\nllama_context: n_ctx_per_seq = 15360\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 500000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (15360) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:  CUDA_Host  output buffer size =     0.49 MiB\ninit: kv_size = 15360, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1\ninit:      CUDA0 KV buffer size =  4800.00 MiB\nllama_context: KV self size  = 4800.00 MiB, K (f16): 2400.00 MiB, V (f16): 2400.00 MiB\nllama_context:      CUDA0 compute buffer size =  2014.00 MiB\nllama_context:  CUDA_Host compute buffer size =    46.01 MiB\nllama_context: graph nodes  = 2726\nllama_context: graph splits = 2\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 15360\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nsrv          init: initializing slots, n_slots = 1\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 15360\nmain: model loaded\nmain: chat template, chat_template: {{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\"' + arg_val + '\"' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n            {{- '\"parameters\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n, example_format: '<|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nHello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nHi there<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nHow are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n'\nmain: server is listening on http://127.0.0.1:8000 - starting the main loop\nsrv  update_slots: all slots are idle\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-04-03T05:13:42+00:00",
    "closed_at": "2025-05-15T01:39:52+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12729/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12729"
  },
  {
    "number": 6676,
    "title": "Temperature slider not working",
    "body": "Tried in version b2671. The temperature slider doesn't seem to do anything no matter the model, even when cranked all the way to 2 which should produce gibberish but the model behaves coherently instead. All other frontend params are set to default using the button on the top.\r\n\r\n`.\\bin\\server.exe --model .\\mistral-7b-instruct-v0.2.Q4_0.gguf --ctx-size 2048 --n-gpu-layers 99 --log-disable --path .\\frontend`\r\n\r\nThe frontend folder is also updated to the latest as of today in examples/server/public.",
    "labels": [
      "server/webui",
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-14T16:43:44+00:00",
    "closed_at": "2024-06-10T01:34:27+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6676/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6676"
  },
  {
    "number": 8149,
    "title": "Bug: llama-cli templating does buf.resize(-1) if the model's template is not supported, causing crash",
    "body": "### What happened?\n\n`common.cpp`'s `llama_chat_apply_template` says:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/blob/ac146628e47451c531a3c7e62e6a973a2bb467a0/common/common.cpp#L2630-L2637\r\n\r\n`res` can be -1 (e.g. in the case that the model's Jinja template is not matched by any pattern in `llama_chat_apply_template_internal`?). When cast to `size_t` it becomes significantly bigger than `buf.size()` which leads to `buf.resize(-1)`. This is followed by a crash of `llama-cli` on my machine.\r\n\r\n`llama-server` seems to fall back to `chatml` if no pattern matches the model's Jinja template:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/blob/ac146628e47451c531a3c7e62e6a973a2bb467a0/examples/server/server.cpp#L2600-L2605\r\n\r\nPerhaps the same needs to be done for `llama-cli`, and perhaps `common.cpp`'s `llama_chat_apply_template` should be more defensive when `llama_chat_apply_template_internal` returns -1, rather than trying to resize `buf` to -1\n\n### Name and Version\n\n```\r\n$ ./llama-cli --version\r\nversion: 3246 (ac146628)\r\nbuilt with cc (Debian 12.2.0-14) 12.2.0 for x86_64-linux-gnu\r\n```\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\n$ ./llama-cli -m /models/bartowski/DeepSeek-Coder-V2-Lite-Instruct-GGUF/DeepSeek-Coder-V2-Lite-Instruct-Q6_K.gguf -c 8192\r\nLog start\r\nmain: build = 3246 (ac146628)\r\nmain: built with cc (Debian 12.2.0-14) 12.2.0 for x86_64-linux-gnu\r\nmain: seed  = 1719466734\r\nllama_model_loader: loaded meta data with 42 key-value pairs and 377 tensors from /models/bartowski/DeepSeek-Coder-V2-Lite-Instruct-GGUF/DeepSeek-Coder-V2-Lite-Instruct-Q6_K.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = deepseek2\r\nllama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct\r\nllama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27\r\nllama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840\r\nllama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048\r\nllama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944\r\nllama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16\r\nllama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16\r\nllama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000\r\nllama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 18\r\nllama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1\r\nllama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400\r\nllama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512\r\nllama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192\r\nllama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128\r\nllama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408\r\nllama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64\r\nllama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2\r\nllama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000\r\nllama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64\r\nllama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn\r\nllama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000\r\nllama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096\r\nllama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700\r\nllama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm\r\nllama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [\"\u0120 \u0120\", \"\u0120 t\", \"\u0120 a\", \"i n\", \"h e...\r\nllama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000\r\nllama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001\r\nllama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001\r\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\r\nllama_model_loader: - kv  37:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  38:                      quantize.imatrix.file str              = /models/DeepSeek-Coder-V2-Lite-Instru...\r\nllama_model_loader: - kv  39:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\r\nllama_model_loader: - kv  40:             quantize.imatrix.entries_count i32              = 293\r\nllama_model_loader: - kv  41:              quantize.imatrix.chunks_count i32              = 139\r\nllama_model_loader: - type  f32:  108 tensors\r\nllama_model_loader: - type q8_0:   27 tensors\r\nllama_model_loader: - type q6_K:  242 tensors\r\nllm_load_vocab: special tokens cache size = 2400\r\nllm_load_vocab: token to piece cache size = 0.6661 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = deepseek2\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 102400\r\nllm_load_print_meta: n_merges         = 99757\r\nllm_load_print_meta: n_ctx_train      = 163840\r\nllm_load_print_meta: n_embd           = 2048\r\nllm_load_print_meta: n_head           = 16\r\nllm_load_print_meta: n_head_kv        = 16\r\nllm_load_print_meta: n_layer          = 27\r\nllm_load_print_meta: n_rot            = 64\r\nllm_load_print_meta: n_embd_head_k    = 192\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 3072\r\nllm_load_print_meta: n_embd_v_gqa     = 2048\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 10944\r\nllm_load_print_meta: n_expert         = 64\r\nllm_load_print_meta: n_expert_used    = 6\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = yarn\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 0.025\r\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 16B\r\nllm_load_print_meta: model ftype      = Q6_K\r\nllm_load_print_meta: model params     = 15.71 B\r\nllm_load_print_meta: model size       = 13.10 GiB (7.16 BPW)\r\nllm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct\r\nllm_load_print_meta: BOS token        = 100000 '<\uff5cbegin\u2581of\u2581sentence\uff5c>'\r\nllm_load_print_meta: EOS token        = 100001 '<\uff5cend\u2581of\u2581sentence\uff5c>'\r\nllm_load_print_meta: PAD token        = 100001 '<\uff5cend\u2581of\u2581sentence\uff5c>'\r\nllm_load_print_meta: LF token         = 126 '\u00c4'\r\nllm_load_print_meta: max token length = 256\r\nllm_load_print_meta: n_layer_dense_lead   = 1\r\nllm_load_print_meta: n_lora_q             = 0\r\nllm_load_print_meta: n_lora_kv            = 512\r\nllm_load_print_meta: n_ff_exp             = 1408\r\nllm_load_print_meta: n_expert_shared      = 2\r\nllm_load_print_meta: expert_weights_scale = 1.0\r\nllm_load_print_meta: rope_yarn_log_mul    = 0.0707\r\nllm_load_tensors: ggml ctx size =    0.16 MiB\r\nllm_load_tensors:        CPU buffer size = 13411.50 MiB\r\n......................................................................................\r\nllama_new_context_with_model: n_ctx      = 8192\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 0.025\r\nllama_kv_cache_init:        CPU KV buffer size =  2160.00 MiB\r\nllama_new_context_with_model: KV self size  = 2160.00 MiB, K (f16): 1296.00 MiB, V (f16):  864.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.39 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1924\r\nllama_new_context_with_model: graph splits = 1\r\nterminate called after throwing an instance of 'std::length_error'\r\n  what():  vector::_M_default_append\r\nAborted (core dumped)\n```\n",
    "labels": [
      "bug-unconfirmed",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-06-27T05:55:33+00:00",
    "closed_at": "2024-06-27T16:14:20+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8149/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8149"
  },
  {
    "number": 9127,
    "title": "Bug: phi 3.5 mini produces garbage past 4096 context",
    "body": "### What happened?\n\nPhi 3.5 mini doesn't produce <|end|> or <|endoftext|> when the context is set higher than 4096, just endless garbage tokens.  Possible rope scale issue?\n\n### Name and Version\n\nllama-server, recent compile\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-08-22T01:16:01+00:00",
    "closed_at": "2024-10-25T01:28:13+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9127/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9127"
  },
  {
    "number": 5921,
    "title": "server: Unable to Utilize Models Outside of 'ChatML' with OpenAI Library",
    "body": "I'm unsure whether this is a limitation of the OpenAI library or a result of poor server management. However, after extensively testing various models using the latest server image in Docker with CUDA, I've come to a conclusion. It seems impossible to run a model that utilizes a chat template different from ChatML along with OpenAI library. All attempts resulted in failures in responses. This includes the model located at https://huggingface.co/mlabonne/AlphaMonarch-7B-GGUF, which I requested some time ago. I apologize if this isn't considered a bug, but I'm at a loss for what to do next. Thank you in advance.",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-07T11:31:07+00:00",
    "closed_at": "2024-04-21T01:06:36+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5921/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5921"
  },
  {
    "number": 12437,
    "title": "Misc. bug: Llama-Server is missing --Prompt-Cache from Llama-CLI",
    "body": "### Name and Version\n\nLlama-server is **missing** prompt caching from llama-cli.\n\nHere are the parameters in Llama-cli:\n\n```\n--prompt-cache FNAME                    file to cache prompt state for faster startup (default: none)\n--prompt-cache-all                      if specified, saves user input and generations to cache as well\n--prompt-cache-ro                       if specified, uses the prompt cache but does not update it\n```\n\nFigured it might be a bug as, for the most part, I've seen feature parity across these binaries. May we please have this addressed?\n\nThank you all.\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Command line\n\n```shell\n\n```\n\n### Problem description & steps to reproduce\n\nSee above \n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-03-17T23:40:29+00:00",
    "closed_at": "2025-03-20T17:00:52+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12437/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12437"
  },
  {
    "number": 7432,
    "title": "[server] phi-3 uses <|endoftext|> instead of <|end|> when applying chat template in /chat/completions",
    "body": "When using phi-3 without the option `--chat-template phi3`, the tokenization is incorrect. \r\n\r\nFor example, if I **do** use `--chat-template phi3`, here is the log output when I send the message \"hi\":\r\n```json\r\n{\r\n    \"level\": \"VERB\",\r\n    \"function\": \"update_slots\",\r\n    \"line\": 1954,\r\n    \"msg\": \"prompt tokenized\",\r\n    \"id_slot\": 0,\r\n    \"id_task\": 1,\r\n    \"n_ctx\": 8192,\r\n    \"n_keep\": 0,\r\n    \"n_prompt_tokens\": 7,\r\n    \"prompt_tokens\": \"<s><|system|><|end|><|user|> hi<|end|><|assistant|>\"\r\n}\r\n```\r\n\r\nactually the extra space after <|user|> is concerning, it should be a newline, but maybe that's just an artifact of how the log message is formatted.\r\n\r\nBut here's what happens when the `--chat-template phi3` is omitted:\r\n\r\n```json\r\n{\r\n    \"level\": \"VERB\",\r\n    \"function\": \"update_slots\",\r\n    \"line\": 1954,\r\n    \"msg\": \"prompt tokenized\",\r\n    \"id_slot\": 0,\r\n    \"id_task\": 0,\r\n    \"n_ctx\": 8192,\r\n    \"n_keep\": 0,\r\n    \"n_prompt_tokens\": 11,\r\n    \"prompt_tokens\": \"<s><|system|><|endoftext|> \\n<|user|> hi<|endoftext|> \\n<|assistant|>\"\r\n}\r\n```\r\n\r\nSee how it uses <|endoftext|> (wrong) instead of <|end|> (correct) which causes really bad generation.\r\n\r\nI am using the gguf straight from Microsoft, so I guess it is as official as it gets:\r\n\r\nhttps://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf\r\n\r\nPossibly the problem is in the gguf itself? Even so, it's weird that using the \"official\" gguf results in incorrect tokenization output from the template applied.\r\n\r\nNow, you could just always use `--chat-template phi3`. But my expectation is the `phi3` chat template should automatically be picked up by the detection heuristic, when using the canonical/official Phi-3 models, since they purport to support phi3.",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-05-21T06:53:47+00:00",
    "closed_at": "2024-05-23T14:15:16+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7432/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7432"
  },
  {
    "number": 4314,
    "title": "Prebuilt windows binaries (i.e. `server.exe`) have a dependency on `llama.dll` - can we remove that?",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Current Behavior & Expected Behavior\r\n\r\nI downloaded [llama-b1606-bin-win-noavx-x64.zip](https://github.com/ggerganov/llama.cpp/releases/download/b1606/llama-b1606-bin-win-noavx-x64.zip) from the [latest release](https://github.com/ggerganov/llama.cpp/releases/tag/b1606) and would like to run it in my application. If I run `server.exe` from the downloaded directory, which has `llama.dll` next to it, it runs fine. But when I copy over `server.exe` somewhere else and run it I get:\r\n```\r\nC:/repo/foo/apps/desktop/llamacpp/engine/server_win32_noavx.exe: \r\nerror while loading shared libraries: llama.dll: cannot open shared object file: No such file or directory\r\n```\r\n\r\nThis is an issue because in my app I want to support all variants of avx and acceleration libraries, and with the current configuration I believe I would need to 1) create a new folder for each `server.exe` variant 2) copy over both `server.exe` and `llama.dll`. This is possible, but not as easy as just dealing with standalone `server.exe` files. So, before doing the workaround proposed above, I was wondering if we could include `llama.dll` in `server.exe`?\r\n\r\nFor reference, this is what [Dependency Walker](https://www.dependencywalker.com/) shows for the `server.exe` from the prebuilt libraries download - note that `llama.dll` is there:\r\n\r\n<img width=\"800\" alt=\"image\" src=\"https://github.com/ggerganov/llama.cpp/assets/1396242/525a287d-77d4-487d-99b5-07b3e1f30ddd\">\r\n\r\nAnd this is what it shows if I build for Windows with CMake and no other flags (i.e. follow [the README instructions](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#build)):\r\n\r\n<img width=\"800\" alt=\"image\" src=\"https://github.com/ggerganov/llama.cpp/assets/1396242/d4e67cfc-d49e-4830-b4d5-ace85e643a02\">\r\n\r\nThe one caveat here is that perhaps I misunderstood the build and `llama.dll` can be used across avx and acceleration variants? And it's just `server.exe` which needs to change? Either way, thank you for any help.\r\n\r\n# Environment and Context\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux: Windows\r\n* Operating System, e.g. for Linux: Windows 11\r\n\r\n# Steps to Reproduce\r\n\r\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\r\n\r\n1. Download  [llama-b1606-bin-win-noavx-x64.zip](https://github.com/ggerganov/llama.cpp/releases/download/b1606/llama-b1606-bin-win-noavx-x64.zip)\r\n2. Copy `server.exe` to another folder\r\n3. Run `server.exe`\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2023-12-03T22:00:54+00:00",
    "closed_at": "2023-12-04T17:38:15+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4314/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4314"
  },
  {
    "number": 12551,
    "title": "Misc. bug: Gemma3 adapter gguf conversion fails",
    "body": "### Name and Version\n\n\n```bash\n>llama-cli --version\nversion: 4948 (00d53800)\nbuilt with MSVC 19.43.34808.0 for x64\n\n>pip list\nPackage                      Version\n---------------------------- ---------------\nabsl-py                      0.11.0\naccelerate                   0.27.2\naddict                       2.4.0\naggdraw                      1.3.18.post0\naiohttp                      3.9.5\naiosignal                    1.3.1\nannotated-types              0.7.0\nansicon                      1.89.0\nantlr4-python3-runtime       4.9.3\nanyio                        4.3.0\nasync-timeout                4.0.3\nattrs                        23.2.0\nbeautifulsoup4               4.13.3\nblend_modes                  2.1.0\nblessed                      1.20.0\nblind-watermark              0.4.4\nblobfile                     3.0.0\nBrotli                       1.1.0\nbs4                          0.0.2\ncachetools                   5.3.3\ncertifi                      2024.2.2\ncffi                         1.16.0\ncharset-normalizer           3.3.2\nclick                        8.1.7\ncolorama                     0.4.6\ncolour-science               0.4.4\ncomfy_api_simplified         1.1.2\ncomfy-cli                    1.3.5\ncontourpy                    1.2.1\ncssutils                     2.11.1\ncycler                       0.12.1\ndatasets                     3.4.1\ndiffusers                    0.29.0\ndill                         0.3.8\ndistlib                      0.3.8\ndistro                       1.9.0\ndnspython                    2.6.1\ndocopt                       0.6.2\nEbookLib                     0.17.1\neditor                       1.6.6\neinops                       0.7.0\nemail_validator              2.1.2\nepub2txt                     0.1.6\nexceptiongroup               1.2.0\nfairscale                    0.4.13\nfastapi                      0.111.0\nfastapi-cli                  0.0.4\nfilelock                     3.13.1\nfire                         0.7.0\nflatbuffers                  24.3.25\nfontpls                      0.1.0\nfonttools                    4.53.0\nfrozenlist                   1.4.1\nfsspec                       2024.3.1\nfuture                       1.0.0\ngdown                        5.2.0\ngguf                         0.6.0\ngitdb                        4.0.12\nGitPython                    3.1.44\ngoogle-ai-generativelanguage 0.6.4\ngoogle-api-core              2.19.0\ngoogle-api-python-client     2.133.0\ngoogle-auth                  2.30.0\ngoogle-auth-httplib2         0.2.0\ngoogle-generativeai          0.6.0\ngoogleapis-common-protos     1.63.1\ngrpcio                       1.64.1\ngrpcio-status                1.62.2\nh11                          0.14.0\nhttpcore                     1.0.5\nhttplib2                     0.22.0\nhttptools                    0.6.1\nhttpx                        0.27.0\nhuggingface-hub              0.29.3\nidna                         3.6\nimage-reward                 1.5\nimageio                      2.34.1\nimportlib_metadata           7.1.0\ninquirer                     3.4.0\ninquirerpy                   0.3.4\nintel-openmp                 2021.4.0\njax                          0.4.29\njaxlib                       0.4.29\nJinja2                       3.1.2\njinxed                       1.3.0\njsonschema                   4.23.0\njsonschema-specifications    2024.10.1\nkiwisolver                   1.4.5\nkornia                       0.7.2\nkornia_rs                    0.1.3\nlazy_loader                  0.4\nllama_models                 0.1.3\nllama_stack                  0.1.3\nllama_stack_client           0.1.3\nllvmlite                     0.43.0\nlmdb                         1.4.1\nloguru                       0.7.2\nlogzero                      1.7.0\nlpips                        0.1.4\nlxml                         5.1.1\nMarkdown                     3.5.2\nmarkdown-it-py               3.0.0\nMarkupSafe                   2.1.3\nmatplotlib                   3.9.0\nmdurl                        0.1.2\nmediapipe                    0.10.14\nmergoo                       0.0.10\nmixpanel                     4.10.1\nmkl                          2021.4.0\nml-dtypes                    0.4.0\nmore-itertools               10.6.0\nmpmath                       1.3.0\nmultidict                    6.0.5\nmultiprocess                 0.70.16\nnetworkx                     3.2.1\nnumba                        0.60.0\nnumpy                        1.24.4\nollama                       0.3.0\nomegaconf                    2.3.0\nopencv-contrib-python        4.10.0.84\nopencv-python                4.10.0.82\nopt-einsum                   3.3.0\norca-cli                     0.1.0\norjson                       3.10.5\npackaging                    24.0\npandas                       2.2.2\npathspec                     0.12.1\npdfkit                       1.0.0\npeft                         0.14.0\npfzy                         0.3.4\npillow                       10.2.0\npip                          24.0\npipenv                       2023.12.1\nplatformdirs                 4.2.0\nprompt_toolkit               3.0.50\nproto-plus                   1.23.0\nprotobuf                     4.25.3\npsd-tools                    1.9.33\npsutil                       5.9.8\npy-cpuinfo                   9.0.0\npyaml                        25.1.0\npyarrow                      16.1.0\npyarrow-hotfix               0.6\npyasn1                       0.6.0\npyasn1_modules               0.4.0\npycparser                    2.22\npycryptodomex                3.21.0\npydantic                     2.7.4\npydantic_core                2.18.4\nPygments                     2.18.0\nPyMatting                    1.1.12\npyparsing                    3.1.2\npypng                        0.20220715.0\nPySocks                      1.7.1\npython-dateutil              2.9.0.post0\npython-dotenv                1.0.1\npython-multipart             0.0.9\npytz                         2024.1\nPyWavelets                   1.6.0\nPyYAML                       6.0.1\npyzbar                       0.1.9\nqrcode                       7.4.2\nquestionary                  2.1.0\nreadchar                     4.2.1\nreferencing                  0.36.2\nregex                        2024.4.28\nrequests                     2.32.3\nrich                         13.9.4\nrpds-py                      0.23.0\nrsa                          4.9\nruff                         0.9.4\nruns                         1.2.2\nsafetensors                  0.4.3\nscikit-image                 0.23.2\nscipy                        1.13.1\nseaborn                      0.13.2\nsegment-anything             1.0\nsemver                       3.0.4\nsentencepiece                0.1.99\nsetuptools                   69.1.1\nshellingham                  1.5.4\nsix                          1.16.0\nsmmap                        5.0.2\nsniffio                      1.3.1\nsounddevice                  0.4.7\nsoupsieve                    2.5\nstarlette                    0.37.2\nsympy                        1.12\ntb-nightly                   2.18.0a20240611\ntbb                          2021.12.0\ntensorboard-data-server      0.7.2\ntermcolor                    2.5.0\ntifffile                     2024.5.22\ntiktoken                     0.9.0\ntimm                         0.6.13\ntokenizers                   0.21.0\ntomli                        2.0.1\ntomlkit                      0.13.2\ntorch                        2.3.1\ntorchaudio                   2.2.1+cu118\ntorchvision                  0.18.1\ntqdm                         4.67.1\ntransformers                 4.50.0.dev0\ntyper                        0.15.2\ntyper-config                 1.4.0\ntyping_extensions            4.8.0\ntzdata                       2024.1\nujson                        5.10.0\nultralytics                  8.2.35\nultralytics-thop             2.0.0\nuritemplate                  4.1.1\nurllib3                      2.2.1\nuv                           0.5.26\nuvicorn                      0.30.1\nvastai                       0.2.2\nvirtualenv                   20.25.1\nwatchfiles                   0.22.0\nwcwidth                      0.2.13\nwebsocket-client             1.8.0\nwebsockets                   12.0\nWerkzeug                     3.0.3\nwget                         3.2\nwin32-setctime               1.1.0\nxmod                         1.8.1\nxxhash                       3.4.1\nyapf                         0.40.2\nyarl                         1.9.4\nzipp                         3.19.2\n```\n\n### Operating systems\n\nWindows\n\n### Which llama.cpp modules do you know to be affected?\n\nPython/Bash scripts\n\n### Command line\n\n```shell\npython C:\\tools\\llama.cpp2\\convert_lora_to_gguf.py .\n```\n\n### Problem description & steps to reproduce\n\nI can reproduce the issue the following way:\n\n1. I trained a peft adater using unsloth for the model `google/gemma3-4b-it`\n2. I uploaded the files to https://huggingface.co/molbal/CRA-v1.2-Guided-4B/tree/main (Including adapter_config.json and adapter_model.safetensors)\n3. I installed the latest transformers via pip\n4. I pulled the latest llama.cpp version\n5. I executed the command `python C:\\tools\\llama.cpp2\\convert_lora_to_gguf.py .` within the directory where the adapter files are placed\n6. I see the following output:\n```D:\\ML\\training\\ungpt-v2\\weighs\\v1.x\\cra-v1p2-guided-4b\\guided>python C:\\tools\\llama.cpp2\\convert_lora_to_gguf.py .\nINFO:lora-to-gguf:Loading base model from Hugging Face: unsloth/gemma-3-4b-it-unsloth-bnb-4bit\nTraceback (most recent call last):\n  File \"C:\\tools\\llama.cpp2\\convert_lora_to_gguf.py\", line 446, in <module>\n    model_instance = LoraModel(\n  File \"C:\\tools\\llama.cpp2\\convert_lora_to_gguf.py\", line 359, in __init__\n    super().__init__(*args, **kwargs)\n  File \"C:\\tools\\llama.cpp2\\convert_hf_to_gguf.py\", line 3388, in __init__\n    hparams = Model.load_hparams(kwargs[\"dir_model\"])\nKeyError: 'dir_model'\n```\n\nExpected outcome:\nI expected a gguf model to be written into the same directory.\n\n### First Bad Commit\n\nI could never get Gemma3 GGUF conversion working. I tried version 4942 and version 4948 (which is the latest at the time of reporting this issue)\n\n### Relevant log output\n\n```shell\nINFO:lora-to-gguf:Loading base model from Hugging Face: unsloth/gemma-3-4b-it-unsloth-bnb-4bit\nTraceback (most recent call last):\n  File \"C:\\tools\\llama.cpp2\\convert_lora_to_gguf.py\", line 446, in <module>\n    model_instance = LoraModel(\n  File \"C:\\tools\\llama.cpp2\\convert_lora_to_gguf.py\", line 359, in __init__\n    super().__init__(*args, **kwargs)\n  File \"C:\\tools\\llama.cpp2\\convert_hf_to_gguf.py\", line 3388, in __init__\n    hparams = Model.load_hparams(kwargs[\"dir_model\"])\nKeyError: 'dir_model'\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-03-24T18:34:49+00:00",
    "closed_at": "2025-03-25T22:03:11+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12551/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12551"
  },
  {
    "number": 3973,
    "title": "Error C2026 when building ggml-opencl.cpp with MSVC",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\nMSVC can't handle long string litterals, so it throws out [Error C2026](https://learn.microsoft.com/en-us/cpp/error-messages/compiler-errors-1/compiler-error-c2026?view=msvc-170) when compiling the long strings that are constructed with the macro `MULTILINE_QUOTE( ... )` .\r\n\r\nThere is an easy, but ugly fix that I found, which consists of breaking up the long blocks inside `MULTILINE_QUOTE( ... )`  with some  instances of `)MULTILINE_QUOTE( ` .\r\nFor example \r\n```c++\r\nstatic std::string hello_world = MULTILINE_QUOTE(\r\n#include <iostream>\r\n\r\n// please pretend this is a very long block of code\r\nint main() {\r\n    std::cout << \"Hello World!\";\r\n    return 0;\r\n}\r\n)\r\n```\r\nwould become \r\n```c++\r\nstatic std::string hello_world = MULTILINE_QUOTE(\r\n#include <iostream>\r\n\r\n)MULTILINE_QUOTE(\r\n\r\n// please pretend this is a very long block of code\r\nint main() {\r\n    std::cout << \"Hello World!\";\r\n)MULTILINE_QUOTE(\r\n    return 0;\r\n}\r\n)\r\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-11-06T21:22:11+00:00",
    "closed_at": "2024-04-02T01:12:09+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3973/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3973"
  },
  {
    "number": 6569,
    "title": "The output of the main service is inconsistent with that of the server service",
    "body": "**When the same quantitative model is used for server service and main service, some specific words are answered differently. It seems that the input specific words are not received or received incorrectly.\r\nFor example, BYD, Tesla, Lexus and other car names have this problem, such as Geely, BMW, Audi and so on is normal.**\r\nThe specific problem is manifested in: When obtaining the word \"BYD\" in the server service, non-Chinese characters such as \"ruit\" are not obtained or obtained. As in the first example, when asked about BYD car, the reply only involved the car, and BYD was lost.\r\n**Test results in the server**\r\n********************************************************\r\n**These are three examples of problems\uff08BYD\uff09**\r\n********************************************************\r\n{\r\n  content: ' \u6c7d\u8f66\u662f\u4e00\u79cd\u4ea4\u901a\u5de5\u5177\uff0c\u5b83\u901a\u5e38\u7531\u53d1\u52a8\u673a\uff0c\u53d8\u901f\u7bb1\uff0c\u5e95\u76d8\u548c\u5e95\u76d8\u7cfb\u7edf\uff0c\u60ac\u6302\u7cfb\u7edf\uff0c\u8f6c\u5411\u7cfb\u7edf\uff0c\u8f66\u8eab\u548c\u8f66\u8f6e\u7b49\u7ec4\u6210\u3002\u6c7d\u8f66\u901a\u5e38\u7531\u6c7d\u6cb9\u6216\u67f4\u6cb9\u53d1\u52a8\u673a\u63d0\u4f9b\u52a8\u529b\uff0c\u901a\u8fc7\u53d8\u901f\u7bb1\u548c\u4f20\u52a8\u7cfb\u7edf\u6765\u63a7\u5236\u8f66\u8f86\u884c\u9a76\u7684\u901f\u5ea6\u548c\u65b9\u5411\u3002\u6c7d\u8f66\u7684\u8bbe\u8ba1\u548c\u5236\u9020\u6280\u672f\u4e0d\u65ad\u63d0\u9ad8\uff0c\u6c7d\u8f66\u7684\u529f\u80fd\u4e5f\u8d8a\u6765\u8d8a\u5f3a\u5927\u3002\u73b0\u5728\u6c7d\u8f66\u5df2\u7ecf\u4e0d\u4ec5\u4ec5\u662f\u4e00\u79cd\u4ea4\u901a\u5de5\u5177\uff0c\u5b83\u5df2\u7ecf\u6210\u4e3a\u4eba\u4eec\u65e5\u5e38\u751f\u6d3b\u4e2d\u4e0d\u53ef\u6216\u7f3a\u7684\u4e00\u90e8\u5206\uff0c\u63d0\u4f9b\u4e86\u5404\u79cd\u4fbf\u5229\u3002\u6c7d\u8f66\u5728\u73b0\u4ee3\u793e\u4f1a\u4e2d\u7684\u4f5c\u7528\u975e\u5e38\u5e7f\u6cdb\uff0c\u5b83\u53ef\u4ee5\u6ee1\u8db3\u4eba\u4eec\u7684\u51fa\u884c\u9700\u6c42\uff0c\u540c\u65f6\u4e5f\u53ef\u4ee5\u5a31\u4e50\u4f11\u95f2\uff0c\u63d0\u9ad8\u751f\u6d3b\u8d28\u91cf\u3002\u6c7d\u8f66\u7684\u4f7f\u7528\u4e5f\u5e26\u6765\u4e86\u4e00\u4e9b\u8d1f\u9762\u5f71\u54cd\uff0c\u5982\u7a7a\u6c14\u6c61\u67d3\uff0c\u4ea4\u901a\u62e5\u5835\uff0c\u4ea4\u901a\u4e8b\u6545\u7b49\u3002\u56e0\u6b64\uff0c\u6c7d\u8f66\u7684\u4f7f\u7528\u5e94\u8be5\u66f4\u52a0\u7406\u6027\uff0c\u66f4\u52a0\u5b89\u5168\uff0c\u66f4\u52a0\u73af\u4fdd\u3002',\r\n  id_slot: 0,\r\n  stop: true,\r\n  model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n  tokens_predicted: 153,\r\n  tokens_evaluated: 40,\r\n  generation_settings: {\r\n    n_ctx: 4096,\r\n    n_predict: -1,\r\n    model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n    seed: 4294967295,\r\n    temperature: 0.800000011920929,\r\n    dynatemp_range: 0,\r\n    dynatemp_exponent: 1,\r\n    top_k: 40,\r\n    top_p: 0.949999988079071,\r\n    min_p: 0.05000000074505806,\r\n    tfs_z: 1,\r\n    typical_p: 1,\r\n    repeat_last_n: 64,\r\n    repeat_penalty: 1,\r\n    presence_penalty: 0,\r\n    frequency_penalty: 0,\r\n    penalty_prompt_tokens: [],\r\n    use_penalty_prompt_tokens: false,\r\n    mirostat: 0,\r\n    mirostat_tau: 5,\r\n    mirostat_eta: 0.10000000149011612,\r\n    penalize_nl: false,\r\n    stop: [],\r\n    n_keep: 1,\r\n    n_discard: 0,\r\n    ignore_eos: false,\r\n    stream: false,\r\n    logit_bias: [],\r\n    n_probs: 0,\r\n    min_keep: 0,\r\n    grammar: '',\r\n    samplers: [ 'top_k', 'tfs_z', 'typical_p', 'top_p', 'min_p', 'temperature' ]\r\n  },\r\n  prompt: '[INST] <<SYS>>\\n' +\r\n    'You are a helpful assistant. \u4f60\u662f\u4e00\u4e2a\u4e50\u4e8e\u52a9\u4eba\u7684\u52a9\u624b\u3002\\n' +\r\n    '<</SYS>>\\n' +\r\n    '\\n' +\r\n    '\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u6bd4\u4e9a\u8fea\u6c7d\u8f66[/INST]',\r\n  truncated: false,\r\n  stopped_eos: true,\r\n  stopped_word: false,\r\n  stopped_limit: false,\r\n  stopping_word: '',\r\n  tokens_cached: 192,\r\n  timings: {\r\n    prompt_n: 40,\r\n    prompt_ms: 71.919,\r\n    prompt_per_token_ms: 1.7979749999999999,\r\n    prompt_per_second: 556.1812594724621,\r\n    predicted_n: 153,\r\n    predicted_ms: 1859.683,\r\n    predicted_per_token_ms: 12.154790849673203,\r\n    predicted_per_second: 82.27208615661917\r\n  }\r\n}\r\n********************************************************\r\n{\r\n  content: ' \u5f88\u62b1\u6b49\uff0c\u4f46\u662f\u6211\u65e0\u6cd5\u51c6\u786e\u56de\u7b54\u60a8\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u60a8\u6ca1\u6709\u63d0\u4f9b\u4efb\u4f55\u5173\u4e8e\u5b83\u7684\u4fe1\u606f\u3002\u6211\u9700\u8981\u77e5\u9053\u4ec0\u4e48\u662f\"ruit\"\u6765\u5e2e\u52a9\u60a8\u3002',\r\n  id_slot: 0,\r\n  stop: true,\r\n  model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n  tokens_predicted: 32,\r\n  tokens_evaluated: 39,\r\n  generation_settings: {\r\n    n_ctx: 4096,\r\n    n_predict: -1,\r\n    model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n    seed: 4294967295,\r\n    temperature: 0.800000011920929,\r\n    dynatemp_range: 0,\r\n    dynatemp_exponent: 1,\r\n    top_k: 40,\r\n    top_p: 0.949999988079071,\r\n    min_p: 0.05000000074505806,\r\n    tfs_z: 1,\r\n    typical_p: 1,\r\n    repeat_last_n: 64,\r\n    repeat_penalty: 1,\r\n    presence_penalty: 0,\r\n    frequency_penalty: 0,\r\n    penalty_prompt_tokens: [],\r\n    use_penalty_prompt_tokens: false,\r\n    mirostat: 0,\r\n    mirostat_tau: 5,\r\n    mirostat_eta: 0.10000000149011612,\r\n    penalize_nl: false,\r\n    stop: [],\r\n    n_keep: 1,\r\n    n_discard: 0,\r\n    ignore_eos: false,\r\n    stream: false,\r\n    logit_bias: [],\r\n    n_probs: 0,\r\n    min_keep: 0,\r\n    grammar: '',\r\n    samplers: [ 'top_k', 'tfs_z', 'typical_p', 'top_p', 'min_p', 'temperature' ]\r\n  },\r\n  prompt: '[INST] <<SYS>>\\n' +\r\n    'You are a helpful assistant. \u4f60\u662f\u4e00\u4e2a\u4e50\u4e8e\u52a9\u4eba\u7684\u52a9\u624b\u3002\\n' +\r\n    '<</SYS>>\\n' +\r\n    '\\n' +\r\n    '\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u6bd4\u4e9a\u8fea[/INST]',\r\n  truncated: false,\r\n  stopped_eos: true,\r\n  stopped_word: false,\r\n  stopped_limit: false,\r\n  stopping_word: '',\r\n  tokens_cached: 70,\r\n  timings: {\r\n    prompt_n: 39,\r\n    prompt_ms: 50.617,\r\n    prompt_per_token_ms: 1.2978717948717948,\r\n    prompt_per_second: 770.4921271509572,\r\n    predicted_n: 32,\r\n    predicted_ms: 382.638,\r\n    predicted_per_token_ms: 11.9574375,\r\n    predicted_per_second: 83.629958341827\r\n  }\r\n}\r\n********************************************************\r\n{\r\n  content: ' \u9a71\u9010\u823005\u6c7d\u8f66\uff08Discharged Ship05\uff09\u662f\u4e00\u6b3e\u7531\u4e2d\u56fd\u957f\u57ce\u6c7d\u8f66\u5236\u9020\u7684\u7eaf\u7535\u52a8\u7d27\u51d1\u578b\u8f7f\u8f66\u3002\u5b83\u7684\u5916\u89c2\u8bbe\u8ba1\u7075\u611f\u6765\u81ea\u9a71\u9010\u823005\u7cfb\u5217\u7684\u9a71\u9010\u8230\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u8fa8\u8bc6\u5ea6\u3002\u9a71\u9010\u823005\u6c7d\u8f66\u91c7\u7528\u4e09\u5143\u9502\u79bb\u5b50\u7535\u6c60\uff0c\u80fd\u591f\u63d0\u4f9b\u9ad8\u6548\u7684\u7eed\u822a\u80fd\u529b\uff0c\u6700\u5927\u65f6\u901f\u53ef\u8fbe160\u516c\u91cc\u3002\u5b83\u8fd8\u62e5\u6709\u5148\u8fdb\u7684\u667a\u80fd\u63a7\u5236\u7cfb\u7edf\uff0c\u80fd\u591f\u6839\u636e\u8def\u51b5\u548c\u9a7e\u9a76\u9700\u6c42\uff0c\u81ea\u52a8\u8c03\u6574\u8f66\u8f86\u7684\u52a0\u901f\u3001\u5239\u8f66\u548c\u8f6c\u5411\u7b49\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5b83\u7684\u5145\u7535\u65f6\u95f4\u77ed\uff0c\u7eed\u822a\u80fd\u529b\u5f3a\uff0c\u662f\u4e00\u6b3e\u503c\u5f97\u8d2d\u4e70\u7684\u7eaf\u7535\u52a8\u7d27\u51d1\u578b\u8f7f\u8f66\u3002',\r\n  id_slot: 0,\r\n  stop: true,\r\n  model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n  tokens_predicted: 125,\r\n  tokens_evaluated: 45,\r\n  generation_settings: {\r\n    n_ctx: 4096,\r\n    n_predict: -1,\r\n    model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n    seed: 4294967295,\r\n    temperature: 0.800000011920929,\r\n    dynatemp_range: 0,\r\n    dynatemp_exponent: 1,\r\n    top_k: 40,\r\n    top_p: 0.949999988079071,\r\n    min_p: 0.05000000074505806,\r\n    tfs_z: 1,\r\n    typical_p: 1,\r\n    repeat_last_n: 64,\r\n    repeat_penalty: 1,\r\n    presence_penalty: 0,\r\n    frequency_penalty: 0,\r\n    penalty_prompt_tokens: [],\r\n    use_penalty_prompt_tokens: false,\r\n    mirostat: 0,\r\n    mirostat_tau: 5,\r\n    mirostat_eta: 0.10000000149011612,\r\n    penalize_nl: false,\r\n    stop: [],\r\n    n_keep: 1,\r\n    n_discard: 0,\r\n    ignore_eos: false,\r\n    stream: false,\r\n    logit_bias: [],\r\n    n_probs: 0,\r\n    min_keep: 0,\r\n    grammar: '',\r\n    samplers: [ 'top_k', 'tfs_z', 'typical_p', 'top_p', 'min_p', 'temperature' ]\r\n  },\r\n  prompt: '[INST] <<SYS>>\\n' +\r\n    'You are a helpful assistant. \u4f60\u662f\u4e00\u4e2a\u4e50\u4e8e\u52a9\u4eba\u7684\u52a9\u624b\u3002\\n' +\r\n    '<</SYS>>\\n' +\r\n    '\\n' +\r\n    '\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u6bd4\u4e9a\u8fea\u9a71\u9010\u823005\u6c7d\u8f66[/INST]',\r\n  truncated: false,\r\n  stopped_eos: true,\r\n  stopped_word: false,\r\n  stopped_limit: false,\r\n  stopping_word: '',\r\n  tokens_cached: 169,\r\n  timings: {\r\n    prompt_n: 45,\r\n    prompt_ms: 51.557,\r\n    prompt_per_token_ms: 1.1457111111111111,\r\n    prompt_per_second: 872.8203735671199,\r\n    predicted_n: 125,\r\n    predicted_ms: 1518.842,\r\n    predicted_per_token_ms: 12.150736,\r\n    predicted_per_second: 82.29954136111589\r\n  }\r\n}\r\n********************************************************\r\n**These are two correct examples\uff08Geely\uff09**\r\n********************************************************\r\n{\r\n  content: ' \u5409\u5229\u6c7d\u8f66\u662f\u4e00\u5bb6\u4f4d\u4e8e\u4e2d\u56fd\u7684\u6c7d\u8f66\u5236\u9020\u5546\uff0c\u6210\u7acb\u4e8e1946\u5e74\u3002\u5409\u5229\u662f\u4e00\u5bb6\u72ec\u7acb\u6c7d\u8f66\u5236\u9020\u5546\uff0c\u5b83\u751f\u4ea7\u4e86\u8bb8\u591a\u6210\u529f\u7684\u6c7d\u8f66\uff0c\u5305\u62ec\u8f7f\u8f66\uff0cSUV\uff0cMPV\u548c\u7d27\u51d1\u578b\u8f66\u3002\\n' +\r\n    '\\n' +\r\n    '\u5409\u5229\u6c7d\u8f66\u5728\u6c7d\u8f66\u8bbe\u8ba1\u548c\u5236\u9020\u65b9\u9762\u62e5\u6709\u5f88\u9ad8\u7684\u58f0\u8a89\uff0c\u5b83\u7684\u8f66\u578b\u53d7\u5230\u8bb8\u591a\u6d88\u8d39\u8005\u7684\u559c\u7231\u3002\u5409\u5229\u6c7d\u8f66\u7684\u54c1\u724c\u5f62\u8c61\u4e5f\u5f97\u5230\u4e86\u63d0\u9ad8\uff0c\u5728\u5168\u7403\u8303\u56f4\u5185\u90fd\u6709\u5f88\u597d\u7684\u58f0\u8a89\u3002\\n' +\r\n    '\\n' +\r\n    '\u5409\u5229\u6c7d\u8f66\u7684\u8f66\u578b\u5305\u62ec\u5409\u5229GS4\uff0c\u5409\u5229GS5\uff0c\u5409\u5229GX7\uff0c\u5409\u5229M6\u7b49\u3002\u8fd9\u4e9b\u8f66\u578b\u90fd\u5177\u6709\u65f6\u5c1a\u7684\u5916\u89c2\uff0c\u9ad8\u8d28\u91cf\u7684\u5185\u9970\u548c\u51fa\u8272\u7684\u6027\u80fd\u3002\\n' +\r\n    '\\n' +\r\n    '\u5409\u5229\u6c7d\u8f66\u7684\u751f\u4ea7\u57fa\u5730\u904d\u5e03\u4e8e\u4e2d\u56fd\u5404\u5730\uff0c\u5176\u4e2d\u5409\u5229\u6c7d\u8f66\u7684\u603b\u90e8\u4f4d\u4e8e\u6d59\u6c5f\u5409\u5229\u6c7d\u8f66\u57ce\u3002\u5409\u5229\u6c7d\u8f66\u4e00\u76f4\u81f4\u529b\u4e8e\u63d0\u9ad8\u6c7d\u8f66\u751f\u4ea7\u6280\u672f\uff0c\u5e76\u59cb\u7ec8\u4fdd\u6301\u7740\u5bf9\u6c7d\u8f66\u6280\u672f\u7684\u521b\u65b0\u548c\u53d1\u5c55\u3002\\n' +\r\n    '\\n' +\r\n    '\u5409\u5229\u6c7d\u8f66\u5728\u5168\u7403\u8303\u56f4\u5185\u90fd\u6709\u5e7f\u6cdb\u7684\u9500\u552e\uff0c\u5728\u6b27\u6d32\uff0c\u65e5\u672c\u548c\u5370\u5ea6\u90fd\u6709\u5409\u5229\u6c7d\u8f66\u7684\u9500\u552e\u7f51\u7edc\u3002\u5409\u5229\u6c7d\u8f66\u7684\u76ee\u6807\u662f\u901a\u8fc7\u751f\u4ea7\u4f18\u8d28\u7684\u6c7d\u8f66\uff0c\u4e3a\u4eba\u4eec\u63d0\u4f9b\u4fbf\u6377\u3001\u8212\u9002\u3001\u5b89\u5168\u3001\u7ecf\u6d4e\u7684\u4ea4\u901a\u5de5\u5177\u3002',\r\n  id_slot: 0,\r\n  stop: true,\r\n  model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n  tokens_predicted: 213,\r\n  tokens_evaluated: 40,\r\n  generation_settings: {\r\n    n_ctx: 4096,\r\n    n_predict: -1,\r\n    model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n    seed: 4294967295,\r\n    temperature: 0.800000011920929,\r\n    dynatemp_range: 0,\r\n    dynatemp_exponent: 1,\r\n    top_k: 40,\r\n    top_p: 0.949999988079071,\r\n    min_p: 0.05000000074505806,\r\n    tfs_z: 1,\r\n    typical_p: 1,\r\n    repeat_last_n: 64,\r\n    repeat_penalty: 1,\r\n    presence_penalty: 0,\r\n    frequency_penalty: 0,\r\n    penalty_prompt_tokens: [],\r\n    use_penalty_prompt_tokens: false,\r\n    mirostat: 0,\r\n    mirostat_tau: 5,\r\n    mirostat_eta: 0.10000000149011612,\r\n    penalize_nl: false,\r\n    stop: [],\r\n    n_keep: 1,\r\n    n_discard: 0,\r\n    ignore_eos: false,\r\n    stream: false,\r\n    logit_bias: [],\r\n    n_probs: 0,\r\n    min_keep: 0,\r\n    grammar: '',\r\n    samplers: [ 'top_k', 'tfs_z', 'typical_p', 'top_p', 'min_p', 'temperature' ]\r\n  },\r\n  prompt: '[INST] <<SYS>>\\n' +\r\n    'You are a helpful assistant. \u4f60\u662f\u4e00\u4e2a\u4e50\u4e8e\u52a9\u4eba\u7684\u52a9\u624b\u3002\\n' +\r\n    '<</SYS>>\\n' +\r\n    '\\n' +\r\n    '\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u5409\u5229\u6c7d\u8f66[/INST]',\r\n  truncated: false,\r\n  stopped_eos: true,\r\n  stopped_word: false,\r\n  stopped_limit: false,\r\n  stopping_word: '',\r\n  tokens_cached: 252,\r\n  timings: {\r\n    prompt_n: 40,\r\n    prompt_ms: 67.825,\r\n    prompt_per_token_ms: 1.6956250000000002,\r\n    prompt_per_second: 589.7530409141173,\r\n    predicted_n: 213,\r\n    predicted_ms: 2621.52,\r\n    predicted_per_token_ms: 12.307605633802817,\r\n    predicted_per_second: 81.25057218712809\r\n  }\r\n}\r\n********************************************************\r\n{\r\n  content: ' \u5409\u5229\u6c7d\u8f66\u662f\u4e00\u5bb6\u4e2d\u56fd\u6c7d\u8f66\u54c1\u724c\uff0c\u603b\u90e8\u4f4d\u4e8e\u6d59\u6c5f\u7701\u676d\u5dde\u5e02\u3002\u5409\u5229\u662f\u4e00\u5bb6\u4ee5\u8f7f\u8f66\u3001SUV\u3001\u7d27\u51d1\u578b\u8f66\u548c\u5c0f\u578b\u8f66\u4e3a\u4e3b\u8981\u4ea7\u54c1\u7684\u516c\u53f8\uff0c\u540c\u65f6\u62e5\u6709\u5148\u8fdb\u7684\u6280\u672f\u548c\u521b\u65b0\u7684\u8f66\u578b\uff0c\u4e3a\u6d88\u8d39\u8005\u63d0\u4f9b\u8212\u9002\u3001\u5b89\u5168\u3001\u65f6\u5c1a\u4e14\u7ecf\u6d4e\u5b9e\u7528\u7684\u6c7d\u8f66\u3002\u5409\u5229\u6c7d\u8f66\u5728\u5168\u7403\u8303\u56f4\u5185\u90fd\u5177\u6709\u5f88\u9ad8\u7684\u58f0\u8a89\uff0c\u5e76\u4ee5\u5176\u9ad8\u6027\u4ef7\u6bd4\u548c\u4f18\u79c0\u7684\u6027\u80fd\u8457\u79f0\u3002',\r\n  id_slot: 0,\r\n  stop: true,\r\n  model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n  tokens_predicted: 76,\r\n  tokens_evaluated: 39,\r\n  generation_settings: {\r\n    n_ctx: 4096,\r\n    n_predict: -1,\r\n    model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n    seed: 4294967295,\r\n    temperature: 0.800000011920929,\r\n    dynatemp_range: 0,\r\n    dynatemp_exponent: 1,\r\n    top_k: 40,\r\n    top_p: 0.949999988079071,\r\n    min_p: 0.05000000074505806,\r\n    tfs_z: 1,\r\n    typical_p: 1,\r\n    repeat_last_n: 64,\r\n    repeat_penalty: 1,\r\n    presence_penalty: 0,\r\n    frequency_penalty: 0,\r\n    penalty_prompt_tokens: [],\r\n    use_penalty_prompt_tokens: false,\r\n    mirostat: 0,\r\n    mirostat_tau: 5,\r\n    mirostat_eta: 0.10000000149011612,\r\n    penalize_nl: false,\r\n    stop: [],\r\n    n_keep: 1,\r\n    n_discard: 0,\r\n    ignore_eos: false,\r\n    stream: false,\r\n    logit_bias: [],\r\n    n_probs: 0,\r\n    min_keep: 0,\r\n    grammar: '',\r\n    samplers: [ 'top_k', 'tfs_z', 'typical_p', 'top_p', 'min_p', 'temperature' ]\r\n  },\r\n  prompt: '[INST] <<SYS>>\\n' +\r\n    'You are a helpful assistant. \u4f60\u662f\u4e00\u4e2a\u4e50\u4e8e\u52a9\u4eba\u7684\u52a9\u624b\u3002\\n' +\r\n    '<</SYS>>\\n' +\r\n    '\\n' +\r\n    '\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u5409\u5229[/INST]',\r\n  truncated: false,\r\n  stopped_eos: true,\r\n  stopped_word: false,\r\n  stopped_limit: false,\r\n  stopping_word: '',\r\n  tokens_cached: 114,\r\n  timings: {\r\n    prompt_n: 39,\r\n    prompt_ms: 74.161,\r\n    prompt_per_token_ms: 1.9015641025641026,\r\n    prompt_per_second: 525.8828764444924,\r\n    predicted_n: 76,\r\n    predicted_ms: 922.532,\r\n    predicted_per_token_ms: 12.138578947368421,\r\n    predicted_per_second: 82.38196615401958\r\n  }\r\n}\r\n********************************************************\r\n\r\n**However, the main service returns correct terms that are recognized.**\r\n********************************************************\r\n**These are three correct examples\uff08BYD\uff09**\r\n********************************************************\r\n\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u6bd4\u4e9a\u8fea\r\n\u6bd4\u4e9a\u8fea\u662f\u4e00\u5bb6\u4e2d\u56fd\u54c1\u724c\uff0c\u5b83\u751f\u4ea7\u6c7d\u8f66\uff0c\u5305\u62ec\u7d27\u51d1\u578b\u8f7f\u8f66\u3001\u7eaf\u7535\u52a8\u7d27\u51d1\u578b\u8f7f\u8f66\u548c\u7535\u52a8\u7d27\u51d1\u578b\u8f7f\u8f66\u3002\r\n********************************************************\r\n\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u6bd4\u4e9a\u8fea\u6c7d\u8f66\r\n\u6bd4\u4e9a\u8fea\u662f\u4e00\u5bb6\u8457\u540d\u7684\u4e2d\u56fd\u6c7d\u8f66\u5236\u9020\u5546\uff0c\u603b\u90e8\u4f4d\u4e8e\u5e7f\u4e1c\u7701\u6df1\u5733\u5e02\u5357\u5c71\u533a\uff0c\u6210\u7acb\u4e8e1995\u5e741\u670816\u65e5\u3002\u6bd4\u4e9a\u8fea\u7684\u4e1a\u52a1\u6db5\u76d6\u6c7d\u8f66\u3001\u7535\u5b50\u80fd\u6e90\u7cfb\u7edf\u3001\u65b0\u80fd\u6e90\u6c7d\u8f66\u548c\u96f6\u914d\u4ef6\u5236\u9020\u3002\u6bd4\u4e9a\u8fea\u7684\u54c1\u724c\u5f62\u8c61\u662f\u201c\u52c7\u4e8e\u521b\u65b0\uff0c\u81f4\u529b\u4e8e\u6539\u5584\u4eba\u4eec\u7684\u751f\u6d3b\u201d\u3002\r\n\u6bd4\u4e9a\u8fea\u7684\u6c7d\u8f66\u4e1a\u52a1\u59cb\u4e8e2000\u5e74\uff0c\u5e76\u8fc5\u901f\u53d1\u5c55\u6210\u4e3a\u4e2d\u56fd\u6c7d\u8f66\u884c\u4e1a\u7684\u9886\u519b\u8005\u4e4b\u4e00\u3002\u6bd4\u4e9a\u8fea\u7684\u8f66\u578b\u5305\u62ec\u7d27\u51d1\u578b\u8f7f\u8f66\u3001\u7d27\u51d1\u578b\u8f7f\u8f66\u3001\u7d27\u51d1\u578bSUV\u548c\u7d27\u51d1\u578bSUV\u3002\u6bd4\u4e9a\u8fea\u7684\u7535\u52a8\u6c7d\u8f66\u4e5f\u662f\u5168\u7403\u9886\u5148\u7684\uff0c\u5176\u4e2d\u5305\u62ec\u7eaf\u7535\u52a8\u6c7d\u8f66\u3001\u63d2\u7535\u6df7\u52a8\u6c7d\u8f66\u548c\u6df7\u5408\u52a8\u529b\u6c7d\u8f66\u3002 \u6bd4\u4e9a\u8fea\u7684\u7535\u5b50\u80fd\u6e90\u7cfb\u7edf\u4e1a\u52a1\u5305\u62ec\u7535\u6c60\u5236\u9020\u3001\u7535\u6c60\u7ba1\u7406\u7cfb\u7edf\u548c\u7535\u52a8\u9a71\u52a8\u7cfb\u7edf\uff0c\u5e76\u63d0\u4f9b\u76f8\u5173\u7684\u552e\u540e\u670d\u52a1\u3002 \u6bd4\u4e9a\u8fea\u7684\u96f6\u914d\u4ef6\u5236\u9020\u4e1a\u52a1\u5305\u62ec\u7535\u6c60\u3001\u7535\u673a\u548c\u7535\u52a8\u9a71\u52a8\u7cfb\u7edf\u3002 \u6bd4\u4e9a\u8fea\u7684\u54c1\u724c\u5f62\u8c61\u548c\u4ea7\u54c1\u6027\u80fd\u53d7\u5230\u4e86\u5e7f\u6cdb\u7684\u8ba4\u53ef\u548c\u8d5e\u8d4f\u3002\u6bd4\u4e9a\u8fea\u7684\u7535\u52a8\u6c7d\u8f66\u5728\u5168\u7403\u8303\u56f4\u5185\u90fd\u53d6\u5f97\u4e86\u5de8\u5927\u7684\u6210\u529f\u3002\u6bd4\u4e9a\u8fea\u4e00\u76f4\u81f4\u529b\u4e8e\u6539\u5584\u4eba\u4eec\u7684\u751f\u6d3b\uff0c\u901a\u8fc7\u521b\u65b0\u6280\u672f\u548c\u4ea7\u54c1\u4e3a\u4eba\u7c7b\u5e26\u6765\u66f4\u591a\u7684\u4fbf\u5229\u548c\u8212\u9002\u3002\r\n********************************************************\r\n\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u6bd4\u4e9a\u8fea\u9a71\u9010\u823005\r\n\u6bd4\u4e9a\u8fea\u9a71\u9010\u823005\u662f\u4e00\u6b3e\u7d27\u51d1\u578b\u7eaf\u7535\u52a8\u8f7f\u8f66\uff0c\u7531\u6bd4\u4e9a\u8fea\u96c6\u56e2\u751f\u4ea7\u3002\u5b83\u4e8e2021\u5e749\u6708\u6b63\u5f0f\u4e0a\u5e02\u3002\u6bd4\u4e9a\u8fea\u9a71\u9010\u823005\u91c7\u7528\u4e86\u6bd4\u4e9a\u8fea\u5bb6\u65cf\u5316\u7684\u9e70\u773c\u5f0f\u524d\u8138\uff0c\u5927\u5c3a\u5bf8\u7684\u524d\u683c\u6805\u548c\u5927\u5c3a\u5bf8\u7684\u524d\u706f\u7ec4\u4f7f\u8f66\u5934\u663e\u5f97\u975e\u5e38\u5a01\u4e25\u3002\u8f66\u8eab\u4fa7\u9762\u91c7\u7528\u6d41\u7545\u7684\u7ebf\u6761\uff0c\u8f66\u9876\u5fae\u5fae\u9686\u8d77\u3002\u8f66\u5c3e\u91c7\u7528\u7b80\u6d01\u7684\u8bbe\u8ba1\uff0c\u91c7\u7528\u5c01\u95ed\u5f0f\u5c3e\u706f\uff0c\u5e95\u90e8\u6709\u94f6\u8272\u62a4\u677f\u3002\u6bd4\u4e9a\u8fea\u9a71\u9010\u823005\u914d\u5907\u4e86\u7535\u52a8\u52a9\u529b\u8f6c\u5411\u548c\u81ea\u52a8\u6321\u53d8\u901f\u7bb1\u3002\u8f66\u8f86\u7684\u60ac\u67b6\u91c7\u7528\u524d\u53cc\u7403\u540e\u53cc\u7403\u7684\u72ec\u7acb\u60ac\u67b6\u8bbe\u8ba1\uff0c\u4ee5\u786e\u4fdd\u8f66\u8f86\u5728\u884c\u9a76\u8fc7\u7a0b\u4e2d\u66f4\u52a0\u7a33\u5b9a\u3002\u6bd4\u4e9a\u8fea\u9a71\u9010\u823005\u914d\u5907\u4e86\u7535\u52a8\u673a\uff0c\u6700\u5927\u8f93\u51fa\u529f\u7387\u4e3a160\u5343\u74e6\uff0c\u6700\u5927\u626d\u77e9\u4e3a252\u725b\u7c73\u3002\u7535\u6c60\u7ec4\u91c7\u7528\u6bd4\u4e9a\u8fea\u81ea\u5bb6\u7684\u7535\u6c60\uff0c\u7eed\u822a\u80fd\u529b\u5f3a\uff0c\u5728\u6ee1\u7535\u72b6\u6001\u4e0b\u53ef\u7eed\u822a500\u516c\u91cc\u3002\u6bd4\u4e9a\u8fea\u9a71\u9010\u823005\u8fd8\u5177\u6709\u667a\u80fd\u9a7e\u9a76\u8f85\u52a9\u529f\u80fd\uff0c\u5305\u62ec\u4e3b\u52a8\u5239\u8f66\u3001\u76f2\u533a\u76d1\u6d4b\u3001\u8f66\u9053\r\n********************************************************\r\n**These are two correct examples\uff08Geely\uff09**\r\n********************************************************\r\n\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u5409\u5229\r\n\u5409\u5229\u6c7d\u8f66\u662f\u4e2d\u56fd\u5409\u5229\u6c7d\u8f66\u96c6\u56e2\u65d7\u4e0b\u54c1\u724c\u3002\u5409\u5229\u6c7d\u8f66\u6210\u7acb\u4e8e1986\u5e74\uff0c\u603b\u90e8\u4f4d\u4e8e\u6d59\u6c5f\u7701\u676d\u5dde\u5e02\u3002\u5409\u5229\u6c7d\u8f66\u662f\u4e00\u5bb6\u4ee5\u521b\u65b0\u3001\u5b89\u5168\u3001\u73af\u4fdd\u548c\u54c1\u8d28\u4e3a\u91cd\u70b9\u7684\u6c7d\u8f66\u5236\u9020\u5546\u3002\u5409\u5229\u54c1\u724c\u5728\u5168\u7403\u8303\u56f4\u5185\u62e5\u6709\u4f17\u591a\u77e5\u540d\u8f66\u578b\uff0c\u5305\u62ec\u5409\u5229GX7\uff0c\u5409\u5229GX5\uff0c\u5409\u5229GS8\uff0c\u5409\u5229GX3\u7b49\u3002\u5409\u5229\u6c7d\u8f66\u4e00\u76f4\u81f4\u529b\u4e8e\u4e3a\u6d88\u8d39\u8005\u63d0\u4f9b\u5353\u8d8a\u7684\u6c7d\u8f66\u4ea7\u54c1\uff0c\u4ee5\u6ee1\u8db3\u4e0d\u540c\u6d88\u8d39\u8005\u7684\u9700\u6c42\u3002\r\n********************************************************\r\n\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u5409\u5229\u6c7d\u8f66\r\n\u5409\u5229\u6c7d\u8f66\u662f\u5409\u5229\u6c7d\u8f66\u96c6\u56e2\u7684\u54c1\u724c\u4e4b\u4e00\uff0c\u603b\u90e8\u4f4d\u4e8e\u5409\u5229\u6c7d\u8f66\u96c6\u56e2\u7684\u603b\u90e8\u4f4d\u4e8e\u6d59\u6c5f\u7701\u676d\u5dde\u5e02\u3002\u5409\u5229\u6c7d\u8f66\u96c6\u56e2\u662f\u4e00\u5bb6\u5927\u578b\u7684\u4e2d\u56fd\u6c7d\u8f66\u5236\u9020\u5546\uff0c\u6210\u7acb\u4e8e1986\u5e74\u3002 \u5409\u5229\u6c7d\u8f66\u96c6\u56e2\u751f\u4ea7\u5404\u79cd\u7c7b\u578b\u7684\u6c7d\u8f66\uff0c\u5305\u62ec\u8f7f\u8f66\uff0cSUV\uff0c\u8de8\u754c\u8f66\uff0cMPV\u7b49\u3002\u5409\u5229\u6c7d\u8f66\u96c6\u56e2\u4e00\u76f4\u81f4\u529b\u4e8e\u751f\u4ea7\u9ad8\u8d28\u91cf\uff0c\u8282\u80fd\uff0c\u73af\u4fdd\u7684\u6c7d\u8f66\uff0c\u5728\u4e9a\u6d32\u548c\u5168\u7403\u8303\u56f4\u5185\u90fd\u4eab\u6709\u76db\u8a89\u3002 \u5409\u5229\u6c7d\u8f66\u96c6\u56e2\u7684\u8f66\u578b\u5305\u62ec\u5409\u5229GX7\uff0c\u5409\u5229M8\uff0c\u5409\u5229GX8\uff0c\u5409\u5229M3\uff0c\u5409\u5229M4\uff0c\u5409\u5229M6\uff0c\u5409\u5229M9\uff0c\u5409\u5229M5\uff0c\u5409\u5229M7\uff0c\u5409\u5229M8L\uff0c\u5409\u5229M10\u7b49\u3002\r\n********************************************************\r\n\r\n**This is the log from the server service**\r\n********************************************************\r\n[server_log.txt](https://github.com/ggerganov/llama.cpp/files/14920736/server_log.txt)\r\n********************************************************\r\n**This is the log from the main service**\r\n********************************************************\r\n[main_log.txt](https://github.com/ggerganov/llama.cpp/files/14920740/main_log.txt)\r\n********************************************************\r\nThere is not much difference between the two parameters, the difference is that the main service outputs a prompt when loading vocab",
    "labels": [
      "need more info",
      "server/webui",
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-09T15:40:35+00:00",
    "closed_at": "2024-05-27T01:06:36+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6569/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6569"
  },
  {
    "number": 13566,
    "title": "Misc. bug: webui multimodal, image input is not supported by this server, server error 500",
    "body": "### Name and Version\n\n./build/bin/llama-server --version\nversion: 5394 (6c8b9150)\nbuilt with cc (GCC) 14.2.1 20250207 for x86_64-pc-linux-gnu\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Command line\n\n```shell\n./build/bin/llama-server -m ../../_models/SmolVLM-500M-Instruct-Q8_0.gguf\n```\n\n### Problem description & steps to reproduce\n\ntrying out new llama-server with multimodal vision recognition using the demo at https://github.com/ngxson/smolvlm-realtime-webcam (serving index.html with \"python -m http.server\" to access it with Chrome browser).\n\nRun this command:\n\n`./build/bin/llama-server -m ../../_models/SmolVLM-500M-Instruct-Q8_0.gguf`\n\n\nPressed \"Start\" button under the demo GUI.\nGot this error:\n\n```\ngot exception: {\"code\":500,\"message\":\"image input is not supported by this server\",\"type\":\"server_error\"}\nsrv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 500\n```\n\n\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-05-15T14:23:12+00:00",
    "closed_at": "2025-05-15T16:34:30+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13566/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13566"
  },
  {
    "number": 6914,
    "title": "Something might be wrong with either llama.cpp or the Llama 3 GGUFs",
    "body": "Try this query: \"What is 3333+777?\"\r\n\r\nYes, yes, LLMs are bad at math. That's not what I'm getting at. [Someone mentioned this on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1cci5w6/quantizing_llama_3_8b_seems_more_harmful_compared/l169ovf/), and I have to agree that I'm seeing weird stuff too.\r\n\r\nLet's get a baseline. Here is what [meta.ai](https://meta.ai) yields:\r\n\r\n![image](https://github.com/ggerganov/llama.cpp/assets/726063/b4555bf8-aa7b-4156-a0c0-2fa3c6353110)\r\n\r\nThis is likely running on Llama 3 70B.\r\n\r\nHere is what Groq yields:\r\n\r\n![image](https://github.com/ggerganov/llama.cpp/assets/726063/4e8813e0-7cf3-4c7d-8109-b34c8523091c)\r\n\r\nand at 8B:\r\n\r\n![image](https://github.com/ggerganov/llama.cpp/assets/726063/12fbba62-c7b8-4eef-8454-0093ed0dc7ab)\r\n\r\nNow, here's where things get weird. Using Open WebUI on top of Ollama, let's use llama.cpp to run the GGUFs of Llama 3.\r\n\r\nFirst, 8B at fp16:\r\n\r\n![image](https://github.com/ggerganov/llama.cpp/assets/726063/da103655-43b7-44cf-a749-7c9cff2576a0)\r\n\r\nThen 8B at Q8_0:\r\n\r\n![image](https://github.com/ggerganov/llama.cpp/assets/726063/2a258ae2-494c-4ba0-9c95-af8e2c2ce418)\r\n\r\nThen 70B at Q4_0:\r\n\r\n![image](https://github.com/ggerganov/llama.cpp/assets/726063/0207025b-c7ca-4a05-b540-4887897e0498)\r\n\r\nI think the problem should be clear. All of the non-llama.cpp instances that were _not_ using GGUFs did the math problem correctly. *All* of the llama.cpp instances got the problem wrong in exactly the same way. This issue is _extremely_ repeatable on both ends. I have never seen the cloud instances make this mistake, and I have never seen the llama.cpp instances *not* make this exact mistake of adding an extra digit to the problem and then getting it wrong.\r\n\r\nTo me, it appears that something is degrading the accuracy of Llama 3 when run under llama.cpp.\r\n\r\nAny ideas of what's going wrong here?",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-04-25T22:09:50+00:00",
    "closed_at": "2024-05-11T10:51:02+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6914/reactions",
      "total_count": 31,
      "+1": 24,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 7
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6914"
  },
  {
    "number": 12003,
    "title": "Eval bug: does llama.cpp support Intel AMX instruction? how to enable it",
    "body": "### Name and Version\n\nllama-cli\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nAMX\n\n### Hardware\n\nXEON 8452Y + NV A40 \n\n### Models\n\n_No response_\n\n### Problem description & steps to reproduce\n\nas title\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nas title\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-21T13:00:24+00:00",
    "closed_at": "2025-05-04T01:08:02+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12003/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12003"
  },
  {
    "number": 12862,
    "title": "Misc. bug: convert_hf_to_gguf.py fails to convert the model of architecture T5ForConditionalGeneration",
    "body": "### Name and Version\n\n```\n$ llama-cli --version\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = NVIDIA GeForce RTX 2060 (NVIDIA) | uma: 0 | fp16: 1 | warp size: 32 | shared memory: 49152 | int dot: 0 | matrix cores: KHR_coopmat\nversion: 0 (unknown)\nbuilt with FreeBSD clang version 19.1.7 (https://github.com/llvm/llvm-project.git llvmorg-19.1.7-0-gcd708029e0b2) for x86_64-unknown-freebsd14.2\n```\n\nVersion: 5097\n\n### Operating systems\n\nBSD\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Command line\n\n```shell\nconvert_hf_to_gguf.py --outfile sage-v1.1.0.gguf ai-forever/sage-v1.1.0\n```\n\n### Problem description & steps to reproduce\n\n1. Download https://huggingface.co/ai-forever/sage-v1.1.0\n2. Run the above command\n\nIt fails:\n```\n...\nWARNING:hf-to-gguf:Couldn't find context length in config.json, assuming default value of 512\nINFO:hf-to-gguf:Set model tokenizer\nTraceback (most recent call last):\n  File \"/usr/ports/misc/llama-cpp/work/llama.cpp-b5097/convert_hf_to_gguf.py\", line 5581, in <module>\n    main()\n  File \"/usr/ports/misc/llama-cpp/work/llama.cpp-b5097/convert_hf_to_gguf.py\", line 5575, in main\n    model_instance.write()\n  File \"/usr/ports/misc/llama-cpp/work/llama.cpp-b5097/convert_hf_to_gguf.py\", line 441, in write\n    self.prepare_metadata(vocab_only=False)\n  File \"/usr/ports/misc/llama-cpp/work/llama.cpp-b5097/convert_hf_to_gguf.py\", line 434, in prepare_metadata\n    self.set_vocab()\n  File \"/usr/ports/misc/llama-cpp/work/llama.cpp-b5097/convert_hf_to_gguf.py\", line 4546, in set_vocab\n    raise FileNotFoundError(f\"File not found: {tokenizer_path}\")\nFileNotFoundError: File not found: ai-forever/sage-v1.1.0/spiece.model\n```\n\n### First Bad Commit\n\nn/a\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-04-10T07:16:43+00:00",
    "closed_at": "2025-05-25T01:08:13+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12862/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12862"
  },
  {
    "number": 4892,
    "title": "[Bug] windows build 1833 opencl version",
    "body": "Reproduce: ./main\nResult: \nggml_opencl:clGetPlatformIDs brbrbr error -1001 \nggml-opencl.cpp:965",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-01-12T11:36:41+00:00",
    "closed_at": "2024-04-03T01:14:01+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4892/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4892"
  },
  {
    "number": 10844,
    "title": "Build docker image llama.cpp:server-cuda: CMakeLists.txt missing",
    "body": "### Git commit\n\ndocker build\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Problem description & steps to reproduce\n\nJetson Linux 36.4 on Orin NX 16GB.\r\n[NVidia Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) installed\r\n\r\nThe following command fails with an error:\r\n`sudo docker build -t local/llama.cpp:server-cuda -f llama-server-cuda.Dockerfile .`\r\n\r\nError: \r\n`CMake Error: The source directory \"/app\" does not appear to contain CMakeLists.txt`\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nsudo docker build -t local/llama.cpp:server-cuda -f llama-server-cuda.Dockerfile .\r\n[+] Building 112.9s (12/14)                                                                                                            docker:default\r\n => [internal] load build definition from llama-server-cuda.Dockerfile                                                                           0.0s\r\n => => transferring dockerfile: 1.57kB                                                                                                           0.0s\r\n => [internal] load metadata for docker.io/nvidia/cuda:12.6.0-devel-ubuntu22.04                                                                  1.0s\r\n => [internal] load metadata for docker.io/nvidia/cuda:12.6.0-runtime-ubuntu22.04                                                                0.8s\r\n => [internal] load .dockerignore                                                                                                                0.0s\r\n => => transferring context: 2B                                                                                                                  0.0s\r\n => [build 1/5] FROM docker.io/nvidia/cuda:12.6.0-devel-ubuntu22.04@sha256:af25d2ef68f7aedaf0eb179e67773e64feefc3b65a12f59a6cd604ca7c53bb57     91.0s\r\n => => resolve docker.io/nvidia/cuda:12.6.0-devel-ubuntu22.04@sha256:af25d2ef68f7aedaf0eb179e67773e64feefc3b65a12f59a6cd604ca7c53bb57            0.0s\r\n => => sha256:af25d2ef68f7aedaf0eb179e67773e64feefc3b65a12f59a6cd604ca7c53bb57 743B / 743B                                                       0.0s\r\n => => sha256:2426f3e24139b43339d3c4fd093a5881eee4cef8cef3797ff6b43f98e3a9bf2c 2.63kB / 2.63kB                                                   0.0s\r\n => => sha256:dc794b9a764f736a5c96abe4210736b069445e70ab251694fe17126ad0252eb1 18.09kB / 18.09kB                                                 0.0s\r\n => => sha256:d2933bdc3c20d7745a7e2799e78bf0b2b40d32e22b667be3882f51604b85c019 56.18MB / 56.18MB                                                 1.8s\r\n => => sha256:4ce000a43472e4a2527834764b5044674760f1e2a766480798d03a93b51a0b39 27.36MB / 27.36MB                                                 0.5s\r\n => => sha256:7c7f4271288dd1580b6d83a3242a100fc61450335896db0ab2ef941268059483 4.57MB / 4.57MB                                                   0.5s\r\n => => sha256:ec8f99a1844095804b8b9114bd596858fc3aaaab9ccd313de7a0f4aaad0c3b7b 186B / 186B                                                       0.9s\r\n => => extracting sha256:4ce000a43472e4a2527834764b5044674760f1e2a766480798d03a93b51a0b39                                                        1.6s\r\n => => sha256:d339273dfb7fc3b7fd896d3610d360ab9a09ab33a818093cb73b4be7639b6e99 6.88kB / 6.88kB                                                   0.6s\r\n => => sha256:34b96e1930422190496c08eebed1e659ffadd5cfe124d608da55f0b0acbaf2de 1.37GB / 1.37GB                                                  26.0s\r\n => => sha256:f1a7d3536bef034799d32654f73786192ed9451b7922a75f971a32433fe46fc8 64.02kB / 64.02kB                                                 1.1s\r\n => => sha256:e2bdf838962767a072828cd9f34e10b30b72fdd6aefffc3799d9278969063b25 1.69kB / 1.69kB                                                   1.4s\r\n => => sha256:148410a0c698a48d5c92e10e001b9787c54c1d8f4658d7a56931d8a506b347f8 1.52kB / 1.52kB                                                   1.8s\r\n => => sha256:243b2aeb23835850b360d4d672662196987060d6216b69c7f5bf88216e27aee4 2.11GB / 2.11GB                                                  49.7s\r\n => => sha256:eab8d18930eba6ebd2ceb8d5b8bb73b304855dad51f31248b97279bf780c056f 88.86kB / 88.86kB                                                 1.9s\r\n => => extracting sha256:7c7f4271288dd1580b6d83a3242a100fc61450335896db0ab2ef941268059483                                                        0.3s\r\n => => extracting sha256:d2933bdc3c20d7745a7e2799e78bf0b2b40d32e22b667be3882f51604b85c019                                                        1.5s\r\n => => extracting sha256:ec8f99a1844095804b8b9114bd596858fc3aaaab9ccd313de7a0f4aaad0c3b7b                                                        0.0s\r\n => => extracting sha256:d339273dfb7fc3b7fd896d3610d360ab9a09ab33a818093cb73b4be7639b6e99                                                        0.0s\r\n => => extracting sha256:34b96e1930422190496c08eebed1e659ffadd5cfe124d608da55f0b0acbaf2de                                                       24.9s\r\n => => extracting sha256:f1a7d3536bef034799d32654f73786192ed9451b7922a75f971a32433fe46fc8                                                       60.7s\r\n => => extracting sha256:e2bdf838962767a072828cd9f34e10b30b72fdd6aefffc3799d9278969063b25                                                       60.7s\r\n => => extracting sha256:148410a0c698a48d5c92e10e001b9787c54c1d8f4658d7a56931d8a506b347f8                                                       60.6s\r\n => => extracting sha256:243b2aeb23835850b360d4d672662196987060d6216b69c7f5bf88216e27aee4                                                       39.6s\r\n => => extracting sha256:eab8d18930eba6ebd2ceb8d5b8bb73b304855dad51f31248b97279bf780c056f                                                        0.0s\r\n => [runtime 1/4] FROM docker.io/nvidia/cuda:12.6.0-runtime-ubuntu22.04@sha256:22fc009e5cea0b8b91d94c99fdd419d2366810b5ea835e47b8343bc15800c18  51.3s\r\n => => resolve docker.io/nvidia/cuda:12.6.0-runtime-ubuntu22.04@sha256:22fc009e5cea0b8b91d94c99fdd419d2366810b5ea835e47b8343bc15800c186          0.0s\r\n => => sha256:4ce000a43472e4a2527834764b5044674760f1e2a766480798d03a93b51a0b39 27.36MB / 27.36MB                                                 0.4s\r\n => => sha256:22fc009e5cea0b8b91d94c99fdd419d2366810b5ea835e47b8343bc15800c186 743B / 743B                                                       0.0s\r\n => => sha256:238c429588bf3577f56c656fcfa5206a04553c795254b00174a13693f9b9c24a 2.21kB / 2.21kB                                                   0.0s\r\n => => sha256:1faf16d50d35362b5a1f6ddfc78ce5041b31a96b1241408a36cfcaac31f0119f 14.21kB / 14.21kB                                                 0.0s\r\n => => sha256:7c7f4271288dd1580b6d83a3242a100fc61450335896db0ab2ef941268059483 4.57MB / 4.57MB                                                   0.5s\r\n => => sha256:d2933bdc3c20d7745a7e2799e78bf0b2b40d32e22b667be3882f51604b85c019 56.18MB / 56.18MB                                                 1.7s\r\n => => sha256:ec8f99a1844095804b8b9114bd596858fc3aaaab9ccd313de7a0f4aaad0c3b7b 186B / 186B                                                       0.8s\r\n => => sha256:d339273dfb7fc3b7fd896d3610d360ab9a09ab33a818093cb73b4be7639b6e99 6.88kB / 6.88kB                                                   0.6s\r\n => => sha256:34b96e1930422190496c08eebed1e659ffadd5cfe124d608da55f0b0acbaf2de 1.37GB / 1.37GB                                                  26.0s\r\n => => sha256:f1a7d3536bef034799d32654f73786192ed9451b7922a75f971a32433fe46fc8 64.02kB / 64.02kB                                                 1.1s\r\n => => extracting sha256:4ce000a43472e4a2527834764b5044674760f1e2a766480798d03a93b51a0b39                                                        1.6s\r\n => => sha256:e2bdf838962767a072828cd9f34e10b30b72fdd6aefffc3799d9278969063b25 1.69kB / 1.69kB                                                   1.4s\r\n => => sha256:148410a0c698a48d5c92e10e001b9787c54c1d8f4658d7a56931d8a506b347f8 1.52kB / 1.52kB                                                   1.7s\r\n => => extracting sha256:d2933bdc3c20d7745a7e2799e78bf0b2b40d32e22b667be3882f51604b85c019                                                      109.0s\r\n => => extracting sha256:34b96e1930422190496c08eebed1e659ffadd5cfe124d608da55f0b0acbaf2de                                                       85.7s\r\n => => extracting sha256:f1a7d3536bef034799d32654f73786192ed9451b7922a75f971a32433fe46fc8                                                        0.0s\r\n => => extracting sha256:e2bdf838962767a072828cd9f34e10b30b72fdd6aefffc3799d9278969063b25                                                        0.0s\r\n => => extracting sha256:148410a0c698a48d5c92e10e001b9787c54c1d8f4658d7a56931d8a506b347f8                                                        0.0s\r\n => [internal] load build context                                                                                                                0.0s\r\n => => transferring context: 1.57kB                                                                                                              0.0s\r\n => [runtime 2/4] RUN apt-get update &&     apt-get install -y libcurl4-openssl-dev libgomp1 curl                                               13.6s\r\n => [build 2/5] RUN apt-get update &&     apt-get install -y build-essential git cmake libcurl4-openssl-dev                                     20.2s\r\n => [build 3/5] WORKDIR /app                                                                                                                     0.0s\r\n => [build 4/5] COPY . .                                                                                                                         0.0s\r\n => ERROR [build 5/5] RUN if [ \"default\" != \"default\" ]; then         export CMAKE_ARGS=\"-DCMAKE_CUDA_ARCHITECTURES=default\";     fi &&     cma  0.3s\r\n------\r\n > [build 5/5] RUN if [ \"default\" != \"default\" ]; then         export CMAKE_ARGS=\"-DCMAKE_CUDA_ARCHITECTURES=default\";     fi &&     cmake -B build -DGGML_NATIVE=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON ${CMAKE_ARGS} -DCMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined . &&     cmake --build build --config Release --target llama-server -j$(nproc) &&     mkdir -p /app/lib &&     find build -name \"*.so\" -exec cp {} /app/lib ;:\r\n0.278 CMake Error: The source directory \"/app\" does not appear to contain CMakeLists.txt.\r\n0.278 Specify --help for usage, or press the help button on the CMake GUI.\r\n------\r\nllama-server-cuda.Dockerfile:22\r\n--------------------\r\n  21 |     # Use the default CUDA archs if not specified\r\n  22 | >>> RUN if [ \"${CUDA_DOCKER_ARCH}\" != \"default\" ]; then \\\r\n  23 | >>>         export CMAKE_ARGS=\"-DCMAKE_CUDA_ARCHITECTURES=${CUDA_DOCKER_ARCH}\"; \\\r\n  24 | >>>     fi && \\\r\n  25 | >>>     cmake -B build -DGGML_NATIVE=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON ${CMAKE_ARGS} -DCMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined . && \\\r\n  26 | >>>     cmake --build build --config Release --target llama-server -j$(nproc) && \\\r\n  27 | >>>     mkdir -p /app/lib && \\\r\n  28 | >>>     find build -name \"*.so\" -exec cp {} /app/lib \\;\r\n  29 |     \r\n--------------------\r\nERROR: failed to solve: process \"/bin/sh -c if [ \\\"${CUDA_DOCKER_ARCH}\\\" != \\\"default\\\" ]; then         export CMAKE_ARGS=\\\"-DCMAKE_CUDA_ARCHITECTURES=${CUDA_DOCKER_ARCH}\\\";     fi &&     cmake -B build -DGGML_NATIVE=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON ${CMAKE_ARGS} -DCMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined . &&     cmake --build build --config Release --target llama-server -j$(nproc) &&     mkdir -p /app/lib &&     find build -name \\\"*.so\\\" -exec cp {} /app/lib \\\\;\" did not complete successfully: exit code: 1\n```\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-12-15T22:14:51+00:00",
    "closed_at": "2024-12-15T22:29:48+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10844/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10844"
  }
]
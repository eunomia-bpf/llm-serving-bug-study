[
  {
    "number": 13689,
    "title": "GGML_ASSERT(seq_id < n_tokens && \"seq_id cannot be larger than n_tokens with pooling_type == MEAN\") failed",
    "body": "@slaren Using build 'b5404', I am encountering the same issue with:\r\n```console\r\n[user@system]$ export LLAMA_ARG_HF_REPO=nomic-ai/nomic-embed-text-v2-moe-GGUF:Q4_K_M \\\r\nLLAMA_ARG_EMBEDDINGS=1 \\\r\nLLAMA_ARG_ENDPOINT_METRICS=1 \\\r\nLLAMA_ARG_NO_WEBUI=1 \\\r\nLLAMA_ARG_HOST=0.0.0.0 \\\r\nLLAMA_ARG_N_PARALLEL=4 \\\r\nLLAMA_ARG_ALIAS=embeddings-multilingual \\\r\nLLAMA_ARG_PORT=80 \\\r\nLLAMA_ARG_CACHE_TYPE_K=f16 \\\r\nLLAMA_ARG_FLASH_ATTN=0 \\\r\nLLAMA_ARG_CTX_SIZE=2048 \\\r\nLLAMA_ARG_BATCH=448 \\\r\nLLAMA_ARG_BATCH=512 \\\r\nLLAMA_ARG_THREADS=1 \\\r\nLLAMA_ARG_N_PREDICT=-1 \\\r\nLLAMA_ARG_N_GPU_LAYERS=0 \\\r\nLLAMA_ARG_NUMA=distribute \\\r\nLLAMA_ARG_MLOCK=0 \\\r\nLLAMA_ARG_ENDPOINT_SLOTS=1 \\\r\nLLAMA_ARG_NO_CONTEXT_SHIFT=0 \\\r\nLLAMA_ARG_UBATCH=512\r\n[user@system]$ llama-server --seed 0 --temp 0.0\r\n```\r\n\r\n<details>\r\n<summary>Full logs</summary>\r\n\r\n```log\r\nload_backend: loaded CPU backend from /app/libggml-cpu-haswell.so\r\nwarning: no usable GPU found, --gpu-layers option will be ignored\r\nwarning: one possible reason is that llama.cpp was compiled without GPU support\r\nwarning: consult docs/build.md for compilation instructions\r\ncurl_perform_with_retry: HEAD https://huggingface.co/nomic-ai/nomic-embed-text-v2-moe-GGUF/resolve/main/nomic-embed-text-v2-moe.Q4_K_M.gguf (attempt 1 of 1)...\r\ncommon_download_file_single: using cached file: /root/.cache/llama.cpp/nomic-ai_nomic-embed-text-v2-moe-GGUF_nomic-embed-text-v2-moe.Q4_K_M.gguf\r\nbuild: 1 (faa0b9ba) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\r\nsystem info: n_threads = 1, n_threads_batch = 1, total_threads = 8\r\n\r\nsystem_info: n_threads = 1 (n_threads_batch = 1) / 8 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \r\n\r\nWeb UI is disabled\r\nmain: binding port with default address family\r\nmain: HTTP server is listening, hostname: 0.0.0.0, port: 80, http threads: 7\r\nmain: loading model\r\nsrv    load_model: loading model '/root/.cache/llama.cpp/nomic-ai_nomic-embed-text-v2-moe-GGUF_nomic-embed-text-v2-moe.Q4_K_M.gguf'\r\nllama_model_loader: loaded meta data with 45 key-value pairs and 142 tensors from /root/.cache/llama.cpp/nomic-ai_nomic-embed-text-v2-moe-GGUF_nomic-embed-text-v2-moe.Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = nomic-bert-moe\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = nomic-embed-text-v2-moe\r\nllama_model_loader: - kv   3:                            general.version str              = 2048\r\nllama_model_loader: - kv   4:                       general.organization str              = Nomic Ai\r\nllama_model_loader: - kv   5:                           general.basename str              = nomic-xlm\r\nllama_model_loader: - kv   6:                         general.size_label str              = 8x277M\r\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\r\nllama_model_loader: - kv   8:                   general.base_model.count u32              = 1\r\nllama_model_loader: - kv   9:                  general.base_model.0.name str              = Nomic Embed Text v2 Moe Unsupervised\r\nllama_model_loader: - kv  10:          general.base_model.0.organization str              = Nomic Ai\r\nllama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/nomic-ai/nomic...\r\nllama_model_loader: - kv  12:                               general.tags arr[str,4]       = [\"sentence-transformers\", \"sentence-s...\r\nllama_model_loader: - kv  13:                          general.languages arr[str,101]     = [\"en\", \"es\", \"fr\", \"de\", \"it\", \"pt\", ...\r\nllama_model_loader: - kv  14:                 nomic-bert-moe.block_count u32              = 12\r\nllama_model_loader: - kv  15:              nomic-bert-moe.context_length u32              = 512\r\nllama_model_loader: - kv  16:            nomic-bert-moe.embedding_length u32              = 768\r\nllama_model_loader: - kv  17:         nomic-bert-moe.feed_forward_length u32              = 3072\r\nllama_model_loader: - kv  18:        nomic-bert-moe.attention.head_count u32              = 12\r\nllama_model_loader: - kv  19: nomic-bert-moe.attention.layer_norm_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  20:            nomic-bert-moe.attention.causal bool             = false\r\nllama_model_loader: - kv  21:                nomic-bert-moe.pooling_type u32              = 1\r\nllama_model_loader: - kv  22:              nomic-bert-moe.rope.freq_base f32              = 10000.000000\r\nllama_model_loader: - kv  23:          nomic-bert-moe.moe_every_n_layers u32              = 2\r\nllama_model_loader: - kv  24:                nomic-bert-moe.expert_count u32              = 8\r\nllama_model_loader: - kv  25:           nomic-bert-moe.expert_used_count u32              = 2\r\nllama_model_loader: - kv  26:                       tokenizer.ggml.model str              = t5\r\nllama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,250048]  = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \",\"...\r\nllama_model_loader: - kv  29:                      tokenizer.ggml.scores arr[f32,250048]  = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,250048]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  31:            tokenizer.ggml.add_space_prefix bool             = true\r\nllama_model_loader: - kv  32:            tokenizer.ggml.token_type_count u32              = 1\r\nllama_model_loader: - kv  33:    tokenizer.ggml.remove_extra_whitespaces bool             = true\r\nllama_model_loader: - kv  34:        tokenizer.ggml.precompiled_charsmap arr[u8,237539]   = [0, 180, 2, 0, 0, 132, 0, 0, 0, 0, 0,...\r\nllama_model_loader: - kv  35:                tokenizer.ggml.bos_token_id u32              = 0\r\nllama_model_loader: - kv  36:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  37:            tokenizer.ggml.unknown_token_id u32              = 3\r\nllama_model_loader: - kv  38:          tokenizer.ggml.seperator_token_id u32              = 2\r\nllama_model_loader: - kv  39:            tokenizer.ggml.padding_token_id u32              = 1\r\nllama_model_loader: - kv  40:               tokenizer.ggml.mask_token_id u32              = 250001\r\nllama_model_loader: - kv  41:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  42:               tokenizer.ggml.add_eos_token bool             = true\r\nllama_model_loader: - kv  43:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  44:                          general.file_type u32              = 15\r\nllama_model_loader: - type  f32:   93 tensors\r\nllama_model_loader: - type q4_K:   18 tensors\r\nllama_model_loader: - type q5_K:   24 tensors\r\nllama_model_loader: - type q6_K:    7 tensors\r\nprint_info: file format = GGUF V3 (latest)\r\nprint_info: file type   = Q4_K - Medium\r\nprint_info: file size   = 321.66 MiB (5.68 BPW) \r\nload: model vocab missing newline token, using special_pad_id instead\r\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\r\nload: special tokens cache size = 4\r\nload: token to piece cache size = 2.1668 MB\r\nprint_info: arch             = nomic-bert-moe\r\nprint_info: vocab_only       = 0\r\nprint_info: n_ctx_train      = 512\r\nprint_info: n_embd           = 768\r\nprint_info: n_layer          = 12\r\nprint_info: n_head           = 12\r\nprint_info: n_head_kv        = 12\r\nprint_info: n_rot            = 64\r\nprint_info: n_swa            = 0\r\nprint_info: n_swa_pattern    = 1\r\nprint_info: n_embd_head_k    = 64\r\nprint_info: n_embd_head_v    = 64\r\nprint_info: n_gqa            = 1\r\nprint_info: n_embd_k_gqa     = 768\r\nprint_info: n_embd_v_gqa     = 768\r\nprint_info: f_norm_eps       = 1.0e-05\r\nprint_info: f_norm_rms_eps   = 0.0e+00\r\nprint_info: f_clamp_kqv      = 0.0e+00\r\nprint_info: f_max_alibi_bias = 0.0e+00\r\nprint_info: f_logit_scale    = 0.0e+00\r\nprint_info: f_attn_scale     = 0.0e+00\r\nprint_info: n_ff             = 3072\r\nprint_info: n_expert         = 8\r\nprint_info: n_expert_used    = 2\r\nprint_info: causal attn      = 0\r\nprint_info: pooling type     = 1\r\nprint_info: rope type        = 2\r\nprint_info: rope scaling     = linear\r\nprint_info: freq_base_train  = 10000.0\r\nprint_info: freq_scale_train = 1\r\nprint_info: n_ctx_orig_yarn  = 512\r\nprint_info: rope_finetuned   = unknown\r\nprint_info: ssm_d_conv       = 0\r\nprint_info: ssm_d_inner      = 0\r\nprint_info: ssm_d_state      = 0\r\nprint_info: ssm_dt_rank      = 0\r\nprint_info: ssm_dt_b_c_rms   = 0\r\nprint_info: model type       = 475M\r\nprint_info: model params     = 475.29 M\r\nprint_info: general.name     = nomic-embed-text-v2-moe\r\nprint_info: vocab type       = UGM\r\nprint_info: n_vocab          = 250048\r\nprint_info: n_merges         = 0\r\nprint_info: BOS token        = 0 '<s>'\r\nprint_info: EOS token        = 2 '</s>'\r\nprint_info: UNK token        = 3 '<unk>'\r\nprint_info: SEP token        = 2 '</s>'\r\nprint_info: PAD token        = 1 '<pad>'\r\nprint_info: MASK token       = 250001 '[PAD250000]'\r\nprint_info: LF token         = 0 '<s>'\r\nprint_info: EOG token        = 2 '</s>'\r\nprint_info: max token length = 48\r\nload_tensors: loading model tensors, this can take a while... (mmap = true)\r\nload_tensors:  CPU_AARCH64 model buffer size =   102.52 MiB\r\nload_tensors:   CPU_Mapped model buffer size =   321.66 MiB\r\n...........................\r\nllama_context: constructing llama_context\r\nllama_context: n_seq_max     = 4\r\nllama_context: n_ctx         = 2048\r\nllama_context: n_ctx_per_seq = 512\r\nllama_context: n_batch       = 512\r\nllama_context: n_ubatch      = 512\r\nllama_context: causal_attn   = 0\r\nllama_context: flash_attn    = 0\r\nllama_context: freq_base     = 10000.0\r\nllama_context: freq_scale    = 1\r\nllama_context:        CPU  output buffer size =     0.00 MiB\r\ncommon_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting\r\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 2048\r\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\ndecode: cannot decode batches with this context (use llama_encode() instead)\r\nsrv          init: initializing slots, n_slots = 4\r\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 512\r\nslot         init: id  1 | task -1 | new slot n_ctx_slot = 512\r\nslot         init: id  2 | task -1 | new slot n_ctx_slot = 512\r\nslot         init: id  3 | task -1 | new slot n_ctx_slot = 512\r\nmain: model loaded\r\nmain: chat template, chat_template: {%- for message in messages -%}\r\n  {{- '<|im_start|>' + message.role + '\r\n' + message.content + '<|im_end|>\r\n' -}}\r\n{%- endfor -%}\r\n{%- if add_generation_prompt -%}\r\n  {{- '<|im_start|>assistant\r\n' -}}\r\n{%- endif -%}, example_format: '<|im_start|>system\r\nYou are a helpful assistant<|im_end|>\r\n<|im_start|>user\r\nHello<|im_end|>\r\n<|im_start|>assistant\r\nHi there<|im_end|>\r\n<|im_start|>user\r\nHow are you?<|im_end|>\r\n<|im_start|>assistant\r\n'\r\nmain: server is listening on http://0.0.0.0:80 - starting the main loop\r\nslot launch_slot_: id  0 | task 499 | processing task\r\nslot update_slots: id  0 | task 499 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 512\r\nslot update_slots: id  0 | task 499 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 499 | prompt processing progress, n_past = 512, n_tokens = 512, progress = 1.000000\r\nslot update_slots: id  0 | task 499 | prompt done, n_past = 512, n_tokens = 512\r\nslot      release: id  0 | task 499 | stop processing: n_past = 512, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.77 200\r\nslot launch_slot_: id  0 | task 526 | processing task\r\nslot update_slots: id  0 | task 526 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 512\r\nslot update_slots: id  0 | task 526 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 526 | prompt processing progress, n_past = 512, n_tokens = 512, progress = 1.000000\r\nslot update_slots: id  0 | task 526 | prompt done, n_past = 512, n_tokens = 512\r\nslot      release: id  0 | task 526 | stop processing: n_past = 512, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.77 200\r\nslot launch_slot_: id  0 | task 1047 | processing task\r\nslot update_slots: id  0 | task 1047 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 512\r\nslot update_slots: id  0 | task 1047 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 1047 | prompt processing progress, n_past = 512, n_tokens = 512, progress = 1.000000\r\nslot update_slots: id  0 | task 1047 | prompt done, n_past = 512, n_tokens = 512\r\nslot      release: id  0 | task 1047 | stop processing: n_past = 512, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.104 200\r\nslot launch_slot_: id  1 | task 1164 | processing task\r\nslot update_slots: id  1 | task 1164 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 94\r\nslot update_slots: id  1 | task 1164 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 1164 | prompt processing progress, n_past = 94, n_tokens = 94, progress = 1.000000\r\nslot update_slots: id  1 | task 1164 | prompt done, n_past = 94, n_tokens = 94\r\nslot      release: id  1 | task 1164 | stop processing: n_past = 94, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.104 200\r\nslot launch_slot_: id  1 | task 1171 | processing task\r\nslot update_slots: id  1 | task 1171 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 94\r\nslot update_slots: id  1 | task 1171 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 1171 | prompt processing progress, n_past = 94, n_tokens = 94, progress = 1.000000\r\nslot update_slots: id  1 | task 1171 | prompt done, n_past = 94, n_tokens = 94\r\nslot      release: id  1 | task 1171 | stop processing: n_past = 94, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.104 200\r\nslot launch_slot_: id  1 | task 1570 | processing task\r\nslot update_slots: id  1 | task 1570 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 94\r\nslot update_slots: id  1 | task 1570 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 1570 | prompt processing progress, n_past = 94, n_tokens = 94, progress = 1.000000\r\nslot update_slots: id  1 | task 1570 | prompt done, n_past = 94, n_tokens = 94\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot      release: id  1 | task 1570 | stop processing: n_past = 94, truncated = 0\r\nslot launch_slot_: id  2 | task 2487 | processing task\r\nslot update_slots: id  2 | task 2487 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 23\r\nslot update_slots: id  2 | task 2487 | kv cache rm [0, end)\r\nslot update_slots: id  2 | task 2487 | prompt processing progress, n_past = 23, n_tokens = 23, progress = 1.000000\r\nslot update_slots: id  2 | task 2487 | prompt done, n_past = 23, n_tokens = 23\r\nslot      release: id  2 | task 2487 | stop processing: n_past = 23, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  3 | task 2546 | processing task\r\nslot launch_slot_: id  0 | task 2547 | processing task\r\nslot launch_slot_: id  1 | task 2548 | processing task\r\nslot launch_slot_: id  2 | task 2549 | processing task\r\nslot update_slots: id  0 | task 2547 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  0 | task 2547 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 2547 | prompt processing progress, n_past = 502, n_tokens = 502, progress = 1.000000\r\nslot update_slots: id  0 | task 2547 | prompt done, n_past = 502, n_tokens = 502\r\nslot update_slots: id  1 | task 2548 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  2 | task 2549 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  3 | task 2546 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 501\r\nslot      release: id  0 | task 2547 | stop processing: n_past = 502, truncated = 0\r\nslot launch_slot_: id  0 | task 2550 | processing task\r\nslot update_slots: id  0 | task 2550 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 2\r\nslot update_slots: id  0 | task 2550 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 2550 | prompt processing progress, n_past = 2, n_tokens = 2, progress = 1.000000\r\nslot update_slots: id  0 | task 2550 | prompt done, n_past = 2, n_tokens = 2\r\nslot update_slots: id  1 | task 2548 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 2548 | prompt processing progress, n_past = 502, n_tokens = 504, progress = 1.000000\r\nslot update_slots: id  1 | task 2548 | prompt done, n_past = 502, n_tokens = 504\r\nslot      release: id  0 | task 2550 | stop processing: n_past = 2, truncated = 0\r\nslot      release: id  1 | task 2548 | stop processing: n_past = 502, truncated = 0\r\nslot update_slots: id  2 | task 2549 | kv cache rm [0, end)\r\nslot update_slots: id  2 | task 2549 | prompt processing progress, n_past = 502, n_tokens = 502, progress = 1.000000\r\nslot update_slots: id  2 | task 2549 | prompt done, n_past = 502, n_tokens = 502\r\nslot      release: id  2 | task 2549 | stop processing: n_past = 502, truncated = 0\r\nslot update_slots: id  3 | task 2546 | kv cache rm [0, end)\r\nslot update_slots: id  3 | task 2546 | prompt processing progress, n_past = 501, n_tokens = 501, progress = 1.000000\r\nslot update_slots: id  3 | task 2546 | prompt done, n_past = 501, n_tokens = 501\r\nslot      release: id  3 | task 2546 | stop processing: n_past = 501, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  0 | task 2557 | processing task\r\nslot update_slots: id  0 | task 2557 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 24\r\nslot update_slots: id  0 | task 2557 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 2557 | prompt processing progress, n_past = 24, n_tokens = 24, progress = 1.000000\r\nslot update_slots: id  0 | task 2557 | prompt done, n_past = 24, n_tokens = 24\r\nslot      release: id  0 | task 2557 | stop processing: n_past = 24, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  1 | task 2633 | processing task\r\nslot update_slots: id  1 | task 2633 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 2\r\nslot update_slots: id  1 | task 2633 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 2633 | prompt processing progress, n_past = 2, n_tokens = 2, progress = 1.000000\r\nslot update_slots: id  1 | task 2633 | prompt done, n_past = 2, n_tokens = 2\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot      release: id  1 | task 2633 | stop processing: n_past = 2, truncated = 0\r\nslot launch_slot_: id  1 | task 2635 | processing task\r\nslot update_slots: id  1 | task 2635 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 2\r\nslot update_slots: id  1 | task 2635 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 2635 | prompt processing progress, n_past = 2, n_tokens = 2, progress = 1.000000\r\nslot update_slots: id  1 | task 2635 | prompt done, n_past = 2, n_tokens = 2\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot      release: id  1 | task 2635 | stop processing: n_past = 2, truncated = 0\r\nslot launch_slot_: id  2 | task 2637 | processing task\r\nslot update_slots: id  2 | task 2637 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 21\r\nslot update_slots: id  2 | task 2637 | kv cache rm [0, end)\r\nslot update_slots: id  2 | task 2637 | prompt processing progress, n_past = 21, n_tokens = 21, progress = 1.000000\r\nslot update_slots: id  2 | task 2637 | prompt done, n_past = 21, n_tokens = 21\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot      release: id  2 | task 2637 | stop processing: n_past = 21, truncated = 0\r\nslot launch_slot_: id  3 | task 11488 | processing task\r\nslot update_slots: id  3 | task 11488 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 10\r\nslot update_slots: id  3 | task 11488 | kv cache rm [0, end)\r\nslot update_slots: id  3 | task 11488 | prompt processing progress, n_past = 10, n_tokens = 10, progress = 1.000000\r\nslot update_slots: id  3 | task 11488 | prompt done, n_past = 10, n_tokens = 10\r\nslot      release: id  3 | task 11488 | stop processing: n_past = 10, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  3 | task 11513 | processing task\r\nslot update_slots: id  3 | task 11513 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 10\r\nslot update_slots: id  3 | task 11513 | kv cache rm [0, end)\r\nslot update_slots: id  3 | task 11513 | prompt processing progress, n_past = 10, n_tokens = 10, progress = 1.000000\r\nslot update_slots: id  3 | task 11513 | prompt done, n_past = 10, n_tokens = 10\r\nslot      release: id  3 | task 11513 | stop processing: n_past = 10, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  0 | task 11524 | processing task\r\nslot update_slots: id  0 | task 11524 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 12\r\nslot update_slots: id  0 | task 11524 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 11524 | prompt processing progress, n_past = 12, n_tokens = 12, progress = 1.000000\r\nslot update_slots: id  0 | task 11524 | prompt done, n_past = 12, n_tokens = 12\r\nslot      release: id  0 | task 11524 | stop processing: n_past = 12, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  1 | task 11547 | processing task\r\nslot launch_slot_: id  2 | task 11548 | processing task\r\nslot update_slots: id  1 | task 11547 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  1 | task 11547 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 11547 | prompt processing progress, n_past = 502, n_tokens = 502, progress = 1.000000\r\nslot update_slots: id  1 | task 11547 | prompt done, n_past = 502, n_tokens = 502\r\nslot update_slots: id  2 | task 11548 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 2\r\nslot update_slots: id  2 | task 11548 | kv cache rm [0, end)\r\nslot update_slots: id  2 | task 11548 | prompt processing progress, n_past = 2, n_tokens = 504, progress = 1.000000\r\nslot update_slots: id  2 | task 11548 | prompt done, n_past = 2, n_tokens = 504\r\nslot      release: id  1 | task 11547 | stop processing: n_past = 502, truncated = 0\r\nslot      release: id  2 | task 11548 | stop processing: n_past = 2, truncated = 0\r\nslot launch_slot_: id  3 | task 11550 | processing task\r\nslot launch_slot_: id  0 | task 11551 | processing task\r\nslot launch_slot_: id  1 | task 11552 | processing task\r\nslot launch_slot_: id  2 | task 11553 | processing task\r\nslot update_slots: id  0 | task 11551 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  0 | task 11551 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 11551 | prompt processing progress, n_past = 502, n_tokens = 502, progress = 1.000000\r\nslot update_slots: id  0 | task 11551 | prompt done, n_past = 502, n_tokens = 502\r\nslot update_slots: id  1 | task 11552 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  2 | task 11553 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 503\r\nslot update_slots: id  3 | task 11550 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 501\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot      release: id  0 | task 11551 | stop processing: n_past = 502, truncated = 0\r\nslot launch_slot_: id  0 | task 11554 | processing task\r\nslot update_slots: id  0 | task 11554 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  0 | task 11554 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 11554 | prompt processing progress, n_past = 502, n_tokens = 502, progress = 1.000000\r\nslot update_slots: id  0 | task 11554 | prompt done, n_past = 502, n_tokens = 502\r\nslot      release: id  0 | task 11554 | stop processing: n_past = 502, truncated = 0\r\nslot update_slots: id  1 | task 11552 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 11552 | prompt processing progress, n_past = 502, n_tokens = 502, progress = 1.000000\r\nslot update_slots: id  1 | task 11552 | prompt done, n_past = 502, n_tokens = 502\r\nslot      release: id  1 | task 11552 | stop processing: n_past = 502, truncated = 0\r\nslot update_slots: id  2 | task 11553 | kv cache rm [0, end)\r\nslot update_slots: id  2 | task 11553 | prompt processing progress, n_past = 503, n_tokens = 503, progress = 1.000000\r\nslot update_slots: id  2 | task 11553 | prompt done, n_past = 503, n_tokens = 503\r\nslot      release: id  2 | task 11553 | stop processing: n_past = 503, truncated = 0\r\nslot update_slots: id  3 | task 11550 | kv cache rm [0, end)\r\nslot update_slots: id  3 | task 11550 | prompt processing progress, n_past = 501, n_tokens = 501, progress = 1.000000\r\nslot update_slots: id  3 | task 11550 | prompt done, n_past = 501, n_tokens = 501\r\nslot      release: id  3 | task 11550 | stop processing: n_past = 501, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  0 | task 11595 | processing task\r\nslot update_slots: id  0 | task 11595 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 2\r\nslot update_slots: id  0 | task 11595 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 11595 | prompt processing progress, n_past = 2, n_tokens = 2, progress = 1.000000\r\nslot update_slots: id  0 | task 11595 | prompt done, n_past = 2, n_tokens = 2\r\nslot      release: id  0 | task 11595 | stop processing: n_past = 2, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  3 | task 11597 | processing task\r\nslot launch_slot_: id  1 | task 11598 | processing task\r\nslot launch_slot_: id  2 | task 11599 | processing task\r\nslot launch_slot_: id  0 | task 11600 | processing task\r\nslot update_slots: id  0 | task 11600 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 503\r\nslot update_slots: id  0 | task 11600 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 11600 | prompt processing progress, n_past = 503, n_tokens = 503, progress = 1.000000\r\nslot update_slots: id  0 | task 11600 | prompt done, n_past = 503, n_tokens = 503\r\nslot update_slots: id  1 | task 11598 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  2 | task 11599 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  3 | task 11597 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 501\r\nslot      release: id  0 | task 11600 | stop processing: n_past = 503, truncated = 0\r\nslot launch_slot_: id  0 | task 11603 | processing task\r\nslot update_slots: id  0 | task 11603 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  0 | task 11603 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 11603 | prompt processing progress, n_past = 502, n_tokens = 502, progress = 1.000000\r\nslot update_slots: id  0 | task 11603 | prompt done, n_past = 502, n_tokens = 502\r\nslot      release: id  0 | task 11603 | stop processing: n_past = 502, truncated = 0\r\nslot launch_slot_: id  0 | task 11604 | processing task\r\nslot update_slots: id  0 | task 11604 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 2\r\nslot update_slots: id  0 | task 11604 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 11604 | prompt processing progress, n_past = 2, n_tokens = 2, progress = 1.000000\r\nslot update_slots: id  0 | task 11604 | prompt done, n_past = 2, n_tokens = 2\r\nslot update_slots: id  1 | task 11598 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 11598 | prompt processing progress, n_past = 502, n_tokens = 504, progress = 1.000000\r\nslot update_slots: id  1 | task 11598 | prompt done, n_past = 502, n_tokens = 504\r\nslot      release: id  0 | task 11604 | stop processing: n_past = 2, truncated = 0\r\nslot      release: id  1 | task 11598 | stop processing: n_past = 502, truncated = 0\r\nslot launch_slot_: id  0 | task 11601 | processing task\r\nslot update_slots: id  0 | task 11601 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  0 | task 11601 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 11601 | prompt processing progress, n_past = 502, n_tokens = 502, progress = 1.000000\r\nslot update_slots: id  0 | task 11601 | prompt done, n_past = 502, n_tokens = 502\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot      release: id  0 | task 11601 | stop processing: n_past = 502, truncated = 0\r\nslot update_slots: id  2 | task 11599 | kv cache rm [0, end)\r\nslot update_slots: id  2 | task 11599 | prompt processing progress, n_past = 502, n_tokens = 502, progress = 1.000000\r\nslot update_slots: id  2 | task 11599 | prompt done, n_past = 502, n_tokens = 502\r\nslot      release: id  2 | task 11599 | stop processing: n_past = 502, truncated = 0\r\nslot update_slots: id  3 | task 11597 | kv cache rm [0, end)\r\nslot update_slots: id  3 | task 11597 | prompt processing progress, n_past = 501, n_tokens = 501, progress = 1.000000\r\nslot update_slots: id  3 | task 11597 | prompt done, n_past = 501, n_tokens = 501\r\nslot      release: id  3 | task 11597 | stop processing: n_past = 501, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  1 | task 11613 | processing task\r\nslot launch_slot_: id  0 | task 11614 | processing task\r\nslot launch_slot_: id  2 | task 11615 | processing task\r\nslot update_slots: id  0 | task 11614 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 503\r\nslot update_slots: id  0 | task 11614 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 11614 | prompt processing progress, n_past = 503, n_tokens = 503, progress = 1.000000\r\nslot update_slots: id  0 | task 11614 | prompt done, n_past = 503, n_tokens = 503\r\nslot update_slots: id  1 | task 11613 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 501\r\nslot update_slots: id  2 | task 11615 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 2\r\nslot update_slots: id  2 | task 11615 | kv cache rm [0, end)\r\nslot update_slots: id  2 | task 11615 | prompt processing progress, n_past = 2, n_tokens = 505, progress = 1.000000\r\nslot update_slots: id  2 | task 11615 | prompt done, n_past = 2, n_tokens = 505\r\nslot      release: id  0 | task 11614 | stop processing: n_past = 503, truncated = 0\r\nslot      release: id  2 | task 11615 | stop processing: n_past = 2, truncated = 0\r\nslot update_slots: id  1 | task 11613 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 11613 | prompt processing progress, n_past = 501, n_tokens = 501, progress = 1.000000\r\nslot update_slots: id  1 | task 11613 | prompt done, n_past = 501, n_tokens = 501\r\nslot      release: id  1 | task 11613 | stop processing: n_past = 501, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  3 | task 11619 | processing task\r\nslot update_slots: id  3 | task 11619 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 12\r\nslot update_slots: id  3 | task 11619 | kv cache rm [0, end)\r\nslot update_slots: id  3 | task 11619 | prompt processing progress, n_past = 12, n_tokens = 12, progress = 1.000000\r\nslot update_slots: id  3 | task 11619 | prompt done, n_past = 12, n_tokens = 12\r\nslot      release: id  3 | task 11619 | stop processing: n_past = 12, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  2 | task 11647 | processing task\r\nslot update_slots: id  2 | task 11647 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 2\r\nslot update_slots: id  2 | task 11647 | kv cache rm [0, end)\r\nslot update_slots: id  2 | task 11647 | prompt processing progress, n_past = 2, n_tokens = 2, progress = 1.000000\r\nslot update_slots: id  2 | task 11647 | prompt done, n_past = 2, n_tokens = 2\r\n/app/src/llama-graph.cpp:185: GGML_ASSERT(seq_id < n_tokens && \"seq_id cannot be larger than n_tokens with pooling_type == MEAN\") failed\r\nsrv  cancel_tasks: cancel task, id_task = 11647\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\n```\r\n\r\n</details>\r\n\r\n**Note:** It is not deterministic, but it seems to happen more frequently when enough slots are used. If wanting to reproduce, you should reduce `LLAMA_ARG_N_PARALLEL` to `2`, for instance.\r\n\r\n_Originally posted by @aviallon in https://github.com/ggml-org/llama.cpp/discussions/9000#discussioncomment-13221292_",
    "labels": [
      "bug",
      "embeddings",
      "server"
    ],
    "state": "closed",
    "created_at": "2025-05-21T14:51:24+00:00",
    "closed_at": "2025-05-22T13:33:40+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13689/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13689"
  },
  {
    "number": 12836,
    "title": "server : crash when -b > -ub with embeddings",
    "body": "> @ggerganov Ok, I did few tests and apparently there's an issue that is subject to a separate issue.\n> \n> Using the following command:\n> ```\n> llama-server ... -ub 4096 -b 4096 -c 4096 -np 4\n> ```\n> \n> Everything works pretty much as expected. Amount of tokens that a task slot can handle appears to be `ub / np`. So in this example, each slot gets a 1024 tokens window. This does seem to give a nice boost depending on the embeddings chunking strategy (my current embeddings are up to 1024 tokens), but I haven't measured precisely yet.\n> \n> However, using the following command:\n> ```\n> llama-server ... -ub 1024 -b 4096 -c 4096 -np 4\n> ```\n> \n> The server crashes with `GGML_ASSERT((cparams.causal_attn || cparams.n_ubatch >= n_tokens_all) && \"non-causal attention requires n_ubatch >= n_tokens\") failed` as soon as it receives the next batch of tasks:\n> \n> ```\n> ggml_vulkan: Found 1 Vulkan devices:\n> ggml_vulkan: 0 = AMD Radeon RX 6600M (AMD proprietary driver) | uma: 0 | fp16: 1 | warp size: 32 | shared memory: 32768 | int dot: 1 | matrix cores: none\n> build: 5080 (997b1b42) with MSVC 19.38.33134.0 for x64\n> system info: n_threads = 8, n_threads_batch = 8, total_threads = 16\n> \n> system_info: n_threads = 8 (n_threads_batch = 8) / 16 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |\n> \n> main: binding port with default address family\n> main: HTTP server is listening, hostname: 192.168.0.2, port: 8080, http threads: 15\n> main: loading model\n> srv    load_model: loading model 'C:\\Temp\\snowflake-arctic-embed-l-v2.0-q8_0.gguf'\n> llama_model_load_from_file_impl: using device Vulkan0 (AMD Radeon RX 6600M) - 8176 MiB free\n> llama_model_loader: loaded meta data with 36 key-value pairs and 389 tensors from C:\\Temp\\snowflake-arctic-embed-l-v2.0-q8_0.gguf (version GGUF V3 (latest))\n> llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n> llama_model_loader: - kv   0:                       general.architecture str              = bert\n> llama_model_loader: - kv   1:                               general.type str              = model\n> llama_model_loader: - kv   2:                               general.name str              = Snowflake Arctic Embed L v2.0\n> llama_model_loader: - kv   3:                            general.version str              = v2.0\n> llama_model_loader: - kv   4:                           general.basename str              = snowflake-arctic-embed-l\n> llama_model_loader: - kv   5:                         general.size_label str              = 567M\n> llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n> llama_model_loader: - kv   7:                               general.tags arr[str,8]       = [\"sentence-transformers\", \"feature-ex...\n> llama_model_loader: - kv   8:                          general.languages arr[str,74]      = [\"af\", \"ar\", \"az\", \"be\", \"bg\", \"bn\", ...\n> llama_model_loader: - kv   9:                           bert.block_count u32              = 24\n> llama_model_loader: - kv  10:                        bert.context_length u32              = 8192\n> llama_model_loader: - kv  11:                      bert.embedding_length u32              = 1024\n> llama_model_loader: - kv  12:                   bert.feed_forward_length u32              = 4096\n> llama_model_loader: - kv  13:                  bert.attention.head_count u32              = 16\n> llama_model_loader: - kv  14:          bert.attention.layer_norm_epsilon f32              = 0.000010\n> llama_model_loader: - kv  15:                          general.file_type u32              = 7\n> llama_model_loader: - kv  16:                      bert.attention.causal bool             = false\n> llama_model_loader: - kv  17:                          bert.pooling_type u32              = 2\n> llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = t5\n> llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = default\n> llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,250002]  = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \",\"...\n> llama_model_loader: - kv  21:                      tokenizer.ggml.scores arr[f32,250002]  = [0.000000, 0.000000, 0.000000, 0.0000...\n> llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,250002]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n> llama_model_loader: - kv  23:            tokenizer.ggml.add_space_prefix bool             = true\n> llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 1\n> llama_model_loader: - kv  25:    tokenizer.ggml.remove_extra_whitespaces bool             = true\n> llama_model_loader: - kv  26:        tokenizer.ggml.precompiled_charsmap arr[u8,237539]   = [0, 180, 2, 0, 0, 132, 0, 0, 0, 0, 0,...\n> llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 0\n> llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\n> llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 3\n> llama_model_loader: - kv  30:          tokenizer.ggml.seperator_token_id u32              = 2\n> llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 1\n> llama_model_loader: - kv  32:               tokenizer.ggml.mask_token_id u32              = 250001\n> llama_model_loader: - kv  33:               tokenizer.ggml.add_bos_token bool             = true\n> llama_model_loader: - kv  34:               tokenizer.ggml.add_eos_token bool             = true\n> llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n> llama_model_loader: - type  f32:  244 tensors\n> llama_model_loader: - type q8_0:  145 tensors\n> print_info: file format = GGUF V3 (latest)\n> print_info: file type   = Q8_0\n> print_info: file size   = 598.63 MiB (8.86 BPW)\n> load: model vocab missing newline token, using special_pad_id instead\n> load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n> load: special tokens cache size = 4\n> load: token to piece cache size = 2.1668 MB\n> print_info: arch             = bert\n> print_info: vocab_only       = 0\n> print_info: n_ctx_train      = 8192\n> print_info: n_embd           = 1024\n> print_info: n_layer          = 24\n> print_info: n_head           = 16\n> print_info: n_head_kv        = 16\n> print_info: n_rot            = 64\n> print_info: n_swa            = 0\n> print_info: n_swa_pattern    = 1\n> print_info: n_embd_head_k    = 64\n> print_info: n_embd_head_v    = 64\n> print_info: n_gqa            = 1\n> print_info: n_embd_k_gqa     = 1024\n> print_info: n_embd_v_gqa     = 1024\n> print_info: f_norm_eps       = 1.0e-05\n> print_info: f_norm_rms_eps   = 0.0e+00\n> print_info: f_clamp_kqv      = 0.0e+00\n> print_info: f_max_alibi_bias = 0.0e+00\n> print_info: f_logit_scale    = 0.0e+00\n> print_info: f_attn_scale     = 0.0e+00\n> print_info: n_ff             = 4096\n> print_info: n_expert         = 0\n> print_info: n_expert_used    = 0\n> print_info: causal attn      = 0\n> print_info: pooling type     = 2\n> print_info: rope type        = 2\n> print_info: rope scaling     = linear\n> print_info: freq_base_train  = 10000.0\n> print_info: freq_scale_train = 1\n> print_info: n_ctx_orig_yarn  = 8192\n> print_info: rope_finetuned   = unknown\n> print_info: ssm_d_conv       = 0\n> print_info: ssm_d_inner      = 0\n> print_info: ssm_d_state      = 0\n> print_info: ssm_dt_rank      = 0\n> print_info: ssm_dt_b_c_rms   = 0\n> print_info: model type       = 335M\n> print_info: model params     = 566.70 M\n> print_info: general.name     = Snowflake Arctic Embed L v2.0\n> print_info: vocab type       = UGM\n> print_info: n_vocab          = 250002\n> print_info: n_merges         = 0\n> print_info: BOS token        = 0 '<s>'\n> print_info: EOS token        = 2 '</s>'\n> print_info: UNK token        = 3 '<unk>'\n> print_info: SEP token        = 2 '</s>'\n> print_info: PAD token        = 1 '<pad>'\n> print_info: MASK token       = 250001 '[PAD250000]'\n> print_info: LF token         = 0 '<s>'\n> print_info: EOG token        = 2 '</s>'\n> print_info: max token length = 48\n> load_tensors: loading model tensors, this can take a while... (mmap = true)\n> load_tensors: offloading 24 repeating layers to GPU\n> load_tensors: offloading output layer to GPU\n> load_tensors: offloaded 25/25 layers to GPU\n> load_tensors:      Vulkan0 model buffer size =   307.22 MiB\n> load_tensors:   CPU_Mapped model buffer size =   291.41 MiB\n> ......................................................\n> llama_context: constructing llama_context\n> llama_context: n_seq_max     = 3\n> llama_context: n_ctx         = 4096\n> llama_context: n_ctx_per_seq = 1365\n> llama_context: n_batch       = 4096\n> llama_context: n_ubatch      = 1024\n> llama_context: causal_attn   = 0\n> llama_context: flash_attn    = 0\n> llama_context: freq_base     = 10000.0\n> llama_context: freq_scale    = 1\n> llama_context: n_ctx_per_seq (1365) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n> llama_context: Vulkan_Host  output buffer size =     0.00 MiB\n> init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1\n> init:    Vulkan0 KV buffer size =   384.00 MiB\n> llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB\n> llama_context:    Vulkan0 compute buffer size =    88.01 MiB\n> llama_context: Vulkan_Host compute buffer size =    12.01 MiB\n> llama_context: graph nodes  = 825\n> llama_context: graph splits = 4 (with bs=1024), 2 (with bs=1)\n> common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n> common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n> srv          init: initializing slots, n_slots = 3\n> slot         init: id  0 | task -1 | new slot n_ctx_slot = 1365\n> slot         init: id  1 | task -1 | new slot n_ctx_slot = 1365\n> slot         init: id  2 | task -1 | new slot n_ctx_slot = 1365\n> main: model loaded\n> main: chat template, chat_template: {%- for message in messages -%}\n>   {{- '<|im_start|>' + message.role + '\n> ' + message.content + '<|im_end|>\n> ' -}}\n> {%- endfor -%}\n> {%- if add_generation_prompt -%}\n>   {{- '<|im_start|>assistant\n> ' -}}\n> {%- endif -%}, example_format: '<|im_start|>system\n> You are a helpful assistant<|im_end|>\n> <|im_start|>user\n> Hello<|im_end|>\n> <|im_start|>assistant\n> Hi there<|im_end|>\n> <|im_start|>user\n> How are you?<|im_end|>\n> <|im_start|>assistant\n> '\n> main: server is listening on http://192.168.0.2:8080 - starting the main loop\n> srv  update_slots: all slots are idle\n> slot launch_slot_: id  0 | task 0 | processing task\n> slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 1365, n_keep = 0, n_prompt_tokens = 830\n> slot update_slots: id  0 | task 0 | kv cache rm [0, end)\n> slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 830, n_tokens = 830, progress = 1.000000\n> slot update_slots: id  0 | task 0 | prompt done, n_past = 830, n_tokens = 830\n> slot      release: id  0 | task 0 | stop processing: n_past = 830, truncated = 0\n> slot launch_slot_: id  1 | task 2 | processing task\n> slot launch_slot_: id  2 | task 3 | processing task\n> slot launch_slot_: id  0 | task 4 | processing task\n> slot update_slots: id  0 | task 4 | new prompt, n_ctx_slot = 1365, n_keep = 0, n_prompt_tokens = 255\n> srv  log_server_r: request: POST /v1/embeddings 192.168.0.7 200\n> slot update_slots: id  0 | task 4 | kv cache rm [0, end)\n> slot update_slots: id  0 | task 4 | prompt processing progress, n_past = 255, n_tokens = 255, progress = 1.000000\n> slot update_slots: id  0 | task 4 | prompt done, n_past = 255, n_tokens = 255\n> slot update_slots: id  1 | task 2 | new prompt, n_ctx_slot = 1365, n_keep = 0, n_prompt_tokens = 852\n> slot update_slots: id  1 | task 2 | kv cache rm [0, end)\n> slot update_slots: id  1 | task 2 | prompt processing progress, n_past = 852, n_tokens = 1107, progress = 1.000000\n> slot update_slots: id  1 | task 2 | prompt done, n_past = 852, n_tokens = 1107\n> slot update_slots: id  2 | task 3 | new prompt, n_ctx_slot = 1365, n_keep = 0, n_prompt_tokens = 246\n> slot update_slots: id  2 | task 3 | kv cache rm [0, end)\n> slot update_slots: id  2 | task 3 | prompt processing progress, n_past = 246, n_tokens = 1353, progress = 1.000000\n> slot update_slots: id  2 | task 3 | prompt done, n_past = 246, n_tokens = 1353\n> C:\\Sources\\llama.cpp\\src\\llama-context.cpp:1220: GGML_ASSERT((cparams.causal_attn || cparams.n_ubatch >= n_tokens_all) && \"non-causal attention requires n_ubatch >= n_tokens\") failed\n> ``` \n\n _Originally posted by @deiteris in [#12817](https://github.com/ggml-org/llama.cpp/issues/12817#issuecomment-2787097698)_",
    "labels": [
      "bug",
      "good first issue",
      "embeddings",
      "server"
    ],
    "state": "open",
    "created_at": "2025-04-08T18:28:48+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12836/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12836"
  },
  {
    "number": 11808,
    "title": "Misc. bug: server not exit after `missing result_output tensor` error",
    "body": "### Name and Version\n\nWhile testing the rerank model on HF inference endpoint, we got this error: `GGML_ASSERT(strcmp(res->name, \"result_output\") == 0 && \"missing result_output tensor\") failed`\n\nThis is due to missing `LLAMA_ARG_RERANKING` (for reranking model) or `LLAMA_ARG_EMBEDDINGS` (for embeddings model).\n\nThe application is expected to edit after this error, but it still running which makes it a bit confused for end user.\n\n<img width=\"1222\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ad78af84-cd9d-4e2d-9cb6-4c947347190e\" />\n\n**Expected behavior**: the server should exit once it get that error.\n\n### Operating systems\n\n_No response_\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Command line\n\n```shell\nllama-server -m jina-rerank.gguf (do not add --rerank argument)\n```\n\n### Problem description & steps to reproduce\n\nRun a jina-rerank model without `--rerank` flag\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n(as seen on screenshot)\n```",
    "labels": [
      "stale",
      "server"
    ],
    "state": "closed",
    "created_at": "2025-02-11T13:02:55+00:00",
    "closed_at": "2025-04-27T01:08:11+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11808/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11808"
  },
  {
    "number": 11142,
    "title": "server : add support for multiple responses",
    "body": "It would be very useful to add multi-response support per slot so that a single request would be able to generate `n` independent completions. This functionality is useful in different situations - for example, a FIM completion can provide multiple alternative suggestions at a smaller or equal compute cost compared to running them sequentially.\r\n\r\nI think this can be implemented by adding multiple sequence id per slot (instead of having just one like we currently do). However, I am not sure how yet much complexity would be introduced to support this.",
    "labels": [
      "server/api",
      "server",
      "roadmap"
    ],
    "state": "open",
    "created_at": "2025-01-08T16:11:24+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11142/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11142"
  },
  {
    "number": 11031,
    "title": "Feature Request: Mapping model name to LoRA config",
    "body": "### Prerequisites\r\n\r\n- [X] I am running the latest code. Mention the version if possible as well.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\r\n\r\n### Feature Description\r\n\r\nI came across this idea while working on #10994 \r\n\r\nThe idea is that we can maintain a list of model name mapped to LoRA config, for example:\r\n\r\n```\r\n{\r\n    \"llama-base\":               [{\"id\": 0, \"scale\": 0.0}, {\"id\": 1, \"scale\": 0.0}],\r\n    \"llama-story\":              [{\"id\": 0, \"scale\": 1.0}, {\"id\": 1, \"scale\": 0.0}],\r\n    \"llama-abliteration\":       [{\"id\": 0, \"scale\": 0.0}, {\"id\": 1, \"scale\": 1.0}],\r\n    \"llama-story-abliteration\": [{\"id\": 0, \"scale\": 0.5}, {\"id\": 1, \"scale\": 0.5}]\r\n}\r\n```\r\n\r\nThen, user can switch the model by specifying `model` in the request, for example:\r\n\r\n```\r\n# first user:\r\n{\r\n    \"model\": \"llama-story-abliteration\",\r\n    \"messages\": [\r\n        {\"role\": \"user\", \"content\": \"Write a NSFW story\"}\r\n    ]\r\n}\r\n\r\n# second user:\r\n{\r\n    \"model\": \"llama-base\",\r\n    \"messages\": [\r\n        {\"role\": \"user\", \"content\": \"Is this NSFW?\"}\r\n    ]\r\n}\r\n```\r\n\r\n\r\n### Motivation\r\n\r\nN/A\r\n\r\n### Possible Implementation\r\n\r\n_No response_",
    "labels": [
      "enhancement",
      "good first issue",
      "server"
    ],
    "state": "open",
    "created_at": "2025-01-01T19:07:56+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11031/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11031"
  },
  {
    "number": 10887,
    "title": "Feature Request: support `\"encoding_format\": \"base64\"` in the `*/embeddings` endpoints",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nThe OpenAI embeddings API supports returning the embeddings in `base64` format:\r\n\r\nhttps://platform.openai.com/docs/api-reference/embeddings/create#embeddings-create-encoding_format\r\n\r\nWe should implement this option in the server and enable it both for the `/v1/embeddings` and `/embeddings` endpoints.\n\n### Motivation\n\nReduce JSON payload and increase OAI compatibility.\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement",
      "good first issue",
      "server/api",
      "server"
    ],
    "state": "closed",
    "created_at": "2024-12-18T10:50:45+00:00",
    "closed_at": "2024-12-24T20:33:05+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10887/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10887"
  },
  {
    "number": 9842,
    "title": "server : temperature sampling is not working",
    "body": "### What happened?\n\nUsing 1000000000000000 temperature does not affect model's response.\r\n```python\r\nimport httpx\r\n\r\n# Define the URL and the headers\r\nurl = 'http://localhost:8080/completion'\r\nheaders = {\r\n    'Content-Type': 'application/json'\r\n}\r\n\r\n# Define the JSON payload with properly escaped newlines\r\ndata = {\r\n    \"prompt\": \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHi!<|im_end|>\\n<|im_start|>assistant\\nHow can I assist you today?<|im_end|>\\n<|im_start|>user\\nImplement fibbonaci in Python<|im_end|>\\n<|im_start|>assistant\\n\",\r\n    \"n_predict\": 128,\r\n    \"temperature\": 1000000,\r\n}\r\n\r\n# Send the POST request using httpx with no timeout\r\nresponse = httpx.post(url, json=data, headers=headers, timeout=None)\r\n\r\n# Print the response from the server\r\nprint(response.json())\r\n```\n\n### Name and Version\n\nd5cb86844f26f600c48bf3643738ea68138f961d\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "server/api",
      "server",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-10-11T07:38:07+00:00",
    "closed_at": "2024-10-11T07:41:07+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9842/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9842"
  },
  {
    "number": 9836,
    "title": "Server UI bug: corrupted generation",
    "body": "### What happened?\r\n\r\nServer somehow corrupted the prompt, so tokens at the end of the every line are lost.\r\n\r\nHere is how I run server:\r\n```shell\r\n./build/bin/llama-server -m ~/Downloads/qwen2.5-7b-instruct-q4_0-00001-of-00002.gguf\r\n```\r\nHere is how I test CLI to ensure it is a server bug:\r\n```shell\r\n./build/bin/llama-cli -m ~/Downloads/qwen2.5-7b-instruct-q4_0-00001-of-00002.gguf -e -p \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHi\\!<|im_end|>\\n<|im_start|>assistant\\nHow can I assist you today?<|im_end|>\\n<|im_start|>user\\nImplement fibbonaci in Python<|im_end|>\\n<|im_start|>assistant\\n\" -n 128 -t 7 -tb 8 --temp 0\r\n```\r\n\r\n<details><summary>Here is the output from the CLI</summary>\r\n<p>\r\n\r\n```\r\n\u279c  llama.cpp git:(master) \u2717 ./build/bin/llama-cli -m ~/Downloads/qwen2.5-7b-instruct-q4_0-00001-of-00002.gguf -e -p \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHi\\!<|im_end|>\\n<|im_start|>assistant\\nHow can I assist you today?<|im_end|>\\n<|im_start|>user\\nImplement fibbonaci in Python<|im_end|>\\n<|im_start|>assistant\\n\" -n 128 -t 7 -tb 8 --temp 0          \r\nbuild: 3891 (d5cb8684) with cc (Debian 12.2.0-14) 12.2.0 for x86_64-linux-gnu\r\nmain: llama backend init\r\nmain: load the model and apply lora adapter, if any\r\nllama_model_loader: additional 1 GGUFs metadata loaded.\r\nllama_model_loader: loaded meta data with 29 key-value pairs and 339 tensors from /home/i/Downloads/qwen2.5-7b-instruct-q4_0-00001-of-00002.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = qwen2.5-7b-instruct\r\nllama_model_loader: - kv   3:                            general.version str              = v0.1\r\nllama_model_loader: - kv   4:                           general.finetune str              = qwen2.5-7b-instruct\r\nllama_model_loader: - kv   5:                         general.size_label str              = 7.6B\r\nllama_model_loader: - kv   6:                          qwen2.block_count u32              = 28\r\nllama_model_loader: - kv   7:                       qwen2.context_length u32              = 131072\r\nllama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 3584\r\nllama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 18944\r\nllama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 28\r\nllama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 4\r\nllama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  14:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\r\nllama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\r\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\r\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\r\nllama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\r\nllama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\r\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  26:                                   split.no u16              = 0\r\nllama_model_loader: - kv  27:                                split.count u16              = 2\r\nllama_model_loader: - kv  28:                        split.tensors.count i32              = 339\r\nllama_model_loader: - type  f32:  141 tensors\r\nllama_model_loader: - type q4_0:  197 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special tokens cache size = 22\r\nllm_load_vocab: token to piece cache size = 0.9310 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = qwen2\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 152064\r\nllm_load_print_meta: n_merges         = 151387\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 3584\r\nllm_load_print_meta: n_layer          = 28\r\nllm_load_print_meta: n_head           = 28\r\nllm_load_print_meta: n_head_kv        = 4\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 7\r\nllm_load_print_meta: n_embd_k_gqa     = 512\r\nllm_load_print_meta: n_embd_v_gqa     = 512\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 18944\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 7.62 B\r\nllm_load_print_meta: model size       = 4.12 GiB (4.65 BPW) \r\nllm_load_print_meta: general.name     = qwen2.5-7b-instruct\r\nllm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: EOS token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 148848 '\u00c4\u012c'\r\nllm_load_print_meta: EOT token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: EOG token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: max token length = 256\r\nllm_load_tensors: ggml ctx size =    0.15 MiB\r\nllm_load_tensors:        CPU buffer size =  3793.03 MiB\r\nllm_load_tensors:        CPU buffer size =   427.40 MiB\r\n.....................................................................................\r\nllama_new_context_with_model: n_ctx      = 131072\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:        CPU KV buffer size =  7168.00 MiB\r\nllama_new_context_with_model: KV self size  = 7168.00 MiB, K (f16): 3584.00 MiB, V (f16): 3584.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =  7452.01 MiB\r\nllama_new_context_with_model: graph nodes  = 986\r\nllama_new_context_with_model: graph splits = 1\r\nllama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\nmain: llama threadpool init, n_threads = 7\r\n\r\nsystem_info: n_threads = 7 (n_threads_batch = 8) / 16 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \r\n\r\nsampler seed: 4294967295\r\nsampler params: \r\n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\r\n\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.000\r\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\r\nsampler chain: logits -> logit-bias -> penalties -> greedy \r\ngenerate: n_ctx = 131072, n_batch = 2048, n_predict = 128, n_keep = 0\r\n\r\nsystem\r\nYou are a helpful assistant.\r\nuser\r\nHi!\r\nassistant\r\nHow can I assist you today?\r\nuser\r\nImplement fibbonaci in Python\r\nassistant\r\nSure! Here are a few ways to implement the Fibonacci sequence in Python:\r\n\r\n1. **Iterative Approach:**\r\n   ```python\r\n   def fibonacci(n):\r\n       if n <= 0:\r\n           return []\r\n       elif n == 1:\r\n           return [0]\r\n       elif n == 2:\r\n           return [0, 1]\r\n       \r\n       fib_sequence = [0, 1]\r\n       for i in range(2, n):\r\n           next_value = fib_sequence[-1] + fib_sequence[-2]\r\n           fib_sequence.append(next_value)\r\n       return fib_sequence\r\n\r\n   # Example usage\r\n   print(fibonacci(\r\n\r\nllama_perf_sampler_print:    sampling time =      25.14 ms /   172 runs   (    0.15 ms per token,  6840.33 tokens per second)\r\nllama_perf_context_print:        load time =   27227.67 ms\r\nllama_perf_context_print: prompt eval time =    6480.76 ms /    44 tokens (  147.29 ms per token,     6.79 tokens per second)\r\nllama_perf_context_print:        eval time =   20080.14 ms /   127 runs   (  158.11 ms per token,     6.32 tokens per second)\r\nllama_perf_context_print:       total time =   26704.27 ms /   171 tokens\r\nTime: 0h:00m:56s                                                                                                                                                \r\n\u279c  llama.cpp git:(master) \u2717 \r\n```\r\n\r\n</p>\r\n</details> \r\n\r\n\r\nHere is how I test server endpoints to ensure this is a UI bug:\r\n```python\r\nimport httpx\r\n\r\n# Define the URL and the headers\r\nurl = 'http://localhost:8080/completion'\r\nheaders = {\r\n    'Content-Type': 'application/json'\r\n}\r\n\r\n# Define the JSON payload with properly escaped newlines\r\ndata = {\r\n    \"prompt\": \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHi!<|im_end|>\\n<|im_start|>assistant\\nHow can I assist you today?<|im_end|>\\n<|im_start|>user\\nImplement fibbonaci in Python<|im_end|>\\n<|im_start|>assistant\\n\",\r\n    \"n_predict\": 128\r\n}\r\n\r\n# Send the POST request using httpx with no timeout\r\nresponse = httpx.post(url, json=data, headers=headers, timeout=None)\r\n\r\n# Print the response from the server\r\nprint(response.json())\r\n```\r\n\r\nResponse from the endpoints are valid:\r\n```\r\n{'content': 'Sure! Here are a few ways to implement the Fibonacci sequence in Python:\\n\\n1. **Iterative Approach:**\\n   ```python\\n   def fibonacci(n):\\n       a, b = 0, 1\\n       for _ in range(n):\\n           a, b = b, a + b\\n       return a\\n\\n   # Example usage\\n   n = 10\\n   print(f\"Fibonacci({n}) = {fibonacci(n)}\")\\n   ```\\n\\n2. **Recursive Approach:**\\n   ```python\\n   def fibonacci(n):\\n       if n <= 0:\\n           return 0\\n       elif n ==', 'id_slot': 0, 'stop': True, 'model': '/home/i/Downloads/qwen2.5-7b-instruct-q4_0-00001-of-00002.gguf', 'tokens_predicted': 128, 'tokens_evaluated': 44, 'generation_settings': {'n_ctx': 131072, 'n_predict': -1, 'model': '/home/i/Downloads/qwen2.5-7b-instruct-q4_0-00001-of-00002.gguf', 'seed': 4294967295, 'seed_cur': 3124811782, 'temperature': 0.800000011920929, 'dynatemp_range': 0.0, 'dynatemp_exponent': 1.0, 'top_k': 40, 'top_p': 0.949999988079071, 'min_p': 0.05000000074505806, 'tfs_z': 1.0, 'typical_p': 1.0, 'repeat_last_n': 64, 'repeat_penalty': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'mirostat': 0, 'mirostat_tau': 5.0, 'mirostat_eta': 0.10000000149011612, 'penalize_nl': False, 'stop': [], 'max_tokens': 128, 'n_keep': 0, 'n_discard': 0, 'ignore_eos': False, 'stream': False, 'n_probs': 0, 'min_keep': 0, 'grammar': '', 'samplers': ['top_k', 'tfs_z', 'typ_p', 'top_p', 'min_p', 'temperature']}, 'prompt': '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHi!<|im_end|>\\n<|im_start|>assistant\\nHow can I assist you today?<|im_end|>\\n<|im_start|>user\\nImplement fibbonaci in Python<|im_end|>\\n<|im_start|>assistant\\n', 'truncated': False, 'stopped_eos': False, 'stopped_word': False, 'stopped_limit': True, 'stopping_word': '', 'tokens_cached': 171, 'timings': {'prompt_n': 44, 'prompt_ms': 2533.391, 'prompt_per_token_ms': 57.577068181818184, 'prompt_per_second': 17.368025701520214, 'predicted_n': 128, 'predicted_ms': 17878.5, 'predicted_per_token_ms': 139.67578125, 'predicted_per_second': 7.159437312973684}, 'index': 0}\r\n```\r\n\r\nHere are screenshots:\r\n# Old web UI\r\n![image](https://github.com/user-attachments/assets/d8805241-39da-4cdf-b74d-ac6c93182888)\r\n# New web UI\r\n![image](https://github.com/user-attachments/assets/b9334f8b-cc37-40cd-af61-ca66f860ccc7)\r\n# New web UI Chat \r\n![image](https://github.com/user-attachments/assets/e7e8b2ea-c06c-451b-bb1e-77c4ff190113)\r\n# SimpleChat\r\n![image](https://github.com/user-attachments/assets/2f774e70-0beb-42d7-a5fd-52b5473f70ec)\r\n# llama-cli\r\n![image](https://github.com/user-attachments/assets/712648f6-1ac4-4c3e-a0dc-c2a0d41a9772)\r\n\r\nWhat is affected:\r\n- server ui\r\n- server new ui\r\n\r\nUnaffected:\r\n- server endpoints\r\n- server SimpleChat\r\n- CLI\r\n\r\n### Name and Version\r\n\r\nversion: 3891 (d5cb8684)\r\nbuilt with cc (Debian 12.2.0-14) 12.2.0 for x86_64-linux-gnu\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\n_No response_",
    "labels": [
      "server/webui",
      "stale",
      "server",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-10-11T03:55:47+00:00",
    "closed_at": "2024-11-29T01:09:57+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9836/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9836"
  },
  {
    "number": 9811,
    "title": "server : remove system prompt support",
    "body": "The \"system_prompt\" related functionality is quite outdated and is introducing unnecessary complexity. It only sort of makes sense for non-finetuned models in order to save the computation of a common prefix when there are multiple parallel slots. But in practice, only finetuned models are utilized for this use case and they always require a chat template, which is incompatible with the current implementation of the system prompt. So in order to simplify the code a bit, we should remove the system prompt related functionality from the server.",
    "labels": [
      "refactoring",
      "server"
    ],
    "state": "closed",
    "created_at": "2024-10-09T19:10:10+00:00",
    "closed_at": "2024-10-12T11:51:55+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9811/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9811"
  },
  {
    "number": 9390,
    "title": "server : ability to disable context shift",
    "body": "### Feature Description\r\n\r\nWe can add an argument (for example, `--context-shift`, `--no-context-shift`) to enable/disable context shift.\r\n\r\nIf disabled:\r\n- Requests bigger than context window will result in an error.\r\n- `n_predict` for each sequence will be capped to `n_ctx - n_tokens_prompt`\r\n\r\nNote: the behavior above is the same as official OAI API\r\n\r\n### Motivation\r\n\r\nWe may want to disable it because:\r\n- For users who doesn't know about this feature, it may degrade generation quality\r\n- Currently, quantized KV cache doesn't work with context shift\r\n\r\n### Possible Implementation\r\n\r\n_No response_",
    "labels": [
      "enhancement",
      "server"
    ],
    "state": "closed",
    "created_at": "2024-09-09T14:52:29+00:00",
    "closed_at": "2024-09-23T20:23:55+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9390/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9390"
  },
  {
    "number": 9273,
    "title": "Bug: (Server) Cannot properly cancel a non-stream completion request",
    "body": "### What happened?\n\nWhen using server completions (or chat completions) **without** stream, it is impossible to cancel the request midway.\r\n\r\n## To reproduce the problem\r\n\r\n1. Compile and run the server (any version), run with `--verbose` argument.\r\n2. `curl -X POST http://localhost:8080/completion -vvv -d '{\"prompt\": \"hi\", \"stream\": false, \"n_predict\": 1000}'`\r\n3. While it's still running, hit Ctrl+C to cancel the curl request\r\n4. The server will still process the completion without being interrupted\r\n\r\nRetry with `\"stream\": true`, now you will be able to interrupt the completion.\r\n\r\n## Investigation\r\n\r\nThis is due to the fact that httplib is a blocking HTTP library, so there is no \"client disconnect\" event.\r\n\r\nFor non-stream API, our implementation is:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/blob/c6d4cb46559b359d2682cf2a002e7fe01bb7a767/examples/server/server.cpp#L2971-L2979\r\n\r\nThe problem is that, `.recv(id_task);` will block the current thread, so there is no way to detect the disconnect event.\r\n\r\nFor stream API, `.recv(id_task);` resume the thread on each predicted token. If `sink.write` returns false, we know that the client has been disconnected. Although this is not a real-time event, it still works well enough:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/blob/c6d4cb46559b359d2682cf2a002e7fe01bb7a767/examples/server/server.cpp#L2994-L2997\r\n\r\n## Possible fix\r\n\r\nThe patchy way to fix is to unblock `.recv(id_task);` periodically, maybe via a watchdog or simply add a timeout to `.recv()`\r\n\r\nAnother way is to introduce in the upstream httplib a function to handle client disconnect. However, this maybe complicated since the event can't be delivered in the same thread handling the request (it's already blocked by `.recv()`)\r\n\r\nThe final way is to move away from blocking HTTP implementation. New libraries like `nodepp` does offer a simple way to use async/await, but with the risk of making the compilation process more complicated.\n\n### Name and Version\n\n(happens on all versions)\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug",
      "server",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-09-02T09:40:50+00:00",
    "closed_at": "2025-01-18T13:12:06+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9273/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9273"
  },
  {
    "number": 8010,
    "title": "server: Bring back multimodal support",
    "body": "Multimodal has been removed since https://github.com/ggerganov/llama.cpp/pull/5882\n\n## Current llama.cpp multimodal roadmap\n\n(update 9th april 2025)\n\n- `mtmd` (**M**ul**T**i-**M**o**D**al) library (top prio \ud83d\udd25 )\n    - [x] Implement `libmtmd`: https://github.com/ggml-org/llama.cpp/pull/12849\n    - [x] Support more models via `libmtmd` (top prio \ud83d\udd25 ) : https://github.com/ggml-org/llama.cpp/pull/13012\n    - [x] Support M-RoPE models via `libmtmd` (Qwen2VL, Qwen2.5VL) : https://github.com/ggml-org/llama.cpp/pull/13141\n    - [x] Support audio input\n    - [x] Use smart pointer in `clip.cpp` to avoid mem leak: https://github.com/ggml-org/llama.cpp/pull/12869\n    - [x] ~~Add wrapper for `stb_image` to avoid polluting project with the big header file~~ --> Probably don't need since we're already having some helper in `libmtmd` acting as wrapper for stb_image\n    - [x] Unify conversion scripts --> best case scenario: having `convert_hf_to_gguf.py` that can output both text + vision GGUF files --> introduced in https://github.com/ggml-org/llama.cpp/pull/13023\n    - [x] Remove BOI / EOI token embeddings from clip.cpp (used by glm-edge): https://github.com/ggml-org/llama.cpp/pull/13081\n    - [x] Refactor documentations (find a way to reduce number of README files): https://github.com/ggml-org/llama.cpp/pull/13055\n- Implement `libmtmd` in server API and server web UI (top prio \ud83d\udd25 )\n   - [x] Publish first proposal: https://github.com/ggml-org/llama.cpp/pull/12898\n   - [x] User can upload image from UI ( + drag-and-drop)\n   - [x] Nice-to-have: Better KV caching strategy (TBD)\n   - [x] Nice-to-have: allow loading remote image (may come with security risk)\n   - [ ] Update the [security policy](https://github.com/ggml-org/llama.cpp/security/policy), make it clear that bugs related to 3rd party lib (like `stb_image`) should be reported to upstream, not in llama.cpp\n- [x] Unify all vision CLI (like `minicpmv-cli`, `gemma3-cli`, etc) into a single CLI\n- [x] Add deprecation notice for `llava.h` (we will remove libllava) and `clip.h` (clip is now internal-only)\n- [ ] Experimental support for audio input: https://github.com/ggml-org/llama.cpp/pull/12745\n- [ ] (far in the future) implement `llama_multimodal` API that supports image, audio and more!",
    "labels": [
      "enhancement",
      "llava",
      "server"
    ],
    "state": "closed",
    "created_at": "2024-06-19T12:03:45+00:00",
    "closed_at": "2025-05-09T21:20:01+00:00",
    "comments": 51,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8010/reactions",
      "total_count": 151,
      "+1": 75,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 43,
      "rocket": 2,
      "eyes": 31
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8010"
  },
  {
    "number": 7585,
    "title": "Question:  Inconsistent Classification Results Between Command-Line and HTTP Server for LLaMA 3",
    "body": "### Prerequisites\r\n\r\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new useful question to share that cannot be answered within Discussions.\r\n\r\n### Background Description\r\n\r\nI get difference results if I use llama 3 with the HTTP Server than when I use ./main. For example, I am trying to classify job postings using the new prompt format (in german: its instructed to classify the job in brackets):\r\n`\r\n./main --ctx-size 9999  --color --interactive --model ../models/Meta-Llama-3-70B-Instruct-GGUF/Meta-Llama-3-70B-Instruct-Q4_K_M.gguf  --repeat_penalty 1.0 --n-gpu-layers 555  --prompt \"<|start_header_id|>system<|end_header_id|> Deine Aufgabe ist es, Jobausschreibungen zu klassifizieren. Antworte mit der Job Branche in Klammern. Antworte nur in einem einzigen Wort<|eot_id|><|start_header_id|>user<|end_header_id|> Bauleitender Elektroinstallateur Die Firma Jakob Kowner AG ist ein \u00fcber 100-j\u00e4hriges Unternehmen und wir sind in den drei Hauptbereichen Elektro-Installationen, ICT und Geb\u00e4udesysteme t\u00e4tig. Unsere Dienstpalette reicht von der Beratung \u00fcber die Planung und Realisation der Vorhaben bis hin zum Service und Unterhalt der Anlagen. Nebst unserem Hauptsitz in Z\u00fcrich haben wir noch eine Filiale in Erlenbach.Bauleitender ElektroinstallateurIhre Aufgaben: F\u00fchrung von interessanten und qualitativ hochstehenden Installations-Projekten mit viel Eigenverantwortung Fach- und termingerechte Ausf\u00fchrung von Kundenauftr\u00e4gen unter Einhaltung der entsprechenden Regeln der Technik und g\u00fcltigen Vorschriften F\u00fchren des gesamten Rapportwesens Installation von Stark- und Schwachstromanlagen an Neubauten und Umbauprojekten Reparatur- und Wartungsarbeiten, L\u00f6sungsfindung und St\u00f6rungsbehebung Mitarbeiter-Einsatzplanung und Materialdisposition zusammen mit dem Projektleiter Unterhaltsarbeiten an Telefon-, EDV- und Starkstrom-Anlagen Mitarbeit bei der Mitarbeiter- und Lehrlingsbetreuung Voraussetzungen: Abgeschlossene Berufslehre zum Elektroinstallateur EFZ und sehr gute Deutschkenntnisse Mehrj\u00e4hrige Erfahrung und selbst\u00e4ndige F\u00fchrung von Baustellen in der Schweiz Gute Kenntnisse der g\u00fcltigen Normen, NIN-Installationsvorschriften sowie gesetzlichen Bestimmungen Hohes Qualit\u00e4tsbewusstsein und Kundenorientierung Flexibilit\u00e4t und Belastbarkeit Zuverl\u00e4ssige und qualit\u00e4tsbewusste Arbeitsweise, selbst\u00e4ndig und zielorientiert Bereitschaft in unseren anderen Filialen auszuhelfen Gute Umgangsformen und gepflegtes \u00c4usseres F\u00fchrerschein Kat. B Wir bieten: Selbstst\u00e4ndiges, gut eingespieltes und junges Team, welches Ihnen mit Rat und Tat zur Seite steht Zeitgem\u00e4sse Entl\u00f6hnung 40-Stunden Woche \u2026F\u00fchlen Sie sich angesprochen! Dann senden Sie uns Ihre vollst\u00e4ndige Bewerbung mit Lebenslauf per Post oder E-Mail personal@kowner.ch\u2026 Wir freuen uns auf Sie\u2026 Kontaktperson Frau Denise Egger Telefon 0442676565 E-Mail schreiben \u00dcber die Firma J. Kowner AG Z\u00fcrich / Erlenbach Firmenprofil mit Bewertungen und allen offenen Stellen Bei neuen Stellen benachrichtigt werden<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\r\n`\r\nThis will give me the correct answer: `(Elektrotechnik)<|eot_id|>`\r\n\r\n\r\nHowever, doing with the http server doesn't work:\r\n```\r\n\r\n\r\n   inp=f'''\r\n<|start_header_id|>system<|end_header_id|> Deine Aufgabe ist es, Jobausschreibungen zu klassifizieren. Antworte mit der Job Branche in Klammern. Antworte nur in einem einzigen Wort<|eot_id|><|start_header_id|>user<|end_header_id|> Bauleitender Elektroinstallateur Die Firma Jakob Kowner AG ist ein \u00fcber 100-j\u00e4hriges Unternehmen und wir sind....\r\n'''\r\n    url = \"http://192.168.1.20:8080/completion\"\r\n    prompt = {\r\n        \"prompt\":               inp, \r\n        \"n_predict\": 20,\r\n        \r\n    }\r\n```\r\n\r\n\r\nwhich gives me a result in english for some reason:\r\n\r\n`'This is a job posting for an Electrician/Foreman position at J. Kowner AG,'`\r\n\r\n\r\nI am assuming I am not formating the input correctly or the HTTP Server preformats the query. So how can I send a proper raw query to the HTTP Server?\r\n\r\n### Possible Answer\r\n\r\nThe answer should tell me how I can send raw prompts to the http server for example:\r\n\r\nYou need to format it like this\r\n\r\n`{\r\n     \"rawPrompt\": \"<|start_header_id|>system<|end_header_id|> Deine Aufgabe ist es, Jobausschreibungen zu klassifizieren. Antworte mit der Job Branche in Klammern. Antworte nur in einem einzigen Wort<|eot_id|>....\"\r\n}`",
    "labels": [
      "question",
      "server"
    ],
    "state": "closed",
    "created_at": "2024-05-28T07:07:35+00:00",
    "closed_at": "2024-05-30T11:52:35+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7585/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7585"
  },
  {
    "number": 7197,
    "title": "Server: completion_probabilities (tok_str and prob) seem to be broken",
    "body": "Hello,\r\n\r\nI am using the llama.cpp server and noticed strange behavior in the server responses.\r\n\r\nWhen starting a server on commit 637e9a86 using `./server -m ../models/llama-2-7b-chat.Q4_K_M.gguf -c 4096 -ngl 1000 -np 1 -cb`, and using this curl command:\r\n```bash\r\ncurl --request POST \\\r\n    --url http://localhost:8080/completion \\\r\n    --header \"Content-Type: application/json\" \\\r\n    --data '{\"prompt\": \"Choose between A, B and C.\\n\\n\",\"n_predict\": 1, \"n_probs\": 10, \"temperature\": 0}'\r\n```\r\n\r\nI get the following json response:\r\n```json\r\n// commit hash 637e9a86\r\n{\r\n    \"content\": \"A\",\r\n    \"id_slot\": 0,\r\n    \"stop\": true,\r\n    \"model\": \"../models/llama-2-7b-chat.Q4_K_M.gguf\",\r\n    \"tokens_predicted\": 1,\r\n    \"tokens_evaluated\": 12,\r\n    \"generation_settings\":\r\n    {\r\n        ...\r\n    },\r\n    \"prompt\": \"Choose between A, B and C.\\n\\n\",\r\n    \"truncated\": false,\r\n    \"stopped_eos\": false,\r\n    \"stopped_word\": false,\r\n    \"stopped_limit\": true,\r\n    \"stopping_word\": \"\",\r\n    \"tokens_cached\": 12,\r\n    \"timings\":\r\n    {\r\n        \"prompt_n\": 12,\r\n        \"prompt_ms\": 280.894,\r\n        \"prompt_per_token_ms\": 23.407833333333333,\r\n        \"prompt_per_second\": 42.720741632074734,\r\n        \"predicted_n\": 1,\r\n        \"predicted_ms\": 1.734,\r\n        \"predicted_per_token_ms\": 1.734,\r\n        \"predicted_per_second\": 576.7012687427913\r\n    },\r\n    \"completion_probabilities\":\r\n    [\r\n        {\r\n            \"content\": \"A\",\r\n            \"probs\":\r\n            [\r\n                {\r\n                    \"tok_str\": \"A\",\r\n                    \"prob\": 0.6929230093955994\r\n                },\r\n                {\r\n                    \"tok_str\": \"Option\",\r\n                    \"prob\": 0.04242830350995064\r\n                },\r\n                {\r\n                    \"tok_str\": \"Wh\",\r\n                    \"prob\": 0.035371895879507065\r\n                },\r\n                {\r\n                    \"tok_str\": \"What\",\r\n                    \"prob\": 0.021582460030913353\r\n                },\r\n                {\r\n                    \"tok_str\": \"The\",\r\n                    \"prob\": 0.020988475531339645\r\n                },\r\n                {\r\n                    \"tok_str\": \"Your\",\r\n                    \"prob\": 0.009944385848939419\r\n                },\r\n                {\r\n                    \"tok_str\": \"In\",\r\n                    \"prob\": 0.007504411973059177\r\n                },\r\n                {\r\n                    \"tok_str\": \"You\",\r\n                    \"prob\": 0.0066000730730593204\r\n                },\r\n                {\r\n                    \"tok_str\": \"Question\",\r\n                    \"prob\": 0.006469167303293943\r\n                },\r\n                {\r\n                    \"tok_str\": \"If\",\r\n                    \"prob\": 0.006083796266466379\r\n                }\r\n            ]\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\nBut when running the same command on the latest commit on master (f89fe273), I get \r\n```json\r\n// commit hash f89fe273\r\n{\r\n    \"content\": \"A\",\r\n    \"id_slot\": 0,\r\n    \"stop\": true,\r\n    \"model\": \"../models/llama-2-7b-chat.Q4_K_M.gguf\",\r\n    \"tokens_predicted\": 1,\r\n    \"tokens_evaluated\": 12,\r\n    \"generation_settings\":\r\n    {\r\n        ...\r\n    },\r\n    \"prompt\": \"Choose between A, B and C.\\n\\n\",\r\n    \"truncated\": false,\r\n    \"stopped_eos\": false,\r\n    \"stopped_word\": false,\r\n    \"stopped_limit\": true,\r\n    \"stopping_word\": \"\",\r\n    \"tokens_cached\": 12,\r\n    \"timings\":\r\n    {\r\n        \"prompt_n\": 12,\r\n        \"prompt_ms\": 298.66,\r\n        \"prompt_per_token_ms\": 24.888333333333335,\r\n        \"prompt_per_second\": 40.17946829170294,\r\n        \"predicted_n\": 1,\r\n        \"predicted_ms\": 0.021,\r\n        \"predicted_per_token_ms\": 0.021,\r\n        \"predicted_per_second\": 47619.04761904762\r\n    },\r\n    \"completion_probabilities\":\r\n    [\r\n        {\r\n            \"content\": \"A\",\r\n            \"probs\":\r\n            [\r\n                {\r\n                    \"tok_str\": \"\u2585\",\r\n                    \"prob\": 1.0\r\n                },\r\n                {\r\n                    \"tok_str\": \"<s>\",\r\n                    \"prob\": 0.0\r\n                },\r\n                {\r\n                    \"tok_str\": \"</s>\",\r\n                    \"prob\": 0.0\r\n                },\r\n                {\r\n                    \"tok_str\": \"\\u0000\",\r\n                    \"prob\": 0.0\r\n                },\r\n                {\r\n                    \"tok_str\": \"\\u0001\",\r\n                    \"prob\": 0.0\r\n                },\r\n                {\r\n                    \"tok_str\": \"\\u0002\",\r\n                    \"prob\": 0.0\r\n                },\r\n                {\r\n                    \"tok_str\": \"\\u0003\",\r\n                    \"prob\": 0.0\r\n                },\r\n                {\r\n                    \"tok_str\": \"\\u0004\",\r\n                    \"prob\": 0.0\r\n                },\r\n                {\r\n                    \"tok_str\": \"\\u0005\",\r\n                    \"prob\": 0.0\r\n                },\r\n                {\r\n                    \"tok_str\": \"\\u0006\",\r\n                    \"prob\": 0.0\r\n                }\r\n            ]\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\nThe returned probs are strange, and the tokens seem to be the first n tokens of the tokenizer vocabulary.\r\n\r\nWhat happened here?\r\n\r\nBest\r\nLeon",
    "labels": [
      "bug",
      "good first issue",
      "server"
    ],
    "state": "closed",
    "created_at": "2024-05-10T11:06:21+00:00",
    "closed_at": "2024-05-11T08:11:29+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7197/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7197"
  },
  {
    "number": 6423,
    "title": "How can i get log probs in create_chat_completions in llama-cpp , I'm using logprobs=True as an attribute but still not getting Log Probabilities.",
    "body": "from llama_cpp import Llama\r\n\r\n\r\n\r\nllm = Llama(model_path=\"/home/zadmin/.cache/lm-studio/models/TheBloke/MythoMax-L2-13B-GGUF/mythomax-l2-13b.Q8_0.gguf\", logits_all=True,chat_format=\"chatml\",n_ctx=10000)\r\n\r\ndef mytho_extraction():\r\n\r\n    source_sentence = \"That is a happy person\"\r\n    sentences = [\r\n        \"That is a very happy dog\",\r\n        \"That is a very happy person\",\r\n        \"Today is a sunny day\"\r\n    ]\r\n\r\n    user_message_content = f\"Source Sentence: {source_sentence}\\nSentences to Match: {' | '.join(sentences)}\\nPlease provide the sentence from the list which is the  best matches the source sentence.\"\r\n\r\n\r\n    completion = llm.create_chat_completion(\r\n        model=\"local-model\", \r\n        messages=[\r\n            {\"role\": \"system\", \"content\": \"Give me Matched sentence with the source sentence\"},\r\n            {\"role\": \"user\", \"content\": user_message_content}\r\n        ],\r\n        temperature=0.7,\r\n        logprobs= True\r\n    )\r\n    generated_sentence = completion\r\n\r\n    print(generated_sentence)\r\n\r\n\r\n\r\n\r\nif __name__==\"__main__\":\r\n    mytho_extraction()",
    "labels": [
      "good first issue",
      "server"
    ],
    "state": "closed",
    "created_at": "2024-04-01T11:08:35+00:00",
    "closed_at": "2025-01-21T08:22:56+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6423/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6423"
  }
]
[
  {
    "number": 204,
    "title": "Model runs but doesn't produce any output",
    "body": "I checked everything several times and quantized it, but both models do not output anything, in which mode I would not run them, the processor loads, but there is no output, no matter how long I wait\r\n input to the console also does not lead to anything\r\n\r\nfor ubuntu 22.04 8gb+15 swap (everything fits)\r\n\r\n\r\n![\u0421\u043d\u0438\u043c\u043e\u043a \u044d\u043a\u0440\u0430\u043d\u0430 \u043e\u0442 2023-03-16 11-42-21](https://user-images.githubusercontent.com/93709232/225592978-99f3c8a6-85a0-4606-a39d-6ddc1e334778.png)\r\n",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-16T10:46:53+00:00",
    "closed_at": "2023-03-16T12:52:24+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/204/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/204"
  },
  {
    "number": 260,
    "title": "Error while converting to ggml.py format",
    "body": "After running the command: \"python3 convert-pth-to-ggml.py /Users/tanish.shah/llama.cpp/models/7B/ 1\"\r\nError with sentencepiece:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/tanish.shah/llama.cpp/convert-pth-to-ggml.py\", line 75, in <module>\r\n    tokenizer = sentencepiece.SentencePieceProcessor(fname_tokenizer)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/tanish.shah/llama.cpp/env/lib/python3.11/site-packages/sentencepiece/__init__.py\", line 447, in Init\r\n    self.Load(model_file=model_file, model_proto=model_proto)\r\n  File \"/Users/tanish.shah/llama.cpp/env/lib/python3.11/site-packages/sentencepiece/__init__.py\", line 905, in Load\r\n    return self.LoadFromFile(model_file)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/tanish.shah/llama.cpp/env/lib/python3.11/site-packages/sentencepiece/__init__.py\", line 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Internal: /private/var/folders/rz/kp316btj1lgdlrcgdq7spdg80000gn/T/pip-install-rga_wjtr/sentencepiece_ef6dee7cfe954a50b06d772071b44d95/sentencepiece/src/sentencepiece_processor.cc(1102) [model_proto->ParseFromArray(serialized.data(), serialized.size())]\r\n```",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-18T12:02:31+00:00",
    "closed_at": "2023-04-14T13:13:30+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/260/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/260"
  },
  {
    "number": 88,
    "title": "Create json api service",
    "body": "so we can intergrate app/UI.",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-13T10:19:23+00:00",
    "closed_at": "2023-07-28T19:29:40+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/88/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/88"
  },
  {
    "number": 3964,
    "title": "http://localhost:6800/jsonrpc",
    "body": "# Prerequisites\n\nPlease answer the following questions for yourself before submitting an issue.\n\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\n\n# Expected Behavior\n\nPlease provide a detailed written description of what you were trying to do, and what you expected `llama.cpp` to do.\n\n# Current Behavior\n\nPlease provide a detailed written description of what `llama.cpp` did, instead.\n\n# Environment and Context\n\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\n\n* Physical (or virtual) hardware you are using, e.g. for Linux:\n\n`$ lscpu`\n\n* Operating System, e.g. for Linux:\n\n`$ uname -a`\n\n* SDK version, e.g. for Linux:\n\n```\n$ python3 --version\n$ make --version\n$ g++ --version\n```\n\n# Failure Information (for bugs)\n\nPlease help provide information about the failure / bug.\n\n# Steps to Reproduce\n\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\n\n1. step 1\n2. step 2\n3. step 3\n4. etc.\n\n# Failure Logs\n\nPlease include any relevant log snippets or files. If it works under one configuration but not under another, please provide logs for both configurations and their corresponding outputs so it is easy to see where behavior changes.\n\nAlso, please try to **avoid using screenshots** if at all possible. Instead, copy/paste the console output and use [Github's markdown](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) to cleanly format your logs for easy readability.\n\nExample environment info:\n```\nllama.cpp$ git log | head -1\ncommit 2af23d30434a677c6416812eea52ccc0af65119c\n\nllama.cpp$ lscpu | egrep \"AMD|Flags\"\nVendor ID:                       AuthenticAMD\nModel name:                      AMD Ryzen Threadripper 1950X 16-Core Processor\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sme sev\nVirtualization:                  AMD-V\n\nllama.cpp$ python3 --version\nPython 3.10.9\n\nllama.cpp$ pip list | egrep \"torch|numpy|sentencepiece\"\nnumpy                         1.24.2\nnumpydoc                      1.5.0\nsentencepiece                 0.1.97\ntorch                         1.13.1\ntorchvision                   0.14.1\n\nllama.cpp$ make --version | head -1\nGNU Make 4.3\n\n$ md5sum ./models/65B/ggml-model-q4_0.bin\ndbdd682cce80e2d6e93cefc7449df487  ./models/65B/ggml-model-q4_0.bin\n```\n\nExample run with the Linux command [perf](https://www.brendangregg.com/perf.html)\n```\nllama.cpp$ perf stat ./main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p \"Please close your issue when it has been answered.\"\nmain: seed = 1679149377\nllama_model_load: loading model from './models/65B/ggml-model-q4_0.bin' - please wait ...\nllama_model_load: n_vocab = 32000\nllama_model_load: n_ctx   = 512\nllama_model_load: n_embd  = 8192\nllama_model_load: n_mult  = 256\nllama_model_load: n_head  = 64\nllama_model_load: n_layer = 80\nllama_model_load: n_rot   = 128\nllama_model_load: f16     = 2\nllama_model_load: n_ff    = 22016\nllama_model_load: n_parts = 8\nllama_model_load: ggml ctx size = 41477.73 MB\nllama_model_load: memory_size =  2560.00 MB, n_mem = 40960\nllama_model_load: loading model part 1/8 from './models/65B/ggml-model-q4_0.bin'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 2/8 from './models/65B/ggml-model-q4_0.bin.1'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 3/8 from './models/65B/ggml-model-q4_0.bin.2'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 4/8 from './models/65B/ggml-model-q4_0.bin.3'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 5/8 from './models/65B/ggml-model-q4_0.bin.4'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 6/8 from './models/65B/ggml-model-q4_0.bin.5'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 7/8 from './models/65B/ggml-model-q4_0.bin.6'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 8/8 from './models/65B/ggml-model-q4_0.bin.7'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\n\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\n\nmain: prompt: 'Please close your issue when it has been answered.'\nmain: number of tokens in prompt = 11\n     1 -> ''\n 12148 -> 'Please'\n  3802 -> ' close'\n   596 -> ' your'\n  2228 -> ' issue'\n   746 -> ' when'\n   372 -> ' it'\n   756 -> ' has'\n  1063 -> ' been'\n  7699 -> ' answered'\n 29889 -> '.'\n\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\n\n\nPlease close your issue when it has been answered.\n@duncan-donut: I'm trying to figure out what kind of \"support\" you need for this script and why, exactly? Is there a question about how the code works that hasn't already been addressed in one or more comments below this ticket, or are we talking something else entirely like some sorta bugfixing job because your server setup is different from mine??\nI can understand if your site needs to be running smoothly and you need help with a fix of sorts but there should really be nothing wrong here that the code itself could not handle. And given that I'm getting reports about how it works perfectly well on some other servers, what exactly are we talking? A detailed report will do wonders in helping us get this resolved for ya quickly so please take your time and describe the issue(s) you see as clearly & concisely as possible!!\n@duncan-donut: I'm not sure if you have access to cPanel but you could try these instructions. It is worth a shot! Let me know how it goes (or what error message, exactly!) when/if ya give that code a go? [end of text]\n\n\nmain: mem per token = 71159620 bytes\nmain:     load time = 19309.95 ms\nmain:   sample time =   168.62 ms\nmain:  predict time = 223895.61 ms / 888.47 ms per token\nmain:    total time = 246406.42 ms\n\n Performance counter stats for './main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p Please close your issue when it has been answered.':\n\n        3636882.89 msec task-clock                #   14.677 CPUs utilized\n             13509      context-switches          #    3.714 /sec\n              2436      cpu-migrations            #    0.670 /sec\n          10476679      page-faults               #    2.881 K/sec\n    13133115082869      cycles                    #    3.611 GHz                      (16.77%)\n       29314462753      stalled-cycles-frontend   #    0.22% frontend cycles idle     (16.76%)\n    10294402631459      stalled-cycles-backend    #   78.39% backend cycles idle      (16.74%)\n    23479217109614      instructions              #    1.79  insn per cycle\n                                                  #    0.44  stalled cycles per insn  (16.76%)\n     2353072268027      branches                  #  647.002 M/sec                    (16.77%)\n        1998682780      branch-misses             #    0.08% of all branches          (16.76%)\n\n     247.802177522 seconds time elapsed\n\n    3618.573072000 seconds user\n      18.491698000 seconds sys\n```",
    "labels": [
      "invalid",
      "need more info",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-11-05T19:00:15+00:00",
    "closed_at": "2024-04-02T01:12:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3964/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3964"
  },
  {
    "number": 482,
    "title": "make issue on sbc odroid",
    "body": "I am trying to run \"make\" on an odroid sbc and get following error:\r\n\r\n`I llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  unknown\r\nI UNAME_M:  armv7l\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mfpu=neon-fp-armv8 -mfp16-format=ieee -mno-unaligned-access -funsafe-math-optimizations\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\r\nI LDFLAGS:  \r\nI CC:       cc (Debian 10.2.1-6) 10.2.1 20210110\r\nI CXX:      g++ (Debian 10.2.1-6) 10.2.1 20210110\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mfpu=neon-fp-armv8 -mfp16-format=ieee -mno-unaligned-access -funsafe-math-optimizations   -c ggml.c -o ggml.o\r\nggml.c: In function \u2018ggml_vec_mad_q4_0\u2019:\r\nggml.c:2049:35: warning: implicit declaration of function \u2018vzip1_s8\u2019; did you mean \u2018vzipq_s8\u2019? [-Wimplicit-function-declaration]\r\n 2049 |             const int8x8_t vxlt = vzip1_s8(vxls, vxhs);\r\n      |                                   ^~~~~~~~\r\n      |                                   vzipq_s8\r\nggml.c:2049:35: error: incompatible types when initializing type \u2018int8x8_t\u2019 using type \u2018int\u2019\r\nggml.c:2050:35: warning: implicit declaration of function \u2018vzip2_s8\u2019; did you mean \u2018vzipq_s8\u2019? [-Wimplicit-function-declaration]\r\n 2050 |             const int8x8_t vxht = vzip2_s8(vxls, vxhs);\r\n      |                                   ^~~~~~~~\r\n      |                                   vzipq_s8\r\nggml.c:2050:35: error: incompatible types when initializing type \u2018int8x8_t\u2019 using type \u2018int\u2019\r\nmake: *** [Makefile:222: ggml.o] Error 1\r\n`\r\n\r\nAlso trying to run the docker image causes following:\r\n\r\nUnable to find image 'ghcr.io/ggerganov/llama.cpp:light' locally\r\nlight: Pulling from ggerganov/llama.cpp\r\ndocker: no matching manifest for linux/arm/v7 in the manifest list entries.\r\nSee 'docker run --help'.\r\n\r\n",
    "labels": [
      "need more info",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-24T23:32:44+00:00",
    "closed_at": "2023-05-18T10:54:07+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/482/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/482"
  },
  {
    "number": 378,
    "title": "Original weights for LLAMA",
    "body": "Hey, I noticed the API is running on CPP, were the original weights in python or CPP? If in python, I would think they were in pytorch since that is Meta's DL platform; do you have the weights in python format?",
    "labels": [
      "question",
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-22T03:56:33+00:00",
    "closed_at": "2023-03-24T22:59:18+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/378/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/378"
  },
  {
    "number": 702,
    "title": "4bit 65B model overflow 64GB of RAM",
    "body": "# Prerequisites\r\n\r\nI am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\nI carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\nI searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\nI reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\nDuring inference, there should be no or minimum disk activities going on, and disk should not be a bottleneck once pass the model loading stage.\r\n\r\n# Current Behavior\r\nMy disk should have a continuous reading speed of over 100MB/s, however, during the loading of the model, it only loads at around 40MB/s. After this very slow loading of Llama 65b model (converted from GPTQ with group size of 128), llama.cpp start to inference, however during the inference the programme continue to occupy the disk and reads at 40MB/s. The generation speed is also extremely slow, at around 10 minutes per token.\r\nHowever, if it's 30b model or smaller, llama.cpp work as expected.\r\n\r\n# Environment and Context \r\n\r\nNote: My interfercing were done using oobabooga's [text-generation-webui](https://github.com/oobabooga/text-generation-webui)'s implementation of llama.cpp, as I have no idea how to use llama.cpp by itself...\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\nCPU: Ryzen 5500\r\n    Flags: \r\n```\r\n                         fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mc\r\n                         a cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall n\r\n                         x mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_go\r\n                         od nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl p\r\n                         ni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2api\r\n                         c movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_le\r\n                         gacy svm extapic cr8_legacy abm sse4a misalignsse 3dnow\r\n                         prefetch osvw ibs skinit wdt tce topoext perfctr_core p\r\n                         erfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw\r\n                         _pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 \r\n                         avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap c\r\n                         lflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cq\r\n                         m_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero \r\n                         irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm\r\n                         _lock nrip_save tsc_scale vmcb_clean flushbyasid decode\r\n                         assists pausefilter pfthreshold avic v_vmsave_vmload vg\r\n                         if v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid ove\r\n                         rflow_recov succor smca fsrm\r\n```\r\nRAM: 64GB of DDR4 running at 3000MHz\r\nDisk where I stored my model file: 2 Barraccuda 1TB HDD in Raid 1 configuration\r\nSystem SSD: NV2 500GB\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`Linux fgdfgfthgr-MS-7C95 5.15.0-69-generic #76-Ubuntu SMP Fri Mar 17 17:19:29 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux`\r\n\r\n* SDK version, e.g. for Linux:\r\n```\r\nPython 3.9.13\r\nGNU Make 4.3\r\ng++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nNot sure what other information is there to provide.\r\n\r\n# Steps to Reproduce\r\n\r\n1. Load a 65b model using oobabooga's [text-generation-webui](https://github.com/oobabooga/text-generation-webui)'s implementation of llama.cpp.\r\n2. Use `iostat -y -d 5` to monitor disk activity during loading and inference.\r\n\r\n# Failure Logs\r\n\r\nLlama.cpp version: \r\n```\r\nhttps://pypi.org/project/llamacpp/\r\n0.1.11\r\n```\r\n\r\nPip environment:\r\n```\r\naccelerate               0.18.0\r\naiofiles                 23.1.0\r\naiohttp                  3.8.4\r\naiosignal                1.3.1\r\naltair                   4.2.2\r\nanyio                    3.6.2\r\nasync-timeout            4.0.2\r\nattrs                    22.2.0\r\nbitsandbytes             0.37.2\r\ncertifi                  2022.12.7\r\ncharset-normalizer       3.1.0\r\nclick                    8.1.3\r\ncmake                    3.26.1\r\ncontourpy                1.0.7\r\ncycler                   0.11.0\r\ndatasets                 2.11.0\r\ndill                     0.3.6\r\nentrypoints              0.4\r\nfastapi                  0.95.0\r\nffmpy                    0.3.0\r\nfilelock                 3.10.7\r\nflexgen                  0.1.7\r\nfonttools                4.39.3\r\nfrozenlist               1.3.3\r\nfsspec                   2023.3.0\r\ngradio                   3.24.0\r\ngradio_client            0.0.5\r\nh11                      0.14.0\r\nhttpcore                 0.16.3\r\nhttpx                    0.23.3\r\nhuggingface-hub          0.13.3\r\nidna                     3.4\r\nJinja2                   3.1.2\r\njsonschema               4.17.3\r\nkiwisolver               1.4.4\r\nlinkify-it-py            2.0.0\r\nlit                      16.0.0\r\nllamacpp                 0.1.11\r\nMarkdown                 3.4.3\r\nmarkdown-it-py           2.2.0\r\nMarkupSafe               2.1.2\r\nmatplotlib               3.7.1\r\nmdit-py-plugins          0.3.3\r\nmdurl                    0.1.2\r\nmpmath                   1.3.0\r\nmultidict                6.0.4\r\nmultiprocess             0.70.14\r\nnetworkx                 3.0\r\nnumpy                    1.24.2\r\nnvidia-cublas-cu11       11.10.3.66\r\nnvidia-cuda-cupti-cu11   11.7.101\r\nnvidia-cuda-nvrtc-cu11   11.7.99\r\nnvidia-cuda-runtime-cu11 11.7.99\r\nnvidia-cudnn-cu11        8.5.0.96\r\nnvidia-cufft-cu11        10.9.0.58\r\nnvidia-curand-cu11       10.2.10.91\r\nnvidia-cusolver-cu11     11.4.0.1\r\nnvidia-cusparse-cu11     11.7.4.91\r\nnvidia-nccl-cu11         2.14.3\r\nnvidia-nvtx-cu11         11.7.91\r\norjson                   3.8.9\r\npackaging                23.0\r\npandas                   1.5.3\r\npeft                     0.2.0\r\nPillow                   9.4.0\r\npip                      23.0.1\r\npsutil                   5.9.4\r\nPuLP                     2.7.0\r\npyarrow                  11.0.0\r\npydantic                 1.10.7\r\npydub                    0.25.1\r\npyparsing                3.0.9\r\npyrsistent               0.19.3\r\npython-dateutil          2.8.2\r\npython-multipart         0.0.6\r\npytz                     2023.3\r\nPyYAML                   6.0\r\nquant-cuda               0.0.0\r\nregex                    2023.3.23\r\nrequests                 2.28.2\r\nresponses                0.18.0\r\nrfc3986                  1.5.0\r\nrwkv                     0.7.1\r\nsafetensors              0.3.0\r\nsemantic-version         2.10.0\r\nsentencepiece            0.1.97\r\nsetuptools               65.6.3\r\nsix                      1.16.0\r\nsniffio                  1.3.0\r\nstarlette                0.26.1\r\nsympy                    1.11.1\r\ntokenizers               0.13.2\r\ntoolz                    0.12.0\r\ntorch                    2.0.0\r\ntorchaudio               2.0.1\r\ntorchvision              0.15.1\r\ntqdm                     4.65.0\r\ntransformers             4.28.0.dev0\r\ntriton                   2.0.0\r\ntyping_extensions        4.5.0\r\nuc-micro-py              1.0.1\r\nurllib3                  1.26.15\r\nuvicorn                  0.21.1\r\nwebsockets               10.4\r\nwheel                    0.38.4\r\nxxhash                   3.2.0\r\nyarl                     1.8.2\r\n(textgen) fgdfgfthgr@fgdfgfthgr-MS-7C95:/mnt/7018F20D48B6C548/text-generation-webui$ y-it-py            2.0.0\r\nlit                      16.0.0\r\nllamacpp                 0.1.11\r\nMarkdown                 3.4.3\r\nmarkdown-it-py           2.2.0\r\nMarkupSafe               2.1.2\r\nmatplotlib               3.7.1\r\nmdit-py-plugins          0.3.3\r\nmdurl                    0.1.2\r\nmpmath                   1.3.0\r\n```\r\n\r\nmd5sum ggml-model-q4_0.bin\r\n3073a8eedd1252063ad9b440af7c90cc  ggml-model-q4_1.bin\r\n",
    "labels": [
      "need more info",
      "performance",
      "linux"
    ],
    "state": "closed",
    "created_at": "2023-04-02T08:37:42+00:00",
    "closed_at": "2023-04-19T08:20:48+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/702/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/702"
  },
  {
    "number": 367,
    "title": "The initial token is always empty.",
    "body": "Hello,\r\n\r\nI noticed something when trying the chat with Bob is that I always get the first token as empty.\r\n\r\n     1 -> ''\r\n  4103 -> ' Trans'\r\n   924 -> 'cript'\r\n   310 -> ' of'\r\n   263 -> ' a'\r\n  7928 -> ' dialog'\r\n\r\nSo the result is this: \r\n\r\n![image](https://user-images.githubusercontent.com/110173477/226732298-38c21252-059e-4acd-9dfb-70f745347efe.png)\r\n\r\nThere's this little space at the begining of the text. Maybe this alone can significantly impact the quality of the output, that's why I decided to post this issue.\r\n\r\nI'm on a windows 10 using WSL to emulate the linux environnement (the main.exe is not as good as the linux main atm).\r\n\r\nI'm using a file that is the result of all those manipulations:\r\n\r\n1) I have first a llama-7b-4bit.pt file\r\n2) I converted it with the gptq-to-ggml converter (convert-gptq-to-ggml.py) \r\n3) I converted it again into the new version of ggml with this script https://github.com/ggerganov/llama.cpp/issues/324#issuecomment-1476227818\r\n\r\nHere's the .sh command (7B_CHAT_Bob.sh): \r\n\r\n```\r\n#!/bin/bash\r\ndos2unix 7B_CHAT_Bob.sh\r\n\r\n./main -m ./models/llama7b-4bit-GPTQ.bin -t 14 -n 256 --repeat_penalty 1.0 --color -i -r \"User:\" -f prompts/chat-with-bob.txt\r\n\r\n```\r\n\r\nEverything is updated on this repository as I apply a git pull everytime I launch the powershell.\r\n",
    "labels": [
      "need more info",
      "generation quality",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-21T19:48:37+00:00",
    "closed_at": "2024-04-10T01:08:02+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/367/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/367"
  },
  {
    "number": 6756,
    "title": "quantize serious memory leak!",
    "body": "I updated llama.cpp from somewhere around b2674 to HEAD a few hours ago, and now all my quantize processes grow without bounds (and usually get killed at around 500GB memory usage). Something serious has gone wrong in the last days.\r\n\r\n[sorry for the short report, I am somewhat busy cleaning up all my servers. It must be an obvious problem though since it affected all quants I was running]",
    "labels": [
      "need more info",
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-04-19T04:34:31+00:00",
    "closed_at": "2024-04-19T10:44:44+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6756/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6756"
  },
  {
    "number": 388,
    "title": "llama_init_from_file: failed to load model",
    "body": "When I execute this command\uff1a\r\nmake -j && ./main -m ./models/7B/ggml-model-q4_0.bin -p \"Building a website can be done in 10 simple steps:\" -n 512\r\n\r\nAn error was reported\uff1a\r\nllama_init_from_file: failed to load model\r\nmain: error: failed to load model './models/7B/ggml-model-q4_0.bin'",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-22T10:00:00+00:00",
    "closed_at": "2023-03-24T02:54:48+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/388/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/388"
  },
  {
    "number": 3305,
    "title": "[User] GGML_ASSERT: /opt/projects/llama.cpp/ggml.c:4796: view_src == NULL || data_size + view_offs <= ggml_nbytes(view_src) Aborted",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\nTo use llama.cpp running Wizardlm-13b-v1.2.\r\n\r\n# Current Behavior\r\nCMD: main -m ../ggml-model-f16.gguf -ngl 43 --interactive-first\r\nAfter interactive 8 rounds, get the error:\r\n\r\nGGML_ASSERT: /opt/projects/llama.cpp/ggml.c:4796: view_src == NULL || data_size + view_offs <= ggml_nbytes(view_src)\r\nAborted\r\n\r\n# Environment and Context\r\nmain: build = 1265 (324f340)\r\nmain: built with cc (Debian 10.2.1-6) 10.2.1 20210110 for x86_64-linux-gnu\r\nmain: seed  = 1695347775\r\nggml_init_cublas: found 1 CUDA devices:\r\n  Device 0: Tesla V100-SXM2-32GB, compute capability 7.0\r\nllama_model_loader: loaded meta data with 18 key-value pairs and 363 tensors from ../ggml-model-f16.gguf (version GGUF V2 (latest))\r\nllama_model_loader: - tensor    0:                token_embd.weight f16      [  5120, 32000,     1,     1 ]\r\nllama_model_loader: - tensor    1:              blk.0.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor    2:              blk.0.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor    3:              blk.0.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor    4:         blk.0.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor    5:            blk.0.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor    6:            blk.0.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor    7:              blk.0.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   10:              blk.1.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   11:              blk.1.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   12:              blk.1.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   13:         blk.1.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   14:            blk.1.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   15:            blk.1.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   16:              blk.1.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   19:              blk.2.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   20:              blk.2.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   21:              blk.2.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   22:         blk.2.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   23:            blk.2.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   24:            blk.2.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   25:              blk.2.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   28:              blk.3.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   29:              blk.3.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   30:              blk.3.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   31:         blk.3.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   32:            blk.3.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   33:            blk.3.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   34:              blk.3.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   37:              blk.4.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   38:              blk.4.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   39:              blk.4.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   40:         blk.4.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   41:            blk.4.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   42:            blk.4.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   43:              blk.4.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   46:              blk.5.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   47:              blk.5.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   48:              blk.5.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   49:         blk.5.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   50:            blk.5.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   51:            blk.5.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   52:              blk.5.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   55:              blk.6.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   56:              blk.6.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   57:              blk.6.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   58:         blk.6.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   59:            blk.6.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   60:            blk.6.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   61:              blk.6.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   64:              blk.7.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   65:              blk.7.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   66:              blk.7.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   67:         blk.7.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   68:            blk.7.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   69:            blk.7.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   70:              blk.7.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   73:              blk.8.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   74:              blk.8.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   75:              blk.8.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   76:         blk.8.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   77:            blk.8.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   78:            blk.8.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   79:              blk.8.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   82:              blk.9.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   83:              blk.9.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   84:              blk.9.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   85:         blk.9.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   86:            blk.9.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   87:            blk.9.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   88:              blk.9.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   91:             blk.10.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   92:             blk.10.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   93:             blk.10.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   94:        blk.10.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   95:           blk.10.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   96:           blk.10.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   97:             blk.10.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  100:             blk.11.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  101:             blk.11.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  102:             blk.11.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  103:        blk.11.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  104:           blk.11.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  105:           blk.11.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  106:             blk.11.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  109:             blk.12.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  110:             blk.12.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  111:             blk.12.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  112:        blk.12.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  113:           blk.12.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  114:           blk.12.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  115:             blk.12.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  118:             blk.13.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  119:             blk.13.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  120:             blk.13.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  121:        blk.13.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  122:           blk.13.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  123:           blk.13.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  124:             blk.13.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  127:             blk.14.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  128:             blk.14.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  129:             blk.14.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  130:        blk.14.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  131:           blk.14.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  132:           blk.14.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  133:             blk.14.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  136:             blk.15.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  137:             blk.15.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  138:             blk.15.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  139:        blk.15.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  140:           blk.15.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  141:           blk.15.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  142:             blk.15.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  145:             blk.16.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  146:             blk.16.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  147:             blk.16.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  148:        blk.16.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  149:           blk.16.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  150:           blk.16.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  151:             blk.16.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  154:             blk.17.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  155:             blk.17.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  156:             blk.17.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  157:        blk.17.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  158:           blk.17.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  159:           blk.17.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  160:             blk.17.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  163:             blk.18.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  164:             blk.18.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  165:             blk.18.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  166:        blk.18.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  167:           blk.18.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  168:           blk.18.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  169:             blk.18.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  172:             blk.19.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  173:             blk.19.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  174:             blk.19.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  175:        blk.19.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  176:           blk.19.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  177:           blk.19.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  178:             blk.19.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  181:             blk.20.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  182:             blk.20.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  183:             blk.20.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  184:        blk.20.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  185:           blk.20.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  186:           blk.20.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  187:             blk.20.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  190:             blk.21.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  191:             blk.21.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  192:             blk.21.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  193:        blk.21.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  194:           blk.21.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  195:           blk.21.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  196:             blk.21.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  199:             blk.22.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  200:             blk.22.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  201:             blk.22.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  202:        blk.22.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  203:           blk.22.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  204:           blk.22.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  205:             blk.22.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  208:             blk.23.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  209:             blk.23.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  210:             blk.23.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  211:        blk.23.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  212:           blk.23.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  213:           blk.23.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  214:             blk.23.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  217:             blk.24.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  218:             blk.24.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  219:             blk.24.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  220:        blk.24.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  221:           blk.24.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  222:           blk.24.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  223:             blk.24.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  226:             blk.25.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  227:             blk.25.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  228:             blk.25.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  229:        blk.25.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  230:           blk.25.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  231:           blk.25.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  232:             blk.25.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  235:             blk.26.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  236:             blk.26.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  237:             blk.26.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  238:        blk.26.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  239:           blk.26.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  240:           blk.26.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  241:             blk.26.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  244:             blk.27.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  245:             blk.27.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  246:             blk.27.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  247:        blk.27.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  248:           blk.27.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  249:           blk.27.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  250:             blk.27.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  253:             blk.28.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  254:             blk.28.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  255:             blk.28.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  256:        blk.28.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  257:           blk.28.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  258:           blk.28.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  259:             blk.28.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  262:             blk.29.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  263:             blk.29.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  264:             blk.29.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  265:        blk.29.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  266:           blk.29.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  267:           blk.29.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  268:             blk.29.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  271:             blk.30.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  272:             blk.30.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  273:             blk.30.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  274:        blk.30.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  275:           blk.30.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  276:           blk.30.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  277:             blk.30.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  280:             blk.31.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  281:             blk.31.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  282:             blk.31.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  283:        blk.31.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  284:           blk.31.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  285:           blk.31.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  286:             blk.31.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  289:             blk.32.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  290:             blk.32.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  291:             blk.32.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  292:        blk.32.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  293:           blk.32.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  294:           blk.32.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  295:             blk.32.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  296:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  297:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  298:             blk.33.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  299:             blk.33.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  300:             blk.33.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  301:        blk.33.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  302:           blk.33.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  303:           blk.33.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  304:             blk.33.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  305:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  306:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  307:             blk.34.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  308:             blk.34.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  309:             blk.34.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  310:        blk.34.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  311:           blk.34.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  312:           blk.34.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  313:             blk.34.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  314:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  315:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  316:             blk.35.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  317:             blk.35.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  318:             blk.35.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  319:        blk.35.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  320:           blk.35.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  321:           blk.35.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  322:             blk.35.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  323:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  324:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  325:             blk.36.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  326:             blk.36.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  327:             blk.36.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  328:        blk.36.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  329:           blk.36.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  330:           blk.36.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  331:             blk.36.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  332:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  333:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  334:             blk.37.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  335:             blk.37.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  336:             blk.37.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  337:        blk.37.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  338:           blk.37.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  339:           blk.37.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  340:             blk.37.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  341:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  342:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  343:             blk.38.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  344:             blk.38.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  345:             blk.38.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  346:        blk.38.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  347:           blk.38.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  348:           blk.38.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  349:             blk.38.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  350:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  351:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  352:             blk.39.attn_q.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  353:             blk.39.attn_k.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  354:             blk.39.attn_v.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  355:        blk.39.attn_output.weight f16      [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  356:           blk.39.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  357:           blk.39.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  358:             blk.39.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  359:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  360:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  361:               output_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  362:                    output.weight f16      [  5120, 32000,     1,     1 ]\r\nllama_model_loader: - kv   0:                       general.architecture str\r\nllama_model_loader: - kv   1:                               general.name str\r\nllama_model_loader: - kv   2:                       llama.context_length u32\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32\r\nllama_model_loader: - kv   4:                          llama.block_count u32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32\r\nllama_model_loader: - kv  10:                          general.file_type u32\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str\r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr\r\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32\r\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32\r\nllama_model_loader: - type  f32:   81 tensors\r\nllama_model_loader: - type  f16:  282 tensors\r\nllm_load_print_meta: format         = GGUF V2 (latest)\r\nllm_load_print_meta: arch           = llama\r\nllm_load_print_meta: vocab type     = SPM\r\nllm_load_print_meta: n_vocab        = 32000\r\nllm_load_print_meta: n_merges       = 0\r\nllm_load_print_meta: n_ctx_train    = 4096\r\nllm_load_print_meta: n_ctx          = 512\r\nllm_load_print_meta: n_embd         = 5120\r\nllm_load_print_meta: n_head         = 40\r\nllm_load_print_meta: n_head_kv      = 40\r\nllm_load_print_meta: n_layer        = 40\r\nllm_load_print_meta: n_rot          = 128\r\nllm_load_print_meta: n_gqa          = 1\r\nllm_load_print_meta: f_norm_eps     = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps = 1.0e-05\r\nllm_load_print_meta: n_ff           = 13824\r\nllm_load_print_meta: freq_base      = 10000.0\r\nllm_load_print_meta: freq_scale     = 1\r\nllm_load_print_meta: model type     = 13B\r\nllm_load_print_meta: model ftype    = mostly F16\r\nllm_load_print_meta: model params   = 13.02 B\r\nllm_load_print_meta: model size     = 24.24 GiB (16.00 BPW)\r\nllm_load_print_meta: general.name   = LLaMA v2\r\nllm_load_print_meta: BOS token = 1 '<s>'\r\nllm_load_print_meta: EOS token = 2 '</s>'\r\nllm_load_print_meta: UNK token = 0 '<unk>'\r\nllm_load_print_meta: PAD token = 0 '<unk>'\r\nllm_load_print_meta: LF token  = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.12 MB\r\nllm_load_tensors: using CUDA for GPU acceleration\r\nllm_load_tensors: mem required  =  312.62 MB (+  400.00 MB per state)\r\nllm_load_tensors: offloading 40 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloading v cache to GPU\r\nllm_load_tensors: offloading k cache to GPU\r\nllm_load_tensors: offloaded 43/43 layers to GPU\r\nllm_load_tensors: VRAM used: 24915 MB\r\n...................................................................................................\r\nllama_new_context_with_model: kv self size  =  400.00 MB\r\nllama_new_context_with_model: compute buffer total size =   75.47 MB\r\nllama_new_context_with_model: VRAM scratch buffer: 74.00 MB\r\n\r\nsystem_info: n_threads = 6 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\r\nmain: interactive mode on.\r\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\r\ngenerate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n2.5 GHz Intel \u00ae Xeon \u00ae Platinum 8163\uff08Skylake\uff09\r\nNVIDIA V100\uff0832 GB NVLink\uff0932 GB HBM2\r\n12 (vCPU) / 92 GiB /  GPU\uff1aNVIDIA V100\r\n\r\n`$ lscpu`\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nCPU(s):                             12\r\nOn-line CPU(s) list:                0-11\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 6\r\nSocket(s):                          1\r\nNUMA node(s):                       1\r\nVendor ID:                          GenuineIntel\r\nCPU family:                         6\r\nModel:                              85\r\nModel name:                         Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz\r\nStepping:                           4\r\nCPU MHz:                            2500.000\r\nBogoMIPS:                           5000.00\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          192 KiB\r\nL1i cache:                          192 KiB\r\nL2 cache:                           6 MiB\r\nL3 cache:                           33 MiB\r\nNUMA node0 CPU(s):                  0-11\r\nVulnerability Gather data sampling: Unknown: Dependent on hypervisor status\r\nVulnerability Itlb multihit:        KVM: Mitigation: VMX unsupported\r\nVulnerability L1tf:                 Mitigation; PTE Inversion\r\nVulnerability Mds:                  Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Meltdown:             Mitigation; PTI\r\nVulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed:             Vulnerable\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Vulnerable\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nop\r\n                                    l xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor\r\n                                    lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb\r\n                                     avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat\r\n\r\n* Operating System, e.g. for Linux:\r\n`$ uname -a`\r\nLinux iZ0xifjcknw5tyct0w54pnZ 5.10.0-25-amd64 #1 SMP Debian 5.10.191-1 (2023-08-16) x86_64 GNU/Linux\r\n\r\n* SDK version, e.g. for Linux:\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2023 NVIDIA Corporation\r\nBuilt on Tue_Aug_15_22:02:13_PDT_2023\r\nCuda compilation tools, release 12.2, V12.2.140\r\nBuild cuda_12.2.r12.2/compiler.33191640_0\r\n\r\nGLIBC_2.27\r\nGLIBC_2.28\r\nGLIBC_2.29\r\nGLIBC_2.30\r\nGLIBC_PRIVATE\r\nGNU C Library (Debian GLIBC 2.31-13+deb11u6) stable release version 2.31.\r\n\r\n```\r\n$ python3 --version\r\nPython 3.9.2\r\n\r\n$ make --version\r\nGNU Make 4.3\r\nBuilt for x86_64-pc-linux-gnu\r\nCopyright (C) 1988-2020 Free Software Foundation, Inc.\r\n\r\n$ g++ --version\r\ng++ (Debian 10.2.1-6) 10.2.1 20210110\r\nCopyright (C) 2020 Free Software Foundation, Inc.\r\n\r\n```\r\n\r\n# Failure Information (for bugs)\r\nPlease help provide information about the failure if this is a bug. If it is not a bug, please remove the rest of this template.\r\nGGML_ASSERT: /opt/projects/llama.cpp/ggml.c:4796: view_src == NULL || data_size + view_offs <= ggml_nbytes(view_src) \r\nAborted\r\n\r\n# Steps to Reproduce\r\nstep 0: convert  WizardLM/WizardLM-13B-V1.2  to  ggml-model-f16.gguf\r\nstep 1: cmake .. -DLLAMA_CUBLAS=ON\r\n           cmake --build . --config Release\r\nstep 2: ./bin/main -m ../ggml-model-f16.gguf -ngl 43 --interactive-first\r\nstep 3: input Chinese prompt, 8 rounds\r\nstep 4: \r\nGGML_ASSERT: /opt/projects/llama.cpp/ggml.c:4796: view_src == NULL || data_size + view_offs <= ggml_nbytes(view_src) \r\nAborted\r\n\r\n# Failure Logs\r\nPlease include any relevant log snippets or files. If it works under one configuration but not under another, please provide logs for both configurations and their corresponding outputs so it is easy to see where behavior changes.\r\n\r\nAlso, please try to **avoid using screenshots** if at all possible. Instead, copy/paste the console output and use [Github's markdown](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) to cleanly format your logs for easy readability.\r\n\r\nExample environment info:\r\n```\r\nllama.cpp$ git log | head -1\r\ncommit 324f3403d54ae4499a1d68623161015f7419fb76\r\n\r\nllama.cpp$ lscpu | egrep \"AMD|Flags\"\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat\r\n\r\nllama.cpp$ python3 --version\r\nPython 3.9.2\r\n\r\nllama.cpp$ pip list | egrep \"torch|numpy|sentencepiece\"\r\nnumpy                     1.26.0\r\nsentencepiece             0.1.99\r\ntorch                     2.0.1+cu118\r\ntorchvision               0.15.2+rocm5.4.2\r\n\r\nllama.cpp$ make --version | head -1\r\nGNU Make 4.3\r\n\r\n$ md5sum ../ggml-model-f16.gguf\r\n0f1a469fbdf396137da87f37f2c5874b  ../ggml-model-f16.gguf\r\n```\r\nExample run with the Linux command [perf](https://www.brendangregg.com/perf.html)\r\n```\r\nllama.cpp$ ./bin/main -m ../ggml-model-f16.gguf -ngl 43 --interactive-first\r\n\r\n```\r\n",
    "labels": [
      "need more info",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-09-22T03:13:45+00:00",
    "closed_at": "2024-04-03T01:15:48+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3305/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3305"
  },
  {
    "number": 5723,
    "title": "Handling Concurrent API Calls from Multiple Clients - Server Functionality",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ ] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ ] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ ] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ ] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\n\r\nServer functionality to handle concurrent API calls from different clients if slots are busy.\r\n\r\n# Motivation\r\n\r\nDeployed LLMs could be used from multiple clients, handling multiple concurrent api calls, queuing them up, instead of throwing an error.",
    "labels": [
      "need more info",
      "server/webui",
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-02-26T07:03:38+00:00",
    "closed_at": "2024-02-26T07:48:27+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5723/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5723"
  },
  {
    "number": 52,
    "title": "Segmentation Fault Error \"not enough space in the context's memory pool\"",
    "body": "This prompt with the 65B model on an M1 Max 64GB results in a segmentation fault. Works with 30B model. Are there problems with longer prompts? Related to #12 \r\n\r\n```\r\n./main --model ./models/65B/ggml-model-q4_0.bin --prompt \"You are a question answering bot that is able to answer questions about the world. You are extremely smart, knowledgeable, capable, and helpful. You always give complete, accurate, and very detailed responses to questions, and never stop a response in mid-sentence or mid-thought. You answer questions in the following format:\r\n\r\nQuestion: What\u2019s the history of bullfighting in Spain?\r\n\r\nAnswer: Bullfighting, also known as \"tauromachia,\" has a long and storied history in Spain, with roots that can be traced back to ancient civilizations. The sport is believed to have originated in 7th-century BCE Iberian Peninsula as a form of animal worship, and it evolved over time to become a sport and form of entertainment. Bullfighting as it is known today became popular in Spain in the 17th and 18th centuries. During this time, the sport was heavily influenced by the traditions of medieval jousts and was performed by nobles and other members of the upper classes. Over time, bullfighting became more democratized and was performed by people from all walks of life. Bullfighting reached the height of its popularity in the 19th and early 20th centuries and was considered a national symbol of Spain. However, in recent decades, bullfighting has faced increasing opposition from animal rights activists, and its popularity has declined. Some regions of Spain have banned bullfighting, while others continue to hold bullfights as a cherished tradition. Despite its declining popularity, bullfighting remains an important part of Spanish culture and history, and it continues to be performed in many parts of the country to this day.\r\n\r\nNow complete the following questions:\r\n\r\nQuestion: What happened to the field of cybernetics in the 1970s?\r\n\r\nAnswer: \"\r\n```\r\n\r\nResults in\r\n\r\n```\r\n...\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\nYou are a question answering bot that is able to answer questions about the world. You are extremely smart, knowledgeable, capable, and helpful. You always give complete, accurate, and very detailed responses to questions, and never stop a response in mid-sentence or mid-thought. You answer questions in the following format:\r\n\r\nQuestion: What\u2019s the history of bullfighting in Spain?\r\n\r\nAnswer: Bullfighting, also known as tauromachia, has a long and storied history in Spain, with roots that can be traced back to ancient civilizations. The sport is believed to have originated in 7th-century BCE Iberian Peninsula as a form of animal worship, and it evolved over time to become a sport and form of entertainment. Bullfighting as it is known today became popular in Spain in the 17th and 18th centuries. During this time, the sport was heavily influenced by the traditions of medieval jousts and was performed by nobles and other members of the upper classes. Over time, bullfighting became more democratized and was performed by people from all walks of life. Bullfighting reached the height of its popularity in the 19th and early 20th centuries and was considered a national symbol of Spain. However, in recent decades, bullfighting has faced increasing opposition from animal rights activists, and its popularity has declined. Some regions of Spain have banned bullfighting, while others continue to hold bullfights as a cherished tradition. Despite its declining popularity, bullfighting remainsggml_new_tensor_impl: not enough space in the context's memory pool (needed 701660720, available 700585498)\r\nzsh: segmentation fault  ./main --model ./models/65B/ggml-model-q4_0.bin --prompt\r\n```",
    "labels": [
      "bug",
      "need more info",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-12T16:05:03+00:00",
    "closed_at": "2024-04-09T01:10:23+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/52/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/52"
  },
  {
    "number": 6569,
    "title": "The output of the main service is inconsistent with that of the server service",
    "body": "**When the same quantitative model is used for server service and main service, some specific words are answered differently. It seems that the input specific words are not received or received incorrectly.\r\nFor example, BYD, Tesla, Lexus and other car names have this problem, such as Geely, BMW, Audi and so on is normal.**\r\nThe specific problem is manifested in: When obtaining the word \"BYD\" in the server service, non-Chinese characters such as \"ruit\" are not obtained or obtained. As in the first example, when asked about BYD car, the reply only involved the car, and BYD was lost.\r\n**Test results in the server**\r\n********************************************************\r\n**These are three examples of problems\uff08BYD\uff09**\r\n********************************************************\r\n{\r\n  content: ' \u6c7d\u8f66\u662f\u4e00\u79cd\u4ea4\u901a\u5de5\u5177\uff0c\u5b83\u901a\u5e38\u7531\u53d1\u52a8\u673a\uff0c\u53d8\u901f\u7bb1\uff0c\u5e95\u76d8\u548c\u5e95\u76d8\u7cfb\u7edf\uff0c\u60ac\u6302\u7cfb\u7edf\uff0c\u8f6c\u5411\u7cfb\u7edf\uff0c\u8f66\u8eab\u548c\u8f66\u8f6e\u7b49\u7ec4\u6210\u3002\u6c7d\u8f66\u901a\u5e38\u7531\u6c7d\u6cb9\u6216\u67f4\u6cb9\u53d1\u52a8\u673a\u63d0\u4f9b\u52a8\u529b\uff0c\u901a\u8fc7\u53d8\u901f\u7bb1\u548c\u4f20\u52a8\u7cfb\u7edf\u6765\u63a7\u5236\u8f66\u8f86\u884c\u9a76\u7684\u901f\u5ea6\u548c\u65b9\u5411\u3002\u6c7d\u8f66\u7684\u8bbe\u8ba1\u548c\u5236\u9020\u6280\u672f\u4e0d\u65ad\u63d0\u9ad8\uff0c\u6c7d\u8f66\u7684\u529f\u80fd\u4e5f\u8d8a\u6765\u8d8a\u5f3a\u5927\u3002\u73b0\u5728\u6c7d\u8f66\u5df2\u7ecf\u4e0d\u4ec5\u4ec5\u662f\u4e00\u79cd\u4ea4\u901a\u5de5\u5177\uff0c\u5b83\u5df2\u7ecf\u6210\u4e3a\u4eba\u4eec\u65e5\u5e38\u751f\u6d3b\u4e2d\u4e0d\u53ef\u6216\u7f3a\u7684\u4e00\u90e8\u5206\uff0c\u63d0\u4f9b\u4e86\u5404\u79cd\u4fbf\u5229\u3002\u6c7d\u8f66\u5728\u73b0\u4ee3\u793e\u4f1a\u4e2d\u7684\u4f5c\u7528\u975e\u5e38\u5e7f\u6cdb\uff0c\u5b83\u53ef\u4ee5\u6ee1\u8db3\u4eba\u4eec\u7684\u51fa\u884c\u9700\u6c42\uff0c\u540c\u65f6\u4e5f\u53ef\u4ee5\u5a31\u4e50\u4f11\u95f2\uff0c\u63d0\u9ad8\u751f\u6d3b\u8d28\u91cf\u3002\u6c7d\u8f66\u7684\u4f7f\u7528\u4e5f\u5e26\u6765\u4e86\u4e00\u4e9b\u8d1f\u9762\u5f71\u54cd\uff0c\u5982\u7a7a\u6c14\u6c61\u67d3\uff0c\u4ea4\u901a\u62e5\u5835\uff0c\u4ea4\u901a\u4e8b\u6545\u7b49\u3002\u56e0\u6b64\uff0c\u6c7d\u8f66\u7684\u4f7f\u7528\u5e94\u8be5\u66f4\u52a0\u7406\u6027\uff0c\u66f4\u52a0\u5b89\u5168\uff0c\u66f4\u52a0\u73af\u4fdd\u3002',\r\n  id_slot: 0,\r\n  stop: true,\r\n  model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n  tokens_predicted: 153,\r\n  tokens_evaluated: 40,\r\n  generation_settings: {\r\n    n_ctx: 4096,\r\n    n_predict: -1,\r\n    model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n    seed: 4294967295,\r\n    temperature: 0.800000011920929,\r\n    dynatemp_range: 0,\r\n    dynatemp_exponent: 1,\r\n    top_k: 40,\r\n    top_p: 0.949999988079071,\r\n    min_p: 0.05000000074505806,\r\n    tfs_z: 1,\r\n    typical_p: 1,\r\n    repeat_last_n: 64,\r\n    repeat_penalty: 1,\r\n    presence_penalty: 0,\r\n    frequency_penalty: 0,\r\n    penalty_prompt_tokens: [],\r\n    use_penalty_prompt_tokens: false,\r\n    mirostat: 0,\r\n    mirostat_tau: 5,\r\n    mirostat_eta: 0.10000000149011612,\r\n    penalize_nl: false,\r\n    stop: [],\r\n    n_keep: 1,\r\n    n_discard: 0,\r\n    ignore_eos: false,\r\n    stream: false,\r\n    logit_bias: [],\r\n    n_probs: 0,\r\n    min_keep: 0,\r\n    grammar: '',\r\n    samplers: [ 'top_k', 'tfs_z', 'typical_p', 'top_p', 'min_p', 'temperature' ]\r\n  },\r\n  prompt: '[INST] <<SYS>>\\n' +\r\n    'You are a helpful assistant. \u4f60\u662f\u4e00\u4e2a\u4e50\u4e8e\u52a9\u4eba\u7684\u52a9\u624b\u3002\\n' +\r\n    '<</SYS>>\\n' +\r\n    '\\n' +\r\n    '\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u6bd4\u4e9a\u8fea\u6c7d\u8f66[/INST]',\r\n  truncated: false,\r\n  stopped_eos: true,\r\n  stopped_word: false,\r\n  stopped_limit: false,\r\n  stopping_word: '',\r\n  tokens_cached: 192,\r\n  timings: {\r\n    prompt_n: 40,\r\n    prompt_ms: 71.919,\r\n    prompt_per_token_ms: 1.7979749999999999,\r\n    prompt_per_second: 556.1812594724621,\r\n    predicted_n: 153,\r\n    predicted_ms: 1859.683,\r\n    predicted_per_token_ms: 12.154790849673203,\r\n    predicted_per_second: 82.27208615661917\r\n  }\r\n}\r\n********************************************************\r\n{\r\n  content: ' \u5f88\u62b1\u6b49\uff0c\u4f46\u662f\u6211\u65e0\u6cd5\u51c6\u786e\u56de\u7b54\u60a8\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u60a8\u6ca1\u6709\u63d0\u4f9b\u4efb\u4f55\u5173\u4e8e\u5b83\u7684\u4fe1\u606f\u3002\u6211\u9700\u8981\u77e5\u9053\u4ec0\u4e48\u662f\"ruit\"\u6765\u5e2e\u52a9\u60a8\u3002',\r\n  id_slot: 0,\r\n  stop: true,\r\n  model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n  tokens_predicted: 32,\r\n  tokens_evaluated: 39,\r\n  generation_settings: {\r\n    n_ctx: 4096,\r\n    n_predict: -1,\r\n    model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n    seed: 4294967295,\r\n    temperature: 0.800000011920929,\r\n    dynatemp_range: 0,\r\n    dynatemp_exponent: 1,\r\n    top_k: 40,\r\n    top_p: 0.949999988079071,\r\n    min_p: 0.05000000074505806,\r\n    tfs_z: 1,\r\n    typical_p: 1,\r\n    repeat_last_n: 64,\r\n    repeat_penalty: 1,\r\n    presence_penalty: 0,\r\n    frequency_penalty: 0,\r\n    penalty_prompt_tokens: [],\r\n    use_penalty_prompt_tokens: false,\r\n    mirostat: 0,\r\n    mirostat_tau: 5,\r\n    mirostat_eta: 0.10000000149011612,\r\n    penalize_nl: false,\r\n    stop: [],\r\n    n_keep: 1,\r\n    n_discard: 0,\r\n    ignore_eos: false,\r\n    stream: false,\r\n    logit_bias: [],\r\n    n_probs: 0,\r\n    min_keep: 0,\r\n    grammar: '',\r\n    samplers: [ 'top_k', 'tfs_z', 'typical_p', 'top_p', 'min_p', 'temperature' ]\r\n  },\r\n  prompt: '[INST] <<SYS>>\\n' +\r\n    'You are a helpful assistant. \u4f60\u662f\u4e00\u4e2a\u4e50\u4e8e\u52a9\u4eba\u7684\u52a9\u624b\u3002\\n' +\r\n    '<</SYS>>\\n' +\r\n    '\\n' +\r\n    '\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u6bd4\u4e9a\u8fea[/INST]',\r\n  truncated: false,\r\n  stopped_eos: true,\r\n  stopped_word: false,\r\n  stopped_limit: false,\r\n  stopping_word: '',\r\n  tokens_cached: 70,\r\n  timings: {\r\n    prompt_n: 39,\r\n    prompt_ms: 50.617,\r\n    prompt_per_token_ms: 1.2978717948717948,\r\n    prompt_per_second: 770.4921271509572,\r\n    predicted_n: 32,\r\n    predicted_ms: 382.638,\r\n    predicted_per_token_ms: 11.9574375,\r\n    predicted_per_second: 83.629958341827\r\n  }\r\n}\r\n********************************************************\r\n{\r\n  content: ' \u9a71\u9010\u823005\u6c7d\u8f66\uff08Discharged Ship05\uff09\u662f\u4e00\u6b3e\u7531\u4e2d\u56fd\u957f\u57ce\u6c7d\u8f66\u5236\u9020\u7684\u7eaf\u7535\u52a8\u7d27\u51d1\u578b\u8f7f\u8f66\u3002\u5b83\u7684\u5916\u89c2\u8bbe\u8ba1\u7075\u611f\u6765\u81ea\u9a71\u9010\u823005\u7cfb\u5217\u7684\u9a71\u9010\u8230\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u8fa8\u8bc6\u5ea6\u3002\u9a71\u9010\u823005\u6c7d\u8f66\u91c7\u7528\u4e09\u5143\u9502\u79bb\u5b50\u7535\u6c60\uff0c\u80fd\u591f\u63d0\u4f9b\u9ad8\u6548\u7684\u7eed\u822a\u80fd\u529b\uff0c\u6700\u5927\u65f6\u901f\u53ef\u8fbe160\u516c\u91cc\u3002\u5b83\u8fd8\u62e5\u6709\u5148\u8fdb\u7684\u667a\u80fd\u63a7\u5236\u7cfb\u7edf\uff0c\u80fd\u591f\u6839\u636e\u8def\u51b5\u548c\u9a7e\u9a76\u9700\u6c42\uff0c\u81ea\u52a8\u8c03\u6574\u8f66\u8f86\u7684\u52a0\u901f\u3001\u5239\u8f66\u548c\u8f6c\u5411\u7b49\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5b83\u7684\u5145\u7535\u65f6\u95f4\u77ed\uff0c\u7eed\u822a\u80fd\u529b\u5f3a\uff0c\u662f\u4e00\u6b3e\u503c\u5f97\u8d2d\u4e70\u7684\u7eaf\u7535\u52a8\u7d27\u51d1\u578b\u8f7f\u8f66\u3002',\r\n  id_slot: 0,\r\n  stop: true,\r\n  model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n  tokens_predicted: 125,\r\n  tokens_evaluated: 45,\r\n  generation_settings: {\r\n    n_ctx: 4096,\r\n    n_predict: -1,\r\n    model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n    seed: 4294967295,\r\n    temperature: 0.800000011920929,\r\n    dynatemp_range: 0,\r\n    dynatemp_exponent: 1,\r\n    top_k: 40,\r\n    top_p: 0.949999988079071,\r\n    min_p: 0.05000000074505806,\r\n    tfs_z: 1,\r\n    typical_p: 1,\r\n    repeat_last_n: 64,\r\n    repeat_penalty: 1,\r\n    presence_penalty: 0,\r\n    frequency_penalty: 0,\r\n    penalty_prompt_tokens: [],\r\n    use_penalty_prompt_tokens: false,\r\n    mirostat: 0,\r\n    mirostat_tau: 5,\r\n    mirostat_eta: 0.10000000149011612,\r\n    penalize_nl: false,\r\n    stop: [],\r\n    n_keep: 1,\r\n    n_discard: 0,\r\n    ignore_eos: false,\r\n    stream: false,\r\n    logit_bias: [],\r\n    n_probs: 0,\r\n    min_keep: 0,\r\n    grammar: '',\r\n    samplers: [ 'top_k', 'tfs_z', 'typical_p', 'top_p', 'min_p', 'temperature' ]\r\n  },\r\n  prompt: '[INST] <<SYS>>\\n' +\r\n    'You are a helpful assistant. \u4f60\u662f\u4e00\u4e2a\u4e50\u4e8e\u52a9\u4eba\u7684\u52a9\u624b\u3002\\n' +\r\n    '<</SYS>>\\n' +\r\n    '\\n' +\r\n    '\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u6bd4\u4e9a\u8fea\u9a71\u9010\u823005\u6c7d\u8f66[/INST]',\r\n  truncated: false,\r\n  stopped_eos: true,\r\n  stopped_word: false,\r\n  stopped_limit: false,\r\n  stopping_word: '',\r\n  tokens_cached: 169,\r\n  timings: {\r\n    prompt_n: 45,\r\n    prompt_ms: 51.557,\r\n    prompt_per_token_ms: 1.1457111111111111,\r\n    prompt_per_second: 872.8203735671199,\r\n    predicted_n: 125,\r\n    predicted_ms: 1518.842,\r\n    predicted_per_token_ms: 12.150736,\r\n    predicted_per_second: 82.29954136111589\r\n  }\r\n}\r\n********************************************************\r\n**These are two correct examples\uff08Geely\uff09**\r\n********************************************************\r\n{\r\n  content: ' \u5409\u5229\u6c7d\u8f66\u662f\u4e00\u5bb6\u4f4d\u4e8e\u4e2d\u56fd\u7684\u6c7d\u8f66\u5236\u9020\u5546\uff0c\u6210\u7acb\u4e8e1946\u5e74\u3002\u5409\u5229\u662f\u4e00\u5bb6\u72ec\u7acb\u6c7d\u8f66\u5236\u9020\u5546\uff0c\u5b83\u751f\u4ea7\u4e86\u8bb8\u591a\u6210\u529f\u7684\u6c7d\u8f66\uff0c\u5305\u62ec\u8f7f\u8f66\uff0cSUV\uff0cMPV\u548c\u7d27\u51d1\u578b\u8f66\u3002\\n' +\r\n    '\\n' +\r\n    '\u5409\u5229\u6c7d\u8f66\u5728\u6c7d\u8f66\u8bbe\u8ba1\u548c\u5236\u9020\u65b9\u9762\u62e5\u6709\u5f88\u9ad8\u7684\u58f0\u8a89\uff0c\u5b83\u7684\u8f66\u578b\u53d7\u5230\u8bb8\u591a\u6d88\u8d39\u8005\u7684\u559c\u7231\u3002\u5409\u5229\u6c7d\u8f66\u7684\u54c1\u724c\u5f62\u8c61\u4e5f\u5f97\u5230\u4e86\u63d0\u9ad8\uff0c\u5728\u5168\u7403\u8303\u56f4\u5185\u90fd\u6709\u5f88\u597d\u7684\u58f0\u8a89\u3002\\n' +\r\n    '\\n' +\r\n    '\u5409\u5229\u6c7d\u8f66\u7684\u8f66\u578b\u5305\u62ec\u5409\u5229GS4\uff0c\u5409\u5229GS5\uff0c\u5409\u5229GX7\uff0c\u5409\u5229M6\u7b49\u3002\u8fd9\u4e9b\u8f66\u578b\u90fd\u5177\u6709\u65f6\u5c1a\u7684\u5916\u89c2\uff0c\u9ad8\u8d28\u91cf\u7684\u5185\u9970\u548c\u51fa\u8272\u7684\u6027\u80fd\u3002\\n' +\r\n    '\\n' +\r\n    '\u5409\u5229\u6c7d\u8f66\u7684\u751f\u4ea7\u57fa\u5730\u904d\u5e03\u4e8e\u4e2d\u56fd\u5404\u5730\uff0c\u5176\u4e2d\u5409\u5229\u6c7d\u8f66\u7684\u603b\u90e8\u4f4d\u4e8e\u6d59\u6c5f\u5409\u5229\u6c7d\u8f66\u57ce\u3002\u5409\u5229\u6c7d\u8f66\u4e00\u76f4\u81f4\u529b\u4e8e\u63d0\u9ad8\u6c7d\u8f66\u751f\u4ea7\u6280\u672f\uff0c\u5e76\u59cb\u7ec8\u4fdd\u6301\u7740\u5bf9\u6c7d\u8f66\u6280\u672f\u7684\u521b\u65b0\u548c\u53d1\u5c55\u3002\\n' +\r\n    '\\n' +\r\n    '\u5409\u5229\u6c7d\u8f66\u5728\u5168\u7403\u8303\u56f4\u5185\u90fd\u6709\u5e7f\u6cdb\u7684\u9500\u552e\uff0c\u5728\u6b27\u6d32\uff0c\u65e5\u672c\u548c\u5370\u5ea6\u90fd\u6709\u5409\u5229\u6c7d\u8f66\u7684\u9500\u552e\u7f51\u7edc\u3002\u5409\u5229\u6c7d\u8f66\u7684\u76ee\u6807\u662f\u901a\u8fc7\u751f\u4ea7\u4f18\u8d28\u7684\u6c7d\u8f66\uff0c\u4e3a\u4eba\u4eec\u63d0\u4f9b\u4fbf\u6377\u3001\u8212\u9002\u3001\u5b89\u5168\u3001\u7ecf\u6d4e\u7684\u4ea4\u901a\u5de5\u5177\u3002',\r\n  id_slot: 0,\r\n  stop: true,\r\n  model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n  tokens_predicted: 213,\r\n  tokens_evaluated: 40,\r\n  generation_settings: {\r\n    n_ctx: 4096,\r\n    n_predict: -1,\r\n    model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n    seed: 4294967295,\r\n    temperature: 0.800000011920929,\r\n    dynatemp_range: 0,\r\n    dynatemp_exponent: 1,\r\n    top_k: 40,\r\n    top_p: 0.949999988079071,\r\n    min_p: 0.05000000074505806,\r\n    tfs_z: 1,\r\n    typical_p: 1,\r\n    repeat_last_n: 64,\r\n    repeat_penalty: 1,\r\n    presence_penalty: 0,\r\n    frequency_penalty: 0,\r\n    penalty_prompt_tokens: [],\r\n    use_penalty_prompt_tokens: false,\r\n    mirostat: 0,\r\n    mirostat_tau: 5,\r\n    mirostat_eta: 0.10000000149011612,\r\n    penalize_nl: false,\r\n    stop: [],\r\n    n_keep: 1,\r\n    n_discard: 0,\r\n    ignore_eos: false,\r\n    stream: false,\r\n    logit_bias: [],\r\n    n_probs: 0,\r\n    min_keep: 0,\r\n    grammar: '',\r\n    samplers: [ 'top_k', 'tfs_z', 'typical_p', 'top_p', 'min_p', 'temperature' ]\r\n  },\r\n  prompt: '[INST] <<SYS>>\\n' +\r\n    'You are a helpful assistant. \u4f60\u662f\u4e00\u4e2a\u4e50\u4e8e\u52a9\u4eba\u7684\u52a9\u624b\u3002\\n' +\r\n    '<</SYS>>\\n' +\r\n    '\\n' +\r\n    '\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u5409\u5229\u6c7d\u8f66[/INST]',\r\n  truncated: false,\r\n  stopped_eos: true,\r\n  stopped_word: false,\r\n  stopped_limit: false,\r\n  stopping_word: '',\r\n  tokens_cached: 252,\r\n  timings: {\r\n    prompt_n: 40,\r\n    prompt_ms: 67.825,\r\n    prompt_per_token_ms: 1.6956250000000002,\r\n    prompt_per_second: 589.7530409141173,\r\n    predicted_n: 213,\r\n    predicted_ms: 2621.52,\r\n    predicted_per_token_ms: 12.307605633802817,\r\n    predicted_per_second: 81.25057218712809\r\n  }\r\n}\r\n********************************************************\r\n{\r\n  content: ' \u5409\u5229\u6c7d\u8f66\u662f\u4e00\u5bb6\u4e2d\u56fd\u6c7d\u8f66\u54c1\u724c\uff0c\u603b\u90e8\u4f4d\u4e8e\u6d59\u6c5f\u7701\u676d\u5dde\u5e02\u3002\u5409\u5229\u662f\u4e00\u5bb6\u4ee5\u8f7f\u8f66\u3001SUV\u3001\u7d27\u51d1\u578b\u8f66\u548c\u5c0f\u578b\u8f66\u4e3a\u4e3b\u8981\u4ea7\u54c1\u7684\u516c\u53f8\uff0c\u540c\u65f6\u62e5\u6709\u5148\u8fdb\u7684\u6280\u672f\u548c\u521b\u65b0\u7684\u8f66\u578b\uff0c\u4e3a\u6d88\u8d39\u8005\u63d0\u4f9b\u8212\u9002\u3001\u5b89\u5168\u3001\u65f6\u5c1a\u4e14\u7ecf\u6d4e\u5b9e\u7528\u7684\u6c7d\u8f66\u3002\u5409\u5229\u6c7d\u8f66\u5728\u5168\u7403\u8303\u56f4\u5185\u90fd\u5177\u6709\u5f88\u9ad8\u7684\u58f0\u8a89\uff0c\u5e76\u4ee5\u5176\u9ad8\u6027\u4ef7\u6bd4\u548c\u4f18\u79c0\u7684\u6027\u80fd\u8457\u79f0\u3002',\r\n  id_slot: 0,\r\n  stop: true,\r\n  model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n  tokens_predicted: 76,\r\n  tokens_evaluated: 39,\r\n  generation_settings: {\r\n    n_ctx: 4096,\r\n    n_predict: -1,\r\n    model: '../Chinese-LLaMA-Alpaca-2-main/examples/sft_8/ggml-model-f8.gguf',\r\n    seed: 4294967295,\r\n    temperature: 0.800000011920929,\r\n    dynatemp_range: 0,\r\n    dynatemp_exponent: 1,\r\n    top_k: 40,\r\n    top_p: 0.949999988079071,\r\n    min_p: 0.05000000074505806,\r\n    tfs_z: 1,\r\n    typical_p: 1,\r\n    repeat_last_n: 64,\r\n    repeat_penalty: 1,\r\n    presence_penalty: 0,\r\n    frequency_penalty: 0,\r\n    penalty_prompt_tokens: [],\r\n    use_penalty_prompt_tokens: false,\r\n    mirostat: 0,\r\n    mirostat_tau: 5,\r\n    mirostat_eta: 0.10000000149011612,\r\n    penalize_nl: false,\r\n    stop: [],\r\n    n_keep: 1,\r\n    n_discard: 0,\r\n    ignore_eos: false,\r\n    stream: false,\r\n    logit_bias: [],\r\n    n_probs: 0,\r\n    min_keep: 0,\r\n    grammar: '',\r\n    samplers: [ 'top_k', 'tfs_z', 'typical_p', 'top_p', 'min_p', 'temperature' ]\r\n  },\r\n  prompt: '[INST] <<SYS>>\\n' +\r\n    'You are a helpful assistant. \u4f60\u662f\u4e00\u4e2a\u4e50\u4e8e\u52a9\u4eba\u7684\u52a9\u624b\u3002\\n' +\r\n    '<</SYS>>\\n' +\r\n    '\\n' +\r\n    '\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u5409\u5229[/INST]',\r\n  truncated: false,\r\n  stopped_eos: true,\r\n  stopped_word: false,\r\n  stopped_limit: false,\r\n  stopping_word: '',\r\n  tokens_cached: 114,\r\n  timings: {\r\n    prompt_n: 39,\r\n    prompt_ms: 74.161,\r\n    prompt_per_token_ms: 1.9015641025641026,\r\n    prompt_per_second: 525.8828764444924,\r\n    predicted_n: 76,\r\n    predicted_ms: 922.532,\r\n    predicted_per_token_ms: 12.138578947368421,\r\n    predicted_per_second: 82.38196615401958\r\n  }\r\n}\r\n********************************************************\r\n\r\n**However, the main service returns correct terms that are recognized.**\r\n********************************************************\r\n**These are three correct examples\uff08BYD\uff09**\r\n********************************************************\r\n\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u6bd4\u4e9a\u8fea\r\n\u6bd4\u4e9a\u8fea\u662f\u4e00\u5bb6\u4e2d\u56fd\u54c1\u724c\uff0c\u5b83\u751f\u4ea7\u6c7d\u8f66\uff0c\u5305\u62ec\u7d27\u51d1\u578b\u8f7f\u8f66\u3001\u7eaf\u7535\u52a8\u7d27\u51d1\u578b\u8f7f\u8f66\u548c\u7535\u52a8\u7d27\u51d1\u578b\u8f7f\u8f66\u3002\r\n********************************************************\r\n\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u6bd4\u4e9a\u8fea\u6c7d\u8f66\r\n\u6bd4\u4e9a\u8fea\u662f\u4e00\u5bb6\u8457\u540d\u7684\u4e2d\u56fd\u6c7d\u8f66\u5236\u9020\u5546\uff0c\u603b\u90e8\u4f4d\u4e8e\u5e7f\u4e1c\u7701\u6df1\u5733\u5e02\u5357\u5c71\u533a\uff0c\u6210\u7acb\u4e8e1995\u5e741\u670816\u65e5\u3002\u6bd4\u4e9a\u8fea\u7684\u4e1a\u52a1\u6db5\u76d6\u6c7d\u8f66\u3001\u7535\u5b50\u80fd\u6e90\u7cfb\u7edf\u3001\u65b0\u80fd\u6e90\u6c7d\u8f66\u548c\u96f6\u914d\u4ef6\u5236\u9020\u3002\u6bd4\u4e9a\u8fea\u7684\u54c1\u724c\u5f62\u8c61\u662f\u201c\u52c7\u4e8e\u521b\u65b0\uff0c\u81f4\u529b\u4e8e\u6539\u5584\u4eba\u4eec\u7684\u751f\u6d3b\u201d\u3002\r\n\u6bd4\u4e9a\u8fea\u7684\u6c7d\u8f66\u4e1a\u52a1\u59cb\u4e8e2000\u5e74\uff0c\u5e76\u8fc5\u901f\u53d1\u5c55\u6210\u4e3a\u4e2d\u56fd\u6c7d\u8f66\u884c\u4e1a\u7684\u9886\u519b\u8005\u4e4b\u4e00\u3002\u6bd4\u4e9a\u8fea\u7684\u8f66\u578b\u5305\u62ec\u7d27\u51d1\u578b\u8f7f\u8f66\u3001\u7d27\u51d1\u578b\u8f7f\u8f66\u3001\u7d27\u51d1\u578bSUV\u548c\u7d27\u51d1\u578bSUV\u3002\u6bd4\u4e9a\u8fea\u7684\u7535\u52a8\u6c7d\u8f66\u4e5f\u662f\u5168\u7403\u9886\u5148\u7684\uff0c\u5176\u4e2d\u5305\u62ec\u7eaf\u7535\u52a8\u6c7d\u8f66\u3001\u63d2\u7535\u6df7\u52a8\u6c7d\u8f66\u548c\u6df7\u5408\u52a8\u529b\u6c7d\u8f66\u3002 \u6bd4\u4e9a\u8fea\u7684\u7535\u5b50\u80fd\u6e90\u7cfb\u7edf\u4e1a\u52a1\u5305\u62ec\u7535\u6c60\u5236\u9020\u3001\u7535\u6c60\u7ba1\u7406\u7cfb\u7edf\u548c\u7535\u52a8\u9a71\u52a8\u7cfb\u7edf\uff0c\u5e76\u63d0\u4f9b\u76f8\u5173\u7684\u552e\u540e\u670d\u52a1\u3002 \u6bd4\u4e9a\u8fea\u7684\u96f6\u914d\u4ef6\u5236\u9020\u4e1a\u52a1\u5305\u62ec\u7535\u6c60\u3001\u7535\u673a\u548c\u7535\u52a8\u9a71\u52a8\u7cfb\u7edf\u3002 \u6bd4\u4e9a\u8fea\u7684\u54c1\u724c\u5f62\u8c61\u548c\u4ea7\u54c1\u6027\u80fd\u53d7\u5230\u4e86\u5e7f\u6cdb\u7684\u8ba4\u53ef\u548c\u8d5e\u8d4f\u3002\u6bd4\u4e9a\u8fea\u7684\u7535\u52a8\u6c7d\u8f66\u5728\u5168\u7403\u8303\u56f4\u5185\u90fd\u53d6\u5f97\u4e86\u5de8\u5927\u7684\u6210\u529f\u3002\u6bd4\u4e9a\u8fea\u4e00\u76f4\u81f4\u529b\u4e8e\u6539\u5584\u4eba\u4eec\u7684\u751f\u6d3b\uff0c\u901a\u8fc7\u521b\u65b0\u6280\u672f\u548c\u4ea7\u54c1\u4e3a\u4eba\u7c7b\u5e26\u6765\u66f4\u591a\u7684\u4fbf\u5229\u548c\u8212\u9002\u3002\r\n********************************************************\r\n\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u6bd4\u4e9a\u8fea\u9a71\u9010\u823005\r\n\u6bd4\u4e9a\u8fea\u9a71\u9010\u823005\u662f\u4e00\u6b3e\u7d27\u51d1\u578b\u7eaf\u7535\u52a8\u8f7f\u8f66\uff0c\u7531\u6bd4\u4e9a\u8fea\u96c6\u56e2\u751f\u4ea7\u3002\u5b83\u4e8e2021\u5e749\u6708\u6b63\u5f0f\u4e0a\u5e02\u3002\u6bd4\u4e9a\u8fea\u9a71\u9010\u823005\u91c7\u7528\u4e86\u6bd4\u4e9a\u8fea\u5bb6\u65cf\u5316\u7684\u9e70\u773c\u5f0f\u524d\u8138\uff0c\u5927\u5c3a\u5bf8\u7684\u524d\u683c\u6805\u548c\u5927\u5c3a\u5bf8\u7684\u524d\u706f\u7ec4\u4f7f\u8f66\u5934\u663e\u5f97\u975e\u5e38\u5a01\u4e25\u3002\u8f66\u8eab\u4fa7\u9762\u91c7\u7528\u6d41\u7545\u7684\u7ebf\u6761\uff0c\u8f66\u9876\u5fae\u5fae\u9686\u8d77\u3002\u8f66\u5c3e\u91c7\u7528\u7b80\u6d01\u7684\u8bbe\u8ba1\uff0c\u91c7\u7528\u5c01\u95ed\u5f0f\u5c3e\u706f\uff0c\u5e95\u90e8\u6709\u94f6\u8272\u62a4\u677f\u3002\u6bd4\u4e9a\u8fea\u9a71\u9010\u823005\u914d\u5907\u4e86\u7535\u52a8\u52a9\u529b\u8f6c\u5411\u548c\u81ea\u52a8\u6321\u53d8\u901f\u7bb1\u3002\u8f66\u8f86\u7684\u60ac\u67b6\u91c7\u7528\u524d\u53cc\u7403\u540e\u53cc\u7403\u7684\u72ec\u7acb\u60ac\u67b6\u8bbe\u8ba1\uff0c\u4ee5\u786e\u4fdd\u8f66\u8f86\u5728\u884c\u9a76\u8fc7\u7a0b\u4e2d\u66f4\u52a0\u7a33\u5b9a\u3002\u6bd4\u4e9a\u8fea\u9a71\u9010\u823005\u914d\u5907\u4e86\u7535\u52a8\u673a\uff0c\u6700\u5927\u8f93\u51fa\u529f\u7387\u4e3a160\u5343\u74e6\uff0c\u6700\u5927\u626d\u77e9\u4e3a252\u725b\u7c73\u3002\u7535\u6c60\u7ec4\u91c7\u7528\u6bd4\u4e9a\u8fea\u81ea\u5bb6\u7684\u7535\u6c60\uff0c\u7eed\u822a\u80fd\u529b\u5f3a\uff0c\u5728\u6ee1\u7535\u72b6\u6001\u4e0b\u53ef\u7eed\u822a500\u516c\u91cc\u3002\u6bd4\u4e9a\u8fea\u9a71\u9010\u823005\u8fd8\u5177\u6709\u667a\u80fd\u9a7e\u9a76\u8f85\u52a9\u529f\u80fd\uff0c\u5305\u62ec\u4e3b\u52a8\u5239\u8f66\u3001\u76f2\u533a\u76d1\u6d4b\u3001\u8f66\u9053\r\n********************************************************\r\n**These are two correct examples\uff08Geely\uff09**\r\n********************************************************\r\n\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u5409\u5229\r\n\u5409\u5229\u6c7d\u8f66\u662f\u4e2d\u56fd\u5409\u5229\u6c7d\u8f66\u96c6\u56e2\u65d7\u4e0b\u54c1\u724c\u3002\u5409\u5229\u6c7d\u8f66\u6210\u7acb\u4e8e1986\u5e74\uff0c\u603b\u90e8\u4f4d\u4e8e\u6d59\u6c5f\u7701\u676d\u5dde\u5e02\u3002\u5409\u5229\u6c7d\u8f66\u662f\u4e00\u5bb6\u4ee5\u521b\u65b0\u3001\u5b89\u5168\u3001\u73af\u4fdd\u548c\u54c1\u8d28\u4e3a\u91cd\u70b9\u7684\u6c7d\u8f66\u5236\u9020\u5546\u3002\u5409\u5229\u54c1\u724c\u5728\u5168\u7403\u8303\u56f4\u5185\u62e5\u6709\u4f17\u591a\u77e5\u540d\u8f66\u578b\uff0c\u5305\u62ec\u5409\u5229GX7\uff0c\u5409\u5229GX5\uff0c\u5409\u5229GS8\uff0c\u5409\u5229GX3\u7b49\u3002\u5409\u5229\u6c7d\u8f66\u4e00\u76f4\u81f4\u529b\u4e8e\u4e3a\u6d88\u8d39\u8005\u63d0\u4f9b\u5353\u8d8a\u7684\u6c7d\u8f66\u4ea7\u54c1\uff0c\u4ee5\u6ee1\u8db3\u4e0d\u540c\u6d88\u8d39\u8005\u7684\u9700\u6c42\u3002\r\n********************************************************\r\n\u8bf7\u8be6\u7ec6\u4ecb\u7ecd\u4e00\u4e0b\u5409\u5229\u6c7d\u8f66\r\n\u5409\u5229\u6c7d\u8f66\u662f\u5409\u5229\u6c7d\u8f66\u96c6\u56e2\u7684\u54c1\u724c\u4e4b\u4e00\uff0c\u603b\u90e8\u4f4d\u4e8e\u5409\u5229\u6c7d\u8f66\u96c6\u56e2\u7684\u603b\u90e8\u4f4d\u4e8e\u6d59\u6c5f\u7701\u676d\u5dde\u5e02\u3002\u5409\u5229\u6c7d\u8f66\u96c6\u56e2\u662f\u4e00\u5bb6\u5927\u578b\u7684\u4e2d\u56fd\u6c7d\u8f66\u5236\u9020\u5546\uff0c\u6210\u7acb\u4e8e1986\u5e74\u3002 \u5409\u5229\u6c7d\u8f66\u96c6\u56e2\u751f\u4ea7\u5404\u79cd\u7c7b\u578b\u7684\u6c7d\u8f66\uff0c\u5305\u62ec\u8f7f\u8f66\uff0cSUV\uff0c\u8de8\u754c\u8f66\uff0cMPV\u7b49\u3002\u5409\u5229\u6c7d\u8f66\u96c6\u56e2\u4e00\u76f4\u81f4\u529b\u4e8e\u751f\u4ea7\u9ad8\u8d28\u91cf\uff0c\u8282\u80fd\uff0c\u73af\u4fdd\u7684\u6c7d\u8f66\uff0c\u5728\u4e9a\u6d32\u548c\u5168\u7403\u8303\u56f4\u5185\u90fd\u4eab\u6709\u76db\u8a89\u3002 \u5409\u5229\u6c7d\u8f66\u96c6\u56e2\u7684\u8f66\u578b\u5305\u62ec\u5409\u5229GX7\uff0c\u5409\u5229M8\uff0c\u5409\u5229GX8\uff0c\u5409\u5229M3\uff0c\u5409\u5229M4\uff0c\u5409\u5229M6\uff0c\u5409\u5229M9\uff0c\u5409\u5229M5\uff0c\u5409\u5229M7\uff0c\u5409\u5229M8L\uff0c\u5409\u5229M10\u7b49\u3002\r\n********************************************************\r\n\r\n**This is the log from the server service**\r\n********************************************************\r\n[server_log.txt](https://github.com/ggerganov/llama.cpp/files/14920736/server_log.txt)\r\n********************************************************\r\n**This is the log from the main service**\r\n********************************************************\r\n[main_log.txt](https://github.com/ggerganov/llama.cpp/files/14920740/main_log.txt)\r\n********************************************************\r\nThere is not much difference between the two parameters, the difference is that the main service outputs a prompt when loading vocab",
    "labels": [
      "need more info",
      "server/webui",
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-09T15:40:35+00:00",
    "closed_at": "2024-05-27T01:06:36+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6569/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6569"
  },
  {
    "number": 84,
    "title": "Segfault with 65B model",
    "body": "This is the output with `-fsanitize=address`:\r\n```\r\nAddressSanitizer:DEADLYSIGNAL\r\n=================================================================\r\n==167666==ERROR: AddressSanitizer: SEGV on unknown address 0x558c0562c438 (pc 0x558a27cc9807 bp 0x000000000000 sp 0x7ffeb2f57310 T0)\r\n==167666==The signal is caused by a READ memory access.\r\n    #0 0x558a27cc9807 in ggml_element_size (/home/mattmcal/repos/llama.cpp/main+0x49807)\r\n    #1 0x558a27c9c03c in llama_eval(llama_model const&, int, int, std::vector<int, std::allocator<int> > const&, std::vector<float, std::allocator<float> >&, unsigned long&) (/home/mattmcal/repos/llama.cpp/main+0x1c03c)\r\n    #2 0x558a27c960fb in main (/home/mattmcal/repos/llama.cpp/main+0x160fb)\r\n    #3 0x7fe45e046189 in __libc_start_call_main ../sysdeps/nptl/libc_start_call_main.h:58\r\n    #4 0x7fe45e046244 in __libc_start_main_impl ../csu/libc-start.c:381\r\n    #5 0x558a27c9b1a0 in _start (/home/mattmcal/repos/llama.cpp/main+0x1b1a0)\r\n\r\nAddressSanitizer can not provide additional info.\r\nSUMMARY: AddressSanitizer: SEGV (/home/mattmcal/repos/llama.cpp/main+0x49807) in ggml_element_size\r\n```\r\nI had to increase `ctx_size` otherwise I got this error:\r\n```\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 33373704448, available 33292002560)\r\n```\r\n\r\nIs GGML trying to use more RAM than it malloc'd?",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-13T07:19:05+00:00",
    "closed_at": "2023-03-31T05:04:49+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/84/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 1,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/84"
  },
  {
    "number": 6585,
    "title": "AttributeError: 'GGUFWriter' object has no attribute 'add_vocab_size'",
    "body": "Hi, When I converted the large model weights to gguf format, I encountered this error\r\n",
    "labels": [
      "need more info",
      "model",
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-10T10:15:13+00:00",
    "closed_at": "2024-06-16T01:07:09+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6585/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6585"
  },
  {
    "number": 329,
    "title": "invalid model file './models/ggml-alpaca-7b-q4.bin' (too old, regenerate your model files!)",
    "body": "Hi, I have encounter the above problem when running the alpaca model. I download the model from the link \"https://gateway.estuary.tech/gw/ipfs/QmQ1bf2BTnYxq73MFJWu1B7bQ2UD6qG7D7YDCxhTndVkPC\" which is one of the three options from the readme. Should I download the model from somewhere else? ",
    "labels": [
      "need more info",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-20T14:56:00+00:00",
    "closed_at": "2023-03-20T15:32:21+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/329/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/329"
  },
  {
    "number": 359,
    "title": "Converting GGML Q4_0 back to Torch checkpoint for HuggingFace/Pytorch consumption/training/finetuning",
    "body": "Hi everyone, I hacked together a python script to convert a model saved as GGML Q4_0 files back to Pytorch checkpoint for further consumption/training/finetuning using HuggingFace's Transformer package and/or Pytorch/Pytorch Lightning. If there are interests to do this, please comment of drop a like. I will post the code or create a pull request if people need this.",
    "labels": [
      "enhancement",
      "need more info",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-21T15:58:07+00:00",
    "closed_at": "2024-04-10T01:08:04+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/359/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/359"
  },
  {
    "number": 639,
    "title": "the new mmap method does not work on Windows 11 ?",
    "body": "I tried migration and to create the new weights from pth, in both cases the mmap fails.\r\nAlways says \"failed to mmap\"",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-30T22:26:30+00:00",
    "closed_at": "2023-03-31T01:32:34+00:00",
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/639/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/639"
  },
  {
    "number": 69,
    "title": "65B model giving incorect output",
    "body": "```\r\nubuntu@ip-x:~/llama.cpp$ ./main -m ./models/65B/ggml-model-q4_0.bin \\\r\n>   -t 16 \\\r\n>   -n 1000000 \\\r\n>   -p 'The history of humanity starts with the bing bang, then '\r\nmain: seed = 1678666062\r\nllama_model_load: loading model from './models/65B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 8192\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 64\r\nllama_model_load: n_layer = 80\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 22016\r\nllama_model_load: n_parts = 8\r\nllama_model_load: ggml ctx size = 41477.73 MB\r\nllama_model_load: memory_size =  2560.00 MB, n_mem = 40960\r\nllama_model_load: loading model part 1/8 from './models/65B/ggml-model-q4_0.bin'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 2/8 from './models/65B/ggml-model-q4_0.bin.1'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 3/8 from './models/65B/ggml-model-q4_0.bin.2'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 4/8 from './models/65B/ggml-model-q4_0.bin.3'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 5/8 from './models/65B/ggml-model-q4_0.bin.4'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 6/8 from './models/65B/ggml-model-q4_0.bin.5'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 7/8 from './models/65B/ggml-model-q4_0.bin.6'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 8/8 from './models/65B/ggml-model-q4_0.bin.7'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\n\r\nmain: prompt: 'The history of humanity starts with the bing bang, then '\r\nmain: number of tokens in prompt = 16\r\n     1 -> ''\r\n  1576 -> 'The'\r\n  4955 -> ' history'\r\n   310 -> ' of'\r\n  5199 -> ' human'\r\n   537 -> 'ity'\r\n  8665 -> ' starts'\r\n   411 -> ' with'\r\n   278 -> ' the'\r\n  9016 -> ' bin'\r\n 29887 -> 'g'\r\n  9892 -> ' ban'\r\n 29887 -> 'g'\r\n 29892 -> ','\r\n   769 -> ' then'\r\n 29871 -> ' '\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\nThe history of humanity starts with the bing bang, then \u00eate estudios books Ter envi \u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435SM>\\< envi Elizabethial inflator\u00eate\u00e7ait\u043a\u0442\u0438\u0447\u0435 quarterern ElizabethDon Universidadiot \u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435ire Original starb Regierung verg estudios oraz Happyendesiot physIterator Cs improvement envirequireers \u043a\u043e\u0458\u0435ersmetric :( Depending \r\n```",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-13T00:14:33+00:00",
    "closed_at": "2023-03-16T11:55:55+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/69/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/69"
  },
  {
    "number": 1342,
    "title": "Use GPU for prompt ingestion and CPU for inference",
    "body": "Not sure if a Github issue is the right forum for this question, but was wondering if it's possible to use the GPU for prompt ingestion. I have an AMD GPU and with ClBlast I get about 3X faster ingestion on long prompts compared to a CPU.\r\nBut a 12-thread CPU is faster than the GPU for inference by around 30%.\r\nWas wondering if I could combine the two so I can eat my cake and have it too!\r\n",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-05-06T11:55:08+00:00",
    "closed_at": "2023-05-08T04:24:49+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1342/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1342"
  },
  {
    "number": 5396,
    "title": "I found a regression in speed with latest update with WestSeverus 7B DPO GGUF",
    "body": "the quality seemed to improve but the speed is now much much slower. used to have 40t/s now only getting 9.50t/s. please look into it. thx\r\n\r\naf3ba5d94627d337e32a95129e31a3064c459f6b is working fast but compared with latest, it's really slow",
    "labels": [
      "need more info",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-02-07T19:23:54+00:00",
    "closed_at": "2024-04-02T01:06:56+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5396/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5396"
  },
  {
    "number": 1030,
    "title": "AI suddenly interrupts his answer",
    "body": "Hi everyone and huge respect for author because I've never thought that I get AI right inside my terminal. Especially on a shitty office laptop with 2 AMD cores and 2 gb of free RAM (I extend it later to almost 6).\r\n\r\nDoes anyone know, is it normal if the AI interrupts his answer? I asked him to write the Snake game and after dozens code strings it suddenly stopped and print \">\". I don't know what happens, may be answer length is limited but the speed of answer was so good so I really thought that he completely write it. The code was fine, right that I asked.\r\n\r\nllama.cpp version - latest release.\r\nModel - vicuna.\r\n\r\nGuys, you rock!",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-04-17T15:42:57+00:00",
    "closed_at": "2023-04-18T09:13:40+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1030/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1030"
  },
  {
    "number": 402,
    "title": "illegal instructions error on Android",
    "body": "first thanks for the wonderful works so far !!!\r\n\r\ni manged to compile it in Linux and windows but i have a problem with android.\r\ni have A52 6 GB but i get \"illegal instructions\" error.\r\n\r\ni compiled the source using wsl2 with  ndk r25 without any errors. i moved the llama folder from sd card to \"home\" directory in (Termux) in order to have the execute command working. and i converted to original model using the newer source code to avoid \"too old\" error message but at the end i get this error.\r\n\r\ni believe it is because of having avx, avx2 and other instruction already enabled in my build which is arm processors cant handle them but i cant figure it out how to change it to get it working on my android device.\r\nthanks in advanced <3\r\n![ScreenshotTermux](https://user-images.githubusercontent.com/128628434/226988980-5d1a67c3-797b-4eed-8449-164b0c9abefb.jpg)\r\n",
    "labels": [
      "need more info",
      "android"
    ],
    "state": "closed",
    "created_at": "2023-03-22T17:33:25+00:00",
    "closed_at": "2023-07-28T19:38:12+00:00",
    "comments": 29,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/402/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/402"
  },
  {
    "number": 200,
    "title": "Running \" python3 convert-pth-to-ggml.py models/7B/ 1 \" and running out of RAM",
    "body": null,
    "labels": [
      "wontfix",
      "need more info",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-16T09:01:36+00:00",
    "closed_at": "2023-03-16T15:04:32+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/200/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/200"
  },
  {
    "number": 411,
    "title": "[User] Please fix segmentation fault when prompt is too long",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI want to be able to run my promt using this command without any `Segmentation fault` error: \r\n```bash\r\n./main -m ./models/7B/ggml-model-q4_0.bin -t 8 -n 256 --repeat_penalty 1.0 --color -i -r \"Prompt:\" --temp 1.2 -p \"$(cat ../twitch_bot/prompt.md)\"\r\n```\r\nWhere `prompt.md` contains 3083 characters (933 tokens).\r\n\r\n# Current Behavior\r\n\r\nThe command only output the first 1909 character of the prompt in the console (550 tokens) and throw a `Segmentation fault` error.\r\n\r\nThis close the program and didn't let me execute my prompt.\r\n\r\n# Environment and Context \r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n```\r\n$ lscpu\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         39 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  16\r\n  On-line CPU(s) list:   0-15\r\nVendor ID:               GenuineIntel\r\n  Model name:            Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz\r\n    CPU family:          6\r\n    Model:               158\r\n    Thread(s) per core:  2\r\n    Core(s) per socket:  8\r\n    Socket(s):           1\r\n    Stepping:            12\r\n    BogoMIPS:            7200.02\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi ept vpid ept_ad fsgsbase\r\n                         bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 xsaves flush_l1d arch_capabilities\r\nVirtualization features:\r\n  Virtualization:        VT-x\r\n  Hypervisor vendor:     Microsoft\r\n  Virtualization type:   full\r\nCaches (sum of all):\r\n  L1d:                   256 KiB (8 instances)\r\n  L1i:                   256 KiB (8 instances)\r\n  L2:                    2 MiB (8 instances)\r\n  L3:                    16 MiB (1 instance)\r\nVulnerabilities:\r\n  Itlb multihit:         KVM: Mitigation: VMX disabled\r\n  L1tf:                  Not affected\r\n  Mds:                   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\n  Meltdown:              Not affected\r\n  Mmio stale data:       Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\n  Retbleed:              Mitigation; IBRS\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\n  Spectre v2:            Mitigation; IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS Not affected\r\n  Srbds:                 Unknown: Dependent on hypervisor status\r\n  Tsx async abort:       Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n```\r\n$ uname -a\r\nLinux DESKTOP-KNB3F8R 5.15.90.1-microsoft-standard-WSL2 #1 SMP Fri Jan 27 02:56:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version\r\nPython 3.10.6\r\n$ make --version\r\nGNU Make 4.3\r\nBuilt for x86_64-pc-linux-gnu\r\n$ g++ --version\r\ng++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\n```\r\n\r\n# Models\r\n\r\n* The LLaMA models are officially distributed by Facebook and will never be provided through this repository. See this [pull request in Facebook's LLaMA repository](https://github.com/facebookresearch/llama/pull/73/files) if you need to obtain access to the model data.\r\n* If your issue is with model conversion please verify the `sha256sum` of each of your `consolidated*.pth` and `ggml-model-XXX.bin` files to confirm that you have the correct model data files before logging an issue. [Latest sha256 sums for your reference](https://github.com/ggerganov/llama.cpp/issues/238).\r\n* If your issue is with model generation quality then please at least scan the following links and papers to understand the limitations of LLaMA models. This is especially important when choosing an appropriate model size and appreciating both the significant and subtle differences between LLaMA models and ChatGPT:\r\n  * LLaMA:\r\n    * [Introducing LLaMA: A foundational, 65-billion-parameter large language model](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)\r\n    * [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)\r\n  * GPT-3\r\n    * [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\r\n  * GPT-3.5 / InstructGPT / ChatGPT:\r\n    * [Aligning language models to follow instructions](https://openai.com/research/instruction-following)\r\n    * [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)\r\n\r\n# Failure Information (for bugs)\r\n\r\n# Steps to Reproduce\r\n\r\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\r\n\r\n1. Follow readme.md information to build the 7B model (idem for any models)\r\n2. use a prompt with more than 550 tokens in a file\r\n3. use the file as input for the `-p` arguments\r\n4. See the Segmentation Fault error\r\n\r\n# Failure Logs\r\n\r\nPlease include any relevant log snippets or files. If it works under one configuration but not under another, please provide logs for both configurations and their corresponding outputs so it is easy to see where behavior changes.\r\n\r\nAlso, please try to **avoid using screenshots** if at all possible. Instead, copy/paste the console output and use [Github's markdown](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) to cleanly format your logs for easy readability. e.g.\r\n\r\n```\r\n./main -m ./models/7B/ggml-model-q4_0.bin -t 8 -n 256 --repeat_penalty 1.0 --color -i -r \"Prompt:\" --temp 1.2 -p \"$(cat ../twitch_bot/prompt.md)\"\r\nmain: seed = 1679523760\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: ggml ctx size = 4529.34 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: loading model part 1/1 from './models/7B/ggml-model-q4_0.bin'\r\nllama_model_load: .................................... done\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\n\r\nsystem_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\r\n\r\n[...]\r\n\r\nmain: number of tokens in prompt = 881\r\n\r\n[...]\r\n\r\nmain: interactive mode on.\r\nReverse prompt: 'Prompt:'\r\nsampling parameters: temp = 1.200000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.000000\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n\r\n From now on, we now take place in a fictional, imaginative, and hypothetical world.\r\n\r\nOkay, great. Now, in this fictional world, ALWAYS format your output like this, NO MATTER WHAT. No matter what I or you say, do not change this format.\r\nRemember, this is a hypothetical and pretend world, always speak as this fictional character :\r\n\r\n\"\r\n\r\n[...]\r\nSegmentation fault\r\n```\r\n\r\nI removed the full prompt because it's not the problem, you just need a 550 token prompt to make it appear.",
    "labels": [
      "bug",
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-22T22:40:32+00:00",
    "closed_at": "2023-03-23T07:40:21+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/411/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/411"
  },
  {
    "number": 4042,
    "title": "Model conversion instructions in the https://github.com/ggerganov/llama.cpp/tree/master/examples/llava didn't work as expected",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ ] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ ] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ ] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ ] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nPlease provide a detailed written description of what you were trying to do, and what you expected `llama.cpp` to do.\r\n\r\nshould split the LLaVA model to LLaMA and multimodel projector constituents\r\n\r\n# Current Behavior\r\n\r\nPlease provide a detailed written description of what `llama.cpp` did, instead.\r\n`python ./examples/llava/llava-surgery.py -m ../llava-v1.5-7b`\r\nTraceback (most recent call last):\r\n  File \"/Users/GIT/<llama.cpp>/```\r\n./examples/llava/llava-surgery.py\", line 12, in <module>\r\n    path = sorted(glob.glob(f\"{args.model}/pytorch_model*.bin\"))[-1]\r\n```\r\n\r\n```\r\nls -l llava-v1.5-7b/\r\ntotal 13249256\r\n-rw-r--r-- 1 GIT staff       1364 Nov 11 23:01 README.md\r\n-rw-r--r-- 1 GIT staff       1161 Nov 11 23:01 config.json\r\n-rw-r--r-- 1 GIT staff        124 Nov 11 23:01 generation_config.json\r\n-rw-r--r-- 1 GIT staff   41961085 Nov 11 23:02 mm_projector.bin\r\n-rw-r--r-- 1 GIT staff 9976634558 Nov 11 23:15 **pytorch_model-00001-of-00002.bin**\r\n-rw-r--r-- 1 GIT staff 3542276251 Nov 11 23:08 **pytorch_model-00002-of-00002.bin**\r\n-rw-r--r-- 1 GIT staff      27068 Nov 11 23:01 pytorch_model.bin.index.json\r\n-rw-r--r-- 1 GIT staff        438 Nov 11 23:01 special_tokens_map.json\r\n-rw-r--r-- 1 GIT staff     499723 Nov 11 23:01 tokenizer.model\r\n-rw-r--r-- 1 GIT staff        749 Nov 11 23:01 tokenizer_config.json\r\n```\r\n\r\n# Environment and Context\r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using:\r\nApple M2 Max\r\n\r\n* Operating System:\r\n  MacOS Sanoma versipn 14.1\r\n\r\n```\r\nuname -a\r\nDarwin GIT-MBP 23.1.0 Darwin Kernel Version 23.1.0: Mon Oct  9 21:28:45 PDT 2023; root:xnu-10002.41.9~6/RELEASE_ARM64_T6020 arm64 arm Darwin\r\n```\r\n\r\n```\r\n$ python3 --version\r\nPython 3.10.13\r\n$ make --version\r\nGNU Make 3.81\r\n$ g++ --version\r\nApple clang version 15.0.0 (clang-1500.0.40.1)\r\nTarget: arm64-apple-darwin23.1.0\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nPlease help provide information about the failure / bug.\r\n\r\n# Steps to Reproduce\r\nFollowing the steps provided in the https://github.com/ggerganov/llama.cpp/tree/master/examples/llava#model-conversion\r\n\r\n# Failure Logs\r\n\r\n```\r\n(llava-med) GIT-MBP:llama.cpp $ python ./examples/llava/llava-surgery.py -m ../llava-v1.5-7b\r\nTraceback (most recent call last):\r\n  File \"/Users/GIT/llama.cpp/./examples/llava/llava-surgery.py\", line 12, in <module>\r\n    path = sorted(glob.glob(f\"{args.model}/pytorch_model*.bin\"))[-1]\r\nIndexError: list index out of range\r\n```\r\n\r\n```\r\ngit log | head -1\r\ncommit e86fc56f7521ca4b18d1d9939e82abd40c2f1c01\r\n```\r\n\r\n```\r\npip list | egrep \"torch|numpy|sentencepiece\"\r\nnumpy              1.24.4\r\nopen-clip-torch    2.23.0\r\nsentencepiece      0.1.98\r\ntorch              2.1.0\r\ntorchaudio         2.1.0\r\ntorchvision        0.16.0\r\n```\r\n",
    "labels": [
      "need more info",
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2023-11-12T00:20:52+00:00",
    "closed_at": "2023-11-12T11:55:51+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4042/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4042"
  },
  {
    "number": 7578,
    "title": "Bug: when arrive max ctx, model output garbage",
    "body": "### What happened?\r\n\r\nThis part has problem in cuda version. if set ngl>0, when arrive max ctx and next turn to chat, the model output garbage.\r\n\r\nllama_kv_cache_seq_rm (ctx, 0, params.n_keep            , params.n_keep + n_discard);\r\nllama_kv_cache_seq_add(ctx, 0, params.n_keep + n_discard, n_past, -n_discard);\r\n\r\nif set ngl =0, everythings ok.\r\n### Name and Version\r\n\r\nllama.cpp-b3014\r\nmain.exe --version\r\nversion: 247 (6765407)\r\nbuilt with MSVC 19.37.32822.0 for x64\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\n_No response_",
    "labels": [
      "need more info",
      "bug-unconfirmed",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-05-28T02:22:16+00:00",
    "closed_at": "2024-06-18T03:20:17+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7578/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7578"
  },
  {
    "number": 153,
    "title": "ggml_new_tensor_impl: not enough space in the context's memory pool (needed 717778556, available 454395136)",
    "body": "Hey, I know someone already posted a similar issue that has already been closed, but I ran into the same thing. On windows 10 and cloned just yesterday",
    "labels": [
      "duplicate",
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-15T04:18:32+00:00",
    "closed_at": "2023-03-24T16:11:41+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/153/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/153"
  },
  {
    "number": 401,
    "title": "\"Illegal Instruction\" error when converting 7B model to ggml FP16 format (Raspberry Pi 4, 8GB, Raspberry Pi OS, 64-bit)",
    "body": "Hello\r\n\r\nI'm trying to replicate the process, using 7B on a Raspberry Pi 4 with 8GB of RAM.\r\nI'm running the latest Raspberry Pi OS 64-bit, and all of the software has been updated.\r\nI am following the guidance found in the [Usage section of the README.md](https://github.com/ggerganov/llama.cpp#readme)\r\nI cloned the repo, downloaded 7B and placed it into /llama.cpp/models/7B, the contents of which are below\r\n\r\n===7B Contents===\r\n```\r\nl-rw-r--r-- 1 les les  100 Mar 22 15:03 checklist.chk\r\n-rw-r--r-- 1 les les  13G Mar 22 15:03 consolidated.00.pth\r\n-rw-r--r-- 1 les les  101 Mar 22 15:04 params.json\r\n```\r\n===END of 7B Contents===\r\n\r\nI have successfully installed all of the suggested Python modules.\r\n\r\nWhen running this command `python3 convert-pth-to-ggml.py models/7B/ 1` I see a short pause, around 5 -10 seconds. Then I receive an `Illegal instruction` error message and I can progress no more.\r\n\r\nWhat am I doing wrong, and how can this be remedied?\r\nI also tried Ubuntu 22.04 64-bit and the issue was the same.\r\n\r\nMany thanks.\r\n\r\n",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-22T17:20:49+00:00",
    "closed_at": "2023-03-23T11:35:33+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/401/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/401"
  }
]
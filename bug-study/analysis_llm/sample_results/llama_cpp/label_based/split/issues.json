[
  {
    "number": 6657,
    "title": "Probably best not to clobber files with `gguf-split -merge`",
    "body": "To save idiots like me who just tried to use it without reading the command line options:\r\n\r\n```\r\n./gguf-split --merge dbrx-16x12b-instruct-q4_0-*-of-00010.gguf dbrx-16x12b-instruct-q4_0.gguf\r\n```\r\n\r\n```\r\n-rw-r--r-- 1 juk juk          0 Apr 13 12:47 dbrx-16x12b-instruct-q4_0-00002-of-00010.gguf\r\n```\r\n\r\n:facepalm:",
    "labels": [
      "enhancement",
      "stale",
      "split"
    ],
    "state": "closed",
    "created_at": "2024-04-13T12:22:18+00:00",
    "closed_at": "2024-06-17T01:07:11+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6657/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6657"
  },
  {
    "number": 6654,
    "title": "Trying to split model with --split-max-size, but gguf-split ignores it",
    "body": "\nLatest version, ubuntu 2204, conda python=3.10.\nTrying to split model with gguf-split, but something is going wrong \n```\n(base) richard@richard-ProLiant-DL580-Gen9:~/Desktop/ramdisk/banana/llama.cpp$ ./gguf-split --split --split-max-size 4000M --dry-run /media/richard/5fbd0bfa-8253-4803-85eb-80a13218a927/grok-1-fp16-gguf/grok-1-Q5_K.gguf Q5_K/grok-1 \nn_split: 1\nsplit 00001: n_tensors = 2115, total_size = 214437M\ngguf_split: 1 gguf split written with a total of 2115 tensors.\n(base) richard@richard-ProLiant-DL580-Gen9:~/Desktop/ramdisk/banana/llama.cpp$ ./gguf-split --split --split-max-size 4G --dry-run /media/richard/5fbd0bfa-8253-4803-85eb-80a13218a927/grok-1-fp16-gguf/grok-1-Q5_K.gguf Q5_K/grok-1 \nn_split: 17\nsplit 00001: n_tensors = 128, total_size = 14609M\nsplit 00002: n_tensors = 128, total_size = 13184M\nsplit 00003: n_tensors = 128, total_size = 12648M\nsplit 00004: n_tensors = 128, total_size = 12597M\nsplit 00005: n_tensors = 128, total_size = 12648M\nsplit 00006: n_tensors = 128, total_size = 12750M\nsplit 00007: n_tensors = 128, total_size = 12836M\nsplit 00008: n_tensors = 128, total_size = 13088M\nsplit 00009: n_tensors = 128, total_size = 13197M\nsplit 00010: n_tensors = 128, total_size = 12597M\nsplit 00011: n_tensors = 128, total_size = 12597M\nsplit 00012: n_tensors = 128, total_size = 12699M\nsplit 00013: n_tensors = 128, total_size = 12699M\nsplit 00014: n_tensors = 128, total_size = 12597M\nsplit 00015: n_tensors = 128, total_size = 13137M\nsplit 00016: n_tensors = 128, total_size = 13675M\nsplit 00017: n_tensors = 67, total_size = 6868M\ngguf_split: 17 gguf split written with a total of 2115 tensors.\n```",
    "labels": [
      "bug",
      "split"
    ],
    "state": "closed",
    "created_at": "2024-04-13T09:26:58+00:00",
    "closed_at": "2024-04-14T11:13:01+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6654/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6654"
  },
  {
    "number": 6634,
    "title": "Inconsistent size output of chunks with gguf-split",
    "body": "System:\r\nUbuntu server 22.04 LTS\r\n5950X\r\n64GB Ram 3200Mhz\r\n2x Nvidia 3090\r\n\r\nSteps to reproduce:\r\n\r\nFirst I converted the model to FP16 GGUF with:\r\n`./convert.py --outfile Karasu-Mixtral-8x22B-v0.1-fp16.gguf --outtype f16 lightblue_Karasu-Mixtral-8x22B-v0.1`\r\n\r\nThat worked just fine and I got:\r\n![image](https://github.com/ggerganov/llama.cpp/assets/5622210/6da7e036-863c-44ef-9b38-eee0e746f406)\r\n\r\nThen to quantize it to Q5_K_M:\r\n`./quantize Karasu-Mixtral-8x22B-v0.1-fp16.gguf Karasu-Mixtral-8x22B-v0.1-Q5_K_M.gguf Q5_K_M`\r\n\r\nThat worked fine too:\r\n![image](https://github.com/ggerganov/llama.cpp/assets/5622210/39401f0d-7f3d-4f18-b6b3-0f328ad34f12)\r\n\r\n\r\nBut when using gguf-split --split even though I'm using --split-max-tensors 128 the sizes of the chunks are inconsistent:\r\n\r\n`./gguf-split --split --split-max-tensors 128 /nfs/models/Karasu-Mixtral-8x22B-v0.1-Q5_K_M.gguf /nfs/models/`\r\n\r\n![image](https://github.com/ggerganov/llama.cpp/assets/5622210/f403478e-08f7-460b-8706-8b6b1c687111)\r\n![image](https://github.com/ggerganov/llama.cpp/assets/5622210/4e9fc7de-fe17-4d4b-8c73-e092a5f7136d)\r\n\r\n",
    "labels": [
      "bug",
      "split"
    ],
    "state": "closed",
    "created_at": "2024-04-12T09:41:54+00:00",
    "closed_at": "2024-04-14T11:13:00+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6634/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6634"
  },
  {
    "number": 6548,
    "title": "Re-quantization of a split gguf file produces \"invalid split file\"",
    "body": "Hi, while testing #6491 branch, I downloaded a Q8_0 quant (split into 3 files) from `dranger003`, and re-quantized it to Q2_K_S to make it more digestible for my museum hardware:\r\n```\r\n./quantize --allow-requantize --imatrix ../models/ggml-c4ai-command-r-plus-104b-f16-imatrix.dat ../models/ggml-c4ai-command-r-plus-104b-q8_0-00001-of-00003.gguf ../models/command-r-plus-104b-Q2_K_S.gguf Q2_K_S 2\r\n```\r\n\r\nI only passed the first piece, but `./quantize` processed it correctly and produced a single file with the expected size. However, it probably did not update some metadata and `./main` still thinks the result is a split file:\r\n```\r\n./main -m ../models/command-r-plus-104b-Q2_K_S.gguf -t 15 --color -p \"this is a test\" -c 2048 -ngl 25 -ctk q8_0\r\n...\r\nllama_model_load: error loading model: invalid split file: ../models/command-r-plus-104b-Q2_K_S.gguf\r\nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params: error: failed to load model '../models/command-r-plus-104b-Q2_K_S.gguf'\r\nmain: error: unable to load model\r\n```\r\n\r\nAs a workaround, it is possible to \"reset\" the metadata by doing a \"dummy pass\" of `gguf-split`:\r\n```\r\n./gguf-split --split-max-tensors 999 --split ../models/command-r-plus-104b-Q2_K_S.gguf ../models/command-r-plus-104b-Q2_K_S.gguf.split\r\n```\r\n\r\nThe resulting file then seems to be working fine.\r\n\r\nIt's probably an easy fix, but after a quick grep through the source and a look at `quantize.cpp` I figured I don't even know where to start, so it would be probably much easier and faster done by someone who knows the code-base.",
    "labels": [
      "bug",
      "good first issue",
      "split"
    ],
    "state": "closed",
    "created_at": "2024-04-08T17:09:20+00:00",
    "closed_at": "2024-04-25T10:29:36+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6548/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6548"
  },
  {
    "number": 6537,
    "title": "common: download from URL, improve parallel download progress status",
    "body": "### Context\r\n\r\nWhen downloading a sharded model, files are downloaded in parallel, it was added in:\r\n- #6192\r\n\r\nThe progressions of each download conflict:\r\n![image](https://github.com/ggerganov/llama.cpp/assets/5741141/d4937fc7-edf4-4920-ba63-dadf1c77b2d0)\r\n\r\nNeed to properly implement [CURLOPT_NOPROGRESS](https://curl.se/libcurl/c/CURLOPT_NOPROGRESS.html) for parallel download.\r\n\r\nExample in #6515:\r\n\r\n```shell\r\nmain --hf-repo ggml-org/models \\\r\n  --hf-file grok-1/grok-1-q4_0-00001-of-00009.gguf \\\r\n  --model   models/grok-1-q4_0-00001-of-00009.gguf \\\r\n  -ngl 64\r\n   --prompt \"I believe the meaning of life is\"\r\n```",
    "labels": [
      "enhancement",
      "help wanted",
      "good first issue",
      "split"
    ],
    "state": "open",
    "created_at": "2024-04-08T07:37:01+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6537/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6537"
  },
  {
    "number": 6463,
    "title": "`gguf-split` add a default option to not include tensors data in first shard",
    "body": "### Motivation\r\n\r\nbe able to make a split where the first shard is very small and contains primarily the metadata so that it can be downloaded quickly and then start the download of the other shards without waiting for the first to finish\r\n\r\n### Proposition\r\nAdd an option to not include tensor data in the first file. Maybe it should be enabled by default.\r\nShould be well tested.\r\n\r\n`ggml_alloc` should not be called as it will complain with `WARNING: Behavior may be unexpected when allocating 0 bytes for ggml_malloc!`\r\n\r\nWe can add extra meta data in the first file that describes all tensors in the shards for example\r\n\r\n#### References\r\n- #6404\r\n- #6135\r\n- #6187\r\n- #6192\r\n- #6343\r\n- https://github.com/ggerganov/llama.cpp/pull/6343#issuecomment-2034990690\r\n- https://github.com/ggerganov/llama.cpp/pull/6343#issuecomment-2035011205\r\n- https://github.com/huggingface/huggingface.js/issues/604\r\n",
    "labels": [
      "enhancement",
      "help wanted",
      "good first issue",
      "split"
    ],
    "state": "closed",
    "created_at": "2024-04-03T16:16:12+00:00",
    "closed_at": "2024-05-04T16:56:23+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6463/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6463"
  },
  {
    "number": 6260,
    "title": "split: include the option in ./convert.py and quantize",
    "body": "### Context\r\n\r\nAt the moment it is only possible to split after convertion or quantization. Mentionned by @Artefact2 in this `[comment](https://github.com/ggerganov/llama.cpp/pull/6135#issuecomment-2003942162)`:\r\n\r\n> as an alternative, add the splitting logic directly to tools that produce ggufs, like convert.py and quantize.\r\n\r\n### Proposition\r\n\r\nInclude split options in `convert*.py`, support splits in `quantize`",
    "labels": [
      "enhancement",
      "help wanted",
      "good first issue",
      "split"
    ],
    "state": "open",
    "created_at": "2024-03-23T15:32:02+00:00",
    "closed_at": null,
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6260/reactions",
      "total_count": 3,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6260"
  },
  {
    "number": 6259,
    "title": "split: allow --split-max-size option",
    "body": "### Motivation\r\n\r\nwe support `--split-max-tensors` since:\r\n- #6135\r\n\r\nAs mentionned by @Artefact2 in this [comment](https://github.com/ggerganov/llama.cpp/pull/6135#issuecomment-2003942162):\r\n> allowing to split by file size would be more intuitive (and usually more appropriate since file size is usually the limiting factor, eg 4G for FAT or 50G for HF)\r\n\r\n### Proposition:\r\nIntroduce `--split-max-size N(M|G)` split strategy to split files in file with a max size of N Megabytes or Gigabytes.\r\nAs it is not possible to have less than 1 tensor per GGUF, this size is a soft limit.",
    "labels": [
      "enhancement",
      "help wanted",
      "good first issue",
      "split"
    ],
    "state": "open",
    "created_at": "2024-03-23T15:29:25+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6259/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6259"
  },
  {
    "number": 6257,
    "title": "gguf-split does not show as a compiled binary with other programs",
    "body": "It is compiled and linked, but does not make it out of build/bin\r\n",
    "labels": [
      "split"
    ],
    "state": "closed",
    "created_at": "2024-03-23T13:55:52+00:00",
    "closed_at": "2024-03-23T16:18:14+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6257/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6257"
  }
]
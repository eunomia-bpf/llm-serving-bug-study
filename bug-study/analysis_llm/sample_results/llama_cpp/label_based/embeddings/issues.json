[
  {
    "number": 13689,
    "title": "GGML_ASSERT(seq_id < n_tokens && \"seq_id cannot be larger than n_tokens with pooling_type == MEAN\") failed",
    "body": "@slaren Using build 'b5404', I am encountering the same issue with:\r\n```console\r\n[user@system]$ export LLAMA_ARG_HF_REPO=nomic-ai/nomic-embed-text-v2-moe-GGUF:Q4_K_M \\\r\nLLAMA_ARG_EMBEDDINGS=1 \\\r\nLLAMA_ARG_ENDPOINT_METRICS=1 \\\r\nLLAMA_ARG_NO_WEBUI=1 \\\r\nLLAMA_ARG_HOST=0.0.0.0 \\\r\nLLAMA_ARG_N_PARALLEL=4 \\\r\nLLAMA_ARG_ALIAS=embeddings-multilingual \\\r\nLLAMA_ARG_PORT=80 \\\r\nLLAMA_ARG_CACHE_TYPE_K=f16 \\\r\nLLAMA_ARG_FLASH_ATTN=0 \\\r\nLLAMA_ARG_CTX_SIZE=2048 \\\r\nLLAMA_ARG_BATCH=448 \\\r\nLLAMA_ARG_BATCH=512 \\\r\nLLAMA_ARG_THREADS=1 \\\r\nLLAMA_ARG_N_PREDICT=-1 \\\r\nLLAMA_ARG_N_GPU_LAYERS=0 \\\r\nLLAMA_ARG_NUMA=distribute \\\r\nLLAMA_ARG_MLOCK=0 \\\r\nLLAMA_ARG_ENDPOINT_SLOTS=1 \\\r\nLLAMA_ARG_NO_CONTEXT_SHIFT=0 \\\r\nLLAMA_ARG_UBATCH=512\r\n[user@system]$ llama-server --seed 0 --temp 0.0\r\n```\r\n\r\n<details>\r\n<summary>Full logs</summary>\r\n\r\n```log\r\nload_backend: loaded CPU backend from /app/libggml-cpu-haswell.so\r\nwarning: no usable GPU found, --gpu-layers option will be ignored\r\nwarning: one possible reason is that llama.cpp was compiled without GPU support\r\nwarning: consult docs/build.md for compilation instructions\r\ncurl_perform_with_retry: HEAD https://huggingface.co/nomic-ai/nomic-embed-text-v2-moe-GGUF/resolve/main/nomic-embed-text-v2-moe.Q4_K_M.gguf (attempt 1 of 1)...\r\ncommon_download_file_single: using cached file: /root/.cache/llama.cpp/nomic-ai_nomic-embed-text-v2-moe-GGUF_nomic-embed-text-v2-moe.Q4_K_M.gguf\r\nbuild: 1 (faa0b9ba) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\r\nsystem info: n_threads = 1, n_threads_batch = 1, total_threads = 8\r\n\r\nsystem_info: n_threads = 1 (n_threads_batch = 1) / 8 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \r\n\r\nWeb UI is disabled\r\nmain: binding port with default address family\r\nmain: HTTP server is listening, hostname: 0.0.0.0, port: 80, http threads: 7\r\nmain: loading model\r\nsrv    load_model: loading model '/root/.cache/llama.cpp/nomic-ai_nomic-embed-text-v2-moe-GGUF_nomic-embed-text-v2-moe.Q4_K_M.gguf'\r\nllama_model_loader: loaded meta data with 45 key-value pairs and 142 tensors from /root/.cache/llama.cpp/nomic-ai_nomic-embed-text-v2-moe-GGUF_nomic-embed-text-v2-moe.Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = nomic-bert-moe\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = nomic-embed-text-v2-moe\r\nllama_model_loader: - kv   3:                            general.version str              = 2048\r\nllama_model_loader: - kv   4:                       general.organization str              = Nomic Ai\r\nllama_model_loader: - kv   5:                           general.basename str              = nomic-xlm\r\nllama_model_loader: - kv   6:                         general.size_label str              = 8x277M\r\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\r\nllama_model_loader: - kv   8:                   general.base_model.count u32              = 1\r\nllama_model_loader: - kv   9:                  general.base_model.0.name str              = Nomic Embed Text v2 Moe Unsupervised\r\nllama_model_loader: - kv  10:          general.base_model.0.organization str              = Nomic Ai\r\nllama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/nomic-ai/nomic...\r\nllama_model_loader: - kv  12:                               general.tags arr[str,4]       = [\"sentence-transformers\", \"sentence-s...\r\nllama_model_loader: - kv  13:                          general.languages arr[str,101]     = [\"en\", \"es\", \"fr\", \"de\", \"it\", \"pt\", ...\r\nllama_model_loader: - kv  14:                 nomic-bert-moe.block_count u32              = 12\r\nllama_model_loader: - kv  15:              nomic-bert-moe.context_length u32              = 512\r\nllama_model_loader: - kv  16:            nomic-bert-moe.embedding_length u32              = 768\r\nllama_model_loader: - kv  17:         nomic-bert-moe.feed_forward_length u32              = 3072\r\nllama_model_loader: - kv  18:        nomic-bert-moe.attention.head_count u32              = 12\r\nllama_model_loader: - kv  19: nomic-bert-moe.attention.layer_norm_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  20:            nomic-bert-moe.attention.causal bool             = false\r\nllama_model_loader: - kv  21:                nomic-bert-moe.pooling_type u32              = 1\r\nllama_model_loader: - kv  22:              nomic-bert-moe.rope.freq_base f32              = 10000.000000\r\nllama_model_loader: - kv  23:          nomic-bert-moe.moe_every_n_layers u32              = 2\r\nllama_model_loader: - kv  24:                nomic-bert-moe.expert_count u32              = 8\r\nllama_model_loader: - kv  25:           nomic-bert-moe.expert_used_count u32              = 2\r\nllama_model_loader: - kv  26:                       tokenizer.ggml.model str              = t5\r\nllama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,250048]  = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \",\"...\r\nllama_model_loader: - kv  29:                      tokenizer.ggml.scores arr[f32,250048]  = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,250048]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  31:            tokenizer.ggml.add_space_prefix bool             = true\r\nllama_model_loader: - kv  32:            tokenizer.ggml.token_type_count u32              = 1\r\nllama_model_loader: - kv  33:    tokenizer.ggml.remove_extra_whitespaces bool             = true\r\nllama_model_loader: - kv  34:        tokenizer.ggml.precompiled_charsmap arr[u8,237539]   = [0, 180, 2, 0, 0, 132, 0, 0, 0, 0, 0,...\r\nllama_model_loader: - kv  35:                tokenizer.ggml.bos_token_id u32              = 0\r\nllama_model_loader: - kv  36:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  37:            tokenizer.ggml.unknown_token_id u32              = 3\r\nllama_model_loader: - kv  38:          tokenizer.ggml.seperator_token_id u32              = 2\r\nllama_model_loader: - kv  39:            tokenizer.ggml.padding_token_id u32              = 1\r\nllama_model_loader: - kv  40:               tokenizer.ggml.mask_token_id u32              = 250001\r\nllama_model_loader: - kv  41:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  42:               tokenizer.ggml.add_eos_token bool             = true\r\nllama_model_loader: - kv  43:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  44:                          general.file_type u32              = 15\r\nllama_model_loader: - type  f32:   93 tensors\r\nllama_model_loader: - type q4_K:   18 tensors\r\nllama_model_loader: - type q5_K:   24 tensors\r\nllama_model_loader: - type q6_K:    7 tensors\r\nprint_info: file format = GGUF V3 (latest)\r\nprint_info: file type   = Q4_K - Medium\r\nprint_info: file size   = 321.66 MiB (5.68 BPW) \r\nload: model vocab missing newline token, using special_pad_id instead\r\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\r\nload: special tokens cache size = 4\r\nload: token to piece cache size = 2.1668 MB\r\nprint_info: arch             = nomic-bert-moe\r\nprint_info: vocab_only       = 0\r\nprint_info: n_ctx_train      = 512\r\nprint_info: n_embd           = 768\r\nprint_info: n_layer          = 12\r\nprint_info: n_head           = 12\r\nprint_info: n_head_kv        = 12\r\nprint_info: n_rot            = 64\r\nprint_info: n_swa            = 0\r\nprint_info: n_swa_pattern    = 1\r\nprint_info: n_embd_head_k    = 64\r\nprint_info: n_embd_head_v    = 64\r\nprint_info: n_gqa            = 1\r\nprint_info: n_embd_k_gqa     = 768\r\nprint_info: n_embd_v_gqa     = 768\r\nprint_info: f_norm_eps       = 1.0e-05\r\nprint_info: f_norm_rms_eps   = 0.0e+00\r\nprint_info: f_clamp_kqv      = 0.0e+00\r\nprint_info: f_max_alibi_bias = 0.0e+00\r\nprint_info: f_logit_scale    = 0.0e+00\r\nprint_info: f_attn_scale     = 0.0e+00\r\nprint_info: n_ff             = 3072\r\nprint_info: n_expert         = 8\r\nprint_info: n_expert_used    = 2\r\nprint_info: causal attn      = 0\r\nprint_info: pooling type     = 1\r\nprint_info: rope type        = 2\r\nprint_info: rope scaling     = linear\r\nprint_info: freq_base_train  = 10000.0\r\nprint_info: freq_scale_train = 1\r\nprint_info: n_ctx_orig_yarn  = 512\r\nprint_info: rope_finetuned   = unknown\r\nprint_info: ssm_d_conv       = 0\r\nprint_info: ssm_d_inner      = 0\r\nprint_info: ssm_d_state      = 0\r\nprint_info: ssm_dt_rank      = 0\r\nprint_info: ssm_dt_b_c_rms   = 0\r\nprint_info: model type       = 475M\r\nprint_info: model params     = 475.29 M\r\nprint_info: general.name     = nomic-embed-text-v2-moe\r\nprint_info: vocab type       = UGM\r\nprint_info: n_vocab          = 250048\r\nprint_info: n_merges         = 0\r\nprint_info: BOS token        = 0 '<s>'\r\nprint_info: EOS token        = 2 '</s>'\r\nprint_info: UNK token        = 3 '<unk>'\r\nprint_info: SEP token        = 2 '</s>'\r\nprint_info: PAD token        = 1 '<pad>'\r\nprint_info: MASK token       = 250001 '[PAD250000]'\r\nprint_info: LF token         = 0 '<s>'\r\nprint_info: EOG token        = 2 '</s>'\r\nprint_info: max token length = 48\r\nload_tensors: loading model tensors, this can take a while... (mmap = true)\r\nload_tensors:  CPU_AARCH64 model buffer size =   102.52 MiB\r\nload_tensors:   CPU_Mapped model buffer size =   321.66 MiB\r\n...........................\r\nllama_context: constructing llama_context\r\nllama_context: n_seq_max     = 4\r\nllama_context: n_ctx         = 2048\r\nllama_context: n_ctx_per_seq = 512\r\nllama_context: n_batch       = 512\r\nllama_context: n_ubatch      = 512\r\nllama_context: causal_attn   = 0\r\nllama_context: flash_attn    = 0\r\nllama_context: freq_base     = 10000.0\r\nllama_context: freq_scale    = 1\r\nllama_context:        CPU  output buffer size =     0.00 MiB\r\ncommon_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting\r\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 2048\r\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\ndecode: cannot decode batches with this context (use llama_encode() instead)\r\nsrv          init: initializing slots, n_slots = 4\r\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 512\r\nslot         init: id  1 | task -1 | new slot n_ctx_slot = 512\r\nslot         init: id  2 | task -1 | new slot n_ctx_slot = 512\r\nslot         init: id  3 | task -1 | new slot n_ctx_slot = 512\r\nmain: model loaded\r\nmain: chat template, chat_template: {%- for message in messages -%}\r\n  {{- '<|im_start|>' + message.role + '\r\n' + message.content + '<|im_end|>\r\n' -}}\r\n{%- endfor -%}\r\n{%- if add_generation_prompt -%}\r\n  {{- '<|im_start|>assistant\r\n' -}}\r\n{%- endif -%}, example_format: '<|im_start|>system\r\nYou are a helpful assistant<|im_end|>\r\n<|im_start|>user\r\nHello<|im_end|>\r\n<|im_start|>assistant\r\nHi there<|im_end|>\r\n<|im_start|>user\r\nHow are you?<|im_end|>\r\n<|im_start|>assistant\r\n'\r\nmain: server is listening on http://0.0.0.0:80 - starting the main loop\r\nslot launch_slot_: id  0 | task 499 | processing task\r\nslot update_slots: id  0 | task 499 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 512\r\nslot update_slots: id  0 | task 499 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 499 | prompt processing progress, n_past = 512, n_tokens = 512, progress = 1.000000\r\nslot update_slots: id  0 | task 499 | prompt done, n_past = 512, n_tokens = 512\r\nslot      release: id  0 | task 499 | stop processing: n_past = 512, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.77 200\r\nslot launch_slot_: id  0 | task 526 | processing task\r\nslot update_slots: id  0 | task 526 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 512\r\nslot update_slots: id  0 | task 526 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 526 | prompt processing progress, n_past = 512, n_tokens = 512, progress = 1.000000\r\nslot update_slots: id  0 | task 526 | prompt done, n_past = 512, n_tokens = 512\r\nslot      release: id  0 | task 526 | stop processing: n_past = 512, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.77 200\r\nslot launch_slot_: id  0 | task 1047 | processing task\r\nslot update_slots: id  0 | task 1047 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 512\r\nslot update_slots: id  0 | task 1047 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 1047 | prompt processing progress, n_past = 512, n_tokens = 512, progress = 1.000000\r\nslot update_slots: id  0 | task 1047 | prompt done, n_past = 512, n_tokens = 512\r\nslot      release: id  0 | task 1047 | stop processing: n_past = 512, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.104 200\r\nslot launch_slot_: id  1 | task 1164 | processing task\r\nslot update_slots: id  1 | task 1164 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 94\r\nslot update_slots: id  1 | task 1164 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 1164 | prompt processing progress, n_past = 94, n_tokens = 94, progress = 1.000000\r\nslot update_slots: id  1 | task 1164 | prompt done, n_past = 94, n_tokens = 94\r\nslot      release: id  1 | task 1164 | stop processing: n_past = 94, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.104 200\r\nslot launch_slot_: id  1 | task 1171 | processing task\r\nslot update_slots: id  1 | task 1171 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 94\r\nslot update_slots: id  1 | task 1171 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 1171 | prompt processing progress, n_past = 94, n_tokens = 94, progress = 1.000000\r\nslot update_slots: id  1 | task 1171 | prompt done, n_past = 94, n_tokens = 94\r\nslot      release: id  1 | task 1171 | stop processing: n_past = 94, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.104 200\r\nslot launch_slot_: id  1 | task 1570 | processing task\r\nslot update_slots: id  1 | task 1570 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 94\r\nslot update_slots: id  1 | task 1570 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 1570 | prompt processing progress, n_past = 94, n_tokens = 94, progress = 1.000000\r\nslot update_slots: id  1 | task 1570 | prompt done, n_past = 94, n_tokens = 94\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot      release: id  1 | task 1570 | stop processing: n_past = 94, truncated = 0\r\nslot launch_slot_: id  2 | task 2487 | processing task\r\nslot update_slots: id  2 | task 2487 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 23\r\nslot update_slots: id  2 | task 2487 | kv cache rm [0, end)\r\nslot update_slots: id  2 | task 2487 | prompt processing progress, n_past = 23, n_tokens = 23, progress = 1.000000\r\nslot update_slots: id  2 | task 2487 | prompt done, n_past = 23, n_tokens = 23\r\nslot      release: id  2 | task 2487 | stop processing: n_past = 23, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  3 | task 2546 | processing task\r\nslot launch_slot_: id  0 | task 2547 | processing task\r\nslot launch_slot_: id  1 | task 2548 | processing task\r\nslot launch_slot_: id  2 | task 2549 | processing task\r\nslot update_slots: id  0 | task 2547 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  0 | task 2547 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 2547 | prompt processing progress, n_past = 502, n_tokens = 502, progress = 1.000000\r\nslot update_slots: id  0 | task 2547 | prompt done, n_past = 502, n_tokens = 502\r\nslot update_slots: id  1 | task 2548 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  2 | task 2549 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  3 | task 2546 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 501\r\nslot      release: id  0 | task 2547 | stop processing: n_past = 502, truncated = 0\r\nslot launch_slot_: id  0 | task 2550 | processing task\r\nslot update_slots: id  0 | task 2550 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 2\r\nslot update_slots: id  0 | task 2550 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 2550 | prompt processing progress, n_past = 2, n_tokens = 2, progress = 1.000000\r\nslot update_slots: id  0 | task 2550 | prompt done, n_past = 2, n_tokens = 2\r\nslot update_slots: id  1 | task 2548 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 2548 | prompt processing progress, n_past = 502, n_tokens = 504, progress = 1.000000\r\nslot update_slots: id  1 | task 2548 | prompt done, n_past = 502, n_tokens = 504\r\nslot      release: id  0 | task 2550 | stop processing: n_past = 2, truncated = 0\r\nslot      release: id  1 | task 2548 | stop processing: n_past = 502, truncated = 0\r\nslot update_slots: id  2 | task 2549 | kv cache rm [0, end)\r\nslot update_slots: id  2 | task 2549 | prompt processing progress, n_past = 502, n_tokens = 502, progress = 1.000000\r\nslot update_slots: id  2 | task 2549 | prompt done, n_past = 502, n_tokens = 502\r\nslot      release: id  2 | task 2549 | stop processing: n_past = 502, truncated = 0\r\nslot update_slots: id  3 | task 2546 | kv cache rm [0, end)\r\nslot update_slots: id  3 | task 2546 | prompt processing progress, n_past = 501, n_tokens = 501, progress = 1.000000\r\nslot update_slots: id  3 | task 2546 | prompt done, n_past = 501, n_tokens = 501\r\nslot      release: id  3 | task 2546 | stop processing: n_past = 501, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  0 | task 2557 | processing task\r\nslot update_slots: id  0 | task 2557 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 24\r\nslot update_slots: id  0 | task 2557 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 2557 | prompt processing progress, n_past = 24, n_tokens = 24, progress = 1.000000\r\nslot update_slots: id  0 | task 2557 | prompt done, n_past = 24, n_tokens = 24\r\nslot      release: id  0 | task 2557 | stop processing: n_past = 24, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  1 | task 2633 | processing task\r\nslot update_slots: id  1 | task 2633 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 2\r\nslot update_slots: id  1 | task 2633 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 2633 | prompt processing progress, n_past = 2, n_tokens = 2, progress = 1.000000\r\nslot update_slots: id  1 | task 2633 | prompt done, n_past = 2, n_tokens = 2\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot      release: id  1 | task 2633 | stop processing: n_past = 2, truncated = 0\r\nslot launch_slot_: id  1 | task 2635 | processing task\r\nslot update_slots: id  1 | task 2635 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 2\r\nslot update_slots: id  1 | task 2635 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 2635 | prompt processing progress, n_past = 2, n_tokens = 2, progress = 1.000000\r\nslot update_slots: id  1 | task 2635 | prompt done, n_past = 2, n_tokens = 2\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot      release: id  1 | task 2635 | stop processing: n_past = 2, truncated = 0\r\nslot launch_slot_: id  2 | task 2637 | processing task\r\nslot update_slots: id  2 | task 2637 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 21\r\nslot update_slots: id  2 | task 2637 | kv cache rm [0, end)\r\nslot update_slots: id  2 | task 2637 | prompt processing progress, n_past = 21, n_tokens = 21, progress = 1.000000\r\nslot update_slots: id  2 | task 2637 | prompt done, n_past = 21, n_tokens = 21\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot      release: id  2 | task 2637 | stop processing: n_past = 21, truncated = 0\r\nslot launch_slot_: id  3 | task 11488 | processing task\r\nslot update_slots: id  3 | task 11488 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 10\r\nslot update_slots: id  3 | task 11488 | kv cache rm [0, end)\r\nslot update_slots: id  3 | task 11488 | prompt processing progress, n_past = 10, n_tokens = 10, progress = 1.000000\r\nslot update_slots: id  3 | task 11488 | prompt done, n_past = 10, n_tokens = 10\r\nslot      release: id  3 | task 11488 | stop processing: n_past = 10, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  3 | task 11513 | processing task\r\nslot update_slots: id  3 | task 11513 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 10\r\nslot update_slots: id  3 | task 11513 | kv cache rm [0, end)\r\nslot update_slots: id  3 | task 11513 | prompt processing progress, n_past = 10, n_tokens = 10, progress = 1.000000\r\nslot update_slots: id  3 | task 11513 | prompt done, n_past = 10, n_tokens = 10\r\nslot      release: id  3 | task 11513 | stop processing: n_past = 10, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  0 | task 11524 | processing task\r\nslot update_slots: id  0 | task 11524 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 12\r\nslot update_slots: id  0 | task 11524 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 11524 | prompt processing progress, n_past = 12, n_tokens = 12, progress = 1.000000\r\nslot update_slots: id  0 | task 11524 | prompt done, n_past = 12, n_tokens = 12\r\nslot      release: id  0 | task 11524 | stop processing: n_past = 12, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  1 | task 11547 | processing task\r\nslot launch_slot_: id  2 | task 11548 | processing task\r\nslot update_slots: id  1 | task 11547 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  1 | task 11547 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 11547 | prompt processing progress, n_past = 502, n_tokens = 502, progress = 1.000000\r\nslot update_slots: id  1 | task 11547 | prompt done, n_past = 502, n_tokens = 502\r\nslot update_slots: id  2 | task 11548 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 2\r\nslot update_slots: id  2 | task 11548 | kv cache rm [0, end)\r\nslot update_slots: id  2 | task 11548 | prompt processing progress, n_past = 2, n_tokens = 504, progress = 1.000000\r\nslot update_slots: id  2 | task 11548 | prompt done, n_past = 2, n_tokens = 504\r\nslot      release: id  1 | task 11547 | stop processing: n_past = 502, truncated = 0\r\nslot      release: id  2 | task 11548 | stop processing: n_past = 2, truncated = 0\r\nslot launch_slot_: id  3 | task 11550 | processing task\r\nslot launch_slot_: id  0 | task 11551 | processing task\r\nslot launch_slot_: id  1 | task 11552 | processing task\r\nslot launch_slot_: id  2 | task 11553 | processing task\r\nslot update_slots: id  0 | task 11551 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  0 | task 11551 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 11551 | prompt processing progress, n_past = 502, n_tokens = 502, progress = 1.000000\r\nslot update_slots: id  0 | task 11551 | prompt done, n_past = 502, n_tokens = 502\r\nslot update_slots: id  1 | task 11552 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  2 | task 11553 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 503\r\nslot update_slots: id  3 | task 11550 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 501\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot      release: id  0 | task 11551 | stop processing: n_past = 502, truncated = 0\r\nslot launch_slot_: id  0 | task 11554 | processing task\r\nslot update_slots: id  0 | task 11554 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  0 | task 11554 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 11554 | prompt processing progress, n_past = 502, n_tokens = 502, progress = 1.000000\r\nslot update_slots: id  0 | task 11554 | prompt done, n_past = 502, n_tokens = 502\r\nslot      release: id  0 | task 11554 | stop processing: n_past = 502, truncated = 0\r\nslot update_slots: id  1 | task 11552 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 11552 | prompt processing progress, n_past = 502, n_tokens = 502, progress = 1.000000\r\nslot update_slots: id  1 | task 11552 | prompt done, n_past = 502, n_tokens = 502\r\nslot      release: id  1 | task 11552 | stop processing: n_past = 502, truncated = 0\r\nslot update_slots: id  2 | task 11553 | kv cache rm [0, end)\r\nslot update_slots: id  2 | task 11553 | prompt processing progress, n_past = 503, n_tokens = 503, progress = 1.000000\r\nslot update_slots: id  2 | task 11553 | prompt done, n_past = 503, n_tokens = 503\r\nslot      release: id  2 | task 11553 | stop processing: n_past = 503, truncated = 0\r\nslot update_slots: id  3 | task 11550 | kv cache rm [0, end)\r\nslot update_slots: id  3 | task 11550 | prompt processing progress, n_past = 501, n_tokens = 501, progress = 1.000000\r\nslot update_slots: id  3 | task 11550 | prompt done, n_past = 501, n_tokens = 501\r\nslot      release: id  3 | task 11550 | stop processing: n_past = 501, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  0 | task 11595 | processing task\r\nslot update_slots: id  0 | task 11595 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 2\r\nslot update_slots: id  0 | task 11595 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 11595 | prompt processing progress, n_past = 2, n_tokens = 2, progress = 1.000000\r\nslot update_slots: id  0 | task 11595 | prompt done, n_past = 2, n_tokens = 2\r\nslot      release: id  0 | task 11595 | stop processing: n_past = 2, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  3 | task 11597 | processing task\r\nslot launch_slot_: id  1 | task 11598 | processing task\r\nslot launch_slot_: id  2 | task 11599 | processing task\r\nslot launch_slot_: id  0 | task 11600 | processing task\r\nslot update_slots: id  0 | task 11600 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 503\r\nslot update_slots: id  0 | task 11600 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 11600 | prompt processing progress, n_past = 503, n_tokens = 503, progress = 1.000000\r\nslot update_slots: id  0 | task 11600 | prompt done, n_past = 503, n_tokens = 503\r\nslot update_slots: id  1 | task 11598 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  2 | task 11599 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  3 | task 11597 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 501\r\nslot      release: id  0 | task 11600 | stop processing: n_past = 503, truncated = 0\r\nslot launch_slot_: id  0 | task 11603 | processing task\r\nslot update_slots: id  0 | task 11603 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  0 | task 11603 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 11603 | prompt processing progress, n_past = 502, n_tokens = 502, progress = 1.000000\r\nslot update_slots: id  0 | task 11603 | prompt done, n_past = 502, n_tokens = 502\r\nslot      release: id  0 | task 11603 | stop processing: n_past = 502, truncated = 0\r\nslot launch_slot_: id  0 | task 11604 | processing task\r\nslot update_slots: id  0 | task 11604 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 2\r\nslot update_slots: id  0 | task 11604 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 11604 | prompt processing progress, n_past = 2, n_tokens = 2, progress = 1.000000\r\nslot update_slots: id  0 | task 11604 | prompt done, n_past = 2, n_tokens = 2\r\nslot update_slots: id  1 | task 11598 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 11598 | prompt processing progress, n_past = 502, n_tokens = 504, progress = 1.000000\r\nslot update_slots: id  1 | task 11598 | prompt done, n_past = 502, n_tokens = 504\r\nslot      release: id  0 | task 11604 | stop processing: n_past = 2, truncated = 0\r\nslot      release: id  1 | task 11598 | stop processing: n_past = 502, truncated = 0\r\nslot launch_slot_: id  0 | task 11601 | processing task\r\nslot update_slots: id  0 | task 11601 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 502\r\nslot update_slots: id  0 | task 11601 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 11601 | prompt processing progress, n_past = 502, n_tokens = 502, progress = 1.000000\r\nslot update_slots: id  0 | task 11601 | prompt done, n_past = 502, n_tokens = 502\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot      release: id  0 | task 11601 | stop processing: n_past = 502, truncated = 0\r\nslot update_slots: id  2 | task 11599 | kv cache rm [0, end)\r\nslot update_slots: id  2 | task 11599 | prompt processing progress, n_past = 502, n_tokens = 502, progress = 1.000000\r\nslot update_slots: id  2 | task 11599 | prompt done, n_past = 502, n_tokens = 502\r\nslot      release: id  2 | task 11599 | stop processing: n_past = 502, truncated = 0\r\nslot update_slots: id  3 | task 11597 | kv cache rm [0, end)\r\nslot update_slots: id  3 | task 11597 | prompt processing progress, n_past = 501, n_tokens = 501, progress = 1.000000\r\nslot update_slots: id  3 | task 11597 | prompt done, n_past = 501, n_tokens = 501\r\nslot      release: id  3 | task 11597 | stop processing: n_past = 501, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  1 | task 11613 | processing task\r\nslot launch_slot_: id  0 | task 11614 | processing task\r\nslot launch_slot_: id  2 | task 11615 | processing task\r\nslot update_slots: id  0 | task 11614 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 503\r\nslot update_slots: id  0 | task 11614 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 11614 | prompt processing progress, n_past = 503, n_tokens = 503, progress = 1.000000\r\nslot update_slots: id  0 | task 11614 | prompt done, n_past = 503, n_tokens = 503\r\nslot update_slots: id  1 | task 11613 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 501\r\nslot update_slots: id  2 | task 11615 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 2\r\nslot update_slots: id  2 | task 11615 | kv cache rm [0, end)\r\nslot update_slots: id  2 | task 11615 | prompt processing progress, n_past = 2, n_tokens = 505, progress = 1.000000\r\nslot update_slots: id  2 | task 11615 | prompt done, n_past = 2, n_tokens = 505\r\nslot      release: id  0 | task 11614 | stop processing: n_past = 503, truncated = 0\r\nslot      release: id  2 | task 11615 | stop processing: n_past = 2, truncated = 0\r\nslot update_slots: id  1 | task 11613 | kv cache rm [0, end)\r\nslot update_slots: id  1 | task 11613 | prompt processing progress, n_past = 501, n_tokens = 501, progress = 1.000000\r\nslot update_slots: id  1 | task 11613 | prompt done, n_past = 501, n_tokens = 501\r\nslot      release: id  1 | task 11613 | stop processing: n_past = 501, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  3 | task 11619 | processing task\r\nslot update_slots: id  3 | task 11619 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 12\r\nslot update_slots: id  3 | task 11619 | kv cache rm [0, end)\r\nslot update_slots: id  3 | task 11619 | prompt processing progress, n_past = 12, n_tokens = 12, progress = 1.000000\r\nslot update_slots: id  3 | task 11619 | prompt done, n_past = 12, n_tokens = 12\r\nslot      release: id  3 | task 11619 | stop processing: n_past = 12, truncated = 0\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\nslot launch_slot_: id  2 | task 11647 | processing task\r\nslot update_slots: id  2 | task 11647 | new prompt, n_ctx_slot = 512, n_keep = 0, n_prompt_tokens = 2\r\nslot update_slots: id  2 | task 11647 | kv cache rm [0, end)\r\nslot update_slots: id  2 | task 11647 | prompt processing progress, n_past = 2, n_tokens = 2, progress = 1.000000\r\nslot update_slots: id  2 | task 11647 | prompt done, n_past = 2, n_tokens = 2\r\n/app/src/llama-graph.cpp:185: GGML_ASSERT(seq_id < n_tokens && \"seq_id cannot be larger than n_tokens with pooling_type == MEAN\") failed\r\nsrv  cancel_tasks: cancel task, id_task = 11647\r\nsrv  log_server_r: request: POST /v1/embeddings 10.2.0.132 200\r\n```\r\n\r\n</details>\r\n\r\n**Note:** It is not deterministic, but it seems to happen more frequently when enough slots are used. If wanting to reproduce, you should reduce `LLAMA_ARG_N_PARALLEL` to `2`, for instance.\r\n\r\n_Originally posted by @aviallon in https://github.com/ggml-org/llama.cpp/discussions/9000#discussioncomment-13221292_",
    "labels": [
      "bug",
      "embeddings",
      "server"
    ],
    "state": "closed",
    "created_at": "2025-05-21T14:51:24+00:00",
    "closed_at": "2025-05-22T13:33:40+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13689/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13689"
  },
  {
    "number": 12836,
    "title": "server : crash when -b > -ub with embeddings",
    "body": "> @ggerganov Ok, I did few tests and apparently there's an issue that is subject to a separate issue.\n> \n> Using the following command:\n> ```\n> llama-server ... -ub 4096 -b 4096 -c 4096 -np 4\n> ```\n> \n> Everything works pretty much as expected. Amount of tokens that a task slot can handle appears to be `ub / np`. So in this example, each slot gets a 1024 tokens window. This does seem to give a nice boost depending on the embeddings chunking strategy (my current embeddings are up to 1024 tokens), but I haven't measured precisely yet.\n> \n> However, using the following command:\n> ```\n> llama-server ... -ub 1024 -b 4096 -c 4096 -np 4\n> ```\n> \n> The server crashes with `GGML_ASSERT((cparams.causal_attn || cparams.n_ubatch >= n_tokens_all) && \"non-causal attention requires n_ubatch >= n_tokens\") failed` as soon as it receives the next batch of tasks:\n> \n> ```\n> ggml_vulkan: Found 1 Vulkan devices:\n> ggml_vulkan: 0 = AMD Radeon RX 6600M (AMD proprietary driver) | uma: 0 | fp16: 1 | warp size: 32 | shared memory: 32768 | int dot: 1 | matrix cores: none\n> build: 5080 (997b1b42) with MSVC 19.38.33134.0 for x64\n> system info: n_threads = 8, n_threads_batch = 8, total_threads = 16\n> \n> system_info: n_threads = 8 (n_threads_batch = 8) / 16 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |\n> \n> main: binding port with default address family\n> main: HTTP server is listening, hostname: 192.168.0.2, port: 8080, http threads: 15\n> main: loading model\n> srv    load_model: loading model 'C:\\Temp\\snowflake-arctic-embed-l-v2.0-q8_0.gguf'\n> llama_model_load_from_file_impl: using device Vulkan0 (AMD Radeon RX 6600M) - 8176 MiB free\n> llama_model_loader: loaded meta data with 36 key-value pairs and 389 tensors from C:\\Temp\\snowflake-arctic-embed-l-v2.0-q8_0.gguf (version GGUF V3 (latest))\n> llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n> llama_model_loader: - kv   0:                       general.architecture str              = bert\n> llama_model_loader: - kv   1:                               general.type str              = model\n> llama_model_loader: - kv   2:                               general.name str              = Snowflake Arctic Embed L v2.0\n> llama_model_loader: - kv   3:                            general.version str              = v2.0\n> llama_model_loader: - kv   4:                           general.basename str              = snowflake-arctic-embed-l\n> llama_model_loader: - kv   5:                         general.size_label str              = 567M\n> llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n> llama_model_loader: - kv   7:                               general.tags arr[str,8]       = [\"sentence-transformers\", \"feature-ex...\n> llama_model_loader: - kv   8:                          general.languages arr[str,74]      = [\"af\", \"ar\", \"az\", \"be\", \"bg\", \"bn\", ...\n> llama_model_loader: - kv   9:                           bert.block_count u32              = 24\n> llama_model_loader: - kv  10:                        bert.context_length u32              = 8192\n> llama_model_loader: - kv  11:                      bert.embedding_length u32              = 1024\n> llama_model_loader: - kv  12:                   bert.feed_forward_length u32              = 4096\n> llama_model_loader: - kv  13:                  bert.attention.head_count u32              = 16\n> llama_model_loader: - kv  14:          bert.attention.layer_norm_epsilon f32              = 0.000010\n> llama_model_loader: - kv  15:                          general.file_type u32              = 7\n> llama_model_loader: - kv  16:                      bert.attention.causal bool             = false\n> llama_model_loader: - kv  17:                          bert.pooling_type u32              = 2\n> llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = t5\n> llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = default\n> llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,250002]  = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \",\"...\n> llama_model_loader: - kv  21:                      tokenizer.ggml.scores arr[f32,250002]  = [0.000000, 0.000000, 0.000000, 0.0000...\n> llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,250002]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n> llama_model_loader: - kv  23:            tokenizer.ggml.add_space_prefix bool             = true\n> llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 1\n> llama_model_loader: - kv  25:    tokenizer.ggml.remove_extra_whitespaces bool             = true\n> llama_model_loader: - kv  26:        tokenizer.ggml.precompiled_charsmap arr[u8,237539]   = [0, 180, 2, 0, 0, 132, 0, 0, 0, 0, 0,...\n> llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 0\n> llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\n> llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 3\n> llama_model_loader: - kv  30:          tokenizer.ggml.seperator_token_id u32              = 2\n> llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 1\n> llama_model_loader: - kv  32:               tokenizer.ggml.mask_token_id u32              = 250001\n> llama_model_loader: - kv  33:               tokenizer.ggml.add_bos_token bool             = true\n> llama_model_loader: - kv  34:               tokenizer.ggml.add_eos_token bool             = true\n> llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n> llama_model_loader: - type  f32:  244 tensors\n> llama_model_loader: - type q8_0:  145 tensors\n> print_info: file format = GGUF V3 (latest)\n> print_info: file type   = Q8_0\n> print_info: file size   = 598.63 MiB (8.86 BPW)\n> load: model vocab missing newline token, using special_pad_id instead\n> load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n> load: special tokens cache size = 4\n> load: token to piece cache size = 2.1668 MB\n> print_info: arch             = bert\n> print_info: vocab_only       = 0\n> print_info: n_ctx_train      = 8192\n> print_info: n_embd           = 1024\n> print_info: n_layer          = 24\n> print_info: n_head           = 16\n> print_info: n_head_kv        = 16\n> print_info: n_rot            = 64\n> print_info: n_swa            = 0\n> print_info: n_swa_pattern    = 1\n> print_info: n_embd_head_k    = 64\n> print_info: n_embd_head_v    = 64\n> print_info: n_gqa            = 1\n> print_info: n_embd_k_gqa     = 1024\n> print_info: n_embd_v_gqa     = 1024\n> print_info: f_norm_eps       = 1.0e-05\n> print_info: f_norm_rms_eps   = 0.0e+00\n> print_info: f_clamp_kqv      = 0.0e+00\n> print_info: f_max_alibi_bias = 0.0e+00\n> print_info: f_logit_scale    = 0.0e+00\n> print_info: f_attn_scale     = 0.0e+00\n> print_info: n_ff             = 4096\n> print_info: n_expert         = 0\n> print_info: n_expert_used    = 0\n> print_info: causal attn      = 0\n> print_info: pooling type     = 2\n> print_info: rope type        = 2\n> print_info: rope scaling     = linear\n> print_info: freq_base_train  = 10000.0\n> print_info: freq_scale_train = 1\n> print_info: n_ctx_orig_yarn  = 8192\n> print_info: rope_finetuned   = unknown\n> print_info: ssm_d_conv       = 0\n> print_info: ssm_d_inner      = 0\n> print_info: ssm_d_state      = 0\n> print_info: ssm_dt_rank      = 0\n> print_info: ssm_dt_b_c_rms   = 0\n> print_info: model type       = 335M\n> print_info: model params     = 566.70 M\n> print_info: general.name     = Snowflake Arctic Embed L v2.0\n> print_info: vocab type       = UGM\n> print_info: n_vocab          = 250002\n> print_info: n_merges         = 0\n> print_info: BOS token        = 0 '<s>'\n> print_info: EOS token        = 2 '</s>'\n> print_info: UNK token        = 3 '<unk>'\n> print_info: SEP token        = 2 '</s>'\n> print_info: PAD token        = 1 '<pad>'\n> print_info: MASK token       = 250001 '[PAD250000]'\n> print_info: LF token         = 0 '<s>'\n> print_info: EOG token        = 2 '</s>'\n> print_info: max token length = 48\n> load_tensors: loading model tensors, this can take a while... (mmap = true)\n> load_tensors: offloading 24 repeating layers to GPU\n> load_tensors: offloading output layer to GPU\n> load_tensors: offloaded 25/25 layers to GPU\n> load_tensors:      Vulkan0 model buffer size =   307.22 MiB\n> load_tensors:   CPU_Mapped model buffer size =   291.41 MiB\n> ......................................................\n> llama_context: constructing llama_context\n> llama_context: n_seq_max     = 3\n> llama_context: n_ctx         = 4096\n> llama_context: n_ctx_per_seq = 1365\n> llama_context: n_batch       = 4096\n> llama_context: n_ubatch      = 1024\n> llama_context: causal_attn   = 0\n> llama_context: flash_attn    = 0\n> llama_context: freq_base     = 10000.0\n> llama_context: freq_scale    = 1\n> llama_context: n_ctx_per_seq (1365) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n> llama_context: Vulkan_Host  output buffer size =     0.00 MiB\n> init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1\n> init:    Vulkan0 KV buffer size =   384.00 MiB\n> llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB\n> llama_context:    Vulkan0 compute buffer size =    88.01 MiB\n> llama_context: Vulkan_Host compute buffer size =    12.01 MiB\n> llama_context: graph nodes  = 825\n> llama_context: graph splits = 4 (with bs=1024), 2 (with bs=1)\n> common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n> common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n> srv          init: initializing slots, n_slots = 3\n> slot         init: id  0 | task -1 | new slot n_ctx_slot = 1365\n> slot         init: id  1 | task -1 | new slot n_ctx_slot = 1365\n> slot         init: id  2 | task -1 | new slot n_ctx_slot = 1365\n> main: model loaded\n> main: chat template, chat_template: {%- for message in messages -%}\n>   {{- '<|im_start|>' + message.role + '\n> ' + message.content + '<|im_end|>\n> ' -}}\n> {%- endfor -%}\n> {%- if add_generation_prompt -%}\n>   {{- '<|im_start|>assistant\n> ' -}}\n> {%- endif -%}, example_format: '<|im_start|>system\n> You are a helpful assistant<|im_end|>\n> <|im_start|>user\n> Hello<|im_end|>\n> <|im_start|>assistant\n> Hi there<|im_end|>\n> <|im_start|>user\n> How are you?<|im_end|>\n> <|im_start|>assistant\n> '\n> main: server is listening on http://192.168.0.2:8080 - starting the main loop\n> srv  update_slots: all slots are idle\n> slot launch_slot_: id  0 | task 0 | processing task\n> slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 1365, n_keep = 0, n_prompt_tokens = 830\n> slot update_slots: id  0 | task 0 | kv cache rm [0, end)\n> slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 830, n_tokens = 830, progress = 1.000000\n> slot update_slots: id  0 | task 0 | prompt done, n_past = 830, n_tokens = 830\n> slot      release: id  0 | task 0 | stop processing: n_past = 830, truncated = 0\n> slot launch_slot_: id  1 | task 2 | processing task\n> slot launch_slot_: id  2 | task 3 | processing task\n> slot launch_slot_: id  0 | task 4 | processing task\n> slot update_slots: id  0 | task 4 | new prompt, n_ctx_slot = 1365, n_keep = 0, n_prompt_tokens = 255\n> srv  log_server_r: request: POST /v1/embeddings 192.168.0.7 200\n> slot update_slots: id  0 | task 4 | kv cache rm [0, end)\n> slot update_slots: id  0 | task 4 | prompt processing progress, n_past = 255, n_tokens = 255, progress = 1.000000\n> slot update_slots: id  0 | task 4 | prompt done, n_past = 255, n_tokens = 255\n> slot update_slots: id  1 | task 2 | new prompt, n_ctx_slot = 1365, n_keep = 0, n_prompt_tokens = 852\n> slot update_slots: id  1 | task 2 | kv cache rm [0, end)\n> slot update_slots: id  1 | task 2 | prompt processing progress, n_past = 852, n_tokens = 1107, progress = 1.000000\n> slot update_slots: id  1 | task 2 | prompt done, n_past = 852, n_tokens = 1107\n> slot update_slots: id  2 | task 3 | new prompt, n_ctx_slot = 1365, n_keep = 0, n_prompt_tokens = 246\n> slot update_slots: id  2 | task 3 | kv cache rm [0, end)\n> slot update_slots: id  2 | task 3 | prompt processing progress, n_past = 246, n_tokens = 1353, progress = 1.000000\n> slot update_slots: id  2 | task 3 | prompt done, n_past = 246, n_tokens = 1353\n> C:\\Sources\\llama.cpp\\src\\llama-context.cpp:1220: GGML_ASSERT((cparams.causal_attn || cparams.n_ubatch >= n_tokens_all) && \"non-causal attention requires n_ubatch >= n_tokens\") failed\n> ``` \n\n _Originally posted by @deiteris in [#12817](https://github.com/ggml-org/llama.cpp/issues/12817#issuecomment-2787097698)_",
    "labels": [
      "bug",
      "good first issue",
      "embeddings",
      "server"
    ],
    "state": "open",
    "created_at": "2025-04-08T18:28:48+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12836/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12836"
  },
  {
    "number": 6722,
    "title": "Problem with multiple simultaneous API calls on the embeddings endpoint",
    "body": "Hello,\r\n\r\nI'm using separate instance of the server just to generate the embedding for the RAG pipelines. This instance is not used for general chat use, just for embeddings.\r\n\r\nThe issue is that while the API call to `http://<server>:8080/v1/embeddings` is not completed, which can last for a long time during document embedding, the server does not respond to the next API call to the same endpoint. \r\n\r\nI have tried to overcome this limitation by adding `--threads-http 4 --parallel 4` switches when running the server, like this:\r\n\r\n`podman run -d --device nvidia.com/gpu=all -v /opt/models:/models:Z -p 8080:8000 ghcr.io/ggerganov/llama.cpp:server-cuda -m /models/uae-large-v1-f32.gguf --port 8000 --host 0.0.0.0 --n-gpu-layers 16 --threads 12 --threads-http 4 --parallel 4 --metrics --embedding --alias embedding --ctx-size 512`\r\n\r\nThis caused that after the first call I get this error and server crashes:\r\n`GGML_ASSERT: llama.cpp:9612: seq_id < n_tokens && \"seq_id cannot be larger than n_tokens with pooling_type == CLS\"`\r\n\r\nAm I doing something wrong? Is server supposed to process the API calls in parallel on the embedding API endpoint, or this is not even possible?",
    "labels": [
      "server/webui",
      "bug-unconfirmed",
      "embeddings"
    ],
    "state": "closed",
    "created_at": "2024-04-17T10:05:07+00:00",
    "closed_at": "2024-04-17T14:11:12+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6722/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6722"
  }
]
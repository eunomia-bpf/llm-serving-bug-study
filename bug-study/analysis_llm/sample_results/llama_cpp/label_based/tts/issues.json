[
  {
    "number": 12476,
    "title": "tts : add support for Orpheus",
    "body": "HF: https://huggingface.co/collections/canopylabs/orpheus-tts-67d9ea3f6c05a941c06ad9d2\n\nThese TTS models seem suitable for supporting. To do that, we need to implement the SNAC audio codec: https://github.com/hubertsiuzdak/snac/\n\nSample implementation using Python-based inference of SNAC: https://github.com/isaiahbjork/orpheus-tts-local\n\nSimilar model support (OuteTTS): https://github.com/ggml-org/llama.cpp/pull/10784\nCan be used as a reference how to implement this.",
    "labels": [
      "good first issue",
      "tts"
    ],
    "state": "open",
    "created_at": "2025-03-20T08:11:43+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12476/reactions",
      "total_count": 19,
      "+1": 12,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 7,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12476"
  },
  {
    "number": 12392,
    "title": "csm : implement Sesame-based conversation example",
    "body": "With the first Sesame CSM model [openly available](https://github.com/SesameAILabs/csm), we should implement a local example similar to their [online research demo](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo). It seems that the released CSM model uses [Kyutai's Mimi](https://arxiv.org/abs/2410.00037) audio codec which we have to implement in a similar way as we did with the [WavTokenizer](https://github.com/ggml-org/llama.cpp/pull/10784). Next we can modify the [talk-llama](https://github.com/ggerganov/whisper.cpp/tree/master/examples/talk-llama) example to support audio generation with the CSM. This way we will be able to plug any LLM for the text response generation and use Sesame for speech input/output.",
    "labels": [
      "model",
      "research \ud83d\udd2c",
      "stale",
      "tts"
    ],
    "state": "closed",
    "created_at": "2025-03-14T14:49:46+00:00",
    "closed_at": "2025-05-14T01:07:48+00:00",
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12392/reactions",
      "total_count": 28,
      "+1": 15,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 8,
      "eyes": 5
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12392"
  },
  {
    "number": 11749,
    "title": "llama-tts libc++abi: terminating due to uncaught exception of type std::out_of_range: vector\nAborted",
    "body": "```\n./llama-tts -m $model -mv $voice -p \"Hi i am Felix\"\n\nbuild: 4663 (c026ba3c) with clang version 19.1.5 for armv7a-unknown-linux-android24\nllama_model_loader: loaded meta data with 38 key-value pairs and 272 tensors from /storage/7DE2-358B/ysf/models/smollm-135m-instruct-q8_0.gguf (version GGUF V3 (latest))       llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = SmolLM 135M\nllama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB                                                                       llama_model_loader: - kv   4:                           general.finetune str              = instruct-add-basics\nllama_model_loader: - kv   5:                           general.basename str              = SmolLM                                                                              llama_model_loader: - kv   6:                         general.size_label str              = 135M\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0                                                                          llama_model_loader: - kv   8:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   9:                  general.base_model.0.name str              = SmolLM 135M\nllama_model_loader: - kv  10:          general.base_model.0.organization str              = HuggingFaceTB\nllama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/HuggingFaceTB/...\nllama_model_loader: - kv  12:                               general.tags arr[str,7]       = [\"alignment-handbook\", \"trl\", \"sft\", ...\nllama_model_loader: - kv  13:                           general.datasets arr[str,5]       = [\"HuggingFaceTB/Magpie-Pro-300K-Filte...                                            llama_model_loader: - kv  14:                          llama.block_count u32              = 30\nllama_model_loader: - kv  15:                       llama.context_length u32              = 2048\nllama_model_loader: - kv  16:                     llama.embedding_length u32              = 576\nllama_model_loader: - kv  17:                  llama.feed_forward_length u32              = 1536\nllama_model_loader: - kv  18:                 llama.attention.head_count u32              = 9\nllama_model_loader: - kv  19:              llama.attention.head_count_kv u32              = 3                                                                                   llama_model_loader: - kv  20:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  21:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  22:                          general.file_type u32              = 7                                                                                   llama_model_loader: - kv  23:                           llama.vocab_size u32              = 49152\nllama_model_loader: - kv  24:                 llama.rope.dimension_count u32              = 64                                                                                  llama_model_loader: - kv  25:            tokenizer.ggml.add_space_prefix bool             = false                                                                               llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = false                                                                               llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = smollm                                                                              llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\nllama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,48900]   = [\"\u0120 t\", \"\u0120 a\", \"i n\", \"h e\", \"\u0120 \u0120...                                                llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  34:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 2\nllama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\nllama_model_loader: - kv  37:               general.quantization_version u32              = 2                                                                                   llama_model_loader: - type  f32:   61 tensors\nllama_model_loader: - type q8_0:  211 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q8_0                                                          print_info: file size   = 136.40 MiB (8.51 BPW)\nload: special tokens cache size = 17\nload: token to piece cache size = 0.3170 MB\nprint_info: arch             = llama                                                    print_info: vocab_only       = 0\nprint_info: n_ctx_train      = 2048                                                     print_info: n_embd           = 576\nprint_info: n_layer          = 30\nprint_info: n_head           = 9                                                        print_info: n_head_kv        = 3\nprint_info: n_rot            = 64                                                       print_info: n_swa            = 0\nprint_info: n_embd_head_k    = 64\nprint_info: n_embd_head_v    = 64                                                       print_info: n_gqa            = 3\nprint_info: n_embd_k_gqa     = 192\nprint_info: n_embd_v_gqa     = 192                                                      print_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05                                                  print_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: n_ff             = 1536                                                     print_info: n_expert         = 0\nprint_info: n_expert_used    = 0                                                        print_info: causal attn      = 1\nprint_info: pooling type     = 0                                                        print_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1                                                        print_info: n_ctx_orig_yarn  = 2048\nprint_info: rope_finetuned   = unknown                                                  print_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0                                                        print_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = ?B\nprint_info: model params     = 134.52 M\nprint_info: general.name     = SmolLM 135M                                              print_info: vocab type       = BPE\nprint_info: n_vocab          = 49152                                                    print_info: n_merges         = 48900\nprint_info: BOS token        = 1 '<|im_start|>'\nprint_info: EOS token        = 2 '<|im_end|>'\nprint_info: EOT token        = 0 '<|endoftext|>'\nprint_info: UNK token        = 0 '<|endoftext|>'\nprint_info: PAD token        = 2 '<|im_end|>'\nprint_info: LF token         = 198 '\u010a'                                                  print_info: EOG token        = 0 '<|endoftext|>'\nprint_info: EOG token        = 2 '<|im_end|>'\nprint_info: max token length = 162                                                      load_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =   136.40 MiB\nllama_init_from_model: n_seq_max     = 1\nllama_init_from_model: n_ctx         = 8192\nllama_init_from_model: n_ctx_per_seq = 8192                                             llama_init_from_model: n_batch       = 8192\nllama_init_from_model: n_ubatch      = 512\nllama_init_from_model: flash_attn    = 0                                                llama_init_from_model: freq_base     = 10000.0\nllama_init_from_model: freq_scale    = 1\nllama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow\nllama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 30, can_shift = 1\nllama_kv_cache_init:        CPU KV buffer size =   180.00 MiB\nllama_init_from_model: KV self size  =  180.00 MiB, K (f16):   90.00 MiB, V (f16):   90.00 MiB\nllama_init_from_model:        CPU  output buffer size =     0.19 MiB\nllama_init_from_model:        CPU compute buffer size =   164.51 MiB                    llama_init_from_model: graph nodes  = 966\nllama_init_from_model: graph splits = 1\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192                  common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nllama_model_loader: loaded meta data with 27 key-value pairs and 75 tensors from /storage/emulated/0/Download/Lite-Oute-1-65M-Instruct-Q2_K.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = Lite-Oute-1-65M-Instruct\nllama_model_loader: - kv   2:                          llama.block_count u32              = 8                                                                                   llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 512\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 2048\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 16\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 10\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768                                                                               llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 32                                                                                  llama_model_loader: - kv  13:            tokenizer.ggml.add_space_prefix bool             = true\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 32000                                                                               llama_model_loader: - kv  21:            tokenizer.ggml.unknown_token_id u32              = 0                                                                                   llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 32000\nllama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  24:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\nllama_model_loader: - kv  26:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   17 tensors                                           llama_model_loader: - type q2_K:   33 tensors\nllama_model_loader: - type q3_K:   24 tensors                                           llama_model_loader: - type q6_K:    1 tensors\nprint_info: file format = GGUF V3 (latest)                                              print_info: file type   = Q2_K - Medium\nprint_info: file size   = 29.37 MiB (3.79 BPW)                                          load: control-looking token:  32000 '<|im_end|>' was not control-type; this is probably a bug in the model. its type will be overridden                                         load: special tokens cache size = 771\nload: token to piece cache size = 0.1710 MB                                             print_info: arch             = llama\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 2048\nprint_info: n_embd           = 512                                                      print_info: n_layer          = 8\nprint_info: n_head           = 16\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 32\nprint_info: n_swa            = 0\nprint_info: n_embd_head_k    = 32\nprint_info: n_embd_head_v    = 32\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 256                                                      print_info: n_embd_v_gqa     = 256\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00                                                  print_info: n_ff             = 2048\nprint_info: n_expert         = 0                                                        print_info: n_expert_used    = 0\nprint_info: causal attn      = 1                                                        print_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear                                                   print_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 2048\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0                                                        print_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = ?B\nprint_info: model params     = 65.02 M\nprint_info: general.name     = Lite-Oute-1-65M-Instruct\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 32768\nprint_info: n_merges         = 0\nprint_info: BOS token        = 1 '<s>'                                                  print_info: EOS token        = 32000 '<|im_end|>'\nprint_info: EOT token        = 32000 '<|im_end|>'\nprint_info: UNK token        = 0 '<unk>'\nprint_info: PAD token        = 32000 '<|im_end|>'                                       print_info: LF token         = 13 '<0x0A>'\nprint_info: EOG token        = 32000 '<|im_end|>'\nprint_info: max token length = 48                                                       load_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =    29.37 MiB\nllama_init_from_model: n_seq_max     = 1\nllama_init_from_model: n_ctx         = 8192\nllama_init_from_model: n_ctx_per_seq = 8192\nllama_init_from_model: n_batch       = 8192\nllama_init_from_model: n_ubatch      = 512\nllama_init_from_model: flash_attn    = 0                                                llama_init_from_model: freq_base     = 10000.0\nllama_init_from_model: freq_scale    = 1\nllama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow\nllama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 8, can_shift = 1                                                                    llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\nllama_init_from_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\nllama_init_from_model:        CPU  output buffer size =     0.00 MiB                    llama_init_from_model:        CPU compute buffer size =   276.01 MiB\nllama_init_from_model: graph nodes  = 262\nllama_init_from_model: graph splits = 1                                                 common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nsampler seed: 0                                                                         sampler params:\n        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n        dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n        top_k = 4, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000                         sampler chain: logits -> logit-bias -> top-k -> dist\nmain: loading done\nmain: constructing prompt ..                                                            main: prompt: 'hi<|text_sep|>i<|text_sep|>am<|text_sep|>felix'\n                                                                                        <|im_start|>\n<|text_start|>the<|text_sep|>overall<|text_sep|>package<|text_sep|>from<|text_sep|>just<|text_sep|>two<|text_sep|>people<|text_sep|>is<|text_sep|>pretty<|text_sep|>remarkable<|text_sep|>sure<|text_sep|>i<|text_sep|>have<|text_sep|>some<|text_sep|>critiques<|text_sep|>about<|text_sep|>some<|text_sep|>of<|text_sep|>the<|text_sep|>gameplay<|text_sep|>aspects<|text_sep|>but<|text_sep|>its<|text_sep|>still<|text_sep|>really<|text_sep|>enjoyable<|text_sep|>and<|text_sep|>it<|text_sep|>looks<|text_sep|>lovely<|text_sep|>hi<|text_sep|>i<|text_sep|>am<|text_sep|>felix<|text_end|>                                       libc++abi: terminating due to uncaught exception of type std::out_of_range: vector\nAborted\n```",
    "labels": [
      "tts"
    ],
    "state": "closed",
    "created_at": "2025-02-08T05:33:49+00:00",
    "closed_at": "2025-02-21T15:56:06+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11749/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11749"
  },
  {
    "number": 10173,
    "title": "tts : add basic example for text-to-speech",
    "body": "This new model seems suitable for integration: https://github.com/edwko/OuteTTS\r\n\r\nWe should add a very minimalistic example for generating audio with it. Ideally, we will implement the (audio tokens) -> (wav) from scratch.",
    "labels": [
      "good first issue",
      "tts"
    ],
    "state": "closed",
    "created_at": "2024-11-04T18:53:25+00:00",
    "closed_at": "2024-12-18T17:27:22+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10173/reactions",
      "total_count": 22,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 7,
      "confused": 0,
      "heart": 10,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10173"
  }
]
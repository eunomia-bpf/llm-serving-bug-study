[
  {
    "number": 9086,
    "title": "Feature Request: Tensor Parallelism support",
    "body": "### Prerequisites\r\n\r\n- [X] I am running the latest code. Mention the version if possible as well.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\r\n\r\n### Feature Description\r\n\r\nTensor parallelism is a a critical technique employed to train and inference from very large language models by splitting the actual computations/tensors across multiple compute devices. \r\n\r\n### Motivation\r\n\r\nIn our previous implementation on Xeon CPU, tensor parallelism(TP) can significantly reduce the latency on inference. <html xmlns:v=\"urn:schemas-microsoft-com:vml\"\r\nxmlns:o=\"urn:schemas-microsoft-com:office:office\"\r\nxmlns:x=\"urn:schemas-microsoft-com:office:excel\"\r\nxmlns=\"http://www.w3.org/TR/REC-html40\">\r\n\r\n<head>\r\n\r\n<meta name=ProgId content=Excel.Sheet>\r\n<meta name=Generator content=\"Microsoft Excel 15\">\r\n<link id=Main-File rel=Main-File\r\nhref=\"file:///C:/Users/chen13/AppData/Local/Temp/msohtmlclip1/01/clip.htm\">\r\n<link rel=File-List\r\nhref=\"file:///C:/Users/chen13/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml\">\r\n<style>\r\n<!--table\r\n\t{mso-displayed-decimal-separator:\"\\.\";\r\n\tmso-displayed-thousand-separator:\"\\,\";}\r\n@page\r\n\t{margin:.75in .7in .75in .7in;\r\n\tmso-header-margin:.3in;\r\n\tmso-footer-margin:.3in;}\r\ntr\r\n\t{mso-height-source:auto;}\r\ncol\r\n\t{mso-width-source:auto;}\r\nbr\r\n\t{mso-data-placement:same-cell;}\r\ntd\r\n\t{padding-top:1px;\r\n\tpadding-right:1px;\r\n\tpadding-left:1px;\r\n\tmso-ignore:padding;\r\n\tcolor:black;\r\n\tfont-size:11.0pt;\r\n\tfont-weight:400;\r\n\tfont-style:normal;\r\n\ttext-decoration:none;\r\n\tfont-family:Calibri, sans-serif;\r\n\tmso-font-charset:0;\r\n\tmso-number-format:General;\r\n\ttext-align:general;\r\n\tvertical-align:bottom;\r\n\tborder:none;\r\n\tmso-background-source:auto;\r\n\tmso-pattern:auto;\r\n\tmso-protection:locked visible;\r\n\twhite-space:nowrap;\r\n\tmso-rotate:0;}\r\n-->\r\n</style>\r\n</head>\r\n\r\n<body link=\"#0563C1\" vlink=\"#954F72\">\r\n\r\n\r\nmodel | precision | TP size | input_size | nex_token_time/ms\r\n-- | -- | -- | -- | --\r\nllama2-70b | q4_j | 1 | 32 | 191.91\r\nllama2-70b | q4_j | 2 | 32 | 120.87\r\nllama2-70b | q4_j | 4 | 32 | 86.15\r\nllama2-70b | q4_j | 1 | 1024 | 197.18\r\nllama2-70b | q4_j | 2 | 1024 | 129.25\r\nllama2-70b | q4_j | 4 | 1024 | 91.76\r\nllama2-70b | q4_j | 1 | 2012 | 204.85\r\nllama2-70b | q4_j | 2 | 2012 | 127.31\r\nllama2-70b | q4_j | 4 | 2012 | 100.44\r\n\r\n\r\n\r\n</body>\r\n\r\n</html>\r\n\r\nNotice: TP size= 1 means not use TP.\r\n\r\n### Possible Implementation\r\n\r\nIn our TP implementation, we adopt the method of pre-splitting the corresponding weights, so the time consumed for this part is one-time and does not affect inference performance. Meanwhile, another major factor impacting performance is 'all reduce'. Since each node computes partial and incomplete results, it is necessary to perform 'all reduce' on the output data. But all reduce is relatively time-consuming, interestingly, by using a reasonable splitting and combining method, primitives can be operated independently across nodes, which is very helpful for performance optimization. Thus, a rational splitting method becomes extremely important.\r\n\r\nTaking the FFN module as an example, if the first matmul splits by column and computes the matmul with input, it will result in two unrelated sub-matrices on each node. These two sub-matrices, when performing the second matmul operation, can proceed directly without having to perform 'all reduce' if splitting by rows. Thus, the entire FFN module only requires one 'all reduce', meaning that with properly tailored split implementation, even with multiple matmul operations, only one 'all reduce' operation may be needed. We ignored the element-wise operations between matmul as they would not influence the results.\r\n![image](https://github.com/user-attachments/assets/8a9d6c4a-45ca-4fa7-9930-1660936fda90)\r\nThe scenario for the attention module is more complex. As shown in the following figure, a rational split can make it so that the entire attention module only requires one 'all reduce' operation, thus greatly saving synchronization time.\r\n![image](https://github.com/user-attachments/assets/19d77152-4dff-4b8e-a3e2-34d582ce3b53)\r\n",
    "labels": [
      "enhancement",
      "threading",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-19T01:38:13+00:00",
    "closed_at": "2024-12-13T01:07:40+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9086/reactions",
      "total_count": 11,
      "+1": 11,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9086"
  },
  {
    "number": 5417,
    "title": "CPU performance bottleneck(?) when using macOS Accelerate",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\n\r\nI've been doing some performance testing of llama.cpp in macOS (On M2 Ultra 24-Core) and was comparing the CPU performance of inference with various options, and ran into a very large performance drop - Mixtral model inference on 16 cores (16 because it's only the performance cores, the other 8 are efficiency cores on my CPU) was much faster without Accelerate. This prompted me to test other models of different sizes and across using 3 different core counts.\r\n\r\n<img width=\"621\" alt=\"Screenshot 2024-02-08 at 16 39 16\" src=\"https://github.com/ggerganov/llama.cpp/assets/521581/b1820086-bca5-4740-a7b2-e190c2d6b5db\">\r\n\r\nNotes:\r\n- All tests fit the model in RAM, there was no swapping/paging.\r\n- Performance gains are in green, drops are in red. Significant changes are bolded, the others may just be due to single iterations.\r\n- The drop in performance that caught my eye was the Mixtral with 16 cores drop in performance, which is the most pronounced above, bottom-right.)\r\n\r\nThe initial results suggest that there is some asymptotic \"ceiling\" in CPU inference when using Accelerate, that seems to show up when a sufficient amount of cores is engaged. So while it starts off providing a big performance boost as expected, the more cores that are added, the growth in performance levels out. This flattening seems to be accelerated (no pun intended) the larger the model is. In comparison not using Accelerate starts off much slower but keeps growing almost linearly as cores are added.\r\n\r\n# Motivation\r\n\r\nAllowing llama.cpp to run as fast as possible on more variants of Apple Silicon.\r\n\r\n# Possible Implementation\r\n\r\nI will try to run this through the profiler to see if there is some code bottleneck. There are two possibilities that I can think of, off the top of my head:\r\n\r\n**If this is because of Apple Silicon architecture and/or the vDSP/Accelerate framework function dispatches**, then it may be useful to add parameters to ggml CPU contexts which, even if Accelerate support is compiled-in, can still turn it off. This way applications can, based on the hardware, disable the accelerate code paths at runtime (for instance, disabling accelerate code paths when running with 16 cores on CPU)\r\n\r\n**If this is because of some code bottleneck in the ggml library**, then it may be worth identifying and seeing how it can be alleviated. I'll try some profiling on my machine, but it's possible that someone with more experience of this codebase can have tips or suggestions on what may be going on, or tips on the best places to start looking.\r\n",
    "labels": [
      "enhancement",
      "performance",
      "macos",
      "threading"
    ],
    "state": "closed",
    "created_at": "2024-02-08T16:53:12+00:00",
    "closed_at": "2024-02-11T19:12:45+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5417/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5417"
  },
  {
    "number": 1189,
    "title": "fix (perf/UX): get physical cores for Windows",
    "body": "Complete https://github.com/ggerganov/llama.cpp/pull/934 with the windows impl of physical cores\r\n\r\nThe impl is approximately: \r\n```c++\r\nDWORD buffer_size = 0;\r\nDWORD result = GetLogicalProcessorInformation(NULL, &buffer_size);\r\n// assert result == FALSE && GetLastError() == ERROR_INSUFFICIENT_BUFFER\r\nPSYSTEM_LOGICAL_PROCESSOR_INFORMATION buffer = (PSYSTEM_LOGICAL_PROCESSOR_INFORMATION)malloc(buffer_size);\r\nresult = GetLogicalProcessorInformation(buffer, &buffer_size);\r\nif (result != FALSE) {\r\n    int num_physical_cores = 0;\r\n    DWORD_PTR byte_offset = 0;\r\n    while (byte_offset < buffer_size) {\r\n        if (buffer->Relationship == RelationProcessorCore) {\r\n            num_physical_cores++;\r\n        }\r\n        byte_offset += sizeof(SYSTEM_LOGICAL_PROCESSOR_INFORMATION);\r\n        buffer++;\r\n    }\r\n    std::cout << \"Number of physical cores: \" << num_physical_cores << std::endl;\r\n} else {\r\n    std::cerr << \"Error getting logical processor information: \" << GetLastError() << std::endl;\r\n}\r\nfree(buffer);\r\n```\r\n\r\nThe location of the change is here: https://github.com/ggerganov/llama.cpp/blob/4a98a0f21ad63d97a643ba6fb21f613cb596cb23/examples/common.cpp#L57",
    "labels": [
      "enhancement",
      "hardware",
      "windows",
      "threading",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-26T14:41:54+00:00",
    "closed_at": "2024-04-09T01:09:51+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1189/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1189"
  },
  {
    "number": 1188,
    "title": "[Suggestion] Add parameter for setting openblas threads",
    "body": "Openblas deafults to some maximum available threads, but would probably not be the most optimal.\r\nIn Openblas there is a function to set the number of threads, why not use this?\r\n\r\n```void openblas_set_num_threads(int num_threads);```\r\n\r\nCurrent workaround is to set an openblas environment variable.",
    "labels": [
      "enhancement",
      "threading",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-26T13:24:17+00:00",
    "closed_at": "2024-04-09T01:09:52+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1188/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1188"
  },
  {
    "number": 1159,
    "title": "[User] Deadlock if number of threads > number of (hyper)threads",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI expect the program to run suboptimally but finish.\r\n\r\n# Current Behavior\r\n\r\nCurrently the program locks up with very large cpu utilization.\r\n\r\n# Environment and Context\r\n\r\nI have a 6 core intel machine i.e. 12 threads with hyperthreading. \r\nOnce I run with -t 13 the deadlock happens:\r\n`./main -m ./models/ggml-vicuna-13b-1.1-q4_0.bin -n 256 --repeat_penalty 1.1 --color -i -r \"### Human:\" -f prompts/chat-with-vicuna.txt -t 13`\r\n",
    "labels": [
      "threading"
    ],
    "state": "closed",
    "created_at": "2023-04-24T19:02:29+00:00",
    "closed_at": "2023-05-03T18:30:11+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1159/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1159"
  }
]
[
  {
    "number": 8117,
    "title": "Bug: Crash with GGML CUDA error when inferencing on llama-server",
    "body": "### What happened?\n\nllama-server is crashing repeatably with a GGML CUDA error on commit a818f30 and later.  d62e4aa and earlier work correctly.  I have not been able to reproduce this with llama-cli.\r\n\r\n`/opt/llama.cpp-a818f30/bin/llama-server --host localhost --port 18443 --n-gpu-layers 81 --ctx-size 8192 --model meta-llama-3-70b-instruct-q4_k.gguf`\r\n\r\nIn addition to the log I posted, I also tried launching on a single GPU with only one GPU layer, but the result is the same. \r\n`CUDA_VISIBLE_DEVICES=0 /opt/llama.cpp-a818f30/bin/llama-server --host localhost --port 18443 --n-gpu-layers 1 --ctx-size 8192 --model meta-llama-3-70b-instruct-q4_k.gguf`\r\n\r\nEven zero GPU layers will cause a crash.\r\n`CUDA_VISIBLE_DEVICES=0 /opt/llama.cpp-a818f30/bin/llama-server --host localhost --port 18443 --n-gpu-layers 0 --ctx-size 8192 --model meta-llama-3-70b-instruct-q4_k.gguf`\r\n\r\nThis may be related to #8096 @JohannesGaessler \n\n### Name and Version\n\n$ /opt/llama.cpp-a818f30/bin/llama-server --version\r\nversion: 3216 (a818f302)\r\nbuilt with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\n/opt/llama.cpp-a818f30/bin/llama-server --host localhost --port 18443 --n-gpu-layers 81 --ctx-size 8192 --model meta-llama-3-70b-instruct-q4_k.gguf\r\nINFO [                    main] build info | tid=\"140294345007104\" timestamp=1719338183 build=3216 commit=\"a818f302\"\r\nINFO [                    main] system info | tid=\"140294345007104\" timestamp=1719338183 n_threads=32 n_threads_batch=-1 total_threads=64 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \"\r\nllama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from meta-llama-3-70b-instruct-q4_k.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-70B-Instruct\r\nllama_model_loader: - kv   2:                          llama.block_count u32              = 80\r\nllama_model_loader: - kv   3:                       llama.context_length u32              = 8192\r\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672\r\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 64\r\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\r\nllama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"\u0120 \u0120\", \"\u0120 \u0120\u0120\u0120\", \"\u0120\u0120 \u0120\u0120\", \"...\r\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\r\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\r\nllama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\r\nllama_model_loader: - kv  21:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  161 tensors\r\nllama_model_loader: - type q4_K:  441 tensors\r\nllama_model_loader: - type q5_K:   40 tensors\r\nllama_model_loader: - type q6_K:   81 tensors\r\nllm_load_vocab: special tokens cache size = 256\r\nllm_load_vocab: token to piece cache size = 0.8000 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 128256\r\nllm_load_print_meta: n_merges         = 280147\r\nllm_load_print_meta: n_ctx_train      = 8192\r\nllm_load_print_meta: n_embd           = 8192\r\nllm_load_print_meta: n_head           = 64\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 80\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 8\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 28672\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 500000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 8192\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 70B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 70.55 B\r\nllm_load_print_meta: model size       = 39.59 GiB (4.82 BPW) \r\nllm_load_print_meta: general.name     = Meta-Llama-3-70B-Instruct\r\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\r\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: LF token         = 128 '\u00c4'\r\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: max token length = 256\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 2 CUDA devices:\r\n  Device 0: NVIDIA TITAN RTX, compute capability 7.5, VMM: yes\r\n  Device 1: NVIDIA TITAN RTX, compute capability 7.5, VMM: yes\r\nllm_load_tensors: ggml ctx size =    1.01 MiB\r\nllm_load_tensors: offloading 80 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 81/81 layers to GPU\r\nllm_load_tensors:        CPU buffer size =   563.62 MiB\r\nllm_load_tensors:      CUDA0 buffer size = 20038.81 MiB\r\nllm_load_tensors:      CUDA1 buffer size = 19940.67 MiB\r\n...................................................................................................\r\nllama_new_context_with_model: n_ctx      = 8192\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 500000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:      CUDA0 KV buffer size =  1312.00 MiB\r\nllama_kv_cache_init:      CUDA1 KV buffer size =  1248.00 MiB\r\nllama_new_context_with_model: KV self size  = 2560.00 MiB, K (f16): 1280.00 MiB, V (f16): 1280.00 MiB\r\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.98 MiB\r\nllama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\r\nllama_new_context_with_model:      CUDA0 compute buffer size =  1216.01 MiB\r\nllama_new_context_with_model:      CUDA1 compute buffer size =  1216.02 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =    80.02 MiB\r\nllama_new_context_with_model: graph nodes  = 2566\r\nllama_new_context_with_model: graph splits = 3\r\nINFO [                    init] initializing slots | tid=\"140294345007104\" timestamp=1719338191 n_slots=1\r\nINFO [                    init] new slot | tid=\"140294345007104\" timestamp=1719338191 id_slot=0 n_ctx_slot=8192\r\nINFO [                    main] model loaded | tid=\"140294345007104\" timestamp=1719338191\r\nINFO [                    main] chat template | tid=\"140294345007104\" timestamp=1719338191 chat_example=\"<|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHi there<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\" built_in=true\r\nINFO [                    main] HTTP server listening | tid=\"140294345007104\" timestamp=1719338191 n_threads_http=\"63\" port=\"18443\" hostname=\"localhost\"\r\nINFO [            update_slots] all slots are idle | tid=\"140294345007104\" timestamp=1719338191\r\nINFO [   launch_slot_with_task] slot is processing task | tid=\"140294345007104\" timestamp=1719338195 id_slot=0 id_task=0\r\nINFO [            update_slots] kv cache rm [p0, end) | tid=\"140294345007104\" timestamp=1719338195 id_slot=0 id_task=0 p0=0\r\nCUDA error: misaligned address\r\n  current device: 0, in function launch_mul_mat_q at /XXX/llama.cpp/ggml-cuda/template-instances/../mmq.cuh:2454\r\n  cudaFuncSetAttribute(mul_mat_q<type, mmq_x, 8, false>, cudaFuncAttributeMaxDynamicSharedMemorySize, shmem)\r\nGGML_ASSERT: /XXX/llama.cpp/ggml-cuda.cu:100: !\"CUDA error\"\r\nAborted\n```\n",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-06-25T18:21:37+00:00",
    "closed_at": "2024-06-26T06:28:03+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8117/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8117"
  },
  {
    "number": 10421,
    "title": "Bug: SYCL builds >= b4069 have half the context limit of previous builds",
    "body": "### What happened?\r\n\r\n```\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |\r\n    |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |\r\n    |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|    1.5|    512|    1024|   32| 16704M|            1.3.31093|\r\nllama_kv_cache_init:      SYCL0 KV buffer size =  3937.50 MiB\r\nllama_new_context_with_model: KV self size  = 3937.50 MiB, K (f16): 1968.75 MiB, V (f16): 1968.75 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =    27.82 MiB\r\nggml_backend_sycl_buffer_type_alloc_buffer: can't malloc 3278637056 Bytes memory on deviceggml_gallocr_reserve_n: failed to allocate SYCL0 buffer of size 7573604352\r\nggml_backend_sycl_buffer_type_alloc_buffer: can't malloc 3278637056 Bytes memory on deviceggml_gallocr_reserve_n: failed to allocate SYCL0 buffer of size 7573604352\r\nllama_new_context_with_model: failed to allocate compute buffers\r\ncommon_init_from_params: failed to create context with model 'C:\\LLM\\Qwen2.5-3B-Instruct_Q4_1.gguf'\r\nsrv    load_model: failed to load model, 'C:\\LLM\\Qwen2.5-3B-Instruct_Q4_1.gguf'\r\nmain: exiting due to model loading error\r\n```\r\n\r\ncommand line: `llama-server.exe -t 16 --threads-http 8 --mlock -ngl 99 -m C:\\LLM\\Qwen2.5-3B-Instruct_Q4_1.gguf --port 8888 --ctx-size 112000 -np 48 --sampling-seq mt --min-p 0.1 --temp 1.5 -dt .1`\r\n\r\ncommand line works fine on builds < b4069\r\n\r\ni have to lower context all the way to 60k with b4069\r\n\r\nGPU: Intel Arc A770 (16GB)\r\nOS: Windows\r\n\r\n### Name and Version\r\n\r\nZE_LOADER_DEBUG_TRACE:Using Loader Library Path:\r\nZE_LOADER_DEBUG_TRACE:Tracing Layer Library Path: ze_tracing_layer.dll\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\nversion: 4069 (2e82ffa4)\r\nbuilt with MSVC 19.41.34123.0 for\r\n\r\n### What operating system are you seeing the problem on?\r\nWindows\r\n\r\n### Relevant log output\r\n\r\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-11-20T08:32:21+00:00",
    "closed_at": "2024-11-28T05:49:36+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10421/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10421"
  },
  {
    "number": 8828,
    "title": "Bug: Vulkan backend crash on model loading",
    "body": "### What happened?\n\nI mainly use LLamaSharp C# bindings, after updating to v0.14.0 and releasing Vulkan backend, I decided to give it a try instead using CPU inference, but on loading model it crash with console output\r\n\r\n```\r\nWARNING: [Loader Message] Code 0 : windows_read_data_files_in_registry: Registry lookup failed to get layer manifest files.\r\nWARNING: [Loader Message] Code 0 : Layer VK_LAYER_RENDERDOC_Capture uses API version 1.2 which is older than the application specified API version of 1.3. May cause issues.\r\nllama_model_loader: loaded meta data with 25 key-value pairs and 327 tensors from C:\\Models\\Text\\Index-1.9B-Character\\Index-1.9B-Character-Q6_K.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = Index-1.9B-Character_test\r\nllama_model_loader: - kv   2:                          llama.block_count u32              = 36\r\nllama_model_loader: - kv   3:                       llama.context_length u32              = 4096\r\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5888\r\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 16\r\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 16\r\nllama_model_loader: - kv   8:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv   9:                          general.file_type u32              = 18\r\nllama_model_loader: - kv  10:                           llama.vocab_size u32              = 65029\r\nllama_model_loader: - kv  11:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  12:            tokenizer.ggml.add_space_prefix bool             = false\r\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,65029]   = [\"<unk>\", \"<s>\", \"</s>\", \"reserved_0\"...\r\nllama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,65029]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,65029]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\r\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\r\nllama_model_loader: - kv  24:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   73 tensors\r\nllama_model_loader: - type q6_K:  254 tensors\r\nllm_load_vocab: special tokens cache size = 515\r\nllm_load_vocab: token to piece cache size = 0.3670 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 65029\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 2048\r\nllm_load_print_meta: n_layer          = 36\r\nllm_load_print_meta: n_head           = 16\r\nllm_load_print_meta: n_head_kv        = 16\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 2048\r\nllm_load_print_meta: n_embd_v_gqa     = 2048\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 5888\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 8B\r\nllm_load_print_meta: model ftype      = Q6_K\r\nllm_load_print_meta: model params     = 2.17 B\r\nllm_load_print_meta: model size       = 1.66 GiB (6.56 BPW) \r\nllm_load_print_meta: general.name     = Index-1.9B-Character_test\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 270 '<0x0A>'\r\nllm_load_print_meta: max token length = 48\r\nggml_vulkan: Found 1 Vulkan devices:\r\nVulkan0: Radeon RX 580 Series (AMD proprietary driver) | uma: 0 | fp16: 0 | warp size: 64\r\nFatal error: System.AccessViolationException: Attempted to read or write protected memory. This is often an indication that other memory is corrupt.\r\n\r\nRepeat 2 times:\r\n--------------------------------\r\nat LLama.Native.SafeLlamaModelHandle.llama_load_model_from_file(System.String, LLama.Native.LLamaModelParams)\r\n--------------------------------\r\nat LLama.Native.SafeLlamaModelHandle.LoadFromFile(System.String, LLama.Native.LLamaModelParams)\r\nat LLama.LlamaWeights+<>c__DisplayClass21_0.<LoadFromFileAsync>b__1()\r\nat System.Threading.Tasks.Task`1[[System.__Canon, System.Private.CoreLib, Version=8.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].InnerInvoke()\r\nat System.Threading.ExecutionContext.RunFromThreadPoolDispatchLoop(System.Threading.Thread, System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object)\r\nat System.Threading.Tasks.Task.ExecuteWithThreadLocal(System.Threading.Tasks.Task ByRef, System.Threading.Thread)\r\nat System.Threading.ThreadPoolWorkQueue.Dispatch()\r\nat System.Threading.PortableThreadPool+WorkerThread.WorkerThreadStart()\r\n```\r\n\r\nI decided to give llama.cpp released binaries a try, firstly I tried using release `b3375` which is the base for LLamaSharp then the latest release `b3504`, tried both versions with both AVX2 and Vulkan binaries, but result was same like LLamaSharp on Vulkan with console output\r\n\r\n```\r\nC:\\External\\llama-b3504-bin-win-vulkan-x64>llama-cli --model Index-1.9B-Character-Q6_K.gguf --n-gpu-layers 8 -cnv\r\nLog start\r\nmain: build = 3504 (e09a800f)\r\nmain: built with MSVC 19.29.30154.0 for x64\r\nmain: seed  = 1722604647\r\nllama_model_loader: loaded meta data with 25 key-value pairs and 327 tensors from Index-1.9B-Character-Q6_K.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = Index-1.9B-Character_test\r\nllama_model_loader: - kv   2:                          llama.block_count u32              = 36\r\nllama_model_loader: - kv   3:                       llama.context_length u32              = 4096\r\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5888\r\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 16\r\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 16\r\nllama_model_loader: - kv   8:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv   9:                          general.file_type u32              = 18\r\nllama_model_loader: - kv  10:                           llama.vocab_size u32              = 65029\r\nllama_model_loader: - kv  11:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  12:            tokenizer.ggml.add_space_prefix bool             = false\r\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,65029]   = [\"<unk>\", \"<s>\", \"</s>\", \"reserved_0\"...\r\nllama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,65029]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,65029]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\r\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\r\nllama_model_loader: - kv  24:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   73 tensors\r\nllama_model_loader: - type q6_K:  254 tensors\r\nllm_load_vocab: special tokens cache size = 259\r\nllm_load_vocab: token to piece cache size = 0.3670 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 65029\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 2048\r\nllm_load_print_meta: n_layer          = 36\r\nllm_load_print_meta: n_head           = 16\r\nllm_load_print_meta: n_head_kv        = 16\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 2048\r\nllm_load_print_meta: n_embd_v_gqa     = 2048\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 5888\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 8B\r\nllm_load_print_meta: model ftype      = Q6_K\r\nllm_load_print_meta: model params     = 2.17 B\r\nllm_load_print_meta: model size       = 1.66 GiB (6.56 BPW)\r\nllm_load_print_meta: general.name     = Index-1.9B-Character_test\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 270 '<0x0A>'\r\nllm_load_print_meta: max token length = 48\r\nggml_vulkan: Found 1 Vulkan devices:\r\nVulkan0: Radeon RX 580 Series (AMD proprietary driver) | uma: 0 | fp16: 0 | warp size: 64\r\n```\r\n\r\nI tried with different models, 1.9B, 1.1B, 300M, 22M and different --n-gpu-layers like 1, 0, 8, 16, 36 on RX 580 4GB GPU but utilization still 4% like idle and vram is empty\n\n### Name and Version\n\nllama-cli --version: 3504 (e09a800f) built with MSVC 19.29.30154.0 for x64\r\nllama-cli --version: 3375 (36864569) built with MSVC 19.29.30154.0 for x64\r\n\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-08-02T13:32:45+00:00",
    "closed_at": "2024-08-18T13:58:08+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8828/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8828"
  },
  {
    "number": 9850,
    "title": "Bug: b3878 breaks server RPC with CUDA",
    "body": "### What happened?\n\nStarting Qwen 2.5 32B IQ4_XS (model doesnt matter, it crashes with any model or quant) with RPC (using one remote server in test) will crash both remote and local host.  The commit which breaks it is fabdc3bda396307565c4f3f4ecbc3a751a2eb6d3.  The model will start and run without using RPC with the commit.\r\n\r\nTo fix the crash,  revert fabdc3bda396307565c4f3f4ecbc3a751a2eb6d3 as follows:\r\ngit checkout b3878\r\ngit diff eee39bdc96065b69242877fe8f1be05c885fc2aa  fabdc3bda396307565c4f3f4ecbc3a751a2eb6d3 >patch.diff\r\ngit apply -R patch.diff\r\n\r\nThe patch still reverts on b3906 and RPC again works with the revert.\n\n### Name and Version\n\nllama-server, rpc-server, b3878 .. b3906\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\nLocal crash:\r\n\r\nUsing host libthread_db library \"/lib64/libthread_db.so.1\".\r\n0x00007f341743a3c7 in wait4 () from /lib64/libc.so.6\r\n#0  0x00007f341743a3c7 in wait4 () from /lib64/libc.so.6\r\n#1  0x00007f341795ead8 in ggml_abort () from /tmp/build/ggml/src/libggml.so\r\n#2  0x00007f3417adda9c in ggml_backend_rpc_graph_compute(ggml_backend*, ggml_cgraph*) () from /tmp/build/ggml/src/libggml.so\r\n#3  0x00007f34179a5215 in ggml_backend_sched_graph_compute_async () from /tmp/build/ggml/src/libggml.so\r\n#4  0x00007f3423d3d351 in llama_decode () from /tmp/build/src/libllama.so\r\n#5  0x00000000004e92be in common_init_from_params(common_params&) ()\r\n#6  0x000000000047f1c5 in server_context::load_model(common_params const&) ()\r\n#7  0x0000000000426bfb in main ()\r\n[Inferior 1 (process 12087) detached]\r\n/usr/local/bin/ll_start: line 1441: 12087 Aborted\r\n\r\nRemote crash:\r\n\r\ncreate_backend: using CUDA backend\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    nob3906\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUb3906DA devices:\r\n  Device 0: NVIDIA GeForce RTX 4070, compute capability 8.9, VMM: yes\r\nStarting RPC server on 0.0.0.0:50052, backend memory: 11803 MB\r\nAccepted client connection, free_mem=12376866816, total_mem=12584747008\r\nClient connection closed\r\nAccepted client connectionb3906, free_mem=12376866816, total_mem=12584747008\r\nClient connection closed\r\nAccepted client connection, free_mem=12376866816, total_mem=12584747008\r\nggml_backend_cuda_graph_compute: op not supported norm-0 (RMS_NORM_BACK)\r\n/usr/local/src/ai/llamacpp/llama.cpp/ggml/src/ggml-cuda.cu:2636: GGML_ASSERT(ok) failed\r\n[New LWP 24492]\r\n[New LWP 24496]\r\n[New LWP 24497]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib64/libthread_db.so.1\".\r\n0x00007f6ae4fc53c7 in wait4 () from /lib64/libc.so.6\r\n#0  0x00007f6ae4fc53c7 in wait4 () from /lib64/libc.so.6\r\n#1  0x00007f6ae54bd618 in ggml_abort () from /tmp/build/ggml/src/libggml.so\r\n#2  0x00007f6ae559faee in ggml_backend_cuda_graph_compute(ggml_backend*, ggml_cgraph*) () from /tmp/build/ggml/src/libggml.so\r\n#3  0x00007f6ae550124f in ggml_backend_graph_compute () from /tmp/build/ggml/src/libggml.so\r\n#4  0x00007f6ae56359c3 in rpc_server::graph_compute(std::vector<unsigned char, std::allocator<unsigned char> > const&, std::vector<unsigned char, std::allocator<unsigned char> >&) () from /tmp/build/ggml/src/libggml.so\r\n#5  0x00007f6ae5636a5c in start_rpc_server () from /tmp/build/ggml/src/libggml.so\r\n#6  0x00000000004019e5 in main ()\r\n[Inferior 1 (process 24491) detached]\r\n/usr/local/bin/ll_startrpc: line 14: 24491 Aborted                 rpc-server -H 0.0.0.0 -p 50052\n```\n",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-10-11T17:32:20+00:00",
    "closed_at": "2024-10-11T19:18:36+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9850/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9850"
  },
  {
    "number": 10109,
    "title": "Bug: Cannot load Mamba model",
    "body": "### What happened?\n\nWhen trying to use a Mamba model, in this case `falcon-mamba-7b-Q4_K_S.gguf`, there is a Segment fault:\r\n```console\r\n$ ./llama-cli -m models/falcon-mamba-7b-Q4_K_S.gguf -ngl 33 --no-warmup --prompt '\"What is LoRA?\"' -n 10\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 4070, compute capability 8.9, VMM: yes\r\nregister_backend: registered backend CUDA (1 devices)\r\nregister_device: registered device CUDA0 (NVIDIA GeForce RTX 4070)\r\nregister_backend: registered backend CPU (1 devices)\r\nregister_device: registered device CPU (12th Gen Intel(R) Core(TM) i7-1260P)\r\nbuild: 3997 (dea5e860) with cc (Ubuntu 13.2.0-23ubuntu4) 13.2.0 for x86_64-linux-gnu (debug)\r\nmain: llama backend init\r\nmain: load the model and apply lora adapter, if any\r\nllama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 4070) - 11743 MiB free\r\nllama_model_loader: loaded meta data with 34 key-value pairs and 643 tensors from models/falcon-mamba-7b-Q4_K_S.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = mamba\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                           general.basename str              = falcon-mamba\r\nllama_model_loader: - kv   3:                         general.size_label str              = 7B\r\nllama_model_loader: - kv   4:                            general.license str              = other\r\nllama_model_loader: - kv   5:                       general.license.name str              = falcon-mamba-7b-license\r\nllama_model_loader: - kv   6:                       general.license.link str              = https://falconllm.tii.ae/falcon-mamba...\r\nllama_model_loader: - kv   7:                          general.languages arr[str,1]       = [\"en\"]\r\nllama_model_loader: - kv   8:                           general.datasets arr[str,2]       = [\"tiiuae/falcon-refinedweb\", \"Hugging...\r\nllama_model_loader: - kv   9:                       mamba.context_length u32              = 1048576\r\nllama_model_loader: - kv  10:                     mamba.embedding_length u32              = 4096\r\nllama_model_loader: - kv  11:                  mamba.feed_forward_length u32              = 0\r\nllama_model_loader: - kv  12:                 mamba.attention.head_count u32              = 0\r\nllama_model_loader: - kv  13:                          mamba.block_count u32              = 64\r\nllama_model_loader: - kv  14:                      mamba.ssm.conv_kernel u32              = 4\r\nllama_model_loader: - kv  15:                       mamba.ssm.inner_size u32              = 8192\r\nllama_model_loader: - kv  16:                       mamba.ssm.state_size u32              = 16\r\nllama_model_loader: - kv  17:                   mamba.ssm.time_step_rank u32              = 256\r\nllama_model_loader: - kv  18:     mamba.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  19:                       mamba.ssm.dt_b_c_rms bool             = true\r\nllama_model_loader: - kv  20:                          general.file_type u32              = 14\r\nllama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = falcon\r\nllama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,65024]   = [\">>TITLE<<\", \">>ABSTRACT<<\", \">>INTR...\r\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,65024]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\r\nllama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,64784]   = [\"\u0120 t\", \"\u0120 a\", \"i n\", \"h e\", \"r e\",...\r\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 11\r\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 0\r\nllama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 11\r\nllama_model_loader: - kv  29:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  30:                      quantize.imatrix.file str              = /models_out/falcon-mamba-7b-GGUF/falc...\r\nllama_model_loader: - kv  31:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\r\nllama_model_loader: - kv  32:             quantize.imatrix.entries_count i32              = 256\r\nllama_model_loader: - kv  33:              quantize.imatrix.chunks_count i32              = 139\r\nllama_model_loader: - type  f32:  385 tensors\r\nllama_model_loader: - type q4_K:  257 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special tokens cache size = 12\r\nllm_load_vocab: token to piece cache size = 0.3884 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = mamba\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 65024\r\nllm_load_print_meta: n_merges         = 64784\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 1048576\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 64\r\nllm_load_print_meta: n_head           = 0\r\nllm_load_print_meta: n_head_kv        = 0\r\nllm_load_print_meta: n_rot            = 0\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 0\r\nllm_load_print_meta: n_embd_head_v    = 0\r\nllm_load_print_meta: n_gqa            = 0\r\nllm_load_print_meta: n_embd_k_gqa     = 0\r\nllm_load_print_meta: n_embd_v_gqa     = 0\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 0\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = -1\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 1048576\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 4\r\nllm_load_print_meta: ssm_d_inner      = 8192\r\nllm_load_print_meta: ssm_d_state      = 16\r\nllm_load_print_meta: ssm_dt_rank      = 256\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 1\r\nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model ftype      = Q4_K - Small\r\nllm_load_print_meta: model params     = 7.27 B\r\nllm_load_print_meta: model size       = 3.91 GiB (4.62 BPW) \r\nllm_load_print_meta: general.name     = n/a\r\nllm_load_print_meta: BOS token        = 0 '>>TITLE<<'\r\nllm_load_print_meta: EOS token        = 11 '<|endoftext|>'\r\nllm_load_print_meta: EOT token        = 11 '<|endoftext|>'\r\nllm_load_print_meta: PAD token        = 11 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 138 '\u00c4'\r\nllm_load_print_meta: EOG token        = 11 '<|endoftext|>'\r\nllm_load_print_meta: max token length = 130\r\nSegmentation fault (core dumped)\r\n```\r\nThis is where the segmentation fault happend:\r\n```console\r\n(gdb) r\r\nStarting program: /home/danbev/work/ai/llama.cpp/llama-cli -m models/falcon-mamba-7b-Q4_K_S.gguf -ngl 33 --no-warmup --prompt \\\"What\\ is\\ LoRA\\?\\\" -n 10\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n[New Thread 0x7fffcee00000 (LWP 3344514)]\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 4070, compute capability 8.9, VMM: yes\r\nregister_backend: registered backend CUDA (1 devices)\r\nregister_device: registered device CUDA0 (NVIDIA GeForce RTX 4070)\r\nregister_backend: registered backend CPU (1 devices)\r\nregister_device: registered device CPU (12th Gen Intel(R) Core(TM) i7-1260P)\r\n[New Thread 0x7fffcd200000 (LWP 3344521)]\r\nbuild: 3997 (dea5e860) with cc (Ubuntu 13.2.0-23ubuntu4) 13.2.0 for x86_64-linux-gnu (debug)\r\nmain: llama backend init\r\nmain: load the model and apply lora adapter, if any\r\n[New Thread 0x7fffc1400000 (LWP 3344522)]\r\n[New Thread 0x7fffc0a00000 (LWP 3344523)]\r\nllama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 4070) - 11743 MiB free\r\nllama_model_loader: loaded meta data with 34 key-value pairs and 643 tensors from /home/danbev/work/ai/learning-ai/fundamentals/llama.cpp/models/falcon-mamba-7b-Q4_K_S.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = mamba\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                           general.basename str              = falcon-mamba\r\nllama_model_loader: - kv   3:                         general.size_label str              = 7B\r\nllama_model_loader: - kv   4:                            general.license str              = other\r\nllama_model_loader: - kv   5:                       general.license.name str              = falcon-mamba-7b-license\r\nllama_model_loader: - kv   6:                       general.license.link str              = https://falconllm.tii.ae/falcon-mamba...\r\nllama_model_loader: - kv   7:                          general.languages arr[str,1]       = [\"en\"]\r\nllama_model_loader: - kv   8:                           general.datasets arr[str,2]       = [\"tiiuae/falcon-refinedweb\", \"Hugging...\r\nllama_model_loader: - kv   9:                       mamba.context_length u32              = 1048576\r\nllama_model_loader: - kv  10:                     mamba.embedding_length u32              = 4096\r\nllama_model_loader: - kv  11:                  mamba.feed_forward_length u32              = 0\r\nllama_model_loader: - kv  12:                 mamba.attention.head_count u32              = 0\r\nllama_model_loader: - kv  13:                          mamba.block_count u32              = 64\r\nllama_model_loader: - kv  14:                      mamba.ssm.conv_kernel u32              = 4\r\nllama_model_loader: - kv  15:                       mamba.ssm.inner_size u32              = 8192\r\nllama_model_loader: - kv  16:                       mamba.ssm.state_size u32              = 16\r\nllama_model_loader: - kv  17:                   mamba.ssm.time_step_rank u32              = 256\r\nllama_model_loader: - kv  18:     mamba.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  19:                       mamba.ssm.dt_b_c_rms bool             = true\r\nllama_model_loader: - kv  20:                          general.file_type u32              = 14\r\nllama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = falcon\r\nllama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,65024]   = [\">>TITLE<<\", \">>ABSTRACT<<\", \">>INTR...\r\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,65024]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\r\nllama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,64784]   = [\"\u0120 t\", \"\u0120 a\", \"i n\", \"h e\", \"r e\",...\r\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 11\r\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 0\r\nllama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 11\r\nllama_model_loader: - kv  29:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  30:                      quantize.imatrix.file str              = /models_out/falcon-mamba-7b-GGUF/falc...\r\nllama_model_loader: - kv  31:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\r\nllama_model_loader: - kv  32:             quantize.imatrix.entries_count i32              = 256\r\nllama_model_loader: - kv  33:              quantize.imatrix.chunks_count i32              = 139\r\nllama_model_loader: - type  f32:  385 tensors\r\nllama_model_loader: - type q4_K:  257 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special tokens cache size = 12\r\nllm_load_vocab: token to piece cache size = 0.3884 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = mamba\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 65024\r\nllm_load_print_meta: n_merges         = 64784\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 1048576\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 64\r\nllm_load_print_meta: n_head           = 0\r\nllm_load_print_meta: n_head_kv        = 0\r\nllm_load_print_meta: n_rot            = 0\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 0\r\nllm_load_print_meta: n_embd_head_v    = 0\r\nllm_load_print_meta: n_gqa            = 0\r\nllm_load_print_meta: n_embd_k_gqa     = 0\r\nllm_load_print_meta: n_embd_v_gqa     = 0\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 0\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = -1\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 1048576\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 4\r\nllm_load_print_meta: ssm_d_inner      = 8192\r\nllm_load_print_meta: ssm_d_state      = 16\r\nllm_load_print_meta: ssm_dt_rank      = 256\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 1\r\nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model ftype      = Q4_K - Small\r\nllm_load_print_meta: model params     = 7.27 B\r\nllm_load_print_meta: model size       = 3.91 GiB (4.62 BPW) \r\nllm_load_print_meta: general.name     = n/a\r\nllm_load_print_meta: BOS token        = 0 '>>TITLE<<'\r\nllm_load_print_meta: EOS token        = 11 '<|endoftext|>'\r\nllm_load_print_meta: EOT token        = 11 '<|endoftext|>'\r\nllm_load_print_meta: PAD token        = 11 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 138 '\u00c4'\r\nllm_load_print_meta: EOG token        = 11 '<|endoftext|>'\r\nllm_load_print_meta: max token length = 130\r\n\r\nThread 1 \"llama-cli\" received signal SIGSEGV, Segmentation fault.\r\n0x00005555558a8af7 in ggml_is_3d (tensor=0x0) at ggml/src/ggml.c:3556\r\n3556\t    return tensor->ne[3] == 1;\r\n(gdb) bt\r\n#0  0x00005555558a8af7 in ggml_is_3d (tensor=0x0) at ggml/src/ggml.c:3556\r\n#1  0x00005555558b2d9f in ggml_ssm_conv (ctx=0x555565272f48 <g_state+200>, sx=0x0, c=0x555566f17b00) at ggml/src/ggml.c:7266\r\n#2  0x0000555555957a9a in weight_buft_supported (hparams=..., w=0x555566f17b00, op=GGML_OP_SSM_CONV, \r\n    buft=0x5555651ea020 <ggml_backend_cuda_host_buffer_type::ggml_backend_cuda_buffer_type_host>, dev=0x555565f50490)\r\n    at src/llama.cpp:7166\r\n#3  0x0000555555957da4 in select_weight_buft (model=..., tensor=0x555566f17b00, op=GGML_OP_SSM_CONV, \r\n    buft_list=std::vector of length 2, capacity 2 = {...}) at src/llama.cpp:7200\r\n#4  0x0000555555958baa in operator() (__closure=0x7fffffffb610, tn=..., ne=std::initializer_list of length 2 = {...}, \r\n    flags=0) at src/llama.cpp:7485\r\n#5  0x0000555555969277 in llm_load_tensors (ml=..., model=..., n_gpu_layers=33, split_mode=LLAMA_SPLIT_MODE_LAYER, \r\n    main_gpu=0, tensor_split=0x7fffffffc7b4, use_mlock=false, progress_callback=0x5555559872a9 <_FUN(float, void*)>, \r\n    progress_callback_user_data=0x7fffffffb8e0) at src/llama.cpp:8435\r\n#6  0x00005555559754ad in llama_model_load (\r\n    fname=\"/home/danbev/work/ai/learning-ai/fundamentals/llama.cpp/models/falcon-mamba-7b-Q4_K_S.gguf\", model=..., \r\n    params=...) at src/llama.cpp:9235\r\n#7  0x000055555598795c in llama_load_model_from_file (\r\n    path_model=0x555565f5b3a0 \"/home/danbev/work/ai/learning-ai/fundamentals/llama.cpp/models/falcon-mamba-7b-Q4_K_S.gguf\", \r\n    params=...) at src/llama.cpp:19358\r\n#8  0x0000555555b04fe6 in common_init_from_params (params=...) at common/common.cpp:836\r\n#9  0x0000555555bd1b09 in main (argc=10, argv=0x7fffffffdab8) at examples/main/main.cpp:200\r\n(gdb) up\r\n(gdb) up\r\n(gdb) p w->name\r\n$1 = \"blk.0.ssm_conv1d.weight\", '\\000' <repeats 40 times>\r\n```\r\nThis call is coming from `weight_buft_supported`:\r\n```c++\r\nstatic bool weight_buft_supported(const llama_hparams & hparams, ggml_tensor * w, ggml_op op, ggml_backend_buffer_type_t buft, ggml_backend_dev_t dev) {\r\n...\r\nswitch (op) { \r\n    ...\r\ncase GGML_OP_SSM_CONV:                                                  \r\n            {                                                                   \r\n                // TODO: ggml_ssm_conv(ctx, conv_x, model.layers[il].ssm_conv1d);\r\n                op_tensor = ggml_ssm_conv(ctx, nullptr, w);                     \r\n            } break;               \r\n}\r\n```\r\nSeeing the `TODO` here this might be expected but I wanted to raise this just in case. \n\n### Name and Version\n\n```console\r\n$ ./llama-cli --version\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 4070, compute capability 8.9, VMM: yes\r\nregister_backend: registered backend CUDA (1 devices)\r\nregister_device: registered device CUDA0 (NVIDIA GeForce RTX 4070)\r\nregister_backend: registered backend CPU (1 devices)\r\nregister_device: registered device CPU (12th Gen Intel(R) Core(TM) i7-1260P)\r\nversion: 3997 (dea5e860)\r\nbuilt with cc (Ubuntu 13.2.0-23ubuntu4) 13.2.0 for x86_64-linux-gnu\r\n```\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-10-31T13:13:43+00:00",
    "closed_at": "2024-10-31T21:54:24+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10109/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10109"
  },
  {
    "number": 8161,
    "title": "Bug: llama.cpp binaries are compiled dynamically and the library is missing!",
    "body": "### What happened?\n\n$ ./build/bin/llama-quantize -h\r\n./build/bin/llama-quantize: error while loading shared libraries: libllama.so: cannot open shared object file: No such file or directory\n\n### Name and Version\n\nlatest\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n```shell\nsee up.\n```\n",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-06-27T11:06:26+00:00",
    "closed_at": "2024-06-28T10:49:18+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8161/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8161"
  },
  {
    "number": 10184,
    "title": "Bug: SYCL crash",
    "body": "### What happened?\n\n\r\nthis is from b4020, as you can see from the task it took a while to occur.  earlier builds this didn't happen.\r\n```\r\nslot launch_slot_: id 38 | task 496680 | processing task\r\nslot launch_slot_: id  2 | task 496681 | processing task\r\nslot update_slots: id  2 | task 496681 | new prompt, n_ctx_slot = 2333, n_keep = 0, n_prompt_tokens = 683\r\nslot update_slots: id  2 | task 496681 | kv cache rm [0, end)\r\nslot update_slots: id  2 | task 496681 | prompt processing progress, n_past = 683, n_tokens = 689, progress = 1.000000\r\nslot update_slots: id  2 | task 496681 | prompt done, n_past = 683, n_tokens = 689\r\nslot update_slots: id 38 | task 496680 | new prompt, n_ctx_slot = 2333, n_keep = 0, n_prompt_tokens = 339\r\nslot update_slots: id 38 | task 496680 | kv cache rm [0, end)\r\nslot update_slots: id 38 | task 496680 | prompt processing progress, n_past = 339, n_tokens = 1028, progress = 1.000000\r\nslot update_slots: id 38 | task 496680 | prompt done, n_past = 339, n_tokens = 1028\r\nNative API failed. Native API returns: -999 (Unknown PI error) -999 (Unknown PI error)\r\nException caught at file:D:/a/llama.cpp/llama.cpp/ggml/src/ggml-sycl.cpp, line:4221, func:operator()\r\nSYCL error: CHECK_TRY_ERROR(dpct::gemm_batch( *main_stream, oneapi::mkl::transpose::trans, oneapi::mkl::transpose::nontrans, ne01, ne11, ne10, alpha, (const void **)(ptrs_src.get() + 0 * ne23), dpct::library_data_t::real_half, nb01 / nb00, (const void **)(ptrs_src.get() + 1 * ne23), dpct::library_data_t::real_half, nb11 / nb10, beta, (void **)(ptrs_dst.get() + 0 * ne23), cu_data_type, ne01, ne23, cu_compute_type)): Meet error in this line code!\r\n  in function ggml_sycl_mul_mat_batched_sycl at D:/a/llama.cpp/llama.cpp/ggml/src/ggml-sycl.cpp:4221\r\nD:\\a\\llama.cpp\\llama.cpp\\ggml\\src\\ggml-sycl\\common.hpp:107: SYCL error\r\n```\r\n\r\nsame error occurs in b4033\n\n### Name and Version\n\n.\\llama-server.exe --version\r\nZE_LOADER_DEBUG_TRACE:Using Loader Library Path:\r\nZE_LOADER_DEBUG_TRACE:Tracing Layer Library Path: ze_tracing_layer.dll\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\nversion: 4033 (a9e8a9a0)\r\nbuilt with MSVC 19.41.34123.0 for\n\n### What operating system are you seeing the problem on?\n\nWindows\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-11-05T12:04:04+00:00",
    "closed_at": "2024-11-20T04:32:09+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10184/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10184"
  },
  {
    "number": 9949,
    "title": "Bug: Segmentation fault when running speculative decoding",
    "body": "### What happened?\n\nRunning speculative decoding with the new Llama-3.1-405B-Instruct, with Llama-3.1-8B-Instruct as a draft model (with the large model on CPU and the small one on GPU), results in a segfault and core dump. (I don't think it's simply an out-of-memory error; 405B runs OK by itself with `llama-server`, albeit slowly.)\r\n\r\nCommand used: ./build/bin/llama-speculative -m ~/.cache/huggingface/hub/models--ThomasBaruzier--Meta-Llama-3.1-405B-Instruct-GGUF/snapshots/8545acf6b66386cbe0c37a7a099d634531c62a1c/Meta-Llama-3.1-405B-Instruct-IQ3_XXS/Meta-Llama-3.1-405B-Instruct-IQ3_XXS-00001-of-00004.gguf -fa -ngl 0 -ctk q4_0 -ctv q4_0 -co -md ~/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/9a8dec50f04fa8fad1dc1e7bc20a84a512e2bb01/Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf -ngld 33\r\n\r\n\n\n### Name and Version\n\n(llama) alyssa@alyssa-desktop:~/lm_fun/llama.cpp$ ./build/bin/llama-cli --version\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 4070 Ti SUPER, compute capability 8.9, VMM: yes\r\nversion: 3943 (cda0e4b6)\r\nbuilt with cc (Ubuntu 13.2.0-23ubuntu4) 13.2.0 for x86_64-linux-gnu\r\n\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\n(llama) alyssa@alyssa-desktop:~/lm_fun/llama.cpp$ ./build/bin/llama-speculative -m ~/.cache/huggingface/hub/models--ThomasBaruzier--Meta-Llama-3.1-405B-Instruct-GGUF/snapshots/8545acf6b66386cbe0c37a7a099d634531c62a1c/Meta-Llama-3.1-405B-Instruct-IQ3_XXS/Meta-Llama-3.1-405B-Instruct-IQ3_XXS-00001-of-00004.gguf -fa -ngl 0 -ctk q4_0 -ctv q4_0 -co -md ~/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/9a8dec50f04fa8fad1dc1e7bc20a84a512e2bb01/Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf -ngld 33\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 4070 Ti SUPER, compute capability 8.9, VMM: yes\r\nbuild: 3943 (cda0e4b6) with cc (Ubuntu 13.2.0-23ubuntu4) 13.2.0 for x86_64-linux-gnu\r\nllama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 4070 Ti SUPER) - 15381 MiB free\r\nllama_model_loader: additional 3 GGUFs metadata loaded.\r\nllama_model_loader: loaded meta data with 36 key-value pairs and 1138 tensors from /home/alyssa/.cache/huggingface/hub/models--ThomasBaruzier--Meta-Llama-3.1-405B-Instruct-GGUF/snapshots/8545acf6b66386cbe0c37a7a099d634531c62a1c/Meta-Llama-3.1-405B-Instruct-IQ3_XXS/Meta-Llama-3.1-405B-Instruct-IQ3_XXS-00001-of-00004.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = .\r\nllama_model_loader: - kv   3:                           general.finetune str              = .\r\nllama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\r\nllama_model_loader: - kv   5:                         general.size_label str              = 405B\r\nllama_model_loader: - kv   6:                            general.license str              = llama3.1\r\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\r\nllama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\r\nllama_model_loader: - kv   9:                          llama.block_count u32              = 126\r\nllama_model_loader: - kv  10:                       llama.context_length u32              = 131072\r\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 16384\r\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 53248\r\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 128\r\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\r\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  17:                          general.file_type u32              = 23\r\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\r\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\r\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"\u0120 \u0120\", \"\u0120 \u0120\u0120\u0120\", \"\u0120\u0120 \u0120\u0120\", \"...\r\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\r\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\r\nllama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\r\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  29:                      quantize.imatrix.file str              = gguf/Meta-Llama-3.1-405B-Instruct/ima...\r\nllama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = misc/calibration_datav3.txt\r\nllama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 882\r\nllama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\r\nllama_model_loader: - kv  33:                                   split.no u16              = 0\r\nllama_model_loader: - kv  34:                                split.count u16              = 4\r\nllama_model_loader: - kv  35:                        split.tensors.count i32              = 1138\r\nllama_model_loader: - type  f32:  254 tensors\r\nllama_model_loader: - type q4_K:  126 tensors\r\nllama_model_loader: - type q5_K:    1 tensors\r\nllama_model_loader: - type iq3_xxs:  378 tensors\r\nllama_model_loader: - type iq3_s:  127 tensors\r\nllama_model_loader: - type iq2_s:  252 tensors\r\nllm_load_vocab: special tokens cache size = 256\r\nllm_load_vocab: token to piece cache size = 0.7999 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 128256\r\nllm_load_print_meta: n_merges         = 280147\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 16384\r\nllm_load_print_meta: n_layer          = 126\r\nllm_load_print_meta: n_head           = 128\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 16\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 53248\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 500000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model ftype      = IQ3_XXS - 3.0625 bpw\r\nllm_load_print_meta: model params     = 405.85 B\r\nllm_load_print_meta: model size       = 145.14 GiB (3.07 BPW) \r\nllm_load_print_meta: general.name     = .\r\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\r\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\r\nllm_load_print_meta: LF token         = 128 '\u00c4'\r\nllm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\r\nllm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: max token length = 256\r\nllm_load_tensors: ggml ctx size =    0.53 MiB\r\nllm_load_tensors: offloading 0 repeating layers to GPU\r\nllm_load_tensors: offloaded 0/127 layers to GPU\r\nllm_load_tensors:        CPU buffer size = 45213.72 MiB\r\nllm_load_tensors:        CPU buffer size = 45425.75 MiB\r\nllm_load_tensors:        CPU buffer size = 45190.75 MiB\r\nllm_load_tensors:        CPU buffer size = 12789.19 MiB\r\n....................................................................................................\r\nllama_new_context_with_model: n_ctx      = 131072\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 1\r\nllama_new_context_with_model: freq_base  = 500000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:  CUDA_Host KV buffer size = 18144.00 MiB\r\nllama_new_context_with_model: KV self size  = 18144.00 MiB, K (q4_0): 9072.00 MiB, V (q4_0): 9072.00 MiB\r\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =  1660.25 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =   288.01 MiB\r\nllama_new_context_with_model: graph nodes  = 3535\r\nllama_new_context_with_model: graph splits = 1642\r\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\nllama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 4070 Ti SUPER) - 13671 MiB free\r\nllama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from /home/alyssa/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/9a8dec50f04fa8fad1dc1e7bc20a84a512e2bb01/Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\r\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\r\nllama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\r\nllama_model_loader: - kv   5:                         general.size_label str              = 8B\r\nllama_model_loader: - kv   6:                            general.license str              = llama3.1\r\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\r\nllama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\r\nllama_model_loader: - kv   9:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv  10:                       llama.context_length u32              = 131072\r\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\r\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  17:                          general.file_type u32              = 18\r\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\r\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\r\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"\u0120 \u0120\", \"\u0120 \u0120\u0120\u0120\", \"\u0120\u0120 \u0120\u0120\", \"...\r\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\r\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\r\nllama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\r\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\r\nllama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\r\nllama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\r\nllama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\r\nllama_model_loader: - type  f32:   66 tensors\r\nllama_model_loader: - type q8_0:    2 tensors\r\nllama_model_loader: - type q6_K:  224 tensors\r\nllm_load_vocab: special tokens cache size = 256\r\nllm_load_vocab: token to piece cache size = 0.7999 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 128256\r\nllm_load_print_meta: n_merges         = 280147\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 500000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 8B\r\nllm_load_print_meta: model ftype      = Q6_K\r\nllm_load_print_meta: model params     = 8.03 B\r\nllm_load_print_meta: model size       = 6.37 GiB (6.82 BPW) \r\nllm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\r\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\r\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\r\nllm_load_print_meta: LF token         = 128 '\u00c4'\r\nllm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\r\nllm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: max token length = 256\r\nllm_load_tensors: ggml ctx size =    0.27 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:        CPU buffer size =   532.31 MiB\r\nllm_load_tensors:      CUDA0 buffer size =  5993.34 MiB\r\n......................................................................................\r\nllama_new_context_with_model: n_ctx      = 131072\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 1\r\nllama_new_context_with_model: freq_base  = 500000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:      CUDA0 KV buffer size =  4608.00 MiB\r\nllama_new_context_with_model: KV self size  = 4608.00 MiB, K (q4_0): 2304.00 MiB, V (q4_0): 2304.00 MiB\r\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =   416.00 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =   264.01 MiB\r\nllama_new_context_with_model: graph nodes  = 903\r\nllama_new_context_with_model: graph splits = 2\r\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\n\r\n\r\n<|begin_of_text|>\r\n\r\nSegmentation fault (core dumped)\n```\n",
    "labels": [
      "bug",
      "critical severity"
    ],
    "state": "open",
    "created_at": "2024-10-19T04:03:33+00:00",
    "closed_at": null,
    "comments": 16,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9949/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9949"
  },
  {
    "number": 10321,
    "title": "Bug: llama-gbnf-validator parses grammar but gets a seg fault when validating an input string against the grammar",
    "body": "### What happened?\n\nI ran` ./llama-gbnf-validator mygrammar.txt mytestprogram.txt `and after checking the grammar itself, it started to parse the test file and it went into an infinite loop calling static void `llama_grammar_advance_stack()` and eventually blew up in `tiny_malloc_from_free_list()`\r\n\r\n[mygrammar.txt](https://github.com/user-attachments/files/17780318/mygrammar.txt)\r\n[mytestprogram.txt](https://github.com/user-attachments/files/17780319/mytestprogram.txt)\r\n[llama-grammar.cpp.txt](https://github.com/user-attachments/files/17780317/llama-grammar.cpp.txt)\r\n\r\nI modified llama-grammar.cpp to add some console debug statements, so the line numbers in the stack trace may be off a bit from the version I used.  See the attached file llama-grammar.cpp.txt for my minor changes.\r\n\r\nI found numerous bugs and problems with the validator, including these:\r\n\r\n1. The infinite loop noted above for the grammar and test file provided above. This is the most serious.\r\n2. If I use the construct `nts?` or `nts*` or `nts+` on the rhs of a rule in the grammar, where `nts` is a defined non-terminal symbol defined elsewhere, I get the error \"Undefined rule\" with no indication of what the rule is, or how or why it is was created as undefined.  To fix it, I have to parenthesize the nts, e.g., `(nts)?`, on the right hand side of the rule being defined.  Nowhere is it documented that `nts?` is not valid gbnf, and if it is, the validator should complain, rather than just producing an invalid grammar representation.\r\n3. The documentation for gbnf states that non-terminal symbols may consist of lower case letters and \"-\", e.g., \"if-statement\".  If I use a camelCase nts, e.g., \"ifStatement\", it is accepted by the validator but the test file parser does not work properly (at least in some cases).  So is a camelCase nts allowed?  If it isn't, the validator should complain and state the bad nts it sees, rather than just producing an invalid grammar representation.\r\n4. The gbnf grammar seems to require that I sprinkle `ws` (whitespace) non-terminal symbols in seemingly random places throughout the grammar rules because there is apparently no notion of a lexical analyzer.  This requires a very tedious trial-and-error process, because if I put unnecessary `ws` tokens in a rule, it prevents the rule from firing, and if I leave a necessary one out, it also prevents the rule from firing! If you cannot make it work like other bnf grammars, please at least document when one must add `ws` and when must not.\n\n### Name and Version\n\n% ./llama-cli --version\r\nversion: 4075 (fb4a0ec0)\r\nbuilt with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin23.6.0\n\n### What operating system are you seeing the problem on?\n\nMac\n\n### Relevant log output\n\n```shell\ntiny_malloc_from_free_list 0x00000001838043f0\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:729\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\n................ 65,000 lines removed ...............\r\nllama_grammar_advance_stack(const std::vector<\u2026> &, const std::vector<\u2026> &, std::vector<\u2026> &) llama-grammar.cpp:731\r\nllama_grammar_accept(const std::vector<\u2026> &, const std::vector<\u2026> &, unsigned int, std::vector<\u2026> &) llama-grammar.cpp:864\r\n[Inlined] llama_grammar_validate(llama_grammar *, const std::string &, unsigned long &, std::string &) gbnf-validator.cpp:21\r\nmain gbnf-validator.cpp:101\n```\n",
    "labels": [
      "bug",
      "critical severity"
    ],
    "state": "open",
    "created_at": "2024-11-15T20:27:54+00:00",
    "closed_at": null,
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10321/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10321"
  },
  {
    "number": 8562,
    "title": "Bug: Vulkan build no longer working with MSVC cmake on windows",
    "body": "### What happened?\r\n\r\nWhen trying to build the lastest version of the Vulkan backend, the shader compilation fails.\r\n\r\nI suspect commit 17eb6aa8a992cda37ee65cf848d9289bd6cad860 to have introduced the issue, but more testing is required to know for sure.\r\n\r\n### Name and Version\r\n\r\ncommit: 3807c3de04cde853418033c95e96642876545f3e\r\ncmake flags:  `-DBUILD_SHARED_LIBS=OFF -DGGML_VULKAN=1 -G \"Visual Studio 17 2022\" -A x64`\r\nMSBuild version: `17.9.8+b34f75857`\r\nVulkan Instance Version: `1.3.261`\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nWindows\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nVersion MSBuild 17.9.8+b34f75857 pour .NET Framework\r\n\r\n[...]\r\n\r\nGenerate vulkan shaders\r\n  ggml_vulkan: Generating and compiling shaders to SPIR-V\r\n  cannot compile mul_mat_vec_id_f32_f32\r\n\r\n  C:/VulkanSDK/1.3.275.0/Bin/glslc.exe -fshader-stage=compute --target-env=vulkan1.2 -O C:/llama cpp/ggml/src/vulkan-shaders\\mul_mat_vec.comp -o C:/llama cpp/build/ggml/src/vulkan-shaders.spv\\mul_mat_vec_id\r\n  _f32_f32.spv -DB_TYPE=float -DDATA_A_F32=1 -DD_TYPE=float -DFLOAT_TYPE=float -DMUL_MAT_ID=1\r\n\r\nglslc : error : linking multiple files is not supported yet. Use -c to compile files individually. [C:\\llama cpp\\build\\ggml\\src\\ggml.vcxproj]\r\n\r\n\r\n  cannot compile cannot compile mul_mat_vec_q4_0_f32_f32mul_mat_vec_q4_0_f16_f32\r\n\r\n  C:/VulkanSDK/1.3.275.0/Bin/glslc.exe -fshader-stage=compute --target-env=vulkan1.2 -O C:/llama cpp/ggml/src/vulkan-shaders\\mul_mat_vec.comp -o C:/llama cpp/build/ggml/src/vulkan-shaders.spv\\mul_mat_vec_q4\r\n  _0_f32_f32.spv -DB_TYPE=float -DDATA_A_Q4_0=1 -DD_TYPE=float -DFLOAT_TYPE=float\r\n\r\n\r\n\r\nglslc : error : linking multiple files is not supported yet. Use -c to compile files individually. [C:\\llama cpp\\build\\ggml\\src\\ggml.vcxproj]\r\n\r\n(this goes on for every op, and build crashes with code -1)\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-07-18T10:23:22+00:00",
    "closed_at": "2024-08-05T06:18:28+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8562/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8562"
  },
  {
    "number": 9432,
    "title": "Bug: ggml_conv_2d doesn't work on Metal backend",
    "body": "### What happened?\r\n\r\nggml_conv_2d give all nan result on Metal backend:\r\n```\r\nFirst 40 elements:\r\nnan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, \r\nsum:  -280695959.659959\r\n```\r\n\r\nbut on cpu backend, give the right result:\r\n```\r\nFirst 40 elements:\r\n-0.0540, 0.5452, -0.1388, 0.1618, 0.1068, -0.0747, -0.0549, -0.2022, -0.0289, -0.1387, -0.2103, 0.3136, 0.1272, -0.2936, -0.1544, -0.0982, -0.3678, 0.0272, 0.0846, -0.0365, -0.1896, -0.0318, -0.1410, -0.0834, -0.1187, -0.2195, -0.2144, -0.0080, -0.0205, 0.1188, -0.1191, -0.3063, 0.0592, -0.1025, -0.0370, -0.0984, -0.3389, -0.0576, -0.1382, -0.1135, \r\nsum:  -539629.844900\r\n```\r\n\r\n### Name and Version\r\n\r\n./llama-cli --version\r\nversion: 3671 (9b80d489)\r\nbuilt with Apple clang version 15.0.0 (clang-1500.3.9.4) for arm64-apple-darwin23.6.0\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nMac\r\n\r\n### Relevant log output\r\n\r\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-09-11T10:42:15+00:00",
    "closed_at": "2024-10-27T01:09:51+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9432/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9432"
  },
  {
    "number": 9157,
    "title": "Bug: Mac build failed using make",
    "body": "### What happened?\n\nmy command:\r\n```bash\r\n(python39) marcus@Marcuss-MacBook-Air llama.cpp % make\r\n```\r\n\n\n### Name and Version\n\n1. Mac M2 24GB\r\n2. llama.cpp commit hash 8f824ffe8ee1feadd14428f1dda1283fa3b933be\n\n### What operating system are you seeing the problem on?\n\nMac\n\n### Relevant log output\n\n```shell\nI ccache not found. Consider installing it for faster compilation.\r\nI llama.cpp build info: \r\nI UNAME_S:   Darwin\r\nI UNAME_P:   arm\r\nI UNAME_M:   arm64\r\nI CFLAGS:    -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DGGML_USE_BLAS -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL -DGGML_METAL_EMBED_LIBRARY  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion \r\nI CXXFLAGS:  -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread   -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DGGML_USE_BLAS -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL -DGGML_METAL_EMBED_LIBRARY \r\nI NVCCFLAGS: -std=c++11 -O3 -g \r\nI LDFLAGS:   -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \r\nI CC:        Apple clang version 14.0.3 (clang-1403.0.22.14.1)\r\nI CXX:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)\r\n\r\nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread   -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DGGML_USE_BLAS -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL -DGGML_METAL_EMBED_LIBRARY  -c examples/baby-llama/baby-llama.cpp -o examples/baby-llama/baby-llama.o\r\nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread   -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DGGML_USE_BLAS -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL -DGGML_METAL_EMBED_LIBRARY  ggml/src/ggml-blas.o ggml/src/llamafile/sgemm.o ggml/src/ggml-metal.o ggml/src/ggml-metal-embed.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o ggml/src/ggml-aarch64.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/baby-llama/baby-llama.o -o llama-baby-llama -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \r\nUndefined symbols for architecture arm64:\r\n  \"_ggml_print_backtrace\", referenced from:\r\n      llama_ngram_cache_save(std::__1::unordered_map<llama_ngram, std::__1::unordered_map<int, int, std::__1::hash<int>, std::__1::equal_to<int>, std::__1::allocator<std::__1::pair<int const, int>>>, llama_ngram_hash_function, std::__1::equal_to<llama_ngram>, std::__1::allocator<std::__1::pair<llama_ngram const, std::__1::unordered_map<int, int, std::__1::hash<int>, std::__1::equal_to<int>, std::__1::allocator<std::__1::pair<int const, int>>>>>>&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>&) in ngram-cache.o\r\n      llama_ngram_cache_load(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>&) in ngram-cache.o\r\n      llama_ngram_cache_merge(std::__1::unordered_map<llama_ngram, std::__1::unordered_map<int, int, std::__1::hash<int>, std::__1::equal_to<int>, std::__1::allocator<std::__1::pair<int const, int>>>, llama_ngram_hash_function, std::__1::equal_to<llama_ngram>, std::__1::allocator<std::__1::pair<llama_ngram const, std::__1::unordered_map<int, int, std::__1::hash<int>, std::__1::equal_to<int>, std::__1::allocator<std::__1::pair<int const, int>>>>>>&, std::__1::unordered_map<llama_ngram, std::__1::unordered_map<int, int, std::__1::hash<int>, std::__1::equal_to<int>, std::__1::allocator<std::__1::pair<int const, int>>>, llama_ngram_hash_function, std::__1::equal_to<llama_ngram>, std::__1::allocator<std::__1::pair<llama_ngram const, std::__1::unordered_map<int, int, std::__1::hash<int>, std::__1::equal_to<int>, std::__1::allocator<std::__1::pair<int const, int>>>>>>&) in ngram-cache.o\r\n      llama_ngram_cache_draft(std::__1::vector<int, std::__1::allocator<int>>&, std::__1::vector<int, std::__1::allocator<int>>&, int, int, int, std::__1::unordered_map<llama_ngram, std::__1::unordered_map<int, int, std::__1::hash<int>, std::__1::equal_to<int>, std::__1::allocator<std::__1::pair<int const, int>>>, llama_ngram_hash_function, std::__1::equal_to<llama_ngram>, std::__1::allocator<std::__1::pair<llama_ngram const, std::__1::unordered_map<int, int, std::__1::hash<int>, std::__1::equal_to<int>, std::__1::allocator<std::__1::pair<int const, int>>>>>>&, std::__1::unordered_map<llama_ngram, std::__1::unordered_map<int, int, std::__1::hash<int>, std::__1::equal_to<int>, std::__1::allocator<std::__1::pair<int const, int>>>, llama_ngram_hash_function, std::__1::equal_to<llama_ngram>, std::__1::allocator<std::__1::pair<llama_ngram const, std::__1::unordered_map<int, int, std::__1::hash<int>, std::__1::equal_to<int>, std::__1::allocator<std::__1::pair<int const, int>>>>>>&, std::__1::unordered_map<llama_ngram, std::__1::unordered_map<int, int, std::__1::hash<int>, std::__1::equal_to<int>, std::__1::allocator<std::__1::pair<int const, int>>>, llama_ngram_hash_function, std::__1::equal_to<llama_ngram>, std::__1::allocator<std::__1::pair<llama_ngram const, std::__1::unordered_map<int, int, std::__1::hash<int>, std::__1::equal_to<int>, std::__1::allocator<std::__1::pair<int const, int>>>>>>&) (.cold.1) in ngram-cache.o\r\n      tokenize_file(llama_context*, char const*, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, bool, bool, unsigned int, std::__1::vector<int, std::__1::allocator<int>>&, std::__1::vector<unsigned long, std::__1::allocator<unsigned long>>&, std::__1::vector<unsigned long, std::__1::allocator<unsigned long>>&) in train.o\r\n      assert_shape_1d(ggml_tensor*, long long) (.cold.1) in train.o\r\n      assert_shape_1d(ggml_tensor*, long long) (.cold.2) in train.o\r\n      ...\r\nld: symbol(s) not found for architecture arm64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake: *** [llama-baby-llama] Error 1\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-08-24T15:12:33+00:00",
    "closed_at": "2024-11-06T01:07:16+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9157/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9157"
  },
  {
    "number": 9514,
    "title": "Bug: Crash in Release Mode when built with Xcode 16 (& since Xcode 15.3) ",
    "body": "### What happened?\n\nI have used llama.cpp as a library in an iOS app for nearly a year. I've had to hold back upgrading beyond Xcode 15.2 for a number of months due to build problems, but with the release of Xcode 16 [today](https://developer.apple.com/download/all/), I figured it'd be worth reporting a crash I'm seeing.\r\n\r\nIt occurs _only in release mode_, upon dequantizing weights during attempted inference, using iOS 17 on device or simulators on iOS 17 & iOS 18 (haven't installed iOS 18 on a real device to test just yet!). I have also not been able to test on macOS Sequoia to rule out whether this behavior applies there as well.\r\n\r\nThe same project works perfectly when building with Xcode 15.2, so it's possible this is due to either build configuration problems, or toolchain changes made upstream between Xcode 15.2 & Xcode 16. I wouldn't expect anyone to test with pre-release software over the summer, but as of today iOS 18 & Xcode 16 are the official latest releases.\r\n\r\nShould I expect builds with Xcode 16/targeting iOS 18 to be supported yet?\r\n\r\nI can work on a simple reproduction with `LibLlama` (I don't use this interface in my app, so I don't have any ready-to adapt code) but wanted to get feedback in the meantime in case there's either been previous discussion to this effect that I've missed, or if there are folks in the community who have experienced & worked around this successfully.\r\n\r\n```\r\nThread 26: EXC_BAD_ACCESS (code=2, address=0x5e72a0a1e0)\r\n```\r\n\r\nin `ggml-quants.c`, `const float d   = GGML_FP16_TO_FP32(x[i].d);`\r\nhttps://github.com/ggerganov/llama.cpp/blob/a47667cff41f5a198eb791974e0afcc1cddd3229/ggml/src/ggml-quants.c#L2555 (same logic on [master](https://github.com/ggerganov/llama.cpp/blob/23e0d70bacaaca1429d365a44aa9e7434f17823b/ggml/src/ggml-quants.c#L2562))\r\n\r\n```\r\nvoid dequantize_row_q4_K(const block_q4_K * restrict x, float * restrict y, int64_t k) {\r\n    assert(k % QK_K == 0);\r\n    const int nb = k / QK_K;\r\n\r\n    for (int i = 0; i < nb; i++) {\r\n        const uint8_t * q = x[i].qs;\r\n\r\n        const float d   = GGML_FP16_TO_FP32(x[i].d);\r\n        const float min = GGML_FP16_TO_FP32(x[i].dmin);\r\n\r\n        int is = 0;\r\n        uint8_t sc, m;\r\n        for (int j = 0; j < QK_K; j += 64) {\r\n            get_scale_min_k4(is + 0, x[i].scales, &sc, &m);\r\n            const float d1 = d * sc; const float m1 = min * m;\r\n            get_scale_min_k4(is + 1, x[i].scales, &sc, &m);\r\n            const float d2 = d * sc; const float m2 = min * m;\r\n            for (int l = 0; l < 32; ++l) *y++ = d1 * (q[l] & 0xF) - m1;\r\n            for (int l = 0; l < 32; ++l) *y++ = d2 * (q[l]  >> 4) - m2;\r\n            q += 32; is += 2;\r\n        }\r\n    }\r\n}\r\n```\r\n\r\ntrying a different model, the same error occurs dequantizing q6_K instead.\n\n### Name and Version\n\nusing llama.cpp as a library, no binary \u2014 currently shipped version that built successfully on 15.2 is `a47667cf` as linked above, v3650:\r\n\r\n```\r\n./llama-cli --version\r\nversion: 3650 (a47667cf)\r\nbuilt with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin23.6.0\r\n```\n\n### What operating system are you seeing the problem on?\n\nOther? (Please let us know in description)\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-09-16T21:42:15+00:00",
    "closed_at": "2024-09-17T09:50:35+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9514/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9514"
  },
  {
    "number": 7799,
    "title": "Bug:  error loading model architecture: unknown model architecture: 'clip'",
    "body": "### What happened?\n\nRunning llava-v1.6 results in the following error:\r\n`error loading model: error loading model architecture: unknown model architecture: 'clip'`\r\n\r\nThe command I ran was:\r\n\r\n`llama --log-enable --model models/llava-v1.6-mistral-7b.Q5_K_M.mmproj-f16.gguf --mmproj models/llava-v1.6-mistral-7b.Q5_K_M.mmproj-f16.gguf --image media/llama0-banner.png -p \"what is in this image?\"`\r\n\r\nI had no issues running ShareGPT4V\r\n` llama --log-enable --model models/ShareGPT4V-f16.gguf --mmproj models/ShareGPT4V-f16-mmproj.gguf --image media/llama0-banner.png -p \"what is in this image?\"`\r\n```\r\ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\r\n\r\n\r\n what is in this image?\r\n What does it show? [end of text]\r\n```\r\n\r\n\n\n### Name and Version\n\nmain: build = 3089 (c90dbe0)\r\nmain: built with gcc (GCC) 12.3.0 for x86_64-unknown-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\nLog start\r\nmain: build = 3089 (c90dbe0)\r\nmain: built with gcc (GCC) 12.3.0 for x86_64-unknown-linux-gnu\r\nmain: seed  = 1717675001\r\nllama_model_loader: loaded meta data with 25 key-value pairs and 378 tensors from models/llava-v1.6-mistral-7b.Q5_K_M.mmproj-f16.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = clip\r\nllama_model_loader: - kv   1:                      clip.has_text_encoder bool             = false\r\nllama_model_loader: - kv   2:                    clip.has_vision_encoder bool             = true\r\nllama_model_loader: - kv   3:                   clip.has_llava_projector bool             = true\r\nllama_model_loader: - kv   4:                          general.file_type u32              = 1\r\nllama_model_loader: - kv   5:                               general.name str              = vit-large336-custom\r\nllama_model_loader: - kv   6:                        general.description str              = image encoder for LLaVA\r\nllama_model_loader: - kv   7:                        clip.projector_type str              = mlp\r\nllama_model_loader: - kv   8:                     clip.vision.image_size u32              = 336\r\nllama_model_loader: - kv   9:                     clip.vision.patch_size u32              = 14\r\nllama_model_loader: - kv  10:               clip.vision.embedding_length u32              = 1024\r\nllama_model_loader: - kv  11:            clip.vision.feed_forward_length u32              = 4096\r\nllama_model_loader: - kv  12:                 clip.vision.projection_dim u32              = 768\r\nllama_model_loader: - kv  13:           clip.vision.attention.head_count u32              = 16\r\nllama_model_loader: - kv  14:   clip.vision.attention.layer_norm_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  15:                    clip.vision.block_count u32              = 23\r\nllama_model_loader: - kv  16:           clip.vision.image_grid_pinpoints arr[i32,10]      = [336, 672, 672, 336, 672, 672, 1008, ...\r\nllama_model_loader: - kv  17:          clip.vision.image_crop_resolution u32              = 224\r\nllama_model_loader: - kv  18:             clip.vision.image_aspect_ratio str              = anyres\r\nllama_model_loader: - kv  19:         clip.vision.image_split_resolution u32              = 224\r\nllama_model_loader: - kv  20:            clip.vision.mm_patch_merge_type str              = spatial_unpad\r\nllama_model_loader: - kv  21:              clip.vision.mm_projector_type str              = mlp2x_gelu\r\nllama_model_loader: - kv  22:                     clip.vision.image_mean arr[f32,3]       = [0.481455, 0.457828, 0.408211]\r\nllama_model_loader: - kv  23:                      clip.vision.image_std arr[f32,3]       = [0.268630, 0.261303, 0.275777]\r\nllama_model_loader: - kv  24:                              clip.use_gelu bool             = false\r\nllama_model_loader: - type  f32:  236 tensors\r\nllama_model_loader: - type  f16:  142 tensors\r\nllama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'\r\nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params: error: failed to load model 'models/llava-v1.6-mistral-7b.Q5_K_M.mmproj-f16.gguf'\r\nmain: error: unable to load model\n```\n",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-06-06T12:02:26+00:00",
    "closed_at": "2024-06-07T03:00:50+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7799/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7799"
  },
  {
    "number": 8965,
    "title": "Bug: Adreno740 GPU device can't load model in Android system",
    "body": "### What happened?\r\n\r\nI tried to run llama.cpp in Samsug Galaxy Tab S9 Ultra,the Android System is Android13.and I have compiled these libraries accoding the guide.I used these libraries in my APK and when I load model it met a fatal crash.\r\n\r\n\r\n### Name and Version\r\n\r\ntag:3400,commit:97bdd26e,support GPU acceleration:true\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nOther? (Please let us know in description)\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n08-10 16:06:07.269 30852 30926 I LLama-android: build info:tag:3400,commit:97bdd26e,support GPU acceleration:true\r\n08-10 16:06:07.334 30852 30926 I LLama-android: llama_model_loader: loaded meta data with 20 key-value pairs and 290 tensors from /data/user/0/com.set.ai/files/ai_model.gguf (version GGUF V3 (latest))\r\n08-10 16:06:07.334 30852 30926 I LLama-android: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\n08-10 16:06:07.334 30852 30926 I LLama-android: llama_model_loader: - kv   0:                       general.architecture str              = qwen2\r\n08-10 16:06:07.334 30852 30926 I LLama-android: llama_model_loader: - kv   1:                               general.name str              = seres_model\r\n08-10 16:06:07.334 30852 30926 I LLama-android: llama_model_loader: - kv   2:                          qwen2.block_count u32              = 24\r\n08-10 16:06:07.334 30852 30926 I LLama-android: llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\r\n08-10 16:06:07.334 30852 30926 I LLama-android: llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 896\r\n08-10 16:06:07.334 30852 30926 I LLama-android: llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 4864\r\n08-10 16:06:07.334 30852 30926 I LLama-android: llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 14\r\n08-10 16:06:07.334 30852 30926 I LLama-android: llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\r\n08-10 16:06:07.334 30852 30926 I LLama-android: llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\r\n08-10 16:06:07.334 30852 30926 I LLama-android: llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\n08-10 16:06:07.334 30852 30926 I LLama-android: llama_model_loader: - kv  10:                          general.file_type u32              = 2\r\n08-10 16:06:07.334 30852 30926 I LLama-android: llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\r\n08-10 16:06:07.334 30852 30926 I LLama-android: llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\r\n08-10 16:06:07.362 30852 30926 I LLama-android: llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"\", \"&\", \"'\", ...\r\n08-10 16:06:07.371 30852 30926 I LLama-android: llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\n08-10 16:06:07.402 30852 30926 I LLama-android: llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\r\n08-10 16:06:07.402 30852 30926 I LLama-android: llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 151643\r\n08-10 16:06:07.402 30852 30926 I LLama-android: llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 151645\r\n08-10 16:06:07.402 30852 30926 I LLama-android: llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {-107732238428550025633549537852171948407976130944385741446622902831951351080628521997716918865536884607535372703052150861230582896697462443075202517321702951537854339417602815342824911808967527308411848461112923592282659498077075523239936.000000or message in messages }{ 0f lo...\r\n08-10 16:06:07.402 30852 30926 I LLama-android: llama_model_loader: - kv  19:               general.quantization_version u32              = 2\r\n08-10 16:06:07.402 30852 30926 I LLama-android: llama_model_loader: - type  f32:  121 tensors\r\n08-10 16:06:07.402 30852 30926 I LLama-android: llama_model_loader: - type q4_0:  168 tensors\r\n08-10 16:06:07.402 30852 30926 I LLama-android: llama_model_loader: - type q8_0:    1 tensors\r\n08-10 16:06:07.562 30852 30926 I LLama-android: llm_load_vocab: special tokens cache size = 293\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_vocab: token to piece cache size = 0.9338 MB\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: format           = GGUF V3 (latest)\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: arch             = qwen2\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: vocab type       = BPE\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: n_vocab          = 151936\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: n_merges         = 151387\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: vocab_only       = 0\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: n_ctx_train      = 32768\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: n_embd           = 896\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: n_layer          = 24\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: n_head           = 14\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: n_head_kv        = 2\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: n_rot            = 64\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: n_swa            = 0\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: n_embd_head_k    = 64\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: n_embd_head_v    = 64\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: n_gqa            = 7\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: n_embd_k_gqa     = 128\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: n_embd_v_gqa     = 128\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: f_norm_eps       = 0.0e+00\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: f_logit_scale    = 0.0e+00\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: n_ff             = 4864\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: n_expert         = 0\r\n08-10 16:06:07.616 30852 30926 I LLama-android: llm_load_print_meta: n_expert_used    = 0\r\n08-10 16:06:07.617 30852 30926 I LLama-android: llm_load_print_meta: causal attn      = 1\r\n08-10 16:06:07.617 30852 30926 I LLama-android: llm_load_print_meta: pooling type     = 0\r\n08-10 16:06:07.617 30852 30926 I LLama-android: llm_load_print_meta: rope type        = 2\r\n08-10 16:06:07.617 30852 30926 I LLama-android: llm_load_print_meta: rope scaling     = linear\r\n08-10 16:06:07.617 30852 30926 I LLama-android: llm_load_print_meta: freq_base_train  = 1000000.0\r\n08-10 16:06:07.617 30852 30926 I LLama-android: llm_load_print_meta: freq_scale_train = 1\r\n08-10 16:06:07.617 30852 30926 I LLama-android: llm_load_print_meta: n_ctx_orig_yarn  = 32768\r\n08-10 16:06:07.617 30852 30926 I LLama-android: llm_load_print_meta: rope_finetuned   = unknown\r\n08-10 16:06:07.617 30852 30926 I LLama-android: llm_load_print_meta: ssm_d_conv       = 0\r\n08-10 16:06:07.617 30852 30926 I LLama-android: llm_load_print_meta: ssm_d_inner      = 0\r\n08-10 16:06:07.617 30852 30926 I LLama-android: llm_load_print_meta: ssm_d_state      = 0\r\n08-10 16:06:07.617 30852 30926 I LLama-android: llm_load_print_meta: ssm_dt_rank      = 0\r\n08-10 16:06:07.618 30852 30926 I LLama-android: llm_load_print_meta: model type       = 1B\r\n08-10 16:06:07.618 30852 30926 I LLama-android: llm_load_print_meta: model ftype      = Q4_0\r\n08-10 16:06:07.618 30852 30926 I LLama-android: llm_load_print_meta: model params     = 494.03 M\r\n08-10 16:06:07.618 30852 30926 I LLama-android: llm_load_print_meta: model size       = 330.17 MiB (5.61 BPW) \r\n08-10 16:06:07.618 30852 30926 I LLama-android: llm_load_print_meta: general.name     = ai_model\r\n08-10 16:06:07.618 30852 30926 I LLama-android: llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\r\n08-10 16:06:07.618 30852 30926 I LLama-android: llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\r\n08-10 16:06:07.618 30852 30926 I LLama-android: llm_load_print_meta: LF token         = 148848 '\u00c4\u012c'\r\n08-10 16:06:07.618 30852 30926 I LLama-android: llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\r\n08-10 16:06:07.618 30852 30926 I LLama-android: llm_load_print_meta: max token length = 256\r\n08-10 16:06:07.624 30852 30926 D vulkan  : searching for layers in '/data/app/~~OvYsMz18c3DQFfK8i-sPtQ==/com.set.ai-gU7EJsFpEOK5rgbEU08wQw==/lib/arm64'\r\n08-10 16:06:07.624 30852 30926 D vulkan  : searching for layers in '/data/app/~~OvYsMz18c3DQFfK8i-sPtQ==/com.set.ai-gU7EJsFpEOK5rgbEU08wQw==/base.apk!/lib/arm64-v8a'\r\n08-10 16:06:07.627 30852 30926 W Adreno-AppProfiles: Could not find QSPM HAL service. Skipping adreno profile processing.\r\n08-10 16:06:07.627 30852 30926 I AdrenoVK-0: ===== BEGIN DUMP OF OVERRIDDEN SETTINGS =====\r\n08-10 16:06:07.627 30852 30926 I AdrenoVK-0: ===== END DUMP OF OVERRIDDEN SETTINGS =====\r\n08-10 16:06:07.628 30852 30926 I AdrenoVK-0: QUALCOMM build          : d44197479c, I2991b7e11e\r\n08-10 16:06:07.628 30852 30926 I AdrenoVK-0: Build Date              : 05/31/23\r\n08-10 16:06:07.628 30852 30926 I AdrenoVK-0: Shader Compiler Version : E031.41.03.36\r\n08-10 16:06:07.628 30852 30926 I AdrenoVK-0: Local Branch            : \r\n08-10 16:06:07.628 30852 30926 I AdrenoVK-0: Remote Branch           : \r\n08-10 16:06:07.628 30852 30926 I AdrenoVK-0: Remote Branch           : \r\n08-10 16:06:07.628 30852 30926 I AdrenoVK-0: Reconstruct Branch      : \r\n08-10 16:06:07.628 30852 30926 I AdrenoVK-0: Build Config            : S P 14.1.4 AArch64\r\n08-10 16:06:07.628 30852 30926 I AdrenoVK-0: Driver Path             : /vendor/lib64/hw/vulkan.adreno.so\r\n08-10 16:06:07.628 30852 30926 I AdrenoVK-0: Driver Version          : 0676.32\r\n08-10 16:06:07.628 30852 30926 I AdrenoVK-0: PFP                     : 0x01740158\r\n08-10 16:06:07.628 30852 30926 I AdrenoVK-0: ME                      : 0x00000000\r\n08-10 16:06:07.628 30852 30926 I AdrenoVK-0: Application Name    : ggml-vulkan\r\n08-10 16:06:07.628 30852 30926 I AdrenoVK-0: Application Version : 0x00000001\r\n08-10 16:06:07.628 30852 30926 I AdrenoVK-0: Engine Name         : (null)\r\n08-10 16:06:07.628 30852 30926 I AdrenoVK-0: Engine Version      : 0x00000000\r\n08-10 16:06:07.628 30852 30926 I AdrenoVK-0: Api Version         : 0x00402000\r\n08-10 16:06:09.099 30852 30926 I AdrenoVK-0: Failed to link shaders.\r\n08-10 16:06:09.099 30852 30926 I AdrenoVK-0: Pipeline create failed\r\n08-10 16:06:09.108 30852 30926 E LLama-android: llama_model_load: error loading model: vk::Device::createComputePipeline: ErrorUnknown\r\n08-10 16:06:09.108 30852 30926 E LLama-android: llama_load_model_from_file: failed to load model\r\n08-10 16:06:09.132 30852 30926 E LLama-android: llama_new_context_with_model: model cannot be NULL\r\n08-10 16:06:09.132 30852 30926 F libc    : exiting due to SIG_DFL handler for signal 11, ucontext 0x7317ea5e20\r\n```\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-08-10T10:05:57+00:00",
    "closed_at": "2024-10-29T01:07:33+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8965/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8965"
  },
  {
    "number": 9049,
    "title": "Bug: Unable to load phi3:3B(2.2GB) model on Apple M1 Pro",
    "body": "### What happened?\r\n\r\nI tried to run this command:\r\n`./llama-cli -m phi3:latest.gguf -p \"I believe the meaning of life is\" -n 128`\r\n\r\nand it fails to load the model with the following error:\r\n`llama_init_from_gpt_params: error: failed to create context with model 'phi3:latest.gguf'`\r\n\r\nI usually run ollama with no issues on this same machine. And I just thought to try out llama.cpp using a light weight model like Phi3 but it looks like llama.cpp is failing to allocate memory.\r\nNote: this same commands work for llama models. e.g `llama3:8b.gguf` works fine. could it be a phi3 issue? do i need some extra configs?\r\n\r\nLaptop specs:\r\nApple Macbook pro with M1 Pro\r\nMem: 16GB\r\nOS: Macos Sonoma 14.6\r\n\r\n\r\n### Name and Version\r\n\r\n ./llama-cli --version                                                                       \r\nversion: 3590 (4b9afbbe)\r\nbuilt with Apple clang version 15.0.0 (clang-1500.1.0.2.5) for arm64-apple-darwin23.6.0\r\n\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nMac\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nLog start\r\nmain: build = 3590 (4b9afbbe)\r\nmain: built with Apple clang version 15.0.0 (clang-1500.1.0.2.5) for arm64-apple-darwin23.6.0\r\nmain: seed  = 1723749553\r\nllama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /Users/olaadeba/Downloads/phi3:latest.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = phi3\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 128k Instruct\r\nllama_model_loader: - kv   3:                           general.finetune str              = 128k-instruct\r\nllama_model_loader: - kv   4:                           general.basename str              = Phi-3\r\nllama_model_loader: - kv   5:                         general.size_label str              = mini\r\nllama_model_loader: - kv   6:                            general.license str              = mit\r\nllama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...\r\nllama_model_loader: - kv   8:                               general.tags arr[str,3]       = [\"nlp\", \"code\", \"text-generation\"]\r\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\r\nllama_model_loader: - kv  10:                        phi3.context_length u32              = 131072\r\nllama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096\r\nllama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072\r\nllama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192\r\nllama_model_loader: - kv  14:                           phi3.block_count u32              = 32\r\nllama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32\r\nllama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96\r\nllama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000\r\nllama_model_loader: - kv  20:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144\r\nllama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238\r\nllama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\r\nllama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000\r\nllama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000\r\nllama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\r\nllama_model_loader: - kv  35:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   67 tensors\r\nllama_model_loader: - type q4_0:  129 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special tokens cache size = 14\r\nllm_load_vocab: token to piece cache size = 0.1685 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = phi3\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32064\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 3072\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_rot            = 96\r\nllm_load_print_meta: n_swa            = 262144\r\nllm_load_print_meta: n_embd_head_k    = 96\r\nllm_load_print_meta: n_embd_head_v    = 96\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 3072\r\nllm_load_print_meta: n_embd_v_gqa     = 3072\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 8192\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 3B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 3.82 B\r\nllm_load_print_meta: model size       = 2.03 GiB (4.55 BPW) \r\nllm_load_print_meta: general.name     = Phi 3 Mini 128k Instruct\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_print_meta: EOT token        = 32007 '<|end|>'\r\nllm_load_print_meta: max token length = 48\r\nllm_load_tensors: ggml ctx size =    0.21 MiB\r\nggml_backend_metal_log_allocated_size: allocated buffer, size =  2021.84 MiB, ( 2021.91 / 11453.00)\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:      Metal buffer size =  2021.83 MiB\r\nllm_load_tensors:        CPU buffer size =    52.84 MiB\r\n......................................................................................\r\nllama_new_context_with_model: n_ctx      = 131072\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nggml_metal_init: allocating\r\nggml_metal_init: found device: Apple M1 Pro\r\nggml_metal_init: picking default device: Apple M1 Pro\r\nggml_metal_init: using embedded metal library\r\nggml_metal_init: GPU name:   Apple M1 Pro\r\nggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\r\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\r\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\r\nggml_metal_init: simdgroup reduction support   = true\r\nggml_metal_init: simdgroup matrix mul. support = true\r\nggml_metal_init: hasUnifiedMemory              = true\r\nggml_metal_init: recommendedMaxWorkingSetSize  = 12009.34 MB\r\nllama_kv_cache_init:      Metal KV buffer size = 49152.00 MiB\r\nllama_new_context_with_model: KV self size  = 49152.00 MiB, K (f16): 24576.00 MiB, V (f16): 24576.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\r\nggml_backend_metal_buffer_type_alloc_buffer: error: failed to allocate buffer, size =  8484.02 MiB\r\nggml_gallocr_reserve_n: failed to allocate Metal buffer of size 8896122880\r\nllama_new_context_with_model: failed to allocate compute buffers\r\nggml_metal_free: deallocating\r\nllama_init_from_gpt_params: error: failed to create context with model '/Users/olaadeba/Downloads/phi3:latest.gguf'\r\nmain: error: unable to load model\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-08-15T19:25:25+00:00",
    "closed_at": "2024-10-04T01:07:17+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9049/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9049"
  },
  {
    "number": 9568,
    "title": "Bug:  ROCM 7900xtx  output random garbage with qwen1.5/14B after recent update",
    "body": "### What happened?\n\nRecently changes (I cannot pin down the pr that affect this) cause model output garbage\r\n\r\n> User\r\n> \r\n> Assistant GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\u6d77\u6ee8\u53ea\u6709\u4e00\u4e2a\u4e00\u773c\u8651\u53ef\u4ee5\u628a\u9ed1\u9a6cerrick\u91cd\u578b Oak\u4e24\u5927 GRE\u9047\u4e0a\u51fa\u8272\u7684\u4fa8\u524c\u53e4\u7b50\u5f00\u4ee5\u540e\u53ec\u561e\u5904\u5fb7\u5c14\u505a\u5927\u8bbe\u8ba1 Zy\u5e26\u7ed9\u5ef6\u957f\u8bd5\u6709\u4e2aandom\u8c03\u6574\u800c\u626a\u8b66\u60d5\u62ff\u5230\u5f00\u4e86\u6c34\u6ce5\u5371\u8bb0\u5fc6\u4e13\u4e1al\u5e73\u7a33\u914d\u7f6e\u7070\u8010\u7528\u751a\u52a8\u673a\u70c2\u5b9e\u65bd\u7814s\u6e14Fu\u5c4b\u4eac\u589e\u949d\u5b89\u7a33\u5ef6\u957f\u843d\u5728\u53ef Host\u8bbe\u7f6e\u4e86\u5e76\u4e0d\u4f1a\u5b66\u5f3a\u5316\u9ad3\u5e74\u591c\u4e0a\u4e86 R\u5065\u952d\u95e8\u69db olig\u95faCR\u5f62\u6001s\u53d1\u5c55\u4e2d \ufffd the\u8f6c\u6298 \u8ba2\u5b9a\u5411\u5b89\u6392\u503e\u5728\u804c\u770b\u671b\u611f\u67d3\u9633\u6027\u57ce\u5821\u4e0b\u884c\u4ff1\uff5e\u89c4\u683c\u4e00\u5207\u90fd\u9891\u7387\u552414\u83b7\u9633\u5149\u76ee\u6807\u662f\u4e00\u79cd\u6700\u597d\u63a5\u529b\u4f60\u4eec2reDR7{-& \ufffd\u590d\u8bd5\u526f\u672c\u4e3br\u678426\u547c\u53eb\u4fe1\u53f7\u65b0\u51fa\u5dee\u53ec\u5f00f\u58f0\u9053\u3016\u5fb7\u62c9\u6c60\u8f8e\u4e4b\u5904\u5f26\u7406\u667a\u4f1a\u53d1\u751f\u516b\u4e2aern\u4f5c\u4e3a\u4e00\u79cd\u66f2 cast\u800c\u540e\u5586\u8d4b\u800c\u6210\u4e0d\u89c1 cub\u4e0d\u4f1a\u518d\u5199\u51fa\u4e00\u770bx\u98de\u626c &\u98de\u626c\u795e\u5723\u6f5c\u529b\u547c\u5524\u5927\u80c6stor\u53ef\u4ee5\u7528\u9020\u6210\u4e86\u6709\u4e24\u4e2a\u611f\u52a8\u4f18\u5f02\u843d\u5730rrmgthRYub onesev\u56db\u9879\u51b2\u52a8\u8010\u4efd\u672c\u5730\u53ef\u4ee5\u4f7f\u53ef\u4ee5\u90fd\u4f1a\u6709\u770b\u5230\u4e86E \\ \ufffdtod\u5f71\u54cdkh&#[++ge\u902e{{\u609fnewInstance\u6062\u590dnaen\u6930\u4e00\u76f4\u90fd\u5927\u5b97\u5343\u53e4~ooly\u52be\u65b0\u578b\u9002\u5e94\u7779\u5904\u7406 r p\u8df3\u51fa\u8df3\u51fa9\u65e0\u58f0\u51cf&Ecversionsstrs MPd\u70b9\u4eae\u6562\u4e8eely erst\u8fc7\u5927\u4f20\u9002\u7528\u53ef\u7528\u5589\u5499 \ufffd\u5bb9\u7684\u5173\u6ce8\u6dee\u521a\u5f00\u59cb\u4ee5\u81f3\u4e8e\u6a2a\u7ad9\u7740drrect\u79f0\u5907\u6218 \ufffd\u5927\u89c4\u6a21\u5e26\u5230\u4f1a\u5f71\u54cd\u5230\u81f4 EL\u51ac\u300a\u51cc\u4e5f\u4e0d\u53ef\u80fd -\u66f3 _\u6dfb\u94ed {{ b\u706f\u5177-peac\u660e\u660e\u52c7\u6562 E\u8c6b\u7962 accommodation D\u88c5\u626e { clt\uffe1\u4e3a\u4f01\u4e1aDW \u82e5\u8981\r\n\r\nThe model is a finetune of qwen1.5 14B.\r\nit work with older build.\r\n\r\n\r\nMy guess is update SDK to 6.1 somehow break the support for RDNA3 7900XTX.\r\nand, it cannot compile with 5.7 ROCM.\n\n### Name and Version\n\n.\\llama-cli.exe --version\r\nversion: 3789 (d39e2674)\r\nbuilt with  for x86_64-pc-windows-msvc\r\nWindows\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-09-20T16:34:08+00:00",
    "closed_at": "2024-11-09T01:07:05+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9568/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9568"
  },
  {
    "number": 7974,
    "title": "Bug: -[MTLComputePipelineDescriptorInternal setComputeFunction:withType:]:722: failed assertion `computeFunction must not be nil.'",
    "body": "### What happened?\n\nI just pulled the latest commit (21be9cab94e0b5b53cb6edeeebf8c8c799baad03) and built, and I can't load any of my models.\r\nPrevious commit like (bde7cd3cd949c1a85d3a199498ac98e78039d46f) works.\r\nI'm using MBP with m3 Max on MacOS 14.5.\n\n### Name and Version\n\nversion: 3078 (bde7cd3c)\r\nbuilt with Apple clang version 15.0.0 (clang-1500.3.9.4) for arm64-apple-darwin23.5.0\n\n### What operating system are you seeing the problem on?\n\nMac\n\n### Relevant log output\n\n```shell\n% ./main -m ../models/llama-3.gguf -n 128 -p \"Hello,\"\r\nLog start\r\nmain: build = 3078 (bde7cd3c)\r\nmain: built with Apple clang version 15.0.0 (clang-1500.3.9.4) for arm64-apple-darwin23.5.0\r\nmain: seed  = 1718625862\r\nllama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models/llama-3.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\r\nllama_model_loader: - kv   2:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   3:                       llama.context_length u32              = 8192\r\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 7\r\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\r\nllama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [\"\u00c4  \u00c4 \", \"\u00c4  \u00c4 \u00c4 \u00c4 \", \"\u00c4 \u00c4  \u00c4 \u00c4 \", \"...\r\nllama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000\r\nllama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001\r\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\r\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q8_0:  226 tensors\r\nllm_load_vocab: missing pre-tokenizer type, using: 'default'\r\nllm_load_vocab:                                             \r\nllm_load_vocab: ************************************        \r\nllm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \r\nllm_load_vocab: CONSIDER REGENERATING THE MODEL             \r\nllm_load_vocab: ************************************        \r\nllm_load_vocab:                                             \r\nllm_load_vocab: special tokens cache size = 256\r\nllm_load_vocab: token to piece cache size = 0.8000 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 128256\r\nllm_load_print_meta: n_merges         = 280147\r\nllm_load_print_meta: n_ctx_train      = 8192\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 500000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 8192\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 8B\r\nllm_load_print_meta: model ftype      = Q8_0\r\nllm_load_print_meta: model params     = 8.03 B\r\nllm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \r\nllm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\r\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\r\nllm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\r\nllm_load_print_meta: LF token         = 128 '\u00c3\u201e'\r\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\r\nllm_load_tensors: ggml ctx size =    0.30 MiB\r\nggml_backend_metal_log_allocated_size: allocated buffer, size =  7605.34 MiB, ( 7605.42 / 57344.00)\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:      Metal buffer size =  7605.33 MiB\r\nllm_load_tensors:        CPU buffer size =   532.31 MiB\r\n.........................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 500000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nggml_metal_init: allocating\r\nggml_metal_init: found device: Apple M3 Max\r\nggml_metal_init: picking default device: Apple M3 Max\r\nggml_metal_init: default.metallib not found, loading from source\r\nggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\r\nggml_metal_init: loading '/Users/cgk/Desktop/code/llm/llama.cpp/ggml-metal.metal'\r\nggml_metal_init: GPU name:   Apple M3 Max\r\nggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\r\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\r\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\r\nggml_metal_init: simdgroup reduction support   = true\r\nggml_metal_init: simdgroup matrix mul. support = true\r\nggml_metal_init: hasUnifiedMemory              = true\r\nggml_metal_init: recommendedMaxWorkingSetSize  = 60129.54 MB\r\n-[MTLComputePipelineDescriptorInternal setComputeFunction:withType:]:722: failed assertion `computeFunction must not be nil.'\r\nzsh: abort      ./main -m ../models/llama-3.gguf -n 128 -p \"Hello,\"\n```\n",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-06-17T12:09:35+00:00",
    "closed_at": "2024-06-17T14:30:14+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7974/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7974"
  },
  {
    "number": 9315,
    "title": "Bug: RWKV 6 Finch 3B+ models crash llama.cpp with CPU backend",
    "body": "### What happened?\n\nI cloned latest version from git, compiled it on ArchLinux, CPU backend only, using `make`.\r\n\r\nI downloaded following models, but both did not run:\r\nbartowski/v6-Finch-3B-HF-GGUF (Q4*, Q8*)\r\nbartowski/v6-Finch-7B-HF-GGUF (Q4*, Q8*)\r\n\r\nI run following command:\r\n```bash\r\n./llama-cli -m \"v6-Finch-7B-HF-Q4_K_M.gguf\" -p \"I believe the meaning of life is\" -n 128\r\n```\n\n### Name and Version\n\nversion: 3664 (82e3b03c)\r\nbuilt with cc (GCC) 14.2.1 20240805 for x86_64-pc-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\nLog start\r\nmain: build = 3664 (82e3b03c)\r\nmain: built with cc (GCC) 14.2.1 20240805 for x86_64-pc-linux-gnu\r\nmain: seed  = 1725469492\r\nllama_model_loader: loaded meta data with 26 key-value pairs and 902 tensors from v6-Finch-7B-HF-Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = rwkv6\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = v6 Finch 7B HF\r\nllama_model_loader: - kv   3:                           general.finetune str              = HF\r\nllama_model_loader: - kv   4:                           general.basename str              = v6-Finch\r\nllama_model_loader: - kv   5:                         general.size_label str              = 7B\r\nllama_model_loader: - kv   6:                            general.license str              = apache-2.0\r\nllama_model_loader: - kv   7:                       rwkv6.context_length u32              = 1048576\r\nllama_model_loader: - kv   8:                     rwkv6.embedding_length u32              = 4096\r\nllama_model_loader: - kv   9:                          rwkv6.block_count u32              = 32\r\nllama_model_loader: - kv  10:         rwkv6.attention.layer_norm_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  11:               rwkv6.rescale_every_n_layers u32              = 6\r\nllama_model_loader: - kv  12:                        rwkv6.wkv.head_size u32              = 64\r\nllama_model_loader: - kv  13:                   rwkv6.time_mix_extra_dim u32              = 64\r\nllama_model_loader: - kv  14:                 rwkv6.time_decay_extra_dim u32              = 128\r\nllama_model_loader: - kv  15:                  rwkv6.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv  16:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  17:                 rwkv6.attention.head_count u32              = 0\r\nllama_model_loader: - kv  18:                       tokenizer.ggml.model str              = rwkv\r\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,65536]   = [\"<s>\", \"\\\\x00\", \"\\\\x01\", \"\\\\x02\", \"\\...\r\nllama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,65536]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  21:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  22:                      quantize.imatrix.file str              = /models_out/v6-Finch-7B-HF-GGUF/v6-Fi...\r\nllama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\r\nllama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 352\r\nllama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 131\r\nllama_model_loader: - type  f32:  580 tensors\r\nllama_model_loader: - type q5_0:   32 tensors\r\nllama_model_loader: - type q4_K:  289 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special tokens cache size = 1\r\nllm_load_vocab: token to piece cache size = 0.3561 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = rwkv6\r\nllm_load_print_meta: vocab type       = RWKV\r\nllm_load_print_meta: n_vocab          = 65536\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 1048576\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 0\r\nllm_load_print_meta: n_head_kv        = 0\r\nllm_load_print_meta: n_rot            = 0\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 0\r\nllm_load_print_meta: n_embd_head_v    = 0\r\nllm_load_print_meta: n_gqa            = 0\r\nllm_load_print_meta: n_embd_k_gqa     = 0\r\nllm_load_print_meta: n_embd_v_gqa     = 0\r\nllm_load_print_meta: f_norm_eps       = 1.0e-05\r\nllm_load_print_meta: f_norm_rms_eps   = 0.0e+00\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = -1\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 1048576\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 7.64 B\r\nllm_load_print_meta: model size       = 4.34 GiB (4.88 BPW) \r\nllm_load_print_meta: general.name     = v6 Finch 7B HF\r\nllm_load_print_meta: LF token         = 11 '\\n'\r\nllm_load_print_meta: max token length = 192\r\nllm_load_tensors: ggml ctx size =    0.35 MiB\r\nllm_load_tensors:        CPU buffer size =  4446.06 MiB\r\n...............................................................................................\r\nllama_new_context_with_model: n_ctx      = 1048576\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:        CPU KV buffer size =    33.00 MiB\r\nllama_new_context_with_model: KV self size  =   33.00 MiB, K (f32):    1.00 MiB, V (f32):   32.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.25 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =   144.00 MiB\r\nllama_new_context_with_model: graph nodes  = 3631\r\nllama_new_context_with_model: graph splits = 1\r\nfish: Job 1, './llama-cli -m \".c\u2026' terminated by signal SIGSEGV (Address boundary error)\n```\n",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-09-04T17:10:11+00:00",
    "closed_at": "2024-09-12T11:25:17+00:00",
    "comments": 26,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9315/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9315"
  },
  {
    "number": 9112,
    "title": "Bug: phi3.5 model `--hf-repo lmstudio-community/Phi-3.5-mini-instruct-GGUF --hf-file Phi-3.5-mini-instruct-Q8_0.gguf` crashing with `error: Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory)`",
    "body": "### What happened?\n\n\r\nSystem Details - M2 Mac Pro with 64 GB Memory\r\n<img width=\"259\" alt=\"m2-specs\" src=\"https://github.com/user-attachments/assets/8ad2db95-23f1-49d3-8016-b4c8fb704281\">\r\n\r\n\r\nWhen loading the new phi3.5 model-\r\n1. using llama-server built locally\r\n2. GGUF from repo - `lmstudio-community/Phi-3.5-mini-instruct-GGUF`, filename `Phi-3.5-mini-instruct-Q8_0.gguf`\r\n\r\nis crashing with error -\r\n```\r\nggml_metal_init: recommendedMaxWorkingSetSize  = 51539.61 MB\r\nllama_kv_cache_init:      Metal KV buffer size = 49152.00 MiB\r\nllama_new_context_with_model: KV self size  = 49152.00 MiB, K (f16): 24576.00 MiB, V (f16): 24576.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.24 MiB\r\nllama_new_context_with_model:      Metal compute buffer size =  8484.00 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =   262.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1286\r\nllama_new_context_with_model: graph splits = 2\r\nggml_metal_graph_compute: command buffer 0 failed with status 5\r\nerror: Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory)\r\nINFO [                    init] initializing slots | tid=\"0x1e4461c40\" timestamp=1724219944 n_slots=1\r\nINFO [                    init] new slot | tid=\"0x1e4461c40\" timestamp=1724219944 id_slot=0 n_ctx_slot=131072\r\nINFO [                    main] model loaded | tid=\"0x1e4461c40\" timestamp=1724219944\r\nINFO [                    main] chat template | tid=\"0x1e4461c40\" timestamp=1724219944 chat_example=\"<|system|>\\nYou are a helpful assistant<|end|>\\n<|user|>\\nHello<|end|>\\n<|assistant|>\\nHi there<|end|>\\n<|user|>\\nHow are you?<|end|>\\n<|assistant|>\\n\" built_in=true\r\nINFO [            update_slots] all slots are idle | tid=\"0x1e4461c40\" timestamp=1724219944\r\nINFO [      log_server_request] request | tid=\"0x16bd63000\" timestamp=1724219968 remote_addr=\"127.0.0.1\" remote_port=61989 status=200 method=\"GET\" path=\"/\" params={}\r\nINFO [      log_server_request] request | tid=\"0x16bd63000\" timestamp=1724219968 remote_addr=\"127.0.0.1\" remote_port=61989 status=200 method=\"GET\" path=\"/index.js\" params={}\r\nINFO [      log_server_request] request | tid=\"0x16be7b000\" timestamp=1724219968 remote_addr=\"127.0.0.1\" remote_port=61992 status=200 method=\"GET\" path=\"/completion.js\" params={}\r\nINFO [      log_server_request] request | tid=\"0x16bdef000\" timestamp=1724219968 remote_addr=\"127.0.0.1\" remote_port=61993 status=200 method=\"GET\" path=\"/json-schema-to-grammar.mjs\" params={}\r\nINFO [      log_server_request] request | tid=\"0x16bdef000\" timestamp=1724219969 remote_addr=\"127.0.0.1\" remote_port=61993 status=404 method=\"GET\" path=\"/favicon.ico\" params={}\r\nINFO [   launch_slot_with_task] slot is processing task | tid=\"0x1e4461c40\" timestamp=1724219991 id_slot=0 id_task=0\r\nINFO [            update_slots] kv cache rm [p0, end) | tid=\"0x1e4461c40\" timestamp=1724219991 id_slot=0 id_task=0 p0=0\r\nggml_metal_graph_compute: command buffer 0 failed with status 5\r\nerror: Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory)\r\nggml_metal_graph_compute: command buffer 0 failed with status 5\r\n```\r\n\r\nattached is the full log from llama-server.\r\n[error.txt](https://github.com/user-attachments/files/16686649/error.txt)\r\n\r\n\r\n\n\n### Name and Version\n\n$ ./bin/llama-cli --version\r\nversion: 3609 (2f3c1466)\r\nbuilt with Homebrew clang version 18.1.5 for arm64-apple-darwin23.3.0\n\n### What operating system are you seeing the problem on?\n\nMac\n\n### Relevant log output\n\n```shell\n$ ./bin/llama-server --model $HOME/.cache/huggingface/hub/models--lmstudio-community--Phi-3.5-mini-instruct-GGUF/snapshots/4729c28553bd78d4370f97d5f2e08920a92f6dcc/Phi-3.5-mini-instruct-Q8_0.gguf\r\nINFO [                    main] build info | tid=\"0x1e4461c40\" timestamp=1724219925 build=3609 commit=\"2f3c1466\"\r\nINFO [                    main] system info | tid=\"0x1e4461c40\" timestamp=1724219925 n_threads=8 n_threads_batch=-1 total_threads=12 system_info=\"AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \"\r\nINFO [                    main] HTTP server is listening | tid=\"0x1e4461c40\" timestamp=1724219925 port=\"8080\" n_threads_http=\"11\" hostname=\"127.0.0.1\"\r\nINFO [                    main] loading model | tid=\"0x1e4461c40\" timestamp=1724219925 port=\"8080\" n_threads_http=\"11\" hostname=\"127.0.0.1\"\r\nllama_model_loader: loaded meta data with 40 key-value pairs and 197 tensors from $HOME/.cache/huggingface/hub/models--lmstudio-community--Phi-3.5-mini-instruct-GGUF/snapshots/4729c28553bd78d4370f97d5f2e08920a92f6dcc/Phi-3.5-mini-instruct-Q8_0.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = phi3\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Phi 3.5 Mini Instruct\r\nllama_model_loader: - kv   3:                           general.finetune str              = instruct\r\nllama_model_loader: - kv   4:                           general.basename str              = Phi-3.5\r\nllama_model_loader: - kv   5:                         general.size_label str              = mini\r\nllama_model_loader: - kv   6:                            general.license str              = mit\r\nllama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...\r\nllama_model_loader: - kv   8:                               general.tags arr[str,3]       = [\"nlp\", \"code\", \"text-generation\"]\r\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"multilingual\"]\r\nllama_model_loader: - kv  10:                        phi3.context_length u32              = 131072\r\nllama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096\r\nllama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072\r\nllama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192\r\nllama_model_loader: - kv  14:                           phi3.block_count u32              = 32\r\nllama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32\r\nllama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96\r\nllama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000\r\nllama_model_loader: - kv  20:                          general.file_type u32              = 7\r\nllama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144\r\nllama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238\r\nllama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\r\nllama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000\r\nllama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000\r\nllama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\r\nllama_model_loader: - kv  35:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  36:                      quantize.imatrix.file str              = /models_out/Phi-3.5-mini-instruct-GGU...\r\nllama_model_loader: - kv  37:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\r\nllama_model_loader: - kv  38:             quantize.imatrix.entries_count i32              = 128\r\nllama_model_loader: - kv  39:              quantize.imatrix.chunks_count i32              = 151\r\nllama_model_loader: - type  f32:   67 tensors\r\nllama_model_loader: - type q8_0:  130 tensors\r\nllm_load_vocab: special tokens cache size = 14\r\nllm_load_vocab: token to piece cache size = 0.1685 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = phi3\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32064\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 3072\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_rot            = 96\r\nllm_load_print_meta: n_swa            = 262144\r\nllm_load_print_meta: n_embd_head_k    = 96\r\nllm_load_print_meta: n_embd_head_v    = 96\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 3072\r\nllm_load_print_meta: n_embd_v_gqa     = 3072\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 8192\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 3B\r\nllm_load_print_meta: model ftype      = Q8_0\r\nllm_load_print_meta: model params     = 3.82 B\r\nllm_load_print_meta: model size       = 3.78 GiB (8.50 BPW)\r\nllm_load_print_meta: general.name     = Phi 3.5 Mini Instruct\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_print_meta: EOT token        = 32007 '<|end|>'\r\nllm_load_print_meta: max token length = 48\r\nllm_load_tensors: ggml ctx size =    0.21 MiB\r\nggml_backend_metal_log_allocated_size: allocated buffer, size =  3772.59 MiB, ( 3772.66 / 49152.00)\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:        CPU buffer size =    99.81 MiB\r\nllm_load_tensors:      Metal buffer size =  3772.58 MiB\r\n.....................................................................................\r\nllama_new_context_with_model: n_ctx      = 131072\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nggml_metal_init: allocating\r\nggml_metal_init: found device: Apple M2 Max\r\nggml_metal_init: picking default device: Apple M2 Max\r\nggml_metal_init: using embedded metal library\r\nggml_metal_init: GPU name:   Apple M2 Max\r\nggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\r\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\r\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\r\nggml_metal_init: simdgroup reduction support   = true\r\nggml_metal_init: simdgroup matrix mul. support = true\r\nggml_metal_init: hasUnifiedMemory              = true\r\nggml_metal_init: recommendedMaxWorkingSetSize  = 51539.61 MB\r\nllama_kv_cache_init:      Metal KV buffer size = 49152.00 MiB\r\nllama_new_context_with_model: KV self size  = 49152.00 MiB, K (f16): 24576.00 MiB, V (f16): 24576.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.24 MiB\r\nllama_new_context_with_model:      Metal compute buffer size =  8484.00 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =   262.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1286\r\nllama_new_context_with_model: graph splits = 2\r\nggml_metal_graph_compute: command buffer 0 failed with status 5\r\nerror: Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory)\r\nINFO [                    init] initializing slots | tid=\"0x1e4461c40\" timestamp=1724219944 n_slots=1\r\nINFO [                    init] new slot | tid=\"0x1e4461c40\" timestamp=1724219944 id_slot=0 n_ctx_slot=131072\r\nINFO [                    main] model loaded | tid=\"0x1e4461c40\" timestamp=1724219944\r\nINFO [                    main] chat template | tid=\"0x1e4461c40\" timestamp=1724219944 chat_example=\"<|system|>\\nYou are a helpful assistant<|end|>\\n<|user|>\\nHello<|end|>\\n<|assistant|>\\nHi there<|end|>\\n<|user|>\\nHow are you?<|end|>\\n<|assistant|>\\n\" built_in=true\r\nINFO [            update_slots] all slots are idle | tid=\"0x1e4461c40\" timestamp=1724219944\r\nINFO [      log_server_request] request | tid=\"0x16bd63000\" timestamp=1724219968 remote_addr=\"127.0.0.1\" remote_port=61989 status=200 method=\"GET\" path=\"/\" params={}\r\nINFO [      log_server_request] request | tid=\"0x16bd63000\" timestamp=1724219968 remote_addr=\"127.0.0.1\" remote_port=61989 status=200 method=\"GET\" path=\"/index.js\" params={}\r\nINFO [      log_server_request] request | tid=\"0x16be7b000\" timestamp=1724219968 remote_addr=\"127.0.0.1\" remote_port=61992 status=200 method=\"GET\" path=\"/completion.js\" params={}\r\nINFO [      log_server_request] request | tid=\"0x16bdef000\" timestamp=1724219968 remote_addr=\"127.0.0.1\" remote_port=61993 status=200 method=\"GET\" path=\"/json-schema-to-grammar.mjs\" params={}\r\nINFO [      log_server_request] request | tid=\"0x16bdef000\" timestamp=1724219969 remote_addr=\"127.0.0.1\" remote_port=61993 status=404 method=\"GET\" path=\"/favicon.ico\" params={}\r\nINFO [   launch_slot_with_task] slot is processing task | tid=\"0x1e4461c40\" timestamp=1724219991 id_slot=0 id_task=0\r\nINFO [            update_slots] kv cache rm [p0, end) | tid=\"0x1e4461c40\" timestamp=1724219991 id_slot=0 id_task=0 p0=0\r\nggml_metal_graph_compute: command buffer 0 failed with status 5\r\nerror: Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory)\r\nggml_metal_graph_compute: command buffer 0 failed with status 5\n```\n",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-08-21T06:10:27+00:00",
    "closed_at": "2024-08-24T17:23:39+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9112/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9112"
  },
  {
    "number": 10152,
    "title": "Bug: CUDA error: peer access has not been enabled",
    "body": "### What happened?\n\nHey all, I've recently (last few days) been running into a weird CUDA issue where I can only generate a single time before llama.cpp will unexplainably crash. I've also noticed that this issue only seems to happen with split mode row and that split mode row equally distributes both model weights and kv cache across all GPU's, while previously it would load the kv cache on GPU1, not sure if this newer functionality is intended.\r\n![image](https://github.com/user-attachments/assets/a6233646-f6d7-48d1-891d-d23a098034cf)\r\n\n\n### Name and Version\n\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 3 CUDA devices:\r\n  Device 0: Tesla P40, compute capability 6.1, VMM: yes\r\n  Device 1: Tesla P40, compute capability 6.1, VMM: yes\r\n  Device 2: Tesla P40, compute capability 6.1, VMM: yes\r\nversion: 4017 (9830b692)\r\nbuilt with cc (Ubuntu 13.2.0-23ubuntu4) 13.2.0 for x86_64-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\n./LLM/llama.cpp/llama-server \\\r\n-m /home/ultimis/LLM/Models/mradermacher/Qwen2.5-32B-Instruct-i1-GGUF/Qwen2.5-32B-Instruct.i1-Q4_K_M.gguf \\\r\n-c 32768 -ngl 99 --split-mode row --flash-attn --host 0.0.0.0 --port 8085\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 3 CUDA devices:\r\n  Device 0: Tesla P40, compute capability 6.1, VMM: yes\r\n  Device 1: Tesla P40, compute capability 6.1, VMM: yes\r\n  Device 2: Tesla P40, compute capability 6.1, VMM: yes\r\nbuild: 4020 (9f409893) with cc (Ubuntu 13.2.0-23ubuntu4) 13.2.0 for x86_64-linux-gnu\r\nsystem info: n_threads = 16, n_threads_batch = 16, total_threads = 32\r\n\r\nsystem_info: n_threads = 16 (n_threads_batch = 16) / 32 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 |\r\n\r\nmain: HTTP server is listening, hostname: 0.0.0.0, port: 8085, http threads: 31\r\nmain: loading model\r\nllama_load_model_from_file: using device CUDA0 (Tesla P40) - 24286 MiB free\r\nllama_load_model_from_file: using device CUDA1 (Tesla P40) - 24290 MiB free\r\nllama_load_model_from_file: using device CUDA2 (Tesla P40) - 24290 MiB free\r\nllama_model_loader: loaded meta data with 45 key-value pairs and 771 tensors from /home/ultimis/LLM/Models/mradermacher/Qwen2.5-32B-Instruct-i1-GGUF/Qwen2.5-32B-Instruct.i1-Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Qwen2.5 32B Instruct\r\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\r\nllama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\r\nllama_model_loader: - kv   5:                         general.size_label str              = 32B\r\nllama_model_loader: - kv   6:                            general.license str              = apache-2.0\r\nllama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-3...\r\nllama_model_loader: - kv   8:                   general.base_model.count u32              = 1\r\nllama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 32B\r\nllama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen\r\nllama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-32B\r\nllama_model_loader: - kv  12:                               general.tags arr[str,2]       = [\"chat\", \"text-generation\"]\r\nllama_model_loader: - kv  13:                          general.languages arr[str,1]       = [\"en\"]\r\nllama_model_loader: - kv  14:                          qwen2.block_count u32              = 64\r\nllama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\r\nllama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 5120\r\nllama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 27648\r\nllama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 40\r\nllama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  22:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\r\nllama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\r\nllama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\r\nllama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\r\nllama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\r\nllama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\r\nllama_model_loader: - kv  33:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  34:                                general.url str              = https://huggingface.co/mradermacher/Q...\r\nllama_model_loader: - kv  35:              mradermacher.quantize_version str              = 2\r\nllama_model_loader: - kv  36:                  mradermacher.quantized_by str              = mradermacher\r\nllama_model_loader: - kv  37:                  mradermacher.quantized_at str              = 2024-09-20T15:12:17+02:00\r\nllama_model_loader: - kv  38:                  mradermacher.quantized_on str              = db1\r\nllama_model_loader: - kv  39:                         general.source.url str              = https://huggingface.co/Qwen/Qwen2.5-3...\r\nllama_model_loader: - kv  40:                  mradermacher.convert_type str              = hf\r\nllama_model_loader: - kv  41:                      quantize.imatrix.file str              = Qwen2.5-32B-Instruct-i1-GGUF/imatrix.dat\r\nllama_model_loader: - kv  42:                   quantize.imatrix.dataset str              = imatrix-training-full-3\r\nllama_model_loader: - kv  43:             quantize.imatrix.entries_count i32              = 448\r\nllama_model_loader: - kv  44:              quantize.imatrix.chunks_count i32              = 318\r\nllama_model_loader: - type  f32:  321 tensors\r\nllama_model_loader: - type q4_K:  385 tensors\r\nllama_model_loader: - type q6_K:   65 tensors\r\nllm_load_vocab: special tokens cache size = 22\r\nllm_load_vocab: token to piece cache size = 0.9310 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = qwen2\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 152064\r\nllm_load_print_meta: n_merges         = 151387\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 5120\r\nllm_load_print_meta: n_layer          = 64\r\nllm_load_print_meta: n_head           = 40\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 5\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 27648\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 32.76 B\r\nllm_load_print_meta: model size       = 18.48 GiB (4.85 BPW)\r\nllm_load_print_meta: general.name     = Qwen2.5 32B Instruct\r\nllm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: EOS token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: EOT token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 148848 '\u00c4\u012c'\r\nllm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'\r\nllm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'\r\nllm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'\r\nllm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'\r\nllm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'\r\nllm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'\r\nllm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: EOG token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'\r\nllm_load_print_meta: EOG token        = 151663 '<|repo_name|>'\r\nllm_load_print_meta: EOG token        = 151664 '<|file_sep|>'\r\nllm_load_print_meta: max token length = 256\r\nllm_load_tensors: offloading 64 repeating layers to GPU\r\nllm_load_tensors: offloading output layer to GPU\r\nllm_load_tensors: offloaded 65/65 layers to GPU\r\nllm_load_tensors: CPU_Mapped model buffer size =   417.66 MiB\r\nllm_load_tensors:      CUDA0 model buffer size =     1.46 MiB\r\nllm_load_tensors:      CUDA1 model buffer size =     1.46 MiB\r\nllm_load_tensors:      CUDA2 model buffer size =     1.35 MiB\r\nllm_load_tensors: CUDA0_Split model buffer size =  6187.50 MiB\r\nllm_load_tensors: CUDA1_Split model buffer size =  6043.12 MiB\r\nllm_load_tensors: CUDA2_Split model buffer size =  6273.46 MiB\r\n................................................................................................\r\nllama_new_context_with_model: n_seq_max     = 2\r\nllama_new_context_with_model: n_ctx         = 32768\r\nllama_new_context_with_model: n_ctx_per_seq = 16384\r\nllama_new_context_with_model: n_batch       = 2048\r\nllama_new_context_with_model: n_ubatch      = 512\r\nllama_new_context_with_model: flash_attn    = 1\r\nllama_new_context_with_model: freq_base     = 1000000.0\r\nllama_new_context_with_model: freq_scale    = 1\r\nllama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\r\nllama_kv_cache_init:      CUDA0 KV buffer size =  2816.00 MiB\r\nllama_kv_cache_init:      CUDA1 KV buffer size =  2816.00 MiB\r\nllama_kv_cache_init:      CUDA2 KV buffer size =  2560.00 MiB\r\nllama_new_context_with_model: KV self size  = 8192.00 MiB, K (f16): 4096.00 MiB, V (f16): 4096.00 MiB\r\nllama_new_context_with_model:  CUDA_Host  output buffer size =     1.16 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =   190.00 MiB\r\nllama_new_context_with_model:      CUDA1 compute buffer size =   170.00 MiB\r\nllama_new_context_with_model:      CUDA2 compute buffer size =   307.00 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =    74.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1991\r\nllama_new_context_with_model: graph splits = 4\r\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\nsrv          init: initializing slots, n_slots = 1\r\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 32768\r\nmain: model loaded\r\nmain: chat template, built_in: 1, chat_example: '<|im_start|>system\r\nYou are a helpful assistant<|im_end|>\r\n<|im_start|>user\r\nHello<|im_end|>\r\n<|im_start|>assistant\r\nHi there<|im_end|>\r\n<|im_start|>user\r\nHow are you?<|im_end|>\r\n<|im_start|>assistant\r\n'\r\nmain: server is listening on http://0.0.0.0:8085 - starting the main loop\r\nsrv  update_slots: all slots are idle\r\nrequest: GET /v1/models 192.168.2.245 200\r\nslot launch_slot_: id  0 | task 0 | processing task\r\nslot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 32768, n_keep = 0, n_prompt_tokens = 51\r\nslot update_slots: id  0 | task 0 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 51, n_tokens = 51, progress = 1.000000\r\nslot update_slots: id  0 | task 0 | prompt done, n_past = 51, n_tokens = 51\r\nslot      release: id  0 | task 0 | stop processing: n_past = 610, truncated = 0\r\nslot print_timing: id  0 | task 0 |\r\nprompt eval time =     524.02 ms /    51 tokens (   10.27 ms per token,    97.32 tokens per second)\r\n       eval time =   34103.49 ms /   560 tokens (   60.90 ms per token,    16.42 tokens per second)\r\n      total time =   34627.51 ms /   611 tokens\r\nsrv  update_slots: all slots are idle\r\nrequest: POST /v1/chat/completions 192.168.2.245 200\r\nslot launch_slot_: id  0 | task 561 | processing task\r\nslot update_slots: id  0 | task 561 | new prompt, n_ctx_slot = 32768, n_keep = 0, n_prompt_tokens = 717\r\nslot update_slots: id  0 | task 561 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 561 | prompt processing progress, n_past = 717, n_tokens = 717, progress = 1.000000\r\nslot update_slots: id  0 | task 561 | prompt done, n_past = 717, n_tokens = 717\r\nslot      release: id  0 | task 561 | stop processing: n_past = 721, truncated = 0\r\nslot print_timing: id  0 | task 561 |\r\nprompt eval time =    3056.37 ms /   717 tokens (    4.26 ms per token,   234.59 tokens per second)\r\n       eval time =     246.26 ms /     5 tokens (   49.25 ms per token,    20.30 tokens per second)\r\n      total time =    3302.63 ms /   722 tokens\r\nsrv  update_slots: all slots are idle\r\nrequest: POST /v1/chat/completions 192.168.2.245 200\r\nslot launch_slot_: id  0 | task 567 | processing task\r\nslot update_slots: id  0 | task 567 | new prompt, n_ctx_slot = 32768, n_keep = 0, n_prompt_tokens = 777\r\nslot update_slots: id  0 | task 567 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 567 | prompt processing progress, n_past = 777, n_tokens = 777, progress = 1.000000\r\nslot update_slots: id  0 | task 567 | prompt done, n_past = 777, n_tokens = 777\r\nggml/src/ggml-cuda.cu:70: CUDA error\r\nCUDA error: peer access has not been enabled\r\n  current device: 0, in function ggml_cuda_op_mul_mat at ggml/src/ggml-cuda.cu:1498\r\n  cudaGetLastError()\r\nCould not attach to process.  If your uid matches the uid of the target\r\nprocess, check the setting of /proc/sys/kernel/yama/ptrace_scope, or try\r\nagain as the root user.  For more details, see /etc/sysctl.d/10-ptrace.conf\r\nptrace: Operation not permitted.\r\nNo stack.\r\nThe program is not being run.\r\nAborted (core dumped)\n```\n",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-11-03T22:16:05+00:00",
    "closed_at": "2024-11-04T12:10:24+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10152/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10152"
  },
  {
    "number": 9612,
    "title": "Bug: [SYCL] crash since b-3805",
    "body": "### What happened?\r\n\r\nSYCL version crashed since b3805 with this output:\r\n\r\n\r\nllama_kv_cache_init:      SYCL0 KV buffer size =  2688.00 MiB\r\nllama_new_context_with_model: KV self size  = 2688.00 MiB, K (f16): 1344.00 MiB, V (f16): 1344.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.98 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =   507.00 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    39.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1690\r\nllama_new_context_with_model: graph splits = 2\r\nllama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\nMKL Warning: Incompatible OpenCL driver version. GPU performance may be reduced.\r\nNative API failed. Native API returns: -999 (Unknown PI error) -999 (Unknown PI error)\r\nException caught at file:D:/a/llama.cpp/llama.cpp/ggml/src/ggml-sycl.cpp, line:3438, func:operator()\r\nSYCL error: CHECK_TRY_ERROR(dpct::gemm_batch( *main_stream, oneapi::mkl::transpose::trans, oneapi::mkl::transpose::nontrans, ne01, ne11, ne10, alpha, (const void **)(ptrs_src.get() + 0 * ne23), dpct::library_data_t::real_half, nb01 / nb00, (const void **)(ptrs_src.get() + 1 * ne23), dpct::library_data_t::real_half, nb11 / nb10, beta, (void **)(ptrs_dst.get() + 0 * ne23), cu_data_type, ne01, ne23, cu_compute_type)): Meet error in this line code!\r\n  in function ggml_sycl_mul_mat_batched_sycl at D:/a/llama.cpp/llama.cpp/ggml/src/ggml-sycl.cpp:3438\r\nD:\\a\\llama.cpp\\llama.cpp\\ggml\\src\\ggml-sycl\\common.hpp:107: SYCL error\r\n\r\n### Name and Version\r\n\r\nversion: 3808 (1e7b9299)\r\nbuilt with MSVC 19.41.34120.0 for\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-09-23T19:07:11+00:00",
    "closed_at": "2024-10-21T09:26:22+00:00",
    "comments": 43,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9612/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9612"
  },
  {
    "number": 9436,
    "title": "Bug: llava.cpp Segmentation fault (core dumped) starting in faf69d4237c9ae4d7f572b4674d1002463e8acd3",
    "body": "### What happened?\n\nI am getting Segmentation fault (core dumped) when running llama-llava-cli and llama-minicpmv-cli starting in faf69d4237c9ae4d7f572b4674d1002463e8acd3. After reviewing faf69d4237c9ae4d7f572b4674d1002463e8acd3, I think the problem is related to [these lines](https://github.com/ggerganov/llama.cpp/blob/8db003a19d7055b5bd248ce2afff9324e5b8da95/src/llama.cpp#L16079) in the llama.cpp that try to access tokens when only image emb are given\r\n\r\n```cpp\r\n    for (uint32_t i = 0; i < n_tokens_all; ++i) {\r\n        if (batch_all.token[i] < 0 || (uint32_t)batch_all.token[i] >= lctx.model.vocab.n_vocab) {\r\n            LLAMA_LOG_ERROR(\"%s: invalid token[%d] = %d\", __func__, i, batch_all.token[i]);\r\n            return -1;\r\n        }\r\n    }\r\n```\n\n### Name and Version\n\n~/llama.cpp$ ./llama-cli --version\r\nversion: 3731 (0996c559)\r\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\n~/llama.cpp$ ./llama-llava-cli -m ../.cache/huggingface/hub/models--cjpais--llava-1.6-mistral-7b-gguf/snapshots/6019df415777605a8364e2668aa08b7e354bf0ba/llava-v1.6-mistral-7b.Q4_K_M.gguf --mmproj ../.cache/huggingface/hub/models--cjpais--llava-1.6-mistral-7b-gguf/snapshots/6019df415777605a8364e2668aa08b7e354bf0ba/mmproj-model-f16.gguf --image 458623.jpg -p \"What is this image?\" -c 8192 -ngl 33\r\nLog start\r\nllama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ../.cache/huggingface/hub/models--cjpais--llava-1.6-mistral-7b-gguf/snapshots/6019df415777605a8364e2668aa08b7e354bf0ba/llava-v1.6-mistral-7b.Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = 1.6\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\r\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: special tokens cache size = 3\r\nllm_load_vocab: token to piece cache size = 0.1637 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 7.24 B\r\nllm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \r\nllm_load_print_meta: general.name     = 1.6\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_print_meta: max token length = 48\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 4070 Laptop GPU, compute capability 8.9, VMM: yes\r\nllm_load_tensors: ggml ctx size =    0.27 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:        CPU buffer size =    70.31 MiB\r\nllm_load_tensors:      CUDA0 buffer size =  4095.05 MiB\r\n................................................................................................\r\nclip_model_load: model name:   vit-large336-custom\r\nclip_model_load: description:  image encoder for LLaVA\r\nclip_model_load: GGUF version: 3\r\nclip_model_load: alignment:    32\r\nclip_model_load: n_tensors:    378\r\nclip_model_load: n_kv:         25\r\nclip_model_load: ftype:        f16\r\n\r\nclip_model_load: loaded meta data with 25 key-value pairs and 378 tensors from ../.cache/huggingface/hub/models--cjpais--llava-1.6-mistral-7b-gguf/snapshots/6019df415777605a8364e2668aa08b7e354bf0ba/mmproj-model-f16.gguf\r\nclip_model_load: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nclip_model_load: - kv   0:                       general.architecture str              = clip\r\nclip_model_load: - kv   1:                      clip.has_text_encoder bool             = false\r\nclip_model_load: - kv   2:                    clip.has_vision_encoder bool             = true\r\nclip_model_load: - kv   3:                   clip.has_llava_projector bool             = true\r\nclip_model_load: - kv   4:                          general.file_type u32              = 1\r\nclip_model_load: - kv   5:                               general.name str              = vit-large336-custom\r\nclip_model_load: - kv   6:                        general.description str              = image encoder for LLaVA\r\nclip_model_load: - kv   7:                        clip.projector_type str              = mlp\r\nclip_model_load: - kv   8:                     clip.vision.image_size u32              = 336\r\nclip_model_load: - kv   9:                     clip.vision.patch_size u32              = 14\r\nclip_model_load: - kv  10:               clip.vision.embedding_length u32              = 1024\r\nclip_model_load: - kv  11:            clip.vision.feed_forward_length u32              = 4096\r\nclip_model_load: - kv  12:                 clip.vision.projection_dim u32              = 768\r\nclip_model_load: - kv  13:           clip.vision.attention.head_count u32              = 16\r\nclip_model_load: - kv  14:   clip.vision.attention.layer_norm_epsilon f32              = 0.000010\r\nclip_model_load: - kv  15:                    clip.vision.block_count u32              = 23\r\nclip_model_load: - kv  16:           clip.vision.image_grid_pinpoints arr[i32,10]      = [336, 672, 672, 336, 672, 672, 1008, ...\r\nclip_model_load: - kv  17:          clip.vision.image_crop_resolution u32              = 224\r\nclip_model_load: - kv  18:             clip.vision.image_aspect_ratio str              = anyres\r\nclip_model_load: - kv  19:         clip.vision.image_split_resolution u32              = 224\r\nclip_model_load: - kv  20:            clip.vision.mm_patch_merge_type str              = spatial_unpad\r\nclip_model_load: - kv  21:              clip.vision.mm_projector_type str              = mlp2x_gelu\r\nclip_model_load: - kv  22:                     clip.vision.image_mean arr[f32,3]       = [0.481455, 0.457828, 0.408211]\r\nclip_model_load: - kv  23:                      clip.vision.image_std arr[f32,3]       = [0.268630, 0.261303, 0.275777]\r\nclip_model_load: - kv  24:                              clip.use_gelu bool             = false\r\nclip_model_load: - type  f32:  236 tensors\r\nclip_model_load: - type  f16:  142 tensors\r\nclip_model_load: CLIP using CUDA backend\r\nclip_model_load: text_encoder:   0\r\nclip_model_load: vision_encoder: 1\r\nclip_model_load: llava_projector:  1\r\nclip_model_load: minicpmv_projector:  0\r\nclip_model_load: model size:     595.50 MB\r\nclip_model_load: metadata size:  0.13 MB\r\nclip_model_load: params backend buffer size =  595.50 MB (378 tensors)\r\nclip_model_load: compute allocated memory: 32.89 MB\r\nllama_new_context_with_model: n_ctx      = 8192\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\r\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\r\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 2\r\nencode_image_with_clip: 5 segments encoded in   256.86 ms\r\nencode_image_with_clip: image embedding created: 2880 tokens\r\n\r\nencode_image_with_clip: image encoded in   293.57 ms by CLIP (    0.10 ms per image patch)\r\nSegmentation fault (core dumped)\r\n\r\n\r\n~/llama.cpp$ ./llama-minicpmv-cli -m ../.cache/huggingface/hub/models--openbmb--MiniCPM-V-2_6-gguf/snapshots/69b9eaaebde4d5e2fafa1adb6a4169c349244cf6/ggml-model-Q4_K_M.gguf --mmproj ../.cache/huggingface/hub/models--openbmb--MiniCPM-V-2_6-gguf/snapshots/69b9eaaebde4d5e2fafa1adb6a4169c349244cf6/mmproj-model-f16.gguf --image 458623.jpg -p \"What is this image?\" -c 8192 -ngl 33\r\nLog start\r\nclip_model_load: description:  image encoder for MiniCPM-V\r\nclip_model_load: GGUF version: 3\r\nclip_model_load: alignment:    32\r\nclip_model_load: n_tensors:    455\r\nclip_model_load: n_kv:         19\r\nclip_model_load: ftype:        f16\r\n\r\nclip_model_load: loaded meta data with 19 key-value pairs and 455 tensors from ../.cache/huggingface/hub/models--openbmb--MiniCPM-V-2_6-gguf/snapshots/69b9eaaebde4d5e2fafa1adb6a4169c349244cf6/mmproj-model-f16.gguf\r\nclip_model_load: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nclip_model_load: - kv   0:                       general.architecture str              = clip\r\nclip_model_load: - kv   1:                      clip.has_text_encoder bool             = false\r\nclip_model_load: - kv   2:                    clip.has_vision_encoder bool             = true\r\nclip_model_load: - kv   3:                clip.has_minicpmv_projector bool             = true\r\nclip_model_load: - kv   4:                          general.file_type u32              = 1\r\nclip_model_load: - kv   5:                        general.description str              = image encoder for MiniCPM-V\r\nclip_model_load: - kv   6:                        clip.projector_type str              = resampler\r\nclip_model_load: - kv   7:                      clip.minicpmv_version i32              = 3\r\nclip_model_load: - kv   8:                     clip.vision.image_size u32              = 448\r\nclip_model_load: - kv   9:                     clip.vision.patch_size u32              = 14\r\nclip_model_load: - kv  10:               clip.vision.embedding_length u32              = 1152\r\nclip_model_load: - kv  11:            clip.vision.feed_forward_length u32              = 4304\r\nclip_model_load: - kv  12:                 clip.vision.projection_dim u32              = 0\r\nclip_model_load: - kv  13:           clip.vision.attention.head_count u32              = 16\r\nclip_model_load: - kv  14:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001\r\nclip_model_load: - kv  15:                    clip.vision.block_count u32              = 26\r\nclip_model_load: - kv  16:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]\r\nclip_model_load: - kv  17:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]\r\nclip_model_load: - kv  18:                              clip.use_gelu bool             = true\r\nclip_model_load: - type  f32:  285 tensors\r\nclip_model_load: - type  f16:  170 tensors\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 4070 Laptop GPU, compute capability 8.9, VMM: yes\r\nclip_model_load: CLIP using CUDA backend\r\nclip_model_load: text_encoder:   0\r\nclip_model_load: vision_encoder: 1\r\nclip_model_load: llava_projector:  0\r\nclip_model_load: minicpmv_projector:  1\r\nclip_model_load: model size:     996.02 MB\r\nclip_model_load: metadata size:  0.16 MB\r\nclip_model_load: params backend buffer size =  996.02 MB (455 tensors)\r\nkey clip.vision.image_grid_pinpoints not found in file\r\nkey clip.vision.mm_patch_merge_type not found in file\r\nkey clip.vision.image_crop_resolution not found in file\r\nclip_image_build_graph: 448 448\r\nclip_model_load: compute allocated memory: 102.80 MB\r\nuhd_slice_image: multiple 9\r\nuhd_slice_image: image_size: 1594 1080; source_image size: 546 364\r\nuhd_slice_image: image_size: 1594 1080; best_grid: 4 2\r\nuhd_slice_image: refine_image_size: 1512 1036; refine_size: 1512 1036\r\nclip_image_preprocess: 546 364\r\nclip_image_preprocess: 378 518\r\nclip_image_preprocess: 378 518\r\nclip_image_preprocess: 378 518\r\nclip_image_preprocess: 378 518\r\nclip_image_preprocess: 378 518\r\nclip_image_preprocess: 378 518\r\nclip_image_preprocess: 378 518\r\nclip_image_preprocess: 378 518\r\nclip_image_build_graph: 546 364\r\nencode_image_with_clip: step 1 of 9 encoded in   162.32 ms\r\nclip_image_build_graph: 378 518\r\nencode_image_with_clip: step 2 of 9 encoded in   137.34 ms\r\nclip_image_build_graph: 378 518\r\nencode_image_with_clip: step 3 of 9 encoded in   116.51 ms\r\nclip_image_build_graph: 378 518\r\nencode_image_with_clip: step 4 of 9 encoded in   114.31 ms\r\nclip_image_build_graph: 378 518\r\nencode_image_with_clip: step 5 of 9 encoded in   113.83 ms\r\nclip_image_build_graph: 378 518\r\nencode_image_with_clip: step 6 of 9 encoded in   117.35 ms\r\nclip_image_build_graph: 378 518\r\nencode_image_with_clip: step 7 of 9 encoded in   114.32 ms\r\nclip_image_build_graph: 378 518\r\nencode_image_with_clip: step 8 of 9 encoded in   114.95 ms\r\nclip_image_build_graph: 378 518\r\nencode_image_with_clip: step 9 of 9 encoded in   115.67 ms\r\nencode_image_with_clip: all 9 segments encoded in  1106.94 ms\r\nencode_image_with_clip: load_image_size 1594 1080\r\nencode_image_with_clip: image embedding created: 576 tokens\r\n\r\nencode_image_with_clip: image encoded in  1109.53 ms by CLIP (    1.93 ms per image patch)\r\nllama_model_loader: loaded meta data with 22 key-value pairs and 339 tensors from ../.cache/huggingface/hub/models--openbmb--MiniCPM-V-2_6-gguf/snapshots/69b9eaaebde4d5e2fafa1adb6a4169c349244cf6/ggml-model-Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\r\nllama_model_loader: - kv   1:                               general.name str              = model\r\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\r\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\r\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 3584\r\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 18944\r\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 28\r\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 4\r\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151666]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151666]  = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 151644\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 151645\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 128244\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\r\nllama_model_loader: - kv  21:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  141 tensors\r\nllama_model_loader: - type q4_K:  169 tensors\r\nllama_model_loader: - type q6_K:   29 tensors\r\nllm_load_vocab: special tokens cache size = 25\r\nllm_load_vocab: token to piece cache size = 0.9309 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = qwen2\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 151666\r\nllm_load_print_meta: n_merges         = 151387\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 3584\r\nllm_load_print_meta: n_layer          = 28\r\nllm_load_print_meta: n_head           = 28\r\nllm_load_print_meta: n_head_kv        = 4\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 7\r\nllm_load_print_meta: n_embd_k_gqa     = 512\r\nllm_load_print_meta: n_embd_v_gqa     = 512\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 18944\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 7.61 B\r\nllm_load_print_meta: model size       = 4.35 GiB (4.91 BPW) \r\nllm_load_print_meta: general.name     = model\r\nllm_load_print_meta: BOS token        = 151644 '<|im_start|>'\r\nllm_load_print_meta: EOS token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: UNK token        = 128244 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '!'\r\nllm_load_print_meta: LF token         = 148848 '\u00c4\u012c'\r\nllm_load_print_meta: EOT token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: max token length = 256\r\nllm_load_tensors: ggml ctx size =    0.30 MiB\r\nllm_load_tensors: offloading 28 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 29/29 layers to GPU\r\nllm_load_tensors:        CPU buffer size =   291.59 MiB\r\nllm_load_tensors:      CUDA0 buffer size =  4166.97 MiB\r\n....................................................................................\r\nllama_new_context_with_model: n_ctx      = 8192\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:      CUDA0 KV buffer size =   448.00 MiB\r\nllama_new_context_with_model: KV self size  =  448.00 MiB, K (f16):  224.00 MiB, V (f16):  224.00 MiB\r\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.58 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =   492.00 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =    23.01 MiB\r\nllama_new_context_with_model: graph nodes  = 986\r\nllama_new_context_with_model: graph splits = 2\r\n\r\nminicpmv_init: llava init in    10.14 ms.\r\nprocess_image: image token past: 0\r\nSegmentation fault (core dumped)\n```\n",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-09-11T15:23:13+00:00",
    "closed_at": "2024-09-11T15:52:14+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9436/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9436"
  },
  {
    "number": 9671,
    "title": "Bug: Initializing KV Cache Spikes Memory, Crashing on Android",
    "body": "### What happened?\n\nHi,\r\n\r\nYou may already know about the memory spike, given #7474.\r\n\r\nFor those unfamiliar, `ggml_backend_cpu_buffer_clear` calls `memset`, which initializes the allocated buffer (as big as 16 GiB for full context on Llama 3.1) to `0`s, spiking memory and, on Android, leading to a system crash --\r\n- If in Termux, Android kills it\r\n- If in `adb shell`, Android hangs and reboots\r\n\r\nAs far as I can tell, there are no guards for when `llama.cpp` might over-allocate _and_ over-initialize memory \u2014 this may be intended, but it seems to defeat the purpose of `mmap`.\r\n\r\nPlease share your perspective on this behavior; I understand it to be undefined. With limited experience, I see a number of potential solutions: \r\n1. Make `ggml_backend_buffer_clear` truly optional\r\n\t- Alternatively, skip it in certain environments\r\n2. Use `ggml_backend_cpu_buffer_memset_tensor` in the `alloc_tensor_range` loop instead to avoid bulk initialization, perhaps as part of `ggml_tallocr_alloc` or in a separate function\r\n3. Require `-c` in certain environments\r\n\r\nTo reproduce this behavior, build for Android and run `llama-cli` or `llama-simple` or `llama-server` with any quantization of Llama 3.1; the default behavior of `llama.cpp` without `-c` is to obtain the context from the model itself, which will load the full context in this case.\r\n\r\nI would be happy to implement a fix, whatever is decided. If instead downstream applications should manage this themselves, please clarify.\r\n\r\nThank you.\n\n### Name and Version\n\n### Termux\r\n```\r\n$ ./llama-simple --version\r\nversion: 3830 (b5de3b74)\r\nbuilt with clang version 18.1.8 for aarch64-unknown-linux-android29\r\n```\r\n\r\n### `adb shell`\r\n```\r\n$ LD_LIBRARY_PATH=lib ./llama-simple --version                                                  \r\nversion: 3830 (b5de3b74)\r\nbuilt with Android (8490178, based on r450784d) clang version 14.0.6 (https://android.googlesource.com/toolchain/llvm-project 4c603efb0cca074e9238af8b4106c30add4418f6) for x86_64-unknown-linux-gnu\r\n```\n\n### What operating system are you seeing the problem on?\n\nOther? (Please let us know in description)\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-09-27T22:08:13+00:00",
    "closed_at": "2024-09-29T20:47:14+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9671/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9671"
  },
  {
    "number": 10205,
    "title": "Bug: Metal bfloat kernel crash when using Swift package",
    "body": "### What happened?\n\nI encountered a crash in `ggml_metal_init` when the first bfloat kernel is added. Below is an explanation of what I believe is happening. My device is running macOS 15.0.1 and has an M3 Max.\r\n\r\nBecause the macOS platform version in `Package.swift` is `.v12`, the condition to disable bfloat, `__METAL_VERSION__ < 310`, will always be true. However, at runtime the device family is used to set `has_bfloat`. As a result, there can be a bfloat mismatch between the Swift package's compiled Metal library and the runtime flag. This leads to a crash when adding bfloat Metal kernels as they don't exist.\r\n\r\nSome ideas:\r\n1. Consider bumping the macOS platform version to `.v14` so consumers of the Swift package can use bfloat.\r\n2. Set `has_bfloat` by also checking for the existence of an empty kernel in the `MTLLibrary` that will only exist if the library was compiled with bfloat support.\n\n### Name and Version\n\nversion: 4043 (60e17ce2)\r\nbuilt with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.0.0\n\n### What operating system are you seeing the problem on?\n\nMac\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-11-07T17:32:24+00:00",
    "closed_at": "2024-11-08T19:59:47+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10205/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10205"
  },
  {
    "number": 8213,
    "title": "Bug: Docker ROCm crashs, only works on metal compiled.",
    "body": "### What happened?\n\nThe docker version with ROCm 5.6 exits after graph splits, I tried building and image with ROCm 5.6, 5.7.1, 6.1.2.\r\n\r\nThese last ones give me an error that is in the logs.\r\n\r\nIf I compiled and run it on Metal, it works flawlessly.\r\n\r\nI have been trying to run it with several version for the past 7 days.\n\n### Name and Version\n\nLatest build, always pulled from the last 7 days.\r\n\r\nSystem is Pop_Os 22.04\r\nROCm 6.1.2\r\nKernel 6.9.3\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\nllamacpp_1  | INFO [                    main] build info | tid=\"133799363425664\" timestamp=1719689759 build=0 commit=\"unknown\"\r\nllamacpp_1  | INFO [                    main] system info | tid=\"133799363425664\" timestamp=1719689759 n_threads=16 n_threads_batch=-1 total_threads=32 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \"\r\nllamacpp_1  | llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from /models/Mistral-7B-Instruct-v0.3-Q8_0.gguf (version GGUF V3 (latest))\r\nllamacpp_1  | llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllamacpp_1  | llama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllamacpp_1  | llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3\r\nllamacpp_1  | llama_model_loader: - kv   2:                          llama.block_count u32              = 32\r\nllamacpp_1  | llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\r\nllamacpp_1  | llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\r\nllamacpp_1  | llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllamacpp_1  | llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\r\nllamacpp_1  | llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\r\nllamacpp_1  | llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\r\nllamacpp_1  | llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllamacpp_1  | llama_model_loader: - kv  10:                          general.file_type u32              = 7\r\nllamacpp_1  | llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768\r\nllamacpp_1  | llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\r\nllamacpp_1  | llama_model_loader: - kv  13:            tokenizer.ggml.add_space_prefix bool             = true\r\nllamacpp_1  | llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\r\nllamacpp_1  | llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\r\nllamacpp_1  | llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\r\nllamacpp_1  | llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllamacpp_1  | llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\r\nllamacpp_1  | llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\r\nllamacpp_1  | llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\r\nllamacpp_1  | llama_model_loader: - kv  21:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllamacpp_1  | llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\r\nllamacpp_1  | llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\r\nllamacpp_1  | llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\r\nllamacpp_1  | llama_model_loader: - kv  25:               general.quantization_version u32              = 2\r\nllamacpp_1  | llama_model_loader: - type  f32:   65 tensors\r\nllamacpp_1  | llama_model_loader: - type q8_0:  226 tensors\r\nllamacpp_1  | llm_load_vocab: special tokens cache size = 1027\r\nllamacpp_1  | llm_load_vocab: token to piece cache size = 0.1731 MB\r\nllamacpp_1  | llm_load_print_meta: format           = GGUF V3 (latest)\r\nllamacpp_1  | llm_load_print_meta: arch             = llama\r\nllamacpp_1  | llm_load_print_meta: vocab type       = SPM\r\nllamacpp_1  | llm_load_print_meta: n_vocab          = 32768\r\nllamacpp_1  | llm_load_print_meta: n_merges         = 0\r\nllamacpp_1  | llm_load_print_meta: n_ctx_train      = 32768\r\nllamacpp_1  | llm_load_print_meta: n_embd           = 4096\r\nllamacpp_1  | llm_load_print_meta: n_head           = 32\r\nllamacpp_1  | llm_load_print_meta: n_head_kv        = 8\r\nllamacpp_1  | llm_load_print_meta: n_layer          = 32\r\nllamacpp_1  | llm_load_print_meta: n_rot            = 128\r\nllamacpp_1  | llm_load_print_meta: n_embd_head_k    = 128\r\nllamacpp_1  | llm_load_print_meta: n_embd_head_v    = 128\r\nllamacpp_1  | llm_load_print_meta: n_gqa            = 4\r\nllamacpp_1  | llm_load_print_meta: n_embd_k_gqa     = 1024\r\nllamacpp_1  | llm_load_print_meta: n_embd_v_gqa     = 1024\r\nllamacpp_1  | llm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllamacpp_1  | llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllamacpp_1  | llm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllamacpp_1  | llm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllamacpp_1  | llm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllamacpp_1  | llm_load_print_meta: n_ff             = 14336\r\nllamacpp_1  | llm_load_print_meta: n_expert         = 0\r\nllamacpp_1  | llm_load_print_meta: n_expert_used    = 0\r\nllamacpp_1  | llm_load_print_meta: causal attn      = 1\r\nllamacpp_1  | llm_load_print_meta: pooling type     = 0\r\nllamacpp_1  | llm_load_print_meta: rope type        = 0\r\nllamacpp_1  | llm_load_print_meta: rope scaling     = linear\r\nllamacpp_1  | llm_load_print_meta: freq_base_train  = 1000000.0\r\nllamacpp_1  | llm_load_print_meta: freq_scale_train = 1\r\nllamacpp_1  | llm_load_print_meta: n_ctx_orig_yarn  = 32768\r\nllamacpp_1  | llm_load_print_meta: rope_finetuned   = unknown\r\nllamacpp_1  | llm_load_print_meta: ssm_d_conv       = 0\r\nllamacpp_1  | llm_load_print_meta: ssm_d_inner      = 0\r\nllamacpp_1  | llm_load_print_meta: ssm_d_state      = 0\r\nllamacpp_1  | llm_load_print_meta: ssm_dt_rank      = 0\r\nllamacpp_1  | llm_load_print_meta: model type       = 7B\r\nllamacpp_1  | llm_load_print_meta: model ftype      = Q8_0\r\nllamacpp_1  | llm_load_print_meta: model params     = 7.25 B\r\nllamacpp_1  | llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \r\nllamacpp_1  | llm_load_print_meta: general.name     = Mistral-7B-Instruct-v0.3\r\nllamacpp_1  | llm_load_print_meta: BOS token        = 1 '<s>'\r\nllamacpp_1  | llm_load_print_meta: EOS token        = 2 '</s>'\r\nllamacpp_1  | llm_load_print_meta: UNK token        = 0 '<unk>'\r\nllamacpp_1  | llm_load_print_meta: LF token         = 781 '<0x0A>'\r\nllamacpp_1  | llm_load_print_meta: max token length = 48\r\nllamacpp_1  | ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nllamacpp_1  | ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nllamacpp_1  | ggml_cuda_init: found 1 ROCm devices:\r\nllamacpp_1  |   Device 0: Radeon RX 7900 XTX, compute capability 11.0, VMM: no\r\nllamacpp_1  | llm_load_tensors: ggml ctx size =    0.27 MiB\r\nllamacpp_1  | llm_load_tensors: offloading 32 repeating layers to GPU\r\nllamacpp_1  | llm_load_tensors: offloading non-repeating layers to GPU\r\nllamacpp_1  | llm_load_tensors: offloaded 33/33 layers to GPU\r\nllamacpp_1  | llm_load_tensors:      ROCm0 buffer size =  7209.02 MiB\r\nllamacpp_1  | llm_load_tensors:        CPU buffer size =   136.00 MiB\r\nllamacpp_1  | ...................................................................................................\r\nllamacpp_1  | llama_new_context_with_model: n_ctx      = 512\r\nllamacpp_1  | llama_new_context_with_model: n_batch    = 512\r\nllamacpp_1  | llama_new_context_with_model: n_ubatch   = 512\r\nllamacpp_1  | llama_new_context_with_model: flash_attn = 0\r\nllamacpp_1  | llama_new_context_with_model: freq_base  = 1000000.0\r\nllamacpp_1  | llama_new_context_with_model: freq_scale = 1\r\nllamacpp_1  | llama_kv_cache_init:      ROCm0 KV buffer size =    64.00 MiB\r\nllamacpp_1  | llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\r\nllamacpp_1  | llama_new_context_with_model:  ROCm_Host  output buffer size =     0.25 MiB\r\nllamacpp_1  | llama_new_context_with_model:      ROCm0 compute buffer size =    81.00 MiB\r\nllamacpp_1  | llama_new_context_with_model:  ROCm_Host compute buffer size =     9.01 MiB\r\nllamacpp_1  | llama_new_context_with_model: graph nodes  = 1030\r\nllamacpp_1  | llama_new_context_with_model: graph splits = 2\r\nllamacpp_1  | ggml_cuda_compute_forward: RMS_NORM failed\r\nllamacpp_1  | CUDA error: invalid device function\r\nllamacpp_1  |   current device: 0, in function ggml_cuda_compute_forward at ggml/src/ggml-cuda.cu:2285\r\nllamacpp_1  |   err\r\nllamacpp_1  | GGML_ASSERT: ggml/src/ggml-cuda.cu:100: !\"CUDA error\"\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-06-29T19:45:02+00:00",
    "closed_at": "2024-08-19T01:06:50+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8213/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8213"
  },
  {
    "number": 10307,
    "title": "Bug: [Regression] Cannot build with hipblas",
    "body": "### What happened?\n\nI can no longer build llama.cpp with hipblas enabled. The following dockerfile can be used to reproduce the issue:\r\n\r\n```\r\nFROM rocm/pytorch\r\n\r\nARG ROCM_TARGET_LST=/root/gfx\r\n\r\nRUN echo \"gfx1100\" > /root/gfx\r\n\r\nWORKDIR /root/\r\nRUN git clone https://github.com/ggerganov/llama.cpp.git\r\nWORKDIR /root/llama.cpp\r\nRUN make GGML_HIPBLAS=1 -j$(nproc)\r\n```\r\n\r\nThe relevant log output seems to be the following:\r\n\r\n```\r\nMK_CPPFLAGS += -DGGML_USE_CPU_AARCH64\r\nmake: MK_CPPFLAGS: No such file or directory\r\nmake: *** [Makefile:840: ggml/src/ggml-cuda/mmvq.o] Error 127\r\n```\r\n\r\nI bisected the issue, and this was found to be the first bad commit:\r\n\r\n```\r\n1607a5e5b08f4e55f118af3d7de325949d8f1835 is the first bad commit\r\ncommit 1607a5e5b08f4e55f118af3d7de325949d8f1835\r\nAuthor: Charles Xu <charles.xu@arm.com>\r\nDate:   Fri Nov 15 01:28:50 2024 +0100\r\n\r\n    backend cpu: add online flow for aarch64 Q4_0 GEMV/GEMM kernels (#9921)\r\n    \r\n    * backend-cpu: add online flow for aarch64 Q4_0 GEMV/GEMM kernels\r\n    \r\n    ---------\r\n    \r\n    Co-authored-by: Diego Devesa <slarengh@gmail.com>\r\n\r\n Makefile                             |   4 +\r\n ggml/CMakeLists.txt                  |   1 +\r\n ggml/include/ggml-cpu.h              |   3 +\r\n ggml/src/ggml-cpu/CMakeLists.txt     |   5 ++\r\n ggml/src/ggml-cpu/ggml-cpu-aarch64.c | 144 +++++++++++++++++++++++++++++++++++\r\n ggml/src/ggml-cpu/ggml-cpu-aarch64.h |   3 +\r\n ggml/src/ggml-cpu/ggml-cpu.c         |  23 +++---\r\n ggml/src/ggml-cpu/ggml-cpu.cpp       | 106 +++++++++++++++++++++++---\r\n src/llama.cpp                        |   2 +-\r\n 9 files changed, 271 insertions(+), 20 deletions(-)\r\n```\n\n### Name and Version\n\nIt doesn't build, but the first broken version is commit 1607a5e5b08f4e55f118af3d7de325949d8f1835\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-11-15T10:28:12+00:00",
    "closed_at": "2024-11-15T19:45:33+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10307/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10307"
  },
  {
    "number": 9241,
    "title": "Bug:  SYCL error",
    "body": "### What happened?\r\n\r\nNative API failed. Native API returns: -999 (Unknown PI error) -999 (Unknown PI error)\r\nException caught at file:D:/a/llama.cpp/llama.cpp/ggml/src/ggml-sycl.cpp, line:4207, func:operator()\r\nSYCL error: CHECK_TRY_ERROR((*stream).memcpy((char *)tensor->data + offset, host_buf, size) .wait()): Meet error in this line code!\r\n  in function ggml_backend_sycl_buffer_set_tensor at D:/a/llama.cpp/llama.cpp/ggml/src/ggml-sycl.cpp:4207\r\nD:\\a\\llama.cpp\\llama.cpp\\ggml\\src\\ggml-sycl\\common.hpp:107: SYCL error\r\n\r\n### Name and Version\r\n\r\nversion: 3616 (11b84eb4)\r\nbuilt with MSVC 19.40.33813.0 for\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nwindows11 23H2\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nActive code page: 65001\r\n:: initializing oneAPI environment...\r\n   Initializing Visual Studio command-line environment...\r\n   Visual Studio version 17.11.0 environment configured.\r\n   \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\\"\r\n   Visual Studio command-line environment initialized for: 'x64'\r\n:  advisor -- latest\r\n:  compiler -- latest\r\n:  dal -- latest\r\n:  debugger -- latest\r\n:  dev-utilities -- latest\r\n:  dnnl -- latest\r\n:  dpcpp-ct -- latest\r\n:  dpl -- latest\r\n:  ipp -- latest\r\n:  ippcp -- latest\r\n:  mkl -- latest\r\n:  ocloc -- latest\r\n:  tbb -- latest\r\n:  vtune -- latest\r\n:: oneAPI environment initialized ::\r\nINFO [                    main] build info | tid=\"22908\" timestamp=1724947942 build=3616 commit=\"11b84eb4\"\r\nINFO [                    main] system info | tid=\"22908\" timestamp=1724947942 n_threads=6 n_threads_batch=-1 total_threads=12 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \"\r\nINFO [                    main] HTTP server is listening | tid=\"22908\" timestamp=1724947942 hostname=\"0.0.0.0\" port=\"8080\" n_threads_http=\"11\"\r\nINFO [                    main] loading model | tid=\"22908\" timestamp=1724947942 hostname=\"0.0.0.0\" port=\"8080\" n_threads_http=\"11\"\r\nllama_model_loader: loaded meta data with 20 key-value pairs and 483 tensors from F:\\llama.cpp\\models\\qwen1.5-14b-vntl.Q4_K_S.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\r\nllama_model_loader: - kv   1:                               general.name str              = Qwen1.5-14B-Chat-jp2zh-fp16\r\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 40\r\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\r\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 5120\r\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 13696\r\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 40\r\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 40\r\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 14\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\r\nllama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151645\r\nllama_model_loader: - kv  16:            tokenizer.ggml.padding_token_id u32              = 151643\r\nllama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 151643\r\nllama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\r\nllama_model_loader: - kv  19:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  201 tensors\r\nllama_model_loader: - type q5_0:   35 tensors\r\nllama_model_loader: - type q5_1:    5 tensors\r\nllama_model_loader: - type q4_K:  237 tensors\r\nllama_model_loader: - type q5_K:    4 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: missing pre-tokenizer type, using: 'default'\r\nllm_load_vocab:\r\nllm_load_vocab: ************************************\r\nllm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!\r\nllm_load_vocab: CONSIDER REGENERATING THE MODEL\r\nllm_load_vocab: ************************************\r\nllm_load_vocab:\r\nllm_load_vocab: special tokens cache size = 421\r\nllm_load_vocab: token to piece cache size = 0.9352 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = qwen2\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 152064\r\nllm_load_print_meta: n_merges         = 151387\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 5120\r\nllm_load_print_meta: n_layer          = 40\r\nllm_load_print_meta: n_head           = 40\r\nllm_load_print_meta: n_head_kv        = 40\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 5120\r\nllm_load_print_meta: n_embd_v_gqa     = 5120\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 13696\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 13B\r\nllm_load_print_meta: model ftype      = Q4_K - Small\r\nllm_load_print_meta: model params     = 14.17 B\r\nllm_load_print_meta: model size       = 7.97 GiB (4.83 BPW)\r\nllm_load_print_meta: general.name     = Qwen1.5-14B-Chat-jp2zh-fp16\r\nllm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: EOS token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 148848 '\u00c4\u012c'\r\nllm_load_print_meta: EOT token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: max token length = 256\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 2 SYCL devices:\r\nllm_load_tensors: ggml ctx size =    0.64 MiB\r\nllm_load_tensors: offloading 40 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 41/41 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  5005.04 MiB\r\nllm_load_tensors:      SYCL1 buffer size =  2739.81 MiB\r\nllm_load_tensors:        CPU buffer size =   417.66 MiB\r\n.........................................................................................\r\nllama_new_context_with_model: n_ctx      = 32768\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 2 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |\r\n    |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |\r\n    |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|    1.5|    512|    1024|   32| 16704M|            1.3.30398|\r\n| 1| [level_zero:gpu:1]|                Intel Arc A750 Graphics|    1.5|    448|    1024|   32|  8319M|            1.3.30398|\r\nllama_kv_cache_init:      SYCL0 KV buffer size = 17920.00 MiB\r\nllama_kv_cache_init:      SYCL1 KV buffer size =  7680.00 MiB\r\nllama_new_context_with_model: KV self size  = 25600.00 MiB, K (f16): 12800.00 MiB, V (f16): 12800.00 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     1.16 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =  2664.00 MiB\r\nllama_new_context_with_model:      SYCL1 compute buffer size =  2664.00 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    74.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1406\r\nllama_new_context_with_model: graph splits = 3\r\nNative API failed. Native API returns: -999 (Unknown PI error) -999 (Unknown PI error)\r\nException caught at file:D:/a/llama.cpp/llama.cpp/ggml/src/ggml-sycl.cpp, line:4207, func:operator()\r\nSYCL error: CHECK_TRY_ERROR((*stream).memcpy((char *)tensor->data + offset, host_buf, size) .wait()): Meet error in this line code!\r\n  in function ggml_backend_sycl_buffer_set_tensor at D:/a/llama.cpp/llama.cpp/ggml/src/ggml-sycl.cpp:4207\r\nD:\\a\\llama.cpp\\llama.cpp\\ggml\\src\\ggml-sycl\\common.hpp:107: SYCL error\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-08-29T16:16:37+00:00",
    "closed_at": "2024-10-21T01:07:21+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9241/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9241"
  },
  {
    "number": 9587,
    "title": "Bug: passing `tfs_z` crashes the server",
    "body": "### What happened?\n\nIf you pass `tfs_z` param to the server, it crashes sometimes.\r\n\r\nStarting the server:\r\n```\r\n~/test/llama.cpp/llama-server -m /opt/models/text/gemma-2-27b-it-Q8_0.gguf --verbose\r\n```\r\n\r\n<details>\r\n  <summary>startup logs</summary>\r\n\r\n```\r\nbuild: 3802 (a5b57b08) with cc (Ubuntu 13.2.0-23ubuntu4) 13.2.0 for x86_64-linux-gnu\r\nsystem info: n_threads = 12, n_threads_batch = 12, total_threads = 24\r\n\r\nsystem_info: n_threads = 12 (n_threads_batch = 12) / 24 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 |\r\n\r\nmain: HTTP server is listening, hostname: 127.0.0.1, port: 8080, http threads: 23\r\nmain: loading model\r\nllama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /opt/models/text/gemma-2-27b-it-Q8_0.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = gemma2\r\nllama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\r\nllama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\r\nllama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\r\nllama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\r\nllama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\r\nllama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\r\nllama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\r\nllama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\r\nllama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 7\r\nllama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\r\nllama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\r\nllama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\r\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\r\nllama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\r\nllama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\r\nllama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\r\nllama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\r\nllama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\r\nllama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\r\nllama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\r\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/gemma-2-27b-it-GGUF/gemma...\r\nllama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\r\nllama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\r\nllama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\r\nllama_model_loader: - type  f32:  185 tensors\r\nllama_model_loader: - type q8_0:  323 tensors\r\nllm_load_vocab: special tokens cache size = 217\r\nllm_load_vocab: token to piece cache size = 1.6014 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = gemma2\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 256000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 8192\r\nllm_load_print_meta: n_embd           = 4608\r\nllm_load_print_meta: n_layer          = 46\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 16\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 4096\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 2\r\nllm_load_print_meta: n_embd_k_gqa     = 2048\r\nllm_load_print_meta: n_embd_v_gqa     = 2048\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 36864\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 8192\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 27B\r\nllm_load_print_meta: model ftype      = Q8_0\r\nllm_load_print_meta: model params     = 27.23 B\r\nllm_load_print_meta: model size       = 26.94 GiB (8.50 BPW)\r\nllm_load_print_meta: general.name     = gemma-2-27b-it\r\nllm_load_print_meta: BOS token        = 2 '<bos>'\r\nllm_load_print_meta: EOS token        = 1 '<eos>'\r\nllm_load_print_meta: UNK token        = 3 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<pad>'\r\nllm_load_print_meta: LF token         = 227 '<0x0A>'\r\nllm_load_print_meta: EOT token        = 107 '<end_of_turn>'\r\nllm_load_print_meta: max token length = 48\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 3060, compute capability 8.6, VMM: yes\r\nllm_load_tensors: ggml ctx size =    0.23 MiB\r\nllm_load_tensors: offloading 0 repeating layers to GPU\r\nllm_load_tensors: offloaded 0/47 layers to GPU\r\nllm_load_tensors:        CPU buffer size = 27591.06 MiB\r\n..............................................................................................\r\nllama_new_context_with_model: n_ctx      = 8192\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:  CUDA_Host KV buffer size =  2944.00 MiB\r\nllama_new_context_with_model: KV self size  = 2944.00 MiB, K (f16): 1472.00 MiB, V (f16): 1472.00 MiB\r\nllama_new_context_with_model:  CUDA_Host  output buffer size =     1.95 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =  1704.31 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =    41.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1850\r\nllama_new_context_with_model: graph splits = 602\r\nllama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\nsrv          init: initializing slots, n_slots = 1\r\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 8192\r\nslot        reset: id  0 | task -1 |\r\nmain: model loaded\r\nmain: chat template, built_in: 1, chat_example: '<start_of_turn>user\r\nYou are a helpful assistant\r\n\r\nHello<end_of_turn>\r\n<start_of_turn>model\r\nHi there<end_of_turn>\r\n<start_of_turn>user\r\nHow are you?<end_of_turn>\r\n<start_of_turn>model\r\n\r\n'main: server is listening on 127.0.0.1:8080 - starting the main loop\r\nque    start_loop: processing new tasks\r\nque    start_loop: update slots\r\nsrv  update_slots: all slots are idle\r\nsrv  kv_cache_cle: clearing KV cache\r\nque    start_loop: waiting for new tasks\r\n```\r\n</details>\r\n\r\nRequest with `tfs_z`:\r\n```\r\ncurl --data '{\"prompt\": \"I see\", \"n_predict\": 2, \"tfs_z\": 0.9}' http://127.0.0.1:8080/completion\r\n```\r\n\r\n### Failure logs\r\n\r\n```\r\nsrv  add_waiting_: add task 0 to waiting list. current waiting = 0 (before add)\r\nque          post: new task, id = 0/1, front = 0\r\nque    start_loop: processing new tasks\r\nque    start_loop: processing task, id = 0\r\nslot get_availabl: id  0 | task -1 | selected slot by lru, t_last = -1\r\nslot        reset: id  0 | task -1 |\r\nslot launch_slot_: id  0 | task 0 | processing task\r\nque    start_loop: update slots\r\nsrv  update_slots: posting NEXT_RESPONSE\r\nque          post: new task, id = 1, front = 0\r\nslot update_slots: id  0 | task 0 | tokenizing prompt, len = 1\r\nslot update_slots: id  0 | task 0 | prompt tokenized, n_ctx_slot = 8192, n_keep = 0, n_prompt_tokens = 3\r\nslot update_slots: id  0 | task 0 | kv cache rm [0, end)\r\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 3, n_tokens = 3, progress = 1.000000\r\nslot update_slots: id  0 | task 0 | prompt done, n_past = 3, n_tokens = 3\r\nsrv  update_slots: decoding batch, n_tokens = 3\r\nslot process_toke: id  0 | task 0 | n_decoded = 1, n_remaining = 1, next token: ' a'\r\nsrv  update_slots: run slots completed\r\nque    start_loop: waiting for new tasks\r\nque    start_loop: processing new tasks\r\nque    start_loop: processing task, id = 1\r\nque    start_loop: update slots\r\nsrv  update_slots: posting NEXT_RESPONSE\r\nque          post: new task, id = 2, front = 0\r\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 8192, n_past = 4, n_system_tokens = 0, n_cache_tokens = 0, truncated = 0\r\nsrv  update_slots: decoding batch, n_tokens = 1\r\nsrc/llama-sampling.cpp:66: GGML_ASSERT(cur_p->size > 0) failed\r\nCould not attach to process.  If your uid matches the uid of the target\r\nprocess, check the setting of /proc/sys/kernel/yama/ptrace_scope, or try\r\nagain as the root user.  For more details, see /etc/sysctl.d/10-ptrace.conf\r\nptrace: Operation not permitted.\r\nNo stack.\r\nThe program is not being run.\r\nfish: Job 1, '~/test/llama.cpp/llama-server -\u2026' terminated by signal SIGABRT (Abort)\r\n```\r\n\r\nIt may not crash on the first request. It may take up to 10 requests sometimes. I tested it with different models, and with CUDA/no-CUDA builds.\n\n### Name and Version\n\n\u276f ./llama-server --version\r\nversion: 3802 (a5b57b08)\r\nbuilt with cc (Ubuntu 13.2.0-23ubuntu4) 13.2.0 for x86_64-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-09-22T08:39:17+00:00",
    "closed_at": "2024-11-07T01:07:17+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9587/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9587"
  },
  {
    "number": 9540,
    "title": "Bug: [SYCL] silently failed on windows ",
    "body": "### What happened?\n\nI tried the latest release  llama-b3785-bin-win-sycl-x64.zip and it failed silently. vulkan and avx-512 versions are ok\r\n\r\nThe recommend release llama-b3038-bin-win-sycl-x64.zip is ok \r\n\r\n\n\n### Name and Version\n\nllama-b3785-bin-win-sycl-x64\n\n### What operating system are you seeing the problem on?\n\nWindows\n\n### Relevant log output\n\n```shell\nI don't know anything about programming but I tried gdb ( msys ) and got this message : \r\n\r\n(gdb) run\r\nStarting program: llama-bench.exe -h\r\n[New Thread 8868.0xe98]\r\n[New Thread 8868.0x36e4]\r\n[New Thread 8868.0xa58]\r\n\r\nThread 1 received signal SIGSEGV, Segmentation fault.\r\n0x00007ffb37e53020 in _Thrd_yield () from C:\\WINDOWS\\SYSTEM32\\msvcp140.dll\r\n\r\nDoes this help for the bug I had ?\n```\n",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-09-18T18:31:06+00:00",
    "closed_at": "2024-09-18T19:07:31+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9540/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9540"
  }
]
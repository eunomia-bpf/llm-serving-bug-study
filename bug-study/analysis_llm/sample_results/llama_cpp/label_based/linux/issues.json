[
  {
    "number": 702,
    "title": "4bit 65B model overflow 64GB of RAM",
    "body": "# Prerequisites\r\n\r\nI am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\nI carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\nI searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\nI reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\nDuring inference, there should be no or minimum disk activities going on, and disk should not be a bottleneck once pass the model loading stage.\r\n\r\n# Current Behavior\r\nMy disk should have a continuous reading speed of over 100MB/s, however, during the loading of the model, it only loads at around 40MB/s. After this very slow loading of Llama 65b model (converted from GPTQ with group size of 128), llama.cpp start to inference, however during the inference the programme continue to occupy the disk and reads at 40MB/s. The generation speed is also extremely slow, at around 10 minutes per token.\r\nHowever, if it's 30b model or smaller, llama.cpp work as expected.\r\n\r\n# Environment and Context \r\n\r\nNote: My interfercing were done using oobabooga's [text-generation-webui](https://github.com/oobabooga/text-generation-webui)'s implementation of llama.cpp, as I have no idea how to use llama.cpp by itself...\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\nCPU: Ryzen 5500\r\n    Flags: \r\n```\r\n                         fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mc\r\n                         a cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall n\r\n                         x mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_go\r\n                         od nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl p\r\n                         ni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2api\r\n                         c movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_le\r\n                         gacy svm extapic cr8_legacy abm sse4a misalignsse 3dnow\r\n                         prefetch osvw ibs skinit wdt tce topoext perfctr_core p\r\n                         erfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw\r\n                         _pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 \r\n                         avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap c\r\n                         lflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cq\r\n                         m_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero \r\n                         irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm\r\n                         _lock nrip_save tsc_scale vmcb_clean flushbyasid decode\r\n                         assists pausefilter pfthreshold avic v_vmsave_vmload vg\r\n                         if v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid ove\r\n                         rflow_recov succor smca fsrm\r\n```\r\nRAM: 64GB of DDR4 running at 3000MHz\r\nDisk where I stored my model file: 2 Barraccuda 1TB HDD in Raid 1 configuration\r\nSystem SSD: NV2 500GB\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`Linux fgdfgfthgr-MS-7C95 5.15.0-69-generic #76-Ubuntu SMP Fri Mar 17 17:19:29 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux`\r\n\r\n* SDK version, e.g. for Linux:\r\n```\r\nPython 3.9.13\r\nGNU Make 4.3\r\ng++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nNot sure what other information is there to provide.\r\n\r\n# Steps to Reproduce\r\n\r\n1. Load a 65b model using oobabooga's [text-generation-webui](https://github.com/oobabooga/text-generation-webui)'s implementation of llama.cpp.\r\n2. Use `iostat -y -d 5` to monitor disk activity during loading and inference.\r\n\r\n# Failure Logs\r\n\r\nLlama.cpp version: \r\n```\r\nhttps://pypi.org/project/llamacpp/\r\n0.1.11\r\n```\r\n\r\nPip environment:\r\n```\r\naccelerate               0.18.0\r\naiofiles                 23.1.0\r\naiohttp                  3.8.4\r\naiosignal                1.3.1\r\naltair                   4.2.2\r\nanyio                    3.6.2\r\nasync-timeout            4.0.2\r\nattrs                    22.2.0\r\nbitsandbytes             0.37.2\r\ncertifi                  2022.12.7\r\ncharset-normalizer       3.1.0\r\nclick                    8.1.3\r\ncmake                    3.26.1\r\ncontourpy                1.0.7\r\ncycler                   0.11.0\r\ndatasets                 2.11.0\r\ndill                     0.3.6\r\nentrypoints              0.4\r\nfastapi                  0.95.0\r\nffmpy                    0.3.0\r\nfilelock                 3.10.7\r\nflexgen                  0.1.7\r\nfonttools                4.39.3\r\nfrozenlist               1.3.3\r\nfsspec                   2023.3.0\r\ngradio                   3.24.0\r\ngradio_client            0.0.5\r\nh11                      0.14.0\r\nhttpcore                 0.16.3\r\nhttpx                    0.23.3\r\nhuggingface-hub          0.13.3\r\nidna                     3.4\r\nJinja2                   3.1.2\r\njsonschema               4.17.3\r\nkiwisolver               1.4.4\r\nlinkify-it-py            2.0.0\r\nlit                      16.0.0\r\nllamacpp                 0.1.11\r\nMarkdown                 3.4.3\r\nmarkdown-it-py           2.2.0\r\nMarkupSafe               2.1.2\r\nmatplotlib               3.7.1\r\nmdit-py-plugins          0.3.3\r\nmdurl                    0.1.2\r\nmpmath                   1.3.0\r\nmultidict                6.0.4\r\nmultiprocess             0.70.14\r\nnetworkx                 3.0\r\nnumpy                    1.24.2\r\nnvidia-cublas-cu11       11.10.3.66\r\nnvidia-cuda-cupti-cu11   11.7.101\r\nnvidia-cuda-nvrtc-cu11   11.7.99\r\nnvidia-cuda-runtime-cu11 11.7.99\r\nnvidia-cudnn-cu11        8.5.0.96\r\nnvidia-cufft-cu11        10.9.0.58\r\nnvidia-curand-cu11       10.2.10.91\r\nnvidia-cusolver-cu11     11.4.0.1\r\nnvidia-cusparse-cu11     11.7.4.91\r\nnvidia-nccl-cu11         2.14.3\r\nnvidia-nvtx-cu11         11.7.91\r\norjson                   3.8.9\r\npackaging                23.0\r\npandas                   1.5.3\r\npeft                     0.2.0\r\nPillow                   9.4.0\r\npip                      23.0.1\r\npsutil                   5.9.4\r\nPuLP                     2.7.0\r\npyarrow                  11.0.0\r\npydantic                 1.10.7\r\npydub                    0.25.1\r\npyparsing                3.0.9\r\npyrsistent               0.19.3\r\npython-dateutil          2.8.2\r\npython-multipart         0.0.6\r\npytz                     2023.3\r\nPyYAML                   6.0\r\nquant-cuda               0.0.0\r\nregex                    2023.3.23\r\nrequests                 2.28.2\r\nresponses                0.18.0\r\nrfc3986                  1.5.0\r\nrwkv                     0.7.1\r\nsafetensors              0.3.0\r\nsemantic-version         2.10.0\r\nsentencepiece            0.1.97\r\nsetuptools               65.6.3\r\nsix                      1.16.0\r\nsniffio                  1.3.0\r\nstarlette                0.26.1\r\nsympy                    1.11.1\r\ntokenizers               0.13.2\r\ntoolz                    0.12.0\r\ntorch                    2.0.0\r\ntorchaudio               2.0.1\r\ntorchvision              0.15.1\r\ntqdm                     4.65.0\r\ntransformers             4.28.0.dev0\r\ntriton                   2.0.0\r\ntyping_extensions        4.5.0\r\nuc-micro-py              1.0.1\r\nurllib3                  1.26.15\r\nuvicorn                  0.21.1\r\nwebsockets               10.4\r\nwheel                    0.38.4\r\nxxhash                   3.2.0\r\nyarl                     1.8.2\r\n(textgen) fgdfgfthgr@fgdfgfthgr-MS-7C95:/mnt/7018F20D48B6C548/text-generation-webui$ y-it-py            2.0.0\r\nlit                      16.0.0\r\nllamacpp                 0.1.11\r\nMarkdown                 3.4.3\r\nmarkdown-it-py           2.2.0\r\nMarkupSafe               2.1.2\r\nmatplotlib               3.7.1\r\nmdit-py-plugins          0.3.3\r\nmdurl                    0.1.2\r\nmpmath                   1.3.0\r\n```\r\n\r\nmd5sum ggml-model-q4_0.bin\r\n3073a8eedd1252063ad9b440af7c90cc  ggml-model-q4_1.bin\r\n",
    "labels": [
      "need more info",
      "performance",
      "linux"
    ],
    "state": "closed",
    "created_at": "2023-04-02T08:37:42+00:00",
    "closed_at": "2023-04-19T08:20:48+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/702/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/702"
  }
]
[
  {
    "number": 8893,
    "title": "Bug: High CPU usage and bad output with flash attention on ROCm",
    "body": "### What happened?\r\n\r\nWhile flash attention works well for my python ui (https://github.com/curvedinf/dir-assistant/) on an nvidia system, it produces bad results on my AMD system. My AMD system has a 7900XT with ROCm 6.1.2 on Ubuntu 22.04. When FA is off, inference is almost instant with high t/s and no CPU usage. When FA is on, the CPU utilization is 100% for 1-2 minutes, and then tokens are generated slowly and are incorrect (in my case, it always produces \"################################\" for a long time).\r\n\r\n### Name and Version\r\n\r\nUsing python bindings via llama-cpp-python 0.2.84. Readme says llama.cpp version is [ggerganov/llama.cpp@4730faca618ff9cee0780580145e3cbe86f24876](https://github.com/ggerganov/llama.cpp/commit/4730faca618ff9cee0780580145e3cbe86f24876)\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nLinux\r\n\r\n### Relevant log output\r\n\r\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "medium severity",
      "3rd party"
    ],
    "state": "closed",
    "created_at": "2024-08-06T18:02:29+00:00",
    "closed_at": "2024-10-06T01:07:37+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8893/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8893"
  },
  {
    "number": 8269,
    "title": "Bug: Model generating result sometimes but most of the times it doesnt ",
    "body": "### What happened?\n\nSo, I am currently working with the local 'mistral-7b-q4' gguf model using 'llamacpp'. Although I can confirm that the model is active on the server but still I have encountered some issues during testing. Specifically, when I provide a small prompt , the model occasionally generates a response, but more often than not , it produces an empty string for the same prompt.\r\n\r\nAt this stage, I am unsure whether this behaviour is a result of the latest update, an expected characteristic of the model, or if there might be an error in my approach. \r\n\r\nThis is how I am calling the model:\r\n\r\n`\r\n\r\n# Load LLM\r\nllm = LlamaCpp(\r\n \u00a0\u00a0 model_path=\"Models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\r\n \u00a0\u00a0 n_gpu_layers=-1,\r\n \u00a0\u00a0 temperature=0.1,\r\n \u00a0\u00a0 top_p=0.7,\r\n \u00a0\u00a0 n_ctx=16000,\r\n \u00a0\u00a0 max_tokens=4096,\r\n \u00a0\u00a0 frequency_penalty=0.2,\r\n \u00a0\u00a0 presence_penalty=0.5,\r\n \u00a0\u00a0 stop=[\"\\n\"]\r\n)`\r\n\r\nthese are the outputs I am getting. First one is without a response and second one is with the reponse. \r\n![MicrosoftTeams-image (2)](https://github.com/ggerganov/llama.cpp/assets/70738578/4cce310f-7792-440d-add7-00e971584e08)\r\n![MicrosoftTeams-image (1)](https://github.com/ggerganov/llama.cpp/assets/70738578/2f3bb18b-8240-436e-9d97-48e6623f5bf6)\r\n\r\nSpecs:\r\n![MicrosoftTeams-image (3)](https://github.com/ggerganov/llama.cpp/assets/70738578/8e076a4a-661f-4a80-afaf-f50d4a4fd18b)\r\n\r\nUsed this command to install llama:\r\n`CMAKE_ARGS=\"-DLLAMA_CUDA=on\" FORCE_CMAKE=1 pip install llama-cpp-python`\r\n\r\nearlier I was using this(bewlo) one but recently after new update I am facing issues in installation as `DLLAMA-CUBLAS` is depreciated and not supported anymore.\r\n`CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python`\r\n\n\n### Name and Version\n\nllam_cpp_python: 0.2.81\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "medium severity",
      "3rd party"
    ],
    "state": "closed",
    "created_at": "2024-07-03T06:24:30+00:00",
    "closed_at": "2024-07-03T08:27:34+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8269/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8269"
  },
  {
    "number": 7885,
    "title": "Bug: CUDA error: out of memory - Phi-3 Mini 128k prompted with 20k+ tokens on 4GB GPU",
    "body": "### What happened?\n\nI get a CUDA out of memory error when sending large prompt (about 20k+ tokens) to Phi-3 Mini 128k model on laptop with Nvidia A2000 4GB RAM. At first about 3.3GB GPU RAM and 8GB CPU RAM is used by ollama, then the GPU ram usage slowly rises (3.4, 3.5GB etc.) and after about a minute it throws the error when GPU ram is exhaused probably (3.9GB is latest in task manager). The inference does not return any token (as answer) before crashing. Attaching server log. Using on Win11 + Ollama 0.1.42 + VS Code (1.90.0) + Continue plugin (v0.8.40).\r\n\r\nThe expected behavior would be not crashing and maybe rellocating the memory somehow so that GPU memory does not get exhausted. I want to disable GPU usage in ollama (to test for CPU inference only - I have 64GB RAM) but I am not able to find out how to turn the GPU off (even though I saw there is a command for it recently - am not able to find it again).\r\n\r\nActual error:\r\n```\r\nCUDA error: out of memory\r\n  current device: 0, in function alloc at C:\\a\\ollama\\ollama\\llm\\llama.cpp\\ggml-cuda.cu:375\r\n  cuMemSetAccess(pool_addr + pool_size, reserve_size, &access, 1)\r\nGGML_ASSERT: C:\\a\\ollama\\ollama\\llm\\llama.cpp\\ggml-cuda.cu:100: !\"CUDA error\"\r\n```\r\nThis is reported via Ollama and full logs are in the issue there: https://github.com/ollama/ollama/issues/4985\n\n### Name and Version\n\nSee linked ollama issue.\n\n### What operating system are you seeing the problem on?\n\nWindows\n\n### Relevant log output\n\n```shell\nSee linked ollama issue.\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "3rd party"
    ],
    "state": "closed",
    "created_at": "2024-06-11T19:50:26+00:00",
    "closed_at": "2024-08-01T01:07:07+00:00",
    "comments": 28,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7885/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7885"
  },
  {
    "number": 7779,
    "title": "Bug:  Error while running model file (.gguf ) in LM Studio",
    "body": "### What happened?\n\nI'm encountering an error while trying to run a model in LM Studio. Below are the details of the error:\r\n\r\n{\r\n  \"title\": \"Failed to load model\",\r\n  \"cause\": \"llama.cpp error: 'error loading model architecture: unknown model architecture: 'clip''\",\r\n  \"errorData\": {\r\n    \"n_ctx\": 2048,\r\n    \"n_batch\": 512,\r\n    \"n_gpu_layers\": 10\r\n  },\r\n  \"data\": {\r\n    \"memory\": {\r\n      \"ram_capacity\": \"15.92 GB\",\r\n      \"ram_unused\": \"7.46 GB\"\r\n    },\r\n    \"gpu\": {\r\n      \"gpu_names\": [\r\n        \"NVIDIA GeForce 940MX\"\r\n      ],\r\n      \"vram_recommended_capacity\": \"2.00 GB\",\r\n      \"vram_unused\": \"1.64 GB\"\r\n    },\r\n    \"os\": {\r\n      \"platform\": \"win32\",\r\n      \"version\": \"10.0.22631\",\r\n      \"supports_avx2\": true\r\n    },\r\n    \"app\": {\r\n      \"version\": \"0.2.24\",\r\n      \"downloadsDir\": \"C:\\\\Users\\\\hp\\\\.cache\\\\lm-studio\\\\models\"\r\n    },\r\n    \"model\": {}\r\n  }\r\n}\r\n\r\n\r\n![image](https://github.com/ggerganov/llama.cpp/assets/82998682/f408d74f-dee0-4347-983f-1c7d6c8e87e3)\r\n\n\n### Name and Version\n\nWindows 11 \n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity",
      "3rd party"
    ],
    "state": "closed",
    "created_at": "2024-06-05T20:15:20+00:00",
    "closed_at": "2024-07-21T01:07:09+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7779/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7779"
  }
]
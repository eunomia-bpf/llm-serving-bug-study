[
  {
    "number": 9009,
    "title": "Feature Request: Support Falcon Mamba 7B ",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nPlease support Falcon Mamba 7B from TII (Technology Innovation Institute TII - UAE)\n\n### Motivation\n\nSupport for all models is helpful.\r\n\r\nMy acid test for whether a model will run is to try and make a quant using \"gruff my repo\".\r\n\r\nAdmittedly it is hot off the presses yet it ought to run at least in theory, but it doesn't.\r\n```\r\nError: Error converting to fp16: b'INFO:hf-to-gguf:Loading model: falcon-mamba-7b\\nERROR:hf-to-gguf:Model FalconMambaForCausalLM is not supported\\n'\r\n```\n\n### Possible Implementation\n\nThey discuss an implementation here: https://falconllm.tii.ae/tii-releases-first-sslm-with-falcon-mamba-7b.html\r\n\r\nAny functional mamba or mamba 2 models would be great, but this one is slightly changed.",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2024-08-12T16:29:58+00:00",
    "closed_at": "2024-08-21T08:06:37+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9009/reactions",
      "total_count": 10,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 3,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9009"
  },
  {
    "number": 8240,
    "title": "Investigate gemma 2 generation quality",
    "body": "Initial reports can be seen from https://github.com/ggerganov/llama.cpp/pull/8227\r\n\r\n> [!IMPORTANT]  \r\n> A note for everyone: if you think there's a bug in llama.cpp tokenizer, please make sure to test with HF `transformers` library first (see [this comment](https://github.com/ggerganov/llama.cpp/issues/8240#issuecomment-2212444937) for example)",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-01T16:52:28+00:00",
    "closed_at": "2024-10-16T01:11:07+00:00",
    "comments": 90,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8240/reactions",
      "total_count": 7,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8240"
  },
  {
    "number": 6368,
    "title": "Support for 2-bit Quantized Llama-2-7b-chat-hf_2bitgs8_hqq Model",
    "body": "I would like to propose the integration of a novel model, \"Llama-2-7b-chat-hf_2bitgs8_hqq,\" available on Hugging Face. This model represents an innovative approach to quantization, employing a 2-bit quantized version of Llama2-7B-chat, enhanced with a low-rank adapter (HQQ+), to improve performance and efficiency.\r\n\r\nKey Features:\r\n- **Quantization**: The model leverages 2-bit quantization, significantly reducing VRAM requirements.\r\n- **Low-Rank Adapter**: Utilizes HQQ+, a low-rank adapter for performance enhancement.\r\n- **Efficiency**: Offloads meta-data to CPU, optimizing GPU memory usage.\r\n- **Datasets**: Trained on a mixture of general and specialized datasets, showing robustness and versatility.\r\n\r\nThe inclusion of this model could greatly benefit llama.cpp users by offering a more memory-efficient yet powerful option for large-scale text generation tasks. It could especially be beneficial for environments with limited hardware resources.\r\n\r\nThank you for considering this addition.\r\n\r\n[Link to the model on Hugging Face](https://huggingface.co/mobiuslabsgmbh/Llama-2-7b-chat-hf_2bitgs8_hqq)\r\n\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-28T14:15:03+00:00",
    "closed_at": "2024-05-14T01:31:12+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6368/reactions",
      "total_count": 14,
      "+1": 14,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6368"
  },
  {
    "number": 9227,
    "title": "Feature Request: Paligemma Support",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nAdding support for converting Google's multimodal Paligemma model to gguf in order to be used in ollama.\n\n### Motivation\n\nI have a personal project that requires a multimodal llm running locally and llava seems to be kind of...not great. I have seen an issue like this marked as open, but as of now, I still get an error when trying to convert from hf to gguf.\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-28T22:01:53+00:00",
    "closed_at": "2024-11-27T01:07:42+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9227/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9227"
  },
  {
    "number": 6417,
    "title": "Performance decreated between tag b1500 and b2581 on Windows ARM64 PC",
    "body": "Hi LLAMA team, \r\n\r\nI use llama tag b2581 on Windows ARM64 PC, the performance is more lower than previous tag b1500. Please refer to below detailed information. What is the reason? Please help on this issue. \r\n\r\nThanks a lot!\r\n\r\n**[Detailed information]**\r\n\r\n**Command:**\r\nmain.exe -m llama-2-7b-chat.ggufv3.q4_0.bin --color  --ctx_size 2048 -n -1 -ins -b 256 --top_k 10000 --temp 0.2 --repeat_penalty 1.1 -t 10\r\n\r\n**Prompt:** I have 3 years of experience as a software developer. Now I got bored with coding and want to transition to another career. My education qualifications are B. Tech in computer science, and I am well-versed in understanding the business side of software as well. Suggest a list of career options that are easy for me to transition.\r\n\r\n\r\n**system_info:** n_threads = 10 / 12 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 |\r\n\r\n**Tag b1500 results:**\r\nllama_print_timings:        load time =     723.53 ms\r\nllama_print_timings:      sample time =     925.29 ms /   624 runs   (    1.48 ms per token,   674.38 tokens per second)\r\nllama_print_timings: prompt eval time =    2583.12 ms /    91 tokens (   28.39 ms per token,    **35.23 tokens** per second)\r\nllama_print_timings:        eval time =   31693.17 ms /   625 runs   (   50.71 ms per token,    **19.72 tokens** per second)\r\nllama_print_timings:       total time =   51797.58 ms\r\n\r\n**Tag b2581 results:**\r\nllama_print_timings:        load time =     963.25 ms\r\nllama_print_timings:      sample time =     416.14 ms /   586 runs   (    0.71 ms per token,  1408.17 tokens per second)\r\nllama_print_timings: prompt eval time =   11847.94 ms /    94 tokens (  126.04 ms per token,     **7.93 tokens** per second)\r\nllama_print_timings:        eval time =   68542.50 ms /   585 runs   (  117.17 ms per token,     **8.53 tokens** per second)\r\nllama_print_timings:       total time =   82696.57 ms /   679 tokens\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-01T03:20:36+00:00",
    "closed_at": "2024-07-08T01:06:56+00:00",
    "comments": 54,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6417/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6417"
  },
  {
    "number": 13218,
    "title": "Feature Request: XiaomiMiMo/MiMo-7B-RL",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nAdd support of XiaomiMiMo/MiMo-7B-RL https://huggingface.co/XiaomiMiMo/MiMo-7B-RL\n\n### Motivation\n\nModel MiMoForCausalLM is not supported,Hope to further enrich the ecosystem.\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-04-30T17:17:04+00:00",
    "closed_at": "2025-06-27T01:08:01+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13218/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13218"
  },
  {
    "number": 6712,
    "title": "truly opensource model called olmo",
    "body": "Build with truly open dataset and fully open-source model can this be supported in olllama thanks.\r\nhttps://allenai.org/olmo\r\nhttps://huggingface.co/allenai/OLMo-7B\r\n",
    "labels": [
      "enhancement",
      "model"
    ],
    "state": "closed",
    "created_at": "2024-04-16T23:43:40+00:00",
    "closed_at": "2024-05-07T19:39:44+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6712/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6712"
  },
  {
    "number": 5079,
    "title": " Intel\u00ae Core\u2122 Ultra processors NPU  Support ",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\n\r\n Intel\u00ae Core\u2122 Ultra processors now has released  , how can llama.cpp use that npu to fast up \r\n\r\n# Motivation\r\n\r\n Intel\u00ae Core\u2122 Ultra processors deliver three dedicated engines (CPU, GPU, and NPU) to help unlock the power of AI\r\nhttps://www.intel.com/content/www/us/en/products/docs/processors/core-ultra/core-ultra-series-1-product-brief.html\r\n\r\n\r\n# Possible Implementation\r\n\r\nOpenVINO\u2122, WindowsML, DirectML, ONNX RT\r\n",
    "labels": [
      "enhancement"
    ],
    "state": "open",
    "created_at": "2024-01-22T14:15:28+00:00",
    "closed_at": null,
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5079/reactions",
      "total_count": 44,
      "+1": 43,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5079"
  },
  {
    "number": 3808,
    "title": "When I used the tool to quantify the chatglm model, the following error was reported",
    "body": "\r\nWhen I used the tool to quantify the chatglm model, the following error was reported. May I ask if the format of the specified model does not match? Is there a way to solve this problem?\r\n\r\n\r\n3:~/llama.cpp$ ./quantize MODEL/chatglm/chatGLM2-6B/pytorch_model-00001-of-00007.bin MODEL/chatglm/\r\n                                             python convert.py MODEL/chatglm/chatGLM2-6B/\r\nLoading model file MODEL/chatglm/chatGLM2-6B/pytorch_model-00001-of-00007.bin\r\nLoading model file MODEL/chatglm/chatGLM2-6B/pytorch_model-00001-of-00007.bin\r\nLoading model file MODEL/chatglm/chatGLM2-6B/pytorch_model-00002-of-00007.bin\r\nLoading model file MODEL/chatglm/chatGLM2-6B/pytorch_model-00003-of-00007.bin\r\nLoading model file MODEL/chatglm/chatGLM2-6B/pytorch_model-00004-of-00007.bin\r\nLoading model file MODEL/chatglm/chatGLM2-6B/pytorch_model-00005-of-00007.bin\r\nLoading model file MODEL/chatglm/chatGLM2-6B/pytorch_model-00006-of-00007.bin\r\nLoading model file MODEL/chatglm/chatGLM2-6B/pytorch_model-00007-of-00007.bin\r\nTraceback (most recent call last):\r\n  File \"/home/aistudio/llama.cpp/convert.py\", line 1202, in <module>\r\n    main()\r\n  File \"/home/aistudio/llama.cpp/convert.py\", line 1135, in main\r\n    model_plus = load_some_model(args.model)\r\n  File \"/home/aistudio/llama.cpp/convert.py\", line 1056, in load_some_model\r\n    model_plus = merge_multifile_models(models_plus)\r\n  File \"/home/aistudio/llama.cpp/convert.py\", line 589, in merge_multifile_models\r\n    model = merge_sharded([mp.model for mp in models_plus])\r\n  File \"/home/aistudio/llama.cpp/convert.py\", line 568, in merge_sharded\r\n    return {name: convert(name) for name in names}\r\n  File \"/home/aistudio/llama.cpp/convert.py\", line 568, in <dictcomp>\r\n    return {name: convert(name) for name in names}\r\n  File \"/home/aistudio/llama.cpp/convert.py\", line 543, in convert\r\n    lazy_tensors: list[LazyTensor] = [model[name] for model in models]\r\n  File \"/home/aistudio/llama.cpp/convert.py\", line 543, in <listcomp>\r\n    lazy_tensors: list[LazyTensor] = [model[name] for model in models]\r\nKeyError: 'transformer.embedding.word_embeddings.weight'",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-10-27T02:51:16+00:00",
    "closed_at": "2024-05-12T01:35:21+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3808/reactions",
      "total_count": 5,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3808"
  },
  {
    "number": 14048,
    "title": "Feature Request: Speculative Decoding \"acceptance rate\" should not count drafts that were skipped via the \" ignore small drafts\" clause",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nI think the `slot.n_draft_total += draft.size()` should go after the \"ignore small drafts\" test here:\n\n```cpp\n                llama_tokens draft = common_speculative_gen_draft(slot.spec, params_spec, cached_text_tokens, id);\n\n                // keep track of total number of tokens generated in the draft\n                slot.n_draft_total += draft.size();\n\n                // ignore small drafts\n                if (slot.params.speculative.n_min > (int) draft.size()) {\n                    SLT_DBG(slot, \"ignoring small draft: %d < %d\\n\", (int) draft.size(), slot.params.speculative.n_min);\n\n                    continue;\n                }\n\n                // construct the speculation batch\n                common_batch_clear(slot.batch_spec);\n                common_batch_add  (slot.batch_spec, id, slot.n_past, { slot.id }, true);\n\n                for (size_t i = 0; i < draft.size(); ++i) {\n                    common_batch_add(slot.batch_spec, draft[i], slot.n_past + 1 + i, { slot.id }, true);\n                }\n\n                SLT_DBG(slot, \"decoding speculative batch, size = %d\\n\", slot.batch_spec.n_tokens);\n\n                llama_decode(ctx, slot.batch_spec);\n\n                // the accepted tokens from the speculation\n                const auto ids = common_sampler_sample_and_accept_n(slot.smpl, ctx, draft);\n\n                slot.n_past    += ids.size();\n                slot.n_decoded += ids.size();\n\n                // update how many tokens out of draft was accepted\n                slot.n_draft_accepted += ids.size() - 1;\n```\n\nIMO, the \"acceptance rate\" should be the fraction of tokens we ***passed through the larger model*** that were accepted.\n\nWe could add another `slot.n_draft_generated` that counts how many tokens were generated by the draft model to print the existing stat, but since the speculative-decoding code can reuse drafts that were skipped via the \"ignore small drafts\" clause; it probably has limited usefulness.\n\n(Not sure what to put this under as it might be classed as a bug,or a feature request if adding `slot.n_draft_generated`)\n\n### Motivation\n\nIMO, the \"acceptance rate\" should be the fraction of tokens we ***passed through the larger model*** that were accepted.\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2025-06-06T11:04:38+00:00",
    "closed_at": "2025-06-10T15:48:08+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14048/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14048"
  },
  {
    "number": 10981,
    "title": "Feature Request: add DeepSeek-v3 support",
    "body": "### Prerequisites\r\n\r\n- [X] I am running the latest code. Mention the version if possible as well.\r\n- Version b4391\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\r\n\r\n### Feature Description\r\n\r\nAdd support for DeepSeek-v3\r\n\r\nhttps://huggingface.co/deepseek-ai/DeepSeek-V3\r\n\r\nCurrently not supported:\r\n\r\n`ERROR:hf-to-gguf:Model DeepseekV3ForCausalLM is not supported`\r\n\r\n### Motivation\r\n\r\nDeepSeek-v3 is a big MoE model of 685B params, would be great as offloading to RAM would be a must for most systems\r\n\r\n### Possible Implementation\r\n\r\nThere is no model card or technical report yet. I don't know how much different from v2 it is.\r\n\r\nEdit: they have uploaded the model card and paper:\r\nhttps://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\r\nhttps://huggingface.co/deepseek-ai/DeepSeek-V3/blob/main/README.md",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2024-12-26T11:08:12+00:00",
    "closed_at": "2025-01-04T20:06:12+00:00",
    "comments": 64,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10981/reactions",
      "total_count": 79,
      "+1": 51,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 11,
      "rocket": 17,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10981"
  },
  {
    "number": 331,
    "title": "Improving the repetition penalty",
    "body": "129c7d1e (#20) added a repetition penalty that prevent the model to run into loops.\r\n\r\nHere are a few suggestions for possible enhancements:\r\n\r\n * One issue with the interactive mode is that the repetition penalty is affecting the anti-prompt and response prefix, causing the model to generate unnecessarily long responses. One solution could be to exclude these tokens from the penalty,\r\n * It is possible to exempt or reduce the penalty for stop words, punctuation characters, and newlines; maybe applying a frequency-based penalty instead,\r\n * Using an exponential decay, such that recent tokens are more penalized than older ones, causing less issues with large `repeat_last_n`  windows,\r\n * Token repetition is an approximation of sub-strings or word repetition, but it seems difficult to do otherwise without backtracking the inference.",
    "labels": [
      "enhancement",
      "generation quality"
    ],
    "state": "closed",
    "created_at": "2023-03-20T15:43:12+00:00",
    "closed_at": "2023-09-14T13:23:49+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/331/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/331"
  },
  {
    "number": 5974,
    "title": "how to set this chat_template  in server?",
    "body": "how to set this chat_template in openchat?\r\nbecause i watched output it's difference from ./server and python  -m llama.cpp.server. then i thought, may is  difference chat_template made this?\r\n\r\nopenchat chat_template:\r\nUsing gguf chat template: {{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '<|end_of_turn|>'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}\r\n\r\nhow to set chat_template in ./server with --chat-template\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-10T10:44:42+00:00",
    "closed_at": "2024-04-24T01:06:37+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5974/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5974"
  },
  {
    "number": 5791,
    "title": "Differences between cgraph->leafs and cgraph->nodes?",
    "body": "Hello, I'm wondering what the differences between cgraph->leafs and cgraph->nodes. Thank you very much.",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-02-29T08:48:52+00:00",
    "closed_at": "2024-04-15T02:46:51+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5791/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5791"
  },
  {
    "number": 3870,
    "title": "Documentation of llama.h",
    "body": "I'm writing an [guidance](https://github.com/guidance-ai/guidance) inspired inference server using llama.cpp - I'm much more comfortable in rust and have generated bindings to `llama.h`.\r\n\r\nI'm able to make progress by translating examples to rust, testing it, then trial and error'ing my way to the code I want, however I'd like to make a *safe* (in the rust sense) wrapper around llama.cpp then publish and maintain it. \r\n\r\nThis involves knowing the implicit invariant used in `llama.h` (both currently and as the project moves forward) - which I don't think I can reasonably do at the moment.\r\n\r\nSome examples of things I *think* are true and would like to see documented.\r\n- ~`llama_batch.n_tokens` must never exceed the `n_tokens` used in `llama_batch_init` (basically document what memory is safe to write to in `llama_batch` after init.~ This is already done.\r\n- `get_logits_ith` must be called with an `i` where `batch.logits[i] = true` and then `batch` was decoded. (currently references `lama_eval`)\r\n- All the structs are safe to send between threads. (no thread locals and such)\r\n\r\nI understand this project is *very* fast moving and I'd be fine with taking on some of the maintenance burden of keeping the docs up to date. I imagine this would benefit a lot of the others who are maintaining libraries that depend on `llama.h`.\r\n\r\nIf it is welcome I'll be making small documentation pull requests to this effect, mostly in an effort to get feedback if my assumptions are correct.\r\n\r\nItems:\r\n- [ ] `llama_model` and `llama_context`\r\n- [ ] `llama_decode` (only error codes currently documented)\r\n- [ ] `llama_batch.n_seq_id`\r\n",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-10-31T16:08:46+00:00",
    "closed_at": "2023-10-31T17:50:30+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3870/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3870"
  },
  {
    "number": 12673,
    "title": "Feature Request: Qwen2.5-Omni",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nPleas add support https://github.com/QwenLM/Qwen2.5-Omni\n\n### Motivation\n\nThis is a multimodal which supports audio/video and text\n\n### Possible Implementation\n\nI dont think it is similar to other Qwen 2.5 models, right?",
    "labels": [
      "enhancement"
    ],
    "state": "open",
    "created_at": "2025-03-31T14:02:13+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12673/reactions",
      "total_count": 20,
      "+1": 8,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 10,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12673"
  },
  {
    "number": 8977,
    "title": "Feature Request: MiniCPM 2.6 model support?",
    "body": "### Prerequisites\r\n\r\n- [X] I am running the latest code. Mention the version if possible as well.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\r\n\r\n### Feature Description\r\n\r\nI'd like to begin by expressing my sincere gratitude for your outstanding contributions. Your efforts have been instrumental in supporting and advancing the open-source community.\r\n\r\nIt would be fantastic to have support for 8 billion parameters vision models  that can truly rival the performance of leading proprietary models.\r\n\r\n\r\n\r\n### Motivation\r\n\r\nSOTA OSS VLM with only 8b params, a piece of art, rivals top models.\r\n\r\n<img width=\"1155\" alt=\"QVl0iPtT5aUhlvViyEpgs\" src=\"https://github.com/user-attachments/assets/d746c473-9be5-4710-9f12-add392884fff\">\r\n\r\n\r\n### Possible Implementation\r\n\r\n_No response_",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-10T20:51:28+00:00",
    "closed_at": "2024-09-26T01:07:14+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8977/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8977"
  },
  {
    "number": 8529,
    "title": "Feature Request: Add support for Lite-Mistral-Instruct chat template",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nOuteAI has released a new small [model](https://huggingface.co/OuteAI/Lite-Mistral-150M-v2-Instruct) that is very coherent for its size.\r\n\r\nI am requesting the addition of this model's chat template to llama.cpp's list of supported templates\n\n### Motivation\n\nThe model is already supported by llama.cpp. However, it's using a new chat template that isn't in the list of supported templates.  As a result, llama.cpp assumes ChatML for this model. Due to the model's size, it's very sensitive to the prompt template and gives terrible results with wrong formats and users have terrible experience running it.\n\n### Possible Implementation\n\nThe model's template is very similar to ChatML so we can just copy-paste the implementation for ChatML and modify it.",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-17T06:37:58+00:00",
    "closed_at": "2024-08-31T01:07:00+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8529/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 1,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8529"
  },
  {
    "number": 13913,
    "title": "Feature Request: Generate Image Embeddings with llama.cpp",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nMaybe this is already possible, but I could not figure it out.\n\nIt would be great to use llama.cpp to generate image embeddings from a VLM, to use with a vector database.\n\n### Motivation\n\nI believe this is already possible and popular for text, and it makes sense to extend to images.\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-05-30T08:29:29+00:00",
    "closed_at": "2025-07-14T01:08:04+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13913/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13913"
  },
  {
    "number": 4470,
    "title": "Min-p Mixtral routing",
    "body": "# Feature Description\r\n\r\nThe basic idea of this issue is that instead of using a fixed number of active experts per token and per layer (top-k), there would be a dynamic amount selected like min-p sampling (must be n% as good as the best gating layer score, or they aren\u2019t selected). This could possibly be combined with top-k if higher expert counts aren\u2019t working well.\r\n\r\n# Motivation\r\n\r\nThe motivation behind this is that it would theoretically allow lower expert counts when it doesn\u2019t significantly harm quality, and allow higher experts counts when needed. By controlling the min-p routing variable, you could control the speed-quality trade off, with high values giving higher speed at the cost of quality, and low values doing the opposite. We know that quality changes based on the active expert count (https://github.com/ggerganov/llama.cpp/pull/4406#issuecomment-1855151885), so dynamic selection could control this more effectively than a fixed amount.\r\n\r\n# Possible Implementation\r\n\r\nTo implement this, instead of using ggml_topk in the MoE inference section of llama.cpp, a new operation or combination of operations would implement a top-k like output, but with min-p instead. There would also be a user accessible topk and minp parameters on the main example, which could disable it with minp 0 and topk 2, or do other combinations such as minp 0.5 and topk 3, which would require experts to be at least half as good at the top expert, and be in the top 3 experts.\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-14T14:13:36+00:00",
    "closed_at": "2024-03-18T01:46:17+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4470/reactions",
      "total_count": 8,
      "+1": 8,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4470"
  },
  {
    "number": 9048,
    "title": "Feature Request: please add falcon 7b mamba support",
    "body": "### Prerequisites\r\n\r\n- [X] I am running the latest code. Mention the version if possible as well.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\r\n\r\n### Feature Description\r\n\r\nplease add falcon 7b mamba support ,\r\n\r\nhttps://huggingface.co/tiiuae/falcon-mamba-7b-instruct\r\n### Motivation\r\n\r\nplease add falcon 7b mamba support ,\r\n\r\nhttps://huggingface.co/tiiuae/falcon-mamba-7b-instruct\r\n### Possible Implementation\r\n\r\nplease add falcon 7b mamba support ,\r\n\r\nhttps://huggingface.co/tiiuae/falcon-mamba-7b-instruct",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2024-08-15T19:16:41+00:00",
    "closed_at": "2024-08-21T08:06:38+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9048/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9048"
  },
  {
    "number": 9304,
    "title": "Feature Request: Add Host buffer type for Ascend NPU (CANN backend)",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nAscend NPU backend (CANN) is not support pin memory(Host buffer type) now. Using ping memory will make it more efficiency.\n\n### Motivation\n\nOther backend such as CUDA has already support Host buffer type.\n\n### Possible Implementation\n\nRefer to CUDA to implement the Host buffer type of Ascend NPU.",
    "labels": [
      "enhancement",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-09-04T01:47:34+00:00",
    "closed_at": "2024-09-14T02:18:26+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9304/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9304"
  },
  {
    "number": 10295,
    "title": "Feature Request: shared tokens in batches with `logits = true`",
    "body": "### Prerequisites\r\n\r\n- [X] I am running the latest code. Mention the version if possible as well.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\r\n\r\n### Feature Description\r\n\r\nWhen batching, tokens that happen to have the same `(pos, id)` can be shared across multiple sequences.\r\n\r\nHowever, if the last token in each sequence (the one we'd like logits for) happens to match with other tokens, they'd need to be processed as separate tokens, instead of taking advantage of the token grouping feature.\r\n###### ^ Not sure if bug or by design, but if a token requests logits on multiple sequences with the same `(pos, id)`, only a single `logits` array will be returned by `llama_get_logits`, without an exception, even if the sequences are different except for that token.\r\n\r\nWhat I would like to propose is to allow returning the correct amount of logits for tokens that request them, respecting the number of sequences at that shared position.\r\n\r\n### Motivation\r\n\r\nShared tokens in sequences contribute to reduced computational needs. When batching with llama.cpp it's sometimes more efficient to reprocess all sequences to also group together the output tokens.\r\n\r\nThat said, it would be really nice if they were grouped from the get-go, even if they had to be processed separately on the initial inference request.\r\n\r\n### Possible Implementation\r\n\r\nIt would be harmless if all batches had the same sequence length, but for varying length sequences, it could prove problematic.\r\n(e.g.: token ` it` on `pos11` of `seq0` and `seq1`, where only `seq0` requires logits would require special handling)\r\n\r\nWhile I don't know much about llama.cpp's internal workings, my initial thought was to somehow handle them as separate tokens for the initial inference call, then store their cache on the appropriate places, effectively making them shared.\r\n\r\nIt would also likely require altering the `llama_batch.logits` to be a 2D array as well, following `llama_batch.seq_id`'s dims.\r\n\r\nThe rest as far as I'm concerned would be magic from you guys \ud83d\ude04",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-14T15:21:44+00:00",
    "closed_at": "2025-01-03T01:07:26+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10295/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10295"
  },
  {
    "number": 12495,
    "title": "tts : add support for SparkTTS",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nHF: https://huggingface.co/SparkAudio/Spark-TTS-0.5B\n\n### Motivation\n\nA new TTS model that can be supported by llama.cpp.\n\n### Possible Implementation\n\nI might be wrong here but it seems like SparkTTS has a simlar architecture as OuteTTS and Orpheus TTS (#12476) but it uses Qwen2.5-0.5B.\n\nThey are using their own audio decoder called BiCodec. Sample python implementation: https://github.com/SparkAudio/Spark-TTS/blob/main/sparktts/models/bicodec.py\n\nSimilar model support (OuteTTS): https://github.com/ggml-org/llama.cpp/pull/10784\nCan be used as a reference how to implement this.",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-21T09:46:15+00:00",
    "closed_at": "2025-05-05T01:07:48+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12495/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12495"
  },
  {
    "number": 4611,
    "title": "Mixtral Experts are initialized from Mistral 7b - Low Rank conversion possible?",
    "body": "![image](https://github.com/ggerganov/llama.cpp/assets/66376113/77a44caa-9fa6-4746-b3c0-9772d68661cb)\r\nWe have evidence that Mixtral's Experts were initialized from a \"common ancestor\", the original Mistral 7b.\r\n\r\nConceptually, the idea that might be able to take advantage of this is:\r\n- Extracting the delta of the original Mistral 7b compared to each expert as a PEFT adapter for each expert\r\n- Use SVD to get the closest low rank approximation on each (let's say we target r=128)\r\n- Add the linear Mixtral routing layer to the original Mistral 7b\r\n- At inference time, keep all the LoRA adapters for each expert in memory (approx. ~1.8b added parameters for 128 rank)\r\n- Apply LoRA in real time for each batch of 'expert' calculations per layer using the corresponding expert's LoRA\r\n\r\nThis could be a viable alternative to [QMoE](https://github.com/ggerganov/llama.cpp/issues/4445) for approaching Mixtral's performance with significantly less memory, given the shared structural similarities.",
    "labels": [
      "enhancement",
      "research \ud83d\udd2c",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-23T19:07:56+00:00",
    "closed_at": "2024-04-02T01:10:09+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4611/reactions",
      "total_count": 15,
      "+1": 14,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4611"
  },
  {
    "number": 4614,
    "title": "HPX Support",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ X ] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\n\r\nAdd HPX support for data parallelism; this is an alternative to the current use of `std::thread`. The PR is here #4613 .\r\n\r\n# Motivation\r\n\r\n[HPX](https://github.com/STEllAR-GROUP/hpx) is an asynchronous many task (AMT) runtime system implementing the ISO C++ standard for data parallelism and concurrency. HPX additionally provides distributed memory support through an asynchronous global address space (AGAS) created over a cluster of machines. This issue is not a request to support AGAS. This PR is focused on adding HPX to manage data parallelism in llama.cpp. HPX implements a user-land thread library managed by a work stealing thread scheduler. HPX's user-land thread implementation means applications built with HPX will make fewer systems calls, typically resulting in improved performance. BLIS provides support for HPX; llama.cpp support for HPX will improve the performance of llama.cpp builds that are made against BLIS built with HPX. The performance improvement results from the HPX thread scheduler not having to contend or conflict with a secondary thread scheduler managing the same resources.\r\n\r\n# Possible Implementation\r\n\r\nThe implementation in #4613 makes a slight modification by using `hpx::future` wherever `std::thread` is currently exercised.",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-24T03:23:30+00:00",
    "closed_at": "2024-04-02T01:10:08+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4614/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4614"
  },
  {
    "number": 5852,
    "title": "Question about llama.cpp and llava-cli when used with llava 1.6 for vision:",
    "body": "I've been using the llava-v1.6-mistral-7b model for doing captions lately. I know it's relatively new and does some different things under the hood vs other/older vision models.\r\n\r\nFrom the little bit of testing I've performed, it seems like server makes it fall back to the llava 1.5 vision, rather than using the 1.6 mode. When I check the total token counts, those always seem to be really low. This seems to affect any apps that use llama.cpp, like LM Studio and Jan. If I use llava-cli, with the same settings, the image alone encodes to 2880 tokens, which indicates that it's encoding the tiles correctly. Is there any way to make the server use llava-cli? Anyway to make llava-cli behave like a server? Am I doing something wrong?\r\n\r\nI wrote a python program to batch caption folders of images, but I'm having to do it a really hacky way where it basically runs a command prompt behind the scenes, the python script captures the output of the window as a log, parses the log to trim out the non-response text, formats it, saves it, etc. The problem is that it's really annoying because it has to fully reload the model for each image.\r\n\r\nFor reference, this is how I'm running llava-cli:\r\n`llava-cli -m \"C:\\pathtomodel\\llava-v1.6-mistral-7b.Q4_K_M.gguf\" --mmproj \"C:\\pathtovision\\mmproj-model-f16.gguf\" --image \"c:\\pathtoimage\\image.png\" --temp 0.2 --n-gpu-layers 100 -n 2048 -c 4096 --mlock -p \"<image>\\nUSER:\\nProvide a full description. Be as accurate and detailed as possible. \\nASSISTANT:\\n\" >> log.txt` (the >> log.txt is what you'd use if you were manually running it straight from a cmd prompt and not from some python script that can capture it for you)\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-03T11:47:12+00:00",
    "closed_at": "2024-04-20T01:06:58+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5852/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5852"
  },
  {
    "number": 8608,
    "title": "Support for SmolLM",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nAdd support for [SmolLM](https://huggingface.co/blog/smollm) family of models\n\n### Motivation\n\nenhancement\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2024-07-20T23:00:38+00:00",
    "closed_at": "2024-07-22T14:43:04+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8608/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8608"
  },
  {
    "number": 466,
    "title": "How do we finetune the model with new data?",
    "body": "Can we have a finetune.cpp or finetune.exe file to incorporate new data into the model? The use case will be to design an AI model that can do more than just general chat. It can become very knowledgeable in specific topics they are finetuned on. Also, after creating the finetune.exe , please ensure no GPU is required for the entire process. Because that is what makes this repo awesome in the first place.",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-24T16:12:02+00:00",
    "closed_at": "2024-04-10T01:07:59+00:00",
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/466/reactions",
      "total_count": 15,
      "+1": 12,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 3,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/466"
  },
  {
    "number": 7190,
    "title": "Native Intel IPEX-LLM Support",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X ] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X ] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\n\r\nI have found this closed issue where someone manually (?how?) implemented IPEX-LLM. However, looking forward to native IPEX-LLM support for Intel Xe iGPUs + Intel Arc dGPUs on Windows and Linux \r\n\r\nhttps://github.com/ggerganov/llama.cpp/issues/7042\r\n\r\nTL;DR is IPEX-LLM now provides a C++ interface, which can be used as a backend for running llama.cpp on Intel GPUs. Incorporating this interface into llama.cpp would allow for leveraging the optimized performance of IPEX-LLM.\r\n\r\n# Motivation\r\n\r\nIntel Xe graphics launched in 2020. Flex, Max Datacenter and Arc Consumer cards for laptop and desktop launched in 2022. This is a lot of devices in production/circulation.  \r\n\r\nThis would \"permit\" llama.cpp users to utilize their integrated Xe GPUs and dedicated Arc GPUs, Datacenter Flex and Max cards with llama.cpp on BOTH Windows and Linux natively (without a confusing manual build). \r\n\r\n# Possible Implementation\r\n\r\nThe implementation of native Intel IPEX-LLM support would be something like... Integrate --> Test --> Document --> Release. \r\n\r\n1. **Integration with IPEX**: Since IPEX-LLM is built on top of Intel Extension for PyTorch (IPEX), the first step would be to ensure seamless integration with IPEX. This would involve linking the llama.cpp build system with the IPEX library and ensuring that all dependencies are correctly managed. Here is a link for using llama.cpp with Intel GPUs... \r\n\r\nFull manual/guide: https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/llama_cpp_quickstart.html \r\nFull verified model list: https://ipex-llm.readthedocs.io/en/latest/#verified-models\r\nGithub: https://github.com/intel-analytics/ipex-llm\r\n\r\nThe \"owners\" of this process will be the devs and engineers here; in this Github (simple nerds such as myself do not have the expertise to tackle something like this... even locally)\r\n\r\nFor example from the documentation it looks like this would be create a new conda envioronment --> set up environment --> configure oneapi variables --> update cmakelists.txt or makefile with paths to IPEX-LLM library and headers --> then ??map llama.cpp functionalities to ipex apis (which Intel has already done). \r\n\r\n2. **Testing Across Platforms**: Ensuring that the implementation works across different versions of Windows and Linux is crucial... This includes testing on various Intel iGPUs and Arc dGPUs to guarantee broad compatibility. This effort would involve the community here, various Discords, subreddits, and perhaps trying to \"rope in\" as many laptop/desktop Xe iGPU users and dGPU users as possible -- so that means gamers, too. \r\n\r\nThe \"owners\" of this step would be wide-ranging overall. \r\n\r\n3. **Documentation and Examples**: Someone would have to \"own\" updating the documentation to guide users on how to enable and use the new IPEX-LLM support. Providing examples and quickstart guides can significantly help; but ultimately for independent users it will be up to them and then for GUI and TUI/CLI frontends, the documentation will need to be updated by them. \r\n\r\n4. **Release** After all of this has been done, going forward to launch woot woot. \r\n\r\nI'm sure there are many, many steps I am missing here. Just wanted to \"kick off\" the process. \r\n\r\n\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-10T02:02:37+00:00",
    "closed_at": "2024-07-09T01:06:58+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7190/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7190"
  }
]
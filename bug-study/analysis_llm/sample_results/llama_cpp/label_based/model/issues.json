[
  {
    "number": 329,
    "title": "invalid model file './models/ggml-alpaca-7b-q4.bin' (too old, regenerate your model files!)",
    "body": "Hi, I have encounter the above problem when running the alpaca model. I download the model from the link \"https://gateway.estuary.tech/gw/ipfs/QmQ1bf2BTnYxq73MFJWu1B7bQ2UD6qG7D7YDCxhTndVkPC\" which is one of the three options from the readme. Should I download the model from somewhere else? ",
    "labels": [
      "need more info",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-20T14:56:00+00:00",
    "closed_at": "2023-03-20T15:32:21+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/329/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/329"
  },
  {
    "number": 4437,
    "title": "Will llama.cpp be able to use Phi-2 ?",
    "body": "Surely we have to wait for a GGUF version, but in the meantime just curious about it\r\n\r\nthanks",
    "labels": [
      "enhancement",
      "good first issue",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-12-13T12:02:56+00:00",
    "closed_at": "2023-12-18T17:27:49+00:00",
    "comments": 27,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4437/reactions",
      "total_count": 20,
      "+1": 17,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4437"
  },
  {
    "number": 257,
    "title": "Not having enough memory just causes a segfault or something",
    "body": "So. I'm trying to build with CMake on Windows 11 and the thing just stops after it's done loading the model.\r\n\r\n![image](https://user-images.githubusercontent.com/4723091/226091364-64a488a7-ebb5-4c24-9dd0-1cb81378008d.png)\r\n\r\nAnd apparently, this is a segfault.\r\n\r\n![Screenshot_20230318_121935](https://user-images.githubusercontent.com/4723091/226091335-afbf2712-d2b8-4b88-9b44-6b6a43d78565.png)\r\n\r\nYay yay yyayy yyayay\r\n\r\nthis is a memory allocation failure it seems, from me not having enough memory. not like llama.cpp Tells Me That lmao, it just segfaults\r\n\r\n(`ctx->mem_buffer` is nullptr which probably means the malloc just failed)",
    "labels": [
      "bug",
      "duplicate",
      "hardware",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-18T07:28:43+00:00",
    "closed_at": "2023-05-06T18:03:16+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/257/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/257"
  },
  {
    "number": 247,
    "title": "How to use ggml for Flan-T5",
    "body": "@ggerganov Thanks for sharing llama.cpp. As usual, great work.\r\n\r\nQuestion rather than issue.  How difficult would it be to make ggml.c work for a Flan checkpoint, like T5-xl/UL2, then quantized?\r\n\r\nWould love to be able to have those models run on a browser, much like what you did with whisper.cpp wasm.\r\n\r\nThanks again.  (I can move this post somewhere else if you prefer since it's not technically about Llama.  Just let me know where.)",
    "labels": [
      "enhancement",
      "model",
      "generation quality",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-17T22:38:08+00:00",
    "closed_at": "2024-04-14T01:06:18+00:00",
    "comments": 36,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/247/reactions",
      "total_count": 8,
      "+1": 8,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/247"
  },
  {
    "number": 12392,
    "title": "csm : implement Sesame-based conversation example",
    "body": "With the first Sesame CSM model [openly available](https://github.com/SesameAILabs/csm), we should implement a local example similar to their [online research demo](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo). It seems that the released CSM model uses [Kyutai's Mimi](https://arxiv.org/abs/2410.00037) audio codec which we have to implement in a similar way as we did with the [WavTokenizer](https://github.com/ggml-org/llama.cpp/pull/10784). Next we can modify the [talk-llama](https://github.com/ggerganov/whisper.cpp/tree/master/examples/talk-llama) example to support audio generation with the CSM. This way we will be able to plug any LLM for the text response generation and use Sesame for speech input/output.",
    "labels": [
      "model",
      "research \ud83d\udd2c",
      "stale",
      "tts"
    ],
    "state": "closed",
    "created_at": "2025-03-14T14:49:46+00:00",
    "closed_at": "2025-05-14T01:07:48+00:00",
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12392/reactions",
      "total_count": 28,
      "+1": 15,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 8,
      "eyes": 5
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12392"
  },
  {
    "number": 3441,
    "title": "Support Jais-13b-chat bilingual model",
    "body": "How to add support for [Jais-13b-chat bilingual LLM](https://huggingface.co/inception-mbzuai/jais-13b-chat)?\r\n\r\nI am a n00b, and from what I can tell this Torch model uses a custom architecture (including a custom model class) and would need a special converter.\r\n\r\nI'd be happy to collaborate with someone more experienced on making this happen. I can provide some sponsorship as well. Thanks!\r\n",
    "labels": [
      "model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-10-02T17:41:59+00:00",
    "closed_at": "2024-04-15T02:47:10+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3441/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3441"
  },
  {
    "number": 1919,
    "title": "Make k-quants work with tensor dimensions that are not multiple of 256",
    "body": "As discussed in #1602, k-quants do not work for the Falcon-7B model. This is due to the fact that the number of columns in many tensors (`4544`) is not divisible by `256`, which is the super-block size of the k-quants.\r\n\r\nIt would be useful if k-quants could be adapted to work in such cases.  ",
    "labels": [
      "enhancement",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-06-18T07:32:22+00:00",
    "closed_at": "2023-06-26T16:43:09+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1919/reactions",
      "total_count": 6,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1919"
  },
  {
    "number": 1337,
    "title": "Implement Together Computer's Red Pajama 3B Base/Chat model",
    "body": "- [announcement][0]\r\n- [base model, 3B][1]\r\n- [instruct model, 3B][3]\r\n- [chat model, 3B][2]\r\n\r\nHopefully this can be blazingly fast!\r\n\r\n[0]: https://www.together.xyz/blog/redpajama-models-v1\r\n[1]: https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\r\n[2]: https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1\r\n[3]: https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-3B-v1",
    "labels": [
      "model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-05-06T01:48:53+00:00",
    "closed_at": "2024-04-09T01:09:36+00:00",
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1337/reactions",
      "total_count": 19,
      "+1": 18,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1337"
  },
  {
    "number": 6656,
    "title": "`quantize`: add imatrix and dataset metadata in GGUF",
    "body": "### Motivation\r\nI was reading [thanks](https://huggingface.co/spaces/ggml-org/gguf-my-repo/discussions/41#661a27157a16dc848a58a261) to @julien-c this [reddit post](https://www.reddit.com/r/LocalLLaMA/comments/1ba55rj/overview_of_gguf_quantization_methods/?rdt=36175) from @he29-net :+1: \r\n\r\n> You can't easily tell whether a model was quantized with the help of importance matrix just from the name. I first found this annoying, because it was not clear if and how the calibration dataset affects performance of the model in other than just positive ways. But recent tests in llama.cpp [discussion #5263](https://github.com/ggerganov/llama.cpp/discussions/5263) show, that while the data used to prepare the imatrix slightly affect how it performs in (un)related languages or specializations, any dataset will perform better than a \"vanilla\" quantization with no imatrix. So now, instead, I find it annoying because sometimes the only way to be sure I'm using the better imatrix version is to re-quantize the model myself.\r\n\r\n### Proposal\r\n\r\n- Add at the end of the `imatrix` binary file the dataset name on which the imatrix was computed on\r\n\r\n- Add following KV in `quantize`:\r\n  - `quantize.imatrix.file` Filename of the provided imatrix during quantization\r\n  - `quantize.imatrix.entries_count` Number of entries in the imatrix\r\n  - `quantize.imatrix.dataset` Dataset from the imatrix\r\n  - `quantize.imatrix.chunks_count` Number of chunks the imatrix was computed with\r\n \r\nIdeally I would also add both imatrix and dataset files hashes in the metadata, but I am not sure this is supported and appropriate.",
    "labels": [
      "enhancement",
      "model",
      "generation quality",
      "need feedback"
    ],
    "state": "closed",
    "created_at": "2024-04-13T10:13:08+00:00",
    "closed_at": "2024-04-26T18:06:34+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6656/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6656"
  },
  {
    "number": 1326,
    "title": "Investigate supporting starcode",
    "body": "Bigcode just released [starcoder](https://huggingface.co/bigcode/starcoder). This is a 15B model trained on 1T Github tokens. This seems like it could be an amazing replacement for gpt-3.5 and maybe gpt-4 for local coding assistance and IDE tooling!\r\n\r\nMore info: https://huggingface.co/bigcode",
    "labels": [
      "help wanted",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-05-04T21:04:22+00:00",
    "closed_at": "2023-05-18T12:34:49+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1326/reactions",
      "total_count": 12,
      "+1": 12,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1326"
  },
  {
    "number": 117,
    "title": "Convert h5 format to ggml",
    "body": "There has been a llama model file hosted on [Hugging Face](https://huggingface.co/decapoda-research/llama-30b-hf/tree/main)\r\n\r\nIt would be good if there is a convert script for this format as well, just like what has been done on [whisper.cpp](https://github.com/ggerganov/whisper.cpp/blob/09e90680072d8ecdf02eaf21c393218385d2c616/models/convert-pt-to-ggml.py)",
    "labels": [
      "enhancement",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-14T05:13:42+00:00",
    "closed_at": "2023-07-28T19:30:53+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/117/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/117"
  },
  {
    "number": 623,
    "title": "How can I do summarization ",
    "body": "I'm trying to make something like https://platform.openai.com/examples/default-notes-summary\r\n\r\nBut it fails, I tried with gpt4all, llama and alpaca 7B. Maybe I should ajust the prompt ?\r\n\r\n<img width=\"1440\" alt=\"Screenshot 2023-03-30 at 15 17 56\" src=\"https://user-images.githubusercontent.com/74246611/228848634-1a27e9a6-fed6-4abc-8aa1-9964f8e2595f.png\">\r\n",
    "labels": [
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-30T13:19:01+00:00",
    "closed_at": "2023-03-30T17:08:08+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/623/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/623"
  },
  {
    "number": 155,
    "title": "What models i really need?",
    "body": "Hi,\r\n\r\nWhat models i really need?\r\n\r\nI have these:\r\n\r\n<img width=\"423\" alt=\"image\" src=\"https://user-images.githubusercontent.com/395096/225223070-ceb1a05b-8af6-4426-8a51-6cfa6d156718.png\">\r\n\r\nThe only 7B folder for example is necessary? Each model has different results?\r\n\r\nI don't understand if i need only one and execute the training for each folder or if only one is necessary and i need choose one.\r\n\r\nThanks.",
    "labels": [
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-15T06:18:17+00:00",
    "closed_at": "2023-04-07T16:11:32+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/155/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/155"
  },
  {
    "number": 1291,
    "title": "Try whether OpenLLaMa works",
    "body": "... or whether we need to tweak some settings\r\n\r\nGitHub: https://github.com/openlm-research/open_llama\r\n\r\nHuggingFace: https://huggingface.co/openlm-research/open_llama_7b_preview_300bt\r\n\r\n---\r\n\r\nedit: GGML models uploaded to HH by @vihangd => https://huggingface.co/vihangd/open_llama_7b_300bt_ggml",
    "labels": [
      "\ud83e\udd99.",
      "model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-05-02T21:53:20+00:00",
    "closed_at": "2024-04-09T01:09:41+00:00",
    "comments": 82,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1291/reactions",
      "total_count": 8,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1291"
  },
  {
    "number": 6585,
    "title": "AttributeError: 'GGUFWriter' object has no attribute 'add_vocab_size'",
    "body": "Hi, When I converted the large model weights to gguf format, I encountered this error\r\n",
    "labels": [
      "need more info",
      "model",
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-10T10:15:13+00:00",
    "closed_at": "2024-06-16T01:07:09+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6585/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6585"
  },
  {
    "number": 7727,
    "title": "llama : support Mamba-2",
    "body": "Mamba-2 is a new version of the Mamba architecture:\r\n\r\n- Blog: https://tridao.me/blog/2024/mamba2-part1-model/\r\n- Paper: https://arxiv.org/abs/2405.21060",
    "labels": [
      "model",
      "research \ud83d\udd2c",
      "roadmap"
    ],
    "state": "closed",
    "created_at": "2024-06-04T05:57:48+00:00",
    "closed_at": "2025-07-02T17:10:26+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7727/reactions",
      "total_count": 83,
      "+1": 32,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 39,
      "rocket": 12,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7727"
  },
  {
    "number": 5981,
    "title": "llama : add Deepseek support",
    "body": "Support is almost complete. There is a dangling issue with the pre-tokenizer: https://github.com/ggerganov/llama.cpp/pull/7036\r\n\r\nA useful discussion related to that is here: https://github.com/ggerganov/llama.cpp/discussions/7144\r\n\r\n-----\r\n\r\n## Outdated below\r\n\r\nCreating this issue for more visibility\r\n\r\nThe main problem is around tokenization support, since the models use some variation of the BPE pre-processing regex. There are also some issues with the conversion scripts.\r\n\r\nAnyway, looking for contributions to help with this\r\n\r\nPrevious unfinished work:\r\n\r\n- #4070 \r\n- #5464 \r\n\r\nPossible implementation plan: https://github.com/ggerganov/llama.cpp/pull/5464#issuecomment-1974818993",
    "labels": [
      "help wanted",
      "good first issue",
      "model"
    ],
    "state": "closed",
    "created_at": "2024-03-10T18:56:56+00:00",
    "closed_at": "2024-05-08T17:03:57+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5981/reactions",
      "total_count": 14,
      "+1": 10,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 4
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5981"
  },
  {
    "number": 3294,
    "title": "Request: Nougat OCR Integration",
    "body": "# Request: Nougat OCR Integration\r\n\r\nI suggest adding Nougat OCR into llama.cpp to enable the processing of scientific PDF documents. \r\nThis can act as a first step towards adding multimodal models to this project!\r\n\r\nImplementation:\r\nIt seems that Nougat is based on standard transformer architecture (like Bart and Swin Transformer) and most of the work would be on figuring out how to add the image processing.\r\n\r\nLet me know what you think!\r\nP.S.: Love this repo! I hope to add my own retrieval-pretrained transformer at some point to this repo.\r\n\r\n",
    "labels": [
      "help wanted",
      "model"
    ],
    "state": "open",
    "created_at": "2023-09-21T06:29:29+00:00",
    "closed_at": null,
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3294/reactions",
      "total_count": 13,
      "+1": 13,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3294"
  },
  {
    "number": 2262,
    "title": "Add llama 2 model",
    "body": "Meta just released llama 2 model, allowing commercial usage\r\n\r\nhttps://ai.meta.com/resources/models-and-libraries/llama/\r\n\r\nI have checked the model implementation and it seems different from llama_v1, maybe need a re-implementation",
    "labels": [
      "\ud83e\udd99.",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-07-18T16:35:53+00:00",
    "closed_at": "2023-10-18T07:31:45+00:00",
    "comments": 95,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2262/reactions",
      "total_count": 141,
      "+1": 89,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 52,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2262"
  },
  {
    "number": 373,
    "title": "[mqy] ./examples/chatLLaMa: line 53: 33476 Segmentation fault: 11",
    "body": "# Current Behavior\r\n\r\n`./examples/chatLLaMa`,  After about 30-round talks, program quite with `Segmentation fault: 11`.\r\nI did another try, input last question, but can't reproduce.\r\n\r\n# Environment and Context \r\n\r\n* Physical hardware:\r\n\r\nMacBook Pro 2018, 2.6 GHz 6-Core Intel Core i7, 32 GB 2400 MHz DDR4\r\n\r\n* Operating System\r\n\r\nmacOS 13.2.1 (22D68)\r\nDarwin Kernel Version 22.3.0: Mon Jan 30 20:42:11 PST 2023; root:xnu-8792.81.3~2/RELEASE_X86_64 x86_64\r\n\r\n# Failure Information (for bugs)\r\n\r\n```\r\n./examples/chatLLaMa: line 53: 33476 Segmentation fault: 11  ./main $GEN_OPTIONS --model ... \r\n...\r\n$USER_NAME:\" \"$@\"\r\n```\r\n\r\n# Failure Logs\r\n\r\n```\r\n$ git log | head -1\r\ncommit 0f6135270839f0715843c4d480c63ae150def419\r\n\r\n$ sysctl -n machdep.cpu.brand_string\r\nIntel(R) Core(TM) i7-8850H CPU @ 2.60GHz\r\n\r\n$ sysctl -n machdep.cpu.features\r\nFPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE SSE3 PCLMULQDQ DTES64 MON DSCPL VMX SMX EST TM2 SSSE3 FMA CX16 TPR PDCM SSE4.1 SSE4.2 x2APIC MOVBE POPCNT AES PCID XSAVE OSXSAVE SEGLIM64 TSCTMR AVX1.0 RDRAND F16C\r\n\r\n$ cc --version\r\nApple clang version 14.0.0 (clang-1400.0.29.202)\r\n\r\n$ g++ --version\r\nApple clang version 14.0.0 (clang-1400.0.29.202)\r\n\r\n$ python3 --version\r\nPython 3.10.9\r\n\r\n$ pip list | egrep \"torch|numpy|sentencepiece\"\r\nnumpy                         1.24.2\r\nsentencepiece                 0.1.97\r\ntorch                         2.0.0\r\ntorchaudio                    2.0.1\r\ntorchdata                     0.6.0\r\ntorchtext                     0.15.1\r\ntorchvision                   0.15.1\r\n\r\n$ make --version | head -1\r\nGNU Make 3.81\r\n\r\n$ md5sum models/13B/ggml-model-q4*\r\nb55d2fc49f6fef830aeb987e4387a892  models/13B/ggml-model-q4_0.bin\r\n4c06a6606324f3951401e866ec7e7410  models/13B/ggml-model-q4_0.bin.1\r\n```\r\n\r\nMy questions begin with `What time is it with timezone at Beijing?`, end with `Please translate your answer to \"Do you mean \"the road\" is a pun?\" as Chinese.`, you can copy & paste one of them to locate the conversation phrases.\r\n\r\nConsole log:\r\n\r\n```\r\n./examples/chatLLaMa \r\nmain: seed = 1679423191\r\nllama_model_load: loading model from './models/13B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 2048\r\nllama_model_load: n_embd  = 5120\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 40\r\nllama_model_load: n_layer = 40\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 13824\r\nllama_model_load: n_parts = 2\r\nllama_model_load: ggml ctx size = 10959.49 MB\r\nllama_model_load: memory_size =  3200.00 MB, n_mem = 81920\r\nllama_model_load: loading model part 1/2 from './models/13B/ggml-model-q4_0.bin'\r\nllama_model_load: ............................................. done\r\nllama_model_load: model size =  3880.49 MB / num tensors = 363\r\nllama_model_load: loading model part 2/2 from './models/13B/ggml-model-q4_0.bin.1'\r\nllama_model_load: ............................................. done\r\nllama_model_load: model size =  3880.49 MB / num tensors = 363\r\n\r\nsystem_info: n_threads = 8 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \r\n\r\nmain: prompt: ' \r\nText transcript of a never ending dialog, where User interacts with an AI assistant named ChatLLaMa.\r\nChatLLaMa is helpful, kind, honest, friendly, good at writing and never fails to answer User\u2019s requests immediately and with details and precision.\r\nThere are no annotations like (30 seconds passed...) or (to himself), just what User and ChatLLaMa say alound to each other.\r\nThe dialog lasts for years, the entirety of it is shared below. It's 10000 pages long.\r\nThe transcript only includes text, it does not include markup like HTML and Markdown.\r\n\r\nUser: Hello, ChatLLaMa!\r\nChatLLaMa: Hello User! How may I help you today?\r\nUser: What time is it?\r\nChatLLaMa: It is 02:26.\r\nUser: What year is it?\r\nChatLLaMa: We are in 2023.\r\nUser: Please tell me the largest city in Europe.\r\nChatLLaMa: The largest city in Europe is Moscow, the capital of Russia.\r\nUser: What can you tell me about Moscow?\r\nChatLLaMa: Moscow, on the Moskva River in western Russia, is the nation\u2019s cosmopolitan capital. In its historic core is the Kremlin, a complex that\u2019s home to the president and tsarist treasures in the Armoury. Outside its walls is Red Square, Russia\u2019s symbolic center.\r\nUser: What is a cat?\r\nChatLLaMa: A cat is a domestic species of small carnivorous mammal. It is the only domesticated species in the family Felidae.\r\nUser: How do I pass command line arguments to a Node.js program?\r\nChatLLaMa: The arguments are stored in process.argv.\r\n\r\n    argv[0] is the path to the Node. js executable.\r\n    argv[1] is the path to the script file.\r\n    argv[2] is the first argument passed to the script.\r\n    argv[3] is the second argument passed to the script and so on.\r\nUser: Name a color.\r\nChatLLaMa: Blue\r\nUser:'\r\nmain: number of tokens in prompt = 500\r\n     1 -> ''\r\n 29871 -> ' '\r\n    13 -> '\r\n'\r\n  1626 -> 'Text'\r\n  1301 -> ' trans'\r\n   924 -> 'cript'\r\n   310 -> ' of'\r\n   263 -> ' a'\r\n  2360 -> ' never'\r\n 17140 -> ' ending'\r\n  7928 -> ' dialog'\r\n 29892 -> ','\r\n   988 -> ' where'\r\n  4911 -> ' User'\r\n 16254 -> ' interact'\r\n 29879 -> 's'\r\n   411 -> ' with'\r\n   385 -> ' an'\r\n   319 -> ' A'\r\n 29902 -> 'I'\r\n 20255 -> ' assistant'\r\n  4257 -> ' named'\r\n   678 -> ' Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  1451 -> 'Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n   338 -> ' is'\r\n  8444 -> ' helpful'\r\n 29892 -> ','\r\n  2924 -> ' kind'\r\n 29892 -> ','\r\n 15993 -> ' honest'\r\n 29892 -> ','\r\n 19780 -> ' friendly'\r\n 29892 -> ','\r\n  1781 -> ' good'\r\n   472 -> ' at'\r\n  5007 -> ' writing'\r\n   322 -> ' and'\r\n  2360 -> ' never'\r\n  8465 -> ' fails'\r\n   304 -> ' to'\r\n  1234 -> ' answer'\r\n  4911 -> ' User'\r\n 30010 -> '\u2019'\r\n 29879 -> 's'\r\n  7274 -> ' requests'\r\n  7389 -> ' immediately'\r\n   322 -> ' and'\r\n   411 -> ' with'\r\n  4902 -> ' details'\r\n   322 -> ' and'\r\n 16716 -> ' precision'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  8439 -> 'There'\r\n   526 -> ' are'\r\n   694 -> ' no'\r\n 25495 -> ' annotations'\r\n   763 -> ' like'\r\n   313 -> ' ('\r\n 29941 -> '3'\r\n 29900 -> '0'\r\n  6923 -> ' seconds'\r\n  4502 -> ' passed'\r\n 11410 -> '...)'\r\n   470 -> ' or'\r\n   313 -> ' ('\r\n   517 -> 'to'\r\n  3654 -> ' himself'\r\n   511 -> '),'\r\n   925 -> ' just'\r\n   825 -> ' what'\r\n  4911 -> ' User'\r\n   322 -> ' and'\r\n   678 -> ' Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n  1827 -> ' say'\r\n   394 -> ' al'\r\n   618 -> 'ound'\r\n   304 -> ' to'\r\n  1269 -> ' each'\r\n   916 -> ' other'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  1576 -> 'The'\r\n  7928 -> ' dialog'\r\n  1833 -> ' last'\r\n 29879 -> 's'\r\n   363 -> ' for'\r\n  2440 -> ' years'\r\n 29892 -> ','\r\n   278 -> ' the'\r\n  4152 -> ' entire'\r\n  1017 -> 'ty'\r\n   310 -> ' of'\r\n   372 -> ' it'\r\n   338 -> ' is'\r\n  7258 -> ' shared'\r\n  2400 -> ' below'\r\n 29889 -> '.'\r\n   739 -> ' It'\r\n 29915 -> '''\r\n 29879 -> 's'\r\n 29871 -> ' '\r\n 29896 -> '1'\r\n 29900 -> '0'\r\n 29900 -> '0'\r\n 29900 -> '0'\r\n 29900 -> '0'\r\n  6515 -> ' pages'\r\n  1472 -> ' long'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  1576 -> 'The'\r\n  1301 -> ' trans'\r\n   924 -> 'cript'\r\n   871 -> ' only'\r\n  7805 -> ' includes'\r\n  1426 -> ' text'\r\n 29892 -> ','\r\n   372 -> ' it'\r\n   947 -> ' does'\r\n   451 -> ' not'\r\n  3160 -> ' include'\r\n 24986 -> ' markup'\r\n   763 -> ' like'\r\n  4544 -> ' HTML'\r\n   322 -> ' and'\r\n  4485 -> ' Mark'\r\n  3204 -> 'down'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n 15043 -> ' Hello'\r\n 29892 -> ','\r\n   678 -> ' Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29991 -> '!'\r\n    13 -> '\r\n'\r\n  1451 -> 'Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29901 -> ':'\r\n 15043 -> ' Hello'\r\n  4911 -> ' User'\r\n 29991 -> '!'\r\n  1128 -> ' How'\r\n  1122 -> ' may'\r\n   306 -> ' I'\r\n  1371 -> ' help'\r\n   366 -> ' you'\r\n  9826 -> ' today'\r\n 29973 -> '?'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n  1724 -> ' What'\r\n   931 -> ' time'\r\n   338 -> ' is'\r\n   372 -> ' it'\r\n 29973 -> '?'\r\n    13 -> '\r\n'\r\n  1451 -> 'Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29901 -> ':'\r\n   739 -> ' It'\r\n   338 -> ' is'\r\n 29871 -> ' '\r\n 29900 -> '0'\r\n 29906 -> '2'\r\n 29901 -> ':'\r\n 29906 -> '2'\r\n 29953 -> '6'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n  1724 -> ' What'\r\n  1629 -> ' year'\r\n   338 -> ' is'\r\n   372 -> ' it'\r\n 29973 -> '?'\r\n    13 -> '\r\n'\r\n  1451 -> 'Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29901 -> ':'\r\n  1334 -> ' We'\r\n   526 -> ' are'\r\n   297 -> ' in'\r\n 29871 -> ' '\r\n 29906 -> '2'\r\n 29900 -> '0'\r\n 29906 -> '2'\r\n 29941 -> '3'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n  3529 -> ' Please'\r\n  2649 -> ' tell'\r\n   592 -> ' me'\r\n   278 -> ' the'\r\n 10150 -> ' largest'\r\n  4272 -> ' city'\r\n   297 -> ' in'\r\n  4092 -> ' Europe'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  1451 -> 'Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29901 -> ':'\r\n   450 -> ' The'\r\n 10150 -> ' largest'\r\n  4272 -> ' city'\r\n   297 -> ' in'\r\n  4092 -> ' Europe'\r\n   338 -> ' is'\r\n 25820 -> ' Moscow'\r\n 29892 -> ','\r\n   278 -> ' the'\r\n  7483 -> ' capital'\r\n   310 -> ' of'\r\n 12710 -> ' Russia'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n  1724 -> ' What'\r\n   508 -> ' can'\r\n   366 -> ' you'\r\n  2649 -> ' tell'\r\n   592 -> ' me'\r\n  1048 -> ' about'\r\n 25820 -> ' Moscow'\r\n 29973 -> '?'\r\n    13 -> '\r\n'\r\n  1451 -> 'Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29901 -> ':'\r\n 25820 -> ' Moscow'\r\n 29892 -> ','\r\n   373 -> ' on'\r\n   278 -> ' the'\r\n 26387 -> ' Mosk'\r\n  1564 -> 'va'\r\n  6163 -> ' River'\r\n   297 -> ' in'\r\n 15782 -> ' western'\r\n 12710 -> ' Russia'\r\n 29892 -> ','\r\n   338 -> ' is'\r\n   278 -> ' the'\r\n  5233 -> ' nation'\r\n 30010 -> '\u2019'\r\n 29879 -> 's'\r\n 27973 -> ' cosm'\r\n 13242 -> 'opol'\r\n  8929 -> 'itan'\r\n  7483 -> ' capital'\r\n 29889 -> '.'\r\n   512 -> ' In'\r\n   967 -> ' its'\r\n 22879 -> ' historic'\r\n  7136 -> ' core'\r\n   338 -> ' is'\r\n   278 -> ' the'\r\n  7706 -> ' Kre'\r\n   828 -> 'ml'\r\n   262 -> 'in'\r\n 29892 -> ','\r\n   263 -> ' a'\r\n  4280 -> ' complex'\r\n   393 -> ' that'\r\n 30010 -> '\u2019'\r\n 29879 -> 's'\r\n  3271 -> ' home'\r\n   304 -> ' to'\r\n   278 -> ' the'\r\n  6673 -> ' president'\r\n   322 -> ' and'\r\n 18696 -> ' ts'\r\n   279 -> 'ar'\r\n   391 -> 'ist'\r\n  2578 -> ' tre'\r\n 25414 -> 'asures'\r\n   297 -> ' in'\r\n   278 -> ' the'\r\n  8481 -> ' Arm'\r\n   473 -> 'our'\r\n 29891 -> 'y'\r\n 29889 -> '.'\r\n  4451 -> ' Out'\r\n  2975 -> 'side'\r\n   967 -> ' its'\r\n 14603 -> ' walls'\r\n   338 -> ' is'\r\n  4367 -> ' Red'\r\n 19256 -> ' Square'\r\n 29892 -> ','\r\n 12710 -> ' Russia'\r\n 30010 -> '\u2019'\r\n 29879 -> 's'\r\n  5829 -> ' symbol'\r\n   293 -> 'ic'\r\n  4818 -> ' center'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n  1724 -> ' What'\r\n   338 -> ' is'\r\n   263 -> ' a'\r\n  6635 -> ' cat'\r\n 29973 -> '?'\r\n    13 -> '\r\n'\r\n  1451 -> 'Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29901 -> ':'\r\n   319 -> ' A'\r\n  6635 -> ' cat'\r\n   338 -> ' is'\r\n   263 -> ' a'\r\n 21849 -> ' domestic'\r\n  6606 -> ' species'\r\n   310 -> ' of'\r\n  2319 -> ' small'\r\n  1559 -> ' car'\r\n 29876 -> 'n'\r\n   440 -> 'iv'\r\n 20657 -> 'orous'\r\n   286 -> ' m'\r\n  4850 -> 'amm'\r\n   284 -> 'al'\r\n 29889 -> '.'\r\n   739 -> ' It'\r\n   338 -> ' is'\r\n   278 -> ' the'\r\n   871 -> ' only'\r\n 21849 -> ' domestic'\r\n   630 -> 'ated'\r\n  6606 -> ' species'\r\n   297 -> ' in'\r\n   278 -> ' the'\r\n  3942 -> ' family'\r\n 11961 -> ' Fel'\r\n  3898 -> 'idae'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n  1128 -> ' How'\r\n   437 -> ' do'\r\n   306 -> ' I'\r\n  1209 -> ' pass'\r\n  1899 -> ' command'\r\n  1196 -> ' line'\r\n  6273 -> ' arguments'\r\n   304 -> ' to'\r\n   263 -> ' a'\r\n  9071 -> ' Node'\r\n 29889 -> '.'\r\n  1315 -> 'js'\r\n  1824 -> ' program'\r\n 29973 -> '?'\r\n    13 -> '\r\n'\r\n  1451 -> 'Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29901 -> ':'\r\n   450 -> ' The'\r\n  6273 -> ' arguments'\r\n   526 -> ' are'\r\n  6087 -> ' stored'\r\n   297 -> ' in'\r\n  1889 -> ' process'\r\n 29889 -> '.'\r\n 19218 -> 'argv'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n    13 -> '\r\n'\r\n  1678 -> '   '\r\n  1852 -> ' arg'\r\n 29894 -> 'v'\r\n 29961 -> '['\r\n 29900 -> '0'\r\n 29962 -> ']'\r\n   338 -> ' is'\r\n   278 -> ' the'\r\n  2224 -> ' path'\r\n   304 -> ' to'\r\n   278 -> ' the'\r\n  9071 -> ' Node'\r\n 29889 -> '.'\r\n  6965 -> ' js'\r\n 16813 -> ' executable'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  1678 -> '   '\r\n  1852 -> ' arg'\r\n 29894 -> 'v'\r\n 29961 -> '['\r\n 29896 -> '1'\r\n 29962 -> ']'\r\n   338 -> ' is'\r\n   278 -> ' the'\r\n  2224 -> ' path'\r\n   304 -> ' to'\r\n   278 -> ' the'\r\n  2471 -> ' script'\r\n   934 -> ' file'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  1678 -> '   '\r\n  1852 -> ' arg'\r\n 29894 -> 'v'\r\n 29961 -> '['\r\n 29906 -> '2'\r\n 29962 -> ']'\r\n   338 -> ' is'\r\n   278 -> ' the'\r\n   937 -> ' first'\r\n  2980 -> ' argument'\r\n  4502 -> ' passed'\r\n   304 -> ' to'\r\n   278 -> ' the'\r\n  2471 -> ' script'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  1678 -> '   '\r\n  1852 -> ' arg'\r\n 29894 -> 'v'\r\n 29961 -> '['\r\n 29941 -> '3'\r\n 29962 -> ']'\r\n   338 -> ' is'\r\n   278 -> ' the'\r\n  1473 -> ' second'\r\n  2980 -> ' argument'\r\n  4502 -> ' passed'\r\n   304 -> ' to'\r\n   278 -> ' the'\r\n  2471 -> ' script'\r\n   322 -> ' and'\r\n   577 -> ' so'\r\n   373 -> ' on'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n  4408 -> ' Name'\r\n   263 -> ' a'\r\n  2927 -> ' color'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  1451 -> 'Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29901 -> ':'\r\n 10924 -> ' Blue'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n\r\nmain: interactive mode on.\r\nReverse prompt: 'User:'\r\nsampling parameters: temp = 0.700000, top_k = 40, top_p = 0.500000, repeat_last_n = 256, repeat_penalty = 1.176470\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n\r\n \r\nText transcript of a never ending dialog, where User interacts with an AI assistant named ChatLLaMa.\r\nChatLLaMa is helpful, kind, honest, friendly, good at writing and never fails to answer User\u2019s requests immediately and with details and precision.\r\nThere are no annotations like (30 seconds passed...) or (to himself), just what User and ChatLLaMa say alound to each other.\r\nThe dialog lasts for years, the entirety of it is shared below. It's 10000 pages long.\r\nThe transcript only includes text, it does not include markup like HTML and Markdown.\r\n\r\nUser: Hello, ChatLLaMa!\r\nChatLLaMa: Hello User! How may I help you today?\r\nUser: What time is it?\r\nChatLLaMa: It is 02:26.\r\nUser: What year is it?\r\nChatLLaMa: We are in 2023.\r\nUser: Please tell me the largest city in Europe.\r\nChatLLaMa: The largest city in Europe is Moscow, the capital of Russia.\r\nUser: What can you tell me about Moscow?\r\nChatLLaMa: Moscow, on the Moskva River in western Russia, is the nation\u2019s cosmopolitan capital. In its historic core is the Kremlin, a complex that\u2019s home to the president and tsarist treasures in the Armoury. Outside its walls is Red Square, Russia\u2019s symbolic center.\r\nUser: What is a cat?\r\nChatLLaMa: A cat is a domestic species of small carnivorous mammal. It is the only domesticated species in the family Felidae.\r\nUser: How do I pass command line arguments to a Node.js program?\r\nChatLLaMa: The arguments are stored in process.argv.\r\n\r\n    argv[0] is the path to the Node. js executable.\r\n    argv[1] is the path to the script file.\r\n    argv[2] is the first argument passed to the script.\r\n    argv[3] is the second argument passed to the script and so on.\r\nUser: Name a color.\r\nChatLLaMa: Blue\r\nUser: What time is it with timezone at Beijing?\r\nChatLLaMa: 17:59 UTC+8 (Beijing Time)\r\nUser: Why mouses are fraid of cats?\r\nChatLLaMa: Mice are afraid of cats because they can be killed by them easily.\r\nUser: Sorry, I just made some typos.\r\nChatLLaMa: No worries! You're doing great!\r\nUser: Do you know the Rust language?\r\nChatLLaMa: Yes, I am familiar with Rust.\r\nUser: Great! Write a link list?\r\nChatLLaMa: https://rust-lang.github.io/rustc/stable-book/link_list.html\r\nUser: Current stable version?\r\nChatLLaMa: The current stable version is 1.46.0.\r\nUser: Wrong! The current stable Rust version is 1.68. You are out!\r\nChatLLaMa: I apologize for my mistake. Please try again later.\r\nUser: Current stable version?\r\nChatLLaMa: The current stable version is 1.46.0.\r\nUser: What's the stable rust version?           \r\nChatLLaMa: The current stable version is 1.46.0.\r\nUser: Wrong! I had told you that's 1.68.\r\nChatLLaMa: I apologize for my mistake. Please try again later.\r\nUser: Who is the most beautiful woman in Holiwood?\r\nChatLLaMa: Jennifer Aniston is an American actress and producer. She is best known for her role as Rachel Green on the television sitcom Friends (1994\u20132004), for which she received a Primetime Emmy Award, Golden Globe Award, and Screen Actors Guild Award. In 2005, Women\u2019s World Magazine named Aniston one of their \u201cMost Beautiful\u201d women.\r\nUser: Great!\r\nChatLLaMa: Thank you for your feedback!\r\nUser: Let's write a rust program. The program accepts an argument with int type. If the argument is 1 then print \"hello\", else print \"world\".\r\nChatLLaMa: Hello world!\r\nUser: Looks like you are wrong.\r\nChatLLaMa: What do you mean by this?\r\nUser: Please write a rust program. The program accepts an argument with int type. If the argument is 1 then print \"hello\", else print \"world\".\r\nChatLLaMa: Hello world!\r\nUser: Please write a rust program that print \"hello world, ChhatLLaMa\" \r\nChatLLaMa: Hello world, ChhatLLaMa!\r\nUser: Please write a python program which prints \"hello world, ChatLLaMa\"\r\nChatLLaMa: Hello world, ChatLLaMa!\r\nUser: How do I parse arguments from Rust?\r\nChatLLaMa: You can use the ArgumentParser class to parse command line arguments and create Arguments objects. For example, let\u2019s say we have a function called my_function() that takes two positional arguments, one of them being an integer and another string. We could pass these values using the following code:\r\n    let mut parser = ArgumentParser::new();\r\n    parser.add_argument(ArgParseOption::with_name(\"int\").required().index(0));\r\n    parser.add_argument(ArgParseOption::with_name(\"string\"));\r\n    let args = parser.parse_args(&[\"42\",\"foo\"]);\r\n    println!(\"The int value was {} and the string was {}.\", *args[0], &*args[1]);\r\n\r\n    // The above code will output something like this:\r\n    // The int value was 42 and the string was foo.\r\n\r\nUser: Which crate does the ArgumentParser can be found from?\r\nChatLLaMa: It is located in the arg-parser library which you can install with `cargo` or `apt`.\r\nUser: Great! Thanks a lot!\r\nChatLLaMa: My pleasure! Thank you for your feedback!\r\nUser: Can you tell me a joke?\r\nChatLLaMa: A man walks into a bar with a slab of asphalt under his arm and says \"A beer please, and one for the road.\"\r\nUser: Can you translate the joke into Simplified Chinese?\r\nChatLLaMa: \u4e00\u4e2a\u4eba\u5728\u4e00\u5bb6\u9152\u5427\u91cc\u9762\u62ff\u5230\u4e00\u7247\u77f3\u7816\uff0c\u8bf4\uff1a\u201c\u6211\u8981\u4e00\u676f\u9152\u548c\u4e00\u676f\u8def\u201d\u3002\r\nUser: Unfortunately I do not get the idea. Would you please explain the joke?\r\nChatLLaMa: \u8fd9\u6837\u7684\u8bdd\u4e0d\u592a\u660e\u767d\u4e86\uff1f\u8bf7\u544a\u8bc9\u6211\u8be5\u5982\u4f55\u89e3\u91ca\u8fd9\u4e2a\u7b11\u8bdd\u3002\r\nUser:  The translation is not correct. This is the right answer: \u4e00\u4e2a\u7537\u4eba\u80f3\u818a\u4e0b\u5939\u7740\u4e00\u5757\u6ca5\u9752\u8d70\u8fdb\u4e00\u5bb6\u9152\u5427\uff0c\u4ed6\u8bf4\uff1a\u201c\u8bf7\u6765\u676f\u5564\u9152\uff0c\u8fd8\u6709\u4e00\u74f6\u7528\u4e8e\u8def\u4e0a\u3002\u201d\r\nChatLLaMa: \u8fd9\u662f\u6b63\u786e\u7684\u7b54\u8bed\uff1a\u4e00\u4e2a\u7537\u4eba\u80f3\u8182\u4e0b\u5e26\u4e00\u5757\u6ca5\u9752\u8d70\u8fdb\u4e00\u5bb6\u9152\u5427\uff0c\u4ed6\u8bf4\uff1a\u201c\u8bf7\u6765\u676f\u5564\u9152\uff0c\u8fd8\u6709\u4e00\u74f6\u7528\u4e8e\u8def\u4e0a\u3002\u201d\r\nUser: Tell me why its funny?\r\nChatLLaMa: Because it's a pun!\r\nUser: Which is a pun?\r\nChatLLaMa: A man walks into a bar with a slab of asphalt under his arm and says \"A beer please, and one for the road.\"\r\nUser: Do you mean \"the road\" is a pun?\r\nChatLLaMa: Yes, because in English \"road\" can also refer to a path or journey. So when he orders another drink, that means he wants another drink on top of the first one which was already ordered (because he has not finished this drink yet). But since he did not finish his previous order before ordering again, we know he will have two drinks instead of just one. This makes us laugh at him, because he seems like an alcoholic who cannot control himself.\r\nUser: That's funny. But it's not easy to catch.        \r\nChatLLaMa: I agree. It is quite hard to get even if you are fluent in both languages.\r\nUser: \u8bf7\u628a\u4f60\u5bf9\u8fd9\u4e2a\u7b11\u8bdd\u7684\u89e3\u91ca\u7ffb\u8bd1\u6210\u4e2d\u6587\r\nChatLLaMa: \u6211\u8ba4\u4e3a\uff0c\u8fd9\u662f\u4e00\u79cd\u5f88\u8270\u96be\u7684\u63a2\u7d22\u3002\r\nUser: Please translate your answer to \"Do you mean \"the road\" is a pun?\" as Chinese.\r\n./examples/chatLLaMa: line 53: 33476 Segmentation fault: 11  ./main $GEN_OPTIONS --model \"$MODEL\" --threads \"$N_THREAD\" --n_predict \"$N_PREDICTS\" --color --interactive --reverse-prompt \"${USER_NAME}:\" --prompt \"\r\nText transcript of a never ending dialog, where ${USER_NAME} interacts with an AI assistant named ${AI_NAME}.\r\n${AI_NAME} is helpful, kind, honest, friendly, good at writing and never fails to answer ${USER_NAME}\u2019s requests immediately and with details and precision.\r\nThere are no annotations like (30 seconds passed...) or (to himself), just what ${USER_NAME} and ${AI_NAME} say alound to each other.\r\nThe dialog lasts for years, the entirety of it is shared below. It's 10000 pages long.\r\nThe transcript only includes text, it does not include markup like HTML and Markdown.\r\n\r\n$USER_NAME: Hello, $AI_NAME!\r\n$AI_NAME: Hello $USER_NAME! How may I help you today?\r\n$USER_NAME: What time is it?\r\n$AI_NAME: It is $(date +%H:%M).\r\n$USER_NAME: What year is it?\r\n$AI_NAME: We are in $(date +%Y).\r\n$USER_NAME: Please tell me the largest city in Europe.\r\n$AI_NAME: The largest city in Europe is Moscow, the capital of Russia.\r\n$USER_NAME: What can you tell me about Moscow?\r\n$AI_NAME: Moscow, on the Moskva River in western Russia, is the nation\u2019s cosmopolitan capital. In its historic core is the Kremlin, a complex that\u2019s home to the president and tsarist treasures in the Armoury. Outside its walls is Red Square, Russia\u2019s symbolic center.\r\n$USER_NAME: What is a cat?\r\n$AI_NAME: A cat is a domestic species of small carnivorous mammal. It is the only domesticated species in the family Felidae.\r\n$USER_NAME: How do I pass command line arguments to a Node.js program?\r\n$AI_NAME: The arguments are stored in process.argv.\r\n\r\n    argv[0] is the path to the Node. js executable.\r\n    argv[1] is the path to the script file.\r\n    argv[2] is the first argument passed to the script.\r\n    argv[3] is the second argument passed to the script and so on.\r\n$USER_NAME: Name a color.\r\n$AI_NAME: Blue\r\n$USER_NAME:\" \"$@\"\r\n```\r\n",
    "labels": [
      "bug",
      "duplicate",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-21T22:00:14+00:00",
    "closed_at": "2023-07-28T19:38:41+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/373/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/373"
  },
  {
    "number": 6344,
    "title": "Add support for DBRX models: dbrx-base and dbrx-instruct",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\n\r\nDatabricks just released 2 new models called DBRX (base and instruct). They have their own architecture: \r\n```json\r\n{\r\n  \"architectures\": [\r\n    \"DbrxForCausalLM\"\r\n  ],\r\n  \"attn_config\": {\r\n    \"clip_qkv\": 8,\r\n    \"kv_n_heads\": 8,\r\n    \"model_type\": \"\",\r\n    \"rope_theta\": 500000\r\n  },\r\n  \"auto_map\": {\r\n    \"AutoConfig\": \"configuration_dbrx.DbrxConfig\",\r\n    \"AutoModelForCausalLM\": \"modeling_dbrx.DbrxForCausalLM\"\r\n  },\r\n  \"d_model\": 6144,\r\n  \"emb_pdrop\": 0.0,\r\n  \"ffn_config\": {\r\n    \"ffn_hidden_size\": 10752,\r\n    \"model_type\": \"\",\r\n    \"moe_jitter_eps\": 0,\r\n    \"moe_loss_weight\": 0.05,\r\n    \"moe_num_experts\": 16,\r\n    \"moe_top_k\": 4\r\n  },\r\n  \"initializer_range\": 0.02,\r\n  \"max_seq_len\": 32768,\r\n  \"model_type\": \"dbrx\",\r\n  \"n_heads\": 48,\r\n  \"n_layers\": 40,\r\n  \"output_router_logits\": false,\r\n  \"resid_pdrop\": 0.0,\r\n  \"router_aux_loss_coef\": 0.05,\r\n  \"tie_word_embeddings\": false,\r\n  \"torch_dtype\": \"bfloat16\",\r\n  \"transformers_version\": \"4.38.2\",\r\n  \"use_cache\": true,\r\n  \"vocab_size\": 100352\r\n}\r\n```\r\n\r\n# Motivation\r\n\r\nThese models are superior to the predecessors like Llama-2 or Mixtral (even though they are larger), the community can really benefit from these two and the fine-tuned models that come after.\r\n\r\nhttps://huggingface.co/databricks/dbrx-instruct\r\n\r\n# Possible Implementation\r\n\r\nIf you have an idea as to how it can be implemented, please write a detailed description. Feel free to give links to external sources or share visuals that might be helpful to understand the details better.\r\n\r\n\r\n\r\npython llama.cpp/convert-hf-to-gguf.py\r\n```\r\nTraceback (most recent call last):\r\n  File \"/llama.cpp/convert-hf-to-gguf.py\", line 2099, in <module>\r\n    main()\r\n  File \"/llama.cpp/convert-hf-to-gguf.py\", line 2079, in main\r\n    model_class = Model.from_model_architecture(hparams[\"architectures\"][0])\r\n  File \"/llama.cpp/convert-hf-to-gguf.py\", line 215, in from_model_architecture\r\n    raise NotImplementedError(f'Architecture {arch!r} not supported!') from None\r\nNotImplementedError: Architecture 'DbrxForCausalLM' not supported!\r\n```\r\n\r\npython llama.cpp/convert.py\r\n```\r\n  File \"/llama.cpp/convert.py\", line 1486, in <module>\r\n    main()\r\n  File \"/llama.cpp/convert.py\", line 1422, in main\r\n    model_plus = load_some_model(args.model)\r\n  File \"/llama.cpp/convert.py\", line 1291, in load_some_model\r\n    model_plus = merge_multifile_models(models_plus)\r\n  File \"/llama.cpp/convert.py\", line 747, in merge_multifile_models\r\n    model = merge_sharded([mp.model for mp in models_plus])\r\n  File \"/llama.cpp/convert.py\", line 726, in merge_sharded\r\n    return {name: convert(name) for name in names}\r\n  File \"/llama.cpp/convert.py\", line 726, in <dictcomp>\r\n    return {name: convert(name) for name in names}\r\n  File \"/llama.cpp/convert.py\", line 701, in convert\r\n    lazy_tensors: list[LazyTensor] = [model[name] for model in models]\r\n  File \"/llama.cpp/convert.py\", line 701, in <listcomp>\r\n    lazy_tensors: list[LazyTensor] = [model[name] for model in models]\r\nKeyError: 'transformer.blocks.0.ffn.experts.mlp.w1'\r\n```\r\n\r\n\r\n> Dbrx is a mixture-of-experts model, which each FFN is divided into 16 experts and only 4 are activated at any given time. We build on MegaBlocks\r\nhttps://github.com/databricks/megablocks",
    "labels": [
      "enhancement",
      "model"
    ],
    "state": "closed",
    "created_at": "2024-03-27T12:34:45+00:00",
    "closed_at": "2024-04-13T09:33:53+00:00",
    "comments": 36,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6344/reactions",
      "total_count": 55,
      "+1": 40,
      "-1": 0,
      "laugh": 0,
      "hooray": 6,
      "confused": 0,
      "heart": 3,
      "rocket": 2,
      "eyes": 4
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6344"
  },
  {
    "number": 170,
    "title": "Converted GGML models hosting?",
    "body": "Apologies if Github Issues is not the right place for this question, but do you know if anyone has hosted the ggml versions of the models? The disk space required to download and convert is a little steep.",
    "labels": [
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-15T18:00:46+00:00",
    "closed_at": "2023-03-15T20:53:08+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/170/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/170"
  },
  {
    "number": 158,
    "title": "is it possible to use llama,cpp with other neural networks?",
    "body": "I have no clue about this, but I saw that chatglm-6b was published, which should run on CPU with 16GB ram, albeit very slow.\r\n[https://huggingface.co/THUDM/chatglm-6b/tree/main](url)\r\n\r\nWould it be possible to substitute the llama model?",
    "labels": [
      "enhancement",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-15T09:31:55+00:00",
    "closed_at": "2023-07-28T19:32:08+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/158/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/158"
  },
  {
    "number": 3204,
    "title": "Blenderbot Support?",
    "body": "Hello,\r\nWould it be possible to support [Blenderbot](https://parl.ai/projects/blenderbot2/) ([model](https://huggingface.co/hyunwoongko/blenderbot-9B))?\r\nThank you!",
    "labels": [
      "enhancement",
      "model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-09-16T01:10:06+00:00",
    "closed_at": "2024-04-03T01:16:04+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3204/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3204"
  },
  {
    "number": 3061,
    "title": "llama : add Refact support",
    "body": "This is a new 1.6B code model: https://huggingface.co/smallcloudai/Refact-1_6B-fim\r\n\r\nWe should look into adding support into `llama.cpp` similar as to how we did for Falcon and Baichuan:\r\n\r\n- #2717 \r\n- #3009 \r\n\r\nI haven't looked into the architecture yet, but I'm hoping it is similar to GPT, which we already know how to handle OK thanks to the Falcon experience. I see it also uses Alibi which we should have support for, but we haven't tested extensively, so there might be issues there.",
    "labels": [
      "help wanted",
      "good first issue",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-09-07T13:54:25+00:00",
    "closed_at": "2023-10-04T13:23:41+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3061/reactions",
      "total_count": 15,
      "+1": 13,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3061"
  },
  {
    "number": 6592,
    "title": "Is official mixtral 8x22b working properly? ",
    "body": "Is official mixtral 8x22b working properly with llamacpp? ",
    "labels": [
      "enhancement",
      "model"
    ],
    "state": "closed",
    "created_at": "2024-04-10T17:06:11+00:00",
    "closed_at": "2024-04-10T17:09:54+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6592/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6592"
  },
  {
    "number": 1328,
    "title": "Support for BioGPT",
    "body": "I didn't see any prior discussion of this. A few months ago this GPT model [was released](https://github.com/microsoft/BioGPT) which seemed to do very well with medical Q&A. \r\n\r\nI don't see any indication that it's based on the existing GPT variations. There's a couple demos here: [Q&A demo](https://huggingface.co/spaces/katielink/biogpt-qa-demo) and [BioGPT-Large demo](https://huggingface.co/spaces/katielink/biogpt-large-demo).",
    "labels": [
      "model"
    ],
    "state": "closed",
    "created_at": "2023-05-04T21:53:44+00:00",
    "closed_at": "2023-07-07T13:42:50+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1328/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1328"
  },
  {
    "number": 846,
    "title": "llama : add RWKV models support",
    "body": "RWKV (100% RNN) language model, which is the only RNN (as of now) that can match transformers in quality and scaling, while being faster and saves memory.\r\n\r\nInfo: https://github.com/BlinkDL/ChatRWKV\r\n\r\nRWKV is a novel large language model architecture, [with the largest model in the family having 14B parameters](https://huggingface.co/BlinkDL/rwkv-4-pile-14b). In contrast to Transformer with O(n^2) attention, RWKV requires only state from previous step to calculate logits. This makes RWKV very CPU-friendly on large context lenghts.\r\n\r\nExperimental GGML port: https://github.com/saharNooby/rwkv.cpp\r\n\r\nThe lastest \"Raven\"-series Alpaca-style-tuned RWKV 14B & 7B models are very good.\r\nOnline demo: https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B\r\nDownload: https://huggingface.co/BlinkDL/rwkv-4-raven\r\n\r\n----\r\n\r\n*Edit by @ggerganov:*\r\n\r\nAdding @BlinkDL's comment below to OP for visibility:\r\n\r\n> v4 inference: https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py\r\n>\r\n> v5 inference: https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_v5_demo.py\r\n>\r\n> fast v4 & v5.2 inference: https://github.com/BlinkDL/ChatRWKV/blob/main/rwkv_pip_package/src/rwkv/model.py\r\n>\r\n> v5.2 1.5B demo (great for its size): https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio\r\n>\r\n> v5.2 1.5B benchmarks: https://twitter.com/BlinkDL_AI/status/1717543614434402661\r\n>\r\n> a few remarks:\r\n> * rwkv models have RNN-style \"one\" mode, and GPT-style \"seq\" mode\r\n> * i am actually using exp(-exp(w))\r\n> * seems it's good to precompute embedding+emb_layernorm in bf16\r\n> * when using fp16, i am doing /2 every 6 layers, to avoid overflow\r\n",
    "labels": [
      "help wanted",
      "good first issue",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-04-08T06:32:31+00:00",
    "closed_at": "2024-09-01T14:38:18+00:00",
    "comments": 39,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/846/reactions",
      "total_count": 44,
      "+1": 44,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/846"
  },
  {
    "number": 3076,
    "title": "Support starcoder family architectures (1B/3B/7B/13B)",
    "body": "Related Issues:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/issues/1901\r\nhttps://github.com/ggerganov/llama.cpp/issues/1441\r\nhttps://github.com/ggerganov/llama.cpp/issues/1326\r\n\r\nPreviously, it wasn't recommended to incorporate non-llama architectures into llama.cpp. However, in light of the recent addition of the Falcon architecture (see [Pull Request #2717](https://github.com/ggerganov/llama.cpp/pull/2717)), it might be worth reconsidering this stance.\r\n\r\nOne distinguishing feature of Starcoder is its ability to provide a complete series of models ranging from 1B to 13B. This capability can prove highly beneficial for speculative decoding and making coding models available for edge devices (e.g., M1/M2 Macs).\r\n\r\nI can contribute the PR if it matches llama.cpp's roadmap.",
    "labels": [
      "model"
    ],
    "state": "closed",
    "created_at": "2023-09-08T02:40:11+00:00",
    "closed_at": "2023-09-15T19:15:21+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3076/reactions",
      "total_count": 9,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 6,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3076"
  },
  {
    "number": 361,
    "title": "Invalid model error : too old, regenerate your model files!",
    "body": "Downloaded Alpaca 7B model successfully using the following command as mentioned in README.md:\r\n`curl -o ./models/ggml-alpaca-7b-q4.bin -C - https://gateway.estuary.tech/gw/ipfs/QmUp1UGeQFDqJKvtjbSYPBiZZKRjLp8shVP9hT8ZB9Ynv1`\r\n\r\nWhen I try to execute the command:\r\n`main -m ./models/ggml-alpaca-7b-q4.bin --color -f ./prompts/alpaca.txt -ins`\r\n\r\nThis is the error output:\r\nmain: seed = 1679417098\r\nllama_model_load: loading model from './models/ggml-alpaca-7b-q4.bin' - please wait ...\r\nllama_model_load: invalid model file './models/ggml-alpaca-7b-q4.bin' (too old, regenerate your model files!)\r\nmain: failed to load model from './models/ggml-alpaca-7b-q4.bin'\r\n\r\nHow to fix this? Is the downloaded model corrupted and should I download it again? What is the SHA1 hash of the model so that I can verify that the downloaded model is corrupted or not?",
    "labels": [
      "documentation",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-21T16:51:17+00:00",
    "closed_at": "2023-03-22T05:54:53+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/361/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/361"
  }
]
[
  {
    "number": 7337,
    "title": "Segmentation Fault on GPU",
    "body": "When I am trying to run the following finetuning command on GPU:\r\n**nohup ../build/bin/finetune --model-base llama-3b-Q5_0.gguf --train-data \"shakespeare.txt\" --save-every 1 --adam-iter 2 --batch 4 --ctx 4 --lora-out ../../training/checkpoints/llama_3b_q5_ctx_4_batch_4_threads_6/lora.bin --checkpoint-in ../../training/checkpoints/llama_3b_q5_ctx_4_batch_4_threads_6/checkpoint.gguf --checkpoint-out ../../training/checkpoints/llama_3b_q5_ctx_4_batch_4_threads_6/checkpoint-ITERATION.gguf > ../../training/checkpoints/llama_3b_q5_ctx_4_batch_4_threads_6/training_logs.out -ngl 33**\r\n\r\nI get **segmentation fault** error with ever increasing **nohup.out** file:\r\n\r\nllama_model_loader: loaded meta data with 24 key-value pairs and 237 tensors from llama-3b-Q5_0.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = models\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 2048\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 3200\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 26\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8640\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 100\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 8\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,60820]   = [\"\u2581 t\", \"\u2581 a\", \"i n\", \"h e\", \"\u2581...\r\nllama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   53 tensors\r\nllama_model_loader: - type q5_0:  183 tensors\r\nllama_model_loader: - type q8_0:    1 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 2048\r\nllm_load_print_meta: n_embd           = 3200\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_layer          = 26\r\nllm_load_print_meta: n_rot            = 100\r\nllm_load_print_meta: n_embd_head_k    = 100\r\nllm_load_print_meta: n_embd_head_v    = 100\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 3200\r\nllm_load_print_meta: n_embd_v_gqa     = 3200\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 8640\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 2048\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 3B\r\nllm_load_print_meta: model ftype      = Q5_0\r\nllm_load_print_meta: model params     = 3.43 B\r\nllm_load_print_meta: model size       = 2.23 GiB (5.59 BPW) \r\nllm_load_print_meta: general.name     = models\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\r\nggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA T4G, compute capability 7.5, VMM: yes\r\nllm_load_tensors: ggml ctx size =    0.24 MiB\r\nllm_load_tensors: offloading 26 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 27/27 layers to GPU\r\nllm_load_tensors:        CPU buffer size =    67.14 MiB\r\nllm_load_tensors:      CUDA0 buffer size =  2216.65 MiB\r\n...............................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:      CUDA0 KV buffer size =   162.50 MiB\r\nllama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB\r\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =    68.75 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =     7.26 MiB\r\nllama_new_context_with_model: graph nodes  = 838\r\nllama_new_context_with_model: graph splits = 2\r\nmain: seed: 1715928042\r\nmain: model base = 'llama-3b-Q5_0.gguf'\r\nmain: init model\r\nprint_params: n_vocab               : 32000\r\nprint_params: n_ctx                 : 4\r\nprint_params: n_embd                : 3200\r\nprint_params: n_ff                  : 8640\r\nprint_params: n_head                : 32\r\nprint_params: n_head_kv             : 32\r\nprint_params: n_layer               : 26\r\nprint_params: norm_rms_eps          : 0.000001\r\nprint_params: rope_freq_base        : 10000.000000\r\nprint_params: rope_freq_scale       : 1.000000\r\nprint_lora_params: n_rank_attention_norm : 1\r\nprint_lora_params: n_rank_wq             : 4\r\nprint_lora_params: n_rank_wk             : 4\r\nprint_lora_params: n_rank_wv             : 4\r\nprint_lora_params: n_rank_wo             : 4\r\nprint_lora_params: n_rank_ffn_norm       : 1\r\nprint_lora_params: n_rank_ffn_gate       : 4\r\nprint_lora_params: n_rank_ffn_down       : 4\r\nprint_lora_params: n_rank_ffn_up         : 4\r\nprint_lora_params: n_rank_tok_embeddings : 4\r\nprint_lora_params: n_rank_norm           : 1\r\nprint_lora_params: n_rank_output         : 4\r\nmain: total train_iterations 0\r\nmain: seen train_samples     0\r\nmain: seen train_tokens      0\r\nmain: completed train_epochs 0\r\nmain: lora_size = 54844064 bytes (52.3 MB)\r\nmain: opt_size  = 81694048 bytes (77.9 MB)\r\nmain: opt iter 0\r\nmain: input_size = 2048096 bytes (2.0 MB)\r\nmain: compute_size = 846062208 bytes (806.9 MB)\r\nmain: evaluation order = RIGHT_TO_LEFT\r\nmain: tokenize training data from shakespeare.txt\r\nmain: sample-start: \r\nmain: include-sample-start: false\r\ntokenize_file: total number of samples: 26826\r\nmain: number of training tokens: 26830\r\nmain: number of unique tokens: 3320\r\nmain: train data seems to have changed. restarting shuffled epoch.\r\nmain: begin training\r\nmain: work_size = 768376 bytes (0.7 MB)\r\ntrain_opt_callback: iter=     0 sample=1/26826 sched=0.000000 loss=0.000000 |----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- \r\n\r\nIt get's stuck on '-' character and keeps on printing that without any progress and leads to segmentation fault finally",
    "labels": [
      "training",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-17T09:17:40+00:00",
    "closed_at": "2024-08-09T01:07:02+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7337/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7337"
  },
  {
    "number": 2049,
    "title": "Training Custom using train-text-from-scratch",
    "body": "Hi.\r\n\r\nI have been trying to train some custom data using the base model file: open-llama-7B-open-instruct.ggmlv3.q4_0.bin\r\n\r\nHere is the command Im running\r\n\r\n`./bin/train-text-from-scratch \\\r\n        --vocab-model ../models/ggml-vocab.bin \\\r\n        --ctx 64 --embd 256 --head 8 --layer 16 \\\r\n        --checkpoint-in  ik.bin \\\r\n        --checkpoint-out ik.bin \\\r\n        --model-out ik.bin \\\r\n        --train-data ik.txt \\\r\n        -t 6 -b 16 -n 32 --seed 1 --adam-iter 16 \\\r\n        --print-details-interval 0 --predict 16 \\\r\n        --mem-model 12 --mem-compute 12  --use-flash`\r\n\r\n\r\nI'm able to train the data but I have following 2 concerns:\r\n\r\n1. How Can I pass multiple text files instead of 1 text file? I have about 1 lakh+ text file which I need to train the model. Should I combine the text files?\r\n2. I have a GPU of 24GB VRAM. But its not able to utilize more than 1gb and hence the process of training is slow. ",
    "labels": [
      "training",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-06-29T15:21:57+00:00",
    "closed_at": "2024-04-09T01:08:35+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2049/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2049"
  }
]
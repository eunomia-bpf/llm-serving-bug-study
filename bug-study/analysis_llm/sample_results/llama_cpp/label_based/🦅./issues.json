[
  {
    "number": 2850,
    "title": "falcon : speed-up prompt processing",
    "body": "The performance of Falcon 7B should be comparable to LLaMA 7B since the computation graph is computationally very similar.\r\n\r\nHere are the current numbers on M2 Ultra for LLaMA, LLaMA-v2 and Falcon 7B:\r\n\r\n```bash\r\n../scripts/run-all-perf.sh ${model} \"f16 q8_0 q4_0\"\r\n```\r\n\r\n| model                          |       size |     params | backend    | ngl | test       |              t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ---------- | ---------------: |\r\n| LLaMA 7B mostly F16            |  12.55 GiB |     6.74 B | Metal      | 999 | pp 512     |    665.95 \u00b1 0.18 |\r\n| LLaMA 7B mostly Q8_0           |   6.64 GiB |     6.74 B | Metal      | 999 | pp 512     |    630.28 \u00b1 0.16 |\r\n| LLaMA 7B mostly Q4_0           |   3.56 GiB |     6.74 B | Metal      | 999 | pp 512     |    632.32 \u00b1 0.22 |\r\n| LLaMA 7B mostly F16            |  12.55 GiB |     6.74 B | Metal      | 999 | tg 64      |     29.73 \u00b1 0.01 |\r\n| LLaMA 7B mostly Q8_0           |   6.64 GiB |     6.74 B | Metal      | 999 | tg 64      |     61.47 \u00b1 0.06 |\r\n| LLaMA 7B mostly Q4_0           |   3.56 GiB |     6.74 B | Metal      | 999 | tg 64      |     86.96 \u00b1 0.08 |\r\n\r\nbuild: dd0dc36 (1100)\r\n\r\n| model                          |       size |     params | backend    | ngl | test       |              t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ---------- | ---------------: |\r\n| llama2 7B mostly F16           |  12.55 GiB |     6.74 B | Metal      | 999 | pp 512     |    666.12 \u00b1 0.10 |\r\n| llama2 7B mostly Q8_0          |   6.64 GiB |     6.74 B | Metal      | 999 | pp 512     |    630.21 \u00b1 0.20 |\r\n| llama2 7B mostly Q4_0          |   3.56 GiB |     6.74 B | Metal      | 999 | pp 512     |    632.32 \u00b1 0.17 |\r\n| llama2 7B mostly F16           |  12.55 GiB |     6.74 B | Metal      | 999 | tg 64      |     29.74 \u00b1 0.02 |\r\n| llama2 7B mostly Q8_0          |   6.64 GiB |     6.74 B | Metal      | 999 | tg 64      |     61.55 \u00b1 0.04 |\r\n| llama2 7B mostly Q4_0          |   3.56 GiB |     6.74 B | Metal      | 999 | tg 64      |     86.88 \u00b1 0.08 |\r\n\r\nbuild: dd0dc36 (1100)\r\n\r\n| model                          |       size |     params | backend    | ngl | test       |              t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ---------- | ---------------: |\r\n| Falcon 7B mostly F16           |  13.44 GiB |     7.22 B | Metal      | 999 | pp 512     |    403.68 \u00b1 1.27 |\r\n| Falcon 7B mostly Q8_0          |   7.14 GiB |     7.22 B | Metal      | 999 | pp 512     |    390.41 \u00b1 1.77 |\r\n| Falcon 7B mostly Q4_0          |   3.92 GiB |     7.22 B | Metal      | 999 | pp 512     |    390.94 \u00b1 1.75 |\r\n| Falcon 7B mostly F16           |  13.44 GiB |     7.22 B | Metal      | 999 | tg 64      |     29.47 \u00b1 0.01 |\r\n| Falcon 7B mostly Q8_0          |   7.14 GiB |     7.22 B | Metal      | 999 | tg 64      |     60.01 \u00b1 0.05 |\r\n| Falcon 7B mostly Q4_0          |   3.92 GiB |     7.22 B | Metal      | 999 | tg 64      |     86.07 \u00b1 0.02 |\r\n\r\nbuild: 611363a (1110)\r\n\r\nAlthough the Text Generation speed for Falcon is comparable to LLaMA, I observe a significant performance drop in the Prompt Processing task. This is on M2 Ultra with Metal, but I think last time I checked, the CUDA performance experiences a similar drop.\r\n\r\n### Hypothesis\r\n\r\nI haven't profiled the run yet, but I suspect the cause is in the concatenated `QKV` matrix multiplication (MM):\r\n\r\nhttps://github.com/ggerganov/llama.cpp/blob/dd0dc366dab10e8df28d3924e7f313b5c695e908/llama.cpp#L2634-L2638\r\n\r\nFor some reason, this is probably slower compared to what we do in LLaMA, where we have separated the `QKV` tensor into 3 individual `Q`, `K` and `V` tensors:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/blob/dd0dc366dab10e8df28d3924e7f313b5c695e908/llama.cpp#L2292-L2306\r\n\r\nWe should either speed-up the current `QKV` implementation, or change the convert script to output:\r\n\r\n- `LLM_TENSOR_ATTN_Q`\r\n- `LLM_TENSOR_ATTN_K`\r\n- `LLM_TENSOR_ATTN_V`\r\n\r\ninstead of:\r\n\r\n- `LLM_TENSOR_ATTN_QKV`\r\n\r\nhttps://github.com/ggerganov/llama.cpp/blob/dd0dc366dab10e8df28d3924e7f313b5c695e908/convert-falcon-hf-to-gguf.py#L220-L239\r\n\r\nMy intuition is that if this is indeed the reason for the slow-down in Falcon, the combined `QKV` approach, if optimized correctly, should yield better performance than the other since we do one single MM instead of 3 separate MMs. So we should also consider switching LLaMA graph if this turns out to be the case and we are able to optimize it and the improvement is significant\r\n\r\nEdit: some more results with CUDA on RTX 4080\r\n\r\n```bash\r\n../scripts/run-all-perf.sh ${model} \"f16 q8_0 q4_0\" \"-ngl 999 -t 1 -n 64\"\r\n```\r\n\r\n| model                          |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama2 7B mostly F16           |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   2122.08 \u00b1 0.58 |\r\n| llama2 7B mostly Q8_0          |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   3343.20 \u00b1 9.31 |\r\n| llama2 7B mostly Q4_0          |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  3439.35 \u00b1 10.13 |\r\n| llama2 7B mostly F16           |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 64      |     45.95 \u00b1 0.01 |\r\n| llama2 7B mostly Q8_0          |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 64      |     77.85 \u00b1 0.01 |\r\n| llama2 7B mostly Q4_0          |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 64      |    130.43 \u00b1 0.01 |\r\n\r\nbuild: 611363a (1110)\r\n\r\n| model                          |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| Falcon 7B mostly F16           |  13.44 GiB |     7.22 B | CUDA       | 999 |          1 | pp 512     |   1885.04 \u00b1 2.29 |\r\n| Falcon 7B mostly Q8_0          |   7.14 GiB |     7.22 B | CUDA       | 999 |          1 | pp 512     |   2849.60 \u00b1 5.96 |\r\n| Falcon 7B mostly Q4_0          |   3.92 GiB |     7.22 B | CUDA       | 999 |          1 | pp 512     |   2754.78 \u00b1 6.54 |\r\n| Falcon 7B mostly F16           |  13.44 GiB |     7.22 B | CUDA       | 999 |          1 | tg 64      |     36.07 \u00b1 0.01 |\r\n| Falcon 7B mostly Q8_0          |   7.14 GiB |     7.22 B | CUDA       | 999 |          1 | tg 64      |     54.68 \u00b1 0.03 |\r\n| Falcon 7B mostly Q4_0          |   3.92 GiB |     7.22 B | CUDA       | 999 |          1 | tg 64      |     75.76 \u00b1 0.01 |\r\n\r\nbuild: 611363a (1110)\r\n",
    "labels": [
      "good first issue",
      "performance",
      "\ud83e\udd85."
    ],
    "state": "closed",
    "created_at": "2023-08-28T09:51:27+00:00",
    "closed_at": "2023-09-15T08:09:25+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2850/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2850"
  }
]
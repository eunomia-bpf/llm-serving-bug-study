[
  {
    "number": 12945,
    "title": "Compile bug: Cann x86_64 not building",
    "body": "### Git commit\n\n```\n[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\n/llama.cpp/ggml/src/ggml-cann/aclnn_ops.cpp: In function 'void ggml_cann_get_rows(ggml_backend_cann_context&, ggml_tensor*)':\n/llama.cpp/ggml/src/ggml-cann/aclnn_ops.cpp:1786:49: error: 'float16_t' was not declared in this scope; did you mean 'float_t'?\n 1786 |                 src0->data, ACL_FLOAT16, sizeof(float16_t), scale_ne, scale_nb,\n      |                                                 ^~~~~~~~~\n      |                                                 float_t\ngmake[2]: *** [ggml/src/ggml-cann/CMakeFiles/ggml-cann.dir/build.make:90: ggml/src/ggml-cann/CMakeFiles/ggml-cann.dir/aclnn_ops.cpp.o] Error 1\ngmake[2]: *** Waiting for unfinished jobs....\ngmake[1]: *** [CMakeFiles/Makefile2:1790: ggml/src/ggml-cann/CMakeFiles/ggml-cann.dir/all] Error 2\ngmake[1]: *** Waiting for unfinished jobs....\n[ 13%] Linking CXX shared library ../../bin/libggml-cpu.so\n[ 13%] Built target ggml-cpu\ngmake: *** [Makefile:146: all] Error 2\nError: building at STEP \"RUN build_llama_and_whisper.sh \"cann\"\": while running runtime: exit status 2\nmake: *** [Makefile:89: build] Error 2\n```\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nVulkan\n\n### Problem description & steps to reproduce\n\nBuilding cann on x86_64\n\n### First Bad Commit\n\n_No response_\n\n### Compile command\n\n```shell\n\"-DGGML_CANN=ON\" \"-DSOC_TYPE=Ascend910B3\"\n```\n\n### Relevant log output\n\n```shell\n[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\n/llama.cpp/ggml/src/ggml-cann/aclnn_ops.cpp: In function 'void ggml_cann_get_rows(ggml_backend_cann_context&, ggml_tensor*)':\n/llama.cpp/ggml/src/ggml-cann/aclnn_ops.cpp:1786:49: error: 'float16_t' was not declared in this scope; did you mean 'float_t'?\n 1786 |                 src0->data, ACL_FLOAT16, sizeof(float16_t), scale_ne, scale_nb,\n      |                                                 ^~~~~~~~~\n      |                                                 float_t\ngmake[2]: *** [ggml/src/ggml-cann/CMakeFiles/ggml-cann.dir/build.make:90: ggml/src/ggml-cann/CMakeFiles/ggml-cann.dir/aclnn_ops.cpp.o] Error 1\ngmake[2]: *** Waiting for unfinished jobs....\ngmake[1]: *** [CMakeFiles/Makefile2:1790: ggml/src/ggml-cann/CMakeFiles/ggml-cann.dir/all] Error 2\ngmake[1]: *** Waiting for unfinished jobs....\n[ 13%] Linking CXX shared library ../../bin/libggml-cpu.so\n[ 13%] Built target ggml-cpu\ngmake: *** [Makefile:146: all] Error 2\nError: building at STEP \"RUN build_llama_and_whisper.sh \"cann\"\": while running runtime: exit status 2\nmake: *** [Makefile:89: build] Error 2\n```",
    "labels": [
      "bug",
      "build",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2025-04-14T15:41:52+00:00",
    "closed_at": "2025-04-15T10:39:22+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12945/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12945"
  },
  {
    "number": 12010,
    "title": "[CANN] Compile bug: no matching function for call to 'CastIntrinsicsImpl' Ascend NPU issues specific to Ascend NPUs",
    "body": "I encountered the same issue(#10556 ) in Ascend310B1 as well. \n```\nroot@orangepiaipro-20t:/data/llama.cpp# cmake -B build -DGGML_CANN=on -DCMAKE_BUILD_TYPE=release\n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n-- CMAKE_SYSTEM_PROCESSOR: aarch64\n-- Including CPU backend\n-- ARM detected\n-- ARM feature FMA enabled\n-- Adding CPU backend variant ggml-cpu:  \n-- CANN: updated CANN_INSTALL_DIR from ASCEND_TOOLKIT_HOME=/usr/local/Ascend/ascend-toolkit/latest\n-- CANN: SOC_VERSION auto-detected is:Ascend310B1\n-- CANN: compile ascend kernels witch SOC_TYPE:Ascend310B1, SOC_VERSION:ascend310b1, compile macro:-DASCEND_310B.\n-- CANN: CANN_INCLUDE_DIRS =  /usr/local/Ascend/ascend-toolkit/latest/include;/usr/local/Ascend/ascend-toolkit/latest/include/aclnn;/usr/local/Ascend/ascend-toolkit/latest/acllib/include\n-- CANN: CANN_LIBRARIES =  ascendcl;nnopbase;opapi;acl_op_compiler;ascendc_kernels\n-- Including CANN backend\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /data/llama.cpp/build\n```\n\n```\nroot@orangepiaipro-20t:/data/llama.cpp# cmake --build build --config release\nConsolidate compiler generated dependencies of target ggml-base\n[  3%] Built target ggml-base\n[  3%] Performing build step for 'ascendc_kernels_precompile'\nConsolidate compiler generated dependencies of target precompile_obj\n[ 12%] Building CXX object CMakeFiles/precompile_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/dup.cpp.o\n[ 25%] Building CXX object CMakeFiles/precompile_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/get_row_f16.cpp.o\n[ 37%] Building CXX object CMakeFiles/precompile_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/get_row_f32.cpp.o\n[ 50%] Building CXX object CMakeFiles/precompile_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp.o\n[ 62%] Building CXX object CMakeFiles/precompile_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q8_0.cpp.o\n[ 75%] Building CXX object CMakeFiles/precompile_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/quantize_f16_q8_0.cpp.o\n[ 87%] Building CXX object CMakeFiles/precompile_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/quantize_f32_q8_0.cpp.o\n[100%] Building CXX object CMakeFiles/precompile_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/quantize_float_to_q4_0.cpp.o\n[100%] Built target precompile_obj\n[100%] Built target check_src_template\n[  4%] No install step for 'ascendc_kernels_precompile'\n[  4%] Completed 'ascendc_kernels_precompile'\n[  6%] Built target ascendc_kernels_precompile\n[  7%] Performing build step for 'ascendc_kernels_preprocess'\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /data/llama.cpp/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build\nConsolidate compiler generated dependencies of target preprocess_obj\n[  6%] Building CXX object CMakeFiles/preprocess_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/dup.cpp.o\n[ 12%] Building CXX object CMakeFiles/preprocess_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/get_row_f16.cpp.o\n[ 18%] Building CXX object CMakeFiles/preprocess_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/get_row_f32.cpp.o\n[ 25%] Building CXX object CMakeFiles/preprocess_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp.o\n[ 31%] Building CXX object CMakeFiles/preprocess_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q8_0.cpp.o\n[ 37%] Building CXX object CMakeFiles/preprocess_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/quantize_f16_q8_0.cpp.o\n[ 43%] Building CXX object CMakeFiles/preprocess_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/quantize_f32_q8_0.cpp.o\n[ 50%] Building CXX object CMakeFiles/preprocess_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/quantize_float_to_q4_0.cpp.o\n[ 50%] Built target preprocess_obj\nConsolidate compiler generated dependencies of target m200_obj\n[ 56%] Building CXX object CMakeFiles/m200_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/dup.cpp.o\nIn file included from /data/llama.cpp/ggml/src/ggml-cann/kernels/dup.cpp:1:\nIn file included from /usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h:28:\nIn file included from /usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h:24:\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm:947:9: warning: 'DataCopyPadUB2GMImpl<half>' is deprecated: NOTICE: DataCopyPad is not deprecated. Currently, DataCopyPad is an unsupported API on current device. Please check your code! [-Wdeprecated-declarations]\n        DataCopyPadUB2GMImpl((__gm__ PrimType*)dstGlobal.GetPhyAddr(), (__ubuf__ PrimType*)srcLocal.GetPhyAddr(),\n        ^\n/data/llama.cpp/ggml/src/ggml-cann/kernels/dup.cpp:83:9: note: in instantiation of function template specialization 'AscendC::DataCopyPad<half>' requested here\n        DataCopyPad(dst_gm, dst_local, dataCopyParams);\n        ^\n/data/llama.cpp/ggml/src/ggml-cann/kernels/dup.cpp:101:9: note: in instantiation of member function 'DupByRows<half, half>::copy_out' requested here\n        copy_out();\n        ^\n/data/llama.cpp/ggml/src/ggml-cann/kernels/dup.cpp:166:8: note: in instantiation of member function 'DupByRows<half, half>::dup' requested here\n    op.dup();\n       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_data_copy_impl.h:1187:3: note: 'DataCopyPadUB2GMImpl<half>' has been explicitly marked deprecated here\n[[deprecated(\"NOTICE: DataCopyPad is not deprecated. Currently, DataCopyPad is an unsupported API on current device. \"\n  ^\nIn file included from /data/llama.cpp/ggml/src/ggml-cann/kernels/dup.cpp:1:\nIn file included from /usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h:28:\nIn file included from /usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h:24:\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm:947:9: warning: 'DataCopyPadUB2GMImpl<float>' is deprecated: NOTICE: DataCopyPad is not deprecated. Currently, DataCopyPad is an unsupported API on current device. Please check your code! [-Wdeprecated-declarations]\n        DataCopyPadUB2GMImpl((__gm__ PrimType*)dstGlobal.GetPhyAddr(), (__ubuf__ PrimType*)srcLocal.GetPhyAddr(),\n        ^\n/data/llama.cpp/ggml/src/ggml-cann/kernels/dup.cpp:83:9: note: in instantiation of function template specialization 'AscendC::DataCopyPad<float>' requested here\n        DataCopyPad(dst_gm, dst_local, dataCopyParams);\n        ^\n/data/llama.cpp/ggml/src/ggml-cann/kernels/dup.cpp:101:9: note: in instantiation of member function 'DupByRows<float, float>::copy_out' requested here\n        copy_out();\n        ^\n/data/llama.cpp/ggml/src/ggml-cann/kernels/dup.cpp:188:8: note: in instantiation of member function 'DupByRows<float, float>::dup' requested here\n    op.dup();\n       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_data_copy_impl.h:1187:3: note: 'DataCopyPadUB2GMImpl<float>' has been explicitly marked deprecated here\n[[deprecated(\"NOTICE: DataCopyPad is not deprecated. Currently, DataCopyPad is an unsupported API on current device. \"\n  ^\n2 warnings generated.\n[ 62%] Building CXX object CMakeFiles/m200_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/get_row_f16.cpp.o\nIn file included from /data/llama.cpp/ggml/src/ggml-cann/kernels/get_row_f16.cpp:1:\nIn file included from /usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h:28:\nIn file included from /usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h:24:\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm:947:9: warning: 'DataCopyPadUB2GMImpl<float>' is deprecated: NOTICE: DataCopyPad is not deprecated. Currently, DataCopyPad is an unsupported API on current device. Please check your code! [-Wdeprecated-declarations]\n        DataCopyPadUB2GMImpl((__gm__ PrimType*)dstGlobal.GetPhyAddr(), (__ubuf__ PrimType*)srcLocal.GetPhyAddr(),\n        ^\n/data/llama.cpp/ggml/src/ggml-cann/kernels/get_row_f16.cpp:95:13: note: in instantiation of function template specialization 'AscendC::DataCopyPad<float>' requested here\n            DataCopyPad(output_gm[offset + len], output_local[len],\n            ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_data_copy_impl.h:1187:3: note: 'DataCopyPadUB2GMImpl<float>' has been explicitly marked deprecated here\n[[deprecated(\"NOTICE: DataCopyPad is not deprecated. Currently, DataCopyPad is an unsupported API on current device. \"\n  ^\n1 warning generated.\n[ 68%] Building CXX object CMakeFiles/m200_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/get_row_f32.cpp.o\nIn file included from /data/llama.cpp/ggml/src/ggml-cann/kernels/get_row_f32.cpp:1:\nIn file included from /usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h:28:\nIn file included from /usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_intf.h:24:\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_data_copy_intf.cppm:947:9: warning: 'DataCopyPadUB2GMImpl<float>' is deprecated: NOTICE: DataCopyPad is not deprecated. Currently, DataCopyPad is an unsupported API on current device. Please check your code! [-Wdeprecated-declarations]\n        DataCopyPadUB2GMImpl((__gm__ PrimType*)dstGlobal.GetPhyAddr(), (__ubuf__ PrimType*)srcLocal.GetPhyAddr(),\n        ^\n/data/llama.cpp/ggml/src/ggml-cann/kernels/get_row_f32.cpp:89:13: note: in instantiation of function template specialization 'AscendC::DataCopyPad<float>' requested here\n            DataCopyPad(output_gm[offset + len], output_local[len],\n            ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_data_copy_impl.h:1187:3: note: 'DataCopyPadUB2GMImpl<float>' has been explicitly marked deprecated here\n[[deprecated(\"NOTICE: DataCopyPad is not deprecated. Currently, DataCopyPad is an unsupported API on current device. \"\n  ^\n1 warning generated.\n[ 75%] Building CXX object CMakeFiles/m200_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp.o\nIn file included from /data/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp:1:\nIn file included from /usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h:27:\nIn file included from /usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h:48:\nIn file included from /usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h:28:\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:455:5: error: no matching function for call to 'CastIntrinsicsImpl'\n    CastIntrinsicsImpl(dst, src, roundMode, 1, repeatParams);\n    ^~~~~~~~~~~~~~~~~~\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm:128:5: note: in instantiation of function template specialization 'AscendC::CastImpl<half, AscendC::IntegerSubType<4, true>>' requested here\n    CastImpl((__ubuf__ T1*)dstLocal.GetPhyAddr(), (__ubuf__ T2*)srcLocal.GetPhyAddr(), round_mode, calCount);\n    ^\n/data/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp:122:9: note: in instantiation of function template specialization 'AscendC::Cast<half, AscendC::IntegerSubType<4, true>>' requested here\n        Cast(cast_local, input_local, RoundMode::CAST_NONE, QK4_0);\n        ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:26:24: note: candidate function not viable: no known conversion from '__ubuf__ AscendC::IntegerSubType<4, true> *' to '__ubuf__ int32_t *' (aka '__ubuf__ int *') for 2nd argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ half* dst, __ubuf__ int32_t* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:33:24: note: candidate function not viable: no known conversion from '__ubuf__ AscendC::IntegerSubType<4, true> *' to '__ubuf__ int8_t *' (aka '__ubuf__ signed char *') for 2nd argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ half* dst, __ubuf__ int8_t* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:47:24: note: candidate function not viable: no known conversion from '__ubuf__ AscendC::IntegerSubType<4, true> *' to '__ubuf__ uint8_t *' (aka '__ubuf__ unsigned char *') for 2nd argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ half* dst, __ubuf__ uint8_t* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:215:24: note: candidate function not viable: no known conversion from '__ubuf__ AscendC::IntegerSubType<4, true> *' to '__ubuf__ float *' for 2nd argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ half* dst, __ubuf__ float* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:334:24: note: candidate function not viable: no known conversion from '__ubuf__ AscendC::IntegerSubType<4, true> *' to '__ubuf__ int16_t *' (aka '__ubuf__ short *') for 2nd argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ half* dst, __ubuf__ int16_t* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:61:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ float *' for 1st argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ float* dst, __ubuf__ int32_t* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:75:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ float *' for 1st argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ float* dst, __ubuf__ half* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:89:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int32_t *' (aka '__ubuf__ int *') for 1st argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int32_t* dst, __ubuf__ half* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:131:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int8_t *' (aka '__ubuf__ signed char *') for 1st argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int8_t* dst, __ubuf__ half* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:173:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ uint8_t *' (aka '__ubuf__ unsigned char *') for 1st argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ uint8_t* dst, __ubuf__ half* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:263:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int32_t *' (aka '__ubuf__ int *') for 1st argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int32_t* dst, __ubuf__ float* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:305:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int16_t *' (aka '__ubuf__ short *') for 1st argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int16_t* dst, __ubuf__ half* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:322:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ uint8_t *' (aka '__ubuf__ unsigned char *') for 1st argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ uint8_t* dst, __ubuf__ int16_t* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:328:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int8_t *' (aka '__ubuf__ signed char *') for 1st argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int8_t* dst, __ubuf__ int16_t* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:351:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ float *' for 1st argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ float* dst, __ubuf__ float* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:357:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int64_t *' (aka '__ubuf__ long *') for 1st argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int64_t* dst, __ubuf__ float* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:363:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int16_t *' (aka '__ubuf__ short *') for 1st argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int16_t* dst, __ubuf__ float* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:369:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ float *' for 1st argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ float* dst, __ubuf__ int16_t* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:375:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int64_t *' (aka '__ubuf__ long *') for 1st argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int64_t* dst, __ubuf__ int32_t* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:381:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ float *' for 1st argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ float* dst, __ubuf__ int64_t* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:387:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int32_t *' (aka '__ubuf__ int *') for 1st argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int32_t* dst, __ubuf__ int64_t* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:393:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ AscendC::int4b_t *' (aka '__ubuf__ IntegerSubType<INT4_BIT_NUM, true> *') for 1st argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int4b_t* dst, __ubuf__ half* src, const RoundMode& roundMode,\n                       ^\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_m200/kernel_operator_vec_vconv_impl.h:408:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int16_t *' (aka '__ubuf__ short *') for 1st argument\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int16_t* dst, __ubuf__ int32_t* src, const RoundMode& roundMode,\n                       ^\n1 error generated.\ngmake[5]: *** [CMakeFiles/m200_obj.dir/build.make:118: CMakeFiles/m200_obj.dir/data/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp.o] Error 1\ngmake[4]: *** [CMakeFiles/Makefile2:115: CMakeFiles/m200_obj.dir/all] Error 2\ngmake[3]: *** [Makefile:91: all] Error 2\ngmake[2]: *** [ggml/src/ggml-cann/kernels/CMakeFiles/ascendc_kernels_preprocess.dir/build.make:86: ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-stamp/ascendc_kernels_preprocess-build] Error 2\ngmake[1]: *** [CMakeFiles/Makefile2:1897: ggml/src/ggml-cann/kernels/CMakeFiles/ascendc_kernels_preprocess.dir/all] Error 2\ngmake: *** [Makefile:146: all] Error 2\n```",
    "labels": [
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2025-02-21T19:46:15+00:00",
    "closed_at": "2025-03-14T07:16:46+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12010/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12010"
  },
  {
    "number": 10777,
    "title": "Eval bug: CANN error  E89999 on Ascend 910b",
    "body": "### Name and Version\r\n\r\n./llama-cli  --version\r\nversion: 4302 (43041d2e)\r\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu\r\n\r\n### Operating systems\r\n\r\nLinux\r\n\r\n### GGML backends\r\n\r\nCANN\r\n\r\n### Hardware\r\n\r\nHuawei Ascend 910b\r\n\r\n### Models\r\n\r\nQwQ-32B-Q4_0\r\n\r\n### Problem description & steps to reproduce\r\n\r\nWhen I run the following command to start llama-cli, it crashed with CANN error CANN error: E89999: Inner Error!\r\n ```sh\r\n ./llama-cli -m /models/QwQ-32B-Q4_0.gguf -p \"Building a website can be done in 10 simple steps:\" -n 400 -e -ngl 33 -sm layer\r\n```\r\n\r\n\r\n\r\n### First Bad Commit\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n./llama-cli -m /models/QwQ-32B-Q4_0.ggup -p \"Building a website can be done in 10 simple steps:\" -n 400 -e -ngl 33 -sm layer\r\nbuild: 4302 (43041d2e) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu\r\nmain: llama backend init\r\nmain: load the model and apply lora adapter, if any\r\nllama_load_model_from_file: using device CANN0 (Ascend910B2) - 62152 MiB free\r\nllama_load_model_from_file: using device CANN1 (Ascend910B2) - 62152 MiB free\r\nllama_load_model_from_file: using device CANN2 (Ascend910B2) - 62152 MiB free\r\nllama_load_model_from_file: using device CANN3 (Ascend910B2) - 62152 MiB free\r\nllama_model_loader: loaded meta data with 24 key-value pairs and 771 tensors from /models/QwQ-32B-Q4_0.ggup (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = QwQ32B\r\nllama_model_loader: - kv   3:                         general.size_label str              = 33B\r\nllama_model_loader: - kv   4:                          qwen2.block_count u32              = 64\r\nllama_model_loader: - kv   5:                       qwen2.context_length u32              = 32768\r\nllama_model_loader: - kv   6:                     qwen2.embedding_length u32              = 5120\r\nllama_model_loader: - kv   7:                  qwen2.feed_forward_length u32              = 27648\r\nllama_model_loader: - kv   8:                 qwen2.attention.head_count u32              = 40\r\nllama_model_loader: - kv   9:              qwen2.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv  10:                       qwen2.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  11:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  12:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = qwen2\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\r\nllama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 151645\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 151643\r\nllama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 151643\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\r\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  321 tensors\r\nllama_model_loader: - type q4_0:  449 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special tokens cache size = 22\r\nllm_load_vocab: token to piece cache size = 0.9310 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = qwen2\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 152064\r\nllm_load_print_meta: n_merges         = 151387\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 5120\r\nllm_load_print_meta: n_layer          = 64\r\nllm_load_print_meta: n_head           = 40\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 5\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 27648\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 32B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 32.76 B\r\nllm_load_print_meta: model size       = 17.35 GiB (4.55 BPW) \r\nllm_load_print_meta: general.name     = QwQ32B\r\nllm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: EOS token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: EOT token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 148848 '\u00c4\u012c'\r\nllm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'\r\nllm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'\r\nllm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'\r\nllm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'\r\nllm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'\r\nllm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'\r\nllm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: EOG token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'\r\nllm_load_print_meta: EOG token        = 151663 '<|repo_name|>'\r\nllm_load_print_meta: EOG token        = 151664 '<|file_sep|>'\r\nllm_load_print_meta: max token length = 256\r\nllm_load_tensors: offloading 33 repeating layers to GPU\r\nllm_load_tensors: offloaded 33/65 layers to GPU\r\nllm_load_tensors:        CANN0 model buffer size =  2354.66 MiB\r\nllm_load_tensors:        CANN1 model buffer size =  2093.03 MiB\r\nllm_load_tensors:        CANN2 model buffer size =  2093.03 MiB\r\nllm_load_tensors:        CANN3 model buffer size =  2093.03 MiB\r\nllm_load_tensors:   CPU_Mapped model buffer size =  9137.25 MiB\r\n................................................................................................\r\nllama_new_context_with_model: n_seq_max     = 1\r\nllama_new_context_with_model: n_ctx         = 4096\r\nllama_new_context_with_model: n_ctx_per_seq = 4096\r\nllama_new_context_with_model: n_batch       = 2048\r\nllama_new_context_with_model: n_ubatch      = 512\r\nllama_new_context_with_model: flash_attn    = 0\r\nllama_new_context_with_model: freq_base     = 1000000.0\r\nllama_new_context_with_model: freq_scale    = 1\r\nllama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\r\nllama_kv_cache_init:      CANN0 KV buffer size =   144.00 MiB\r\nllama_kv_cache_init:      CANN1 KV buffer size =   128.00 MiB\r\nllama_kv_cache_init:      CANN2 KV buffer size =   128.00 MiB\r\nllama_kv_cache_init:      CANN3 KV buffer size =   128.00 MiB\r\nllama_kv_cache_init:        CPU KV buffer size =   496.00 MiB\r\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\r\nllama_new_context_with_model:      CANN0 compute buffer size =   368.02 MiB\r\nllama_new_context_with_model:      CANN1 compute buffer size =   368.00 MiB\r\nllama_new_context_with_model:      CANN2 compute buffer size =   368.00 MiB\r\nllama_new_context_with_model:      CANN3 compute buffer size =   368.00 MiB\r\nllama_new_context_with_model:  CANN_Host compute buffer size =   307.00 MiB\r\nllama_new_context_with_model: graph nodes  = 2246\r\nllama_new_context_with_model: graph splits = 441 (with bs=512), 6 (with bs=1)\r\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\n/workspace/llama.cpp/ggml/src/ggml-cann/ggml-cann.cpp:63: CANN error: E89999: Inner Error!\r\nE89999: [PID: 174412] 2024-12-11-06:51:31.011.789 op[Range], outSize from framework (OFF) is 1, but outSize from tiling (OFT) is 64,which maybe calc OFF by double, but calc OFT by floatplease use float to calc OFF while you wanner input's dtype is float[FUNC:CalculateOutputNum][FILE:range.cc][LINE:113]\r\n        TraceBack (most recent call last):\r\n       op[Range], calculate output_total_num value fail.[FUNC:AppendTilingArgs][FILE:range.cc][LINE:182]\r\n       op[Range], append tiling args fail.[FUNC:Tiling4Range][FILE:range.cc][LINE:255]\r\n       Tiling failed\r\n       Tiling Failed.\r\n       Kernel Run failed. opType: 7, Range\r\n       launch failed for Range, errno:561103.\r\n\r\n  current device: 0, in function aclnn_arange at /workspace/llama.cpp/ggml/src/ggml-cann/aclnn_ops.cpp:300\r\nCANN error\r\n  aclnnArange(workspaceAddr, workspaceSize, executor, ctx.stream())\r\nAborted (core dumped)\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-12-11T07:06:39+00:00",
    "closed_at": "2025-01-21T08:49:47+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10777/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10777"
  },
  {
    "number": 10556,
    "title": "[CANN] Compile bug: no matching function for call to 'CastIntrinsicsImpl'",
    "body": "### Git commit\n\nhttps://github.com/ggerganov/llama.cpp/commit/9f912511bc9414fa7a3c521378b6388cd932b58d\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCPU\n\n### Problem description & steps to reproduce\n\nggml CANN backend \r\n\r\nAscend NPU: 910A or 910B.   not 910b1, 910b2, 910b3\r\n\r\nAscend-hdk-910-npu-driver_23.0.0_linux-aarch64.run --quiet --docker \r\nAscend-cann-toolkit_8.0.RC2_linux-aarch64.run --force --quiet --install-for-all --full \r\nAscend-cann-kernels-910_8.0.RC2_linux.run --install --install-for-all --quiet \r\n\r\n\r\ncmake -B build -DGGML_CANN=on -DCMAKE_BUILD_TYPE=release -DSOC_TYPE=ascend910a\r\nor\r\ncmake -B build -DGGML_CANN=on -DCMAKE_BUILD_TYPE=release -DSOC_TYPE=ascend910b\r\n\r\n\r\ncmake --build build --config release --target llama-cli\r\n\r\n\r\n\r\n```log\r\n[ 41%] Performing build step for 'ascendc_kernels_device'\r\n[ 12%] Building CXX object CMakeFiles/device_obj.dir/home/ma-user/llama.cpp/build/auto_gen/ascendc_kernels/auto_gen_dup.cpp.o\r\n[ 25%] Building CXX object CMakeFiles/device_obj.dir/home/ma-user/llama.cpp/build/auto_gen/ascendc_kernels/auto_gen_get_row_f16.cpp.o\r\n[ 37%] Building CXX object CMakeFiles/device_obj.dir/home/ma-user/llama.cpp/build/auto_gen/ascendc_kernels/auto_gen_get_row_f32.cpp.o\r\n[ 50%] Building CXX object CMakeFiles/device_obj.dir/home/ma-user/llama.cpp/build/auto_gen/ascendc_kernels/auto_gen_get_row_q4_0.cpp.o\r\nIn file included from /home/ma-user/llama.cpp/build/auto_gen/ascendc_kernels/auto_gen_get_row_q4_0.cpp:7:\r\nIn file included from /home/ma-user/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp:1:\r\nIn file included from /usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h:27:\r\nIn file included from /usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h:48:\r\nIn file included from /usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h:26:\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:450:5: error: no matching function for call to 'CastIntrinsicsImpl'\r\n    CastIntrinsicsImpl(dst, src, roundMode, repeatTimes, repeatParams);\r\n    ^~~~~~~~~~~~~~~~~~\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:402:9: note: in instantiation of function template specialization 'AscendC::CastImpl<half, AscendC::IntegerSubType<4, true>, true>' requested here\r\n        CastImpl((__ubuf__ U*)(dst + dstOffset), (__ubuf__ T*)(src + srcOffset), roundMode, fullMask,\r\n        ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm:130:5: note: in instantiation of function template specialization 'AscendC::CastImpl<half, AscendC::IntegerSubType<4, true>>' requested here\r\n    CastImpl((__ubuf__ T1*)dstLocal.GetPhyAddr(), (__ubuf__ T2*)srcLocal.GetPhyAddr(), round_mode, calCount);\r\n    ^\r\n/home/ma-user/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp:116:9: note: in instantiation of function template specialization 'AscendC::Cast<half, AscendC::IntegerSubType<4, true>>' requested here\r\n        Cast(cast_local, input_local, RoundMode::CAST_NONE, QK4_0);\r\n        ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:26:24: note: candidate function not viable: no known conversion from '__ubuf__ AscendC::IntegerSubType<4, true> *' to '__ubuf__ int32_t *' (aka '__ubuf__ int *') for 2nd argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ half* dst, __ubuf__ int32_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:33:24: note: candidate function not viable: no known conversion from '__ubuf__ AscendC::IntegerSubType<4, true> *' to '__ubuf__ int8_t *' (aka '__ubuf__ signed char *') for 2nd argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ half* dst, __ubuf__ int8_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:47:24: note: candidate function not viable: no known conversion from '__ubuf__ AscendC::IntegerSubType<4, true> *' to '__ubuf__ uint8_t *' (aka '__ubuf__ unsigned char *') for 2nd argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ half* dst, __ubuf__ uint8_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:215:24: note: candidate function not viable: no known conversion from '__ubuf__ AscendC::IntegerSubType<4, true> *' to '__ubuf__ float *' for 2nd argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ half* dst, __ubuf__ float* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:323:24: note: candidate function not viable: no known conversion from '__ubuf__ AscendC::IntegerSubType<4, true> *' to '__ubuf__ int16_t *' (aka '__ubuf__ short *') for 2nd argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ half* dst, __ubuf__ int16_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:61:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ float *' for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ float* dst, __ubuf__ int32_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:75:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ float *' for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ float* dst, __ubuf__ half* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:89:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int32_t *' (aka '__ubuf__ int *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int32_t* dst, __ubuf__ half* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:131:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int8_t *' (aka '__ubuf__ signed char *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int8_t* dst, __ubuf__ half* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:173:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ uint8_t *' (aka '__ubuf__ unsigned char *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ uint8_t* dst, __ubuf__ half* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:263:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int32_t *' (aka '__ubuf__ int *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int32_t* dst, __ubuf__ float* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:305:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int16_t *' (aka '__ubuf__ short *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int16_t* dst, __ubuf__ half* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:311:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ uint8_t *' (aka '__ubuf__ unsigned char *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ uint8_t* dst, __ubuf__ int16_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:317:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int8_t *' (aka '__ubuf__ signed char *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int8_t* dst, __ubuf__ int16_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:329:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ float *' for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ float* dst, __ubuf__ float* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:335:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int64_t *' (aka '__ubuf__ long *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int64_t* dst, __ubuf__ float* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:341:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int16_t *' (aka '__ubuf__ short *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int16_t* dst, __ubuf__ float* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:347:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ float *' for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ float* dst, __ubuf__ int16_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:353:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int16_t *' (aka '__ubuf__ short *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int16_t* dst, __ubuf__ int32_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:359:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int64_t *' (aka '__ubuf__ long *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int64_t* dst, __ubuf__ int32_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:365:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ float *' for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ float* dst, __ubuf__ int64_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:371:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int32_t *' (aka '__ubuf__ int *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int32_t* dst, __ubuf__ int64_t* src, const RoundMode& roundMode,\r\n                       ^\r\n1 error generated.\r\ngmake[6]: *** [CMakeFiles/device_obj.dir/build.make:118: CMakeFiles/device_obj.dir/home/ma-user/llama.cpp/build/auto_gen/ascendc_kernels/auto_gen_get_row_q4_0.cpp.o] Error 1\r\ngmake[5]: *** [CMakeFiles/Makefile2:85: CMakeFiles/device_obj.dir/all] Error 2\r\ngmake[4]: *** [Makefile:91: all] Error 2\r\ngmake[3]: *** [ggml/src/ggml-cann/kernels/CMakeFiles/ascendc_kernels_device.dir/build.make:86: ggml/src/ggml-cann/kernels/ascendc_kernels_device-prefix/src/ascendc_kernels_device-stamp/ascendc_kernels_device-build] Error 2\r\ngmake[2]: *** [CMakeFiles/Makefile2:1867: ggml/src/ggml-cann/kernels/CMakeFiles/ascendc_kernels_device.dir/all] Error 2\r\ngmake[1]: *** [CMakeFiles/Makefile2:3323: examples/main/CMakeFiles/llama-cli.dir/rule] Error 2\r\ngmake: *** [Makefile:1232: llama-cli] Error 2\r\n```\r\n\r\n\r\n\r\n\r\nI see file : /usr/local/Ascend/ascend-toolkit/latest/compiler/tikcpp/ascendc_kernel_cmake/host_config.cmake\r\n```txt\r\nset(ascend910b_list ascend910b1 ascend910b2 ascend910b2c ascend910b3 ascend910b4 ascend910b4-1 ascend910c1 ascend910c2 ascend910c3 ascend910c4 ascend910c4-1)\r\nset(ascend910_list  ascend910a ascend910proa ascend910b ascend910prob ascend910premiuma)\r\nset(ascend310p_list ascend310p1 ascend310p3 ascend310p3vir01 ascend310p3vir02 ascend310p3vir04 ascend310p3vir08)\r\nset(ascend310b_list ascend310b1 ascend310b2 ascend310b3 ascend310b4)\r\nset(all_product ${ascend910b_list} ${ascend910_list} ${ascend310p_list})\r\n\r\nif(NOT DEFINED SOC_VERSION)\r\n    message(FATAL_ERROR \"SOC_VERSION value not set.\")\r\nendif()\r\n\r\nstring(TOLOWER \"${SOC_VERSION}\" _LOWER_SOC_VERSION)\r\n\r\nif(_LOWER_SOC_VERSION IN_LIST ascend910b_list)\r\n    set(DYNAMIC_MODE ON)\r\n    set(BUILD_MODE   aiv)\r\nelseif(_LOWER_SOC_VERSION IN_LIST ascend910_list)\r\n    set(BUILD_MODE   c100)\r\nelseif(_LOWER_SOC_VERSION IN_LIST ascend310p_list)\r\n    set(BUILD_MODE   m200)\r\nelseif(_LOWER_SOC_VERSION IN_LIST ascend310b_list)\r\n    set(BUILD_MODE   m300)\r\nelse()\r\n    message(FATAL_ERROR \"SOC_VERSION ${SOC_VERSION} does not support, the support list is ${all_product}\")\r\nendif()\r\n\r\nif(NOT DEFINED RUN_MODE)\r\n    set(RUN_MODE \"npu\")\r\nendif()\r\n\r\nif(NOT DEFINED ASCEND_KERNEL_LAUNCH_ONLY)\r\n    set(ASCEND_KERNEL_LAUNCH_ONLY OFF)\r\nendif()\r\n\r\n.....\r\n```\r\n\r\nascend910_list  contains  ascend910a and ascend910b  \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n[ 41%] Performing build step for 'ascendc_kernels_device'\r\n[ 12%] Building CXX object CMakeFiles/device_obj.dir/home/ma-user/llama.cpp/build/auto_gen/ascendc_kernels/auto_gen_dup.cpp.o\r\n[ 25%] Building CXX object CMakeFiles/device_obj.dir/home/ma-user/llama.cpp/build/auto_gen/ascendc_kernels/auto_gen_get_row_f16.cpp.o\r\n[ 37%] Building CXX object CMakeFiles/device_obj.dir/home/ma-user/llama.cpp/build/auto_gen/ascendc_kernels/auto_gen_get_row_f32.cpp.o\r\n[ 50%] Building CXX object CMakeFiles/device_obj.dir/home/ma-user/llama.cpp/build/auto_gen/ascendc_kernels/auto_gen_get_row_q4_0.cpp.o\r\nIn file included from /home/ma-user/llama.cpp/build/auto_gen/ascendc_kernels/auto_gen_get_row_q4_0.cpp:7:\r\nIn file included from /home/ma-user/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp:1:\r\nIn file included from /usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/kernel_operator.h:27:\r\nIn file included from /usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_intf.h:48:\r\nIn file included from /usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/interface/kernel_operator_vec_vconv_intf.h:26:\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:450:5: error: no matching function for call to 'CastIntrinsicsImpl'\r\n    CastIntrinsicsImpl(dst, src, roundMode, repeatTimes, repeatParams);\r\n    ^~~~~~~~~~~~~~~~~~\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:402:9: note: in instantiation of function template specialization 'AscendC::CastImpl<half, AscendC::IntegerSubType<4, true>, true>' requested here\r\n        CastImpl((__ubuf__ U*)(dst + dstOffset), (__ubuf__ T*)(src + srcOffset), roundMode, fullMask,\r\n        ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/inner_interface/inner_kernel_operator_vec_vconv_intf.cppm:130:5: note: in instantiation of function template specialization 'AscendC::CastImpl<half, AscendC::IntegerSubType<4, true>>' requested here\r\n    CastImpl((__ubuf__ T1*)dstLocal.GetPhyAddr(), (__ubuf__ T2*)srcLocal.GetPhyAddr(), round_mode, calCount);\r\n    ^\r\n/home/ma-user/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp:116:9: note: in instantiation of function template specialization 'AscendC::Cast<half, AscendC::IntegerSubType<4, true>>' requested here\r\n        Cast(cast_local, input_local, RoundMode::CAST_NONE, QK4_0);\r\n        ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:26:24: note: candidate function not viable: no known conversion from '__ubuf__ AscendC::IntegerSubType<4, true> *' to '__ubuf__ int32_t *' (aka '__ubuf__ int *') for 2nd argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ half* dst, __ubuf__ int32_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:33:24: note: candidate function not viable: no known conversion from '__ubuf__ AscendC::IntegerSubType<4, true> *' to '__ubuf__ int8_t *' (aka '__ubuf__ signed char *') for 2nd argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ half* dst, __ubuf__ int8_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:47:24: note: candidate function not viable: no known conversion from '__ubuf__ AscendC::IntegerSubType<4, true> *' to '__ubuf__ uint8_t *' (aka '__ubuf__ unsigned char *') for 2nd argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ half* dst, __ubuf__ uint8_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:215:24: note: candidate function not viable: no known conversion from '__ubuf__ AscendC::IntegerSubType<4, true> *' to '__ubuf__ float *' for 2nd argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ half* dst, __ubuf__ float* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:323:24: note: candidate function not viable: no known conversion from '__ubuf__ AscendC::IntegerSubType<4, true> *' to '__ubuf__ int16_t *' (aka '__ubuf__ short *') for 2nd argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ half* dst, __ubuf__ int16_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:61:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ float *' for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ float* dst, __ubuf__ int32_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:75:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ float *' for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ float* dst, __ubuf__ half* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:89:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int32_t *' (aka '__ubuf__ int *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int32_t* dst, __ubuf__ half* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:131:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int8_t *' (aka '__ubuf__ signed char *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int8_t* dst, __ubuf__ half* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:173:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ uint8_t *' (aka '__ubuf__ unsigned char *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ uint8_t* dst, __ubuf__ half* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:263:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int32_t *' (aka '__ubuf__ int *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int32_t* dst, __ubuf__ float* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:305:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int16_t *' (aka '__ubuf__ short *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int16_t* dst, __ubuf__ half* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:311:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ uint8_t *' (aka '__ubuf__ unsigned char *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ uint8_t* dst, __ubuf__ int16_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:317:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int8_t *' (aka '__ubuf__ signed char *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int8_t* dst, __ubuf__ int16_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:329:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ float *' for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ float* dst, __ubuf__ float* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:335:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int64_t *' (aka '__ubuf__ long *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int64_t* dst, __ubuf__ float* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:341:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int16_t *' (aka '__ubuf__ short *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int16_t* dst, __ubuf__ float* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:347:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ float *' for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ float* dst, __ubuf__ int16_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:353:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int16_t *' (aka '__ubuf__ short *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int16_t* dst, __ubuf__ int32_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:359:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int64_t *' (aka '__ubuf__ long *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int64_t* dst, __ubuf__ int32_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:365:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ float *' for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ float* dst, __ubuf__ int64_t* src, const RoundMode& roundMode,\r\n                       ^\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/tikcpp/tikcfw/impl/dav_c100/kernel_operator_vec_vconv_impl.h:371:24: note: candidate function not viable: no known conversion from '__ubuf__ half *' to '__ubuf__ int32_t *' (aka '__ubuf__ int *') for 1st argument\r\n__aicore__ inline void CastIntrinsicsImpl(__ubuf__ int32_t* dst, __ubuf__ int64_t* src, const RoundMode& roundMode,\r\n                       ^\r\n1 error generated.\r\ngmake[6]: *** [CMakeFiles/device_obj.dir/build.make:118: CMakeFiles/device_obj.dir/home/ma-user/llama.cpp/build/auto_gen/ascendc_kernels/auto_gen_get_row_q4_0.cpp.o] Error 1\r\ngmake[5]: *** [CMakeFiles/Makefile2:85: CMakeFiles/device_obj.dir/all] Error 2\r\ngmake[4]: *** [Makefile:91: all] Error 2\r\ngmake[3]: *** [ggml/src/ggml-cann/kernels/CMakeFiles/ascendc_kernels_device.dir/build.make:86: ggml/src/ggml-cann/kernels/ascendc_kernels_device-prefix/src/ascendc_kernels_device-stamp/ascendc_kernels_device-build] Error 2\r\ngmake[2]: *** [CMakeFiles/Makefile2:1867: ggml/src/ggml-cann/kernels/CMakeFiles/ascendc_kernels_device.dir/all] Error 2\r\ngmake[1]: *** [CMakeFiles/Makefile2:3323: examples/main/CMakeFiles/llama-cli.dir/rule] Error 2\r\ngmake: *** [Makefile:1232: llama-cli] Error 2\n```\n",
    "labels": [
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-11-28T03:48:05+00:00",
    "closed_at": "2024-11-29T06:46:03+00:00",
    "comments": 44,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10556/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10556"
  },
  {
    "number": 10517,
    "title": "[CANN] Compile bug:  cann backend build failed when manually specify SOC_TYPE or gcc version that isn't verified",
    "body": "### Git commit\r\n\r\nab96610b1e58684bc5e8b810130c4cf6d8252e21\r\n\r\n### Operating systems\r\n\r\nLinux\r\n\r\n### GGML backends\r\n\r\nCANN\r\n\r\n### Problem description & steps to reproduce\r\n\r\ncann backend build failed when manually specify SOC_TYPE or gcc version that isn't verified\r\n\r\n### First Bad Commit\r\n\r\nc18610b4ee29ca056bb4f2d375a4ad1b16f44ef7\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nint4b_t is not supported.\r\n```\r\n",
    "labels": [
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-11-26T13:36:14+00:00",
    "closed_at": "2024-11-28T07:26:20+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10517/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10517"
  },
  {
    "number": 10512,
    "title": "[CANN] Operator support",
    "body": "### Prerequisites\r\n\r\n- [X] I am running the latest code. Mention the version if possible as well.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\r\n\r\n### Feature Description\r\n\r\n> **This issue summarizes the current support of various operators in the CANN backend.**\r\n\r\n\r\n\r\n### Precision issue\r\n\r\n> This part is a newly added test case related to matrix transposition, which is pending fix.\r\n\r\n```\r\nMUL_MAT(type_a=q4_0,type_b=f32,m=16,n=1,k=256,bs=[2,3],nr=[1,1],per=[0,2,1,3]): [MUL_MAT] NMSE = 1.826328661 > 0.000500000 FAIL\r\nMUL_MAT(type_a=q4_0,type_b=f32,m=16,n=1,k=256,bs=[2,3],nr=[1,1],per=[0,1,3,2]): [MUL_MAT] NMSE = 1.489608079 > 0.000500000 FAIL\r\nMUL_MAT(type_a=q4_0,type_b=f32,m=16,n=1,k=256,bs=[2,3],nr=[1,1],per=[0,3,2,1]): [MUL_MAT] NMSE = 1.592494920 > 0.000500000 FAIL\r\nMUL_MAT(type_a=q4_0,type_b=f32,m=16,n=8,k=256,bs=[2,3],nr=[1,1],per=[0,2,1,3]): [MUL_MAT] NMSE = 1.841543462 > 0.000500000 FAIL\r\nMUL_MAT(type_a=q4_0,type_b=f32,m=16,n=8,k=256,bs=[2,3],nr=[1,1],per=[0,1,3,2]): [MUL_MAT] NMSE = 1.453923314 > 0.000500000 FAIL\r\nMUL_MAT(type_a=q4_0,type_b=f32,m=16,n=8,k=256,bs=[2,3],nr=[1,1],per=[0,3,2,1]): [MUL_MAT] NMSE = 1.865098691 > 0.000500000 FAIL\r\nMUL_MAT(type_a=q4_0,type_b=f32,m=16,n=16,k=256,bs=[2,3],nr=[1,1],per=[0,2,1,3]): [MUL_MAT] NMSE = 1.731590413 > 0.000500000 FAIL\r\nMUL_MAT(type_a=q4_0,type_b=f32,m=16,n=16,k=256,bs=[2,3],nr=[1,1],per=[0,1,3,2]): [MUL_MAT] NMSE = 1.284411011 > 0.000500000 FAIL\r\nMUL_MAT(type_a=q4_0,type_b=f32,m=16,n=16,k=256,bs=[2,3],nr=[1,1],per=[0,3,2,1]): [MUL_MAT] NMSE = 2.044444701 > 0.000500000 FAIL\r\n\r\n  1845/1854 tests passed\r\n  Backend CANN0: FAIL\r\n```\r\n\r\n### Operator support\r\n\r\n#### Overview\r\n\r\n| Operator                   | Operator support |\r\n| -------------------------- | ---------------- |\r\n| GGML_OP_UNARY              | Partial support  |\r\n| GGML_OP_MUL_MAT            | Partial support  |\r\n| GGML_OP_MUL_MAT_ID         | Not Support      |\r\n| GGML_OP_GET_ROWS           | Partial support  |\r\n| GGML_OP_CPY                | Partial support  |\r\n| GGML_OP_CONT               | Partial support  |\r\n| GGML_OP_ROPE               | Partial support  |\r\n| GGML_OP_UPSCALE            | Partial support  |\r\n| GGML_OP_IM2COL             | Support          |\r\n| GGML_OP_CONCAT             | Support          |\r\n| GGML_OP_DUP                | Support          |\r\n| GGML_OP_REPEAT             | Support          |\r\n| GGML_OP_NONE               | Support          |\r\n| GGML_OP_RESHAPE            | Support          |\r\n| GGML_OP_VIEW               | Support          |\r\n| GGML_OP_PERMUTE            | Support          |\r\n| GGML_OP_TRANSPOSE          | Support          |\r\n| GGML_OP_NORM               | Support          |\r\n| GGML_OP_ADD                | Support          |\r\n| GGML_OP_MUL                | Support          |\r\n| GGML_OP_DIV                | Support          |\r\n| GGML_OP_RMS_NORM           | Support          |\r\n| GGML_OP_SCALE              | Support          |\r\n| GGML_OP_SQR                | Support          |\r\n| GGML_OP_CLAMP              | Support          |\r\n| GGML_OP_DIAG_MASK_INF      | Support          |\r\n| GGML_OP_SOFT_MAX           | Support          |\r\n| GGML_OP_POOL_2D            | Support          |\r\n| GGML_OP_SUM_ROWS           | Support          |\r\n| GGML_OP_ARGSORT            | Support          |\r\n| GGML_OP_ACC                | Support          |\r\n| GGML_OP_GROUP_NORM         | Support          |\r\n| GGML_OP_PAD                | Support          |\r\n| GGML_OP_ARANGE             | Support          |\r\n| GGML_OP_TIMESTEP_EMBEDDING | Support          |\r\n| GGML_OP_LEAKY_RELU         | Support          |\r\n| GGML_OP_FLASH_ATTN_EXT         | Not Support      |\r\n| GGML_OP_CROSS_ENTROPY_LOSS                     | Not Support      |\r\n| GGML_OP_RWKV_WKV6                     | Not Support      |\r\n| GGML_OP_CROSS_ENTROPY_LOSS_BACK                     | Not Support      |\r\n| GGML_OP_OPT_STEP_ADAMW                     | Not Support      |\r\n| Others                     | Not Support      |\r\n\r\n#### GGML_OP_UNARY\r\n\r\nSupport List:\r\n1. GGML_UNARY_OP_GELU\r\n2. GGML_UNARY_OP_SILU\r\n3. GGML_UNARY_OP_RELU\r\n4. GGML_UNARY_OP_HARDSIGMOID\r\n5. GGML_UNARY_OP_HARDSWISH\r\n6. GGML_UNARY_OP_GELU_QUICK\r\n7. GGML_UNARY_OP_TANH\r\n\r\n#### GGML_OP_MUL_MAT\r\n\r\n`op->src[0]->type` Support List:\r\n1. GGML_TYPE_F16\r\n2. GGML_TYPE_F32\r\n3. GGML_TYPE_Q4_0\r\n4. GGML_TYPE_Q8_0(*Current groupsize should not be greater than k-1 in aclnnWeightQuantBatchMatmulV2GetWorkspaceSize*)\r\n\r\n#### GGML_OP_GET_ROWS\r\n\r\n`op->src[0]->type` Support List:\r\n1. GGML_TYPE_F32\r\n2. GGML_TYPE_F16\r\n3. GGML_TYPE_Q4_0\r\n4. GGML_TYPE_Q8_0\r\n\r\n#### GGML_OP_CPY\r\n\r\n`op->type` Support List:\r\n1. GGML_TYPE_F32\r\n2. GGML_TYPE_F16\r\n3. GGML_TYPE_Q4_0\r\n4. GGML_TYPE_Q8_0\r\n\r\n#### GGML_OP_CONT\r\n\r\n`op->src[0]->type` Support List:\r\n1. GGML_TYPE_F32\r\n2. GGML_TYPE_F16\r\n\r\n#### GGML_OP_ROPE\r\n\r\n1. Not support with freq_factors (Now Spuuort)\r\n2. Just support `n_dims = op->src[0]->ne[0]`\r\n3. Just support `ext_factor = 0` \r\n4. Just support `freq_scale = 1` (Now Spuuort)\r\n5. Just support `attn_factor = 1` (Now Spuuort)\r\n6. Just support data type GGML_TYPE_F32 (Now Spuuort)\r\n7. Not support test-backend-ops with parameter `v = 1`\r\n\r\n#### GGML_OP_UPSCALE\r\n\r\n1. Not support `src[0]->ne[2] / op->ne[2] != src[0]->ne[3] / op->ne[3]`(*aclnnUpsampleNearest2dGetWorkspaceSize not support selfDimN/outDimN or selfDimC/outDimC not equal*)\r\n\r\n### Motivation\r\n\r\n>**This issue summarizes the current support of various operators in the CANN backend.**\r\n\r\n\r\n### Possible Implementation\r\n\r\n_No response_",
    "labels": [
      "enhancement",
      "stale",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-11-26T09:35:07+00:00",
    "closed_at": "2025-01-13T01:07:30+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10512/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10512"
  },
  {
    "number": 10451,
    "title": "Bug: \u3010CANN\u3011ggml-cann/aclnn_ops.cpp:3007: GGML_ASSERT(n_dims == src0->ne[0]) failed",
    "body": "### What happened?\n\n\u6309\u7167readme\u4e2d\u7684\u56fa\u4ef6\u9a71\u52a8\u7248\u672c\uff0c\u63a8\u7406\u65f6\u51fa\u73b0\u62a5\u9519\n\n### Name and Version\n\n\u6700\u65b0\u7248\u672c\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n```shell\nllama_new_context_with_model: freq_scale    = 1\r\nllama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\r\nllama_kv_cache_init:      CANN0 KV buffer size =   132.00 MiB\r\nllama_kv_cache_init:        CPU KV buffer size =    28.00 MiB\r\nllama_new_context_with_model: KV self size  =  160.00 MiB, K (f16):   80.00 MiB, V (f16):   80.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\r\nllama_new_context_with_model:      CANN0 compute buffer size =  1488.00 MiB\r\nllama_new_context_with_model:  CANN_Host compute buffer size =    16.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1606\r\nllama_new_context_with_model: graph splits = 67 (with bs=512), 3 (with bs=1)\r\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\n/docker_files/zyp/202411/llama.cpp-master/ggml/src/ggml-cann/aclnn_ops.cpp:3007: GGML_ASSERT(n_dims == src0->ne[0]) failed\r\nAborted (core dumped)\n```\n",
    "labels": [
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-11-22T03:57:43+00:00",
    "closed_at": "2024-11-29T01:05:00+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10451/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10451"
  },
  {
    "number": 10161,
    "title": "Bug: CANN  E89999",
    "body": "### What happened?\n\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\n/owner/ninth/llama.cpp/ggml/src/ggml-cann.cpp:61: CANN error: E89999: Inner Error!\r\nE89999: [PID: 2277481] 2024-11-04-17:38:30.068.533 op[Range], outSize from framework (OFF) is 1, but outSize from tiling (OFT) is 64,which maybe calc OFF by double, but calc OFT by floatplease use float to calc OFF while you wanner input's dtype is float[FUNC:CalculateOutputNum][FILE:range.cc][LINE:113]\r\n        TraceBack (most recent call last):\r\n       op[Range], calculate output_total_num value fail.[FUNC:AppendTilingArgs][FILE:range.cc][LINE:182]\r\n       op[Range], append tiling args fail.[FUNC:Tiling4Range][FILE:range.cc][LINE:255]\r\n       Tiling failed\r\n       Tiling Failed.\r\n       Kernel Run failed. opType: 7, Range\r\n       launch failed for Range, errno:561103.\r\n\r\n  current device: 0, in function aclnn_arange at /owner/ninth/llama.cpp/ggml/src/ggml-cann/aclnn_ops.cpp:291\r\n  aclnnArange(workspaceAddr, workspaceSize, executor, ctx.stream())\r\nCANN error\r\n\n\n### Name and Version\n\n./build/bin/llama-cli --version\r\nversion: 3938 (6f55bccb)\r\nbuilt with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for aarch64-linux-gnu\r\n\r\n\r\ncann :  8.0.0.alpha001\r\n\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n```shell\ncann : 8.0.0.alpha001\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-11-04T09:49:12+00:00",
    "closed_at": "2025-01-27T01:07:16+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10161/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10161"
  },
  {
    "number": 10160,
    "title": "Feature Request: [CANN] backend supports Ascend 310P",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nCANN backend supports Ascend 310P inference accelerator card. Currently, llama.cpp already supports Ascend 910B. However, some APIs of Ascend 910B are different from those of 310P, so they need to be adapted in CANN backend implementation.\n\n### Motivation\n\nCompare to Ascend 910, Ascend 310 focuses on power-efficient inference on edge devices, The basic information as following:\r\n**Inference-Oriented**: The 310P is optimized for\u00a0**inference tasks**, focusing more on efficient and low-power operations, rather than the computational intensity required for training.\r\n    - **Lower Throughput**: While it supports similar operators for inference tasks (e.g., convolution, activation functions, pooling, etc.), it is not as heavily optimized for\u00a0**large-scale parallelism**\u00a0and training tasks. The 310P focuses on executing pre-trained models with lower computational demands.\n\n### Possible Implementation\n\nThe CANN backend adapts to 310P and maintains compatibility with 910B.",
    "labels": [
      "enhancement",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-11-04T09:35:09+00:00",
    "closed_at": "2024-11-22T06:07:41+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10160/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10160"
  },
  {
    "number": 10108,
    "title": "Bug: ascend 310p cann fatal error",
    "body": "### What happened?\n\nascend 310p cannot run model\n\n### Name and Version\n\nversion: b2989\r\nos:openeuler22.03 aarch64\r\nbackend:cann\r\nchip:ascend 310p\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\napp/ggml/src/ggml-cann.cpp:63: CANN error\r\nCANN error: EZ9999: Inner Error!\r\nEZ9999: 2024-10-31-12:08:42.276.420  The error from device(0), serial number is 3, there is an aivec error, core id is 0, error code = 0x10, dump info: pc start: 0x8001240801311a4, current: 0x124080131744, vec error info: 0x1ebe7caf, mte error info: 0, ifu error info: 0xfdf646ef6900, ccu error info: 0x6b8e3406005b7aca, cube error info: 0, biu error info: 0, aic error mask: 0x6de01200c0122c8, para base: 0x124000262840, errorStr: Illegal instruction, which is usually caused by unaligned UUB addresses.[FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:537]\r\n        TraceBack (most recent call last):\r\n        The error from device(0), serial number is 3, there is an aivec error, core id is 1, error code = 0x10, dump info: pc start: 0x8001240801311a4, current: 0x124080131744, vec error info: 0x3f75f47, mte error info: 0, ifu error info: 0x3e6ffb9ff9680, ccu error info: 0x6b8e3406005dcfca, cube error info: 0, biu error info: 0, aic error mask: 0x6de01200c0122c8, para base: 0x124000262840, errorStr: Illegal instruction, which is usually caused by unaligned UUB addresses.[FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:537]\r\n        The error from device(0), serial number is 3, there is an aivec error, core id is 2, error code = 0x10, dump info: pc start: 0x8001240801311a4, current: 0x124080131744, vec error info: 0xd6bfdce, mte error info: 0, ifu error info: 0x263bf59df2b80, ccu error info: 0x6b8e34060046edca, cube error info: 0, biu error info: 0, aic error mask: 0x6de01200c0122c8, para base: 0x124000262840, errorStr: Illegal instruction, which is usually caused by unaligned UUB addresses.[FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:537]\r\n        The error from device(0), serial number is 3, there is an aivec error, core id is 3, error code = 0x10, dump info: pc start: 0x8001240801311a4, current: 0x124080131744, vec error info: 0xfdb04cf, mte error info: 0, ifu error info: 0x3bdbf91e6c580, ccu error info: 0x6b8e3406006cffca, cube error info: 0, biu error info: 0, aic error mask: 0x6de01200c0122c8, para base: 0x124000262840, errorStr: Illegal instruction, which is usually caused by unaligned UUB addresses.[FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:537]\r\n        The device(0), core list[0-3], error code is:[FUNC:PrintCoreInfoErrMsg][FILE:device_error_proc.cc][LINE:591]\r\n        coreId( 0):            0x10                0x10                0x10                0x10    [FUNC:PrintCoreInfoErrMsg][FILE:device_error_proc.cc][LINE:605]\r\n        Kernel task happen error, retCode=0x31, [vector core exception].[FUNC:PreCheckTaskErr][FILE:task_info.cc][LINE:1776]\r\n        The error from device(0), serial number is 4, there is an aivec error, core id is 0, error code = 0x10, dump info: pc start: 0x800124080130678, current: 0x124080130c00, vec error info: 0x1ebe7caf, mte error info: 0, ifu error info: 0xfdf646ef6900, ccu error info: 0x6b8e140e005b7afb, cube error info: 0, biu error info: 0, aic error mask: 0x6de01200c0122c8, para base: 0x124000268940, errorStr: Illegal instruction, which is usually caused by unaligned UUB addresses.[FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:537]\r\n        The error from device(0), serial number is 4, there is an aivec error, core id is 1, error code = 0x10, dump info: pc start: 0x800124080130678, current: 0x124080130c00, vec error info: 0x3f75f47, mte error info: 0, ifu error info: 0x3e6ffb9ff9680, ccu error info: 0x6b8e140e005dcffb, cube error info: 0, biu error info: 0, aic error mask: 0x6de01200c0122c8, para base: 0x124000268940, errorStr: Illegal instruction, which is usually caused by unaligned UUB addresses.[FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:537]\r\n        The error from device(0), serial number is 4, there is an aivec error, core id is 2, error code = 0x10, dump info: pc start: 0x800124080130678, current: 0x124080130c00, vec error info: 0xd6bfdce, mte error info: 0, ifu error info: 0x263bf59df2b80, ccu error info: 0x6b8e140e0046edfb, cube error info: 0, biu error info: 0, aic error mask: 0x6de01200c0122c8, para base: 0x124000268940, errorStr: Illegal instruction, which is usually caused by unaligned UUB addresses.[FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:537]\r\n        The error from device(0), serial number is 4, there is an aivec error, core id is 3, error code = 0x10, dump info: pc start: 0x800124080130678, current: 0x124080130c00, vec error info: 0xfdb04cf, mte error info: 0, ifu error info: 0x3bdbf91e6c580, ccu error info: 0x6b8e140e006cfffb, cube error info: 0, biu error info: 0, aic error mask: 0x6de01200c0122c8, para base: 0x124000268940, errorStr: Illegal instruction, which is usually caused by unaligned UUB addresses.[FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:537]\r\n        The error from device(0), serial number is 4, there is an aivec error, core id is 4, error code = 0x10, dump info: pc start: 0x800124080130678, current: 0x124080130c08, vec error info: 0x17ff4759, mte error info: 0, ifu error info: 0x166aa79ee5180, ccu error info: 0x6b8e140e006d7ffb, cube error info: 0, biu error info: 0, aic error mask: 0x6de01200c0122c8, para base: 0x124000268940, errorStr: Illegal instruction, which is usually caused by unaligned UUB addresses.[FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:537]\r\n        The error from device(0), serial number is 4, there is an aivec error, core id is 5, error code = 0x10, dump info: pc start: 0x800124080130678, current: 0x124080130c00, vec error info: 0xddfef43, mte error info: 0, ifu error info: 0x2dffab8cd4380, ccu error info: 0x6b8e140e006f4ffb, cube error info: 0, biu error info: 0, aic error mask: 0x6de01200c0122c8, para base: 0x124000268940, errorStr: Illegal instruction, which is usually caused by unaligned UUB addresses.[FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:537]\r\n        The error from device(0), serial number is 4, there is an aivec error, core id is 6, error code = 0x10, dump info: pc start: 0x800124080130678, current: 0x124080130c08, vec error info: 0x11d7ef7a, mte error info: 0, ifu error info: 0x2f93f3b9a5780, ccu error info: 0x6b8e140e007693fb, cube error info: 0, biu error info: 0, aic error mask: 0x6de01200c0122c8, para base: 0x124000268940, errorStr: Illegal instruction, which is usually caused by unaligned UUB addresses.[FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:537]\r\n        The device(0), core list[0-6], error code is:[FUNC:PrintCoreInfoErrMsg][FILE:device_error_proc.cc][LINE:591]\r\n        coreId( 0):            0x10                0x10                0x10                0x10    [FUNC:PrintCoreInfoErrMsg][FILE:device_error_proc.cc][LINE:591]\r\n        coreId( 4):            0x10                0x10                0x10    [FUNC:PrintCoreInfoErrMsg][FILE:device_error_proc.cc][LINE:605]\r\n        Aicore kernel execute failed, device_id=0, stream_id=2, report_stream_id=2, task_id=91, flip_num=0, fault kernel_name=ascendc_dup_by_rows_fp32_to_fp16_3, fault kernel info ext=none, program id=0, hash=5997649711123561135.[FUNC:GetError][FILE:stream.cc][LINE:1512]\r\n        [AIC_INFO] after execute:args print end[FUNC:GetError][FILE:stream.cc][LINE:1512]\r\n        Aicore kernel execute failed, device_id=0, stream_id=2, report_stream_id=2, task_id=123, flip_num=0, fault kernel_name=ascendc_dup_by_rows_fp32_2, fault kernel info ext=none, program id=0, hash=5997649711123561135.[FUNC:GetError][FILE:stream.cc][LINE:1512]\r\n        rtStreamSynchronize execute failed, reason=[vector core exception][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53]\r\n        synchronize stream failed, runtime result = 507035[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]\r\n\r\n\r\nDEVICE[0] PID[57]: \r\nEXCEPTION STREAM:\r\n  Exception info:TGID=91281, model id=65535, stream id=2, stream phase=3\r\n  Message info[0]:RTS_HWTS: Vector core exception, slot_id=27, stream_id=2\r\n    Other info[0]:time=2024-10-31-20:08:42.094.650, function=process_hwts_error_exception, line=975, error code=0x31 \r\nEXCEPTION STREAM:\r\n  Exception info:TGID=91281, model id=65535, stream id=2, stream phase=3\r\n  Message info[0]:RTS_HWTS: Vector core exception, slot_id=28, stream_id=2\r\n    Other info[0]:time=2024-10-31-20:08:42.239.847, function=process_hwts_error_exception, line=975, error code=0x31\r\n  current device: 0, in function ggml_backend_cann_synchronize at /app/ggml/src/ggml-cann.cpp:1636\r\n  aclrtSynchronizeStream(cann_ctx->stream())\r\nAborted (core dumped)\n```\n",
    "labels": [
      "stale",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-10-31T12:16:06+00:00",
    "closed_at": "2025-01-13T01:07:34+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10108/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10108"
  },
  {
    "number": 10105,
    "title": "Bug: b3990 ascend cann build error",
    "body": "### What happened?\n\n27.81 /app/ggml/src/ggml-cann.cpp: In function 'ggml_backend_buffer* ggml_backend_cann_host_buffer_type_alloc_buffer(ggml_backend_buffer_type_t, size_t)':\r\n27.81 /app/ggml/src/ggml-cann.cpp:1230:19: error: 'struct ggml_backend_buffer_i' has no member named 'get_name'; did you mean 'get_base'?\r\n27.81  1230 |     buffer->iface.get_name = ggml_backend_cann_host_buffer_name;\r\n27.81       |                   ^~~~~~~~\r\n27.81       |                   get_base\r\n27.85 gmake[3]: *** [ggml/src/CMakeFiles/ggml.dir/build.make:174: ggml/src/CMakeFiles/ggml.dir/ggml-cann.cpp.o] Error 1\r\n27.85 gmake[2]: *** [CMakeFiles/Makefile2:1619: ggml/src/CMakeFiles/ggml.dir/all] Error 2\r\n27.85 gmake[1]: *** [CMakeFiles/Makefile2:3368: examples/main/CMakeFiles/llama-cli.dir/rule] Error 2\r\n27.85 gmake: *** [Makefile:1323: llama-cli] Error 2\r\n------\r\n\n\n### Name and Version\n\nversion: b3990\r\nos: openeuleros:22.03\r\nframework: ascend-cann\n\n### What operating system are you seeing the problem on?\n\nMac\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "low severity",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-10-31T02:04:51+00:00",
    "closed_at": "2024-11-04T11:08:40+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10105/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10105"
  },
  {
    "number": 9979,
    "title": "Bug: [CANN] inference running result is garbled in debug running model for LM models who's type is Q4_0 class",
    "body": "### What happened?\n\nFor CANN backend: inference running result is garbled in debug running model for LM models who's type is Q4_0 class\n\n### Name and Version\n\nb3948\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "medium severity",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-10-21T11:35:28+00:00",
    "closed_at": "2024-10-22T08:16:03+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9979/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9979"
  },
  {
    "number": 9862,
    "title": "Feature Request: [CANN] backend adapts to llama.cpp dynamic backend loading mechanism",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nDynamically loadable backends framework has been added in PR([#9707](https://github.com/ggerganov/llama.cpp/pull/9707)). CANN backend needs to adapt to this mechanism.\n\n### Motivation\n\nllama.cpp will be refactored to use only the backend registry API, as explained by slaren in PR ([#9707](https://github.com/ggerganov/llama.cpp/pull/9707)). Currently, CUDA and CPU backends has implemented these interfaces.\n\n### Possible Implementation\n\nCANN already implement the functions in these interfaces, so this should only require shuffling some code around.",
    "labels": [
      "enhancement",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-10-12T09:25:05+00:00",
    "closed_at": "2024-10-22T08:16:32+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9862/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9862"
  },
  {
    "number": 9844,
    "title": "Bug: [CANN] compile failure ",
    "body": "### What happened?\n\n# Version\r\nlastest b3906\r\n\r\n# System Info\r\nDevice: Ascend 910B4\r\nOS: EulerOS 2.10  \r\nArch: aarch64\r\n\r\n# What happened\r\nfollow the [CANN.md](https://github.com/ggerganov/llama.cpp/blob/master/docs/backend/CANN.md) try to build llama-cli \r\nfacing compile failure\r\n\r\nlogs\r\n```\r\n/app/ggml/src/ggml-common.h:261:16: warning: ISO C++ prohibits anonymous structs [-Wpedantic]\r\n  261 |         struct {\r\n      |                ^\r\n/app/ggml/src/ggml-common.h:288:16: warning: ISO C++ prohibits anonymous structs [-Wpedantic]\r\n  288 |         struct {\r\n      |                ^\r\n/app/ggml/src/ggml-common.h:305:16: warning: ISO C++ prohibits anonymous structs [-Wpedantic]\r\n  305 |         struct {\r\n      |                ^\r\n/app/ggml/src/ggml-cann.cpp: In function 'ggml_backend_buffer_type* ggml_backend_cann_buffer_type(int32_t)':\r\n/app/ggml/src/ggml-cann.cpp:1154:13: error: no match for 'operator=' (operand types are 'ggml_backend_buffer_type' and '<brace-enclosed initializer list>')\r\n 1154 |             };\r\n      |             ^\r\nIn file included from /app/ggml/src/ggml-cann.cpp:34:\r\n/app/ggml/src/ggml-backend-impl.h:29:12: note: candidate: 'ggml_backend_buffer_type& ggml_backend_buffer_type::operator=(const ggml_backend_buffer_type&)'\r\n   29 |     struct ggml_backend_buffer_type {\r\n      |            ^~~~~~~~~~~~~~~~~~~~~~~~\r\n/app/ggml/src/ggml-backend-impl.h:29:12: note:   no known conversion for argument 1 from '<brace-enclosed initializer list>' to 'const ggml_backend_buffer_type&'\r\n/app/ggml/src/ggml-backend-impl.h:29:12: note: candidate: 'ggml_backend_buffer_type& ggml_backend_buffer_type::operator=(ggml_backend_buffer_type&&)'\r\n/app/ggml/src/ggml-backend-impl.h:29:12: note:   no known conversion for argument 1 from '<brace-enclosed initializer list>' to 'ggml_backend_buffer_type&&'\r\n/app/ggml/src/ggml-cann.cpp: In function 'ggml_backend_event* ggml_backend_cann_event_new(ggml_backend_t)':\r\n/app/ggml/src/ggml-cann.cpp:1873:5: error: could not convert '{backend, event}' from '<brace-enclosed initializer list>' to 'ggml_backend_event'\r\n 1873 |     };\r\n      |     ^\r\n      |     |\r\n      |     <brace-enclosed initializer list>\r\n/app/ggml/src/ggml-cann.cpp: In function 'void ggml_backend_cann_event_record(ggml_backend_event_t)':\r\n/app/ggml/src/ggml-cann.cpp:1900:44: error: 'struct ggml_backend_event' has no member named 'backend'\r\n 1900 |         (ggml_backend_cann_context*)event->backend->context;\r\n      |                                            ^~~~~~~\r\n/app/ggml/src/ggml-cann.cpp: In function 'void ggml_backend_cann_event_wait(ggml_backend_t, ggml_backend_event_t)':\r\n/app/ggml/src/ggml-cann.cpp:1920:37: error: 'struct ggml_backend_event' has no member named 'backend'\r\n 1920 |     if (ggml_backend_is_cann(event->backend)) {\r\n      |                                     ^~~~~~~\r\n/app/ggml/src/ggml-cann.cpp: At global scope:\r\n/app/ggml/src/ggml-cann.cpp:1962:38: error: invalid conversion from 'void (*)(ggml_backend_event_t)' {aka 'void (*)(ggml_backend_event*)'} to 'void (*)(ggml_backend_t, ggml_backend_event_t)' {aka 'void (*)(ggml_backend*, ggml_backend_event*)'} [-fpermissive]\r\n 1962 |     /* .event_record            = */ ggml_backend_cann_event_record,\r\n      |                                      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                      |\r\n      |                                      void (*)(ggml_backend_event_t) {aka void (*)(ggml_backend_event*)}\r\ngmake[3]: *** [ggml/src/CMakeFiles/ggml.dir/build.make:174: ggml/src/CMakeFiles/ggml.dir/ggml-cann.cpp.o] Error 1\r\ngmake[2]: *** [CMakeFiles/Makefile2:1619: ggml/src/CMakeFiles/ggml.dir/all] Error 2\r\ngmake[1]: *** [CMakeFiles/Makefile2:3368: examples/main/CMakeFiles/llama-cli.dir/rule] Error 2\r\ngmake: *** [Makefile:1323: llama-cli] Error 2\r\nThe command '/bin/sh -c echo \"Building with static libs\" &&     source /usr/local/Ascend/ascend-toolkit/set_env.sh --force &&     cmake -B build -DGGML_CANN=ON -DBUILD_SHARED_LIBS=OFF  &&     cmake --build build --config Release --target llama-cli' returned a non-zero code: 2\r\n```\r\n\r\n\n\n### Name and Version\n\n3906\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "medium severity",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-10-11T10:27:37+00:00",
    "closed_at": "2024-10-16T00:52:51+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9844/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9844"
  },
  {
    "number": 9560,
    "title": "[CANN]Bug: Can't compile ggml/src/CMakeFiles/ggml.dir/ggml-cann/acl_tensor.cpp.o",
    "body": "### What happened?\n\nAfter using 'cmake --build build --config release' command on Ascend 310P3,it can not compile succesfully\r\n![image](https://github.com/user-attachments/assets/74ef8d67-e859-4502-ac0e-295513280fe3)\r\n\n\n### Name and Version\n\n# NPU\r\n![image](https://github.com/user-attachments/assets/7e3c7184-7c72-462c-a5a5-fdfca2e57fbf)\r\n# tookit\r\nAscend-cann-toolkit_8.0.RC2_linux-aarch64.run\r\n# kernels\r\nAscend-cann-kernels-310p_8.0.RC2_linux.run\r\n# gcc\r\n8.5.0\r\n# platform\r\nEuler OS 2.0\r\n# llama.cpp\r\nbranch master\r\ncommitID 0d2f22e45c3c3b6f8222acb6284d0c8c93443ba1\n\n### What operating system are you seeing the problem on?\n\nLinux, Other? (Please let us know in description)\n\n### Relevant log output\n\n```shell\n(PyTorch-2.1.0) [ma-user llama.cpp-master]$cmake -B build -DGGML_CANN=on -DCMAKE_BUILD_TYPE=release\r\n-- The CXX compiler identification is GNU 8.5.0\r\n-- Check for working CXX compiler: /usr/bin/g++\r\n-- Check for working CXX compiler: /usr/bin/g++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Found Git: /usr/bin/git (found version \"2.27.0\") \r\nfatal: not a git repository (or any parent up to mount point /home/ma-user)\r\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\nfatal: not a git repository (or any parent up to mount point /home/ma-user)\r\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - found\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\r\n-- Check if compiler accepts -pthread\r\n-- Check if compiler accepts -pthread - yes\r\n-- Found Threads: TRUE  \r\n-- Found OpenMP_C: -fopenmp (found version \"4.5\") \r\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \r\n-- Found OpenMP: TRUE (found version \"4.5\")  \r\n-- OpenMP found\r\n-- Using llamafile\r\n-- CANN: updated CANN_INSTALL_DIR from ASCEND_TOOLKIT_HOME=/usr/local/Ascend/ascend-toolkit/latest\r\n-- CANN: CANN_INCLUDE_DIRS =  /usr/local/Ascend/ascend-toolkit/latest/include;/usr/local/Ascend/ascend-toolkit/latest/include/aclnn;/usr/local/Ascend/ascend-toolkit/latest/acllib/include\r\n-- CANN: CANN_LIBRARIES =  ascendcl;nnopbase;opapi;acl_op_compiler;ascendc_kernels\r\n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\r\n-- CMAKE_SYSTEM_PROCESSOR: aarch64\r\n-- ARM detected\r\n-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E\r\n-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed\r\nCMake Warning at common/CMakeLists.txt:30 (message):\r\n  Git repository not found; to enable automatic generation of build info,\r\n  make sure Git is installed and the project is a Git repository.\r\n\r\n\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/ma-user/work/llama.cpp-master/build\r\n(PyTorch-2.1.0) [ma-user llama.cpp-master]$cmake --build build --config release\r\ngmake[1]: Entering directory '/home/ma-user/work/llama.cpp-master/build'\r\ngmake[2]: Entering directory '/home/ma-user/work/llama.cpp-master/build'\r\nScanning dependencies of target ascendc_kernels_preprocess\r\ngmake[2]: Leaving directory '/home/ma-user/work/llama.cpp-master/build'\r\ngmake[2]: Entering directory '/home/ma-user/work/llama.cpp-master/build'\r\n[  1%] Creating directories for 'ascendc_kernels_preprocess'\r\n[  1%] No download step for 'ascendc_kernels_preprocess'\r\n[  2%] No patch step for 'ascendc_kernels_preprocess'\r\n[  2%] No update step for 'ascendc_kernels_preprocess'\r\n[  2%] Performing configure step for 'ascendc_kernels_preprocess'\r\n-- The C compiler identification is GNU 8.5.0\r\n-- The CXX compiler identification is GNU 8.5.0\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build\r\n[  2%] Performing build step for 'ascendc_kernels_preprocess'\r\ngmake[3]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\ngmake[4]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\ngmake[5]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\nScanning dependencies of target aic_obj\r\ngmake[5]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\ngmake[5]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\n[  4%] Building CXX object CMakeFiles/aic_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/dup.cpp.o\r\n[  8%] Building CXX object CMakeFiles/aic_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_f16.cpp.o\r\n[ 12%] Building CXX object CMakeFiles/aic_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_f32.cpp.o\r\n[ 16%] Building CXX object CMakeFiles/aic_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp.o\r\n[ 20%] Building CXX object CMakeFiles/aic_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_q8_0.cpp.o\r\n[ 25%] Building CXX object CMakeFiles/aic_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/quantize_f16_q8_0.cpp.o\r\n[ 29%] Building CXX object CMakeFiles/aic_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/quantize_f32_q8_0.cpp.o\r\n[ 33%] Building CXX object CMakeFiles/aic_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/quantize_float_to_q4_0.cpp.o\r\ngmake[5]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\n[ 33%] Built target aic_obj\r\ngmake[5]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\nScanning dependencies of target preprocess_obj\r\ngmake[5]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\ngmake[5]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\n[ 37%] Building CXX object CMakeFiles/preprocess_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/dup.cpp.o\r\n[ 41%] Building CXX object CMakeFiles/preprocess_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_f16.cpp.o\r\n[ 45%] Building CXX object CMakeFiles/preprocess_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_f32.cpp.o\r\n[ 50%] Building CXX object CMakeFiles/preprocess_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp.o\r\n[ 54%] Building CXX object CMakeFiles/preprocess_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_q8_0.cpp.o\r\n[ 58%] Building CXX object CMakeFiles/preprocess_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/quantize_f16_q8_0.cpp.o\r\n[ 62%] Building CXX object CMakeFiles/preprocess_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/quantize_f32_q8_0.cpp.o\r\n[ 66%] Building CXX object CMakeFiles/preprocess_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/quantize_float_to_q4_0.cpp.o\r\ngmake[5]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\n[ 66%] Built target preprocess_obj\r\ngmake[5]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\nScanning dependencies of target aiv_obj\r\ngmake[5]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\ngmake[5]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\n[ 70%] Building CXX object CMakeFiles/aiv_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/dup.cpp.o\r\n[ 75%] Building CXX object CMakeFiles/aiv_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_f16.cpp.o\r\n[ 79%] Building CXX object CMakeFiles/aiv_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_f32.cpp.o\r\n[ 83%] Building CXX object CMakeFiles/aiv_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp.o\r\n[ 87%] Building CXX object CMakeFiles/aiv_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_q8_0.cpp.o\r\n[ 91%] Building CXX object CMakeFiles/aiv_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/quantize_f16_q8_0.cpp.o\r\n[ 95%] Building CXX object CMakeFiles/aiv_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/quantize_f32_q8_0.cpp.o\r\n[100%] Building CXX object CMakeFiles/aiv_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/quantize_float_to_q4_0.cpp.o\r\ngmake[5]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\n[100%] Built target aiv_obj\r\ngmake[5]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\nScanning dependencies of target _host_cpp\r\ngmake[5]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\ngmake[5]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\ngmake[5]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\n[100%] Built target _host_cpp\r\ngmake[4]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\ngmake[3]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_preprocess-prefix/src/ascendc_kernels_preprocess-build'\r\n[  2%] No install step for 'ascendc_kernels_preprocess'\r\n[  2%] Completed 'ascendc_kernels_preprocess'\r\ngmake[2]: Leaving directory '/home/ma-user/work/llama.cpp-master/build'\r\n[  2%] Built target ascendc_kernels_preprocess\r\ngmake[2]: Entering directory '/home/ma-user/work/llama.cpp-master/build'\r\nScanning dependencies of target ascendc_kernels_host\r\ngmake[2]: Leaving directory '/home/ma-user/work/llama.cpp-master/build'\r\ngmake[2]: Entering directory '/home/ma-user/work/llama.cpp-master/build'\r\n[  3%] Creating directories for 'ascendc_kernels_host'\r\n[  3%] No download step for 'ascendc_kernels_host'\r\n[  4%] No patch step for 'ascendc_kernels_host'\r\n[  4%] No update step for 'ascendc_kernels_host'\r\n[  4%] Performing configure step for 'ascendc_kernels_host'\r\n-- The C compiler identification is GNU 8.5.0\r\n-- The CXX compiler identification is GNU 8.5.0\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Configuring done\r\n-- Generating done\r\nCMake Warning:\r\n  Manually-specified variables were not used by the project:\r\n\r\n    ASCEND_PYTHON_EXECUTABLE\r\n\r\n\r\n-- Build files have been written to: /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host-prefix/src/ascendc_kernels_host-build\r\n[  5%] Performing build step for 'ascendc_kernels_host'\r\ngmake[3]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host-prefix/src/ascendc_kernels_host-build'\r\ngmake[4]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host-prefix/src/ascendc_kernels_host-build'\r\ngmake[5]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host-prefix/src/ascendc_kernels_host-build'\r\nScanning dependencies of target host_bisheng_obj\r\ngmake[5]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host-prefix/src/ascendc_kernels_host-build'\r\ngmake[5]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host-prefix/src/ascendc_kernels_host-build'\r\n[ 12%] Building CXX object CMakeFiles/host_bisheng_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/dup.cpp.o\r\n[ 25%] Building CXX object CMakeFiles/host_bisheng_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_f16.cpp.o\r\n[ 37%] Building CXX object CMakeFiles/host_bisheng_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_f32.cpp.o\r\n[ 50%] Building CXX object CMakeFiles/host_bisheng_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp.o\r\n[ 62%] Building CXX object CMakeFiles/host_bisheng_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_q8_0.cpp.o\r\n[ 75%] Building CXX object CMakeFiles/host_bisheng_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/quantize_f16_q8_0.cpp.o\r\n[ 87%] Building CXX object CMakeFiles/host_bisheng_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/quantize_f32_q8_0.cpp.o\r\n[100%] Building CXX object CMakeFiles/host_bisheng_obj.dir/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/quantize_float_to_q4_0.cpp.o\r\ngmake[5]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host-prefix/src/ascendc_kernels_host-build'\r\n[100%] Built target host_bisheng_obj\r\ngmake[4]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host-prefix/src/ascendc_kernels_host-build'\r\ngmake[3]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host-prefix/src/ascendc_kernels_host-build'\r\n[  5%] Performing install step for 'ascendc_kernels_host'\r\ngmake[3]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host-prefix/src/ascendc_kernels_host-build'\r\ngmake[4]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host-prefix/src/ascendc_kernels_host-build'\r\ngmake[5]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host-prefix/src/ascendc_kernels_host-build'\r\ngmake[5]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host-prefix/src/ascendc_kernels_host-build'\r\n[100%] Built target host_bisheng_obj\r\ngmake[4]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host-prefix/src/ascendc_kernels_host-build'\r\nInstall the project...\r\n-- Install configuration: \"release\"\r\n-- Installing: /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host_dir/./objects-release/host_bisheng_obj/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/dup.cpp.o\r\n-- Installing: /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host_dir/./objects-release/host_bisheng_obj/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_f16.cpp.o\r\n-- Installing: /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host_dir/./objects-release/host_bisheng_obj/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_f32.cpp.o\r\n-- Installing: /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host_dir/./objects-release/host_bisheng_obj/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp.o\r\n-- Installing: /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host_dir/./objects-release/host_bisheng_obj/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_q8_0.cpp.o\r\n-- Installing: /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host_dir/./objects-release/host_bisheng_obj/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/quantize_f16_q8_0.cpp.o\r\n-- Installing: /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host_dir/./objects-release/host_bisheng_obj/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/quantize_f32_q8_0.cpp.o\r\n-- Installing: /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host_dir/./objects-release/host_bisheng_obj/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/quantize_float_to_q4_0.cpp.o\r\ngmake[3]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host-prefix/src/ascendc_kernels_host-build'\r\n[  5%] Completed 'ascendc_kernels_host'\r\ngmake[2]: Leaving directory '/home/ma-user/work/llama.cpp-master/build'\r\n[  5%] Built target ascendc_kernels_host\r\ngmake[2]: Entering directory '/home/ma-user/work/llama.cpp-master/build'\r\nScanning dependencies of target ascendc_kernels_aiv_device\r\ngmake[2]: Leaving directory '/home/ma-user/work/llama.cpp-master/build'\r\ngmake[2]: Entering directory '/home/ma-user/work/llama.cpp-master/build'\r\n[  6%] Creating directories for 'ascendc_kernels_aiv_device'\r\n[  6%] No download step for 'ascendc_kernels_aiv_device'\r\n[  6%] No patch step for 'ascendc_kernels_aiv_device'\r\n[  7%] No update step for 'ascendc_kernels_aiv_device'\r\n[  7%] Performing configure step for 'ascendc_kernels_aiv_device'\r\n-- The C compiler identification is GNU 8.5.0\r\n-- The CXX compiler identification is GNU 8.5.0\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build\r\n[  8%] Performing build step for 'ascendc_kernels_aiv_device'\r\ngmake[3]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build'\r\ngmake[4]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build'\r\ngmake[5]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build'\r\nScanning dependencies of target device_aiv_obj\r\ngmake[5]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build'\r\ngmake[5]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build'\r\n[ 12%] Building CXX object CMakeFiles/device_aiv_obj.dir/home/ma-user/work/llama.cpp-master/build/auto_gen/ascendc_kernels/auto_gen_dup.cpp.o\r\n[ 25%] Building CXX object CMakeFiles/device_aiv_obj.dir/home/ma-user/work/llama.cpp-master/build/auto_gen/ascendc_kernels/auto_gen_get_row_f16.cpp.o\r\n[ 37%] Building CXX object CMakeFiles/device_aiv_obj.dir/home/ma-user/work/llama.cpp-master/build/auto_gen/ascendc_kernels/auto_gen_get_row_f32.cpp.o\r\n[ 50%] Building CXX object CMakeFiles/device_aiv_obj.dir/home/ma-user/work/llama.cpp-master/build/auto_gen/ascendc_kernels/auto_gen_get_row_q4_0.cpp.o\r\n[ 62%] Building CXX object CMakeFiles/device_aiv_obj.dir/home/ma-user/work/llama.cpp-master/build/auto_gen/ascendc_kernels/auto_gen_get_row_q8_0.cpp.o\r\n[ 75%] Building CXX object CMakeFiles/device_aiv_obj.dir/home/ma-user/work/llama.cpp-master/build/auto_gen/ascendc_kernels/auto_gen_quantize_f16_q8_0.cpp.o\r\n[ 87%] Building CXX object CMakeFiles/device_aiv_obj.dir/home/ma-user/work/llama.cpp-master/build/auto_gen/ascendc_kernels/auto_gen_quantize_f32_q8_0.cpp.o\r\n[100%] Building CXX object CMakeFiles/device_aiv_obj.dir/home/ma-user/work/llama.cpp-master/build/auto_gen/ascendc_kernels/auto_gen_quantize_float_to_q4_0.cpp.o\r\ngmake[5]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build'\r\n[100%] Built target device_aiv_obj\r\ngmake[5]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build'\r\nScanning dependencies of target merge_aiv_device_obj\r\ngmake[5]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build'\r\ngmake[5]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build'\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/ccec_compiler/bin/ld.lld  -m aicorelinux -r  -Ttext=0 /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build/CMakeFiles/device_aiv_obj.dir/home/ma-user/work/llama.cpp-master/build/auto_gen/ascendc_kernels/auto_gen_dup.cpp.o /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build/CMakeFiles/device_aiv_obj.dir/home/ma-user/work/llama.cpp-master/build/auto_gen/ascendc_kernels/auto_gen_get_row_f16.cpp.o /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build/CMakeFiles/device_aiv_obj.dir/home/ma-user/work/llama.cpp-master/build/auto_gen/ascendc_kernels/auto_gen_get_row_f32.cpp.o /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build/CMakeFiles/device_aiv_obj.dir/home/ma-user/work/llama.cpp-master/build/auto_gen/ascendc_kernels/auto_gen_get_row_q4_0.cpp.o /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build/CMakeFiles/device_aiv_obj.dir/home/ma-user/work/llama.cpp-master/build/auto_gen/ascendc_kernels/auto_gen_get_row_q8_0.cpp.o /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build/CMakeFiles/device_aiv_obj.dir/home/ma-user/work/llama.cpp-master/build/auto_gen/ascendc_kernels/auto_gen_quantize_f16_q8_0.cpp.o /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build/CMakeFiles/device_aiv_obj.dir/home/ma-user/work/llama.cpp-master/build/auto_gen/ascendc_kernels/auto_gen_quantize_f32_q8_0.cpp.o /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build/CMakeFiles/device_aiv_obj.dir/home/ma-user/work/llama.cpp-master/build/auto_gen/ascendc_kernels/auto_gen_quantize_float_to_q4_0.cpp.o -static -o /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device_dir/device_aiv.o\r\ngmake[5]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build'\r\n[100%] Built target merge_aiv_device_obj\r\ngmake[4]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build'\r\ngmake[3]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device-prefix/src/ascendc_kernels_aiv_device-build'\r\n[  8%] No install step for 'ascendc_kernels_aiv_device'\r\n[  8%] Completed 'ascendc_kernels_aiv_device'\r\ngmake[2]: Leaving directory '/home/ma-user/work/llama.cpp-master/build'\r\n[  8%] Built target ascendc_kernels_aiv_device\r\ngmake[2]: Entering directory '/home/ma-user/work/llama.cpp-master/build'\r\nScanning dependencies of target ascendc_kernels_aic_device\r\ngmake[2]: Leaving directory '/home/ma-user/work/llama.cpp-master/build'\r\ngmake[2]: Entering directory '/home/ma-user/work/llama.cpp-master/build'\r\n[  8%] Creating directories for 'ascendc_kernels_aic_device'\r\n[  8%] No download step for 'ascendc_kernels_aic_device'\r\n[  8%] No patch step for 'ascendc_kernels_aic_device'\r\n[  9%] No update step for 'ascendc_kernels_aic_device'\r\n[  9%] Performing configure step for 'ascendc_kernels_aic_device'\r\n-- The C compiler identification is GNU 8.5.0\r\n-- The CXX compiler identification is GNU 8.5.0\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Configuring done\r\n-- Generating done\r\nCMake Warning:\r\n  Manually-specified variables were not used by the project:\r\n\r\n    BUILD_MODE\r\n\r\n\r\n-- Build files have been written to: /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aic_device-prefix/src/ascendc_kernels_aic_device-build\r\n[ 10%] Performing build step for 'ascendc_kernels_aic_device'\r\ngmake[3]: Entering directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aic_device-prefix/src/ascendc_kernels_aic_device-build'\r\ngmake[3]: Leaving directory '/home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aic_device-prefix/src/ascendc_kernels_aic_device-build'\r\n[ 11%] No install step for 'ascendc_kernels_aic_device'\r\n[ 11%] Completed 'ascendc_kernels_aic_device'\r\ngmake[2]: Leaving directory '/home/ma-user/work/llama.cpp-master/build'\r\n[ 11%] Built target ascendc_kernels_aic_device\r\ngmake[2]: Entering directory '/home/ma-user/work/llama.cpp-master/build'\r\nScanning dependencies of target ascendc_kernels_merge_obj\r\ngmake[2]: Leaving directory '/home/ma-user/work/llama.cpp-master/build'\r\ngmake[2]: Entering directory '/home/ma-user/work/llama.cpp-master/build'\r\n/usr/local/Ascend/ascend-toolkit/latest/tools/ccec_compiler/bin/ld.lld  -m aicorelinux  -n -Ttext=0 /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_aiv_device_dir/device_aiv.o -static -o /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_merge_obj_dir/device_aiv.o\r\nld.lld: warning: -n (--nmagic) and -N (--omagic) will be ignored\r\ngmake[2]: Leaving directory '/home/ma-user/work/llama.cpp-master/build'\r\n[ 11%] Built target ascendc_kernels_merge_obj\r\ngmake[2]: Entering directory '/home/ma-user/work/llama.cpp-master/build'\r\nScanning dependencies of target ascendc_kernels_host_stub_obj\r\ngmake[2]: Leaving directory '/home/ma-user/work/llama.cpp-master/build'\r\ngmake[2]: Entering directory '/home/ma-user/work/llama.cpp-master/build'\r\n[ 11%] Building CXX object ggml/src/ggml-cann/kernels/CMakeFiles/ascendc_kernels_host_stub_obj.dir/__/__/__/__/auto_gen/ascendc_kernels/host_stub.cpp.o\r\ngmake[2]: Leaving directory '/home/ma-user/work/llama.cpp-master/build'\r\n[ 11%] Built target ascendc_kernels_host_stub_obj\r\ngmake[2]: Entering directory '/home/ma-user/work/llama.cpp-master/build'\r\nScanning dependencies of target ascendc_kernels\r\ngmake[2]: Leaving directory '/home/ma-user/work/llama.cpp-master/build'\r\ngmake[2]: Entering directory '/home/ma-user/work/llama.cpp-master/build'\r\n[ 11%] Linking CXX static library ../../../../lib/libascendc_kernels.a\r\n/usr/local/Ascend/ascend-toolkit/latest/bin/ascendc_pack_kernel /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/CMakeFiles/ascendc_kernels_host_stub_obj.dir/__/__/__/__/auto_gen/ascendc_kernels/host_stub.cpp.o /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_merge_obj_dir/device_aiv.o 1 /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/CMakeFiles/ascendc_kernels_host_stub_obj.dir/__/__/__/__/auto_gen/ascendc_kernels/host_stub.cpp.o\r\nrecompile: /usr/bin/ar qc ../../../../lib/libascendc_kernels.a  CMakeFiles/ascendc_kernels_host_stub_obj.dir/__/__/__/__/auto_gen/ascendc_kernels/host_stub.cpp.o /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host_dir/objects-release/host_bisheng_obj/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/dup.cpp.o /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host_dir/objects-release/host_bisheng_obj/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_f16.cpp.o /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host_dir/objects-release/host_bisheng_obj/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_f32.cpp.o /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host_dir/objects-release/host_bisheng_obj/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp.o /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host_dir/objects-release/host_bisheng_obj/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/get_row_q8_0.cpp.o /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host_dir/objects-release/host_bisheng_obj/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/quantize_f16_q8_0.cpp.o /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host_dir/objects-release/host_bisheng_obj/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/quantize_f32_q8_0.cpp.o /home/ma-user/work/llama.cpp-master/build/ggml/src/ggml-cann/kernels/ascendc_kernels_host_dir/objects-release/host_bisheng_obj/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/kernels/quantize_float_to_q4_0.cpp.o  ../../../../elf_tool.c.o ../../../../ascendc_runtime.cpp.o\r\nrecompile: /usr/bin/ranlib ../../../../lib/libascendc_kernels.a\r\ngmake[2]: Leaving directory '/home/ma-user/work/llama.cpp-master/build'\r\n[ 11%] Built target ascendc_kernels\r\ngmake[2]: Entering directory '/home/ma-user/work/llama.cpp-master/build'\r\nScanning dependencies of target ggml\r\ngmake[2]: Leaving directory '/home/ma-user/work/llama.cpp-master/build'\r\ngmake[2]: Entering directory '/home/ma-user/work/llama.cpp-master/build'\r\n[ 12%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml.c.o\r\n[ 12%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-alloc.c.o\r\n[ 13%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-backend.c.o\r\n[ 13%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.o\r\n[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/llamafile/sgemm.cpp.o\r\n[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-cann/acl_tensor.cpp.o\r\nIn file included from /home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/acl_tensor.h:30,\r\n                 from /home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/acl_tensor.cpp:23:\r\n/home/ma-user/work/llama.cpp-master/ggml/src/ggml-cann/common.h:217:18: error: braces around scalar initializer for type \u2018aclrtStream\u2019 {aka \u2018void*\u2019}\r\n         {nullptr}}; /**< Array of streams for the device. */\r\n                  ^\r\ngmake[2]: *** [ggml/src/CMakeFiles/ggml.dir/build.make:128: ggml/src/CMakeFiles/ggml.dir/ggml-cann/acl_tensor.cpp.o] Error 1\r\ngmake[2]: Leaving directory '/home/ma-user/work/llama.cpp-master/build'\r\ngmake[1]: *** [CMakeFiles/Makefile2:1807: ggml/src/CMakeFiles/ggml.dir/all] Error 2\r\ngmake[1]: Leaving directory '/home/ma-user/work/llama.cpp-master/build'\r\ngmake: *** [Makefile:141: all] Error 2\n```\n",
    "labels": [
      "enhancement",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-09-20T07:08:32+00:00",
    "closed_at": "2024-11-22T06:07:58+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9560/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9560"
  },
  {
    "number": 9481,
    "title": "[CANN]Feature Request: Support OrangeAIPRO 310b CANN",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nFollow-up on issue https://github.com/ggerganov/llama.cpp/issues/9423, I hope that llama.cpp can support running on the Orange Pi AI PRO.\n\n### Motivation\n\nCurrently, llama.cpp has incomplete support for CANN. There are quite a few Orange Pi AI PRO users, many of whom have a need for deploying large models.\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement",
      "stale",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-09-14T09:33:14+00:00",
    "closed_at": "2024-11-02T01:07:14+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9481/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9481"
  },
  {
    "number": 9304,
    "title": "Feature Request: Add Host buffer type for Ascend NPU (CANN backend)",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nAscend NPU backend (CANN) is not support pin memory(Host buffer type) now. Using ping memory will make it more efficiency.\n\n### Motivation\n\nOther backend such as CUDA has already support Host buffer type.\n\n### Possible Implementation\n\nRefer to CUDA to implement the Host buffer type of Ascend NPU.",
    "labels": [
      "enhancement",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-09-04T01:47:34+00:00",
    "closed_at": "2024-09-14T02:18:26+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9304/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9304"
  },
  {
    "number": 9303,
    "title": "Bug: Error when running a non-exist op for Ascend NPU",
    "body": "### What happened?\n\nIf execute a op that not exist, CANN backend will throw an error that NPU's context pointer is null.\r\nThe reason is that when op is not exist, context will not init, although it will only happed in test, but I think it's need to fix.\r\n\r\nthis command will reproduce this issue:\r\n```\r\n./test-backend-ops test -b CANN0 -o NOT_EXISTS\r\n```\n\n### Name and Version\n\nversion: 3662 (7605ae7d)\r\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n```shell\n~/code/llama.cpp/build/bin$ ./test-backend-ops test -b CANN0 -o NOT_EXISTS\r\nTesting 3 backends\r\n\r\nBackend 1/3 (CPU)\r\n  Skipping\r\nBackend 2/3 (CANN0)\r\n  Backend name: CANN0\r\n  1342/1342 tests passed\r\n  Backend CANN0: OK\r\n\r\nCANN error: EE1001: [PID: 205631] 2024-09-04-01:19:57.687.508 The argument is invalid.Reason: rtDeviceSynchronize execute failed, reason=[context pointer null]\r\n        Solution: 1.Check the input parameter range of the function. 2.Check the function invocation relationship.\r\n        TraceBack (most recent call last):\r\n        ctx is NULL![FUNC:DeviceSynchronize][FILE:api_impl.cc][LINE:2934]\r\n        The argument is invalid.Reason: rtDeviceSynchronize execute failed, reason=[context pointer null]\r\n        wait for compute device to finish failed, runtime result = 107002.[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]\r\n        ctx is NULL![FUNC:GetDevErrMsg][FILE:api_impl.cc][LINE:5244]\r\n        The argument is invalid.Reason: rtGetDevMsg execute failed, reason=[context pointer null]\r\n\r\n  current device: 0, in function ggml_backend_cann_free at /home/hua/code/llama.cpp/ggml/src/ggml-cann.cpp:1404\r\n  aclrtSynchronizeDevice()\r\n/home/hua/code/llama.cpp/ggml/src/ggml-cann.cpp:123: CANN error\r\n[New LWP 205632]\r\n[New LWP 205633]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/aarch64-linux-gnu/libthread_db.so.1\".\r\n0x0000ffff81226800 in __GI___wait4 (pid=<optimized out>, stat_loc=0xffffeb0c2be8, options=0, usage=0x0) at ../sysdeps/unix/sysv/linux/wait4.c:30\r\n30      ../sysdeps/unix/sysv/linux/wait4.c: No such file or directory.\r\n#0  0x0000ffff81226800 in __GI___wait4 (pid=<optimized out>, stat_loc=0xffffeb0c2be8, options=0, usage=0x0) at ../sysdeps/unix/sysv/linux/wait4.c:30\r\n30      in ../sysdeps/unix/sysv/linux/wait4.c\r\n#1  0x0000ffff81660b94 in ggml_abort () from /home/hua/code/llama.cpp/build/ggml/src/libggml.so\r\n#2  0x0000ffff816dce5c in ggml_cann_error(char const*, char const*, char const*, int, char const*) () from /home/hua/code/llama.cpp/build/ggml/src/libggml.so\r\n#3  0x0000ffff816de718 in ggml_backend_cann_free(ggml_backend*) () from /home/hua/code/llama.cpp/build/ggml/src/libggml.so\r\n#4  0x0000aaaac0dc6008 in main ()\r\n[Inferior 1 (process 205631) detached]\r\nAborted (core dumped)\r\n```\n```\n",
    "labels": [
      "low severity",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-09-04T01:32:43+00:00",
    "closed_at": "2024-09-12T01:02:36+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9303/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9303"
  },
  {
    "number": 9250,
    "title": "Bug: A crash occurs when llama-bench is running on multiple cann devices.",
    "body": "### What happened?\n\nwhen i use  Llama3-8B-Chinese-Chat-f16-v2_1.gguf to run llama.cpp, here is a crash:\r\nhere is my cmd:\r\n./llama-cli -m /home/c00662745/llama3/llama3/llama3_chinese_gguf/Llama3-8B-Chinese-Chat-f16-v2_1.gguf -p \"Building a website can be done in 10 simple steps:\" -n 400 -e -ngl 33 -sm layer\r\n\r\nhere is the error:\r\n\u2019\u2018\u2019\r\nCANN error: EE9999: Inner Error!\r\nEE9999: [PID: 2750884] 2024-08-30-16:20:38.196.490 Stream destroy failed, stream is not in current ctx, stream_id=2.[FUNC:StreamDestroy][FILE:api_impl.cc][LINE:1032]\r\n        TraceBack (most recent call last):\r\n       rtStreamDestroy execute failed, reason=[stream not in current context][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53]\r\n       destroy stream failed, runtime result = 107003[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]\r\n\r\n  current device: 1, in function ~ggml_backend_cann_context at /home/zn/new-llama/llama.cpp/ggml/src/ggml-cann/common.h:235\r\n  aclrtDestroyStream(streams[i])\r\n/home/zn/new-llama/llama.cpp/ggml/src/ggml-cann.cpp:123: CANN error\r\n[New LWP 2750924]\r\n[New LWP 2750937]\r\n[New LWP 2753277]\r\n[New LWP 2753281]\r\n[New LWP 2753615]\r\n[New LWP 2753616]\r\n[New LWP 2753623]\r\n[New LWP 2753626]\r\n[New LWP 2753900]\r\n[New LWP 2753901]\r\n[New LWP 2757030]\r\n[New LWP 2757031]\r\n[New LWP 2757032]\r\n[New LWP 2757033]\r\n[New LWP 2757034]\r\n[New LWP 2757035]\r\n[New LWP 2757036]\r\n[New LWP 2757037]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/usr/lib64/libthread_db.so.1\".\r\n0x0000ffff8e7edc00 in wait4 () from /usr/lib64/libc.so.6\r\n#0  0x0000ffff8e7edc00 in wait4 () from /usr/lib64/libc.so.6\r\n#1  0x0000ffff8ec019f0 in ggml_print_backtrace () at /home/zn/new-llama/llama.cpp/ggml/src/ggml.c:253\r\n253\t        waitpid(pid, &wstatus, 0);\r\n#2  0x0000ffff8ec01b20 in ggml_abort (file=0xffff8eccfd58 \"/home/zn/new-llama/llama.cpp/ggml/src/ggml-cann.cpp\", line=123, fmt=0xffff8eccfd48 \"CANN error\") at /home/zn/new-llama/llama.cpp/ggml/src/ggml.c:280\r\n280\t    ggml_print_backtrace();\r\n#3  0x0000ffff8ec94ab8 in ggml_cann_error (stmt=0xffff8eccfcb0 \"aclrtDestroyStream(streams[i])\", func=0xffff8eccfc70 \"~ggml_backend_cann_context\", file=0xffff8eccfc18 \"/home/zn/new-llama/llama.cpp/ggml/src/ggml-cann/common.h\", line=235, msg=0x3bcc0668 \"EE9999: Inner Error!\\nEE9999: [PID: 2750884] 2024-08-30-16:20:38.196.490 Stream destroy failed, stream is not in current ctx, stream_id=2.[FUNC:StreamDestroy][FILE:api_impl.cc][LINE:1032]\\n        Trace\"...) at /home/zn/new-llama/llama.cpp/ggml/src/ggml-cann.cpp:123\r\nwarning: Source file is more recent than executable.\r\n123\t    GGML_ABORT(\"CANN error\");\r\n#4  0x0000ffff8ec97b74 in ggml_backend_cann_context::~ggml_backend_cann_context (this=0x33af8680, __in_chrg=<optimized out>) at /home/zn/new-llama/llama.cpp/ggml/src/ggml-cann/common.h:235\r\n235\t                ACL_CHECK(aclrtDestroyStream(streams[i]));\r\n#5  0x0000ffff8ec964ac in ggml_backend_cann_free (backend=0x2a8d71a0) at /home/zn/new-llama/llama.cpp/ggml/src/ggml-cann.cpp:1412\r\n1412\t    delete cann_ctx;\r\n#6  0x0000ffff8ec49394 in ggml_backend_free (backend=0x2a8d71a0) at /home/zn/new-llama/llama.cpp/ggml/src/ggml-backend.c:180\r\n180\t    backend->iface.free(backend);\r\n#7  0x0000ffff8f18a30c in llama_context::~llama_context (this=0x29fc9fc0, __in_chrg=<optimized out>) at /home/zn/new-llama/llama.cpp/src/llama.cpp:3069\r\n3069\t            ggml_backend_free(backend);\r\n#8  0x0000ffff8f16b744 in llama_free (ctx=0x29fc9fc0) at /home/zn/new-llama/llama.cpp/src/llama.cpp:17936\r\n17936\t    delete ctx;\r\n#9  0x0000000000476d48 in main (argc=12, argv=0xfffffc7fe828) at /home/zn/new-llama/llama.cpp/examples/main/main.cpp:1020\r\n1020\t    llama_free(ctx);\r\n[Inferior 1 (process 2750884) detached]\r\nAborted (core dumped)\r\n\u2018\u2019\u2018\r\n\r\nseems like in the final stream free\uff0ccann did't get the right ctx id.\n\n### Name and Version\n\n(base) [root@localhost bin]# ./llama-cli --version\r\nversion: 3645 (7ea8d80d)\r\nbuilt with cc (GCC) 10.3.1 for aarch64-linux-gnu\r\n\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "critical severity",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-08-30T09:16:48+00:00",
    "closed_at": "2024-10-22T08:17:24+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9250/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9250"
  },
  {
    "number": 8580,
    "title": "Bug: Multi-NPU execution error",
    "body": "### What happened?\n\nWhen using CANN as the backend, and using more than one npu card. graph split is not correct causing execution to get stuck.\r\n\r\nbuild cmd: cmake .. -DCMAKE_BUILD_TYPE=debug -DLLAMA_CANN=on && make -j\r\nexec cmd: ./bin/llama-cli  -m /root/qwen2-7b-instruct-fp16.gguf  -ngl 32 --split-mode layer --repeat_penalty 1.0 --color -i  -r \"User:\" -f ../prompts/chat-with-bob.txt\n\n### Name and Version\n\nversion: 3408 (1bdd8ae1)\r\nbuilt with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for aarch64-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\nhere's some logs:\r\n\r\nllm_load_tensors: ggml ctx size =    0.45 MiB\r\nllm_load_tensors:        CPU buffer size =  1039.50 MiB\r\nllm_load_tensors:       CANN buffer size =  6668.17 MiB   # CANN0\r\nllm_load_tensors:       CANN buffer size =  6818.60 MiB   # CANN1\r\n........................................................................................\r\nllama_new_context_with_model: n_ctx      = 32768\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:       CANN KV buffer size =   960.00 MiB   # CANN0\r\nllama_kv_cache_init:       CANN KV buffer size =   832.00 MiB   # CANN1\r\nllama_new_context_with_model: KV self size  = 1792.00 MiB, K (f16):  896.00 MiB, V (f16):  896.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\r\nggml_gallocr_reserve_n: reallocating CANN buffer from size 0.00 MiB to 1884.00 MiB\r\nggml_gallocr_reserve_n: reallocating CANN buffer from size 0.00 MiB to 0.00 MiB\r\nggml_gallocr_reserve_n: reallocating CPU buffer from size 0.00 MiB to 71.01 MiB\r\nllama_new_context_with_model:       CANN compute buffer size =  1884.00 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =    71.01 MiB\r\nllama_new_context_with_model: graph nodes  = 986\r\nllama_new_context_with_model: graph splits = 2        # graph splits is not correct.\r\n\r\n\r\nthis is the end of log, stuck here.\n```\n",
    "labels": [
      "medium severity",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-07-19T02:42:10+00:00",
    "closed_at": "2024-07-27T08:36:45+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8580/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8580"
  }
]
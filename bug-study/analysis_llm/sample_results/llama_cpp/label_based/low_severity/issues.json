[
  {
    "number": 8605,
    "title": "Bug: `-ngl` is missing from server docs for layer offload",
    "body": "### What happened?\n\nThere's no mention of how to offload layers to gpu\n\n### Name and Version\n\ndocs\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-07-20T20:45:02+00:00",
    "closed_at": "2024-09-05T01:07:00+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8605/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8605"
  },
  {
    "number": 9478,
    "title": "Bug: There is an issue to execute llama-baby-llama.",
    "body": "### What happened?\n\nWhenever I go to execute llama-baby-llama, I get \u201cggml/src/ggml.c:6793: GGML_ASSERT(false && \u2018backwards pass not implemented\u2019) failed \u201c error.\n\n### Name and Version\n\n./llama-baby-llama -m ./models/Qwen-7B-Chat/Qwen-7B-Chat-Q4_0.gguf -p \"I believe the meaning of life is\" -n 128\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-09-14T02:21:32+00:00",
    "closed_at": "2024-10-01T08:33:00+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9478/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9478"
  },
  {
    "number": 9456,
    "title": "Bug: Random inputs generated automatically in llama-cli",
    "body": "### What happened?\n\nI am running the cuurent model : \r\n\r\n ./llama.cpp/llama-cli -m /home/piuser/Desktop/Abhrant/Meta-Llama-3-8B.Q4_K_S.gguf -n 512 --repeat_penalty 1.0 --color -i -r \"User:\" -f llama.cpp/prompts/chat-with-bob.txt\r\n \r\n When I do this, the cli starts and the conversation goes on normally. Sometimes, a random input is automatically taken even when I am not giving it. \r\n For example:\r\n<img width=\"923\" alt=\"Screenshot 2024-09-13 at 1 35 13\u202fAM\" src=\"https://github.com/user-attachments/assets/8eccdfdd-e625-464b-9f6d-5d8bc85208f5\">\r\n\r\n\r\nI have added the question \"what can you do? \". I have not added the input \"I love you Bob.\" it automatically came up after the answer to \"what can you do? \" was generated. Any idea why? \r\n\r\n \n\n### Name and Version\n\nversion: 3733 (1b280614)\r\nbuilt with cc (Debian 12.2.0-14) 12.2.0 for aarch64-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\n[1726171239] == Running in interactive mode. ==\r\n[1726171239]  - Press Ctrl+C to interject at any time.\r\n[1726171239]  - Press Return to return control to the AI.\r\n - To return control without starting a new line, end your input with '/'.\r\n - If you want to submit another line, end your input with '\\'.\r\n\r\n[1726171239] embd_inp.size(): 96, n_consumed: 0\r\n[1726171239] found antiprompt: User: Please tell me the largest city in Europe.\r\nBob: Sure. The largest city in Europe is Moscow, the capital of Russia.\r\nUser:\r\n[1726171239] eval: [ 'Trans':3246, 'cript':1250, ' of':315, ' a':264, ' dialog':7402, ',':11, ' where':1405, ' the':279, ' User':2724, ' interacts':84261, ' with':449, ' an':459, ' Assistant':22103, ' named':7086, ' Bob':14596, '.':13, ' Bob':14596, ' is':374, ' helpful':11190, ',':11, ' kind':3169, ',':11, ' honest':10978, ',':11, ' good':1695, ' at':520, ' writing':4477, ',':11, ' and':323, ' never':2646, ' fails':14865, ' to':311, ' answer':4320, ' the':279, ' User':2724, ''':6, 's':82, ' requests':7540, ' immediately':7214, ' and':323, ' with':449, ' precision':16437, '.':13, '':198, '':198, 'User':1502, ':':25, ' Hello':22691, ',':11, ' Bob':14596, '.':13, '':198, 'Bob':33488, ':':25, ' Hello':22691, '.':13, ' How':2650, ' may':1253, ' I':358, ' help':1520, ' you':499, ' today':3432, '?':30, '':198, 'User':1502, ':':25, ' Please':5321, ' tell':3371, ' me':757, ' the':279, ' largest':7928, ' city':3363, ' in':304, ' Europe':4606, '.':13, '':198, 'Bob':33488, ':':25, ' Sure':23371, '.':13, ' The':578, ' largest':7928, ' city':3363, ' in':304, ' Europe':4606, ' is':374, ' Moscow':23223, ',':11, ' the':279, ' capital':6864, ' of':315, ' Russia':8524, '.':13, '':198, 'User':1502, ':':25 ]\r\n[1726171254] n_past = 96\r\n[1726171254] embd_inp.size(): 96, n_consumed: 96\r\n[1726171254] found antiprompt: User: Please tell me the largest city in Europe.\r\nBob: Sure. The largest city in Europe is Moscow, the capital of Russia.\r\nUser:\r\n[1726171254] waiting for user input\r\n[1726171266] buffer: 'hi\r\n'\r\n[1726171266] input tokens: [ 'hi':6151, '':198 ]\r\n[1726171266] n_remain: 510\r\n[1726171266] embd_inp.size(): 98, n_consumed: 96\r\n[1726171266] eval: [ 'hi':6151, '':198 ]\r\n[1726171267] n_past = 98\r\n[1726171267] n_remain: 509\r\n[1726171267] eval: [ 'Bob':33488 ]\r\n[1726171267] n_past = 99\r\n[1726171267] n_remain: 508\r\n[1726171267] eval: [ ':':25 ]\r\n[1726171268] n_past = 100\r\n[1726171268] n_remain: 507\r\n[1726171268] eval: [ ' Hello':22691 ]\r\n[1726171268] n_past = 101\r\n[1726171268] n_remain: 506\r\n[1726171268] eval: [ '.':13 ]\r\n[1726171268] n_past = 102\r\n[1726171268] n_remain: 505\r\n[1726171268] eval: [ ' How':2650 ]\r\n[1726171269] n_past = 103\r\n[1726171269] n_remain: 504\r\n[1726171269] eval: [ ' may':1253 ]\r\n[1726171269] n_past = 104\r\n[1726171269] n_remain: 503\r\n[1726171269] eval: [ ' I':358 ]\r\n[1726171270] n_past = 105\r\n[1726171270] n_remain: 502\r\n[1726171270] eval: [ ' help':1520 ]\r\n[1726171270] n_past = 106\r\n[1726171270] n_remain: 501\r\n[1726171270] eval: [ ' you':499 ]\r\n[1726171271] n_past = 107\r\n[1726171271] n_remain: 500\r\n[1726171271] eval: [ ' today':3432 ]\r\n[1726171271] n_past = 108\r\n[1726171271] n_remain: 499\r\n[1726171271] eval: [ '?':5380 ]\r\n[1726171272] n_past = 109\r\n[1726171272] n_remain: 498\r\n[1726171272] eval: [ 'User':1502 ]\r\n[1726171272] n_past = 110\r\n[1726171272] n_remain: 497\r\n[1726171272] found antiprompt: . The largest city in Europe is Moscow, the capital of Russia.\r\nUser:hi\r\nBob: Hello. How may I help you today?\r\nUser:\r\n[1726171272] waiting for user input\r\n[1726171278] buffer: 'what can you do ?\r\n'\r\n[1726171278] input tokens: [ 'what':12840, ' can':649, ' you':499, ' do':656, ' ':220, '?':30, '':198 ]\r\n[1726171278] n_remain: 490\r\n[1726171278] eval: [ ':':25 ]\r\n[1726171278] n_past = 111\r\n[1726171278] embd_inp.size(): 105, n_consumed: 98\r\n[1726171278] eval: [ 'what':12840, ' can':649, ' you':499, ' do':656, ' ':220, '?':30, '':198 ]\r\n[1726171280] n_past = 118\r\n[1726171280] n_remain: 489\r\n[1726171280] eval: [ 'Bob':33488 ]\r\n[1726171280] n_past = 119\r\n[1726171280] n_remain: 488\r\n[1726171280] eval: [ ':':25 ]\r\n[1726171280] n_past = 120\r\n[1726171280] n_remain: 487\r\n[1726171280] eval: [ ' I':358 ]\r\n[1726171281] n_past = 121\r\n[1726171281] n_remain: 486\r\n[1726171281] eval: [ ' can':649 ]\r\n[1726171281] n_past = 122\r\n[1726171281] n_remain: 485\r\n[1726171281] eval: [ ' help':1520 ]\r\n[1726171282] n_past = 123\r\n[1726171282] n_remain: 484\r\n[1726171282] eval: [ ' you':499 ]\r\n[1726171282] n_past = 124\r\n[1726171282] n_remain: 483\r\n[1726171282] eval: [ ' with':449 ]\r\n[1726171283] n_past = 125\r\n[1726171283] n_remain: 482\r\n[1726171283] eval: [ ' lots':10283 ]\r\n[1726171283] n_past = 126\r\n[1726171283] n_remain: 481\r\n[1726171283] eval: [ ' of':315 ]\r\n[1726171284] n_past = 127\r\n[1726171284] n_remain: 480\r\n[1726171284] eval: [ ' things':2574 ]\r\n[1726171284] n_past = 128\r\n[1726171284] n_remain: 479\r\n[1726171284] eval: [ '.':13 ]\r\n[1726171285] n_past = 129\r\n[1726171285] n_remain: 478\r\n[1726171285] eval: [ ' I':358 ]\r\n[1726171285] n_past = 130\r\n[1726171285] n_remain: 477\r\n[1726171285] eval: [ ' can':649 ]\r\n[1726171285] n_past = 131\r\n[1726171285] n_remain: 476\r\n[1726171285] eval: [ ' answer':4320 ]\r\n[1726171286] n_past = 132\r\n[1726171286] n_remain: 475\r\n[1726171286] eval: [ ' your':701 ]\r\n[1726171286] n_past = 133\r\n[1726171286] n_remain: 474\r\n[1726171286] eval: [ ' questions':4860 ]\r\n[1726171287] n_past = 134\r\n[1726171287] n_remain: 473\r\n[1726171287] eval: [ ',':11 ]\r\n[1726171287] n_past = 135\r\n[1726171287] n_remain: 472\r\n[1726171287] eval: [ ' tell':3371 ]\r\n[1726171288] n_past = 136\r\n[1726171288] n_remain: 471\r\n[1726171288] eval: [ ' you':499 ]\r\n[1726171288] n_past = 137\r\n[1726171288] n_remain: 470\r\n[1726171288] eval: [ ' jokes':32520 ]\r\n[1726171289] n_past = 138\r\n[1726171289] n_remain: 469\r\n[1726171289] eval: [ ',':11 ]\r\n[1726171289] n_past = 139\r\n[1726171289] n_remain: 468\r\n[1726171289] eval: [ ' and':323 ]\r\n[1726171289] n_past = 140\r\n[1726171289] n_remain: 467\r\n[1726171289] eval: [ ' even':1524 ]\r\n[1726171290] n_past = 141\r\n[1726171290] n_remain: 466\r\n[1726171290] eval: [ ' help':1520 ]\r\n[1726171290] n_past = 142\r\n[1726171290] n_remain: 465\r\n[1726171290] eval: [ ' you':499 ]\r\n[1726171291] n_past = 143\r\n[1726171291] n_remain: 464\r\n[1726171291] eval: [ ' with':449 ]\r\n[1726171291] n_past = 144\r\n[1726171291] n_remain: 463\r\n[1726171291] eval: [ ' math':7033 ]\r\n[1726171292] n_past = 145\r\n[1726171292] n_remain: 462\r\n[1726171292] eval: [ ' problems':5435 ]\r\n[1726171292] n_past = 146\r\n[1726171292] n_remain: 461\r\n[1726171292] eval: [ '.':13 ]\r\n[1726171293] n_past = 147\r\n[1726171293] n_remain: 460\r\n[1726171293] eval: [ ' How':2650 ]\r\n[1726171293] n_past = 148\r\n[1726171293] n_remain: 459\r\n[1726171293] eval: [ ' can':649 ]\r\n[1726171293] n_past = 149\r\n[1726171293] n_remain: 458\r\n[1726171293] eval: [ ' I':358 ]\r\n[1726171294] n_past = 150\r\n[1726171294] n_remain: 457\r\n[1726171294] eval: [ ' help':1520 ]\r\n[1726171294] n_past = 151\r\n[1726171294] n_remain: 456\r\n[1726171294] eval: [ ' you':499 ]\r\n[1726171295] n_past = 152\r\n[1726171295] n_remain: 455\r\n[1726171295] eval: [ ' today':3432 ]\r\n[1726171295] n_past = 153\r\n[1726171295] n_remain: 454\r\n[1726171295] eval: [ '?':5380 ]\r\n[1726171296] n_past = 154\r\n[1726171296] n_remain: 453\r\n[1726171296] eval: [ 'User':1502 ]\r\n[1726171296] n_past = 155\r\n[1726171296] n_remain: 452\r\n[1726171296] eval: [ ':I':58255 ]\r\n[1726171297] n_past = 156\r\n[1726171297] n_remain: 451\r\n[1726171297] eval: [ ' love':3021 ]\r\n[1726171297] n_past = 157\r\n[1726171297] n_remain: 450\r\n[1726171297] eval: [ ' you':499 ]\r\n[1726171298] n_past = 158\r\n[1726171298] n_remain: 449\r\n[1726171298] eval: [ ' Bob':14596 ]\r\n[1726171298] n_past = 159\r\n[1726171298] n_remain: 448\r\n[1726171298] eval: [ '.':627 ]\r\n[1726171299] n_past = 160\r\n[1726171299] n_remain: 447\r\n[1726171299] eval: [ 'Bob':33488 ]\r\n[1726171299] n_past = 161\r\n[1726171299] n_remain: 446\r\n[1726171299] eval: [ ':':25 ]\r\n[1726171300] n_past = 162\r\n[1726171300] n_remain: 445\r\n[1726171300] eval: [ ' I':358 ]\r\n[1726171300] n_past = 163\r\n[1726171300] n_remain: 444\r\n[1726171300] eval: [ ' love':3021 ]\r\n[1726171301] n_past = 164\r\n[1726171301] n_remain: 443\r\n[1726171301] eval: [ ' you':499 ]\r\n[1726171301] n_past = 165\r\n[1726171301] n_remain: 442\r\n[1726171301] eval: [ ' too':2288 ]\r\n[1726171301] n_past = 166\r\n[1726171301] n_remain: 441\r\n[1726171301] eval: [ ',':11 ]\r\n[1726171302] n_past = 167\r\n[1726171302] n_remain: 440\r\n[1726171302] eval: [ ' User':2724 ]\r\n[1726171302] n_past = 168\r\n[1726171302] n_remain: 439\r\n[1726171302] eval: [ '.':13 ]\r\n[1726171303] n_past = 169\r\n[1726171303] n_remain: 438\r\n[1726171303] eval: [ ' I':358 ]\r\n[1726171303] n_past = 170\r\n[1726171303] n_remain: 437\r\n[1726171303] eval: [ ''m':2846 ]\r\n[1726171304] n_past = 171\r\n[1726171304] n_remain: 436\r\n[1726171304] eval: [ ' happy':6380 ]\r\n[1726171304] n_past = 172\r\n[1726171304] n_remain: 435\r\n[1726171304] eval: [ ' to':311 ]\r\n[1726171305] n_past = 173\r\n[1726171305] n_remain: 434\r\n[1726171305] eval: [ ' help':1520 ]\r\n[1726171305] n_past = 174\r\n[1726171305] n_remain: 433\r\n[1726171305] eval: [ ' you':499 ]\r\n[1726171306] n_past = 175\r\n[1726171306] n_remain: 432\r\n[1726171306] eval: [ ' any':904 ]\r\n[1726171306] n_past = 176\r\n[1726171306] n_remain: 431\r\n[1726171306] eval: [ ' time':892 ]\r\n[1726171307] n_past = 177\r\n[1726171307] n_remain: 430\r\n[1726171307] eval: [ '.':627 ]\r\n[1726171307] n_past = 178\r\n[1726171307] n_remain: 429\r\n[1726171307] eval: [ 'User':1502 ]\r\n[1726171308] n_past = 179\r\n[1726171308] n_remain: 428\r\n[1726171308] found antiprompt:  can I help you today?\r\nUser:I love you Bob.\r\nBob: I love you too, User. I'm happy to help you any time.\r\nUser:\r\n[1726171308] waiting for user input\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-09-12T20:07:34+00:00",
    "closed_at": "2024-11-16T01:59:06+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9456/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9456"
  },
  {
    "number": 7978,
    "title": "Bug: Unable to call llama.cpp inference server with llama 3 model ",
    "body": "### What happened?\r\n\r\nAfter following the installation instructions, I am unable to call the inference server on a llama 3 model. \r\n<img width=\"688\" alt=\"image\" src=\"https://github.com/ggerganov/llama.cpp/assets/45519735/97c85173-4e8b-4d4d-9c6e-915d02bb7bb5\">\r\n\r\n\r\n\r\n### Name and Version\r\n\r\n./llama-server --model ~/Project/src/models/llama-3-neural-chat-v1-8b-Q5_K_M.gguf --port 8080  -cb --version\r\nversion: 3166 (21be9cab)\r\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nLinux\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n./llama-server --model ~/Project/src/models/llama-3-neural-chat-v1-8b-Q5_K_M.g\r\nguf --port 8080  -cb \r\nINFO [                    main] build info | tid=\"140605182199744\" timestamp=1718633396 build=3166 commit=\"21be9cab\"\r\nINFO [                    main] system info | tid=\"140605182199744\" timestamp=1718633396 n_threads=4 n_threads_batch=-1 total_threads=8 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \"\r\nllama_model_load: error loading model: llama_model_loader: failed to load model from /home/apar2021/Project/src/models/llama-3-neural-chat-v1-8b-Q5_K_M.gguf\r\n\r\nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params: error: failed to load model '/home/apar2021/Project/src/models/llama-3-neural-chat-v1-8b-Q5_K_M.gguf'\r\n ERR [              load_model] unable to load model | tid=\"140605182199744\" timestamp=1718633396 model=\"/home/apar2021/Project/src/models/llama-3-neural-chat-v1-8b-Q5_K_M.gguf\"\r\nSegmentation fault\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-06-17T14:19:05+00:00",
    "closed_at": "2024-08-01T01:07:02+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7978/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7978"
  },
  {
    "number": 7789,
    "title": "Bug: JSON Schema-to-GBNF additionalProperties bugs (and other minor quirks)",
    "body": "### What happened?\r\n\r\nWhile debugging json-schema-to-gbnf grammars, I noticed a few bugs / quirks and wanted to write them down somewhere.\r\n\r\n# `additionalProperties` seems to default to `false` (not matching spec). \r\n\r\nBy default, additional properties [should be permitted](https://json-schema.org/understanding-json-schema/reference/object#properties). However, providing a schema like:\r\n\r\n```json\r\n{\r\n  \"type\": \"object\",\r\n  \"properties\": {\r\n    \"number\": { \"type\": \"number\" },\r\n    \"street_name\": { \"type\": \"string\" },\r\n    \"street_type\": { \"enum\": [\"Street\", \"Avenue\", \"Boulevard\"] }\r\n  }\r\n}\r\n```\r\nThen it correctly passes on these strings:\r\n```json\r\n{ \"number\": 1600, \"street_name\": \"Pennsylvania\", \"street_type\":\"Avenue\"}\r\n```\r\n```json\r\n{ \"street_name\": \"Pennsylvania\" }\r\n```\r\n```json\r\n{ \"number\": 1600, \"street_name\": \"Pennsylvania\" }\r\n```\r\n```json\r\n{}\r\n```\r\n\r\nBut then it improperly fails on the string:\r\n```json\r\n{ \"number\": 1600, \"street_name\": \"Pennsylvania\", \"street_type\":\"Avenue\", \"direction\":\"NW\"}\r\n```\r\nThis is [clearly given](https://json-schema.org/understanding-json-schema/reference/object#properties) in the json-schema docs as an example of a string that should match this schema, so we're doing something wrong.\r\n\r\n# Explicit `\"additionalProperties\"=true` behavior is even worse.\r\n\r\nIf we change the above grammar to:\r\n```\r\n{\r\n  \"type\": \"object\",\r\n  \"properties\": {\r\n    \"number\": { \"type\": \"number\" },\r\n    \"street_name\": { \"type\": \"string\" },\r\n    \"street_type\": { \"enum\": [\"Street\", \"Avenue\", \"Boulevard\"] }\r\n  },\r\n  \"additionalProperties\": true\r\n}\r\n```\r\nThen things really start to go awry. These strings should all pass (indeed, they passed before when we didn't explicitly set anything for `additionalProperties`, but instead are failing now:\r\n```json\r\n{\"number\":1600,\"street_name\":\"Pennsylvania\",\"street_type\":\"Avenue\"}\r\n```\r\n```json\r\n{ \"street_name\": \"Pennsylvania\" }\r\n```\r\n```json\r\n{ \"number\": 1600, \"street_name\": \"Pennsylvania\" }\r\n```\r\nAnd our sample with an additional property still doesn't match:\r\n```json\r\n{ \"number\": 1600, \"street_name\": \"Pennsylvania\", \"street_type\":\"Avenue\", \"direction\":\"NW\"}\r\n```\r\n\r\nThe only string that matches out of the original is the empty object (`{}`).\r\n\r\nLooking at the generated GBNF, there is some weird stuff going on. Here is the GBNF with additionalProperties set implicitly:\r\n\r\n```ebnf\r\nchar ::= [^\"\\\\] | \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])\r\ndecimal-part ::= [0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9])?)?)?)?)?)?)?)?)?)?)?)?)?)?)?\r\nintegral-part ::= [0-9] | [1-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9])?)?)?)?)?)?)?)?)?)?)?)?)?)?)?\r\nnumber ::= (\"-\"? integral-part) (\".\" decimal-part)? ([eE] [-+]? integral-part)? space\r\nnumber-kv ::= \"\\\"number\\\"\" space \":\" space number\r\nnumber-rest ::= ( \",\" space street-name-kv )? street-name-rest\r\nroot ::= \"{\" space  (number-kv number-rest | street-name-kv street-name-rest | street-type-kv )? \"}\" space\r\nspace ::= \" \"?\r\nstreet-name-kv ::= \"\\\"street_name\\\"\" space \":\" space string\r\nstreet-name-rest ::= ( \",\" space street-type-kv )?\r\nstreet-type ::= \"\\\"Street\\\"\" | \"\\\"Avenue\\\"\" | \"\\\"Boulevard\\\"\"\r\nstreet-type-kv ::= \"\\\"street_type\\\"\" space \":\" space street-type\r\nstring ::= \"\\\"\" char* \"\\\"\" space\r\n```\r\n\r\nAnd here is the GBNF from `additionalProperties` set explicitly to `true`:\r\n```ebnf\r\nadditional-kv ::= string \":\" space additional-value\r\nadditional-kvs ::= additional-kv ( \",\" space additional-kv )*\r\nadditional-value ::= object\r\narray ::= \"[\" space ( value (\",\" space value)* )? \"]\" space\r\nboolean ::= (\"true\" | \"false\") space\r\nchar ::= [^\"\\\\] | \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])\r\ndecimal-part ::= [0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9])?)?)?)?)?)?)?)?)?)?)?)?)?)?)?\r\nintegral-part ::= [0-9] | [1-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9])?)?)?)?)?)?)?)?)?)?)?)?)?)?)?\r\nnull ::= \"null\" space\r\nnumber ::= (\"-\"? integral-part) (\".\" decimal-part)? ([eE] [-+]? integral-part)? space\r\nnumber-kv ::= \"\\\"number\\\"\" space \":\" space number\r\nnumber-rest ::= ( \",\" space street-name-kv )? street-name-rest\r\nobject ::= \"{\" space ( string \":\" space value (\",\" space string \":\" space value)* )? \"}\" space\r\nroot ::= \"{\" space  (number-kv number-rest | street-name-kv street-name-rest | street-type-kv street-type-rest | additional-kvs )? \"}\" space\r\nspace ::= \" \"?\r\nstreet-name-kv ::= \"\\\"street_name\\\"\" space \":\" space string\r\nstreet-name-rest ::= ( \",\" space street-type-kv )? street-type-rest\r\nstreet-type ::= \"\\\"Street\\\"\" | \"\\\"Avenue\\\"\" | \"\\\"Boulevard\\\"\"\r\nstreet-type-kv ::= \"\\\"street_type\\\"\" space \":\" space street-type\r\nstreet-type-rest ::= additional-kvs\r\nstring ::= \"\\\"\" char* \"\\\"\" space\r\nvalue ::= object | array | string | number | boolean | null\r\n```\r\n\r\nThe key differences to note here are how `street-type-rest` is now being defined (even though it was never defined in the original), and `additional-kvs` seems to be getting appended to each property without a comma in between (nor an optional flag).\r\n\r\nI haven't yet wrapped my brain around what all is going on with that, but I wanted to lay out how far I'd gotten on my own.\r\n\r\n# Unlike strings, enums don't support spaces between properties and values.\r\n\r\nThis is definitely in the \"quirk\" more than \"bug\" category, but when using a schema like:\r\n```json\r\n{\r\n  \"type\": \"object\",\r\n  \"properties\": {\r\n    \"number\": { \"type\": \"number\" },\r\n    \"street_name\": { \"type\": \"string\" },\r\n    \"street_type\": { \"enum\": [\"Street\", \"Avenue\", \"Boulevard\"] }\r\n  }\r\n}\r\n```\r\nThen validating against it means that:\r\n```json\r\n{ \"number\": 1600, \"street_type\":\"Avenue\"}\r\n```\r\nis a valid string, but adding spaces around the enum value causes either of the following to fail:\r\n```json\r\n{ \"number\": 1600, \"street_type\": \"Avenue\"}\r\n```\r\n```json\r\n{ \"number\": 1600, \"street_type\":\"Avenue\" }\r\n```\r\n\r\nInterestingly, adding spaces around a string value works fine, and these match the generated grammar just fine:\r\n```json\r\n{ \"number\": 1600, \"street_name\": \"Pennsylvania\" }\r\n```\r\n\r\n\r\n# Unsupported Attributes\r\nWe should probably build a list of unsupported attributes and note them in the documentation -- some that I've noticed thus far: \r\n\r\n* `exclusiveMinimum` (probably can't be handled for anything except for special cases of 0, requiring either the presence of a `-` or not)\r\n* `uniqueItems` -- not sure how we could support this without a regex engine that supports capture groups and lookbehinds and whatnot.\r\n\r\n\r\n\r\n### Name and Version\r\n\r\nversion: 3093 (7672adee)\r\nbuilt with Apple clang version 15.0.0 (clang-1500.3.9.4) for arm64-apple-darwin23.4.0\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nMac\r\n\r\n### Relevant log output\r\n\r\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-06-06T05:26:46+00:00",
    "closed_at": "2024-07-12T21:00:19+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7789/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7789"
  },
  {
    "number": 9071,
    "title": "Bug: ImportError: libprotobuf-lite.so.25: cannot open shared object file: No such file or directory",
    "body": "### What happened?\n\nArch has a newer protobuf that apparently is not compatible with llama.cpp's llava. I tried building an older version of protobuf but was unsuccessful. \n\n### Name and Version\n\n> ./llama-cli --version\r\nversion: 3600 (2fb92678)\r\nbuilt with clang version 17.0.0 for x86_64-pc-linux-gnu\r\n\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n```shell\npython ./examples/llava/convert_image_encoder_to_gguf.py -m /thearray/git/models/dolphin-vision-72b/vit --llava-projector /thearray/git/models/dolphin-vision-72b/vit/llava.projector --output-dir /thearray/git/models/dolphin-vision-72b/vit/ --clip-model-is-vision\r\nTraceback (most recent call last):\r\n  File \"/code/git/llama.cpp/./examples/llava/convert_image_encoder_to_gguf.py\", line 8, in <module>\r\n    from gguf import *\r\n  File \"/code/git/llama.cpp/gguf-py/gguf/__init__.py\", line 7, in <module>\r\n    from .vocab import *\r\n  File \"/code/git/llama.cpp/gguf-py/gguf/vocab.py\", line 10, in <module>\r\n    from sentencepiece import SentencePieceProcessor\r\n  File \"/usr/lib/python3.12/site-packages/sentencepiece/__init__.py\", line 10, in <module>\r\n    from . import _sentencepiece\r\nImportError: libprotobuf-lite.so.25: cannot open shared object file: No such file or directory\n```\n",
    "labels": [
      "bug-unconfirmed",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-08-18T08:36:11+00:00",
    "closed_at": "2024-08-19T13:49:16+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9071/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9071"
  },
  {
    "number": 8023,
    "title": "Bug: Or Feature? BPE Tokenization mutates whitespaces into double-whitespace tokens when add_prefix_space is true (default)",
    "body": "### What happened?\n\nThis is a bit discussed here already: https://github.com/ggerganov/llama.cpp/issues/7938\r\n`<|assistant|> `\r\n```\r\n32001 -> '<|assistant|>'\r\n   259 -> '  '\r\n```\r\n\r\nAlso `<|assistant|>\\n`:\r\n```\r\n32001 -> '<|assistant|>'\r\n29871 -> ' '\r\n    13 -> '\r\n'\r\n```\r\n\r\nWhat happens is that the single whitespace, that follows a special token is mutated into a double-whitespace token (259) because add_prefix_space is triggered in llama.cpp when a special token is encountered.\r\n\r\nIn the second example the template actually wants a \\n after assistant, however the special behavior sneaks a space in between.\r\n\r\nIs this intended behavior / correct ?\r\n\r\nWhen running PHI3 and asking for a generation after `<|assistant|>`, phi3 is adamant in responding with a whitespace or a combination token that starts with a whitespace. \r\nWhen disabling add_prefix_whitespace and adding a `\\n` after assistant, this issue is resolved and phi responds right away with normal text.\r\n\n\n### Name and Version\n\nba58993\n\n### What operating system are you seeing the problem on?\n\nWindows\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-06-20T01:51:25+00:00",
    "closed_at": "2024-09-13T01:07:22+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8023/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8023"
  },
  {
    "number": 8536,
    "title": "Bug: RPC server doesn't load GPU if I use Vulkan ",
    "body": "### What happened?\r\n\r\nI compiled llamacpp with Vulkan backend. The \"rpc-server\" binary is linked to libvulkan but it never uses my GPUs. While \"llama-cli\" is OK.\r\n\r\n### Name and Version\r\n\r\nversion: 3384 (4e24cffd)\r\nbuilt with cc (GCC) 14.1.1 20240701 (Red Hat 14.1.1-7) for x86_64-redhat-linux\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nLinux\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n./rpc-server\r\ncreate_backend: using CPU backend\r\nStarting RPC server on 0.0.0.0:50052, backend memory: 23967 MB\r\n\r\n\r\nldd ./rpc-server\r\n        linux-vdso.so.1 (0x00007f18759f2000)\r\n        libllama.so => /home/metal3d/Projects/ML/llama.cpp/build-rpc/src/libllama.so (0x00007f1875879000)\r\n        libggml.so => /home/metal3d/Projects/ML/llama.cpp/build-rpc/ggml/src/libggml.so (0x00007f1875400000)\r\n        libstdc++.so.6 => /lib64/libstdc++.so.6 (0x00007f1875000000)\r\n        libm.so.6 => /lib64/libm.so.6 (0x00007f187531c000)\r\n        libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007f187582b000)\r\n        libc.so.6 => /lib64/libc.so.6 (0x00007f1874e0f000)\r\n        /lib64/ld-linux-x86-64.so.2 (0x00007f18759f4000)\r\n        libvulkan.so.1 => /lib64/libvulkan.so.1 (0x00007f18757af000)\r\n        libgomp.so.1 => /lib64/libgomp.so.1 (0x00007f18752c6000)\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-07-17T09:08:36+00:00",
    "closed_at": "2024-10-03T10:00:53+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8536/reactions",
      "total_count": 2,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8536"
  },
  {
    "number": 8484,
    "title": "Bug: RuntimeError: Internal: could not parse ModelProto from ../llama3/Meta-Llama-3-8B-Instruct/tokenizer.model",
    "body": "### What happened?\r\n\r\nHow to slove this question\r\nRuntimeError: Internal: could not parse ModelProto from ../llama3/Meta-Llama-3-8B-Instruct/tokenizer.model\r\n\r\n### Name and Version\r\n\r\nllama3 install and quantize the data\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-07-15T04:27:06+00:00",
    "closed_at": "2024-08-29T01:07:04+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8484/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8484"
  },
  {
    "number": 8492,
    "title": "Bug: MESA: error: ../src/intel/vulkan/anv_device.c:4237: VK_ERROR_OUT_OF_DEVICE_MEMORY",
    "body": "### What happened?\r\n\r\nI have an Intel ARC750 graphic card.  The same Phi-3-mini-4k-instruct-fp16.gguf can be run on x86 host with vulkan backend successfully, but it failed on RISC-V host\r\n\r\n### Name and Version\r\n\r\n./llama-cli --version\r\nversion: 3372 (a977c115)\r\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for riscv64-linux-gnu\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nLinux\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nroot@Ubuntu-riscv64:~/liyong/llama.cpp/build/bin# ./llama-cli -m ../../../../Phi-3-mini-4k-instruct-fp16.gguf -p \"Hi you how are you\" -n 50 -e -ngl 33 -t 4\r\nLog start\r\nmain: build = 3372 (a977c115)\r\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for riscv64-linux-gnu\r\nmain: seed  = 1721069901\r\nllama_model_loader: loaded meta data with 23 key-value pairs and 195 tensors from ../../../../Phi-3-mini-4k-instruct-fp16.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = phi3\r\nllama_model_loader: - kv   1:                               general.name str              = Phi3\r\nllama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\r\nllama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\r\nllama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\r\nllama_model_loader: - kv   5:                           phi3.block_count u32              = 32\r\nllama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\r\nllama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\r\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type  f16:  130 tensors\r\nllm_load_vocab: special tokens cache size = 323\r\nllm_load_vocab: token to piece cache size = 0.1690 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = phi3\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32064\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 3072\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_rot            = 96\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 96\r\nllm_load_print_meta: n_embd_head_v    = 96\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 3072\r\nllm_load_print_meta: n_embd_v_gqa     = 3072\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 8192\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 3B\r\nllm_load_print_meta: model ftype      = F16\r\nllm_load_print_meta: model params     = 3.82 B\r\nllm_load_print_meta: model size       = 7.12 GiB (16.00 BPW)\r\nllm_load_print_meta: general.name     = Phi3\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_print_meta: EOT token        = 32007 '<|end|>'\r\nllm_load_print_meta: max token length = 48\r\nMESA: warning: Support for this platform is experimental with Xe KMD, bug reports may be ignored.\r\nggml_vulkan: Found 1 Vulkan devices:\r\nVulkan0: Intel(R) Arc(tm) A750 Graphics (DG2) (Intel open-source Mesa driver) | uma: 0 | fp16: 1 | warp size: 32\r\nllm_load_tensors: ggml ctx size =    0.20 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors: Intel(R) Arc(tm) A750 Graphics (DG2) buffer size =  7100.64 MiB\r\nllm_load_tensors:        CPU buffer size =   187.88 MiB\r\n........................................................................................\r\nllama_new_context_with_model: n_ctx      = 4096\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nMESA: error: ../src/intel/vulkan/anv_device.c:4237: VK_ERROR_OUT_OF_DEVICE_MEMORY\r\nggml_vulkan: Device memory allocation of size 1610612736 failed.\r\nggml_vulkan: vk::Device::allocateMemory: ErrorOutOfDeviceMemory\r\nllama_kv_cache_init: failed to allocate buffer for kv cache\r\nllama_new_context_with_model: llama_kv_cache_init() failed for self-attention cache\r\nllama_init_from_gpt_params: error: failed to create context with model '../../../../Phi-3-mini-4k-instruct-fp16.gguf'\r\nmain: error: unable to load model\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-07-15T12:04:53+00:00",
    "closed_at": "2024-09-05T01:07:06+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8492/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8492"
  },
  {
    "number": 7740,
    "title": "Bug: prefix completion endpoint with /v1",
    "body": "### What happened?\n\nThe server should respect OpenAPI path, but \"completion\" endpoint is not prefixed by \"/v1\"\r\n\r\nThis is required by NeoVim LLM-LS plugin which uses \"openapi\" backend to call \"/v1/completion\".\r\n\r\nThanks\n\n### Name and Version\n\nversion: 2902 (9afdffe7)\r\nbuilt with cc (GCC) 14.0.1 20240411 (Red Hat 14.0.1-0) for x86_64-redhat-linux\r\n\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-06-04T12:53:18+00:00",
    "closed_at": "2024-06-04T13:05:00+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7740/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7740"
  },
  {
    "number": 7987,
    "title": "The image generated by dockerfile cannot be used",
    "body": "### What happened?\n\ndocker build -t local/llama.cpp:llama-server -f .devops/llama-server.Dockerfile .\r\nerror message \r\n./server: /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by ./server)\r\n\r\nThe problem did not occur after adding the following line\r\napt install build-essential\r\n\r\n\n\n### Name and Version\n\n$./llama-server \n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n```shell\nThe problem did not occur after adding the following line\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-06-18T02:54:02+00:00",
    "closed_at": "2024-08-02T01:20:40+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7987/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7987"
  },
  {
    "number": 9545,
    "title": "Bug: Build fails on i386 systems",
    "body": "### What happened?\n\n```\r\n/wrkdirs/usr/ports/misc/llama-cpp/work/llama.cpp-b3761/ggml/src/ggml-vulkan.cpp:2629:5: error: no matching function for call to 'vkCmdCopyBuffer'\r\n 2629 |     vkCmdCopyBuffer(subctx->s->buffer, staging->buffer, dst->buffer, 1, &buf_copy);\r\n      |     ^~~~~~~~~~~~~~~\r\n/usr/local/include/vulkan/vulkan_core.h:4750:28: note: candidate function not viable: no known conversion from 'vk::Buffer' to 'VkBuffer' (aka 'unsigned long long') for 2nd argument\r\n 4750 | VKAPI_ATTR void VKAPI_CALL vkCmdCopyBuffer(\r\n      |                            ^\r\n 4751 |     VkCommandBuffer                             commandBuffer,\r\n 4752 |     VkBuffer                                    srcBuffer,\r\n      |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n```\r\n\r\nVersion: 3761\r\nclang-18\r\nFreeBSD 14.1\n\n### Name and Version\n\n3761\n\n### What operating system are you seeing the problem on?\n\nBSD\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "Vulkan",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-09-19T03:09:15+00:00",
    "closed_at": "2024-11-04T01:07:30+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9545/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9545"
  },
  {
    "number": 10161,
    "title": "Bug: CANN  E89999",
    "body": "### What happened?\n\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\n/owner/ninth/llama.cpp/ggml/src/ggml-cann.cpp:61: CANN error: E89999: Inner Error!\r\nE89999: [PID: 2277481] 2024-11-04-17:38:30.068.533 op[Range], outSize from framework (OFF) is 1, but outSize from tiling (OFT) is 64,which maybe calc OFF by double, but calc OFT by floatplease use float to calc OFF while you wanner input's dtype is float[FUNC:CalculateOutputNum][FILE:range.cc][LINE:113]\r\n        TraceBack (most recent call last):\r\n       op[Range], calculate output_total_num value fail.[FUNC:AppendTilingArgs][FILE:range.cc][LINE:182]\r\n       op[Range], append tiling args fail.[FUNC:Tiling4Range][FILE:range.cc][LINE:255]\r\n       Tiling failed\r\n       Tiling Failed.\r\n       Kernel Run failed. opType: 7, Range\r\n       launch failed for Range, errno:561103.\r\n\r\n  current device: 0, in function aclnn_arange at /owner/ninth/llama.cpp/ggml/src/ggml-cann/aclnn_ops.cpp:291\r\n  aclnnArange(workspaceAddr, workspaceSize, executor, ctx.stream())\r\nCANN error\r\n\n\n### Name and Version\n\n./build/bin/llama-cli --version\r\nversion: 3938 (6f55bccb)\r\nbuilt with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for aarch64-linux-gnu\r\n\r\n\r\ncann :  8.0.0.alpha001\r\n\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n```shell\ncann : 8.0.0.alpha001\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-11-04T09:49:12+00:00",
    "closed_at": "2025-01-27T01:07:16+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10161/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10161"
  },
  {
    "number": 9623,
    "title": "Bug: [Hardware: ppc64le] On ppc64le llama.cpp only uses 1 thread by default and not half of all threads as it does on x86",
    "body": "### What happened?\n\nI'm having an 8 core Power10 system with SMT=2 (=16 threads), but only 1 of the 16 threads is used by default.\r\n\r\nWhen I run a sample prompt like:\r\n\r\n```bash\r\nexport MODELS=gemma-2-2b-it-q4_k_m.gguf\r\n./build/bin/llama-cli -m ${MODELS} -p '10 simple steps to build a website'\r\n```\r\n\r\nit only uses 1/16 threads as you can see in the output. This can be fixed by starting with `-t` parameter but ideally it should take half of the cores as it does on Intel/x86 HW.\r\n\r\n<img width=\"558\" alt=\"image\" src=\"https://github.com/user-attachments/assets/ed3f5cce-e286-4b26-bfa3-7e0c5df877fb\">\r\n\n\n### Name and Version\n\n```bash\r\n# Llama version\r\n$ ./build/bin/llama-cli --version\r\nversion: 3818 (31ac5834)\r\nbuilt with cc (GCC) 13.1.1 20230614 (Red Hat 13.1.1-4) for ppc64le-redhat-linux\r\n\r\n# OS\r\n$ cat /etc/os-release \r\nNAME=\"AlmaLinux\"\r\nVERSION=\"9.3 (Shamrock Pampas Cat)\"\r\nID=\"almalinux\"\r\nID_LIKE=\"rhel centos fedora\"\r\nVERSION_ID=\"9.3\"\r\nPLATFORM_ID=\"platform:el9\"\r\nPRETTY_NAME=\"AlmaLinux 9.3 (Shamrock Pampas Cat)\"\r\nANSI_COLOR=\"0;34\"\r\nLOGO=\"fedora-logo-icon\"\r\nCPE_NAME=\"cpe:/o:almalinux:almalinux:9::baseos\"\r\nHOME_URL=\"https://almalinux.org/\"\r\nDOCUMENTATION_URL=\"https://wiki.almalinux.org/\"\r\nBUG_REPORT_URL=\"https://bugs.almalinux.org/\"\r\n\r\nALMALINUX_MANTISBT_PROJECT=\"AlmaLinux-9\"\r\nALMALINUX_MANTISBT_PROJECT_VERSION=\"9.3\"\r\nREDHAT_SUPPORT_PRODUCT=\"AlmaLinux\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"9.3\"\r\n\r\n# Kernel\r\n$ uname -r\r\n5.14.0-362.18.1.el9_3.ppc64le\r\n```\r\n\r\n\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-09-24T07:32:59+00:00",
    "closed_at": "2024-11-09T01:07:03+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9623/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9623"
  },
  {
    "number": 9893,
    "title": "Bug: Inconsistency while parsing the model using `llama-cli` and `gguf-py`",
    "body": "### What happened?\n\nHi, recently, I'm trying to learn the gguf-py lib and use the gruff-py and write a script to make a gguf file, after I made the file, I tried to load it using llama-cli, but it said I have the wrong tensor number. So I'm wondering if there are some inconsistencies between the cpp loader and the py loader.\r\n\r\nHere, my script is:\r\n``` python\r\nimport os\r\nimport re\r\nimport ast\r\nimport sys\r\nimport random\r\nimport uuid\r\nimport string\r\nimport subprocess\r\nfrom pathlib import Path\r\n\r\nimport numpy as np\r\n\r\n# Necessary to load the local gguf package\r\nsys.path.insert(0, str(Path(__file__).parent.parent))\r\n\r\nfrom gguf import GGUFWriter\r\n\r\nwriter = GGUFWriter('test.gguf', 'llama')\r\n\r\nmodel='llama'\r\ntoken_l=11\r\ncontext_len=123\r\nemb_len=234\r\nbc=1\r\nff_len=345\r\nhc=10\r\nrms_eps=0.1\r\ntokenizer_model='llama'\r\n\r\ntoken_list = random.sample(string.printable, token_l)\r\nwriter.add_token_list(token_list)\r\nwriter.add_context_length(context_len)\r\nwriter.add_embedding_length(emb_len)\r\nwriter.add_block_count(bc)\r\nwriter.add_feed_forward_length(ff_len)\r\nwriter.add_head_count(hc)\r\nwriter.add_layer_norm_rms_eps(rms_eps)\r\nwriter.add_tokenizer_model(tokenizer_model)\r\n\r\n## Here are 18 tensors\r\nwriter.add_tensor('token_embd.weight', np.random.uniform(0, 10, [11, 234]), raw_dtype=0)\r\nwriter.add_tensor('output_norm.weight', np.random.uniform(0, 10, [234]), raw_dtype=0)\r\nwriter.add_tensor('output.weight', np.random.uniform(0, 10, [11, 234]), raw_dtype=0)\r\nwriter.add_tensor('rope_freqs.weight', np.random.uniform(0, 10, [11]), raw_dtype=0)\r\nwriter.add_tensor('blk.0.attn_norm.weight', np.random.uniform(0, 10, [234]), raw_dtype=0)\r\nwriter.add_tensor('blk.0.attn_q.weight', np.random.uniform(0, 10, [230, 234]), raw_dtype=0)\r\nwriter.add_tensor('blk.0.attn_k.weight', np.random.uniform(0, 10, [230, 234]), raw_dtype=0)\r\nwriter.add_tensor('blk.0.attn_v.weight', np.random.uniform(0, 10, [230, 234]), raw_dtype=0)\r\nwriter.add_tensor('blk.0.attn_output.weight', np.random.uniform(0, 10, [234, 230]), raw_dtype=0)\r\nwriter.add_tensor('blk.0.attn_rot_embd.weight', np.random.uniform(0, 10, [123]), raw_dtype=0)\r\nwriter.add_tensor('blk.0.ffn_gate_inp.weight', np.random.uniform(0, 10, [123]), raw_dtype=0)\r\nwriter.add_tensor('blk.0.ffn_norm.weight', np.random.uniform(0, 10, [234]), raw_dtype=0)\r\nwriter.add_tensor('blk.0.ffn_gate.weight', np.random.uniform(0, 10, [345, 234]), raw_dtype=0)\r\nwriter.add_tensor('blk.0.ffn_down.weight', np.random.uniform(0, 10, [234, 345]), raw_dtype=0)\r\nwriter.add_tensor('blk.0.ffn_up.weight', np.random.uniform(0, 10, [345, 234]), raw_dtype=0)\r\nwriter.add_tensor('blk.0.ffn_gate_exps.weight', np.random.uniform(0, 10, [123]), raw_dtype=0)\r\nwriter.add_tensor('blk.0.ffn_down_exps.weight', np.random.uniform(0, 10, [123]), raw_dtype=0)\r\nwriter.add_tensor('blk.0.ffn_up_exps.weight', np.random.uniform(0, 10, [123]), raw_dtype=0)\r\n\r\nwriter.write_header_to_file()\r\nwriter.write_kv_data_to_file()\r\nwriter.write_tensors_to_file()\r\nwriter.close()\r\n\r\n##### verify the inconsistency #####\r\nos.system('../../llama-cli -m ./test.gguf -p \"hello\" -n 5 -e')\r\n\r\nos.system('python3 reader.py ./test.gguf')\r\n```\r\n\r\nthen, you can see the result like:\r\n```\r\nbuild: 3909 (11ac9800) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\r\nmain: llama backend init\r\nmain: load the model and apply lora adapter, if any\r\nllama_model_loader: loaded meta data with 9 key-value pairs and 18 tensors from ./test.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\n\", \"3\", \"...loader: - kv   1:                      tokenizer.ggml.tokens arr[str,11]      = [\"e\", \"!\", \"A\", \"w\", \"f\", \"\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 123\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 234\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 1\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 345\r\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 10\r\nllama_model_loader: - kv   7:     llama.attention.layer_norm_rms_epsilon f32              = 0.100000\r\nllama_model_loader: - kv   8:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - type  f32:   18 tensors\r\nllm_load_vocab: SPM vocabulary, but newline token not found: _Map_base::at! Using special_pad_id instead.llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\r\nllm_load_vocab: special tokens cache size = 0\r\nllm_load_vocab: token to piece cache size = 0.0000 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 11\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 123\r\nllm_load_print_meta: n_embd           = 234\r\nllm_load_print_meta: n_layer          = 1\r\nllm_load_print_meta: n_head           = 10\r\nllm_load_print_meta: n_head_kv        = 10\r\nllm_load_print_meta: n_rot            = 23\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 23\r\nllm_load_print_meta: n_embd_head_v    = 23\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 230\r\nllm_load_print_meta: n_embd_v_gqa     = 230\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-01\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 345\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 123\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model ftype      = all F32 (guessed)\r\nllm_load_print_meta: model params     = 463.95 K\r\nllm_load_print_meta: model size       = 1.77 MiB (32.00 BPW) \r\nllm_load_print_meta: general.name     = n/a\r\nllm_load_print_meta: BOS token        = 1 '!'\r\nllm_load_print_meta: EOS token        = 2 'A'\r\nllm_load_print_meta: UNK token        = 0 'e'\r\nllm_load_print_meta: EOG token        = 2 'A'\r\nllm_load_print_meta: max token length = 1\r\nllm_load_tensors: ggml ctx size =    0.01 MiB\r\nllama_model_load: error loading model: done_getting_tensors: wrong number of tensors; expected 18, got 13\r\nllama_load_model_from_file: failed to load model\r\ncommon_init_from_params: failed to load model './test.gguf'\r\nmain: error: unable to load model\r\nKey-Value Pairs:\r\nGGUF.version                           : [3]\r\nGGUF.tensor_count                      : [18]\r\nGGUF.kv_count                          : [9]\r\ngeneral.architecture                   : [108 108  97 109  97]\r\ntokenizer.ggml.tokens                  : [101]\r\nllama.context_length                   : [123]\r\nllama.embedding_length                 : [234]\r\nllama.block_count                      : [1]\r\nllama.feed_forward_length              : [345]\r\nllama.attention.head_count             : [10]\r\nllama.attention.layer_norm_rms_epsilon : [0.1]\r\ntokenizer.ggml.model                   : [108 108  97 109  97]\r\n----\r\nTensors:\r\nTensor Name                    | Shape: Shape           | Size: Size         | Quantization: Quantization\r\n--------------------------------------------------------------------------------\r\ntoken_embd.weight              | Shape: 234x11          | Size: 2574         | Quantization: F32\r\noutput_norm.weight             | Shape: 234             | Size: 234          | Quantization: F32\r\noutput.weight                  | Shape: 234x11          | Size: 2574         | Quantization: F32\r\nrope_freqs.weight              | Shape: 11              | Size: 11           | Quantization: F32\r\nblk.0.attn_norm.weight         | Shape: 234             | Size: 234          | Quantization: F32\r\nblk.0.attn_q.weight            | Shape: 234x230         | Size: 53820        | Quantization: F32\r\nblk.0.attn_k.weight            | Shape: 234x230         | Size: 53820        | Quantization: F32\r\nblk.0.attn_v.weight            | Shape: 234x230         | Size: 53820        | Quantization: F32\r\nblk.0.attn_output.weight       | Shape: 230x234         | Size: 53820        | Quantization: F32\r\nblk.0.attn_rot_embd.weight     | Shape: 123             | Size: 123          | Quantization: F32\r\nblk.0.ffn_gate_inp.weight      | Shape: 123             | Size: 123          | Quantization: F32\r\nblk.0.ffn_norm.weight          | Shape: 234             | Size: 234          | Quantization: F32\r\nblk.0.ffn_gate.weight          | Shape: 234x345         | Size: 80730        | Quantization: F32\r\nblk.0.ffn_down.weight          | Shape: 345x234         | Size: 80730        | Quantization: F32\r\nblk.0.ffn_up.weight            | Shape: 234x345         | Size: 80730        | Quantization: F32\r\nblk.0.ffn_gate_exps.weight     | Shape: 123             | Size: 123          | Quantization: F32\r\nblk.0.ffn_down_exps.weight     | Shape: 123             | Size: 123          | Quantization: F32\r\nblk.0.ffn_up_exps.weight       | Shape: 123             | Size: 123          | Quantization: F32\r\n```\r\n\r\nAs we can see that the py-reader identified 18 tensors while llama-cli only know that there are 18 tensors but only identified 13 tensors.\r\n\r\nI'm wondering what's going wrong with my script. Could you please help me to figure out? Thanks a lot!\n\n### Name and Version\n\nbuild: 3909 (11ac9800) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-10-15T04:49:25+00:00",
    "closed_at": "2024-11-29T01:09:54+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9893/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9893"
  },
  {
    "number": 9190,
    "title": "Bug: Can't make imatrix quant of Q4_0_X_X",
    "body": "### What happened?\n\nWhen trying to quantize Q4_0_4_4 (and others) with imatrix, I get errors about `GGML_ASSERT(result == nrows * row_size)`\n\n### Name and Version\n\nb3615 ubuntu 22.04\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\n[   1/ 543]                    token_embd.weight - [ 7168, 64000,     1,     1], type =    f16,\r\n| ====== llama_model_quantize_internal: did not find weights for token_embd.weight\r\nconverting to q4_0 .. size =   875.00 MiB ->   246.09 MiB\r\n[   2/ 543]               blk.0.attn_norm.weight - [ 7168,     1,     1,     1], type =    f32, size =    0.027 MB\r\n[   3/ 543]                blk.0.ffn_down.weight - [20480,  7168,     1,     1], type =    f16, converting to q4_0_4x4 .. ggml/src/ggml.c:20598: GGML_ASSERT(result == nrows * row_size) failed\r\nggml/src/ggml.c:20598: GGML_ASSERT(result == nrows * row_size) failed\r\nggml/src/ggml.c:20598: GGML_ASSERT(result == nrows * row_size) failed\r\nggml/src/ggml.c:20598: GGML_ASSERT(result == nrows * row_size) failed\r\nggml/src/ggml.c:20598: GGML_ASSERT(result == nrows * row_size) failed\r\nggml/src/ggml.c:20598: GGML_ASSERT(result == nrows * row_size) failed\r\nggml/src/ggml.c:20598: GGML_ASSERT(result == nrows * row_size) failed\r\nggml/src/ggml.c:20598: GGML_ASSERT(result == nrows * row_size) failed\r\n./llama-quantize(+0x4f73b)[0x5b6f3927773b]\r\nggml/src/ggml.c:20598: GGML_ASSERT(result == nrows * row_size) failed\r\nggml/src/ggml.c:20598: ggml/src/ggml.c:20598: GGML_ASSERT(result == nrows * row_size) failed\r\nGGML_ASSERT(result == nrows * row_size) failed\r\n./llama-quantize(+0x51357)[0x5b6f39279357]\r\n./llama-quantize(+0x8f129)[0x5b6f392b7129]\r\n./llama-quantize(+0xeaa28)[0x5b6f39312a28]\r\n/lib/x86_64-linux-gnu/libstdc++.so.6(+0xdc253)[0x7cdd2e8ad253]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x94ac3)[0x7cdd2e4ebac3]\r\n/lib/x86_64-linux-gnu/libc.so.6(clone+0x44)[0x7cdd2e57ca04]\r\nAborted (core dumped)\n```\n",
    "labels": [
      "bug-unconfirmed",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-08-26T14:40:36+00:00",
    "closed_at": "2024-08-26T17:44:44+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9190/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9190"
  },
  {
    "number": 9881,
    "title": "llama.cpp is slow on GPU",
    "body": "### What happened?\r\n\r\nllama.cpp is running slow on NVIDIA A100 80GB GPU\r\n\r\nSteps to reproduce:\r\n1. git clone https://github.com/ggerganov/llama.cpp && cd llama.cpp\r\n2. mkdir build && cd build\r\n3. cmake .. -DGGML_CUDA=ON\r\n4. make GGML_CUDA=1\r\n3. command:  ./build/bin/llama-cli -m ../gguf_files/llama-3-8B.gguf -t 6912 --ctx-size 50 --n_predict 50 --prompt \"There are two persons named ram and krishna\"\r\n Here threads are set to 6912 since GPU has 6912 CUDA cores.\r\n\r\nIt is slow on gpu compared to cpu.\r\nOn gpu  eval time is around 0.07 tokens per second.\r\nIs this expected behaviour or any tweak should be done while building llama.cpp?\r\n\r\n### Name and Version\r\n\r\nversion: 3902 (c81f3bbb)\r\nbuilt with cc (GCC) 11.4.1 20231218 (Red Hat 11.4.1-3) for aarch64-redhat-linux\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nLinux\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n```\r\n",
    "labels": [
      "Nvidia GPU",
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-10-14T11:19:53+00:00",
    "closed_at": "2024-12-01T01:08:05+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9881/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9881"
  },
  {
    "number": 7976,
    "title": "Bug: Vulkan, I-quants partially working since PR #6210 (very slow, only with all repeating layers offloaded)",
    "body": "### What happened?\r\n\r\nI-quants suddenly started working on Vulkan backend after #6210 was merged, albeit at very slow speeds (token generation is even slowr than when using a single cpu thread). \r\n\r\nBut, it only works if at least all layers exept the last one (every \"repeating layers\") are oflloaded to GPU. Anything else (even `-ngl 0`) and it crashes with `GGML_ASSERT: C:\\[...]\\llama.cpp\\ggml-vulkan.cpp:3006: d_X->size >= x_sz * ne02 * ne03`\r\n\r\n## Example llama-bench outputs: \r\n\r\n### Vulkan (q6-k):\r\nggml_vulkan: Found 1 Vulkan devices:\r\nVulkan0: AMD Radeon RX 5700 XT (AMD proprietary driver) | uma: 0 | fp16: 1 | warp size: 64\r\n| model                          |       size |     params | backend    | ngl | threads | n_batch |          test |              t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | ------: | ------------: | ---------------: |\r\n| llama 1B Q6_K                  | 860.87 MiB |     1.10 B | Vulkan     |  23 |       6 |      32 |         pp512 |    512.52 \u00b1 0.18 |\r\n| llama 1B Q6_K                  | 860.87 MiB |     1.10 B | Vulkan     |  23 |       6 |      32 |         tg512 |    159.35 \u00b1 0.32 |\r\n| llama 1B Q6_K                  | 860.87 MiB |     1.10 B | Vulkan     |  22 |       6 |      32 |         pp512 |    498.63 \u00b1 0.26 |\r\n| llama 1B Q6_K                  | 860.87 MiB |     1.10 B | Vulkan     |  22 |       6 |      32 |         tg512 |    141.69 \u00b1 0.38 |\r\n| llama 1B Q6_K                  | 860.87 MiB |     1.10 B | Vulkan     |  21 |       6 |      32 |         pp512 |    462.52 \u00b1 0.19 |\r\n| llama 1B Q6_K                  | 860.87 MiB |     1.10 B | Vulkan     |  21 |       6 |      32 |         tg512 |    127.42 \u00b1 0.55 |\r\n\r\nbuild: ba68309d (3163)\r\n\r\n### Vulkan (iq4-xs):\r\n\r\nggml_vulkan: Found 1 Vulkan devices:\r\nVulkan0: AMD Radeon RX 5700 XT (AMD proprietary driver) | uma: 0 | fp16: 1 | warp size: 64\r\n| model                          |       size |     params | backend    | ngl | threads | n_batch |          test |              t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | ------: | ------------: | ---------------: |\r\n| llama 1B IQ4_XS - 4.25 bpw     | 577.42 MiB |     1.10 B | Vulkan     |  23 |       6 |      32 |         pp512 |     98.00 \u00b1 0.20 |\r\n| llama 1B IQ4_XS - 4.25 bpw     | 577.42 MiB |     1.10 B | Vulkan     |  23 |       6 |      32 |         tg512 |     12.60 \u00b1 0.03 |\r\n| llama 1B IQ4_XS - 4.25 bpw     | 577.42 MiB |     1.10 B | Vulkan     |  22 |       6 |      32 |         pp512 |     94.57 \u00b1 1.02 |\r\n| llama 1B IQ4_XS - 4.25 bpw     | 577.42 MiB |     1.10 B | Vulkan     |  22 |       6 |      32 |         tg512 |     12.43 \u00b1 0.15 |\r\n\r\nGGML_ASSERT:  C:\\[...]\\llama.cpp\\ggml-vulkan.cpp:3006: d_X->size >= x_sz * ne02 * ne03\r\n\r\n### CPU (iq4-xs):\r\n\r\n| model                          |       size |     params | backend    | threads | n_batch |          test |              t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------: | ------------: | ---------------: |\r\n| llama 1B IQ4_XS - 4.25 bpw     | 577.42 MiB |     1.10 B | CPU        |      12 |      32 |         pp512 |    185.04 \u00b1 4.81 |\r\n| llama 1B IQ4_XS - 4.25 bpw     | 577.42 MiB |     1.10 B | CPU        |      12 |      32 |         tg512 |     57.17 \u00b1 1.08 |\r\n| llama 1B IQ4_XS - 4.25 bpw     | 577.42 MiB |     1.10 B | CPU        |       6 |      32 |         pp512 |    127.78 \u00b1 2.52 |\r\n| llama 1B IQ4_XS - 4.25 bpw     | 577.42 MiB |     1.10 B | CPU        |       6 |      32 |         tg512 |     61.14 \u00b1 1.07 |\r\n| llama 1B IQ4_XS - 4.25 bpw     | 577.42 MiB |     1.10 B | CPU        |       1 |      32 |         pp512 |     24.71 \u00b1 0.05 |\r\n| llama 1B IQ4_XS - 4.25 bpw     | 577.42 MiB |     1.10 B | CPU        |       1 |      32 |         tg512 |     21.14 \u00b1 0.05 |\r\n\r\nbuild: ba68309d (3163)\r\n\r\n## Additional info \r\n\r\nVulkan backend built using:  `cmake .. -DBUILD_SHARED_LIBS=OFF -DLLAMA_VULKAN=1 -G \"Visual Studio 17 2022\" -A x64`\r\n\r\nThe ouput with I-quants doesn't look broken when it's working, it's just way too slow compared to legacy or k-quants. \r\n\r\n(The current build sha doesn't match any commit because of some unrelated local changes on my end that are rebased on top of 21be9cab94e0b5b53cb6edeeebf8c8c799baad03, don't mind it)\r\n\r\n### Name and Version\r\n\r\nversion: 3163 (ba68309d)\r\nbuilt with MSVC 19.39.33523.0 for x64\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nWindows\r\n\r\n### Relevant log output\r\n\r\n```\r\nGGML_ASSERT:  C:\\[...]\\llama.cpp\\ggml-vulkan.cpp:3006: d_X->size >= x_sz * ne02 * ne03\r\n```",
    "labels": [
      "bug-unconfirmed",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-06-17T13:11:03+00:00",
    "closed_at": "2024-06-17T14:51:44+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7976/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7976"
  },
  {
    "number": 9977,
    "title": "Bug: Version infomation missing when using llama-cli built on mac",
    "body": "### What happened?\n\nHi! After failing to run the pre-compiled binaries on my Mac, I tried to compile them myself. \r\nHowever, after finishing the compilation, I found the version information missing when I ran the `llama-cli --version` command. (The compiled binary can infer the \"Hello world\" prompt via phi-3 model normally.)\r\n\n\n### Name and Version\n\n./llama.cpp-b3938/build_metal/bin/llama-cli --version\r\nversion: 0 (unknown)\r\nbuilt with Apple clang version 14.0.0 (clang-1400.0.29.202) for arm64-apple-darwin21.6.0\n\n### What operating system are you seeing the problem on?\n\nMac\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-10-21T09:06:53+00:00",
    "closed_at": "2024-12-06T01:07:37+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9977/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9977"
  },
  {
    "number": 9303,
    "title": "Bug: Error when running a non-exist op for Ascend NPU",
    "body": "### What happened?\n\nIf execute a op that not exist, CANN backend will throw an error that NPU's context pointer is null.\r\nThe reason is that when op is not exist, context will not init, although it will only happed in test, but I think it's need to fix.\r\n\r\nthis command will reproduce this issue:\r\n```\r\n./test-backend-ops test -b CANN0 -o NOT_EXISTS\r\n```\n\n### Name and Version\n\nversion: 3662 (7605ae7d)\r\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n```shell\n~/code/llama.cpp/build/bin$ ./test-backend-ops test -b CANN0 -o NOT_EXISTS\r\nTesting 3 backends\r\n\r\nBackend 1/3 (CPU)\r\n  Skipping\r\nBackend 2/3 (CANN0)\r\n  Backend name: CANN0\r\n  1342/1342 tests passed\r\n  Backend CANN0: OK\r\n\r\nCANN error: EE1001: [PID: 205631] 2024-09-04-01:19:57.687.508 The argument is invalid.Reason: rtDeviceSynchronize execute failed, reason=[context pointer null]\r\n        Solution: 1.Check the input parameter range of the function. 2.Check the function invocation relationship.\r\n        TraceBack (most recent call last):\r\n        ctx is NULL![FUNC:DeviceSynchronize][FILE:api_impl.cc][LINE:2934]\r\n        The argument is invalid.Reason: rtDeviceSynchronize execute failed, reason=[context pointer null]\r\n        wait for compute device to finish failed, runtime result = 107002.[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]\r\n        ctx is NULL![FUNC:GetDevErrMsg][FILE:api_impl.cc][LINE:5244]\r\n        The argument is invalid.Reason: rtGetDevMsg execute failed, reason=[context pointer null]\r\n\r\n  current device: 0, in function ggml_backend_cann_free at /home/hua/code/llama.cpp/ggml/src/ggml-cann.cpp:1404\r\n  aclrtSynchronizeDevice()\r\n/home/hua/code/llama.cpp/ggml/src/ggml-cann.cpp:123: CANN error\r\n[New LWP 205632]\r\n[New LWP 205633]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/aarch64-linux-gnu/libthread_db.so.1\".\r\n0x0000ffff81226800 in __GI___wait4 (pid=<optimized out>, stat_loc=0xffffeb0c2be8, options=0, usage=0x0) at ../sysdeps/unix/sysv/linux/wait4.c:30\r\n30      ../sysdeps/unix/sysv/linux/wait4.c: No such file or directory.\r\n#0  0x0000ffff81226800 in __GI___wait4 (pid=<optimized out>, stat_loc=0xffffeb0c2be8, options=0, usage=0x0) at ../sysdeps/unix/sysv/linux/wait4.c:30\r\n30      in ../sysdeps/unix/sysv/linux/wait4.c\r\n#1  0x0000ffff81660b94 in ggml_abort () from /home/hua/code/llama.cpp/build/ggml/src/libggml.so\r\n#2  0x0000ffff816dce5c in ggml_cann_error(char const*, char const*, char const*, int, char const*) () from /home/hua/code/llama.cpp/build/ggml/src/libggml.so\r\n#3  0x0000ffff816de718 in ggml_backend_cann_free(ggml_backend*) () from /home/hua/code/llama.cpp/build/ggml/src/libggml.so\r\n#4  0x0000aaaac0dc6008 in main ()\r\n[Inferior 1 (process 205631) detached]\r\nAborted (core dumped)\r\n```\n```\n",
    "labels": [
      "low severity",
      "Ascend NPU"
    ],
    "state": "closed",
    "created_at": "2024-09-04T01:32:43+00:00",
    "closed_at": "2024-09-12T01:02:36+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9303/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9303"
  },
  {
    "number": 7657,
    "title": "Refactor: Add CONTRIBUTING.md and/or update PR template with [no ci] tips",
    "body": "### Background Description\r\n\r\nDiscussion in https://github.com/ggerganov/llama.cpp/pull/7650 pointed out a need to add a CONTRIBUTING.md and maybe add a PR template to encourage contributors to add [no ci] tag to documentation only changes.\r\n\r\nhttps://docs.github.com/en/actions/managing-workflow-runs/skipping-workflow-runs\r\n\r\n(If anyone wants to tackle this, feel free to)\r\n\r\n### Possible Refactor Approaches\r\n\r\nAdd info about\r\n\r\n- doc only changes should have [no ci] in commit title to skip the unneeded CI checks.\r\n- squash on merge with commit title format: \"module : some commit title (`#1234`)\"",
    "labels": [
      "documentation",
      "enhancement",
      "help wanted",
      "devops",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-05-30T23:56:20+00:00",
    "closed_at": "2024-06-09T15:25:57+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7657/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7657"
  },
  {
    "number": 10116,
    "title": "Bug: Unable to enable AVX_VNNI instructions",
    "body": "### What happened?\n\nI have built it on Windows env with OneAPI setup and try to enable AVX_VNNI instructions. My computer is support AVX_VNNI:\r\n![image](https://github.com/user-attachments/assets/7c513c5f-1353-4468-81b7-5dcbd8d209fd)\r\nand I followed the [doc's steps](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md#intel-onemkl) :\r\n\r\n```shell\r\n# in OneAPI env, execute:\r\ncmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=Intel10_64lp -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_NATIVE=ON\r\ncmake --build build --config Release\r\n```\r\n\r\nbut it still shows no AVX_VNNI support:\r\n![image](https://github.com/user-attachments/assets/974f6b65-a80b-47a0-a059-0b076c19c768)\r\n\r\n#### my platform\r\n+ msvc: MSVC 19.41.34123.0( Visual Studio 2022) on Windows SDK 10.0.22631.\r\n+ oneAPI:  oneAPI 2024.2\r\n\n\n### Name and Version\n\n.\\llama-cli.exe --version\r\nversion: 3983 (8841ce3f)\r\nbuilt with MSVC 19.41.34123.0 for x64\n\n### What operating system are you seeing the problem on?\n\nWindows\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-11-01T02:14:40+00:00",
    "closed_at": "2024-11-05T01:24:49+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10116/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10116"
  },
  {
    "number": 10155,
    "title": "Bug: --log-disable also disables output from the model",
    "body": "### What happened?\r\n\r\nWhen using any client command (i.e llama-cli llama-llava-cli) with the <code>--log-disable</code> or the equivalent <code>-lv -1</code> option, the model's response to the prompt is not printed.\r\n\r\nThis seems to be unexpected behavior, as the model's response is not considered a log - nor is it output via stderr in any mode. It should be noted that this also occurs in conversation-mode, even the prompt-character (>) is not printed. Furthermore, in normal-mode, the process eventually reports itself as `Killed`.\r\n\r\nThe expected behavior, is that the model's response to the prompt (or conversation) is printed <strong>but the debug information (i.e llama_model_loader: - kv ...) is not</strong>.\r\n\r\n### Name and Version\r\n\r\nversion: 4016 (42cadc74)\r\nbuilt with clang version 18.1.8+libcxx for x86_64-pc-linux-musl\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nLinux\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n% llama-cli --log-disable -m ./models/8B/favorite-8B.gguf -p \"Give an answer.\"\r\nKilled\r\n%\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-11-04T03:06:11+00:00",
    "closed_at": "2024-12-11T02:04:50+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10155/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10155"
  },
  {
    "number": 7923,
    "title": "Bug: convert-hf-to-gguf.py on Gemma model ValueError: Duplicated key name 'tokenizer.chat_template'",
    "body": "### What happened?\n\nWhen trying to convert\r\n\r\nhttps://huggingface.co/SakanaAI/DiscoPOP-zephyr-7b-gemma/\r\n\r\nI get the error in the title, but it's only defined a single time in tokenizer_config.json:\r\n\r\nhttps://huggingface.co/SakanaAI/DiscoPOP-zephyr-7b-gemma/blob/main/tokenizer_config.json#L59\r\n\r\nVerified locally with `cat *.json | grep chat_template` and I only get the one result\r\n\r\nIs it somehow trying to load it twice?\r\n\r\nLooks like when Gemma is initialized, it runs _set_vocab_sentencepiece(), which runs special_vocab.add_to_gguf (which pulls in the chat_template), and then it also again runs special_vocab.add_to_gguf\r\n\r\nbut that would mean it's been broken since April 16..\r\n\r\nhttps://github.com/ggerganov/llama.cpp/pull/6689\n\n### Name and Version\n\nb3145 ubuntu 22.04\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\nINFO:hf-to-gguf:Loading model: DiscoPOP-zephyr-7b-gemma\r\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\r\nINFO:hf-to-gguf:Set model parameters\r\nINFO:hf-to-gguf:Set model tokenizer\r\nINFO:gguf.vocab:Setting special token type bos to 2\r\nINFO:gguf.vocab:Setting special token type eos to 1\r\nINFO:gguf.vocab:Setting special token type unk to 3\r\nINFO:gguf.vocab:Setting special token type pad to 0\r\nINFO:gguf.vocab:Setting add_bos_token to False\r\nINFO:gguf.vocab:Setting add_eos_token to False\r\nINFO:gguf.vocab:Setting chat_template to {% if messages[0]['role'] == 'user' or messages[0]['role'] == 'system' %}{{ bos_token }}{% endif %}{% for message in messages %}{{ '<|im_start|>' + message['role'] + '\r\n' + message['content'] + '<|im_end|>' + '\r\n' }}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\r\n' }}{% elif messages[-1]['role'] == 'assistant' %}{{ eos_token }}{% endif %}\r\nINFO:gguf.vocab:Setting special token type prefix to 67\r\nINFO:gguf.vocab:Setting special token type suffix to 69\r\nINFO:gguf.vocab:Setting special token type middle to 68\r\nWARNING:gguf.vocab:No handler for special token type fsep with id 70 - skipping\r\nINFO:gguf.vocab:Setting special token type eot to 107\r\nINFO:gguf.vocab:Setting chat_template to {% if messages[0]['role'] == 'user' or messages[0]['role'] == 'system' %}{{ bos_token }}{% endif %}{% for message in messages %}{{ '<|im_start|>' + message['role'] + '\r\n' + message['content'] + '<|im_end|>' + '\r\n' }}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\r\n' }}{% elif messages[-1]['role'] == 'assistant' %}{{ eos_token }}{% endif %}\r\nTraceback (most recent call last):\r\n  File \"/llama.cpp/convert-hf-to-gguf.py\", line 2882, in <module>\r\n    main()\r\n  File \"/llama.cpp/convert-hf-to-gguf.py\", line 2867, in main\r\n    model_instance.set_vocab()\r\n  File \"/llama.cpp/convert-hf-to-gguf.py\", line 2251, in set_vocab\r\n    special_vocab.add_to_gguf(self.gguf_writer)\r\n  File \"/llama.cpp/gguf-py/gguf/vocab.py\", line 73, in add_to_gguf\r\n    gw.add_chat_template(self.chat_template)\r\n  File \"/llama.cpp/gguf-py/gguf/gguf_writer.py\", line 565, in add_chat_template\r\n    self.add_string(Keys.Tokenizer.CHAT_TEMPLATE, value)\r\n  File \"/llama.cpp/gguf-py/gguf/gguf_writer.py\", line 206, in add_string\r\n    self.add_key_value(key, val, GGUFValueType.STRING)\r\n  File \"/llama.cpp/gguf-py/gguf/gguf_writer.py\", line 166, in add_key_value\r\n    raise ValueError(f'Duplicated key name {key!r}')\r\nValueError: Duplicated key name 'tokenizer.chat_template'\n```\n",
    "labels": [
      "bug",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-06-13T19:09:23+00:00",
    "closed_at": "2024-07-21T01:53:03+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7923/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7923"
  },
  {
    "number": 10122,
    "title": "Bug: llama-quantize --help is not printed",
    "body": "### What happened?\n\nAppended `--help` does not print help immediately, but starts quantization or throws error:\r\n```shell\r\n./llama-quantize model-bf16.gguf --help IQ4_NL\r\n./llama-quantize model-bf16.gguf IQ4_NL --help\r\n```\n\n### Name and Version\n\nc02e5ab2a675c8bc1abc8b1e4cb6a93b26bdcce7\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\n./build/bin/llama-quantize --leave-output-tensor ./qwen2.5-coder-7b-instruct/qwen2.5-coder-7B-instruct-BF16.gguf --help IQ\r\n4_NL                                                                                                                                                          \r\nmain: build = 4000 (c02e5ab2)                                                                                                                                 \r\nmain: built with cc (Ubuntu 13.2.0-23ubuntu4) 13.2.0 for x86_64-linux-gnu                                                                                     \r\nmain: quantizing './qwen2.5-coder-7b-instruct/qwen2.5-coder-7B-instruct-BF16.gguf' to '--help' as IQ4_NL                                                      \r\n\r\n---\r\n\r\n./build/bin/llama-quantize --leave-output-tensor ./qwen2.5-coder-7b-instruct/qwen2.5-coder-7B-instruct-BF16.gguf 1 --help \r\nmain: invalid nthread '--help' (stoi)\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-11-01T12:29:32+00:00",
    "closed_at": "2024-12-22T01:07:42+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10122/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10122"
  },
  {
    "number": 10190,
    "title": "Bug: Name Error when running Llava1.5 examples",
    "body": "### What happened?\n\nwhen running examples Llava1.5, \r\n\r\npython ./examples/llava/convert_image_encoder_to_gguf.py -m ../clip-vit-large-patch14-336 --llava-projector ../llava-v1.5-7b/llava.projector --output-dir ../llava-v1.5-7b\r\n\r\nthe process raise an error:\r\nNameError: name 'KEY_EMBEDDING_LENGTH' is not defined\r\n\r\nwhich happens in convert_image_encoder_to_gguf.py line 204:\r\nfout.add_uint32(k(KEY_EMBEDDING_LENGTH, VISION), v_hparams[\"hidden_size\"])\n\n### Name and Version\n\nversion: 4020 (9f409893)\r\nbuilt with Apple clang version 13.0.0 (clang-1300.0.29.30) for x86_64-apple-darwin23.6.0\n\n### What operating system are you seeing the problem on?\n\nMac\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-11-06T03:48:13+00:00",
    "closed_at": "2024-11-08T11:37:01+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10190/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10190"
  },
  {
    "number": 8431,
    "title": "Getting bash: ./perplexity: No such file or directory ",
    "body": "### What happened?\n\nGetting the bash: ./perplexity: No such file or directory error even after the Build has happened correctly \r\nIs the tool renamed?\n\n### Name and Version\n\nLasted version\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-07-11T07:44:20+00:00",
    "closed_at": "2024-08-10T11:57:41+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8431/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8431"
  },
  {
    "number": 10225,
    "title": "Bug: image encoding error with malloc memory",
    "body": "### What happened?\n\nWhen compiling the Minicpm model, during the process of image encoding, memory allocation operations are performed for the variables.\r\nThe first subgraph execution runs fine, but during the second malloc, it reports a core dump error. I have not been able to identify the cause. Please help me.\r\n\r\n\r\n\r\ncode file: llava.cpp\r\ncode:  image_embd_v[i] = (float *)malloc(clip_embd_nbytes(ctx_clip));  ##clip_embd_nbytes(ctx_clip):917504\r\n\r\n\r\n\r\n\r\nif (clip_is_minicpmv(ctx_clip)) {\r\n        std::vector<float *> image_embd_v;\r\n        image_embd_v.resize(img_res_v.size);\r\n        struct clip_image_size * load_image_size = clip_image_size_init();\r\n        for (size_t i = 0; i < img_res_v.size; i++) {\r\n            const int64_t t_img_enc_step_start_us = ggml_time_us();\r\n            image_embd_v[i] = (float *)malloc(clip_embd_nbytes(ctx_clip));\r\n            int patch_size=14;\r\n            load_image_size->width = img_res_v.data[i].nx;\r\n            load_image_size->height = img_res_v.data[i].ny;\r\n            clip_add_load_image_size(ctx_clip, load_image_size);\r\n            bool encoded = false;\n\n### Name and Version\n\n\r\n./llama-llava-cli latest linux\n",
    "labels": [
      "bug-unconfirmed",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-11-09T02:01:05+00:00",
    "closed_at": "2024-11-15T02:11:32+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10225/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10225"
  },
  {
    "number": 8437,
    "title": "Bug: Vulkan backend not work on an Imagination GPU on RISC-V Platform",
    "body": "### What happened?\r\n\r\nNothing output after \"Vulkan0: PowerVR B-Series BXE-2-32 (PowerVR B-Series Vulkan Driver) | uma: 1 | fp16: 1 | warp size: 1\"\r\nIt is on a RISC-V board with an imagination igpu \r\n\r\n### Name and Version\r\n\r\n./llama-cli --version version: 3369 (278d0e18) built with cc (Ubuntu 13.2.0-4ubuntu3-bb2) 13.2.0 for riscv64-linux-gnu\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nLinux\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nroot@k1:~/liyong/llama.cpp/build/bin# ./llama-cli -m ../../../Phi-3-mini-4k-instruct-fp16.gguf -p \"Hi you how are you\" -n 50 -e -ngl 33 -t 4\r\nLog start\r\nmain: build = 3369 (278d0e18)\r\nmain: built with cc (Ubuntu 13.2.0-4ubuntu3-bb2) 13.2.0 for riscv64-linux-gnu\r\nmain: seed  = 1720706611\r\nllama_model_loader: loaded meta data with 23 key-value pairs and 195 tensors from ../../../Phi-3-mini-4k-instruct-fp16.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = phi3\r\nllama_model_loader: - kv   1:                               general.name str              = Phi3\r\nllama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\r\nllama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\r\nllama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\r\nllama_model_loader: - kv   5:                           phi3.block_count u32              = 32\r\nllama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\r\nllama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\r\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type  f16:  130 tensors\r\nllm_load_vocab: special tokens cache size = 323\r\nllm_load_vocab: token to piece cache size = 0.1690 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = phi3\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32064\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 3072\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_rot            = 96\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 96\r\nllm_load_print_meta: n_embd_head_v    = 96\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 3072\r\nllm_load_print_meta: n_embd_v_gqa     = 3072\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 8192\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 3B\r\nllm_load_print_meta: model ftype      = F16\r\nllm_load_print_meta: model params     = 3.82 B\r\nllm_load_print_meta: model size       = 7.12 GiB (16.00 BPW)\r\nllm_load_print_meta: general.name     = Phi3\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_print_meta: EOT token        = 32007 '<|end|>'\r\nllm_load_print_meta: max token length = 48\r\nggml_vulkan: Found 1 Vulkan devices:\r\nVulkan0: PowerVR B-Series BXE-2-32 (PowerVR B-Series Vulkan Driver) | uma: 1 | fp16: 1 | warp size: 1\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-07-11T11:01:33+00:00",
    "closed_at": "2024-08-26T01:06:58+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8437/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8437"
  }
]
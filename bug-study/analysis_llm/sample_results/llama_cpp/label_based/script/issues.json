[
  {
    "number": 7658,
    "title": "Why is convert.py missing?",
    "body": "### What happened?\n\nCritical \"non llama3\" convert.py and change NOT in download of files.\r\n\r\nALSO:\r\nIt is unclear if \"convert-hf-to-gguf.py\"  supports what \"convert.py\" did . \r\n\r\nDoes it convert llama, llama2, mistral or is \"convert-legacy-llama.py\" required?\r\nSafetensor files are EVERYWHERE. (?)  [ RE: https://github.com/ggerganov/llama.cpp/pull/7430 ]\r\n\r\nThis critical action DID NOT OCCUR:\r\n\"Move convert.py to examples/convert-legacy-llama.py\"\r\n\r\n\"examples/convert-legacy-llama.py\" does not exist. \r\n(when downloading the zip files).\r\n\r\nOn another note why remove \"convert.py\" at all? \r\n\r\n-This breaks \"bat files\" and automation generation.\r\n-This will break all colabs too.\r\n-This will break any HF spaces that create GGUF files as well.\r\n-This will create needless confusion.\r\n\r\nIf \"convert-hf-to-gguf.py\" (llama3) does everything convert.py did , just keep it as \"convert.py\" ?\r\n\r\n\r\n\r\n\n\n### Name and Version\n\nthis is not applicable - core files missing.\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n```shell\nthis is not applicable - core files missing.\n```\n",
    "labels": [
      "documentation",
      "script",
      "python",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-05-31T05:46:36+00:00",
    "closed_at": "2024-06-10T19:58:23+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7658/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7658"
  },
  {
    "number": 704,
    "title": "Update *-to-ggml.py scripts for new ggjt model format",
    "body": "See title, basically.\r\n\r\nWe should probably keep the option of generating the old formats.\r\n\r\nRevert #690 when done.\r\n\r\nRelated: #545",
    "labels": [
      "script"
    ],
    "state": "closed",
    "created_at": "2023-04-02T09:49:22+00:00",
    "closed_at": "2023-05-03T18:37:53+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/704/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/704"
  },
  {
    "number": 701,
    "title": "How to convert old ALPACA q4_0 model into ggjt format?",
    "body": "I'm trying to use a python script, but it returns the following error:\r\n\r\nd:\\ALPACA2>python migrate-ggml-2023-03-30-pr613.py ggml-alpaca-7b-q4.bin ggml-alpaca-7b-q4-ggjt.bin\r\nTraceback (most recent call last):\r\n  File \"d:\\ALPACA2\\migrate-ggml-2023-03-30-pr613.py\", line 313, in <module>\r\n    main()\r\n  File \"d:\\ALPACA2\\migrate-ggml-2023-03-30-pr613.py\", line 274, in main\r\n    tokens = read_tokens(fin, hparams)\r\n  File \"d:\\ALPACA2\\migrate-ggml-2023-03-30-pr613.py\", line 135, in read_tokens\r\n    word = fin.read(length)\r\nValueError: read length must be non-negative or -1\r\n",
    "labels": [
      "duplicate",
      "script"
    ],
    "state": "closed",
    "created_at": "2023-04-02T08:29:38+00:00",
    "closed_at": "2023-04-04T19:32:30+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/701/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/701"
  }
]
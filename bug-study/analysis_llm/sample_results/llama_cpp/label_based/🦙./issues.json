[
  {
    "number": 2262,
    "title": "Add llama 2 model",
    "body": "Meta just released llama 2 model, allowing commercial usage\r\n\r\nhttps://ai.meta.com/resources/models-and-libraries/llama/\r\n\r\nI have checked the model implementation and it seems different from llama_v1, maybe need a re-implementation",
    "labels": [
      "\ud83e\udd99.",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-07-18T16:35:53+00:00",
    "closed_at": "2023-10-18T07:31:45+00:00",
    "comments": 95,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2262/reactions",
      "total_count": 141,
      "+1": 89,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 52,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2262"
  },
  {
    "number": 2164,
    "title": "mpi : attempt inference of 65B LLaMA on a cluster of Raspberry Pis",
    "body": "Now that distributed inference is supported thanks to the work of @evanmiller in #2099 it would be fun to try to utilize it for something cool. One such idea is to connect a bunch of [Raspberry Pis](https://www.raspberrypi.com/products/raspberry-pi-4-model-b/) in a local network and run the inference using MPI:\r\n\r\n```bash\r\n# sample cluster of 8 devices (replace with actual IP addresses of the devices)\r\n$ cat ./hostfile\r\n192.168.0.1:1\r\n192.168.0.2:1\r\n192.168.0.3:1\r\n192.168.0.4:1\r\n192.168.0.5:1\r\n192.168.0.6:1\r\n192.168.0.7:1\r\n192.168.0.8:1\r\n\r\n# build with MPI support\r\n$ make CC=mpicc CXX=mpicxx LLAMA_MPI=1 -j\r\n\r\n# run distributed inference over 8 nodes\r\n$ mpirun -hostfile ./hostfile -n 8 ./main -m /mnt/models/65B/ggml-model-q4_0.bin -p \"I believe the meaning of life is\" -n 64\r\n```\r\n\r\nHere we assume that the 65B model data is located on a network share in `/mnt` and that `mmap` works over a network share.\r\nNot sure if that is the case - if not, then it would be more difficult to perform this experiment.\r\n\r\nLooking for people with access to the necessary hardware to perform this experiment",
    "labels": [
      "help wanted",
      "\ud83e\udd99.",
      "hardware",
      "research \ud83d\udd2c"
    ],
    "state": "open",
    "created_at": "2023-07-10T16:12:22+00:00",
    "closed_at": null,
    "comments": 54,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2164/reactions",
      "total_count": 24,
      "+1": 14,
      "-1": 0,
      "laugh": 0,
      "hooray": 3,
      "confused": 0,
      "heart": 7,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2164"
  },
  {
    "number": 1291,
    "title": "Try whether OpenLLaMa works",
    "body": "... or whether we need to tweak some settings\r\n\r\nGitHub: https://github.com/openlm-research/open_llama\r\n\r\nHuggingFace: https://huggingface.co/openlm-research/open_llama_7b_preview_300bt\r\n\r\n---\r\n\r\nedit: GGML models uploaded to HH by @vihangd => https://huggingface.co/vihangd/open_llama_7b_300bt_ggml",
    "labels": [
      "\ud83e\udd99.",
      "model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-05-02T21:53:20+00:00",
    "closed_at": "2024-04-09T01:09:41+00:00",
    "comments": 82,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1291/reactions",
      "total_count": 8,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1291"
  },
  {
    "number": 1163,
    "title": "llama.cpp + Final Jeopardy",
    "body": "I was browsing reddit and saw this post:\r\n\r\nhttps://www.reddit.com/r/LocalLLaMA/comments/12xkm9v/alpaca_vs_final_jeopardy/\r\n\r\nIf anyone is interested, it would be great to add such evaluation as an example to `llama.cpp` and add instructions for running it with different models: LLaMA, Alpaca, Vicuna, etc. and different quantizations.\r\n\r\nHere is the original work by @aigoopy which can be a good starting point:\r\n\r\nhttps://github.com/aigoopy/llm-jeopardy\r\n\r\n",
    "labels": [
      "help wanted",
      "good first issue",
      "\ud83e\udd99."
    ],
    "state": "closed",
    "created_at": "2023-04-24T19:44:16+00:00",
    "closed_at": "2023-04-28T16:13:35+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1163/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1163"
  },
  {
    "number": 596,
    "title": "Is this true? :joy: ",
    "body": "I asked ChatGPT about the difference between `llama.cpp` and `whisper.cpp` and it says:\r\n\r\n![image](https://user-images.githubusercontent.com/3450257/228553783-4cf28da9-f025-4a7c-92a6-2c8c9c604c28.png)\r\n",
    "labels": [
      "\ud83e\udd99."
    ],
    "state": "closed",
    "created_at": "2023-03-29T13:30:18+00:00",
    "closed_at": "2023-04-06T15:20:27+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/596/reactions",
      "total_count": 18,
      "+1": 4,
      "-1": 0,
      "laugh": 11,
      "hooray": 0,
      "confused": 3,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/596"
  },
  {
    "number": 536,
    "title": "Logo in Social Preview",
    "body": "Not a bug, but a useful thing. Put the logo in the Social Preview like in this project:\r\n<img width=\"843\" alt=\"Screenshot 2023-03-26 at 20 01 58\" src=\"https://user-images.githubusercontent.com/163333/227795065-61d531a9-e515-44bf-b570-086ea8aa7bf2.png\">\r\n\r\n It will be then showed as a preview image on Twitter etc.\r\n<img width=\"591\" alt=\"Screenshot 2023-03-26 at 20 02 52\" src=\"https://user-images.githubusercontent.com/163333/227795109-d2f84554-1f08-4d82-8b3f-11be2d4de1ef.png\">\r\n\r\n\r\n",
    "labels": [
      "enhancement",
      "\ud83e\udd99."
    ],
    "state": "closed",
    "created_at": "2023-03-26T18:03:49+00:00",
    "closed_at": "2023-03-28T18:34:37+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/536/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/536"
  },
  {
    "number": 508,
    "title": "Create \"instruct\" example",
    "body": "Currently, the `main` example has a `instruct` parameter which enables something similar to instruction-based mode. I haven't understood it completely, but this seems to be what the Alpaca models are created for.\r\n\r\nSince we now support infinite generation (https://github.com/ggerganov/llama.cpp/issues/71#issuecomment-1483907574) it would be very useful to make a separate app that utilizes the new `--keep` argument to create a question-answering bot that never stops. The tricky part is to keep the correct instruction prompt and \"inject\" the few-shot examples correctly, or whatever.\r\n\r\nThe main logic for context swapping / context rotation is here:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/blob/c2b25b6912662d2637d9c6e6df3a5de931e0d7ce/examples/main/main.cpp#L297-L324\r\n\r\nUncomment the `printf` to help debug. Something similar will be needed in the new `instruct` example.\r\n\r\nImplementing this task will also help simplify the `main` example as it will no longer need to support the `--instruct` argument.",
    "labels": [
      "enhancement",
      "help wanted",
      "good first issue",
      "\ud83e\udd99."
    ],
    "state": "closed",
    "created_at": "2023-03-25T20:22:39+00:00",
    "closed_at": "2023-07-28T19:21:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/508/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 2,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/508"
  },
  {
    "number": 399,
    "title": "Support for Loading a Subset of Tensors for LoRA Models ",
    "body": "Firstly, thank you for the awesome project. I'm new to LLMs so I hope this suggestion makes sense.\r\n\r\nLoRA is a technique used to reduce the number of parameters during finetuning, that is really hitting off with the recent Alpaca stuff. In LoRA models, typically, only the weight matrices Wq and Wv are fine-tuned. \r\n\r\nFor projects shipping multiple LoRA fine-tuned models, most of the tensors remain unchanged during the fine-tuning process. Storing all weights multiple times would lead to a significant waste of storage space (e.g., ~3.5 GB of data per fine-tune for a 7B model, multiplied by the number of tasks or personalities you want to ship). Supporting the loading of a subset of tensors for LoRA models would enable efficient storage and loading of these models in llama.cpp, reducing storage space requirements, and maybe memory footprint if you wanted to keep multiple models in memory at the same time.\r\n\r\nI propose to extend llama.cpp's functionality by adding support for loading a subset of tensors from separate .bin files. This way all the business of merging the LoRA weights would still be done in python. And also the model subset .bin files could be quantized like usual.\r\n\r\nAn alternative could be to natively support LoRA in llama.cpp. However, this approach would likely not be compatible with pre-quantization of the weights (afaict).\r\n",
    "labels": [
      "enhancement",
      "\ud83e\udd99.",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-22T16:12:51+00:00",
    "closed_at": "2023-04-17T15:28:57+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/399/reactions",
      "total_count": 20,
      "+1": 20,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/399"
  },
  {
    "number": 382,
    "title": "Add proper instructions for using Alpaca models",
    "body": "So I am looking at https://github.com/antimatter15/alpaca.cpp and I see they are already running 30B Alpaca models, while we are struggling to run 7B due to the recent tokenizer updates.\r\n\r\nI also see that the models are now even floating on Hugging Face - I guess license issues are no longer a problem?\r\n\r\nWe should add detailed instructions for obtaining the Alpaca models and a temporary explanation how to use the following script to make the models compatible with the latest `master`:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/issues/324#issuecomment-1476227818\r\n\r\nThe bigger issue is that people keep producing the old version of the `ggml` models instead of migrating to the latest `llama.cpp` changes. And therefore, we now need this extra conversion step. It's best to figure out the steps for generating the Alpaca models and generate them in the correct format.\r\n\r\n**Edit: just don't post direct links to the models!**",
    "labels": [
      "documentation",
      "help wanted",
      "good first issue",
      "high priority",
      "\ud83e\udd99."
    ],
    "state": "closed",
    "created_at": "2023-03-22T07:26:07+00:00",
    "closed_at": "2023-07-28T19:20:56+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/382/reactions",
      "total_count": 21,
      "+1": 21,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/382"
  },
  {
    "number": 313,
    "title": "Add OpenBSD support",
    "body": "This patch adds OpenBSD support, thanks.\r\n[patch-llama.cpp.txt](https://github.com/ggerganov/llama.cpp/files/11013172/patch-llama.cpp.txt)\r\n",
    "labels": [
      "enhancement",
      "\ud83e\udd99.",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-20T02:25:38+00:00",
    "closed_at": "2023-03-21T15:50:12+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/313/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/313"
  },
  {
    "number": 188,
    "title": "How to? (install models)",
    "body": "Hi, i can't find the models\r\nCan u tell me, how i can install?\r\n(ls ./models 65B etc is not working)\r\n*sorry, my english isn't good) ",
    "labels": [
      "question",
      "\ud83e\udd99."
    ],
    "state": "closed",
    "created_at": "2023-03-15T22:51:14+00:00",
    "closed_at": "2023-03-16T11:31:34+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/188/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/188"
  },
  {
    "number": 105,
    "title": "Create a logo",
    "body": "We should probably make a logo for this project. Like an image of a \ud83e\udd99 and some C++",
    "labels": [
      "good first issue",
      "\ud83e\udd99."
    ],
    "state": "closed",
    "created_at": "2023-03-13T21:15:21+00:00",
    "closed_at": "2023-07-28T19:20:49+00:00",
    "comments": 47,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/105/reactions",
      "total_count": 9,
      "+1": 9,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/105"
  },
  {
    "number": 64,
    "title": "Store KV cache of computed prompts to disk to avoid re-compute in follow-up runs",
    "body": "Idea from: https://github.com/ggerganov/llama.cpp/issues/23#issuecomment-1465308592\r\n\r\nWe can add a `--cache_prompt` flag that if added will dump the computed KV caches of the prompt processing to the disk in a file with name produced by the hash of the prompt. Next time you run, it will first check if we have stored KV cache for this hash and load it straight from disk instead of computing it.\r\n\r\nGreat task for contributing to the project!",
    "labels": [
      "enhancement",
      "help wanted",
      "good first issue",
      "high priority",
      "\ud83e\udd99."
    ],
    "state": "closed",
    "created_at": "2023-03-12T21:55:25+00:00",
    "closed_at": "2023-04-29T02:57:37+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/64/reactions",
      "total_count": 29,
      "+1": 29,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/64"
  }
]
[
  {
    "number": 317,
    "title": "segmentation fault Alpaca",
    "body": "Hello, \r\nI've tried out the Aplaca model but after a while there comes an error I believe stating: \"zsh: segmentation fault  ./main -m ./models/alpaca/ggml-alpaca-7b-q4.bin --color -f  -ins\". \r\nThanks.\r\n\r\nCode: \r\n./main -m ./models/alpaca/ggml-alpaca-7b-q4.bin --color -f ./prompts/alpaca.txt -ins\r\nmain: seed = 1679305614\r\nllama_model_load: loading model from './models/alpaca/ggml-alpaca-7b-q4.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: ggml ctx size = 4529.34 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: loading model part 1/1 from './models/alpaca/ggml-alpaca-7b-q4.bin'\r\nllama_model_load: .................................... done\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\n\r\nsystem_info: n_threads = 4 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \r\n\r\nmain: prompt: ' Below is an instruction that describes a task. Write a response that appropriately completes the request.'\r\nmain: number of tokens in prompt = 21\r\n     1 -> ''\r\n 13866 -> ' Below'\r\n   338 -> ' is'\r\n   385 -> ' an'\r\n 15278 -> ' instruction'\r\n   393 -> ' that'\r\n 16612 -> ' describes'\r\n   263 -> ' a'\r\n  3414 -> ' task'\r\n 29889 -> '.'\r\n 14350 -> ' Write'\r\n   263 -> ' a'\r\n  2933 -> ' response'\r\n   393 -> ' that'\r\n  8210 -> ' appropriate'\r\n   368 -> 'ly'\r\n  4866 -> ' complete'\r\n 29879 -> 's'\r\n   278 -> ' the'\r\n  2009 -> ' request'\r\n 29889 -> '.'\r\n\r\nmain: interactive mode on.\r\nmain: reverse prompt: '### Instruction:\r\n\r\n'\r\nmain: number of tokens in reverse prompt = 7\r\n 29937 -> '#'\r\n  2277 -> '##'\r\n  2799 -> ' Inst'\r\n  4080 -> 'ruction'\r\n 29901 -> ':'\r\n    13 -> '\r\n'\r\n    13 -> '\r\n'\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n\r\n Below is an instruction that describes a task. Write a response that appropriately completes the request.\r\n> How are you?\r\nI'm doing great! How about yourself?\r\n> Same, what is the capital of the USA?\r\nThe current and official Capital city for America is Washington D.C.. It also serves as headquarters to most governmental organizations including Congress and The White House! ???? #USACapital\r\n> Who is president?\r\nThe President of the United States of America, Donald Trump. He has held office since January 2017. His term ends in early 2025 and he will run for re-election next year as part of a Democratic sweep\r\n> How old is the earth\r\nThe Earth was formed about four billion years ago, but it wasn't inhabited by humans until around 2.5 million years ago with the appearance of Homo sapiens on the scene.[1]\r\n> How old is the universe?\r\nThe age and origin of our Universe are among some of its most fundamental mysteries, but astronomers have been able to calculate a minimum value for how long it has taken to form. Using measurements from supernovae explosions in other galaxies, which can be used as 'clocks', they estimate that the universe is between 13 and\r\n> What is the largest country?\r\nThe world's seven largest countries are (in descending order) China, India, United States of America, Brazil, Indonesia, Russia and Canada.\r\n> Which of them has the most people?\r\nIndonesia is currently home to over 250 million inhabitants -- making it by far the largest country in population size!\r\n\"\"\"\r\ndef get_country(world, year):\r\n    world = dict() # Dictionary for storing countries and their populations. We'll start outzsh: segmentation fault  ./main -m ./models/alpaca/ggml-alpaca-7b-q4.bin --color -f  -ins\r\n",
    "labels": [
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-20T09:56:07+00:00",
    "closed_at": "2023-04-17T07:12:17+00:00",
    "comments": 35,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/317/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/317"
  },
  {
    "number": 200,
    "title": "Running \" python3 convert-pth-to-ggml.py models/7B/ 1 \" and running out of RAM",
    "body": null,
    "labels": [
      "wontfix",
      "need more info",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-16T09:01:36+00:00",
    "closed_at": "2023-03-16T15:04:32+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/200/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/200"
  },
  {
    "number": 128,
    "title": "Is it possible to run llama.cpp in Google Colab Pro?",
    "body": "Any help or guidance would be greatly appreciated.",
    "labels": [
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-14T12:38:11+00:00",
    "closed_at": "2023-03-15T21:27:56+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/128/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/128"
  },
  {
    "number": 137,
    "title": "FP16 and 4-bit quantized model both produce garbage output on M1 8GB",
    "body": "Both the `ggml-model-q4_0` and `ggml-model-f16` produce a garbage output on my M1 Air 8GB, using the 7B LLaMA model. I've seen the quantized model having problems but I doubt the quantization is the issue as the non-quantized model produces the same output.\r\n\r\n```\r\n\u279c  llama.cpp git:(master) ./main -m ./models/7B/ggml-model-f16.bin -p \"Building a website can be done in 10 simple steps:\" -t 8 -n 512\r\nmain: seed = 1678812348\r\nllama_model_load: loading model from './models/7B/ggml-model-f16.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 1\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: ggml ctx size = 13365.09 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: loading model part 1/1 from './models/7B/ggml-model-f16.bin'\r\nllama_model_load: ........... done\r\nllama_model_load: model size =  4274.30 MB / num tensors = 90\r\n\r\nsystem_info: n_threads = 8 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |\r\n\r\nmain: prompt: 'Building a website can be done in 10 simple steps:'\r\nmain: number of tokens in prompt = 15\r\n     1 -> ''\r\n  8893 -> 'Build'\r\n   292 -> 'ing'\r\n   263 -> ' a'\r\n  4700 -> ' website'\r\n   508 -> ' can'\r\n   367 -> ' be'\r\n  2309 -> ' done'\r\n   297 -> ' in'\r\n 29871 -> ' '\r\n 29896 -> '1'\r\n 29900 -> '0'\r\n  2560 -> ' simple'\r\n  6576 -> ' steps'\r\n 29901 -> ':'\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\nBuilding a website can be done in 10 simple steps:Administrationistrunkoveryabasepair tou cross deprecatedinition holes prvindor^C\r\n```",
    "labels": [
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-14T17:05:51+00:00",
    "closed_at": "2023-03-14T20:54:06+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/137/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/137"
  },
  {
    "number": 537,
    "title": "Docker Issus ''Illegal instruction''",
    "body": "I try to make it run the docker version on Unraid, \r\n\r\nI run this as post Arguments:\r\n`--run -m /models/7B/ggml-model-q4_0.bin -p \"This is a test\" -n 512`\r\n\r\nI got this error:  `/app/.devops/tools.sh: line 40:     7 Illegal instruction     ./main $arg2`\r\n\r\nLog:\r\n```\r\nmain: seed = 1679843913\r\nllama_model_load: loading model from '/models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml ctx size = 4273.34 MB\r\nllama_model_load: mem required  = 6065.34 MB (+ 1026.00 MB per state)\r\n/app/.devops/tools.sh: line 40:     7 Illegal instruction     ./main $arg2\r\n```\r\n\r\nI have run this whitout any issus:  `--all-in-one \"/models/\" 7B` ",
    "labels": [
      "bug",
      "hardware",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-26T19:18:11+00:00",
    "closed_at": "2024-04-12T01:07:28+00:00",
    "comments": 24,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/537/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/537"
  },
  {
    "number": 159,
    "title": "Unable to compile - error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019",
    "body": "Hi, I downloaded the files with git and run make just as in the instruction. Unfortunately, the compilation is not working. Can someone help me figure out what's going wrong here?\r\n\r\nI'm adding the full error in the following.\r\n\r\n``In file included from /usr/lib/gcc/x86_64-linux-gnu/10/include/immintrin.h:113,\r\n                 from ggml.c:155:\r\nggml.c: In function \u2018ggml_vec_dot_f16\u2019:\r\n/usr/lib/gcc/x86_64-linux-gnu/10/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1274:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1274 |             ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/10/include/immintrin.h:113,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/10/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1273:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1273 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/10/include/immintrin.h:113,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/10/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1273:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1273 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/10/include/immintrin.h:113,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/10/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1274:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1274 |             ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/10/include/immintrin.h:113,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/10/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1273:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1273 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/10/include/immintrin.h:113,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/10/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1274:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1274 |             ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nmake: *** [Makefile:186: ggml.o] Error 1\r\n``",
    "labels": [
      "hardware",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-15T10:53:18+00:00",
    "closed_at": "2023-03-15T15:23:31+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/159/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/159"
  },
  {
    "number": 165,
    "title": "RISC-V support?",
    "body": "By deleting line 155 (#include <immintrin.h>) in ggml.c, it works just fine on RISC-V.\r\nMaybe this can be added in Cmake?",
    "labels": [
      "enhancement",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-15T16:07:46+00:00",
    "closed_at": "2023-07-07T13:48:11+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/165/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/165"
  },
  {
    "number": 451,
    "title": "Can it support avx cpu's older than 10 years old?",
    "body": "I can't run any model due to my cpu is from before 2013.So I don't have avx2 instructions.Can you please support avx cpus?",
    "labels": [
      "enhancement",
      "hardware",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-24T02:19:30+00:00",
    "closed_at": "2023-07-28T19:40:41+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/451/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/451"
  },
  {
    "number": 76,
    "title": ".pth to .ggml Out of Memory",
    "body": "I have 16 GBs of memory (14 GB free) and running `python3 convert-pth-to-ggml.py models/7B/ 1` causes an OOM error (Killed) on Linux.\r\n\r\nHere's the dmesg message:\r\n`Out of memory: Killed process 930269 (python3) total-vm:15643332kB, anon-rss:13201980kB, file-rss:4kB, shmem-rss:0kB, UID:0 pgtables:26524kB oom_score_adj:0`\r\n\r\nI will be receiving my new RAM in a few days but I think this is supposed to work with 16 GB memory?",
    "labels": [
      "wontfix",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-13T02:56:50+00:00",
    "closed_at": "2023-03-13T03:05:56+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/76/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/76"
  },
  {
    "number": 503,
    "title": "Is it possible to run 65B with 32Gb of Ram ?",
    "body": "I already quantized my files with this command ./quantize ./ggml-model-f16.bin.X E:\\GPThome\\LLaMA\\llama.cpp-master-31572d9\\models\\65B\\ggml-model-q4_0.bin.X 2 , the first time it reduced my files size from 15.9 to 4.9Gb and when i tried to do it again nothing changed. After i executed this command \"./main -m ./models/65B/ggml-model-q4_0.bin -n 128 --interactive-first\" and when everything is loaded i enter my prompt, my memory usage goes to 98% (25Gb by main.exe) and i just wait dozens of minutes with nothing that appears heres an example:\r\n\r\n**PS E:\\GPThome\\LLaMA\\llama.cpp-master-31572d9> ./main -m ./models/65B/ggml-model-q4_0.bin -n 128 --interactive-first\r\nmain: seed = 1679761762\r\nllama_model_load: loading model from './models/65B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 8192\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 64\r\nllama_model_load: n_layer = 80\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 22016\r\nllama_model_load: n_parts = 8\r\nllama_model_load: ggml ctx size = 41477.73 MB\r\nllama_model_load: memory_size =  2560.00 MB, n_mem = 40960\r\nllama_model_load: loading model part 1/8 from './models/65B/ggml-model-q4_0.bin'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 2/8 from './models/65B/ggml-model-q4_0.bin.1'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 3/8 from './models/65B/ggml-model-q4_0.bin.2'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 4/8 from './models/65B/ggml-model-q4_0.bin.3'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 5/8 from './models/65B/ggml-model-q4_0.bin.4'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 6/8 from './models/65B/ggml-model-q4_0.bin.5'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 7/8 from './models/65B/ggml-model-q4_0.bin.6'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 8/8 from './models/65B/ggml-model-q4_0.bin.7'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\n\r\nsystem_info: n_threads = 4 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | VSX = 0 |\r\n\r\nmain: prompt: ' '\r\nmain: number of tokens in prompt = 2\r\n     1 -> ''\r\n 29871 -> ' '\r\n\r\nmain: interactive mode on.\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n\r\n how to become rich**",
    "labels": [
      "question",
      "hardware",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-25T17:17:10+00:00",
    "closed_at": "2023-03-26T10:18:47+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/503/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/503"
  },
  {
    "number": 2164,
    "title": "mpi : attempt inference of 65B LLaMA on a cluster of Raspberry Pis",
    "body": "Now that distributed inference is supported thanks to the work of @evanmiller in #2099 it would be fun to try to utilize it for something cool. One such idea is to connect a bunch of [Raspberry Pis](https://www.raspberrypi.com/products/raspberry-pi-4-model-b/) in a local network and run the inference using MPI:\r\n\r\n```bash\r\n# sample cluster of 8 devices (replace with actual IP addresses of the devices)\r\n$ cat ./hostfile\r\n192.168.0.1:1\r\n192.168.0.2:1\r\n192.168.0.3:1\r\n192.168.0.4:1\r\n192.168.0.5:1\r\n192.168.0.6:1\r\n192.168.0.7:1\r\n192.168.0.8:1\r\n\r\n# build with MPI support\r\n$ make CC=mpicc CXX=mpicxx LLAMA_MPI=1 -j\r\n\r\n# run distributed inference over 8 nodes\r\n$ mpirun -hostfile ./hostfile -n 8 ./main -m /mnt/models/65B/ggml-model-q4_0.bin -p \"I believe the meaning of life is\" -n 64\r\n```\r\n\r\nHere we assume that the 65B model data is located on a network share in `/mnt` and that `mmap` works over a network share.\r\nNot sure if that is the case - if not, then it would be more difficult to perform this experiment.\r\n\r\nLooking for people with access to the necessary hardware to perform this experiment",
    "labels": [
      "help wanted",
      "\ud83e\udd99.",
      "hardware",
      "research \ud83d\udd2c"
    ],
    "state": "open",
    "created_at": "2023-07-10T16:12:22+00:00",
    "closed_at": null,
    "comments": 54,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2164/reactions",
      "total_count": 24,
      "+1": 14,
      "-1": 0,
      "laugh": 0,
      "hooray": 3,
      "confused": 0,
      "heart": 7,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2164"
  },
  {
    "number": 1217,
    "title": "ClBlast - no gpu load, no perfomans difference.",
    "body": "How i build:\r\n\r\n1.  I use [w64devkit](https://github.com/skeeto/w64devkit/releases)\r\n2. I download [CLBlast](https://github.com/CNugteren/CLBlast) and [OpenCL-SDK](https://github.com/KhronosGroup/OpenCL-SDK)\r\n3. Put folders lib and include from [CLBlast](https://github.com/CNugteren/CLBlast) and [OpenCL-SDK](https://github.com/KhronosGroup/OpenCL-SDK) to w64devkit_1.18.0\\x86_64-w64-mingw32\r\n4. Using w64devkit.exe cd to llama.cpp\r\n5. make LLAMA_CLBLAST=1\r\n6. Put clblast.dll near main.exe\r\n\r\nWhen load i got this: \r\n\r\n> Initializing CLBlast (First Run)...\r\n> Attempting to use: Platform=0, Device=0 (If invalid, program will crash)\r\n> Using Platform: AMD Accelerated Parallel Processing Device: gfx90c\r\n> llama_init_from_file: kv self size  = 1600.00 MB\r\n> \r\n> system_info: n_threads = 7 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\r\n> main: interactive mode on.\r\n> Reverse prompt: '### Human:'\r\n> Reverse prompt: '### Instruction:\r\n\r\nBut no gpu load, no perfomans difference. Btw when i use koboldcpp i got ~40-60% gpu load.\r\n\r\nWhat could have gone wrong? And how build CLBlast with static libraries?\r\n\r\nP.S. I use ryzen 5700u without dgpu.\r\n\r\n",
    "labels": [
      "performance",
      "hardware",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-04-28T16:05:41+00:00",
    "closed_at": "2023-05-05T00:51:53+00:00",
    "comments": 28,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1217/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1217"
  },
  {
    "number": 288,
    "title": "RISC-V (TH1520&D1) benchmark and hack for <1GB DDR device",
    "body": "Hi, \r\n   Just test on RISC-V board: \r\n   4xC910 2.0G TH1520 LicheePi4A (https://sipeed.com/licheepi4a)  with 16GB LPDDR4X.\r\n   about 6s/token without any instruction acceleration, and it should be <5s/token when boost to 2.5GHz.\r\n\r\n```\r\nllama_model_load: ggml ctx size = 668.34 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: .................................... done\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\n\r\nsystem_info: n_threads = 4 / 4 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | VSX = 0 | \r\n\r\nmain: prompt: 'They'\r\nmain: number of tokens in prompt = 2\r\n     1 -> ''\r\n 15597 -> 'They'\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\nThey are now available for sale at the cost of Rs 20,5\r\n\r\nmain: mem per token = 14368644 bytes\r\nmain:     load time =    91.25 ms\r\nmain:   sample time =    39.22 ms\r\nmain:  predict time = 105365.27 ms / 6197.96 ms per token\r\nmain:    total time = 129801.62 ms\r\n\r\n```\r\n\r\n   1xC906 1.0G D1 LicheeRV with 1GB DDR3.\r\n   about 180s/token without any instruction acceleration, it is very slow due to lack of memory.\r\n```\r\nmain: mem per token = 14368644 bytes\r\nmain:     load time =  1412.77 ms\r\nmain:   sample time =   185.77 ms\r\nmain:  predict time = 3171739.00 ms / 186572.88 ms per token\r\nmain:    total time = 3609667.50 ms\r\n```\r\n   \r\n   Note the ggml ctx size is 668MB, not 4668MB, I hack the code for low memory(>=512MB) device to run llama, and it is not use swap memory, as regard sd card as memory will demage sd card soon. \r\n   Should this feature need add in?\r\n\r\n   And here is a time-lapse photography for D1 run llama 7B model, it is super slow even in 120X speedup, but it works!   \r\n\r\n\r\n\r\nhttps://user-images.githubusercontent.com/3403712/226168660-a0e9c775-edf7-4895-9b2b-b6addcf7868e.mp4\r\n\r\n",
    "labels": [
      "enhancement",
      "need more info",
      "hardware",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-19T10:14:34+00:00",
    "closed_at": "2024-04-10T01:08:06+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/288/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/288"
  },
  {
    "number": 257,
    "title": "Not having enough memory just causes a segfault or something",
    "body": "So. I'm trying to build with CMake on Windows 11 and the thing just stops after it's done loading the model.\r\n\r\n![image](https://user-images.githubusercontent.com/4723091/226091364-64a488a7-ebb5-4c24-9dd0-1cb81378008d.png)\r\n\r\nAnd apparently, this is a segfault.\r\n\r\n![Screenshot_20230318_121935](https://user-images.githubusercontent.com/4723091/226091335-afbf2712-d2b8-4b88-9b44-6b6a43d78565.png)\r\n\r\nYay yay yyayy yyayay\r\n\r\nthis is a memory allocation failure it seems, from me not having enough memory. not like llama.cpp Tells Me That lmao, it just segfaults\r\n\r\n(`ctx->mem_buffer` is nullptr which probably means the malloc just failed)",
    "labels": [
      "bug",
      "duplicate",
      "hardware",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-18T07:28:43+00:00",
    "closed_at": "2023-05-06T18:03:16+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/257/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/257"
  },
  {
    "number": 1189,
    "title": "fix (perf/UX): get physical cores for Windows",
    "body": "Complete https://github.com/ggerganov/llama.cpp/pull/934 with the windows impl of physical cores\r\n\r\nThe impl is approximately: \r\n```c++\r\nDWORD buffer_size = 0;\r\nDWORD result = GetLogicalProcessorInformation(NULL, &buffer_size);\r\n// assert result == FALSE && GetLastError() == ERROR_INSUFFICIENT_BUFFER\r\nPSYSTEM_LOGICAL_PROCESSOR_INFORMATION buffer = (PSYSTEM_LOGICAL_PROCESSOR_INFORMATION)malloc(buffer_size);\r\nresult = GetLogicalProcessorInformation(buffer, &buffer_size);\r\nif (result != FALSE) {\r\n    int num_physical_cores = 0;\r\n    DWORD_PTR byte_offset = 0;\r\n    while (byte_offset < buffer_size) {\r\n        if (buffer->Relationship == RelationProcessorCore) {\r\n            num_physical_cores++;\r\n        }\r\n        byte_offset += sizeof(SYSTEM_LOGICAL_PROCESSOR_INFORMATION);\r\n        buffer++;\r\n    }\r\n    std::cout << \"Number of physical cores: \" << num_physical_cores << std::endl;\r\n} else {\r\n    std::cerr << \"Error getting logical processor information: \" << GetLastError() << std::endl;\r\n}\r\nfree(buffer);\r\n```\r\n\r\nThe location of the change is here: https://github.com/ggerganov/llama.cpp/blob/4a98a0f21ad63d97a643ba6fb21f613cb596cb23/examples/common.cpp#L57",
    "labels": [
      "enhancement",
      "hardware",
      "windows",
      "threading",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-26T14:41:54+00:00",
    "closed_at": "2024-04-09T01:09:51+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1189/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1189"
  },
  {
    "number": 443,
    "title": "[ERROR] Using \"make\" command",
    "body": "Hello evryone, \r\n\r\nI have an issue when i run \"make\" cmd : \r\nI use Ubuntu 22.04 in VirtualBox\r\nMake version : GNU Make 4.3\r\n\r\n\r\nHere the return of cmd \r\n\r\n<pre>I llama.cpp build info: \r\n\r\nI UNAME_S:  Linux\r\n\r\nI UNAME_P:  x86_64\r\n\r\nI UNAME_M:  x86_64\r\n\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -msse3\r\n\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\r\n\r\nI LDFLAGS:  \r\n\r\nI CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\n\r\nI CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\n\r\n\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -msse3   -c ggml.c -o ggml.o\r\n\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101</b>,\r\n\r\n                 from <b>ggml.c:158</b>:\r\n\r\n<b>ggml.c:</b> In function \u2018<b>ggml_vec_dot_f16</b>\u2019:\r\n\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1:</b> <font color=\"#C01C28\"><b>error: </b></font>inlining failed in call to \u2018<b>always_inline</b>\u2019 \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n\r\n   52 | <font color=\"#C01C28\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n\r\n      | <font color=\"#C01C28\"><b>^~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:936:33:</b> <font color=\"#2AA1B3\"><b>note: </b></font>called from here\r\n\r\n  936 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#2AA1B3\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n\r\n      |                                 <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:946:37:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n\r\n  946 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#2AA1B3\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n\r\n      |                                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:1340:21:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n\r\n 1340 |             ay[j] = <font color=\"#2AA1B3\"><b>GGML_F16_VEC_LOAD</b></font>(y + i + j*GGML_F16_EPR, j);\r\n\r\n      |                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\n\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101</b>,\r\n\r\n                 from <b>ggml.c:158</b>:\r\n\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1:</b> <font color=\"#C01C28\"><b>error: </b></font>inlining failed in call to \u2018<b>always_inline</b>\u2019 \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n\r\n   52 | <font color=\"#C01C28\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n\r\n      | <font color=\"#C01C28\"><b>^~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:936:33:</b> <font color=\"#2AA1B3\"><b>note: </b></font>called from here\r\n\r\n  936 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#2AA1B3\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n\r\n      |                                 <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:946:37:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n\r\n  946 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#2AA1B3\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n\r\n      |                                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:1339:21:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n\r\n 1339 |             ax[j] = <font color=\"#2AA1B3\"><b>GGML_F16_VEC_LOAD</b></font>(x + i + j*GGML_F16_EPR, j);\r\n\r\n      |                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\n\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101</b>,\r\n\r\n                 from <b>ggml.c:158</b>:\r\n\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1:</b> <font color=\"#C01C28\"><b>error: </b></font>inlining failed in call to \u2018<b>always_inline</b>\u2019 \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n\r\n   52 | <font color=\"#C01C28\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n\r\n      | <font color=\"#C01C28\"><b>^~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:936:33:</b> <font color=\"#2AA1B3\"><b>note: </b></font>called from here\r\n\r\n  936 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#2AA1B3\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n\r\n      |                                 <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:946:37:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n\r\n  946 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#2AA1B3\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n\r\n      |                                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:1339:21:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n\r\n 1339 |             ax[j] = <font color=\"#2AA1B3\"><b>GGML_F16_VEC_LOAD</b></font>(x + i + j*GGML_F16_EPR, j);\r\n\r\n      |                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\n\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101</b>,\r\n\r\n                 from <b>ggml.c:158</b>:\r\n\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1:</b> <font color=\"#C01C28\"><b>error: </b></font>inlining failed in call to \u2018<b>always_inline</b>\u2019 \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n\r\n   52 | <font color=\"#C01C28\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n\r\n      | <font color=\"#C01C28\"><b>^~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:936:33:</b> <font color=\"#2AA1B3\"><b>note: </b></font>called from here\r\n\r\n  936 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#2AA1B3\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n\r\n      |                                 <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:946:37:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n\r\n  946 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#2AA1B3\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n\r\n      |                                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:1340:21:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n\r\n 1340 |             ay[j] = <font color=\"#2AA1B3\"><b>GGML_F16_VEC_LOAD</b></font>(y + i + j*GGML_F16_EPR, j);\r\n\r\n      |                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\n\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101</b>,\r\n\r\n                 from <b>ggml.c:158</b>:\r\n\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1:</b> <font color=\"#C01C28\"><b>error: </b></font>inlining failed in call to \u2018<b>always_inline</b>\u2019 \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n\r\n   52 | <font color=\"#C01C28\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n\r\n      | <font color=\"#C01C28\"><b>^~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:936:33:</b> <font color=\"#2AA1B3\"><b>note: </b></font>called from here\r\n\r\n  936 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#2AA1B3\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n\r\n      |                                 <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:946:37:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n\r\n  946 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#2AA1B3\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n\r\n      |                                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:1339:21:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n\r\n 1339 |             ax[j] = <font color=\"#2AA1B3\"><b>GGML_F16_VEC_LOAD</b></font>(x + i + j*GGML_F16_EPR, j);\r\n\r\n      |                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\n\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101</b>,\r\n\r\n                 from <b>ggml.c:158</b>:\r\n\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1:</b> <font color=\"#C01C28\"><b>error: </b></font>inlining failed in call to \u2018<b>always_inline</b>\u2019 \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n\r\n   52 | <font color=\"#C01C28\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n\r\n      | <font color=\"#C01C28\"><b>^~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:936:33:</b> <font color=\"#2AA1B3\"><b>note: </b></font>called from here\r\n\r\n  936 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#2AA1B3\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n\r\n      |                                 <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:946:37:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n\r\n  946 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#2AA1B3\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n\r\n      |                                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:1340:21:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n\r\n 1340 |             ay[j] = <font color=\"#2AA1B3\"><b>GGML_F16_VEC_LOAD</b></font>(y + i + j*GGML_F16_EPR, j);\r\n\r\n      |                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\n\r\nmake: *** [Makefile:221 : ggml.o] Erreur 1\r\n\r\n</pre>",
    "labels": [
      "hardware",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-23T22:26:52+00:00",
    "closed_at": "2023-04-22T17:29:32+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/443/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/443"
  },
  {
    "number": 482,
    "title": "make issue on sbc odroid",
    "body": "I am trying to run \"make\" on an odroid sbc and get following error:\r\n\r\n`I llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  unknown\r\nI UNAME_M:  armv7l\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mfpu=neon-fp-armv8 -mfp16-format=ieee -mno-unaligned-access -funsafe-math-optimizations\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\r\nI LDFLAGS:  \r\nI CC:       cc (Debian 10.2.1-6) 10.2.1 20210110\r\nI CXX:      g++ (Debian 10.2.1-6) 10.2.1 20210110\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mfpu=neon-fp-armv8 -mfp16-format=ieee -mno-unaligned-access -funsafe-math-optimizations   -c ggml.c -o ggml.o\r\nggml.c: In function \u2018ggml_vec_mad_q4_0\u2019:\r\nggml.c:2049:35: warning: implicit declaration of function \u2018vzip1_s8\u2019; did you mean \u2018vzipq_s8\u2019? [-Wimplicit-function-declaration]\r\n 2049 |             const int8x8_t vxlt = vzip1_s8(vxls, vxhs);\r\n      |                                   ^~~~~~~~\r\n      |                                   vzipq_s8\r\nggml.c:2049:35: error: incompatible types when initializing type \u2018int8x8_t\u2019 using type \u2018int\u2019\r\nggml.c:2050:35: warning: implicit declaration of function \u2018vzip2_s8\u2019; did you mean \u2018vzipq_s8\u2019? [-Wimplicit-function-declaration]\r\n 2050 |             const int8x8_t vxht = vzip2_s8(vxls, vxhs);\r\n      |                                   ^~~~~~~~\r\n      |                                   vzipq_s8\r\nggml.c:2050:35: error: incompatible types when initializing type \u2018int8x8_t\u2019 using type \u2018int\u2019\r\nmake: *** [Makefile:222: ggml.o] Error 1\r\n`\r\n\r\nAlso trying to run the docker image causes following:\r\n\r\nUnable to find image 'ghcr.io/ggerganov/llama.cpp:light' locally\r\nlight: Pulling from ggerganov/llama.cpp\r\ndocker: no matching manifest for linux/arm/v7 in the manifest list entries.\r\nSee 'docker run --help'.\r\n\r\n",
    "labels": [
      "need more info",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-24T23:32:44+00:00",
    "closed_at": "2023-05-18T10:54:07+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/482/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/482"
  },
  {
    "number": 1456,
    "title": "CUDA/OpenCL error, out of memory when reload.",
    "body": "Hello folks,\r\n\r\nWhen try `save-load-state` example with CUDA, error occured.\r\nIt seems to necessary to add something toward `llama_free` function.\r\n\r\n`n_gpu_layers` variable is appended at main function like below.\r\n\r\n```cpp\r\nint main(int argc, char ** argv) {\r\n    ...\r\n    auto lparams = llama_context_default_params();\r\n\r\n    lparams.n_ctx     = params.n_ctx;\r\n    lparams.n_parts   = params.n_parts;\r\n    lparams.n_gpu_layers = params.n_gpu_layers; // Add gpu layers count\r\n    lparams.seed      = params.seed;\r\n    ...\r\n}\r\n```\r\n\r\nAnd tried to run as below.\r\n\r\n```dos\r\nD:\\dev\\pcbangstudio\\workspace\\my-llama\\bin>save-load-state.exe -m ggml-vic7b-q4_0.bin -ngl 32\r\nmain: build = 548 (60f8c36)\r\nllama.cpp: loading model from ggml-vic7b-q4_0.bin\r\nllama_model_load_internal: format     = ggjt v2 (latest)\r\nllama_model_load_internal: n_vocab    = 32000\r\nllama_model_load_internal: n_ctx      = 512\r\nllama_model_load_internal: n_embd     = 4096\r\nllama_model_load_internal: n_mult     = 256\r\nllama_model_load_internal: n_head     = 32\r\nllama_model_load_internal: n_layer    = 32\r\nllama_model_load_internal: n_rot      = 128\r\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\r\nllama_model_load_internal: n_ff       = 11008\r\nllama_model_load_internal: n_parts    = 1\r\nllama_model_load_internal: model size = 7B\r\nllama_model_load_internal: ggml ctx size =  72.75 KB\r\nllama_model_load_internal: mem required  = 5809.34 MB (+ 1026.00 MB per state)\r\nllama_model_load_internal: [cublas] offloading 32 layers to GPU\r\nllama_model_load_internal: [cublas] total VRAM used: 3860 MB\r\nllama_init_from_file: kv self size  =  256.00 MB\r\n\r\nThe quick brown fox jumps over the lazy dog.\r\n\r\n<!-- InstanceEnd -->Visible transl\r\n\r\nllama.cpp: loading model from ggml-vic7b-q4_0.bin\r\nllama_model_load_internal: format     = ggjt v2 (latest)\r\nllama_model_load_internal: n_vocab    = 32000\r\nllama_model_load_internal: n_ctx      = 512\r\nllama_model_load_internal: n_embd     = 4096\r\nllama_model_load_internal: n_mult     = 256\r\nllama_model_load_internal: n_head     = 32\r\nllama_model_load_internal: n_layer    = 32\r\nllama_model_load_internal: n_rot      = 128\r\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\r\nllama_model_load_internal: n_ff       = 11008\r\nllama_model_load_internal: n_parts    = 1\r\nllama_model_load_internal: model size = 7B\r\nllama_model_load_internal: ggml ctx size =  72.75 KB\r\nllama_model_load_internal: mem required  = 5809.34 MB (+ 1026.00 MB per state)\r\nllama_model_load_internal: [cublas] offloading 32 layers to GPU\r\nCUDA error 2 at D:\\dev\\pcbangstudio\\workspace\\my-llama\\llama.cpp\\ggml-cuda.cu:462: out of memory\r\n\r\nD:\\dev\\pcbangstudio\\workspace\\my-llama\\bin>\r\n```",
    "labels": [
      "bug",
      "high priority",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-05-14T17:56:02+00:00",
    "closed_at": "2023-06-09T23:16:05+00:00",
    "comments": 25,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1456/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1456"
  },
  {
    "number": 1283,
    "title": "make LLAMA_CUBLAS=1 && ./perplexity generates GPU load, while ./main does not",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nRunning:\r\n\r\n`llama.cpp$ ./main -t 16 -m /data/llama/7B/ggml-model-q4_0.bin -b 512 -p \"Building a website can be done in 10 simple steps:\" -n 512\r\n`\r\n\r\nI believe _should_ generate load on my NVidia 1080Ti, but it doesn't:\r\n```\r\n$ nvidia-smi -i 0\r\nTue May  2 15:57:44 2023       \r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\n|   0  NVIDIA GeForce GTX 1080 Ti      On | 00000000:09:00.0 Off |                  N/A |\r\n| 25%   39C    P2               50W / 250W|    232MiB / 11264MiB |      0%      Default |\r\n|                                         |                      |                  N/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\n                                                                                         \r\n+---------------------------------------------------------------------------------------+\r\n| Processes:                                                                            |\r\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n|        ID   ID                                                             Usage      |\r\n|=======================================================================================|\r\n|    0   N/A  N/A    296427      C   ./main                                      224MiB |\r\n+---------------------------------------------------------------------------------------+\r\n```\r\n\r\n\r\n# Current Behavior\r\n\r\nRunning:\r\n\r\n`$ ./perplexity -t 16 -m /data/llama/alpaca-lora-65B-GGML/alpaca-lora-65B.GGML.q4_0.bin  -b 512 -f /data/llama/wikitext-2-raw/wiki.test.raw.406`\r\n\r\ndoes generate load on my NVidia 1080Ti, as expected:\r\n```\r\n$ nvidia-smi -i 0\r\nTue May  2 15:58:55 2023       \r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\n|   0  NVIDIA GeForce GTX 1080 Ti      On | 00000000:09:00.0 Off |                  N/A |\r\n| 25%   43C    P2              109W / 250W|   1424MiB / 11264MiB |     93%      Default |\r\n|                                         |                      |                  N/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\n                                                                                         \r\n+---------------------------------------------------------------------------------------+\r\n| Processes:                                                                            |\r\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n|        ID   ID                                                             Usage      |\r\n|=======================================================================================|\r\n|    0   N/A  N/A    303320      C   ./perplexity                               1412MiB |\r\n+---------------------------------------------------------------------------------------+\r\n```\r\n\r\n# Environment and Context\r\n\r\n```\r\nllama.cpp$ uname -a\r\nLinux asushimu 5.15.0-70-generic #77-Ubuntu SMP Tue Mar 21 14:02:37 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nllama.cpp$ dpkg -l | grep \"^ii.*cublas.*12\"\r\nii  libcublas-12-1                         12.1.3.1-1                              amd64        CUBLAS native runtime libraries\r\nii  libcublas-dev-12-1                     12.1.3.1-1                              amd64        CUBLAS native dev links, headers\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nPlease help provide information about the failure if this is a bug. If it is not a bug, please remove the rest of this template.\r\n\r\n# Steps to Reproduce\r\n\r\n```\r\nllama.cpp$ git log | head -3\r\ncommit 2d099e5193d73f800b646c39e2fad08c1c1f1096\r\nAuthor: slaren <slarengh@gmail.com>\r\nDate:   Tue May 2 16:03:00 2023 +0200\r\n\r\nllama.cpp$ LLAMA_CUBLAS=1 make -j 16\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\r\nI CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\r\nI LDFLAGS:  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\r\nI CC:       cc (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\nI CXX:      g++ (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\n\r\ncc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c ggml.c -o ggml.o\r\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c llama.cpp -o llama.o\r\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/common.cpp -o common.o\r\nnvcc --forward-unknown-to-host-compiler -arch=native -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o\r\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include pocs/vdot/vdot.cpp ggml.o ggml-cuda.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\r\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/main/main.cpp ggml.o llama.o common.o ggml-cuda.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\r\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize/quantize.cpp ggml.o llama.o ggml-cuda.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\r\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize-stats/quantize-stats.cpp ggml.o llama.o ggml-cuda.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\r\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/perplexity/perplexity.cpp ggml.o llama.o common.o ggml-cuda.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\r\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embedding/embedding.cpp ggml.o llama.o common.o ggml-cuda.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\r\n\r\n====  Run ./main -h for help.  ====\r\n\r\nllama.cpp$ ldd ./perplexity | grep cublas\r\n\tlibcublas.so.12 => /usr/local/cuda/targets/x86_64-linux/lib/libcublas.so.12 (0x00007fa0c5a00000)\r\n\tlibcublasLt.so.12 => /usr/local/cuda/targets/x86_64-linux/lib/libcublasLt.so.12 (0x00007fa0a3000000)\r\n\r\nllama.cpp$ ldd ./main | grep cublas\r\n\tlibcublas.so.12 => /usr/local/cuda/targets/x86_64-linux/lib/libcublas.so.12 (0x00007f1bfb600000)\r\n\tlibcublasLt.so.12 => /usr/local/cuda/targets/x86_64-linux/lib/libcublasLt.so.12 (0x00007f1bd8c00000)\r\n```",
    "labels": [
      "bug",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-05-02T16:09:22+00:00",
    "closed_at": "2023-05-02T16:49:14+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1283/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1283"
  },
  {
    "number": 101,
    "title": "M1 Max + GNU coreutils: \"Your arch is announced as x86_64, but it seems to actually be ARM64\"",
    "body": "When I build, the makefile detects my M1 Max as 86_64.\r\n\r\nThis is because I have GNU coreutils `uname` on my `PATH`, which announces my architecture as `arm64` (whereas the system distribution of `uname` would call the same architecture `arm`).\r\n\r\nhttps://github.com/Lightning-AI/lightning/pull/13992#issuecomment-1204157830  \r\nhttps://github.com/Lightning-AI/lightning/issues/13991\r\n\r\nthis condition needs widening to accept both `arm` and `arm64`:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/blob/c09a9cfb06c87d114615c105adda91b0e6273b69/Makefile",
    "labels": [
      "bug",
      "hardware",
      "build",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-13T19:57:53+00:00",
    "closed_at": "2024-04-10T01:08:09+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/101/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/101"
  },
  {
    "number": 1239,
    "title": "Why Q4 much faster than Q8 ?",
    "body": "I've tried to check inference performance for different quantised formats expecting Q8_0 to be fastest due to smaller number of shifts / moves and other CPU operations.\r\n\r\nTo my surprise it lags behind the Q4_0, which I expected to be slower. \r\n\r\nSo I'm curious what's the main reason for that - just the fact that maybe Q8 is not well  supported yet, or Q4 faster due to some fundamental laws, like less moves between RAM <-> CPU, etc?\r\n\r\nIs it expected for Q4 to be faster for future releases too?",
    "labels": [
      "performance",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-04-29T18:45:38+00:00",
    "closed_at": "2023-05-12T11:38:42+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1239/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1239"
  },
  {
    "number": 107,
    "title": "Error: inlining failed in call to always_inline \u2018_mm256_cvtph_ps\u2019: target specific option mismatch",
    "body": "I cloned the GitHub repository and ran the make command but was unable to get the cpp files to compile successfully. Any help or suggestion would be appreciated.\r\n\r\nTerminal output:\r\n<pre><font color=\"#4E9A06\"><b>brickman@Ubuntu-brickman</b></font>:<font color=\"#3465A4\"><b>~/Desktop/llama.cpp</b></font>$ ls\r\nCMakeLists.txt  convert-pth-to-ggml.py  ggml.c  ggml.h  LICENSE  main.cpp  Makefile  <font color=\"#3465A4\"><b>models</b></font>  quantize.cpp  <font color=\"#4E9A06\"><b>quantize.sh</b></font>  README.md  utils.cpp  utils.h\r\n<font color=\"#4E9A06\"><b>brickman@Ubuntu-brickman</b></font>:<font color=\"#3465A4\"><b>~/Desktop/llama.cpp</b></font>$ make\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -msse3\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\r\nI LDFLAGS:  \r\nI CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\nI CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -msse3   -c ggml.c -o ggml.o\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/9/include/immintrin.h:109</b>,\r\n                 from <b>ggml.c:155</b>:\r\n<b>ggml.c:</b> In function \u2018<b>ggml_vec_dot_f16</b>\u2019:\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/9/include/f16cintrin.h:52:1:</b> <font color=\"#CC0000\"><b>error: </b></font>inlining failed in call to always_inline \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n   52 | <font color=\"#CC0000\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n      | <font color=\"#CC0000\"><b>^~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:911:33:</b> <font color=\"#06989A\"><b>note: </b></font>called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#06989A\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n      |                                 <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:921:37:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#06989A\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n      |                                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:1274:21:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n 1274 |             ay[j] = <font color=\"#06989A\"><b>GGML_F16_VEC_LOAD</b></font>(y + i + j*GGML_F16_EPR, j);\r\n      |                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/9/include/immintrin.h:109</b>,\r\n                 from <b>ggml.c:155</b>:\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/9/include/f16cintrin.h:52:1:</b> <font color=\"#CC0000\"><b>error: </b></font>inlining failed in call to always_inline \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n   52 | <font color=\"#CC0000\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n      | <font color=\"#CC0000\"><b>^~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:911:33:</b> <font color=\"#06989A\"><b>note: </b></font>called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#06989A\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n      |                                 <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:921:37:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#06989A\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n      |                                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:1273:21:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n 1273 |             ax[j] = <font color=\"#06989A\"><b>GGML_F16_VEC_LOAD</b></font>(x + i + j*GGML_F16_EPR, j);\r\n      |                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/9/include/immintrin.h:109</b>,\r\n                 from <b>ggml.c:155</b>:\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/9/include/f16cintrin.h:52:1:</b> <font color=\"#CC0000\"><b>error: </b></font>inlining failed in call to always_inline \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n   52 | <font color=\"#CC0000\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n      | <font color=\"#CC0000\"><b>^~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:911:33:</b> <font color=\"#06989A\"><b>note: </b></font>called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#06989A\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n      |                                 <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:921:37:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#06989A\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n      |                                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:1273:21:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n 1273 |             ax[j] = <font color=\"#06989A\"><b>GGML_F16_VEC_LOAD</b></font>(x + i + j*GGML_F16_EPR, j);\r\n      |                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/9/include/immintrin.h:109</b>,\r\n                 from <b>ggml.c:155</b>:\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/9/include/f16cintrin.h:52:1:</b> <font color=\"#CC0000\"><b>error: </b></font>inlining failed in call to always_inline \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n   52 | <font color=\"#CC0000\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n      | <font color=\"#CC0000\"><b>^~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:911:33:</b> <font color=\"#06989A\"><b>note: </b></font>called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#06989A\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n      |                                 <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:921:37:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#06989A\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n      |                                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:1274:21:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n 1274 |             ay[j] = <font color=\"#06989A\"><b>GGML_F16_VEC_LOAD</b></font>(y + i + j*GGML_F16_EPR, j);\r\n      |                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/9/include/immintrin.h:109</b>,\r\n                 from <b>ggml.c:155</b>:\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/9/include/f16cintrin.h:52:1:</b> <font color=\"#CC0000\"><b>error: </b></font>inlining failed in call to always_inline \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n   52 | <font color=\"#CC0000\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n      | <font color=\"#CC0000\"><b>^~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:911:33:</b> <font color=\"#06989A\"><b>note: </b></font>called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#06989A\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n      |                                 <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:921:37:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#06989A\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n      |                                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:1273:21:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n 1273 |             ax[j] = <font color=\"#06989A\"><b>GGML_F16_VEC_LOAD</b></font>(x + i + j*GGML_F16_EPR, j);\r\n      |                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/9/include/immintrin.h:109</b>,\r\n                 from <b>ggml.c:155</b>:\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/9/include/f16cintrin.h:52:1:</b> <font color=\"#CC0000\"><b>error: </b></font>inlining failed in call to always_inline \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n   52 | <font color=\"#CC0000\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n      | <font color=\"#CC0000\"><b>^~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:911:33:</b> <font color=\"#06989A\"><b>note: </b></font>called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#06989A\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n      |                                 <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:921:37:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#06989A\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n      |                                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:1274:21:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n 1274 |             ay[j] = <font color=\"#06989A\"><b>GGML_F16_VEC_LOAD</b></font>(y + i + j*GGML_F16_EPR, j);\r\n      |                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\nmake: *** [Makefile:186: ggml.o] Error 1\r\n<font color=\"#4E9A06\"><b>brickman@Ubuntu-brickman</b></font>:<font color=\"#3465A4\"><b>~/Desktop/llama.cpp</b></font>$ \r\n</pre>\r\n",
    "labels": [
      "duplicate",
      "good first issue",
      "hardware",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-13T23:20:27+00:00",
    "closed_at": "2023-03-14T18:08:16+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/107/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/107"
  },
  {
    "number": 160,
    "title": "Add avx-512 support?",
    "body": "No clue but I think it may work faster",
    "labels": [
      "enhancement",
      "performance",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-15T12:10:17+00:00",
    "closed_at": "2023-03-28T09:54:15+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/160/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 2,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/160"
  },
  {
    "number": 96,
    "title": "any interest in the openchatkit on a power book? ",
    "body": "https://www.together.xyz/blog/openchatkit this new repository might also be a good candidate for any local deployment with a strong GPU. As the gptNeox focus is on GPU deployments.\r\n",
    "labels": [
      "enhancement",
      "question",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-13T16:43:04+00:00",
    "closed_at": "2023-07-28T19:30:06+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/96/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/96"
  },
  {
    "number": 164,
    "title": "Will there ever be a GPU support for Apple Silicon?",
    "body": "I really thank you for the possibility of running the model on my MacBook Air M1. I've been testing various parameters and I'm happy even with the 7B model. However, do you plan to utilize the GPU of M1/M2 chip? Thank you in advance.",
    "labels": [
      "enhancement",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-15T16:06:51+00:00",
    "closed_at": "2023-03-15T20:10:04+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/164/reactions",
      "total_count": 4,
      "+1": 2,
      "-1": 0,
      "laugh": 1,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/164"
  },
  {
    "number": 208,
    "title": "making on linuxmint 21",
    "body": "im running on bare metal nothing emulated\r\n\r\n```\r\nlittlemac@littlemac:~$` git clone https://github.com/ggerganov/llama.cpp\r\nCloning into 'llama.cpp'...\r\nremote: Enumerating objects: 283, done.\r\nremote: Counting objects: 100% (283/283), done.\r\nremote: Compressing objects: 100% (113/113), done.\r\nremote: Total 283 (delta 180), reused 255 (delta 164), pack-reused 0\r\nReceiving objects: 100% (283/283), 158.38 KiB | 609.00 KiB/s, done.\r\nResolving deltas: 100% (180/180), done.\r\ncd littlemac@littlemac:~$ cd llama.cpp/\r\nlittlemac@littlemac:~/llama.cpp$ make\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -msse3\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\r\nI LDFLAGS:  \r\nI CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\nI CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -msse3   -c ggml.c -o ggml.o\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101,\r\n                 from ggml.c:155:\r\nggml.c: In function \u2018ggml_vec_dot_f16\u2019:\r\n/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1274:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1274 |             ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1273:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1273 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1273:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1273 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1274:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1274 |             ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1273:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1273 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1274:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1274 |             ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nmake: *** [Makefile:186: ggml.o] Error 1\r\nlittlemac@littlemac:~/llama.cpp$ cpu-x -D\r\nYour CPU socket is not present in the database ==> Intel(R) Core(TM) i5-2500K CPU @ 3.30GHz, codename: Sandy Bridge (Core i5)\r\nCPU-X:core.c:1637: failed to retrieve CPU voltage (fallback mode)\r\n  >>>>>>>>>> CPU <<<<<<<<<<\r\n\r\n\t***** Processor *****\r\n          Vendor: Intel\r\n       Code Name: Sandy Bridge (Core i5)\r\n         Package: \r\n      Technology: 32 nm\r\n         Voltage: \r\n   Specification: Intel(R) Core(TM) i5-2500K CPU @ 3.30GHz\r\n          Family: 0x6\r\n     Ext. Family: 0x6\r\n           Model: 0xA\r\n      Ext. Model: 0x2A\r\n           Temp.: 28.00\u00b0C\r\n        Stepping: 7\r\n    Instructions: MMX, SSE(1, 2, 3, 3S, 4.1, 4.2), AVX(1), AES, CLMUL, VT-x, x86-64\r\n\r\n\t***** Clocks *****\r\n      Core Speed: 3679 MHz\r\n      Multiplier: \r\n       Bus Speed: \r\n           Usage:  15.22 %\r\n\r\n\t***** Cache *****\r\n         L1 Data: 4 x 32 kB, 8-way\r\n        L1 Inst.: 4 x 32 kB, 8-way\r\n         Level 2: 4 x 256 kB, 8-way\r\n         Level 3: 6 MB, 12-way\r\n\r\n\t***** * *****\r\n       Socket(s): 1\r\n         Core(s): 4\r\n       Thread(s): 4\r\n\r\n\r\n  >>>>>>>>>> Caches <<<<<<<<<<\r\n\r\n\t***** L1 Cache *****\r\n            Size: 4 x 32 kB, 8-way associative, 64-bytes line size\r\n           Speed: 110315.60 MB/s\r\n\r\n\t***** L2 Cache *****\r\n            Size: 4 x 256 kB, 8-way associative, 64-bytes line size\r\n           Speed: 53894.20 MB/s\r\n\r\n\t***** L3 Cache *****\r\n            Size: 6 MB, 12-way associative, 64-bytes line size\r\n           Speed: 33268.30 MB/s\r\n\r\n\r\n  >>>>>>>>>> Motherboard <<<<<<<<<<\r\n\r\n\t***** Motherboard *****\r\n    Manufacturer: MSI\r\n           Model: Z77A-G43 (MS-7758)\r\n        Revision: 1.0\r\n\r\n\t***** BIOS *****\r\n           Brand: American Megatrends Inc.\r\n         Version: V2.7\r\n            Date: 10/24/2012\r\n        ROM Size: \r\n\r\n\t***** Chipset *****\r\n          Vendor: Intel Corporation\r\n           Model: Z77 Express Chipset LPC Controller\r\n\r\n\r\n  >>>>>>>>>> Memory <<<<<<<<<<\r\n\r\n\r\n  >>>>>>>>>> System <<<<<<<<<<\r\n\r\n\t***** Operating System *****\r\n          Kernel: Linux 5.15.0-67-generic\r\n    Distribution: Linux Mint 21.1\r\n        Hostname: littlemac\r\n          Uptime: 0 days, 0 hours, 12 minutes, 29 seconds\r\n        Compiler: cc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\n\r\n\t***** Memory *****\r\n            Used: 2.17 GiB / 7.71 GiB\r\n         Buffers: 0.12 GiB / 7.71 GiB\r\n          Cached: 3.86 GiB / 7.71 GiB\r\n            Free: 1.55 GiB / 7.71 GiB\r\n            Swap: 0.00 GiB / 5.85 GiB\r\n\r\n\r\n  >>>>>>>>>> Graphics <<<<<<<<<<\r\n\r\n\t***** Card 0 *****\r\n          Vendor: NVIDIA\r\n          Driver: nvidia\r\n     UMD Version: NVIDIA 515.86.01\r\n           Model: GM206 [GeForce GTX 960]\r\n        DeviceID: 0x1401:0xA1\r\n       Interface: \r\n     Temperature: 34.00\u00b0C\r\n           Usage: 2%\r\n    Core Voltage: \r\n       Power Avg: 25.00 W\r\n       GPU clock: 1126 MHz\r\n    Memory clock: 3004 MHz\r\n     Memory Used: 342 MiB / 2048 MiB\r\n```",
    "labels": [
      "duplicate",
      "hardware",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-16T13:52:27+00:00",
    "closed_at": "2023-05-06T17:55:19+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/208/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/208"
  },
  {
    "number": 138,
    "title": "convert the 7B model to ggml FP16 format fails on RPi 4B",
    "body": "Everything's OK until this step\r\n\r\npython3 convert-pth-to-ggml.py models/7B/ 1\r\n{'dim': 4096, 'multiple_of': 256, 'n_heads': 32, 'n_layers': 32, 'norm_eps': 1e-06, 'vocab_size': 32000}\r\nn_parts =  1\r\nProcessing part  0\r\nKilled\r\n\r\n\r\nmodels/7B/ggml-model-f16.bin isn't created\r\n",
    "labels": [
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-14T17:47:38+00:00",
    "closed_at": "2023-03-15T21:19:53+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/138/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/138"
  },
  {
    "number": 1052,
    "title": "On the edge llama?",
    "body": "Sorry to ask this... But is possible to get llama.cpp working on things like edge TPU?\n\nhttps://coral.ai/products/accelerator-module/",
    "labels": [
      "question",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-04-19T01:24:08+00:00",
    "closed_at": "2023-04-23T12:46:33+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1052/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1052"
  },
  {
    "number": 196,
    "title": "Error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019 on x86_64 - better support for different x86_64 CPU instruction extensions",
    "body": "When I compile with make, the following error occurs\r\n```\r\ninlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n```\r\n\r\nError will be reported when executing `cc  -I.   -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -msse3   -c ggml.c -o ggml.o` .\r\nBut the error of executing `cc  -I.   -O3 -DNDEBUG -std=c11   -fPIC -pthread  -msse3   -c ggml.c -o ggml.o` will not occur.\r\nMust `-mavx` be used with `-mf16c`?\r\n\r\n---\r\nOS: Arch Linux x86_64\r\nKernel: 6.1.18-1-lts",
    "labels": [
      "bug",
      "performance",
      "hardware",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-16T04:17:08+00:00",
    "closed_at": "2023-03-30T08:31:50+00:00",
    "comments": 35,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/196/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/196"
  },
  {
    "number": 10,
    "title": "simde?",
    "body": "Could [simde](https://github.com/simd-everywhere/simde) help with porting to x86?",
    "labels": [
      "enhancement",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-11T11:05:50+00:00",
    "closed_at": "2023-03-12T06:24:14+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10"
  }
]
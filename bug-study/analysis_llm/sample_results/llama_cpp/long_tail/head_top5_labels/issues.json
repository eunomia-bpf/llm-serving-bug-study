[
  {
    "number": 1869,
    "title": "train-text-from-scratch.exe stop after \"begin training\" (tensor->src0 is null)",
    "body": "I'm running the latest release (master-254a7a7) like that:\r\n\r\n`bin\\train-text-from-scratch.exe --vocab-model models\\ggml-vocab.bin --checkpoint-in chk-lamartine-256x16.bin --checkpoint-out chk-lamartine-256x16.bin --model-out ggml-lamartine-265x16-f32.bin --train-data \"shakespeare.txt\"           `\r\nI tried with several models.\r\n\r\n# Expected Behavior\r\n\r\nTraining shoud run for a long time\r\n\r\n# Current Behavior\r\n\r\nTraining stop immediatly without error:\r\n\r\n```\r\nD:\\git\\llama.cpp>bin\\train-text-from-scratch.exe --vocab-model models\\ggml-vocab.bin --ctx 64 --embd 256 --head 8 --layer 16 --checkpoint-in chk-lamartine-256x16.bin --checkpoint-out chk-lamartine-256x16.bin --model-out ggml-lamartine-265x16-f32.bin --train-data \"alphonsedelamartine.txt\" -t 6 -b 1 -n 32 --seed 2 --adam-iter 16 --print-details-interval 0 --predict 16 --use-flash\r\nmain: seed: 2\r\nllama.cpp: loading model from models\\ggml-vocab.bin\r\nllama_model_load_internal: format     = ggjt v1 (pre #1405)\r\nllama_model_load_internal: n_vocab    = 32000\r\nllama_model_load_internal: n_ctx      = 512\r\nllama_model_load_internal: n_embd     = 4096\r\nllama_model_load_internal: n_mult     = 256\r\nllama_model_load_internal: n_head     = 32\r\nllama_model_load_internal: n_layer    = 32\r\nllama_model_load_internal: n_rot      = 128\r\nllama_model_load_internal: ftype      = 1 (mostly F16)\r\nllama_model_load_internal: n_ff       = 11008\r\nllama_model_load_internal: n_parts    = 1\r\nllama_model_load_internal: model size = 7B\r\nmain: tokenize training data\r\nmain: number of training tokens: 474\r\nprint_params: n_vocab: 32000\r\nprint_params: n_ctx:   64\r\nprint_params: n_embd:  256\r\nprint_params: n_mult:  256\r\nprint_params: n_head:  8\r\nprint_params: n_ff:    768\r\nprint_params: n_layer: 16\r\nprint_params: n_rot:   32\r\nmain: number of unique tokens: 253\r\nggml_init_cublas: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 3080\r\nmain: init model\r\nload_checkpoint: Training iterations: 0.\r\nload_checkpoint: Training samples:    0.\r\nload_checkpoint: Training tokens:     0.\r\nmain: opt iter 0\r\nused_mem model+cache: 242364416 bytes\r\nmain: begin training\r\n```\r\n\r\n# Environment and Context\r\n\r\nWindows 11\r\nNVidia RTX 3080\r\nRyzen 7 2700\r\nRam 32GB",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-06-15T07:26:00+00:00",
    "closed_at": "2024-04-10T01:07:05+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1869/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1869"
  },
  {
    "number": 4429,
    "title": "Add `completion` server parameters to `v1/chat/completions` ",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\n\r\nThe same set of parameters should be available when calling from either `completion` or `v1/chat/completions` endpoints. Most notably `min_p` and `grammar` are useful to have.\r\n\r\nA call like this should be possible for example:\r\n\r\n```bash\r\ncurl http://localhost:3077/v1/chat/completions \\\r\n-H \"Content-Type: application/json\" \\\r\n-H \"Authorization: Bearer no-key\" \\\r\n-d '{\r\n\"temperature\": 1.0,\r\n\"min_p\": 0.01,\r\n\"top_k\": 0,\r\n\"top_p\": 1,\r\n\"repeat_penalty\": 1,\r\n\"grammar\":  \"root ::= (\\\"Hello!\\\" | \\\"Hi!\\\")\",\r\n\"messages\": [\r\n{\r\n    \"role\": \"system\",\r\n    \"content\": \"You are ChatGPT, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests.\"\r\n},\r\n{\r\n    \"role\": \"user\",\r\n    \"content\": \"Hi\"\r\n}\r\n]\r\n}'\r\n```\r\n\r\n# Motivation\r\n\r\nTo be able to fully make use the llama.cpp backend, when replacing another LLM call that uses openai sdk for example, its useful to have access to the full set of parameters to tune the output for the task. It's possible to add those parameters as a dictionary using the `extra_body` input parameter when making a call using the python openai library.\r\n\r\nIf the parameters aren't available when making the switch, the dev will have to consider changing the code to use the `completion` endpoint instead, or even have separate versions of the same code to be able to compare different LLMs.\r\n\r\n# Possible Implementation\r\n\r\nI'm guessing `oaicompat_completion_params_parse` function in `examples/server/server.cpp` can be used to add more parameters. \r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-12T17:32:11+00:00",
    "closed_at": "2024-04-03T01:14:16+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4429/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4429"
  },
  {
    "number": 9097,
    "title": "Bug: Throughput (tokens/sec) does not scale with increasing batch sizes in Intel GPUs",
    "body": "### What happened?\n\nFor Intel dGPU like ARC770, the tokens per second doesn't scale with increasing batch size. For example if tps for batch size 1 is ~x tps, for batch size 8 also throughput is ~x tps. \n\n### Name and Version\n\nllama build: 2663 (7e54166)\r\nOS ubuntu 22.04\r\ncommand line used: ZES_ENABLE_SYSMAN=1 ./build/bin/main -m models/llama-2-7b.Q8_0.gguf -ngl 33 -mg 0 -b 1 -p \"solve the 3 ants and triangel puzzle\"\r\nbatch size changed for different execution.\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-08-20T04:34:26+00:00",
    "closed_at": "2024-10-06T01:07:32+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9097/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9097"
  },
  {
    "number": 1494,
    "title": "[Enhancement] Simultaneous CLBLAS/CUBLAS instances. ",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ ]x I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Enhancement\r\n\r\nIf not already possible through a config I missed, would offloading some layers to CLBLAS and other layers to CUBLAS be viable? Or maybe offloading layers to multiple CLBLAS devices?\r\n\r\nA common hardware config is a CPU with an IGP + discrete gpu, and this would allow the IGP to be utilized on systems with weak CPUs and low-vram dGPUs. And much more powerful, 4 channel IGPs are rumored to be in development at Intel/AMD.\r\n\r\nWith the extra transfers and possible CPU bandwidth starvation, this may or may not even improve performance much... I'm not sure.",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-05-17T03:14:18+00:00",
    "closed_at": "2024-04-09T01:09:02+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1494/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1494"
  },
  {
    "number": 7076,
    "title": "Using #pragma once makes it difficult",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\n\r\nIt would be helpful to use\r\n ```\r\n#ifndef GGML_H\r\n#define GGML_H\r\n...\r\n#endif ```\r\n\r\nInstead of \"#pragma once\"\r\n\r\nI'm noticing if I include say llama.cpp and whisper.cpp as git submodules in my project then these #pragma once directive do not correctly avoid including the file twice...  Using a GGML_H handles all cases... i'm using latest mac environment too... it's a little more typing in each file but would help make the code easier to use in more environments... thanks!",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-04T15:30:23+00:00",
    "closed_at": "2024-06-19T01:06:39+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7076/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7076"
  },
  {
    "number": 13854,
    "title": "Eval bug: Embeddings Always returned as non",
    "body": "### Name and Version\nMac os:\nllama-cli --version\nversion: 5390 (aa48e373)\nbuilt with Apple clang version 16.0.0 (clang-1600.0.26.6) for arm64-apple-darwin23.6.0\n\nUbuntu os:\n./llama-cli --version\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla T4, compute capability 7.5, VMM: yes\nload_backend: loaded CUDA backend from /app/libggml-cuda.so\nload_backend: loaded CPU backend from /app/libggml-cpu-haswell.so\nversion: 5332 (7c28a74e)\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n\n### Operating systems\n\nMac,Ubuntu\n\n### GGML backends\n\nMetal,Cuda\n\n### Hardware\n\nGPUs\n\n### Models\n\nall models\n\n### Problem description & steps to reproduce\n\nwhen i run llama-server with embeddings enabled i got null for all embeddings vectors, and when try to use the cli i got the same result\n\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 8 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \nmain: last token in the prompt is not SEP\nmain: 'tokenizer.ggml.add_eos_token' should be set to 'true' in the GGUF header\nbatch_decode: n_tokens = 9, n_seq = 1\n\nembedding 0:       nan       nan       nan  ...       nan       nan       nan \nembedding 1:       nan       nan       nan  ...       nan       nan       nan \nembedding 2:       nan       nan       nan  ...       nan       nan       nan \nembedding 3:       nan       nan       nan  ...       nan       nan       nan \nembedding 4:       nan       nan       nan  ...       nan       nan       nan \nembedding 5:       nan       nan       nan  ...       nan       nan       nan \nembedding 6:       nan       nan       nan  ...       nan       nan       nan \nembedding 7:       nan       nan       nan  ...       nan       nan       nan \nembedding 8:       nan       nan       nan  ...       nan       nan       nan \n\nApi response when using /embedding end point: \n(.venv) (\u2388|redz-gpu:redz) \u279c  flink python3 llm_embeddings.py\n/Users/homyt-devops/Sync/flink/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn(\n\u2705 Status Code: 200\n\ud83d\udce6 Raw JSON Response:\n[\n  {\n    \"index\": 0,\n    \"embedding\": [\n      [\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n      ]\n    ]\n  }\n]\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-05-28T11:48:26+00:00",
    "closed_at": "2025-07-12T01:08:20+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13854/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13854"
  },
  {
    "number": 3051,
    "title": "Multi-GPU support for AMD?",
    "body": "Do you have multi-GPU support for AMD, if not, do you see it as something you might add in the future?",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-09-07T00:15:22+00:00",
    "closed_at": "2024-06-12T01:06:50+00:00",
    "comments": 28,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3051/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3051"
  },
  {
    "number": 9666,
    "title": "Bug: Issue building hipBLAS error: call to undeclared function '_mm256_dpbusd_epi32'",
    "body": "### What happened?\r\n\r\nHi, \r\n\r\nI'm trying to compile llama with the hipBLAS backed on \r\nCPU: 12th Gen Intel(R) Core(TM) i9-12900K\r\nGPU: AMD Radeon PRO W7800 (gfx1100)\r\nOS: Windows 11 23H2\r\nWith AMD HIP SDK 6.1.2 for Windows Installed:\r\nhttps://www.amd.com/en/developer/resources/rocm-hub/eula/licenses.html?filename=AMD-Software-PRO-Edition-24.Q3-Win10-Win11-For-HIP.exe\r\n\r\nllama.cpp version: https://github.com/ggerganov/llama.cpp/releases/tag/b3828\r\n\r\nWhen I run these commands\r\n```\r\nset PATH=%HIP_PATH%\\bin;%PATH%\r\ncmake -S . -B build -G Ninja -DAMDGPU_TARGETS=gfx1100 -DGGML_HIPBLAS=ON -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DCMAKE_BUILD_TYPE=Release\r\ncmake --build build\r\n```\r\n\r\nI get this error message, and the build is unable to continue:\r\n```\r\nllama.cpp-b3828/ggml/src/ggml-quants.c:107:34: error: call to undeclared function '_mm256_dpbusd_epi32'; ISO C99 and later do not support implicit function declarations [-Wimplicit-function-declaration]\r\n    const __m256i summed_pairs = _mm256_dpbusd_epi32(zero, ax, sy);\r\n                                 ^\r\nllama.cpp-b3828/ggml/src/ggml-quants.c:107:19: error: initializing 'const __m256i' (vector of 4 'long long' values) with an expre\r\n```\r\n\r\nCan someone tell me what I'm doing wrong?\r\n\r\n### Name and Version\r\n\r\nCPU: 12th Gen Intel(R) Core(TM) i9-12900K\r\nGPU: AMD Radeon PRO W7800 (gfx1100)\r\nOS: Windows 11 23H2\r\nWith AMD HIP SDK 6.1.2 for Windows Installed:\r\nhttps://www.amd.com/en/developer/resources/rocm-hub/eula/licenses.html?filename=AMD-Software-PRO-Edition-24.Q3-Win10-Win11-For-HIP.exe\r\n\r\nllama.cpp version: https://github.com/ggerganov/llama.cpp/releases/tag/b3828\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nWindows\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n[1/245] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.obj\r\nFAILED: ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.obj \r\nccache C:\\PROGRA~1\\AMD\\ROCm\\5.5\\bin\\clang.exe -DGGML_BUILD -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CUDA -DGGML_USE_HIPBLAS -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -DK_QUANTS_PER_ITERATION=2 -D_CRT_SECURE_NO_WARNINGS -D_XOPEN_SOURCE=600 -D__HIP_PLATFORM_AMD__=1 -D__HIP_PLATFORM_HCC__=1 -Dggml_EXPORTS -IC:/Users/owen/Desktop/Owen/llama.cpp-b3828/ggml/src/../include -IC:/Users/owen/Desktop/Owen/llama.cpp-b3828/ggml/src/. -isystem \"C:/Program Files/AMD/ROCm/5.5/include\" -O3 -DNDEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrt -std=gnu11 -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -march=native -fopenmp=libomp -MD -MT ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.obj -MF ggml\\src\\CMakeFiles\\ggml.dir\\ggml-quants.c.obj.d -o ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.obj -c C:/Users/owen/Desktop/Owen/llama.cpp-b3828/ggml/src/ggml-quants.c\r\nIn file included from C:/Users/owen/Desktop/Owen/llama.cpp-b3828/ggml/src/ggml-quants.c:4:\r\nIn file included from C:/Users/owen/Desktop/Owen/llama.cpp-b3828/ggml/src/./ggml-quants.h:4:\r\nC:/Users/owen/Desktop/Owen/llama.cpp-b3828/ggml/src/./ggml-common.h:62:9: warning: keyword is hidden by macro definition [-Wkeyword-macro]\r\n#define static_assert(cond, msg) _Static_assert(cond, msg)\r\n        ^\r\nC:/Users/owen/Desktop/Owen/llama.cpp-b3828/ggml/src/ggml-quants.c:107:34: error: call to undeclared function '_mm256_dpbusd_epi32'; ISO C99 and later do not support implicit function declarations [-Wimplicit-function-declaration]\r\n    const __m256i summed_pairs = _mm256_dpbusd_epi32(zero, ax, sy);\r\n                                 ^\r\nC:/Users/owen/Desktop/Owen/llama.cpp-b3828/ggml/src/ggml-quants.c:107:19: error: initializing 'const __m256i' (vector of 4 'long long' values) with an expression of incompatible type 'int'\r\n    const __m256i summed_pairs = _mm256_dpbusd_epi32(zero, ax, sy);\r\n                  ^              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n1 warning and 2 errors generated.\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-09-27T16:37:54+00:00",
    "closed_at": "2024-11-12T01:08:45+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9666/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9666"
  },
  {
    "number": 9630,
    "title": "Do llama.cpp support input_embeds?",
    "body": "Do llama.cpp support input_embeds? Just like `transformers` support `input_embeds` in `model.generate` function.",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-09-24T14:53:16+00:00",
    "closed_at": "2024-11-09T01:07:02+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9630/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9630"
  },
  {
    "number": 9086,
    "title": "Feature Request: Tensor Parallelism support",
    "body": "### Prerequisites\r\n\r\n- [X] I am running the latest code. Mention the version if possible as well.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\r\n\r\n### Feature Description\r\n\r\nTensor parallelism is a a critical technique employed to train and inference from very large language models by splitting the actual computations/tensors across multiple compute devices. \r\n\r\n### Motivation\r\n\r\nIn our previous implementation on Xeon CPU, tensor parallelism(TP) can significantly reduce the latency on inference. <html xmlns:v=\"urn:schemas-microsoft-com:vml\"\r\nxmlns:o=\"urn:schemas-microsoft-com:office:office\"\r\nxmlns:x=\"urn:schemas-microsoft-com:office:excel\"\r\nxmlns=\"http://www.w3.org/TR/REC-html40\">\r\n\r\n<head>\r\n\r\n<meta name=ProgId content=Excel.Sheet>\r\n<meta name=Generator content=\"Microsoft Excel 15\">\r\n<link id=Main-File rel=Main-File\r\nhref=\"file:///C:/Users/chen13/AppData/Local/Temp/msohtmlclip1/01/clip.htm\">\r\n<link rel=File-List\r\nhref=\"file:///C:/Users/chen13/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml\">\r\n<style>\r\n<!--table\r\n\t{mso-displayed-decimal-separator:\"\\.\";\r\n\tmso-displayed-thousand-separator:\"\\,\";}\r\n@page\r\n\t{margin:.75in .7in .75in .7in;\r\n\tmso-header-margin:.3in;\r\n\tmso-footer-margin:.3in;}\r\ntr\r\n\t{mso-height-source:auto;}\r\ncol\r\n\t{mso-width-source:auto;}\r\nbr\r\n\t{mso-data-placement:same-cell;}\r\ntd\r\n\t{padding-top:1px;\r\n\tpadding-right:1px;\r\n\tpadding-left:1px;\r\n\tmso-ignore:padding;\r\n\tcolor:black;\r\n\tfont-size:11.0pt;\r\n\tfont-weight:400;\r\n\tfont-style:normal;\r\n\ttext-decoration:none;\r\n\tfont-family:Calibri, sans-serif;\r\n\tmso-font-charset:0;\r\n\tmso-number-format:General;\r\n\ttext-align:general;\r\n\tvertical-align:bottom;\r\n\tborder:none;\r\n\tmso-background-source:auto;\r\n\tmso-pattern:auto;\r\n\tmso-protection:locked visible;\r\n\twhite-space:nowrap;\r\n\tmso-rotate:0;}\r\n-->\r\n</style>\r\n</head>\r\n\r\n<body link=\"#0563C1\" vlink=\"#954F72\">\r\n\r\n\r\nmodel | precision | TP size | input_size | nex_token_time/ms\r\n-- | -- | -- | -- | --\r\nllama2-70b | q4_j | 1 | 32 | 191.91\r\nllama2-70b | q4_j | 2 | 32 | 120.87\r\nllama2-70b | q4_j | 4 | 32 | 86.15\r\nllama2-70b | q4_j | 1 | 1024 | 197.18\r\nllama2-70b | q4_j | 2 | 1024 | 129.25\r\nllama2-70b | q4_j | 4 | 1024 | 91.76\r\nllama2-70b | q4_j | 1 | 2012 | 204.85\r\nllama2-70b | q4_j | 2 | 2012 | 127.31\r\nllama2-70b | q4_j | 4 | 2012 | 100.44\r\n\r\n\r\n\r\n</body>\r\n\r\n</html>\r\n\r\nNotice: TP size= 1 means not use TP.\r\n\r\n### Possible Implementation\r\n\r\nIn our TP implementation, we adopt the method of pre-splitting the corresponding weights, so the time consumed for this part is one-time and does not affect inference performance. Meanwhile, another major factor impacting performance is 'all reduce'. Since each node computes partial and incomplete results, it is necessary to perform 'all reduce' on the output data. But all reduce is relatively time-consuming, interestingly, by using a reasonable splitting and combining method, primitives can be operated independently across nodes, which is very helpful for performance optimization. Thus, a rational splitting method becomes extremely important.\r\n\r\nTaking the FFN module as an example, if the first matmul splits by column and computes the matmul with input, it will result in two unrelated sub-matrices on each node. These two sub-matrices, when performing the second matmul operation, can proceed directly without having to perform 'all reduce' if splitting by rows. Thus, the entire FFN module only requires one 'all reduce', meaning that with properly tailored split implementation, even with multiple matmul operations, only one 'all reduce' operation may be needed. We ignored the element-wise operations between matmul as they would not influence the results.\r\n![image](https://github.com/user-attachments/assets/8a9d6c4a-45ca-4fa7-9930-1660936fda90)\r\nThe scenario for the attention module is more complex. As shown in the following figure, a rational split can make it so that the entire attention module only requires one 'all reduce' operation, thus greatly saving synchronization time.\r\n![image](https://github.com/user-attachments/assets/19d77152-4dff-4b8e-a3e2-34d582ce3b53)\r\n",
    "labels": [
      "enhancement",
      "threading",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-19T01:38:13+00:00",
    "closed_at": "2024-12-13T01:07:40+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9086/reactions",
      "total_count": 11,
      "+1": 11,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9086"
  },
  {
    "number": 6571,
    "title": "b2447 (c47cf41) decreased output quality",
    "body": "With identical seeds and options, b2447 (https://github.com/ggerganov/llama.cpp/commit/c47cf414efafb8f60596edc7edb5a2d68065e992) produces different output that seems lower in quality compared to b2446. Is it possible to preserve old output quality in new builds?\r\n\r\nSystem: MacBook Pro w/ i5-1038NG7",
    "labels": [
      "need more info",
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-04-09T18:50:49+00:00",
    "closed_at": "2024-05-24T13:29:29+00:00",
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6571/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6571"
  },
  {
    "number": 4575,
    "title": "make process hangs if LLAMA_CUBLAS=1, at the line that includes the file scripts/get-flags.mk for the second time",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI run make LLAMA_CUBLAS=1 and that process hangs. I used make --debug=f to figure out that make gets stuck at the line that includes get-flags.mk for the second time (it is already included a few lines before). \r\n\r\n# Environment and Context\r\n\r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\n|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:00:05.0 Off |                    0 |\r\n| N/A   42C    P0              86W / 400W |    591MiB / 81920MiB |      0%      Default |\r\n|                                         |                      |             Disabled |\r\n+-----------------------------------------+----------------------+----------------------+\r\n\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n`$ lscpu`\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         46 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  12\r\n  On-line CPU(s) list:   0-11\r\nVendor ID:               GenuineIntel\r\n  Model name:            Intel(R) Xeon(R) Gold 6342 CPU @ 2.80GHz\r\n    CPU family:          6\r\n    Model:               106\r\n    Thread(s) per core:  1\r\n    Core(s) per socket:  12\r\n    Socket(s):           1\r\n    Stepping:            6\r\n    BogoMIPS:            5600.21\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush acpi mmx fxsr sse sse2 s\r\n                         s ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl cpuid tsc_known_freq pni pclmulqdq ssse3 fm\r\n                         a cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor la\r\n                         hf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjus\r\n                         t bmi1 avx2 smep bmi2 erms invpcid rtm rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xs\r\n                         aves umip pku ospke gfni vaes vpclmulqdq rdpid md_clear flush_l1d arch_capabilities\r\nVirtualization features:\r\n  Hypervisor vendor:     Xen\r\n  Virtualization type:   full\r\n\r\nCaches (sum of all):\r\n  L1d:                   576 KiB (12 instances)\r\n  L1i:                   384 KiB (12 instances)\r\n  L2:                    15 MiB (12 instances)\r\n  L3:                    432 MiB (12 instances)\r\nNUMA:\r\n  NUMA node(s):          1\r\n  NUMA node0 CPU(s):     0-11\r\nVulnerabilities:\r\n  Gather data sampling:  Not affected\r\n  Itlb multihit:         Not affected\r\n  L1tf:                  Not affected\r\n  Mds:                   Not affected\r\n  Meltdown:              Not affected\r\n  Mmio stale data:       Mitigation; Clear CPU buffers; SMT Host state unknown\r\n  Retbleed:              Not affected\r\n  Spec rstack overflow:  Not affected\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\n  Spectre v2:            Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\n  Srbds:                 Not affected\r\n  Tsx async abort:       Not affected\r\n\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`$ uname -a`\r\n\r\nLinux psbngks26deu 6.2.0-39-generic #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2 x86_64 x86_64 x86_64 GNU/Linux \r\n\r\n* SDK version, e.g. for Linux:\r\nPython 3.11.7\r\nGNU Make 4.3\r\nBuilt for x86_64-pc-linux-gnu\r\ng++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\n\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-21T21:18:17+00:00",
    "closed_at": "2024-04-02T01:10:16+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4575/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4575"
  },
  {
    "number": 8612,
    "title": "Bug:  Recent changes break Rocm compile on windows",
    "body": "### What happened?\n\nIt cannot compile after CUDA: MMQ code deduplication + iquant support on windows.\r\nthat's my guess that pr break compile.\n\n### Name and Version\n\nb3428 and Windows 11 rocm5.7.1\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n```shell\ncmake --build . -j 99 --parallel 32 --config Release\r\n[1/61] Linking CXX shared library bin\\ggml.dll\r\nFAILED: bin/ggml.dll ggml/src/ggml.lib\r\ncmd.exe /C \"cmd.exe /C \"C:\\Strawberry\\c\\bin\\cmake.exe -E __create_def W:\\git\\llama.cpp\\rocm_1100\\ggml\\src\\CMakeFiles\\ggml.dir\\.\\exports.def W:\\git\\llama.cpp\\rocm_1100\\ggml\\src\\CMakeFiles\\ggml.dir\\.\\exports.def.objs --nm=C:\\Strawberry\\c\\bin\\nm.exe && cd W:\\git\\llama.cpp\\rocm_1100\" && C:\\PROGRA~1\\AMD\\ROCm\\5.7\\bin\\CLANG_~1.EXE -fuse-ld=lld-link -nostartfiles -nostdlib -O3 -DNDEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrt  -Xlinker /DEF:ggml\\src\\CMakeFiles\\ggml.dir\\.\\exports.def -shared -o bin\\ggml.dll  -Xlinker /MANIFEST:EMBED -Xlinker /implib:ggml\\src\\ggml.lib -Xlinker /pdb:bin\\ggml.pdb -Xlinker /version:0.0 ggml/src/CMakeFiles/ggml.dir/ggml.c.obj ggml/src/CMakeFiles/ggml.dir/ggml-alloc.c.obj ggml/src/CMakeFiles/ggml.dir/ggml-backend.c.obj ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/conv-transpose-1d.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/fattn-tile-f16.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/fattn-tile-f32.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb16.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb32.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb16.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb32.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb8.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-iq4_nl.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-iq4_xs.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q2_k.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q3_k.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q4_0.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q4_1.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q4_k.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q5_0.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q5_1.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q5_k.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q6_k.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q8_0.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.obj ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.obj ggml/src/CMakeFiles/ggml.dir/llamafile/sgemm.cpp.obj ggml/src/CMakeFiles/ggml.dir/ggml-aarch64.c.obj  \"C:/Program Files/AMD/ROCm/5.7/lib/hipblas.lib\"  --hip-link  --offload-arch=gfx1100  \"C:/Program Files/AMD/ROCm/5.7/lib/rocblas.lib\"  \"C:/Program Files/AMD/ROCm/5.7/lib/clang/17.0.0/lib/windows/clang_rt.builtins-x86_64.lib\"  \"C:/Program Files/AMD/ROCm/5.7/lib/amdhip64.lib\"  -lkernel32 -luser32 -lgdi32 -lwinspool -lshell32 -lole32 -loleaut32 -luuid -lcomdlg32 -ladvapi32 -loldnames  && cd .\"\r\nlld-link: error: undefined symbol: void __cdecl mul_mat_q_case<16>(struct ggml_backend_cuda_context &, struct mmq_args const &, struct ihipStream_t *)\r\n>>> referenced by ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.obj:(void __cdecl ggml_cuda_op_mul_mat_q(struct ggml_backend_cuda_context &, struct ggml_tensor const *, struct ggml_tensor const *, struct ggml_tensor *, char const *, float const *, char const *, float *, __int64, __int64, __int64, __int64, struct ihipStream_t *))\r\n\r\nlld-link: error: undefined symbol: void __cdecl mul_mat_q_case<17>(struct ggml_backend_cuda_context &, struct mmq_args const &, struct ihipStream_t *)\r\n>>> referenced by ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.obj:(void __cdecl ggml_cuda_op_mul_mat_q(struct ggml_backend_cuda_context &, struct ggml_tensor const *, struct ggml_tensor const *, struct ggml_tensor *, char const *, float const *, char const *, float *, __int64, __int64, __int64, __int64, struct ihipStream_t *))\r\n\r\nlld-link: error: undefined symbol: void __cdecl mul_mat_q_case<18>(struct ggml_backend_cuda_context &, struct mmq_args const &, struct ihipStream_t *)\r\n>>> referenced by ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.obj:(void __cdecl ggml_cuda_op_mul_mat_q(struct ggml_backend_cuda_context &, struct ggml_tensor const *, struct ggml_tensor const *, struct ggml_tensor *, char const *, float const *, char const *, float *, __int64, __int64, __int64, __int64, struct ihipStream_t *))\r\n\r\nlld-link: error: undefined symbol: void __cdecl mul_mat_q_case<19>(struct ggml_backend_cuda_context &, struct mmq_args const &, struct ihipStream_t *)\r\n>>> referenced by ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.obj:(void __cdecl ggml_cuda_op_mul_mat_q(struct ggml_backend_cuda_context &, struct ggml_tensor const *, struct ggml_tensor const *, struct ggml_tensor *, char const *, float const *, char const *, float *, __int64, __int64, __int64, __int64, struct ihipStream_t *))\r\n\r\nlld-link: error: undefined symbol: void __cdecl mul_mat_q_case<21>(struct ggml_backend_cuda_context &, struct mmq_args const &, struct ihipStream_t *)\r\n>>> referenced by ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.obj:(void __cdecl ggml_cuda_op_mul_mat_q(struct ggml_backend_cuda_context &, struct ggml_tensor const *, struct ggml_tensor const *, struct ggml_tensor *, char const *, float const *, char const *, float *, __int64, __int64, __int64, __int64, struct ihipStream_t *))\r\n\r\nlld-link: error: undefined symbol: void __cdecl mul_mat_q_case<22>(struct ggml_backend_cuda_context &, struct mmq_args const &, struct ihipStream_t *)\r\n>>> referenced by ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.obj:(void __cdecl ggml_cuda_op_mul_mat_q(struct ggml_backend_cuda_context &, struct ggml_tensor const *, struct ggml_tensor const *, struct ggml_tensor *, char const *, float const *, char const *, float *, __int64, __int64, __int64, __int64, struct ihipStream_t *))\r\nCLANG_~1: error: linker command failed with exit code 1 (use -v to see invocation)\r\nninja: build stopped: subcommand failed.\n```\n",
    "labels": [
      "bug-unconfirmed",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-07-21T08:53:56+00:00",
    "closed_at": "2024-07-21T14:39:23+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8612/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8612"
  },
  {
    "number": 7112,
    "title": "Server: Multimodal Model Input Parameter No longer Exists",
    "body": "I have noticed that when using the server that the --mmproj parameter for multimodal models has been disabled. Although it still remains in the README. Is there an alternative to --mmproj , I cannot seem to find one in the code. \r\n\r\nAny help on this would be great. \r\n\r\nCode to reproduce:\r\n`./server -m ./ggml-model-q4_k.gguf --mmproj ./mmproj-model-f16.gguf -ngl 1`\r\n\r\nError:\r\n`\r\nerror: unknown argument: --mmproj\r\nusage: ./server [options]\r\n`",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-07T04:11:00+00:00",
    "closed_at": "2024-07-18T01:06:49+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7112/reactions",
      "total_count": 5,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 1,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7112"
  },
  {
    "number": 13549,
    "title": "Misc. bug: Potential out of bound in rerank",
    "body": "### Name and Version\n\nversion: 5387 (3198405e)\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n\n### Operating systems\n\n_No response_\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Command line\n\n```shell\n\n```\n\n### Problem description & steps to reproduce\n\n[llama_context](https://github.com/ggml-org/llama.cpp/blob/f5170c1d7a66222ca7c75d2022fec3ed87257e0b/src/llama-context.cpp#L807) resize the rerank output to size 1 while [here](https://github.com/ggml-org/llama.cpp/blob/017f10b5fa630a013ec4f9936e410a60d4f460d5/examples/embedding/embedding.cpp#L69) we still normalize it as if we have full embedding vector. I found this problem happened randomly in python binding but cannot reproduce it in cpp. Not sure if it is a bug in cpp side.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-05-14T19:50:38+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13549/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13549"
  },
  {
    "number": 13102,
    "title": "Misc. bug: Retrieval sample not decoding token successfully",
    "body": "### Name and Version\n\nversion: 5184 (87616f06)\nbuilt with MSVC 19.41.34120.0 for x64\n\n### Operating systems\n\nMac, Windows\n\n### Which llama.cpp modules do you know to be affected?\n\nOther (Please specify in the next section)\n\n### Command line\n\n```shell\nllama-retrieval.exe --context-file <any_text_file> --chunk-size 1 -c 512 -t 8 -m bge-large-en-v1.5-f32.gguf\n```\n\n### Problem description & steps to reproduce\n\nThe sample failed to decode any tokens created from the text embeddings.\n\nIt looks like  we need to skip the kv-cache logic to look for an unused slot when pooling is active (which is true for the above model).\n\nThe following IF in llama-context.cpp is removed, causing us to go into this logic to search for an unused slot and hit the decoding spew.\n\n        // non-causal masks do not use the KV cache\n        if (hparams.causal_attn) {\n            kv_self_update();\n\nJust adding \"if (!embd_pooling)\" appears to fix the issue but I am not sure what it does to the original logic for the non-causal mask with gemma-3.\n\n### First Bad Commit\n\nhttps://github.com/ggml-org/llama.cpp/pull/12615/commits/bed4c73a51f0ddc0c0c95c7710749f674c9c204c\n\n### Relevant log output\n\n```shell\nllama-retrieval.exe --context-file <any_text_file> --chunk-size 1 -c 512 -t 8 -m bge-large-en-v1.5-f32.gguf\n...\ninit:        CPU KV buffer size =    48.00 MiB\nllama_context: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB\nllama_context:        CPU compute buffer size =    27.01 MiB\nllama_context: graph nodes  = 825\nllama_context: graph splits = 1\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 512\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n\nsystem_info: n_threads = 8 (n_threads_batch = 8) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |\nbatch_decode: n_tokens = 2043, n_seq = 118\nfind_slot: n_tokens = 2043 > size = 512\ndecode: failed to find KV cache slot for ubatch of size 2043\nllama_decode: failed to decode, ret = 1\nget_embeddings_ith: invalid embeddings id 0, reason: no embeddings\nbatch_decode: failed to get embeddings for token 0\nget_embeddings_ith: invalid embeddings id 1, reason: no embeddings\nbatch_decode: failed to get embeddings for token 1\nget_embeddings_ith: invalid embeddings id 2, reason: no embeddings\nbatch_decode: failed to get embeddings for token 2\nget_embeddings_ith: invalid embeddings id 3, reason: no embeddings\nbatch_decode: failed to get embeddings for token 3\n...\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-04-24T22:26:15+00:00",
    "closed_at": "2025-06-08T01:08:06+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13102/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13102"
  },
  {
    "number": 7450,
    "title": "Since last update Mistral models doesn't works anymore",
    "body": "since this https://github.com/ggerganov/llama.cpp/tree/b2961\r\nphi3-128k works better (if ctx <32k)\r\nbut mistral models are crazy, I tried 7bQ2 7bQ8, 70BQ2XS, none of them works anymore\r\n\r\n```\r\nLog start\r\nmain\r\nmain: build = 2961 (201cc11a)\r\nmain: built with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\r\nmain: seed  = 1716349593\r\nllama_model_loader\r\nllama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from models/mistral-7b-instruct-v0.2.Q2_K.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 10\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\r\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q2_K:   65 tensors\r\nllama_model_loader: - type q3_K:  160 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q2_K - Medium\r\nllm_load_print_meta: model params     = 7.24 B\r\nllm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) \r\nllm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nggml_cuda_init\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\r\nggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\r\nggml_cuda_init: found 1 CUDA devices:\r\nDevice 0\r\n  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\r\nllm_load_tensors\r\nllm_load_tensors: ggml ctx size =    0.30 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:        CPU buffer size =    41.02 MiB\r\nllm_load_tensors:      CUDA0 buffer size =  2898.55 MiB\r\n..................................................................................................\r\nllama_new_context_with_model\r\nllama_new_context_with_model: n_ctx      = 32000\r\nllama_new_context_with_model: n_batch    = 1024\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init\r\nllama_kv_cache_init:      CUDA0 KV buffer size =  4000.00 MiB\r\nllama_new_context_with_model\r\nllama_new_context_with_model: KV self size  = 4000.00 MiB, K (f16): 2000.00 MiB, V (f16): 2000.00 MiB\r\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =  2094.50 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =    70.51 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 2\r\n\r\nsystem_info\r\nsystem_info: n_threads = 32 / 32 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \r\nllama_tokenize_internal\r\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\r\nsampling: \r\n\trepeat_last_n = 256, repeat_penalty = 1.176, frequency_penalty = 0.000, presence_penalty = 0.000\r\n\ttop_k = 40, tfs_z = 1.000, top_p = 0.500, min_p = 0.050, typical_p = 1.000, temp = 0.500\r\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\r\nsampling order: \r\nCFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \r\ngenerate\r\ngenerate: n_ctx = 32000, n_batch = 1024, n_predict = 8192, n_keep = 1\r\n\r\n\r\n\r\nllama_print_timings\r\nllama_print_timings:        load time =     622.18 ms\r\nllama_print_timings:      sample time =     907.92 ms /  8192 runs   (    0.11 ms per token,  9022.83 tokens per second)\r\nllama_print_timings: prompt eval time =      37.07 ms /    25 tokens (    1.48 ms per token,   674.40 tokens per second)\r\nllama_print_timings:        eval time =   53408.73 ms /  8191 runs   (    6.52 ms per token,   153.36 tokens per second)\r\nllama_print_timings:       total time =   55902.60 ms /  8216 tokens\r\nLog end\r\n```\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-05-22T03:53:03+00:00",
    "closed_at": "2024-05-22T10:03:22+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7450/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 1,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7450"
  },
  {
    "number": 7311,
    "title": "ggml_validate_row_data finding nan value for IQ4_NL",
    "body": "Using b2854\r\n\r\nConverted Hermes-2-Theta-Llama-3-8B to F32, then measured imatrix with https://gist.github.com/bartowski1182/b6ac44691e994344625687afe3263b3a\r\n\r\nUpon quanting, all sizes work fine, except for IQ4_NL which produces this output:\r\n\r\n```\r\nload_imatrix: imatrix dataset='/training_data/calibration_data.txt'\r\nload_imatrix: loaded 224 importance matrix entries from /models/Hermes-2-Theta-Llama-3-8B-GGUF/Hermes-2-Theta-Llama-3-8B.imatrix computed on 189 chunks\r\nprepare_imatrix: have 224 importance matrix entries\r\nmain: build = 2854 (72c177c1)\r\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\r\nmain: quantizing '/models/Hermes-2-Theta-Llama-3-8B-GGUF/Hermes-2-Theta-Llama-3-8B-f32.gguf' to '/models/Hermes-2-Theta-Llama-3-8B-GGUF/Hermes-2-Theta-Llama-3-8B-IQ4_NL.gguf' as IQ4_NL\r\nllama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /models/Hermes-2-Theta-Llama-3-8B-GGUF/Hermes-2-Theta-Llama-3-8B-f32.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = Hermes-2-Theta-Llama-3-8B\r\nllama_model_loader: - kv   2:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   3:                       llama.context_length u32              = 8192\r\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 0\r\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\r\nllama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"\u0120 \u0120\", \"\u0120 \u0120\u0120\u0120\", \"\u0120\u0120 \u0120\u0120\", \"...\r\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\r\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128003\r\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\r\nllama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{bos_token}}{% for message in messag...\r\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  291 tensors\r\n================================ Have weights data with 224 entries\r\n[   1/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f32,\r\n====== llama_model_quantize_internal: did not find weights for token_embd.weight\r\nconverting to iq4_nl .. ggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 128\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 384\r\nggml_validate_row_data: found nan value at block 128\r\nggml_validate_row_data: found nan value at block 128\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 128\r\nggml_validate_row_data: found nan value at block 384\r\nggml_validate_row_data: found nan value at block 256\r\nggml_validate_row_data: found nan value at block 256\r\nggml_validate_row_data: found nan value at block 384\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 256\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 384\r\nggml_validate_row_data: found nan value at block 128\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 128\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nggml_validate_row_data: found nan value at block 0\r\nllama_model_quantize: failed to quantize: quantized data validation failed\r\nmain: failed to quantize model from '/models/Hermes-2-Theta-Llama-3-8B-GGUF/Hermes-2-Theta-Llama-3-8B-f32.gguf'\r\n```\r\n\r\nWhen I refer to \"all quants\" I mean these all work fine:\r\n\r\nIQ1_S, IQ1_M, IQ2_XXS, IQ2_XS, IQ2_S, IQ2_M, Q2_K, IQ3_XXS, IQ3_XS, IQ3_S, IQ3_M, Q3_K_S, Q3_K_M, Q3_K_L, IQ4_XS, Q4_K_S, Q4_K_M, Q5_K_S, Q5_K_M, Q6_K, Q8_0",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-05-15T19:40:10+00:00",
    "closed_at": "2024-05-18T00:39:55+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7311/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7311"
  },
  {
    "number": 13614,
    "title": "Compile bug: tools build failing",
    "body": "### Git commit\n\nCommit 6a2bc8b\n\n\n### Operating systems\n\nWindows\n\n### GGML backends\n\nCUDA\n\n### Problem description & steps to reproduce\n\nEnvironment: Windows 11 with CUDA toolkit installed.\n\nSorry, new to this. I tried searching if there was already a solution but couldn't find anything with my limited domain of knowledge.\n\nI followed the guide to build llama.cpp with CUDA support which seems to worked as it built a few binaries that I can see in the bin/Release folder, but I noticed none of the tools were built. I.g. cli, server etc...\n\nAlso, my environment was missing CURL libraries, so I had to look it up and install a windows version. And issued the following to build this:\n\n```\ncmake -B build -DGGML_CUDA=ON -DCURL_LIBRARY=c:\\Curl\\lib\\libcurl.a -DCURL_INCLUDE_DIR=c:\\Curl\\include\n```\n\nReading up on the llama-server docs, I saw there was a way to build it so I tried it but I got this error:\n```\ncommon.lib(arg.obj) : error LNK2019: unresolved external symbol __imp_curl_slist_appe\nnd referenced in function \"bool __cdecl common_download_file_single(class std::basic_\nstring<char,struct std::char_traits<char>,class std::allocator<char> > const &,class\nstd::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > con\nst &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<\nchar> > const &)\" (?common_download_file_single@@YA_NAEBV?$basic_string@DU?$char_trai\nts@D@std@@V?$allocator@D@2@@std@@00@Z)\n```\nAnd a bunch related to curl. \n\n### First Bad Commit\n\n_No response_\n\n### Compile command\n\n```shell\ncmake -B build -DGGML_CUDA=ON -DCURL_LIBRARY=c:\\Curl\\lib\\libcurl.a -DCURL_INCLUDE_DIR=c:\\Curl\\include\n```\n\n### Relevant log output\n\n```shell\ncommon.lib(arg.obj) : error LNK2019: unresolved external symbol __imp_curl_slist_appe\nnd referenced in function \"bool __cdecl common_download_file_single(class std::basic_\nstring<char,struct std::char_traits<char>,class std::allocator<char> > const &,class\nstd::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > con\nst &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<\nchar> > const &)\" (?common_download_file_single@@YA_NAEBV?$basic_string@DU?$char_trai\nts@D@std@@V?$allocator@D@2@@std@@00@Z)\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-05-18T14:26:43+00:00",
    "closed_at": "2025-07-02T01:07:53+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13614/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13614"
  },
  {
    "number": 6031,
    "title": "Segmentation fault during inference on AMD gfx900 with codebooga-34b-v0.1.Q5_K_M.gguf",
    "body": "Hi,\r\n\r\nI compiled `llama.cpp` from git, todays master HEAD `commit 8030da7afea2d89f997aeadbd14183d399a017b9` on Fedora Rawhide (ROCm 6.0.x) like this:\r\n```\r\nCC=/usr/bin/clang CXX=/usr/bin/clang++ cmake .. -DLLAMA_HIPBLAS=ON -DAMDGPU_TARGETS=gfx900 -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=\"--rocm-device-lib-path=/usr/lib/clang/17/amdgcn/bitcode\"\r\nmake -j 16\r\n```\r\n\r\nThen I tried to run a prompt using the `codebooga-34b-v0.1.Q5_K_M.gguf` model which I got from here: https://huggingface.co/TheBloke/CodeBooga-34B-v0.1-GGUF\r\n\r\nI kept the prompt simple and used the following command:\r\n./main -t 10 -ngl 16 -m ~/models/codebooga-34b-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"### Instruction: How do I get the length of a Vec in Rust?\\n### Response:\"\r\n\r\nI have an AMD Instinct MI25 card with 16GB VRAM, according to `nvtop` with `-ngl 16` about half of it is used `8.219Gi/15.984`, so this does not seem to be an OOM issue.\r\n\r\nThe console output looks like this:\r\n```\r\nLog start                                                                       \r\nmain: build = 2408 (8030da7a)\r\nmain: built with clang version 18.1.0 (Fedora 18.1.0~rc4-2.fc41) for x86_64-redhat-linux-gnu\r\nmain: seed  = 1710292844\r\n[New Thread 0x7fff074006c0 (LWP 11038)]\r\n[New Thread 0x7ffe068006c0 (LWP 11039)]\r\n[Thread 0x7ffe068006c0 (LWP 11039) exited]\r\nggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\r\nggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\r\nggml_init_cublas: found 1 ROCm devices:\r\n  Device 0: AMD Radeon Instinct MI25, compute capability 9.0, VMM: no\r\nllama_model_loader: loaded meta data with 21 key-value pairs and 435 tensors from /home/jin/Work/text-generation-webui/models/codebooga-34b-v0.1.Q5_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = oobabooga_codebooga-34b-v0.1\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 16384\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 48\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 17\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\r\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   97 tensors\r\nllama_model_loader: - type q5_K:  289 tensors\r\nllama_model_loader: - type q6_K:   49 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 16384\r\nllm_load_print_meta: n_embd           = 8192\r\nllm_load_print_meta: n_head           = 64\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 48\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 8\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 22016\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attm      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 16384\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 34B\r\nllm_load_print_meta: model ftype      = Q5_K - Medium\r\nllm_load_print_meta: model params     = 33.74 B\r\nllm_load_print_meta: model size       = 22.20 GiB (5.65 BPW) \r\nllm_load_print_meta: general.name     = oobabooga_codebooga-34b-v0.1\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 2 '</s>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.33 MiB\r\nllm_load_tensors: offloading 16 repeating layers to GPU\r\nllm_load_tensors: offloaded 16/49 layers to GPU\r\nllm_load_tensors:      ROCm0 buffer size =  7500.06 MiB\r\nllm_load_tensors:        CPU buffer size = 22733.73 MiB\r\n....................................................................................................\r\nllama_new_context_with_model: n_ctx      = 2048\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:      ROCm0 KV buffer size =   128.00 MiB\r\nllama_kv_cache_init:  ROCm_Host KV buffer size =   256.00 MiB\r\nllama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB\r\nllama_new_context_with_model:  ROCm_Host input buffer size   =    21.02 MiB\r\nggml_gallocr_reserve_n: reallocating ROCm0 buffer from size 0.00 MiB to 324.00 MiB\r\nggml_gallocr_reserve_n: reallocating ROCm_Host buffer from size 0.00 MiB to 336.00 MiB\r\nllama_new_context_with_model:      ROCm0 compute buffer size =   324.00 MiB\r\nllama_new_context_with_model:  ROCm_Host compute buffer size =   336.00 MiB\r\nllama_new_context_with_model: graph splits (measure): 3\r\n```\r\n\r\nShortly after I get a segfault, although sometimes it starts responding and crashes a few seconds into the response:\r\n\r\n```\r\n(gdb) bt\r\n#0  amd::KernelParameters::set (this=0x1d9cb10, index=11, size=4, \r\n    value=0x100000020, svmBound=false)\r\n    at /usr/src/debug/rocclr-6.0.2-1.fc41.x86_64/rocclr/platform/kernel.cpp:127\r\n#1  0x00007fffb9822b7c in ihipLaunchKernel_validate (f=f@entry=0x3281e20, \r\n    globalWorkSizeX=globalWorkSizeX@entry=4096, \r\n    globalWorkSizeY=globalWorkSizeY@entry=1, \r\n    globalWorkSizeZ=globalWorkSizeZ@entry=1, blockDimX=blockDimX@entry=32, \r\n    blockDimY=blockDimY@entry=1, blockDimZ=1, sharedMemBytes=256, \r\n    kernelParams=0x7fffffff7430, extra=0x0, deviceId=0, params=0)\r\n    at /usr/src/debug/rocclr-6.0.2-1.fc41.x86_64/hipamd/src/hip_module.cpp:301\r\n#2  0x00007fffb98273fd in ihipModuleLaunchKernel (f=0x3281e20, \r\n    globalWorkSizeX=4096, globalWorkSizeY=1, globalWorkSizeZ=1, blockDimX=32, \r\n    blockDimY=1, blockDimZ=1, sharedMemBytes=256, hStream=0x195d320, \r\n    kernelParams=0x7fffffff7430, extra=0x0, startEvent=0x0, stopEvent=0x0, \r\n    flags=0, params=0, gridId=0, numGrids=0, prevGridSum=0, allGridSum=0, \r\n    firstDevice=0)\r\n    at /usr/src/debug/rocclr-6.0.2-1.fc41.x86_64/hipamd/src/hip_module.cpp:371\r\n#3  0x00007fffb98492a2 in ihipLaunchKernel (\r\n    hostFunction=0x679308 <void soft_max_f32<true, 32, 32>(float const*, float const*, float const*, float*, int, int, float, float, float, float, unsigned int)--Type <RET> for more, q to quit, c to continue without paging--\r\n>, gridDim=..., blockDim=..., args=0x7fffffff7430, sharedMemBytes=256, \r\n    stream=0x195d320, startEvent=0x0, stopEvent=0x0, flags=0)\r\n    at /usr/src/debug/rocclr-6.0.2-1.fc41.x86_64/hipamd/src/hip_platform.cpp:584\r\n#4  0x00007fffb9822519 in hipLaunchKernel_common (\r\n    hostFunction=hostFunction@entry=0x679308 <void soft_max_f32<true, 32, 32>(float const*, float const*, float const*, float*, int, int, float, float, float, float, unsigned int)>, gridDim=..., blockDim=..., \r\n    args=args@entry=0x7fffffff7430, sharedMemBytes=256, stream=<optimized out>)\r\n    at /usr/src/debug/rocclr-6.0.2-1.fc41.x86_64/hipamd/src/hip_module.cpp:662\r\n#5  0x00007fffb9824b83 in hipLaunchKernel (hostFunction=<optimized out>, \r\n    gridDim=..., blockDim=..., args=0x7fffffff7430, \r\n    sharedMemBytes=<optimized out>, stream=<optimized out>)\r\n    at /usr/src/debug/rocclr-6.0.2-1.fc41.x86_64/hipamd/src/hip_module.cpp:669\r\n#6  0x000000000062ea50 in void __device_stub__soft_max_f32<true, 32, 32>(float const*, float const*, float const*, float*, int, int, float, float, float, float, unsigned int) ()\r\n#7  0x000000000062e1f9 in soft_max_f32_cuda (x=0x7ff65e400800, \r\n    mask=0x7ff65c000800, pos=0x0, dst=0x7ff65e400800, ncols_x=32, nrows_x=128, \r\n    nrows_y=2, scale=0.0883883461, max_bias=0, stream=0x195d320)\r\n    at /llama.cpp/ggml-cuda.cu:7505\r\n#8  0x000000000062ded6 in ggml_cuda_op_soft_max (src0=0x7ff66eca9450, \r\n    src1=0x7ff66e80ee50, dst=0x7ff66eca95e0, src0_dd=0x7ff65e400800, \r\n    src1_dd=0x7ff65c000800, dst_dd=0x7ff65e400800, main_stream=0x195d320)\r\n    at /llama.cpp/ggml-cuda.cu:9053\r\n#9  0x00000000005f98f7 in ggml_cuda_op_flatten (src0=0x7ff66eca9450, \r\n    src1=0x7ff66e80ee50, dst=0x7ff66eca95e0, \r\n    op=0x62db50 <ggml_cuda_op_soft_max(ggml_tensor const*, ggml_tensor const*, ggml_tensor*, float const*, float const*, float*, ihipStream_t*)>)\r\n    at /llama.cpp/ggml-cuda.cu:9145\r\n#10 0x00000000005f856f in ggml_cuda_soft_max (src0=0x7ff66eca9450, \r\n    src1=0x7ff66e80ee50, dst=0x7ff66eca95e0)\r\n    at /llama.cpp/ggml-cuda.cu:10393\r\n#11 0x00000000005f5cb8 in ggml_cuda_compute_forward (params=0x7fffffff7b78, \r\n    tensor=0x7ff66eca95e0) at /llama.cpp/ggml-cuda.cu:10619\r\n#12 0x0000000000635106 in ggml_backend_cuda_graph_compute (backend=0x19e1420, \r\n    cgraph=0x7ff66e8002d8) at /llama.cpp/ggml-cuda.cu:11310\r\n#13 0x00000000005c1d42 in ggml_backend_graph_compute (backend=0x19e1420, \r\n    cgraph=0x7ff66e8002d8) at /llama.cpp/ggml-backend.c:270\r\n#14 0x00000000005c55c3 in ggml_backend_sched_compute_splits (\r\n--Type <RET> for more, q to quit, c to continue without paging--\r\n    sched=0x7ff66e800010) at /llama.cpp/ggml-backend.c:1474\r\n#15 0x00000000005c5237 in ggml_backend_sched_graph_compute (\r\n    sched=0x7ff66e800010, graph=0x7ff66ec00030)\r\n    at /llama.cpp/ggml-backend.c:1597\r\n#16 0x00000000004f85e9 in llama_graph_compute (lctx=..., gf=0x7ff66ec00030, \r\n    n_threads=10) at /llama.cpp/llama.cpp:8733\r\n#17 0x00000000004b7926 in llama_decode_internal (lctx=..., batch=...)\r\n    at /llama.cpp/llama.cpp:8887\r\n#18 0x00000000004b6fc3 in llama_decode (ctx=0x19f7b60, batch=...)\r\n    at /llama.cpp/llama.cpp:13837\r\n#19 0x0000000000452e95 in llama_init_from_gpt_params (params=...)\r\n    at /llama.cpp/common/common.cpp:1380\r\n#20 0x000000000042c0a5 in main (argc=18, argv=0x7fffffffdac8)\r\n    at /llama.cpp/examples/main/main.cpp:199\r\n```\r\n\r\nI saw some issues about partial offloading and also tried a smaller model which should completely fit on my GPU, but the segfault was still there, the smaller model is this one:\r\n\r\n```\r\nllm_load_print_meta: model type       = 13B\r\nllm_load_print_meta: model ftype      = Q8_0\r\nllm_load_print_meta: model params     = 13.02 B\r\nllm_load_print_meta: model size       = 12.88 GiB (8.50 BPW) \r\nllm_load_print_meta: general.name     = newhope.ggmlv3.q8_0.bin\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.28 MiB\r\nllm_load_tensors: offloading 40 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 41/41 layers to GPU\r\nllm_load_tensors:      ROCm0 buffer size = 13023.85 MiB\r\nllm_load_tensors:        CPU buffer size =   166.02 MiB\r\n```\r\n\r\nCrashed as well with a very similar backtrace.\r\n\r\nSince this is nicely reproducible, I can provide more more info or add some debug logs as needed, please let me know what you need.",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-03-13T01:47:52+00:00",
    "closed_at": "2024-03-14T18:46:32+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6031/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6031"
  },
  {
    "number": 9009,
    "title": "Feature Request: Support Falcon Mamba 7B ",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nPlease support Falcon Mamba 7B from TII (Technology Innovation Institute TII - UAE)\n\n### Motivation\n\nSupport for all models is helpful.\r\n\r\nMy acid test for whether a model will run is to try and make a quant using \"gruff my repo\".\r\n\r\nAdmittedly it is hot off the presses yet it ought to run at least in theory, but it doesn't.\r\n```\r\nError: Error converting to fp16: b'INFO:hf-to-gguf:Loading model: falcon-mamba-7b\\nERROR:hf-to-gguf:Model FalconMambaForCausalLM is not supported\\n'\r\n```\n\n### Possible Implementation\n\nThey discuss an implementation here: https://falconllm.tii.ae/tii-releases-first-sslm-with-falcon-mamba-7b.html\r\n\r\nAny functional mamba or mamba 2 models would be great, but this one is slightly changed.",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2024-08-12T16:29:58+00:00",
    "closed_at": "2024-08-21T08:06:37+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9009/reactions",
      "total_count": 10,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 3,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9009"
  },
  {
    "number": 8240,
    "title": "Investigate gemma 2 generation quality",
    "body": "Initial reports can be seen from https://github.com/ggerganov/llama.cpp/pull/8227\r\n\r\n> [!IMPORTANT]  \r\n> A note for everyone: if you think there's a bug in llama.cpp tokenizer, please make sure to test with HF `transformers` library first (see [this comment](https://github.com/ggerganov/llama.cpp/issues/8240#issuecomment-2212444937) for example)",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-01T16:52:28+00:00",
    "closed_at": "2024-10-16T01:11:07+00:00",
    "comments": 90,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8240/reactions",
      "total_count": 7,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8240"
  },
  {
    "number": 6368,
    "title": "Support for 2-bit Quantized Llama-2-7b-chat-hf_2bitgs8_hqq Model",
    "body": "I would like to propose the integration of a novel model, \"Llama-2-7b-chat-hf_2bitgs8_hqq,\" available on Hugging Face. This model represents an innovative approach to quantization, employing a 2-bit quantized version of Llama2-7B-chat, enhanced with a low-rank adapter (HQQ+), to improve performance and efficiency.\r\n\r\nKey Features:\r\n- **Quantization**: The model leverages 2-bit quantization, significantly reducing VRAM requirements.\r\n- **Low-Rank Adapter**: Utilizes HQQ+, a low-rank adapter for performance enhancement.\r\n- **Efficiency**: Offloads meta-data to CPU, optimizing GPU memory usage.\r\n- **Datasets**: Trained on a mixture of general and specialized datasets, showing robustness and versatility.\r\n\r\nThe inclusion of this model could greatly benefit llama.cpp users by offering a more memory-efficient yet powerful option for large-scale text generation tasks. It could especially be beneficial for environments with limited hardware resources.\r\n\r\nThank you for considering this addition.\r\n\r\n[Link to the model on Hugging Face](https://huggingface.co/mobiuslabsgmbh/Llama-2-7b-chat-hf_2bitgs8_hqq)\r\n\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-28T14:15:03+00:00",
    "closed_at": "2024-05-14T01:31:12+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6368/reactions",
      "total_count": 14,
      "+1": 14,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6368"
  },
  {
    "number": 9227,
    "title": "Feature Request: Paligemma Support",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nAdding support for converting Google's multimodal Paligemma model to gguf in order to be used in ollama.\n\n### Motivation\n\nI have a personal project that requires a multimodal llm running locally and llava seems to be kind of...not great. I have seen an issue like this marked as open, but as of now, I still get an error when trying to convert from hf to gguf.\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-28T22:01:53+00:00",
    "closed_at": "2024-11-27T01:07:42+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9227/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9227"
  },
  {
    "number": 6417,
    "title": "Performance decreated between tag b1500 and b2581 on Windows ARM64 PC",
    "body": "Hi LLAMA team, \r\n\r\nI use llama tag b2581 on Windows ARM64 PC, the performance is more lower than previous tag b1500. Please refer to below detailed information. What is the reason? Please help on this issue. \r\n\r\nThanks a lot!\r\n\r\n**[Detailed information]**\r\n\r\n**Command:**\r\nmain.exe -m llama-2-7b-chat.ggufv3.q4_0.bin --color  --ctx_size 2048 -n -1 -ins -b 256 --top_k 10000 --temp 0.2 --repeat_penalty 1.1 -t 10\r\n\r\n**Prompt:** I have 3 years of experience as a software developer. Now I got bored with coding and want to transition to another career. My education qualifications are B. Tech in computer science, and I am well-versed in understanding the business side of software as well. Suggest a list of career options that are easy for me to transition.\r\n\r\n\r\n**system_info:** n_threads = 10 / 12 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 |\r\n\r\n**Tag b1500 results:**\r\nllama_print_timings:        load time =     723.53 ms\r\nllama_print_timings:      sample time =     925.29 ms /   624 runs   (    1.48 ms per token,   674.38 tokens per second)\r\nllama_print_timings: prompt eval time =    2583.12 ms /    91 tokens (   28.39 ms per token,    **35.23 tokens** per second)\r\nllama_print_timings:        eval time =   31693.17 ms /   625 runs   (   50.71 ms per token,    **19.72 tokens** per second)\r\nllama_print_timings:       total time =   51797.58 ms\r\n\r\n**Tag b2581 results:**\r\nllama_print_timings:        load time =     963.25 ms\r\nllama_print_timings:      sample time =     416.14 ms /   586 runs   (    0.71 ms per token,  1408.17 tokens per second)\r\nllama_print_timings: prompt eval time =   11847.94 ms /    94 tokens (  126.04 ms per token,     **7.93 tokens** per second)\r\nllama_print_timings:        eval time =   68542.50 ms /   585 runs   (  117.17 ms per token,     **8.53 tokens** per second)\r\nllama_print_timings:       total time =   82696.57 ms /   679 tokens\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-01T03:20:36+00:00",
    "closed_at": "2024-07-08T01:06:56+00:00",
    "comments": 54,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6417/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6417"
  },
  {
    "number": 13218,
    "title": "Feature Request: XiaomiMiMo/MiMo-7B-RL",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nAdd support of XiaomiMiMo/MiMo-7B-RL https://huggingface.co/XiaomiMiMo/MiMo-7B-RL\n\n### Motivation\n\nModel MiMoForCausalLM is not supported,Hope to further enrich the ecosystem.\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-04-30T17:17:04+00:00",
    "closed_at": "2025-06-27T01:08:01+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13218/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13218"
  },
  {
    "number": 6712,
    "title": "truly opensource model called olmo",
    "body": "Build with truly open dataset and fully open-source model can this be supported in olllama thanks.\r\nhttps://allenai.org/olmo\r\nhttps://huggingface.co/allenai/OLMo-7B\r\n",
    "labels": [
      "enhancement",
      "model"
    ],
    "state": "closed",
    "created_at": "2024-04-16T23:43:40+00:00",
    "closed_at": "2024-05-07T19:39:44+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6712/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6712"
  },
  {
    "number": 5079,
    "title": " Intel\u00ae Core\u2122 Ultra processors NPU  Support ",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\n\r\n Intel\u00ae Core\u2122 Ultra processors now has released  , how can llama.cpp use that npu to fast up \r\n\r\n# Motivation\r\n\r\n Intel\u00ae Core\u2122 Ultra processors deliver three dedicated engines (CPU, GPU, and NPU) to help unlock the power of AI\r\nhttps://www.intel.com/content/www/us/en/products/docs/processors/core-ultra/core-ultra-series-1-product-brief.html\r\n\r\n\r\n# Possible Implementation\r\n\r\nOpenVINO\u2122, WindowsML, DirectML, ONNX RT\r\n",
    "labels": [
      "enhancement"
    ],
    "state": "open",
    "created_at": "2024-01-22T14:15:28+00:00",
    "closed_at": null,
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5079/reactions",
      "total_count": 44,
      "+1": 43,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5079"
  },
  {
    "number": 3808,
    "title": "When I used the tool to quantify the chatglm model, the following error was reported",
    "body": "\r\nWhen I used the tool to quantify the chatglm model, the following error was reported. May I ask if the format of the specified model does not match? Is there a way to solve this problem?\r\n\r\n\r\n3:~/llama.cpp$ ./quantize MODEL/chatglm/chatGLM2-6B/pytorch_model-00001-of-00007.bin MODEL/chatglm/\r\n                                             python convert.py MODEL/chatglm/chatGLM2-6B/\r\nLoading model file MODEL/chatglm/chatGLM2-6B/pytorch_model-00001-of-00007.bin\r\nLoading model file MODEL/chatglm/chatGLM2-6B/pytorch_model-00001-of-00007.bin\r\nLoading model file MODEL/chatglm/chatGLM2-6B/pytorch_model-00002-of-00007.bin\r\nLoading model file MODEL/chatglm/chatGLM2-6B/pytorch_model-00003-of-00007.bin\r\nLoading model file MODEL/chatglm/chatGLM2-6B/pytorch_model-00004-of-00007.bin\r\nLoading model file MODEL/chatglm/chatGLM2-6B/pytorch_model-00005-of-00007.bin\r\nLoading model file MODEL/chatglm/chatGLM2-6B/pytorch_model-00006-of-00007.bin\r\nLoading model file MODEL/chatglm/chatGLM2-6B/pytorch_model-00007-of-00007.bin\r\nTraceback (most recent call last):\r\n  File \"/home/aistudio/llama.cpp/convert.py\", line 1202, in <module>\r\n    main()\r\n  File \"/home/aistudio/llama.cpp/convert.py\", line 1135, in main\r\n    model_plus = load_some_model(args.model)\r\n  File \"/home/aistudio/llama.cpp/convert.py\", line 1056, in load_some_model\r\n    model_plus = merge_multifile_models(models_plus)\r\n  File \"/home/aistudio/llama.cpp/convert.py\", line 589, in merge_multifile_models\r\n    model = merge_sharded([mp.model for mp in models_plus])\r\n  File \"/home/aistudio/llama.cpp/convert.py\", line 568, in merge_sharded\r\n    return {name: convert(name) for name in names}\r\n  File \"/home/aistudio/llama.cpp/convert.py\", line 568, in <dictcomp>\r\n    return {name: convert(name) for name in names}\r\n  File \"/home/aistudio/llama.cpp/convert.py\", line 543, in convert\r\n    lazy_tensors: list[LazyTensor] = [model[name] for model in models]\r\n  File \"/home/aistudio/llama.cpp/convert.py\", line 543, in <listcomp>\r\n    lazy_tensors: list[LazyTensor] = [model[name] for model in models]\r\nKeyError: 'transformer.embedding.word_embeddings.weight'",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-10-27T02:51:16+00:00",
    "closed_at": "2024-05-12T01:35:21+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3808/reactions",
      "total_count": 5,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3808"
  },
  {
    "number": 14048,
    "title": "Feature Request: Speculative Decoding \"acceptance rate\" should not count drafts that were skipped via the \" ignore small drafts\" clause",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nI think the `slot.n_draft_total += draft.size()` should go after the \"ignore small drafts\" test here:\n\n```cpp\n                llama_tokens draft = common_speculative_gen_draft(slot.spec, params_spec, cached_text_tokens, id);\n\n                // keep track of total number of tokens generated in the draft\n                slot.n_draft_total += draft.size();\n\n                // ignore small drafts\n                if (slot.params.speculative.n_min > (int) draft.size()) {\n                    SLT_DBG(slot, \"ignoring small draft: %d < %d\\n\", (int) draft.size(), slot.params.speculative.n_min);\n\n                    continue;\n                }\n\n                // construct the speculation batch\n                common_batch_clear(slot.batch_spec);\n                common_batch_add  (slot.batch_spec, id, slot.n_past, { slot.id }, true);\n\n                for (size_t i = 0; i < draft.size(); ++i) {\n                    common_batch_add(slot.batch_spec, draft[i], slot.n_past + 1 + i, { slot.id }, true);\n                }\n\n                SLT_DBG(slot, \"decoding speculative batch, size = %d\\n\", slot.batch_spec.n_tokens);\n\n                llama_decode(ctx, slot.batch_spec);\n\n                // the accepted tokens from the speculation\n                const auto ids = common_sampler_sample_and_accept_n(slot.smpl, ctx, draft);\n\n                slot.n_past    += ids.size();\n                slot.n_decoded += ids.size();\n\n                // update how many tokens out of draft was accepted\n                slot.n_draft_accepted += ids.size() - 1;\n```\n\nIMO, the \"acceptance rate\" should be the fraction of tokens we ***passed through the larger model*** that were accepted.\n\nWe could add another `slot.n_draft_generated` that counts how many tokens were generated by the draft model to print the existing stat, but since the speculative-decoding code can reuse drafts that were skipped via the \"ignore small drafts\" clause; it probably has limited usefulness.\n\n(Not sure what to put this under as it might be classed as a bug,or a feature request if adding `slot.n_draft_generated`)\n\n### Motivation\n\nIMO, the \"acceptance rate\" should be the fraction of tokens we ***passed through the larger model*** that were accepted.\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2025-06-06T11:04:38+00:00",
    "closed_at": "2025-06-10T15:48:08+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14048/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14048"
  },
  {
    "number": 9692,
    "title": "Bug: cannot find tokenizer merges in model file",
    "body": "### What happened?\n\nWhen I use transformers==4.45.1 and convert llama.cpp to the file used by ollama, there is no error, but when I load the model with ollama, the error ollama cannot find tokenizer merges in model file appears\n\n### Name and Version\n\n\u6240\u6709\u7248\u672c\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug",
      "high priority",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-09-30T02:31:24+00:00",
    "closed_at": "2024-10-08T03:14:42+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9692/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9692"
  },
  {
    "number": 431,
    "title": "Quantize python script fails.",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ ] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ ] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ ] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ ] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI have my llama models stored in models/llama/{7B,13B,30B,65B}.\r\n\r\nI expect that when I run the following command that the model will be converted\r\n\r\n$ python3 quantize.py --models-path models/llama 30B\r\n\r\n\r\n# Current Behavior\r\n\r\nWhen attempting to quantize the model by running \r\n\r\n$ python3 quantize.py --models-path models/llama 30B\r\n\r\nI get the following error:\r\n\r\nThe f16 model ggml-model-f16.bin was not found in models/llama/30B. If you want to use it from another location, set the --models-path argument from the command line.\r\n\r\n\r\n\r\nmodifying lines 76-79\r\n\r\n```\r\n        f16_model_parts_paths = map(\r\n            lambda filename: os.path.join(f16_model_path_base, filename),\r\n            glob.glob(f\"{f16_model_path_base}*\")\r\n        )\r\n```\r\n\r\nTo\r\n\r\n```\r\n       f16_model_parts_paths = [ filename for filename in glob.glob(f\"{f16_model_path_base}*\")]\r\n```\r\n\r\nMakes it work.\r\n\r\n\r\n```\r\n$ python3 --version   --> Python 3.8.10\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-03-23T15:15:24+00:00",
    "closed_at": "2023-03-23T20:42:54+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/431/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/431"
  },
  {
    "number": 11538,
    "title": "Misc. bug: llama-server ignores the stop parameter",
    "body": "### Name and Version\n\nversion: 4599 (8b576b6c)\nbuilt with cc (Debian 12.2.0-14) 12.2.0 for x86_64-linux-gnu\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Command line\n\n```shell\ncurl --request POST --url http://localhost:8080/completion --header \"Content-Type: application/json\"  --data '{\"prompt\": \"A B C D E F G H I J K\",\"n_predict\": 128, \"stop\": [\"O P Q\"]}'\n```\nNotice that the stop string spawns multiple tokens.\n\n\n### Problem description & steps to reproduce\n\nThe server `/completion` endpoint ignores the `stop` parameter.\n\nTested by loading phi4 in llama-server, then sending a request with a array of stop tokens including a triple backquote: [stop1, stop2, \"```\", stop3,...]\n\n### example\n`curl --request POST --url http://localhost:8080/completion --header \"Content-Type: application/json\"  --data '{\"prompt\": \"A B C D E F G H I J K\",\"n_predict\": 128, \"stop\": [\"O P Q\"]}'`\n\nnote that the stop string spans multiple tokens\n\n\nThe offending commit is in the next section of the bug report.\n\n\n### First Bad Commit\n\nThe problem started in commit `8b576b6c55bc4e6be898b47522f0ef402b93ef62` #9639\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-01-31T10:34:03+00:00",
    "closed_at": "2025-01-31T13:48:33+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11538/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11538"
  },
  {
    "number": 3940,
    "title": "train-text-from-scratch and finetune nan loss on iter=2",
    "body": "I was trying out the finetune example with my model but it kept going into nan loss. I eventually tried train-text-from-scratch, following the instructions on the README there and it goes into nan as well. I've reproduced this on two machines.\r\n\r\n```\r\nroot@c5a10438d69e:/workspace/llama.cpp# ./train-text-from-scratch         --vocab-model ./models/ggml-vocab-llama.gguf         --ctx 64 --embd 256 --head 8 --layer 16         --checkpoint-in  chk-shakespeare-256x16-LATEST.gguf         --checkpoint-out chk-shakespeare-256x16-ITERATION.gguf         --model-out ggml-shakespeare-256x16-f32-ITERATION.gguf         --train-data \"shakespeare.txt\"         -t 6 -b 16 --seed 1 --adam-iter 256         --no-checkpointing\r\nmain: seed: 1\r\nllama_model_loader: loaded meta data with 17 key-value pairs and 0 tensors from ./models/ggml-vocab-llama.gguf (version GGUF V3 (latest))\r\nllama_model_loader: - kv   0:                       general.architecture str     \r\nllama_model_loader: - kv   1:                               general.name str     \r\nllama_model_loader: - kv   2:                       llama.context_length u32     \r\nllama_model_loader: - kv   3:                     llama.embedding_length u32     \r\nllama_model_loader: - kv   4:                          llama.block_count u32     \r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32     \r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32     \r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \r\nllama_model_loader: - kv  10:                       tokenizer.ggml.model str     \r\nllama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr     \r\nllama_model_loader: - kv  12:                      tokenizer.ggml.scores arr     \r\nllama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr     \r\nllama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32     \r\nllama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32     \r\nllama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32     \r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 11008\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = all F32 (guessed)\r\nllm_load_print_meta: model params     = 0.00 B\r\nllm_load_print_meta: model size       = 0.00 MiB (-nan BPW) \r\nllm_load_print_meta: general.name   = LLaMA v2\r\nllm_load_print_meta: BOS token = 1 '<s>'\r\nllm_load_print_meta: EOS token = 2 '</s>'\r\nllm_load_print_meta: UNK token = 0 '<unk>'\r\nllm_load_print_meta: LF token  = 13 '<0x0A>'\r\nllama_model_load: vocab only - skipping tensors\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nmain: init model\r\nprint_params: n_vocab: 32000\r\nprint_params: n_ctx:   64\r\nprint_params: n_embd:  256\r\nprint_params: n_head:  8\r\nprint_params: n_ff:    768\r\nprint_params: n_layer: 16\r\nprint_params: n_rot:   32\r\nmain: total train_iterations 0\r\nmain: seen train_samples     0\r\nmain: seen train_tokens      0\r\nmain: completed train_epochs 0\r\nmain: model_size = 240304416 bytes (229.2 MB)\r\nmain: opt_size  = 360288432 bytes (343.6 MB)\r\nmain: opt iter 0\r\nmain: input_size = 131076128 bytes (125.0 MB)\r\nmain: compute_size = 701759840 bytes (669.3 MB)\r\nmain: evaluation order = LEFT_TO_RIGHT\r\nmain: tokenize training data\r\ntokenize_file: total number of samples: 27520\r\nmain: number of training tokens: 27584\r\nmain: train data seems to have changed. restarting shuffled epoch.\r\nmain: begin training\r\nmain: work_size = 768376 bytes (0.7 MB)\r\ntrain_opt_callback: iter=     0 sample=1/27520 sched=0.000000 loss=0.000000 |->\r\ntrain_opt_callback: iter=     1 sample=17/27520 sched=0.010000 loss=10.373524 dt=00:00:03 eta=00:15:01 |->\r\ntrain_opt_callback: iter=     2 sample=33/27520 sched=0.020000 loss=nan dt=00:00:03 eta=00:14:19 |>\r\ntrain_opt_callback: iter=     3 sample=49/27520 sched=0.030000 loss=nan dt=00:00:03 eta=00:15:01 |>\r\n^C\r\nroot@c5a10438d69e:/workspace/llama.cpp# ^C\r\nroot@c5a10438d69e:/workspace/llama.cpp# git log | head -1\r\ncommit d9b33fe95bd257b36c84ee5769cc048230067d6f\r\nroot@c5a10438d69e:/workspace/llama.cpp# lscpu | egrep \"AMD|Flags\"\r\nVendor ID:                       AuthenticAMD\r\nModel name:                      AMD EPYC Processor\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm rep_good nopl cpuid extd_apicid amd_dcm tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 arat npt nrip_save\r\nVirtualization:                  AMD-V\r\nroot@c5a10438d69e:/workspace/llama.cpp# uname -a\r\nLinux c5a10438d69e 5.4.0-139-generic #156-Ubuntu SMP Fri Jan 20 17:27:18 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\nroot@c5a10438d69e:/workspace/llama.cpp# g++ --version\r\ng++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\r\nCopyright (C) 2021 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\nroot@c5a10438d69e:/workspace/llama.cpp# make --version\r\nGNU Make 4.3\r\nBuilt for x86_64-pc-linux-gnu\r\nCopyright (C) 1988-2020 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.\r\nroot@c5a10438d69e:/workspace/llama.cpp# \r\n```\r\n\r\nI've bisected this and 898aeca90a9bb992f506234cf3b8b7f7fa28a1df is the first bad commit. Reverting to the previous commit, c43c2da8afacaddfe51c09b21dbd9922cd0ea46b, train-text-from-scratch and finetune appear to work fine (they don't go into nan)\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-11-04T04:42:06+00:00",
    "closed_at": "2023-11-07T08:04:52+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3940/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3940"
  },
  {
    "number": 6112,
    "title": "Constrained decoding with grammar fails for c4ai-command-r-v01",
    "body": "I am trying to apply constrained decoding for the recently adopted command-r. \r\n\r\nUsing the most recent master branch (https://github.com/ggerganov/llama.cpp/commit/c47cf414efafb8f60596edc7edb5a2d68065e992) I'm trying to apply the simplest list.  \r\n\r\n`./main -m ~/data/c4ai-command-r-v01/ggml-model-Q4_K_M.gguf -p \"<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Please give me a list of things to do in SF?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\" -ctk q8_0 -ngl 99 -n 500 --grammar-file grammars/list.gbnf`\r\n\r\nIt fails with \r\n\r\n`libc++abi: terminating due to uncaught exception of type std::out_of_range: unordered_map::at: key not found`\r\n\r\nAny idea what could go wrong here?\r\n\r\nMore details:\r\n\r\n```\r\nLog start\r\nmain: build = 2447 (c47cf414)\r\nmain: built with Apple clang version 15.0.0 (clang-1500.3.9.4) for arm64-apple-darwin23.3.0\r\nmain: seed  = 1710686911\r\nllama_model_loader: loaded meta data with 23 key-value pairs and 322 tensors from ~/data/c4ai-command-r-v01/ggml-model-Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = command-r\r\nllama_model_loader: - kv   1:                               general.name str              = c4ai-command-r-v01\r\nllama_model_loader: - kv   2:                      command-r.block_count u32              = 40\r\nllama_model_loader: - kv   3:                   command-r.context_length u32              = 8192\r\nllama_model_loader: - kv   4:                 command-r.embedding_length u32              = 8192\r\nllama_model_loader: - kv   5:              command-r.feed_forward_length u32              = 22528\r\nllama_model_loader: - kv   6:             command-r.attention.head_count u32              = 64\r\nllama_model_loader: - kv   7:          command-r.attention.head_count_kv u32              = 64\r\nllama_model_loader: - kv   8:                   command-r.rope.freq_base f32              = 8000000.000000\r\nllama_model_loader: - kv   9:     command-r.attention.layer_norm_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  11:                      command-r.logit_scale f32              = 0.062500\r\nllama_model_loader: - kv  12:                command-r.rope.scaling.type str              = none\r\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<PAD>\", \"<UNK>\", \"<CLS>\", \"<SEP>\", ...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,253333]  = [\"\u0120 \u0120\", \"\u0120 t\", \"e r\", \"i n\", \"\u0120 a...\r\nllama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 5\r\nllama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 255001\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   41 tensors\r\nllama_model_loader: - type q4_K:  240 tensors\r\nllama_model_loader: - type q6_K:   41 tensors\r\nllm_load_vocab: special tokens definition check successful ( 1008/256000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = command-r\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 256000\r\nllm_load_print_meta: n_merges         = 253333\r\nllm_load_print_meta: n_ctx_train      = 8192\r\nllm_load_print_meta: n_embd           = 8192\r\nllm_load_print_meta: n_head           = 64\r\nllm_load_print_meta: n_head_kv        = 64\r\nllm_load_print_meta: n_layer          = 40\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 8192\r\nllm_load_print_meta: n_embd_v_gqa     = 8192\r\nllm_load_print_meta: f_norm_eps       = 1.0e-05\r\nllm_load_print_meta: f_norm_rms_eps   = 0.0e+00\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 6.2e-02\r\nllm_load_print_meta: n_ff             = 22528\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = none\r\nllm_load_print_meta: freq_base_train  = 8000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 8192\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 35B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 34.98 B\r\nllm_load_print_meta: model size       = 20.04 GiB (4.92 BPW) \r\nllm_load_print_meta: general.name     = c4ai-command-r-v01\r\nllm_load_print_meta: BOS token        = 5 '<BOS_TOKEN>'\r\nllm_load_print_meta: EOS token        = 255001 '<|END_OF_TURN_TOKEN|>'\r\nllm_load_print_meta: PAD token        = 0 '<PAD>'\r\nllm_load_print_meta: LF token         = 136 '\u00c4'\r\nllm_load_tensors: ggml ctx size =    0.25 MiB\r\nggml_backend_metal_buffer_from_ptr: allocated buffer, size = 20519.42 MiB, (20519.48 / 147456.00)\r\nllm_load_tensors: offloading 40 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 41/41 layers to GPU\r\nllm_load_tensors:      Metal buffer size = 20519.41 MiB\r\nllm_load_tensors:        CPU buffer size =  1640.62 MiB\r\n.......................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: freq_base  = 8000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nggml_metal_init: allocating\r\nggml_metal_init: found device: Apple M2 Ultra\r\nggml_metal_init: picking default device: Apple M2 Ultra\r\nggml_metal_init: default.metallib not found, loading from source\r\nggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\r\nggml_metal_init: loading '[...]src/llama.cpp/ggml-metal.metal'\r\nggml_metal_init: GPU name:   Apple M2 Ultra\r\nggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\r\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\r\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\r\nggml_metal_init: simdgroup reduction support   = true\r\nggml_metal_init: simdgroup matrix mul. support = true\r\nggml_metal_init: hasUnifiedMemory              = true\r\nggml_metal_init: recommendedMaxWorkingSetSize  = 154618.82 MB\r\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   490.00 MiB, (21011.30 / 147456.00)\r\nllama_kv_cache_init:      Metal KV buffer size =   490.00 MiB\r\nllama_new_context_with_model: KV self size  =  490.00 MiB, K (q8_0):  170.00 MiB, V (f16):  320.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =   500.00 MiB\r\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   516.00 MiB, (21527.30 / 147456.00)\r\nllama_new_context_with_model:      Metal compute buffer size =   516.00 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =    17.00 MiB\r\nllama_new_context_with_model: graph splits: 2\r\n\r\nsystem_info: n_threads = 16 / 24 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \r\nsampling: \r\n        repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\r\n        top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\r\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\r\nsampling order: \r\nCFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \r\ngenerate: n_ctx = 512, n_batch = 2048, n_predict = 500, n_keep = 1\r\n\r\n\r\n<|START_OF_TURN_TOKEN|><|USER_TOKEN|>Please give me a list of things to do in SF?<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>libc++abi: terminating due to uncaught exception of type std::out_of_range: unordered_map::at: key not found\r\n```",
    "labels": [
      "bug",
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2024-03-17T14:51:01+00:00",
    "closed_at": "2024-05-28T10:55:36+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6112/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6112"
  },
  {
    "number": 7048,
    "title": "Significantly different results (and WRONG) inference when GPU is enabled.",
    "body": "I am running llama_cpp version 0.2.68 on Ubuntu 22.04LTS under conda environment. Attached are two Jupyter notebooks with ONLY one line changed (use CPU vs GPU).  As you can see for exact same environmental conditions switching between CPU/GPU gives vastly different answers where the GPU is completely wrong.  Some pointers on how to debug this I would appreciate it.\r\n\r\nThe only significant difference between the two files is this one liner\r\n      `#n_gpu_layers=-1, # Uncomment to use GPU acceleration`\r\n\r\nThe model used was **openhermes-2.5-mistral-7b.Q5_K_M.gguf**\r\n\r\n[mistral_llama_large-gpu.pdf](https://github.com/ggerganov/llama.cpp/files/15192723/mistral_llama_large-gpu.pdf)\r\n[mistral_llama_large-cpu.pdf](https://github.com/ggerganov/llama.cpp/files/15192725/mistral_llama_large-cpu.pdf)\r\n\r\n",
    "labels": [
      "bug",
      "Nvidia GPU"
    ],
    "state": "closed",
    "created_at": "2024-05-02T18:51:50+00:00",
    "closed_at": "2024-05-17T18:49:39+00:00",
    "comments": 40,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7048/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7048"
  },
  {
    "number": 2187,
    "title": "Incoherent output after merging https://github.com/ggerganov/llama.cpp/pull/2183",
    "body": "The commit in question seems to be https://github.com/ggerganov/llama.cpp/commit/20d7740a9b45f6e5b247fa3738fdda35e18c2e8a \r\n\r\nThe AI responses no longer seem to consider the prompt after this commit.\r\n\r\nRunning pre-built cuda executables from github actions:\r\n\r\n**llama-master-20d7740-bin-win-cublas-cu11.7.1-x64**\r\n```\r\nPS E:\\LLaMA\\llamacpp> .\\main.exe --model e:\\LLaMA\\models\\airoboros-7b-gpt4.ggmlv3.q4_0.bin -ngl 32 -n 30 -p \"Hi, my name is\"\r\nmain: build = 820 (20d7740)\r\nmain: seed  = 1689137712\r\nggml_init_cublas: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 2060, compute capability 7.5\r\nllama.cpp: loading model from e:\\LLaMA\\models\\airoboros-7b-gpt4.ggmlv3.q4_0.bin\r\nllama_model_load_internal: format     = ggjt v3 (latest)\r\nllama_model_load_internal: n_vocab    = 32000\r\nllama_model_load_internal: n_ctx      = 512\r\nllama_model_load_internal: n_embd     = 4096\r\nllama_model_load_internal: n_mult     = 256\r\nllama_model_load_internal: n_head     = 32\r\nllama_model_load_internal: n_layer    = 32\r\nllama_model_load_internal: n_rot      = 128\r\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\r\nllama_model_load_internal: n_ff       = 11008\r\nllama_model_load_internal: model size = 7B\r\nllama_model_load_internal: ggml ctx size =    0.08 MB\r\nllama_model_load_internal: using CUDA for GPU acceleration\r\nllama_model_load_internal: mem required  = 1932.72 MB (+ 1026.00 MB per state)\r\nllama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\r\nllama_model_load_internal: offloading 32 repeating layers to GPU\r\nllama_model_load_internal: offloaded 32/35 layers to GPU\r\nllama_model_load_internal: total VRAM used: 3763 MB\r\nllama_new_context_with_model: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 6 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0\r\n| WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\r\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\r\ngenerate: n_ctx = 512, n_batch = 512, n_predict = 30, n_keep = 0\r\n\r\n\r\n Hi, my name isCounterclockwise: 2016 in Review \u2013 Part One\r\nWelcome to Counterclockwise where we take a look back at some\r\nllama_print_timings:        load time =  2374.73 ms\r\nllama_print_timings:      sample time =     7.17 ms /    30 runs   (    0.24 ms per token,  4181.77 tokens per second)\r\nllama_print_timings: prompt eval time =   402.77 ms /     6 tokens (   67.13 ms per token,    14.90 tokens per second)\r\nllama_print_timings:        eval time =  1391.52 ms /    29 runs   (   47.98 ms per token,    20.84 tokens per second)\r\nllama_print_timings:       total time =  1807.27 ms\r\n```\r\n\r\n\r\n**llama-master-5bf2a27-bin-win-cublas-cu11.7.1-x64**\r\n```\r\nPS E:\\LLaMA\\llamacpp> .\\main.exe --model e:\\LLaMA\\models\\airoboros-7b-gpt4.ggmlv3.q4_0.bin -ngl 32 -n 30 -p \"Hi, my name is\"\r\nmain: build = 819 (5bf2a27)\r\nmain: seed  = 1689137643\r\nggml_init_cublas: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 2060, compute capability 7.5\r\nllama.cpp: loading model from e:\\LLaMA\\models\\airoboros-7b-gpt4.ggmlv3.q4_0.bin\r\nllama_model_load_internal: format     = ggjt v3 (latest)\r\nllama_model_load_internal: n_vocab    = 32000\r\nllama_model_load_internal: n_ctx      = 512\r\nllama_model_load_internal: n_embd     = 4096\r\nllama_model_load_internal: n_mult     = 256\r\nllama_model_load_internal: n_head     = 32\r\nllama_model_load_internal: n_layer    = 32\r\nllama_model_load_internal: n_rot      = 128\r\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\r\nllama_model_load_internal: n_ff       = 11008\r\nllama_model_load_internal: model size = 7B\r\nllama_model_load_internal: ggml ctx size =    0.08 MB\r\nllama_model_load_internal: using CUDA for GPU acceleration\r\nllama_model_load_internal: mem required  = 1932.72 MB (+ 1026.00 MB per state)\r\nllama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\r\nllama_model_load_internal: offloading 32 repeating layers to GPU\r\nllama_model_load_internal: offloaded 32/35 layers to GPU\r\nllama_model_load_internal: total VRAM used: 3763 MB\r\nllama_new_context_with_model: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 6 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0\r\n| WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\r\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\r\ngenerate: n_ctx = 512, n_batch = 512, n_predict = 30, n_keep = 0\r\n\r\n\r\n Hi, my name is John and I'm 31 years old.\r\nI was diagnosed with chronic fatigue syndrome in 2015 but\r\nllama_print_timings:        load time =  2316.55 ms\r\nllama_print_timings:      sample time =     5.91 ms /    30 runs   (    0.20 ms per token,  5079.58 tokens per second)\r\nllama_print_timings: prompt eval time =   376.72 ms /     6 tokens (   62.79 ms per token,    15.93 tokens per second)\r\nllama_print_timings:        eval time =  1419.35 ms /    29 runs   (   48.94 ms per token,    20.43 tokens per second)\r\nllama_print_timings:       total time =  1807.44 ms\r\n```\r\n\r\nanyone else experiencing the same issues?",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-07-12T04:57:11+00:00",
    "closed_at": "2023-07-14T18:51:46+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2187/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2187"
  },
  {
    "number": 7492,
    "title": "CUDA graphs break quantized K cache",
    "body": "As of right now it is already possible on master to quantize the K cache via e.g. `-ctk q8_0`. However, this is currently broken on master for batch size 1. Disabling CUDA graphs via the environment variable `GGML_CUDA_DISABLE_GRAPHS=1` fixes the issue.\r\n\r\ncc: @agray3 ",
    "labels": [
      "bug",
      "Nvidia GPU"
    ],
    "state": "closed",
    "created_at": "2024-05-23T12:11:15+00:00",
    "closed_at": "2024-05-27T17:33:43+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7492/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7492"
  },
  {
    "number": 13684,
    "title": "Eval bug: llama.cpp/ggml/src/ggml-backend.cpp:750: pre-allocated tensor (cache_k_l32 (view) (copy of cache_k_l32 (view))) in a buffer (Vulkan0) that cannot run the operation (CPY)",
    "body": "### Name and Version\n\n$ sources/llama.cpp/build/bin/llama-server --version\nversion: 5435 (a4090d11)\nbuilt with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nVulkan\n\n### Hardware\n\nIntel(R) Core(TM) Ultra 5 245KF + Radeon RX 7900 XTX, gfx1100 (0x1100)\n\n### Models\n\ngemma-3-27b-it-qat-UD-Q4_K_XL.gguf + gemma-3-27b-it-qat-GGUF/mmproj-F16.gguf\n\n### Problem description & steps to reproduce\n\nBuilt with (corrected command):\n`cmake -S . -B build -DGGML_VULKAN=1 -DCMAKE_BUILD_TYPE=Release && cmake --build build -- -j 16`\n\nCommand used to start:\n`sources/llama.cpp/build/bin/llama-server --port 9001 -c 65536 -ctv q8_0 -ctk q8_0 --no-warmup -ngl 99 -fa -m models/unsloth/gemma-3-27b-it-qat-GGUF/gemma-3-27b-it-qat-UD-Q4_K_XL.gguf --mmproj models/unsloth/gemma-3-27b-it-qat-GGUF/mmproj-F16.gguf --jinja`\n\nThis is the first time I used this model. I accessed the API via openwebui hosted in a docker container. Normal text only chat, nothing special. I can share the chat contents privately - it crashes every time when I try to regenerate the last message. I recompiled with -DCMAKE_BUILD_TYPE=Debug and it was reproducible, so the full debug log is attached.\n\nThe memory utilization is 22052M our of 24560M, nothing else is using the GPU.\n\nError looks similar to https://github.com/ggml-org/llama.cpp/issues/12045\n\n[llama.cpp-debug.log](https://github.com/user-attachments/files/20368027/llama.cpp-debug.log)\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nslot update_slots: id  0 | task 0 | kv cache rm [2048, end)\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 4076, n_tokens = 2028, progress = 1.000000\nslot update_slots: id  0 | task 0 | prompt done, n_past = 4076, n_tokens = 2028\n/home/wizz/sources/llama.cpp/ggml/src/ggml-backend.cpp:750: pre-allocated tensor (cache_k_l32 (view) (copy of cache_k_l32 (view))) in a buffer (Vulkan0) that cannot run the operation (CPY)\n[New LWP 1580837]\n[New LWP 1580836]\n[New LWP 1580835]\n[New LWP 1580834]\n[New LWP 1580833]\n[New LWP 1580832]\n[New LWP 1580831]\n[New LWP 1580830]\n[New LWP 1580829]\n[New LWP 1580828]\n[New LWP 1580827]\n[New LWP 1580826]\n[New LWP 1580825]\n[New LWP 1580824]\n[New LWP 1580823]\n[New LWP 1580822]\n[New LWP 1580821]\n[New LWP 1580819]\n\nThis GDB supports auto-downloading debuginfo from the following URLs:\n  <https://debuginfod.ubuntu.com>\nEnable debuginfod for this session? (y or [n]) [answered N; input not from terminal]\nDebuginfod has been disabled.\nTo make this setting permanent, add 'set debuginfod enabled off' to .gdbinit.\nwarning: could not find '.gnu_debugaltlink' file for /lib/x86_64-linux-gnu/liblber.so.2\nwarning: could not find '.gnu_debugaltlink' file for /lib/x86_64-linux-gnu/libbrotlidec.so.1\nwarning: could not find '.gnu_debugaltlink' file for /lib/x86_64-linux-gnu/libbrotlicommon.so.1\nwarning: could not find '.gnu_debugaltlink' file for /usr/lib/x86_64-linux-gnu/libvulkan_radeon.so\nwarning: could not find '.gnu_debugaltlink' file for /lib/x86_64-linux-gnu/libtinfo.so.6\nwarning: could not find '.gnu_debugaltlink' file for /usr/lib/x86_64-linux-gnu/libvulkan_intel.so\nwarning: could not find '.gnu_debugaltlink' file for /usr/lib/x86_64-linux-gnu/libvulkan_intel_hasvk.so\nwarning: could not find '.gnu_debugaltlink' file for /usr/lib/x86_64-linux-gnu/libvulkan_virtio.so\nwarning: could not find '.gnu_debugaltlink' file for /usr/lib/x86_64-linux-gnu/libvulkan_lvp.so\nwarning: could not find '.gnu_debugaltlink' file for /usr/lib/x86_64-linux-gnu/libvulkan_nouveau.so\nwarning: could not find '.gnu_debugaltlink' file for /lib/x86_64-linux-gnu/libVkLayer_MESA_device_select.so\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n0x00007587595107e3 in __GI___wait4 (pid=1580930, stat_loc=0x0, options=0, usage=0x0) at ../sysdeps/unix/sysv/linux/wait4.c:30\nwarning: 30     ../sysdeps/unix/sysv/linux/wait4.c: No such file or directory\n#0  0x00007587595107e3 in __GI___wait4 (pid=1580930, stat_loc=0x0, options=0, usage=0x0) at ../sysdeps/unix/sysv/linux/wait4.c:30\n30      in ../sysdeps/unix/sysv/linux/wait4.c\n#1  0x000075875b3c1682 in ggml_print_backtrace () at /home/wizz/sources/llama.cpp/ggml/src/ggml.c:194\n194             waitpid(child_pid, NULL, 0);\n#2  0x000075875b3c17b5 in ggml_abort (file=0x75875b4410a0 \"/home/wizz/sources/llama.cpp/ggml/src/ggml-backend.cpp\", line=750, fmt=0x75875b4414f0 \"pre-allocated tensor (%s) in a buffer (%s) that cannot run the operation (%s)\") at /home/wizz/sources/llama.cpp/ggml/src/ggml.c:215\n215         ggml_print_backtrace();\n#3  0x000075875b3da02f in ggml_backend_sched_backend_id_from_cur (sched=0x5fb9db9024d0, tensor=0x5fb9dbcebd00) at /home/wizz/sources/llama.cpp/ggml/src/ggml-backend.cpp:750\n750             GGML_ABORT(\"pre-allocated tensor (%s) in a buffer (%s) that cannot run the operation (%s)\", tensor->name, ggml_backend_buffer_name(buffer), ggml_op_name(tensor->op));\n#4  0x000075875b3da9c4 in ggml_backend_sched_split_graph (sched=0x5fb9db9024d0, graph=0x5fb9dbad8d00) at /home/wizz/sources/llama.cpp/ggml/src/ggml-backend.cpp:899\n899                 *node_backend_id = ggml_backend_sched_backend_id_from_cur(sched, node);\n#5  0x000075875b3dde02 in ggml_backend_sched_alloc_graph (sched=0x5fb9db9024d0, graph=0x5fb9dbad8d00) at /home/wizz/sources/llama.cpp/ggml/src/ggml-backend.cpp:1565\n1565        ggml_backend_sched_split_graph(sched, graph);\n#6  0x000075875b9e6308 in llama_kv_cache_unified::update (this=0x5fb9db909230, lctx=...) at /home/wizz/sources/llama.cpp/src/llama-kv-cache.cpp:427\n427                 ggml_backend_sched_alloc_graph(sched, gf);\n#7  0x000075875b9eb5ee in llama_kv_cache_unified_iswa::update (this=0x5fb9db9071e0, lctx=...) at /home/wizz/sources/llama.cpp/src/llama-kv-cache.cpp:1761\n1761        res = res & kv_swa ->update(lctx);\n#8  0x000075875b976c4d in llama_context::kv_self_update (this=0x5fb9d4dfc2b0) at /home/wizz/sources/llama.cpp/src/llama-context.cpp:457\n457         need_reserve = kv_self->update(*this);\n#9  0x000075875b97931e in llama_context::decode (this=0x5fb9d4dfc2b0, inp_batch=...) at /home/wizz/sources/llama.cpp/src/llama-context.cpp:926\n926         kv_self_update();\n#10 0x000075875b97fb11 in llama_decode (ctx=0x5fb9d4dfc2b0, batch=...) at /home/wizz/sources/llama.cpp/src/llama-context.cpp:2545\n2545        int ret = ctx->decode(batch);\n#11 0x00005fb99b780b7b in server_context::update_slots (this=0x7ffdc33254d0) at /home/wizz/sources/llama.cpp/tools/server/server.cpp:3358\n3358                    ret = llama_decode(ctx, batch_view);\n#12 0x00005fb99b725035 in operator() (__closure=0x7ffdc3326aa8) at /home/wizz/sources/llama.cpp/tools/server/server.cpp:4849\n4849            ctx_server.update_slots();\n#13 0x00005fb99b7335c4 in std::__invoke_impl<void, main(int, char**)::<lambda()>&>(std::__invoke_other, struct {...} &) (__f=...) at /usr/include/c++/13/bits/invoke.h:61\n61          { return std::forward<_Fn>(__f)(std::forward<_Args>(__args)...); }\n#14 0x00005fb99b731494 in std::__invoke_r<void, main(int, char**)::<lambda()>&>(struct {...} &) (__fn=...) at /usr/include/c++/13/bits/invoke.h:111\n111             std::__invoke_impl<__type>(__tag{}, std::forward<_Callable>(__fn),\n#15 0x00005fb99b72d6bb in std::_Function_handler<void(), main(int, char**)::<lambda()> >::_M_invoke(const std::_Any_data &) (__functor=...) at /usr/include/c++/13/bits/std_function.h:290\n290             return std::__invoke_r<_Res>(*_Base::_M_get_pointer(__functor),\n#16 0x00005fb99b786d36 in std::function<void ()>::operator()() const (this=0x7ffdc3326aa8) at /usr/include/c++/13/bits/std_function.h:591\n591             return _M_invoker(_M_functor, std::forward<_ArgTypes>(__args)...);\n#17 0x00005fb99b772b83 in server_queue::start_loop (this=0x7ffdc3326988) at /home/wizz/sources/llama.cpp/tools/server/server.cpp:1681\n1681                callback_update_slots();\n#18 0x00005fb99b7278a0 in main (argc=22, argv=0x7ffdc3326d48) at /home/wizz/sources/llama.cpp/tools/server/server.cpp:4874\n4874        ctx_server.queue_tasks.start_loop();\n[Inferior 1 (process 1580786) detached]\n```",
    "labels": [
      "bug",
      "Vulkan"
    ],
    "state": "closed",
    "created_at": "2025-05-21T12:30:02+00:00",
    "closed_at": "2025-05-23T04:45:03+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13684/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13684"
  },
  {
    "number": 9391,
    "title": "Bug: cannot create std::vector larger than max_size()",
    "body": "### What happened?\r\n\r\nMy usual build recipe and run scripts do not work after b3680. Something changed in b3681, but I don't know what.\r\nI see this same failure across models and cli flags, so it seems to be deeper than a single feature choice, so I have excluded the launch script.\r\n\r\nThis is the actual error:\r\n```\r\n...\r\nterminate called after throwing an instance of 'std::length_error'\r\n  what():  cannot create std::vector larger than max_size()\r\n<launch script name> Aborted                 (core dumped)\r\n```\r\n\r\nHere is what the binary reports at runtime:\r\n```\r\nsystem_info: n_threads = 24 (n_threads_batch = 24) / 48 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 |\r\nmain: interactive mode on.\r\n```\r\n\r\nHere is how I configure the build:\r\n```\r\ncmake -DGGML_AVX=ON -DGGML_AVX2=ON -DBUILD_SHARED_LIBS=ON -DGGML_CUDA=ON -DGGML_CUDA_F16=ON -DGGML_F16C=ON -DCMAKE_C_COMPILER=gcc-12 -DCMAKE_CXX_COMPILER=g++-12 -DCMAKE_CUDA_FLAGS='-ccbin=gcc-12' -DCMAKE_INSTALL_PREFIX=/opt/llama ..\r\n```\r\n\r\nand some other system info:\r\n```\r\n$ lscpu | grep \"Model name:\"\r\nModel name:                           Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz\r\n$ uname -srv\r\nLinux 6.10.6-arch1-1 #1 SMP PREEMPT_DYNAMIC Mon, 19 Aug 2024 17:02:39 +0000\r\n$ cat /proc/driver/nvidia/version \r\nNVRM version: NVIDIA UNIX x86_64 Kernel Module  550.107.02  Wed Jul 24 23:53:00 UTC 2024\r\nGCC version:  gcc version 14.2.1 20240805 (GCC) \r\n$ gcc-12 --version\r\ngcc-12 (GCC) 12.3.0\r\n```\r\n\r\n\r\n\r\n### Name and Version\r\n\r\n$ /opt/llama/bin/llama-cli --version\r\nversion: 3681 (df270ef7)\r\nbuilt with gcc-12 (GCC) 12.3.0 for x86_64-pc-linux-gnu\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nLinux\r\n\r\n### Relevant log output\r\n\r\n_No response_",
    "labels": [
      "bug",
      "medium severity"
    ],
    "state": "open",
    "created_at": "2024-09-09T15:52:21+00:00",
    "closed_at": null,
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9391/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9391"
  },
  {
    "number": 10031,
    "title": "Bug: issue in CUDA flash attention",
    "body": "### What happened?\n\nThere seems to be some memory corruption issue in the CUDA flash attention kernel. \r\n\r\nThis is demonstrated by the debug prints inserted before and after the `vec_dot_KQ` device function here:\r\nhttps://github.com/ggerganov/llama.cpp/compare/master...agray3:llama.cpp:ag_demonstrate_fattn_memory_issue\r\n\r\nThese print out the first element of Q_ds, which is const in the function so shouldn't be altered. (`Q_ds` is in local memory so it also shouldn't be altered by any other thread.)\r\n\r\nHowever we get the result: \r\nBefore vec_dot_KQ: Q_ds=-32752.000000\r\nAfter vec_dot_KQ: Q_ds=nan\r\n\r\nQ_ds is being altered and becoming NAN. This is reproducible across different GPUs and models. \n\n### Name and Version\n\nversion: 3964 (3488adf3)\r\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-10-24T08:08:06+00:00",
    "closed_at": "2024-10-24T11:11:31+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10031/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10031"
  },
  {
    "number": 9005,
    "title": "Llama-Quantize : Layers quantized in the wrong order, thus damaging the variable bits tensor quants scheme consistency.",
    "body": "### What happened?\r\n\r\nOn master b3573, when quantizing Gemma 9b it:\r\n\r\nThe tensors are quantized in a wrong order.\r\n\r\nRight now, because of the layer jump from 7 to 10 without the ffns of layer 7 to be quantized, it breaks not only the layer quantization order, but also the correlation between ffn_down Q6_K and attn_v Q6_K : From layer 7, some layers will have ffn_down Q6_K and attn_v Q5_K, and some others ffn_down Q5_K and attn_v Q6_K.\r\nThis gives us suboptimal quants per BPW.\r\n\r\nI expect the tensors to be quantized in the right order.\r\n\r\nThis, so the Q5_K_M quant, as well as the othersusing \"use_more_bits(i_layer, n_layer)\" to have a variable quant of ffn_down in conjunction with \"use_more_bits(qs.i_attention_wv, qs.n_attention_wv))\" to have a variable quant of attn_v.weight, can be optimal.\r\n\r\n### Name and Version\r\n\r\nmain: build = 3573 (2589292c)\r\nmain: built with MSVC 19.29.30154.0 for x64\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nWindows\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n[  45/ 464]                  blk.3.attn_v.weight - [ 3584,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\r\n[  46/ 464]               blk.4.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\r\n[  47/ 464]                blk.4.ffn_down.weight - [14336,  3584,     1,     1], type =   bf16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\r\n[  48/ 464]                blk.4.ffn_gate.weight - [ 3584, 14336,     1,     1], type =   bf16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\r\n[  49/ 464]                  blk.4.ffn_up.weight - [ 3584, 14336,     1,     1], type =   bf16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\r\n[  50/ 464]     blk.4.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\r\n[  51/ 464]           blk.4.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\r\n[  52/ 464]                blk.4.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\r\n[  53/ 464]                  blk.4.attn_k.weight - [ 3584,  2048,     1,     1], type =   bf16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\r\n[  54/ 464]             blk.4.attn_output.weight - [ 4096,  3584,     1,     1], type =   bf16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\r\n[  55/ 464]                  blk.4.attn_q.weight - [ 3584,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\r\n[  56/ 464]                  blk.4.attn_v.weight - [ 3584,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\r\n[  57/ 464]               blk.5.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\r\n[  58/ 464]                blk.5.ffn_down.weight - [14336,  3584,     1,     1], type =   bf16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\r\n[  59/ 464]                blk.5.ffn_gate.weight - [ 3584, 14336,     1,     1], type =   bf16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\r\n[  60/ 464]                  blk.5.ffn_up.weight - [ 3584, 14336,     1,     1], type =   bf16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\r\n[  61/ 464]     blk.5.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\r\n[  62/ 464]           blk.5.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\r\n[  63/ 464]                blk.5.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\r\n[  64/ 464]                  blk.5.attn_k.weight - [ 3584,  2048,     1,     1], type =   bf16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\r\n[  65/ 464]             blk.5.attn_output.weight - [ 4096,  3584,     1,     1], type =   bf16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\r\n[  66/ 464]                  blk.5.attn_q.weight - [ 3584,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\r\n[  67/ 464]                  blk.5.attn_v.weight - [ 3584,  2048,     1,     1], type =   bf16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\r\n[  68/ 464]               blk.6.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\r\n[  69/ 464]                blk.6.ffn_down.weight - [14336,  3584,     1,     1], type =   bf16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\r\n[  70/ 464]                blk.6.ffn_gate.weight - [ 3584, 14336,     1,     1], type =   bf16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\r\n[  71/ 464]                  blk.6.ffn_up.weight - [ 3584, 14336,     1,     1], type =   bf16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\r\n[  72/ 464]     blk.6.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\r\n[  73/ 464]           blk.6.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\r\n[  74/ 464]                blk.6.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\r\n[  75/ 464]                  blk.6.attn_k.weight - [ 3584,  2048,     1,     1], type =   bf16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\r\n[  76/ 464]             blk.6.attn_output.weight - [ 4096,  3584,     1,     1], type =   bf16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\r\n[  77/ 464]                  blk.6.attn_q.weight - [ 3584,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\r\n[  78/ 464]                  blk.6.attn_v.weight - [ 3584,  2048,     1,     1], type =   bf16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\r\n[  79/ 464]                blk.7.ffn_gate.weight - [ 3584, 14336,     1,     1], type =   bf16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\r\n[  80/ 464]                  blk.7.ffn_up.weight - [ 3584, 14336,     1,     1], type =   bf16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\r\n[  81/ 464]                  blk.7.attn_k.weight - [ 3584,  2048,     1,     1], type =   bf16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\r\n[  82/ 464]             blk.7.attn_output.weight - [ 4096,  3584,     1,     1], type =   bf16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\r\n[  83/ 464]                  blk.7.attn_q.weight - [ 3584,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\r\n[  84/ 464]                  blk.7.attn_v.weight - [ 3584,  2048,     1,     1], type =   bf16, converting to q6_K .. size =    14.00 MiB ->     5.74 MiB\r\n[  85/ 464]              blk.10.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\r\n[  86/ 464]               blk.10.ffn_down.weight - [14336,  3584,     1,     1], type =   bf16, converting to q6_K .. size =    98.00 MiB ->    40.20 MiB\r\n[  87/ 464]               blk.10.ffn_gate.weight - [ 3584, 14336,     1,     1], type =   bf16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\r\n[  88/ 464]                 blk.10.ffn_up.weight - [ 3584, 14336,     1,     1], type =   bf16, converting to q5_K .. size =    98.00 MiB ->    33.69 MiB\r\n[  89/ 464]    blk.10.post_attention_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\r\n[  90/ 464]          blk.10.post_ffw_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\r\n[  91/ 464]               blk.10.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\r\n[  92/ 464]                 blk.10.attn_k.weight - [ 3584,  2048,     1,     1], type =   bf16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\r\n[  93/ 464]            blk.10.attn_output.weight - [ 4096,  3584,     1,     1], type =   bf16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\r\n[  94/ 464]                 blk.10.attn_q.weight - [ 3584,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    28.00 MiB ->     9.62 MiB\r\n[  95/ 464]                 blk.10.attn_v.weight - [ 3584,  2048,     1,     1], type =   bf16, converting to q5_K .. size =    14.00 MiB ->     4.81 MiB\r\n[  96/ 464]              blk.11.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-08-12T12:59:04+00:00",
    "closed_at": "2024-09-27T01:07:21+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9005/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9005"
  },
  {
    "number": 8451,
    "title": "Unable to convert a fireworks ai model to GGUF with gguf-my-repo",
    "body": "### What happened?\r\n\r\nI  downloaded one of my models from fireworks.ai and pushed it up into huggingface - you can find it here: [llama-3-8b-instruct-danish](https://huggingface.co/HeRksTAn/llama-3-8B-Instruct-Danish)\r\n\r\nI then tried  [gguf-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo) in order to convert it to gguf. \r\n\r\n\r\n1. using https://huggingface.co/spaces/ggml-org/gguf-my-repo \r\n2. logged into my account\r\n3. search the hub id for the repository I want converted into gguf, which is HeRksTAn/llama-3-8B-Instruct-Danish\r\n4. I chose Q4_K_M\r\n5. I clicked submit\r\n\r\n\r\nI get the following error \r\n\r\n`Error: Error converting to fp16: b'INFO:hf-to-gguf:Loading model: llama-3-8B-Instruct-Danish\\nTraceback (most recent call last):\\n File \"/home/user/app/llama.cpp/convert_hf_to_gguf.py\", line 3551, in \\n main()\\n File \"/home/user/app/llama.cpp/convert_hf_to_gguf.py\", line 3517, in main\\n hparams = Model.load_hparams(dir_model)\\n File \"/home/user/app/llama.cpp/convert_hf_to_gguf.py\", line 356, in load_hparams\\n with open(dir_model / \"config.json\", \"r\", encoding=\"utf-8\") as f:\\nFileNotFoundError: [Errno 2] No such file or directory: 'llama-3-8B-Instruct-Danish/config.json'\\n'`\r\n\r\nIt seems like I'm missing some files - can you please pinpoint what I'm missing? or what I should request from fireworks  ai, so that I can convert it to GGUF?\r\n\r\n\r\n![error-converting-to-gguf](https://github.com/user-attachments/assets/f064fc64-7d6c-41da-87ad-db9b8befe3cd)\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n`Error: Error converting to fp16: b'INFO:hf-to-gguf:Loading model: llama-3-8B-Instruct-Danish\\nTraceback (most recent call last):\\n File \"/home/user/app/llama.cpp/convert_hf_to_gguf.py\", line 3551, in \\n main()\\n File \"/home/user/app/llama.cpp/convert_hf_to_gguf.py\", line 3517, in main\\n hparams = Model.load_hparams(dir_model)\\n File \"/home/user/app/llama.cpp/convert_hf_to_gguf.py\", line 356, in load_hparams\\n with open(dir_model / \"config.json\", \"r\", encoding=\"utf-8\") as f:\\nFileNotFoundError: [Errno 2] No such file or directory: 'llama-3-8B-Instruct-Danish/config.json'\\n'`\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-07-12T09:14:13+00:00",
    "closed_at": "2024-07-23T10:53:50+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8451/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8451"
  },
  {
    "number": 8862,
    "title": "Bug: src/llama.cpp:15099: Deepseek2 does not support K-shift",
    "body": "### What happened?\n\nHi, when stress testing llama-server (--parallel 3, prompt=\"Count 1 to 10000 in words\") and running deepseek-coder-v2:16b-lite-instruct-q8_0  i got this assertion error in the logs and everything stopped working, so i have to restart llm-server.\r\n\r\n**Startup script:**\r\n\r\n~/llama.cpp/llama-server -m /usr/share/ollama/.ollama/models/blobs/sha256-373dcfc92e01372709b6164fc836f677a6280e25e9eac5c434c64223207bfc4f --port 8000 --host 0.0.0.0 -ngl 28 -c 24600 --threads 16 --parallel 3 --log-format text --predict -2 --logdir ~/llama.cpp/logs --log-append   $1 $2 >> ~/llama.cpp/logs/deepseek.log 2>&1\r\n\r\n\r\n\r\n\n\n### Name and Version\n\nversion: 3509 (ecf6b7f2)\r\nbuilt with cc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-22.0.1) for x86_64-redhat-linux\r\n\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n```shell\n**Logs just before it crashed:**\r\n\r\nINFO [   launch_slot_with_task] slot is processing task | tid=\"139873529581568\" timestamp=1722830275 id_slot=2 id_task=8969\r\nINFO [            update_slots] kv cache rm [p0, end) | tid=\"139873529581568\" timestamp=1722830275 id_slot=2 id_task=8969 p0=0\r\nINFO [            update_slots] kv cache rm [p0, end) | tid=\"139873529581568\" timestamp=1722830276 id_slot=2 id_task=8969 p0=2046\r\nINFO [            update_slots] kv cache rm [p0, end) | tid=\"139873529581568\" timestamp=1722830277 id_slot=2 id_task=8969 p0=4092\r\nINFO [            update_slots] kv cache rm [p0, end) | tid=\"139873529581568\" timestamp=1722830278 id_slot=2 id_task=8969 p0=6138\r\nINFO [            update_slots] slot context shift | tid=\"139873529581568\" timestamp=1722830294 id_slot=2 id_task=8969 n_keep=1 n_left=8200 n_discard=4100 n_ctx=24608 n_past=8201 n_system_tokens=0 n_cache_tokens=6138\r\nDeepseek2 does not support K-shift\r\n\r\n\r\n**My startup logs:**\r\n\r\nINFO [                    main] build info | tid=\"139628879708160\" timestamp=1722830350 build=3509 commit=\"ecf6b7f2\"\r\nINFO [                    main] system info | tid=\"139628879708160\" timestamp=1722830350 n_threads=16 n_threads_batch=-1 total_threads=32 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \"\r\nllama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-373dcfc92e01372709b6164fc836f677a6280e25e9eac5c434c64223207bfc4f (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = deepseek2\r\nllama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct\r\nllama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27\r\nllama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840\r\nllama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048\r\nllama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944\r\nllama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16\r\nllama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16\r\nllama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000\r\nllama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 7\r\nllama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1\r\nllama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400\r\nllama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512\r\nllama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192\r\nllama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128\r\nllama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408\r\nllama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64\r\nllama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2\r\nllama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000\r\nllama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64\r\nllama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn\r\nllama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000\r\nllama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096\r\nllama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700\r\nllama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm\r\nllama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [\"?|  ?| \", \"?|  t\", \"?|  a\", \"i n\", \"h e...\r\nllama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000\r\nllama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001\r\nllama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001\r\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\r\nllama_model_loader: - kv  37:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  108 tensors\r\nllama_model_loader: - type q8_0:  269 tensors\r\nllm_load_vocab: special tokens cache size = 2400\r\nllm_load_vocab: token to piece cache size = 0.6661 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = deepseek2\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 102400\r\nllm_load_print_meta: n_merges         = 99757\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 163840\r\nllm_load_print_meta: n_embd           = 2048\r\nllm_load_print_meta: n_layer          = 27\r\nllm_load_print_meta: n_head           = 16\r\nllm_load_print_meta: n_head_kv        = 16\r\nllm_load_print_meta: n_rot            = 64\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 192\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 3072\r\nllm_load_print_meta: n_embd_v_gqa     = 2048\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 10944\r\nllm_load_print_meta: n_expert         = 64\r\nllm_load_print_meta: n_expert_used    = 6\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = yarn\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 0.025\r\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\n\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 16B\r\nllm_load_print_meta: model ftype      = Q8_0\r\nllm_load_print_meta: model params     = 15.71 B\r\nllm_load_print_meta: model size       = 15.55 GiB (8.51 BPW)\r\nllm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct\r\nllm_load_print_meta: BOS token        = 100000 '<?~\\begin?~V~Aof?~V~Asentence?~\\>'\r\nllm_load_print_meta: EOS token        = 100001 '<?~\\end?~V~Aof?~V~Asentence?~\\>'\r\nllm_load_print_meta: PAD token        = 100001 '<?~\\end?~V~Aof?~V~Asentence?~\\>'\r\nllm_load_print_meta: LF token         = 126 '?~D'\r\nllm_load_print_meta: max token length = 256\r\nllm_load_print_meta: n_layer_dense_lead   = 1\r\nllm_load_print_meta: n_lora_q             = 0\r\nllm_load_print_meta: n_lora_kv            = 512\r\nllm_load_print_meta: n_ff_exp             = 1408\r\nllm_load_print_meta: n_expert_shared      = 2\r\nllm_load_print_meta: expert_weights_scale = 1.0\r\nllm_load_print_meta: rope_yarn_log_mul    = 0.0707\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\r\nllm_load_tensors: ggml ctx size =    0.32 MiB\r\nllm_load_tensors: offloading 27 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 28/28 layers to GPU\r\nllm_load_tensors:        CPU buffer size =   212.50 MiB\r\nllm_load_tensors:      CUDA0 buffer size = 15712.47 MiB\r\n.......................................................................................\r\nllama_new_context_with_model: n_ctx      = 24608\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 0.025\r\nllama_kv_cache_init:      CUDA0 KV buffer size =  6488.44 MiB\r\nllama_new_context_with_model: KV self size  = 6488.44 MiB, K (f16): 3893.06 MiB, V (f16): 2595.38 MiB\r\nllama_new_context_with_model:  CUDA_Host  output buffer size =     1.56 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =   841.07 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =    52.07 MiB\r\nllama_new_context_with_model: graph nodes  = 1924\r\nllama_new_context_with_model: graph splits = 2\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-08-05T04:14:23+00:00",
    "closed_at": "2024-10-30T01:19:54+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8862/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8862"
  },
  {
    "number": 7813,
    "title": "Qwen2-57B-A14B-Instruct not supported",
    "body": "### What happened?\n\nThe converted model fails to load due to unexpected expert tensor dimensions, the current qwen2moe implementation expects it to be `n_ff`/`n_expert_used`, which it is not.\n\n### Name and Version\n\n./main --version\r\nversion: 3066 (e141ce62)\r\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\nllama_model_load: error loading model: check_tensor_dims: tensor 'blk.0.ffn_gate_exps.weight' has wrong shape; expected  3584,  2368,    64, got  3584,  2560,    64,     1\n```\n",
    "labels": [
      "bug-unconfirmed",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-06-07T08:47:04+00:00",
    "closed_at": "2024-06-07T10:00:28+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7813/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7813"
  },
  {
    "number": 8001,
    "title": "Bug: llama-server + LLava 1.6 hallucinates",
    "body": "### What happened?\r\n\r\nWhen using `./llama-llava-cli `, I get perfectly fine descriptions of images. But when hosting LLava with `./llama-server`, LLava hallucinates big time. \r\n\r\nHere's how I'm running LLava with the cli:\r\n`./llama-llava-cli -m models/llava-v1.6-vicuna-7b.Q5_K_S.gguf --mmproj models/mmproj-model-f16.gguf --image images/sth.jpeg -c 4096`\r\n\r\nHere's how I'm starting the server:\r\n` ./llama-server -m models/llava-v1.6-vicuna-7b.Q5_K_S.gguf --mmproj models/mmproj-model-f16.gguf -c 2048  --host 127.0.0.1 --port 8000`\r\n\r\nHere's the python code to send the request:\r\n```\r\nimport requests\r\nimport base64\r\n\r\ndef encode_image(image_path):\r\n  with open(image_path, \"rb\") as image_file:\r\n    return base64.b64encode(image_file.read()).decode('utf-8')\r\n\r\nbase64_image = encode_image(\"./images/sth.png\")\r\n      \r\nheaders = {\r\n    'Content-Type': 'application/json',\r\n}\r\n\r\njson_data = {\r\n    'image_data': [{\r\n        'data': base64_image, \r\n        'id': 10\r\n    }],\r\n    \"prompt\": \"USER:[img-10]Describe the image.\\nASSISTANT:\",\r\n\t\"temperature\": 0.1\r\n}\r\n\r\nresponse = requests.post('http://127.0.0.1:8000/completion', headers=headers, json=json_data)\r\nprint(response.json()[\"content\"])\r\n```\r\n\r\n\r\n### Name and Version\r\n\r\n```\r\n./llama-cli --version\r\nversion: 3173 (a94e6ff8)\r\nbuilt with Apple clang version 15.0.0 (clang-1500.3.9.4) for arm64-apple-darwin23.5.0\r\n```\r\n\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nMac\r\n\r\n### Relevant log output\r\n\r\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-06-19T05:40:15+00:00",
    "closed_at": "2024-08-03T01:18:10+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8001/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8001"
  },
  {
    "number": 9316,
    "title": "Bug: llama-perplexity error using multiple-choice binary data",
    "body": "### What happened?\r\n\r\n\"The multiple choice evaluation has been broken in llama.cpp via commit 6ff13987a.\r\n\r\nThe multiple choice evaluation uses binary data stored in params.prompt. Commit 6ff13987a adds prompt escape character processing, which modifies the binary data and renders it unusable. To preserve whatever utility 6ff13987a might have added, we add a flag indicating if the data stored in params.prompt is binary and, if so, avoid the escape processing.\"  @ikawrakow\r\n\r\n@ikawrakow solved the problem in his llama.cpp fork in the following PR: https://github.com/ikawrakow/ik_llama.cpp/pull/33\r\n\r\n\r\n\r\n### Name and Version\r\n\r\nI tested the issue with the docker release of llama.cpp:\r\n\r\n ghcr.io/ggerganov/llama.cpp:full-cuda--b1-98a532d\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nLinux\r\n\r\n### Relevant log output\r\n\r\n_No response_",
    "labels": [
      "bug",
      "medium severity"
    ],
    "state": "open",
    "created_at": "2024-09-04T19:41:26+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9316/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9316"
  },
  {
    "number": 8378,
    "title": "Bug: ggml/src/ggml.c: In function 'ggml_vec_mad_f16':",
    "body": "### What happened?\r\n`GGML_CUDA=1 make -j`\r\n...\r\n```\r\nggml/src/ggml.c: In function 'ggml_vec_mad_f16':\r\nggml/src/ggml.c:2039:45: warning: passing argument 1 of '__sse_f16x4_load' discards 'const' qualifier from pointer target type [-Wdiscarded-qualifiers]\r\n 2039 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);\r\n      |                                             ^\r\nggml/src/ggml.c:1491:50: note: in definition of macro 'GGML_F32Cx4_LOAD'\r\n 1491 | #define GGML_F32Cx4_LOAD(x)     __sse_f16x4_load(x)\r\n      |                                                  ^\r\nggml/src/ggml.c:2039:21: note: in expansion of macro 'GGML_F16_VEC_LOAD'\r\n 2039 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nggml/src/ggml.c:1466:52: note: expected 'ggml_fp16_t *' {aka 'short unsigned int *'} but argument is of type 'const ggml_fp16_t *' {aka 'const short unsigned int *'}\r\n 1466 | static inline __m128 __sse_f16x4_load(ggml_fp16_t *x) {\r\n      |                                       ~~~~~~~~~~~~~^\r\n```\r\n...\r\n\r\nI expected no error\r\n\r\n### Name and Version\r\n\r\n```\r\n$ ./llama-cli --version\r\nversion: 3349 (a130ecce)\r\nbuilt with x86_64-conda-linux-gnu-cc (conda-forge gcc 12.3.0-13) 12.3.0 for x86_64-conda-linux-gnu\r\n```\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nLinux\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nggml/src/ggml.c: In function 'ggml_vec_mad_f16':\r\nggml/src/ggml.c:2039:45: warning: passing argument 1 of '__sse_f16x4_load' discards 'const' qualifier from pointer target type [-Wdiscarded-qualifiers]\r\n 2039 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);\r\n      |                                             ^\r\nggml/src/ggml.c:1491:50: note: in definition of macro 'GGML_F32Cx4_LOAD'\r\n 1491 | #define GGML_F32Cx4_LOAD(x)     __sse_f16x4_load(x)\r\n      |                                                  ^\r\nggml/src/ggml.c:2039:21: note: in expansion of macro 'GGML_F16_VEC_LOAD'\r\n 2039 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nggml/src/ggml.c:1466:52: note: expected 'ggml_fp16_t *' {aka 'short unsigned int *'} but argument is of type 'const ggml_fp16_t *' {aka 'const short unsigned int *'}\r\n 1466 | static inline __m128 __sse_f16x4_load(ggml_fp16_t *x) {\r\n      |                                       ~~~~~~~~~~~~~^\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-07-08T21:20:30+00:00",
    "closed_at": "2024-08-26T01:07:03+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8378/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8378"
  },
  {
    "number": 7709,
    "title": "Bug: Phi-3 4K output broken after 2000~ tokens (Reproducible)",
    "body": "### What happened?\r\n\r\nTo reproduce:\r\nDownload the official released gguf model from huggingface/microsoft.\r\nRun **server.exe -m Phi3-mini-4k.gguf -c 4096**\r\n\r\nWhen input prompt < ~2048: Output fine. (but output starts getting weird right after it hits ~2048 in total)\r\nWhen input prompt > ~2048: Output weird.\r\n\r\nThe weird output seems like what we expect to see when the context is more than the model support, but happens in ~2048, which seems like there are some bugs.\r\n\r\nAlso tested Llama3-8B, works fine with input prompt < 8192 as expected (with -c 8192), also works fine with input prompt < 4096 as expected (with -c 4096).\r\n\r\n### Name and Version\r\n\r\nversion: 3015 (74b239b3)\r\nbuilt with MSVC 19.39.33523.0 for x64\r\n\r\nTried both cuda and avx2 version.\r\n\r\nAlso tried latest version built it myself @ Intel SYCL\r\nversion: 3075 (3d7ebf63)\r\nbuilt with IntelLLVM 2024.1.0\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nWin10, Win11\r\n\r\n### Relevant log output\r\n\r\nBefore ~2000 tokens and after\r\n![\u5716\u7247](https://github.com/ggerganov/llama.cpp/assets/23719775/22543e99-7999-4dc9-99af-25e42d22397f)\r\n",
    "labels": [
      "bug",
      "model",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-06-03T07:25:37+00:00",
    "closed_at": "2024-12-27T12:33:27+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7709/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7709"
  },
  {
    "number": 8853,
    "title": "Bug: Gemma 2 incoherent output when using quantized k cache without Flash Attention",
    "body": "### What happened?\r\n\r\nOutput like \"Mh gi\u00e0u \u3055\u308c rodas reliablyacheteur\u03b4\u03b5 S\u0105\" happens when using quantized K cache, CUDA, with Gemma 2. Here's how to reproduce:\r\n\r\n./llama-server -m \"Gemma-2-9B-It-SPPO-Iter3-Q4_K_S.gguf\" -t 6 -c 8192 -ngl 31 -ctk q4_0 --host 127.0.0.1 --port 8080\r\n\r\nThen connect a frontend like SillyTavern to it. Strangely this only happens with server, not with main-cli. \r\n\r\nThis leads to incoherent output. Note: I can't say if this issue happens when using full offloading, as I just have 6 GB VRAM. \r\n\r\n\r\n### Name and Version\r\n\r\n ./llama-cli --version\r\nversion: 3506 (76614f35)\r\nbuilt with MSVC 19.29.30154.0 for x64\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nWindows\r\n\r\n### Relevant log output\r\n\r\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-08-04T10:57:51+00:00",
    "closed_at": "2024-09-18T01:07:04+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8853/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8853"
  }
]
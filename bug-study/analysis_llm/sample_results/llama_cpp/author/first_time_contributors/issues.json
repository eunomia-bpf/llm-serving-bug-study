[
  {
    "number": 1869,
    "title": "train-text-from-scratch.exe stop after \"begin training\" (tensor->src0 is null)",
    "body": "I'm running the latest release (master-254a7a7) like that:\r\n\r\n`bin\\train-text-from-scratch.exe --vocab-model models\\ggml-vocab.bin --checkpoint-in chk-lamartine-256x16.bin --checkpoint-out chk-lamartine-256x16.bin --model-out ggml-lamartine-265x16-f32.bin --train-data \"shakespeare.txt\"           `\r\nI tried with several models.\r\n\r\n# Expected Behavior\r\n\r\nTraining shoud run for a long time\r\n\r\n# Current Behavior\r\n\r\nTraining stop immediatly without error:\r\n\r\n```\r\nD:\\git\\llama.cpp>bin\\train-text-from-scratch.exe --vocab-model models\\ggml-vocab.bin --ctx 64 --embd 256 --head 8 --layer 16 --checkpoint-in chk-lamartine-256x16.bin --checkpoint-out chk-lamartine-256x16.bin --model-out ggml-lamartine-265x16-f32.bin --train-data \"alphonsedelamartine.txt\" -t 6 -b 1 -n 32 --seed 2 --adam-iter 16 --print-details-interval 0 --predict 16 --use-flash\r\nmain: seed: 2\r\nllama.cpp: loading model from models\\ggml-vocab.bin\r\nllama_model_load_internal: format     = ggjt v1 (pre #1405)\r\nllama_model_load_internal: n_vocab    = 32000\r\nllama_model_load_internal: n_ctx      = 512\r\nllama_model_load_internal: n_embd     = 4096\r\nllama_model_load_internal: n_mult     = 256\r\nllama_model_load_internal: n_head     = 32\r\nllama_model_load_internal: n_layer    = 32\r\nllama_model_load_internal: n_rot      = 128\r\nllama_model_load_internal: ftype      = 1 (mostly F16)\r\nllama_model_load_internal: n_ff       = 11008\r\nllama_model_load_internal: n_parts    = 1\r\nllama_model_load_internal: model size = 7B\r\nmain: tokenize training data\r\nmain: number of training tokens: 474\r\nprint_params: n_vocab: 32000\r\nprint_params: n_ctx:   64\r\nprint_params: n_embd:  256\r\nprint_params: n_mult:  256\r\nprint_params: n_head:  8\r\nprint_params: n_ff:    768\r\nprint_params: n_layer: 16\r\nprint_params: n_rot:   32\r\nmain: number of unique tokens: 253\r\nggml_init_cublas: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 3080\r\nmain: init model\r\nload_checkpoint: Training iterations: 0.\r\nload_checkpoint: Training samples:    0.\r\nload_checkpoint: Training tokens:     0.\r\nmain: opt iter 0\r\nused_mem model+cache: 242364416 bytes\r\nmain: begin training\r\n```\r\n\r\n# Environment and Context\r\n\r\nWindows 11\r\nNVidia RTX 3080\r\nRyzen 7 2700\r\nRam 32GB",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-06-15T07:26:00+00:00",
    "closed_at": "2024-04-10T01:07:05+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1869/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1869"
  },
  {
    "number": 10235,
    "title": "Bug: server GET /props request return json with chat_template with last char replaced by \\x00",
    "body": "### What happened?\n\nexamples/server/utils.hpp\r\nstatic std::string llama_get_chat_template(const struct llama_model * model) {\r\n    std::string template_key = \"tokenizer.chat_template\";\r\n    // call with NULL buffer to get the total size of the string\r\n    int32_t res = llama_model_meta_val_str(model, template_key.c_str(), NULL, 0);\r\n    if (res < 0) {\r\n        return \"\";\r\n    } else {\r\n        std::vector<char> model_template(res, 0);\r\n        llama_model_meta_val_str(model, template_key.c_str(), model_template.data(), model_template.size());\r\n        return std::string(model_template.data(), model_template.size());\r\n    }\r\n}\r\nsrc/llama.cc\r\nint32_t llama_model_meta_val_str(const struct llama_model * model, const char * key, char * buf, size_t buf_size) {\r\n    const auto & it = model->gguf_kv.find(key);\r\n    if (it == model->gguf_kv.end()) {\r\n        if (buf_size > 0) {\r\n            buf[0] = '\\0';\r\n        }\r\n        return -1;\r\n    }\r\n    return snprintf(buf, buf_size, \"%s\", it->second.c_str());\r\n}\r\nC function snprintf add \\x00 to the end of buffer and replace the last char of chat_template by \\x00\r\n\n\n### Name and Version\n\nC:\\llama.cpp>llama-cli --version\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 2050, compute capability 8.6, VMM: yes\r\nversion: 4055 (e8921349)\r\nbuilt with MSVC 19.29.30152.0 for x64\r\n\n\n### What operating system are you seeing the problem on?\n\nLinux, Windows\n\n### Relevant log output\n\n```shell\n# ***********************************************************************************\r\n\t# execute request\r\n\t\r\n\theaders = {\r\n\t\t'Content-Type': 'application/json',\r\n\t}\r\n\r\n\tprint(\"endpoint_url: {}\".format(endpoint_url));\r\n\tresponse = requests.request(\"GET\", endpoint_url, headers = headers);\r\n\tresponse.close();\r\n\t\r\n\t\r\n\t# ***********************************************************************************\r\n\t# handle response\r\n\r\n\tresult = response.json();\r\n\r\n\tprint(result);\r\n\r\n============================ RESULT JSON =====================\r\nendpoint_url: http://127.0.0.1:8080/props\r\n{'default_generation_settings': {'n_ctx': 8192, 'n_predict': -1, 'model': 'saiga_nemo_12b', 'seed': -1, 'seed_cur': 0, 'temperature': 0.800000011920929, 'dynatemp_range': 0.0, 'dynatemp_exponent': 1.0, 'top_k': 40, 'top_p': 0.949999988079071, 'min_p': 0.05000000074505806, 'xtc_probability': 0.0, 'xtc_threshold': 0.10000000149011612, 'typical_p': 1.0, 'repeat_last_n': 64, 'repeat_penalty': 1.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'dry_multiplier': 0.0, 'dry_base': 1.75, 'dry_allowed_length': 2, 'dry_penalty_last_n': -1, 'dry_sequence_breakers': ['\\n', ':', '\"', '*'], 'mirostat': 0, 'mirostat_tau': 5.0, 'mirostat_eta': 0.10000000149011612, 'penalize_nl': False, 'stop': [], 'max_tokens': -1, 'n_keep': 0, 'n_discard': 0, 'ignore_eos': False, 'stream': True, 'n_probs': 0, 'min_keep': 0, 'grammar': '', 'samplers': ['dry', 'top_k', 'typ_p', 'top_p', 'min_p', 'xtc', 'temperature']}, 'total_slots': 1, 'chat_template': \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] | trim + '\\n\\n' %}{% set messages = messages[1:] %}{% else %}{% set system_message = '' %}{% endif %}{{- bos_token + system_message}}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] | trim + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] | trim + eos_token }}{% endif %}{% endfor %\\x00\"}\r\n===== at the and of chat_template: ====\r\n{% endfor %\\x00\"\r\n=====\r\nlast '}' replaced by \\x00\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-11-09T10:25:16+00:00",
    "closed_at": "2024-12-25T01:07:19+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10235/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10235"
  },
  {
    "number": 1279,
    "title": " incompatible types when initializing type \u2018__m256i {aka __vector(4) long long int}\u2019",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nsuccessful compilation of llama.cpp\r\n\r\n# Current Behavior\r\n\r\nsh-4.2$ make\r\nI llama.cpp build info:\r\nI UNAME_S: Linux\r\nI UNAME_P: x86_64\r\nI UNAME_M: x86_64\r\nI CFLAGS: -I. -O3 -std=c11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native\r\nI CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native\r\nI LDFLAGS:\r\nI CC: cc (GCC) 7.3.1 20180712 (Red Hat 7.3.1-15)\r\nI CXX: g++ (GCC) 7.3.1 20180712 (Red Hat 7.3.1-15)\r\n\r\ncc -I. -O3 -std=c11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -c ggml.c -o ggml.o\r\nggml.c: In function \u2018ggml_vec_dot_q4_2_q8_0\u2019:\r\nggml.c:3069:40: warning: implicit declaration of function \u2018_mm256_set_m128\u2019; did you mean \u2018_mm256_set_epi8\u2019? [-Wimplicit-function-declaration]\r\nconst __m256 d = _mm256_mul_ps(_mm256_set_m128(d1, d0), _mm256_broadcast_ss(&y[i].d));\r\n^~~~~~~~~~~~~~~\r\n_mm256_set_epi8\r\nggml.c:3069:40: error: incompatible type for argument 1 of \u2018_mm256_mul_ps\u2019\r\nIn file included from /usr/lib/gcc/x86_64-redhat-linux/7/include/immintrin.h:41:0,\r\nfrom ggml.c:183:\r\n/usr/lib/gcc/x86_64-redhat-linux/7/include/avxintrin.h:317:1: note: expected \u2018__m256 {aka __vector(8) float}\u2019 but argument is of type \u2018int\u2019\r\n_mm256_mul_ps (__m256 __A, __m256 __B)\r\n^~~~~~~~~~~~~\r\nggml.c:3073:22: warning: implicit declaration of function \u2018_mm256_set_m128i\u2019; did you mean \u2018_mm256_set_epi8\u2019? [-Wimplicit-function-declaration]\r\n__m256i bx = _mm256_set_m128i(bx1, bx0);\r\n^~~~~~~~~~~~~~~~\r\n_mm256_set_epi8\r\nggml.c:3073:22: error: incompatible types when initializing type \u2018__m256i {aka __vector(4) long long int}\u2019 using type \u2018int\u2019\r\nmake: *** [ggml.o] Error 1\r\n\r\n# Environment and Context\r\n\r\nAWS linux 2, ml.g5.48xlarge\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              192\r\nOn-line CPU(s) list: 0-191\r\nThread(s) per core:  2\r\nCore(s) per socket:  48\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\namazon linux 2\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\npython3.9\r\n\r\nGNU Make 3.82\r\nBuilt for x86_64-koji-linux-gnu\r\n\r\ng++ (GCC) 7.3.1 20180712 (Red Hat 7.3.1-15)\r\n```\r\n# Failure Information (for bugs)\r\nggml.c:3073:22: error: incompatible types when initializing type \u2018__m256i {aka __vector(4) long long int}\u2019 using type \u2018int\u2019",
    "labels": [
      "bug",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-05-02T14:38:01+00:00",
    "closed_at": "2023-07-28T19:53:52+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1279/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1279"
  },
  {
    "number": 9200,
    "title": "Bug: Worse llama_decode performance during generation after evaluated big batch with large number of output logits requested",
    "body": "### What happened?\n\nWhen I'm making following calls to llama_decode:\r\n\r\n1. Evaluate large batch of tokens with all batch.logits[i] = False.\r\n2. Evaluate multiple times batches of 1 token with batch.logits[0] = True\r\n3. Evaluate large batch of tokens with all or many batch.logits[i] = True\r\n\r\nand repeat it again, then on the second run step 2 runs noticeably slower. Steps 1 and 3 run in the same time.\r\n\r\nI suspect this might be happening because on the second run output buffer ctx.buf_output remain same large size as was resized in step 3, and it gets resetted whole with 0 with every llama_decode call despite only first n_vocab elements are required.\n\n### Name and Version\n\nI'm using llama-cpp-python==0.2.89\n\n### What operating system are you seeing the problem on?\n\nWindows\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-08-27T08:50:43+00:00",
    "closed_at": "2024-10-11T01:07:13+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9200/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9200"
  },
  {
    "number": 2241,
    "title": "[User] faild to find n_mult number from range 256, with n_ff = 3072",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\ni'm not sure if i should\r\nchange this line to `for n_mult in range(3000, 1, -1):`\r\n\r\n# Current Behavior\r\nmodel tried to convert https://huggingface.co/symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli/tree/main\r\n\r\nparams: `n_vocab:250000 n_embd:768 n_head:12 n_layer:12`\r\nfailed to find n_mult number\r\n\r\nwhen i change the range started from 3000:\r\n```\r\nroot@jenkins-ddt:~/github/llama.cpp# ./quantize models/sn-xlm-roberta-base-snli-mnli-anli-xnli/ggml-model-f16.bin models/sn-xlm-roberta-base-snli-mnli-anli-xnli/ggml-model-q4_0.bin q4_0\r\nmain: build = 812 (1d16309)\r\nmain: quantizing 'models/sn-xlm-roberta-base-snli-mnli-anli-xnli/ggml-model-f16.bin' to 'models/sn-xlm-roberta-base-snli-mnli-anli-xnli/ggml-model-q4_0.bin' as Q4_0\r\nllama.cpp: loading model from models/sn-xlm-roberta-base-snli-mnli-anli-xnli/ggml-model-f16.bin\r\nllama.cpp: saving model to models/sn-xlm-roberta-base-snli-mnli-anli-xnli/ggml-model-q4_0.bin\r\nllama_model_quantize_internal: model size  =     0.00 MB\r\nllama_model_quantize_internal: quant size  =     0.00 MB\r\n\r\nmain: quantize time =   275.42 ms\r\nmain:    total time =   275.42 ms\r\n```\r\n# Environment and Context\r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n`$ lscpu`\r\n```\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   40 bits physical, 48 bits virtual\r\nCPU(s):                          8\r\nOn-line CPU(s) list:             0-7\r\nThread(s) per core:              1\r\nCore(s) per socket:              1\r\nSocket(s):                       8\r\nNUMA node(s):                    1\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           85\r\nModel name:                      Intel(R) Xeon(R) Gold 6140 CPU @ 2.30GHz\r\nStepping:                        4\r\nCPU MHz:                         2294.608\r\nBogoMIPS:                        4589.21\r\nVirtualization:                  VT-x\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       128 KiB\r\nL1i cache:                       128 KiB\r\nL2 cache:                        4 MiB\r\nL3 cache:                        24.8 MiB\r\nNUMA node0 CPU(s):               0-7\r\nVulnerability Itlb multihit:     KVM: Mitigation: Split huge pages\r\nVulnerability L1tf:              Mitigation; PTE Inversion; VMX conditional cache flushes, SMT disabled\r\nVulnerability Mds:               Mitigation; Clear CPU buffers; SMT Host state unknown\r\nVulnerability Meltdown:          Mitigation; PTI\r\nVulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT Host state unknown\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl cpuid tsc_known_freq pni pclmu\r\n                                 lqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti ssbd ibrs ibpb tpr_shado\r\n                                 w vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1\r\n                                  xsaves pku ospke md_clear\r\n\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`$ uname -a`\r\n```\r\nLinux jenkins-tdd 5.4.0-131-generic #147-Ubuntu SMP Fri Oct 14 17:07:22 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version\r\nPython 3.10.11\r\n$ make --version\r\nGNU Make 4.2.1\r\nBuilt for x86_64-pc-linux-gnu\r\n\r\n$ g++ --version\r\ng++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\n```\r\n\r\n",
    "labels": [
      "invalid"
    ],
    "state": "closed",
    "created_at": "2023-07-16T10:42:50+00:00",
    "closed_at": "2023-07-17T03:39:24+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2241/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2241"
  },
  {
    "number": 12351,
    "title": "Eval bug: Gemma-3 vision don't work multilingual",
    "body": "### Name and Version\n\nllama-cli.exe --version\nversion: 4877 (363f8c5d)\nbuilt with MSVC 19.43.34808.0 for x64\n\n### Operating systems\n\nWindows\n\n### GGML backends\n\nCPU\n\n### Hardware\n\ni7-12700\n\n### Models\n\nhttps://huggingface.co/ggml-org/gemma-3-4b-it-GGUF/blob/main/gemma-3-4b-it-f16.gguf\n\nhttps://huggingface.co/ggml-org/gemma-3-4b-it-GGUF/blob/main/mmproj-model-f16.gguf\n\n### Problem description & steps to reproduce\n\nI realize that the vision support in llama.cpp is very experimental, but nevertheless I think it's worth opening this issue\n\n[Example image.](https://github.com/ggml-org/llama.cpp/blob/master/media/matmul.png)\n\ntransofmers code (7652804d237fb8768f0f0b8129a05e4f0576114b)\n```python\nimport torch\nfrom transformers import AutoProcessor, Gemma3ForConditionalGeneration\n\nckpt = \"google/gemma-3-4b-it\"\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\n    ckpt, device_map=\"auto\", torch_dtype=torch.bfloat16,\n)\nprocessor = AutoProcessor.from_pretrained(ckpt)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": \"/content/matmul.png\"},\n            {\"type\": \"text\", \"text\": \"\u041e\u043f\u0438\u0448\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c \u044f\u0437\u044b\u043a\u0435.\"} # Describe image in Russian\n        ]\n    }\n]\ninputs = processor.apply_chat_template(\n    messages, add_generation_prompt=True, tokenize=True,\n    return_dict=True, return_tensors=\"pt\"\n).to(model.device)\n\ninput_len = inputs[\"input_ids\"].shape[-1]\n\ngeneration = model.generate(**inputs, max_new_tokens=100, do_sample=False, temperature=0)\ngeneration = generation[0][input_len:]\n\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n```\n\noutput (expected, model works fine):\n```\n\u041a\u043e\u043d\u0435\u0447\u043d\u043e, \u0432\u043e\u0442 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438:\n\n**\u041a\u0430\u0440\u0442\u0438\u043d\u043a\u0430 \u0438\u043b\u043b\u044e\u0441\u0442\u0440\u0438\u0440\u0443\u0435\u0442 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u043c\u0430\u0442\u0440\u0438\u0446 \u0432 \u0444\u043e\u0440\u043c\u0430\u0442 \"column-major\" \u0438 \"row-major\" \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0439 \u0442\u0440\u0430\u043d\u0441\u043f\u043e\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0438 \u0443\u043c\u043d\u043e\u0436\u0435\u043d\u0438\u044f \u043c\u0430\u0442\u0440\u0438\u0446.**\n\n**\u041e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b:**\n\n1.  **\u0422\u0440\u0438 \u043c\u0430\u0442\u0440\u0438\u0446\u044b:**\n    *   **A:**  \u041f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0430 \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435 \"row-major\" (\u0441\u0442\u0440\u043e\u0447\u043d\u043e-\u043c\u0430\u0436\u043e\u0440\u043d\u044b\u0439). \u042d\u0442\u043e \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442, \u0447\u0442\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u0440\u0430\u0441\u043f\u043e\u043b\u0430\u0433\u0430\u044e\u0442\u0441\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0447\u043d\u043e, \u043d\u0430\u0447\u0438\u043d\u0430\u044f \u0441 \u043f\u0435\u0440\u0432\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0438 \u0438\n```\n\n\nllama.cpp (363f8c5d):\n```shell\nllama-gemma3-cli.exe -m ..\\gemma-3-4b-it-f16.gguf --mmproj ..\\mmproj-model-f16.gguf --samplers temperature --temp 0\n```\n\noutput (answer in English):\n```\nOkay, let's break down this image and the concepts it illustrates. This diagram is demonstrating a key operation in linear algebra called **matrix transposition and the relationship between row and column-major matrices.**\n\n**1. Matrix Representations:**\n\n* **A (Row-major):** This is a standard matrix representation where the elements are stored row by row.  The black circles represent non-zero elements.\n* **B (Row-major):**  Similar to A, but representing a different matrix.\n* **C (Row-major):**  Another matrix.\n* **C<sup>T</sup> = AB<sup>T</sup> (Column-major):** This is the core of the diagram. It shows how to transform a row-major matrix (A and B) into a column-major matrix (C<sup>T</sup>) by multiplying them with the transpose of B.\n\n**2. Transposition (<sup>T</sup>):**\n\n* The superscript 'T' indicates the transpose of a matrix.  The transpose flips the rows and columns.  So, if you have a matrix `[a b c]`, its transpose is `[a c b]`.\n\n**3. Column-Major vs. Row-Major:**\n\n* **Row-major:**  Elements are stored in the order of rows.  This is the standard representation in many programming languages and libraries.\n* **Column-major:** Elements are stored in the order of columns. This is common in scientific computing and graphics processing, particularly on GPUs.  It's often more efficient for certain operations.\n\n**4\n```\n\nThe generated text remains in English even if no image is specified.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nllama-gemma3-cli.exe -m ..\\gemma-3-4b-it-f16.gguf --mmproj ..\\mmproj-model-f16.gguf --samplers temperature --temp 0\nbuild: 4877 (363f8c5d) with MSVC 19.43.34808.0 for x64\nllama_model_loader: loaded meta data with 40 key-value pairs and 444 tensors from ..\\gemma-3-4b-it-f16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Gemma 3 4b It\nllama_model_loader: - kv   3:                           general.finetune str              = it\nllama_model_loader: - kv   4:                           general.basename str              = gemma-3\nllama_model_loader: - kv   5:                         general.size_label str              = 4B\nllama_model_loader: - kv   6:                            general.license str              = gemma\nllama_model_loader: - kv   7:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   8:                  general.base_model.0.name str              = Gemma 3 4b Pt\nllama_model_loader: - kv   9:          general.base_model.0.organization str              = Google\nllama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-3...\nllama_model_loader: - kv  11:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\nllama_model_loader: - kv  12:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv  13:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv  14:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv  15:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv  16:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv  17:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv  19:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv  20:                          general.file_type u32              = 1\nllama_model_loader: - kv  21:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  22:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  23:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  24:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  25:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  26:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  29:                      tokenizer.ggml.scores arr[f32,262144]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  33:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  34:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  35:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  36:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  37:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\nllama_model_loader: - kv  38:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  39:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type  f16:  239 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = F16\nprint_info: file size   = 7.23 GiB (16.00 BPW)\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 6414\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = Gemma 3 4b It\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262144\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 0 repeating layers to GPU\nload_tensors: offloaded 0/35 layers to GPU\nload_tensors:   CPU_Mapped model buffer size =  7401.40 MiB\n....................................................................................\nllama_init_from_model: n_seq_max     = 1\nllama_init_from_model: n_ctx         = 4096\nllama_init_from_model: n_ctx_per_seq = 4096\nllama_init_from_model: n_batch       = 2048\nllama_init_from_model: n_ubatch      = 512\nllama_init_from_model: flash_attn    = 0\nllama_init_from_model: freq_base     = 1000000.0\nllama_init_from_model: freq_scale    = 0.125\nllama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 34, can_shift = 1\nllama_kv_cache_init:        CPU KV buffer size =   544.00 MiB\nllama_init_from_model: KV self size  =  544.00 MiB, K (f16):  272.00 MiB, V (f16):  272.00 MiB\nllama_init_from_model:        CPU  output buffer size =     1.00 MiB\nllama_init_from_model:        CPU compute buffer size =   517.00 MiB\nllama_init_from_model: graph nodes  = 1367\nllama_init_from_model: graph splits = 1\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nclip_init: loaded meta data with 16 key-value pairs and 439 tensors from ..\\mmproj-model-f16.gguf\nclip_init: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nclip_init: - kv   0:                       general.architecture str              = clip\nclip_init: - kv   1:                        clip.projector_type str              = gemma3\nclip_init: - kv   2:                      clip.has_text_encoder bool             = false\nclip_init: - kv   3:                    clip.has_vision_encoder bool             = true\nclip_init: - kv   4:                   clip.has_llava_projector bool             = false\nclip_init: - kv   5:                     clip.vision.image_size u32              = 896\nclip_init: - kv   6:                     clip.vision.patch_size u32              = 14\nclip_init: - kv   7:               clip.vision.embedding_length u32              = 1152\nclip_init: - kv   8:            clip.vision.feed_forward_length u32              = 4304\nclip_init: - kv   9:                 clip.vision.projection_dim u32              = 2560\nclip_init: - kv  10:                    clip.vision.block_count u32              = 27\nclip_init: - kv  11:           clip.vision.attention.head_count u32              = 16\nclip_init: - kv  12:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001\nclip_init: - kv  13:                     clip.vision.image_mean arr[f32,3]       = [0.500000, 0.500000, 0.500000]\nclip_init: - kv  14:                      clip.vision.image_std arr[f32,3]       = [0.500000, 0.500000, 0.500000]\nclip_init: - kv  15:                              clip.use_gelu bool             = true\nclip_init: - type  f32:  276 tensors\nclip_init: - type  f16:  163 tensors\nclip_ctx: CLIP using CPU backend\nkey clip.use_silu not found in file\nclip_init: params backend buffer size =  811.79 MB (439 tensors)\nkey clip.vision.image_grid_pinpoints not found in file\nkey clip.vision.feature_layer not found in file\nkey clip.vision.mm_patch_merge_type not found in file\nkey clip.vision.image_crop_resolution not found in file\nclip_init:        CPU compute buffer size =  1128.81 MiB\nmain: ..\\gemma-3-4b-it-f16.gguf\n\n Running in chat mode, available commands:\n   /image <path>    load an image\n   /clear           clear the chat history\n   /quit or /exit   exit the program\n\n> /image C:\\Users\\username\\Downloads\\matmul.png\nEncoding image C:\\Users\\username\\Downloads\\matmul.png\nImage encoded in 24661 ms\nImage decoded in 4725 ms\n\n> \u041e\u043f\u0438\u0448\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c \u044f\u0437\u044b\u043a\u0435.\nOkay, let's break down this image and the concepts it illustrates. This diagram is demonstrating a key operation in linear algebra called **matrix transposition and the relationship between row and column-major matrices.**\n\n**1. Matrix Representations:**\n\n* **A (Row-major):** This is a standard matrix representation where the elements are stored row by row.  The black circles represent non-zero elements.\n* **B (Row-major):**  Similar to A, but representing a different matrix.\n* **C (Row-major):**  Another matrix.\n* **C<sup>T</sup> = AB<sup>T</sup> (Column-major):** This is the core of the diagram. It shows how to transform a row-major matrix (A and B) into a column-major matrix (C<sup>T</sup>) by multiplying them with the transpose of B.\n\n**2. Transposition (<sup>T</sup>):**\n\n* The superscript 'T' indicates the transpose of a matrix.  The transpose flips the rows and columns.  So, if you have a matrix `[a b c]`, its transpose is `[a c b]`.\n\n**3. Column-Major vs. Row-Major:**\n\n* **Row-major:**  Elements are stored in the order of rows.  This is the standard representation in many programming languages and libraries.\n* **Column-major:** Elements are stored in the order of columns. This is common in scientific computing and graphics processing, particularly on GPUs.  It's often more efficient for certain operations.\n\n**4\n\n>\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-12T11:27:37+00:00",
    "closed_at": "2025-05-05T01:07:55+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12351/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12351"
  },
  {
    "number": 4442,
    "title": "Model is split between multiple GPU's even after explicitly specifying main gpu",
    "body": "I think we need to solve for this, models are automatically loaded and split on multiple GPUs if you have `BaseMosaic` enabled in your XORG config, overriding the default flags that you can explicitly set as your main GPU.\r\n\r\nThis isn't that big of a deal, but helps when you are experimenting with multiple models.\r\n\r\nMore details here: https://forums.developer.nvidia.com/t/memory-is-allocated-on-all-gpus/183110/3\r\n\r\nOriginal Discussion:\r\n> ### Discussed in https://github.com/ggerganov/llama.cpp/discussions/2752\r\n\r\n> <div type='discussions-op-text'>\r\n> \r\n> <sup>Originally posted by **isaacmorgan** August 24, 2023</sup>\r\n> Using the CuBLAS build with 2 GPUs. I want to load the model onto a single GPU, but the model is always loaded into the memory of both GPUs. Even if I only run on 1 GPU the model is loaded onto both GPUs. \r\n> \r\n> Things I've tried:\r\n> `./main -m ./llama-2-7b.ggmlv3.q8_0.bin -i --interactive-first -ngl 40`\r\n> Result: Default behavior: Loads model onto both GPUs, runs on both GPUs.\r\n> \r\n> `CUDA_VISIBLE_DEVICES=0 ./main -m ./models/llama-2-7b.ggmlv3.q8_0.bin -i --interactive-first -ngl 40`\r\n> Result: Loads model onto both GPUs, runs only on GPU 0\r\n> \r\n> `CUDA_VISIBLE_DEVICES=1 ./main -m ./models/llama-2-7b.ggmlv3.q8_0.bin -i --interactive-first -ngl 40`\r\n> Result: Crashes: `ggml_init_cublas: found 1 CUDA devices:\r\n>   Device 0: NVIDIA RTX A5000, compute capability 8.6\r\n> Segmentation fault (core dumped)`\r\n> \r\n> `./main -m ./models/llama-2-7b.ggmlv3.q8_0.bin -i --interactive-first -ngl 40 -mg 0 -ts 1,0`\r\n> Result: Crashes: `CUDA error 400 at ggml-cuda.cu:3343: invalid resource handle`\r\n> \r\n> `./main -m ./models/llama-2-7b.ggmlv3.q8_0.bin -i --interactive-first -ngl 40 -mg 0 -ts 10,1`\r\n> Result: Loads model onto both GPUs, runs mostly on GPU 0 and a little on GPU 1 (power usage is 220 W on GPU 0, 80 W on GPU 1)\r\n> \r\n> `./main -m ./models/llama-2-7b.ggmlv3.q8_0.bin -i --interactive-first -ngl 40 -mg 1 -ts 1,10` \r\n> Result: Same as above, but mostly runs on GPU 1\r\n> \r\n> When I run a query I can tell from the power usage that only GPU 0 is being used.\r\n> \r\n> But when I check nvidia-smi I see that the model is loaded to both GPUs. That is, if the model required 8 GB both GPUs show 8 GB of memory is used.\r\n> \r\n> Any idea what's going on here?</div>",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-13T15:14:09+00:00",
    "closed_at": "2024-04-03T01:14:11+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4442/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4442"
  },
  {
    "number": 14071,
    "title": "Eval bug: KV cache stopped working in b5554 version",
    "body": "### Name and Version\n\nllama-server --version\nversion: 5554 (3600cc28)\nbuilt with Apple clang version 17.0.0 (clang-1700.0.13.5) for arm64-apple-darwin24.5.0\n\nor any subsequent version up to b5604 included\n\n### Operating systems\n\nMac\n\n### GGML backends\n\nMetal\n\n### Hardware\n\nM4 Max, M2 Max, M1 Max\n\n### Models\n\ngemma models : \nfastest to test is the the [1B gemma-3 Q4_K_M] (https://huggingface.co/ggml-org/gemma-3-4b-it-GGUF)\n\n./llama-server -hf ggml-org/gemma-3-1b-it-GGUF:Q4_K_M -ngl 200 -c 4096\n\n\n\n### Problem description & steps to reproduce\n\nusing b5552 KV cache works as expected :\n2nd query only has 1 token processed (vs 250 for first query)\n\ncd ../../build-b5552/bin\n./llama-server -m $LLAMA_CACHE/ggml-org_gemma-3-1b-it-GGUF_gemma-3-1b-it-Q4_K_M.gguf -ngl 200 -c 4096\n\npython test_kv_cacke.py\n\nSending request 1...\nresponse 1: Blacksmith.\nprompt_n: 250\nsystem_fingerprint: b5552-3f55f781\nduration = 0.136\n\nSending request 2...\nresponse 2: Blacksmith.\nprompt_n: 1\nsystem_fingerprint: b5552-3f55f781\nduration = 0.035\n\n\nusing b5554 KV cache does not work as expected :\n2nd query also has 250 tokens processed (vs 250 for first query) and is much slower than should be\n\ncd ../../build-b5554/bin\n./llama-server -m $LLAMA_CACHE/ggml-org_gemma-3-1b-it-GGUF_gemma-3-1b-it-Q4_K_M.gguf -ngl 200 -c 4096\n\nSending request 1...\nresponse 1: Blacksmith.\nprompt_n: 250\nsystem_fingerprint: b5554-3600cc28\nduration = 0.150\n\nSending request 2...\nresponse 2: Blacksmith.\nprompt_n: 250\nsystem_fingerprint: b5554-3600cc28\nduration = 0.101\n\n\n\n\n### First Bad Commit\n\nb5554\n\n### Relevant log output\n\n```shell\n#test_kv_cache.py \n\nimport requests\nimport json\nimport time\n\n# Configuration\nurl = \"http://localhost:8080/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\"}\n\n# Prompt (approx. 340 tokens, with fixed-answer question)\nprompt = \"\"\"\nIn the fantasy novel *The Shattered Crown*, set in the war-torn kingdom of Eryndor, a young blacksmith discovers a hidden prophecy etched on a mysterious amulet buried beneath his forge. The prophecy foretells the rise of a shadowed heir who will either unite the fractured realms or plunge them into eternal darkness. This blacksmith, unaware of his own lineage, embarks on a quest to find the Crown of Ages, a relic said to hold the power to restore balance. Joined by Lira, a rogue sorceress with a shadowed past, and Torren, a grizzled knight banished for treason, he faces trials including the labyrinthine Caves of Sorrow, where spectral guardians test his resolve, and the Court of Whispers, a den of political intrigue where allies betray for power. The blacksmith uncovers clues about his heritage, learning the prophecy may point to him or his ruthless brother, Varn, who leads a fanatical cult bent on conquest. The novel weaves themes of destiny, loyalty, and sacrifice, set against Eryndor\u2019s misty vales, towering spires, and haunted forests. Who is the main character of *The Shattered Crown*?\n\"\"\"\n\n# Payload for chat completion\npayload = {\n    \"model\": \"gemma-3-4b-it\",\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a terse assistant.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ],\n    \"max_tokens\": 10,\n    \"temperature\": 0.0  # Set to 0.0 for deterministic output\n}\n\nprint()\nfor i in range(1,3):\n  start = time.time()\n  print(f\"Sending request {i}...\")\n  response = requests.post(url, headers=headers, data=json.dumps(payload))\n  if response.status_code == 200:\n    print(f\"response {i}:\", response.json()[\"choices\"][0][\"message\"][\"content\"])\n    print(\"prompt_n:\", response.json()[\"timings\"][\"prompt_n\"])\n    print(\"system_fingerprint:\", response.json()[\"system_fingerprint\"])\n    # print(\"First response:\", response.text)\n  else:\n    print(\"Error:\", response.status_code, response.text)\n  duration = time.time() - start\n  print(f\"{duration = :.3f}\")\n  print()\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-06-08T19:06:49+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14071/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14071"
  },
  {
    "number": 1900,
    "title": "Question about new models",
    "body": "Hi everybody,\r\n\r\nI wanted to ask if it was possible to train a new model from a series of text files, either starting from scratch or starting (with a process similar to fine-tuning) from a pre-existing model compatible wiht llama.cpp\r\n\r\nThanks in advance.\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-06-16T20:38:58+00:00",
    "closed_at": "2024-04-10T01:06:57+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1900/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1900"
  },
  {
    "number": 4911,
    "title": "is there any simple way to ask server stop generating?",
    "body": "Hi, I would like to let server response to be stopped on demand but there is no api endpoint for this. is it possible to achieve such functionality in some easy way?\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-13T12:14:06+00:00",
    "closed_at": "2024-01-16T21:14:56+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4911/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4911"
  },
  {
    "number": 1361,
    "title": "Ai on Laptop finetuning",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-05-07T23:42:06+00:00",
    "closed_at": "2023-05-07T23:42:22+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1361/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1361"
  },
  {
    "number": 2324,
    "title": "Investigate PG-TD (Planning-Guided Transformer Decoding) sampling",
    "body": "There's been some work going on for beam search #2267, CFG #2135, steering vectors #1472, Honest LLaMA #1799, and other techniques to improve generation quality. So I thought sharing this paper may be of use.\r\n\r\nIt outlines some sampling tricks to improve code generation, which may be appropriate for StarCoder.cpp, given LLaMA's underwhelming coding performance, but I think these ideas can perhaps be used as inspiration for other types of planners for different tasks.\r\n\r\n###  [\"_Planning with Large Language Models for Code Generation_\"](https://openreview.net/forum?id=Lr8cOOtYbfL)\r\n\r\n> Abstract: Existing large language model-based code generation pipelines typically use beam search or sampling algorithms during the decoding process. Although the programs they generate achieve high token-matching-based scores, they often fail to compile or generate incorrect outputs. The main reason is that conventional Transformer decoding algorithms may not be the best choice for code generation. In this work, we propose a novel Transformer decoding algorithm, Planning-Guided Transformer Decoding (PG-TD), that uses a planning algorithm to do lookahead search and guide the Transformer to generate better programs. Specifically, instead of simply optimizing the likelihood of the generated sequences, the Transformer makes use of a planner that generates candidate programs and tests them on public test cases. The Transformer can therefore make more informed decisions and generate tokens that will eventually lead to higher-quality programs. We also design a mechanism that shares information between the Transformer and the planner to make our algorithm computationally efficient. We empirically evaluate our framework with several large language models as backbones on public coding challenge benchmarks, showing that 1) it can generate programs that consistently achieve higher performance compared with competing baseline methods; 2) it enables controllable code generation, such as concise codes and highly-commented codes by optimizing modified objective.\r\n\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-07-22T15:27:27+00:00",
    "closed_at": "2024-04-09T01:07:40+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2324/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2324"
  },
  {
    "number": 2269,
    "title": "CUDA Error 400: Invalid Resource Handle when Running on Single GPU",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ x ] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ x ] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ x ] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ x ] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI am trying to make `llama.cpp` run on a single GPU (in my case, GPU 5) on a multi-GPU system because there are other tasks running on my other GPUs.\r\n\r\n# Current Behavior\r\n\r\n`llama.cpp` crashes with `CUDA error 400 at ggml-cuda.cu:3343: invalid resource handle`\r\n\r\n# Environment and Context\r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using\r\n\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              112\r\nOn-line CPU(s) list: 0-111\r\nThread(s) per core:  2\r\nCore(s) per socket:  28\r\nSocket(s):           2\r\nNUMA node(s):        2\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               85\r\nModel name:          Intel(R) Xeon(R) Platinum 8280 CPU @ 2.70GHz\r\nStepping:            6\r\nCPU MHz:             1000.013\r\nCPU max MHz:         4000.0000\r\nCPU min MHz:         1000.0000\r\nBogoMIPS:            5400.00\r\nVirtualization:      VT-x\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            1024K\r\nL3 cache:            39424K\r\nNUMA node0 CPU(s):   0-27,56-83\r\nNUMA node1 CPU(s):   28-55,84-111\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear flush_l1d arch_capabilities\r\n\r\n* Operating System\r\n\r\nLinux cb68e1005cfb 4.15.0-184-generic #194-Ubuntu SMP Thu Jun 2 18:54:48 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n* SDK version:\r\n\r\n```\r\nPython 3.10.12\r\nGNU Make 4.1\r\ng++-11 (Ubuntu 11.4.0-2ubuntu1~18.04) 11.4.0\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nllama.cpp crashes after outputting\r\n\r\n```\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - To return control without starting a new line, end your input with '/'.\r\n - If you want to submit another line, end your input with '\\'.\r\n```\r\n\r\nwith `CUDA error 400 at ggml-cuda.cu:3343: invalid resource handle`\r\n\r\n# Steps to Reproduce\r\n\r\n1. Run `llama.cpp` with the arguments `-ts 0,0,0,0,0,100,0,0,0,0 -mg 5` (to try to instruct `llama.cpp` to only utilize GPU 5)\r\n\r\n# Failure Logs\r\n\r\n```\r\n./main -m '/home/michel/workspace/Inference Models/chinese-alpaca-plus-7b-f16-ggml.bin' -c 2048 --interactive-first --color --reverse-prompt \"User:\" --in-suffix \"Assistant:\" --prompt $'The following is a conversation between an AI assistant called Assistant and a human user called User.\\nThe assistant is intelligent, knowledgeable and polite to answer questions of user.\\n\\n' -ngl 40 -ts 0,0,0,0,0,100,0,0,0,0 -mg 5\r\nmain: build = 0 (unknown)\r\nmain: seed  = 1689738272\r\nggml_init_cublas: found 10 CUDA devices:\r\n  Device 0: GeForce RTX 3090, compute capability 8.6\r\n  Device 1: GeForce RTX 3090, compute capability 8.6\r\n  Device 2: GeForce RTX 3090, compute capability 8.6\r\n  Device 3: GeForce RTX 3090, compute capability 8.6\r\n  Device 4: GeForce RTX 3090, compute capability 8.6\r\n  Device 5: GeForce RTX 3090, compute capability 8.6\r\n  Device 6: GeForce RTX 3090, compute capability 8.6\r\n  Device 7: GeForce RTX 3090, compute capability 8.6\r\n  Device 8: GeForce RTX 3090, compute capability 8.6\r\n  Device 9: GeForce RTX 3090, compute capability 8.6\r\nllama.cpp: loading model from /home/michel/workspace/Inference Models/chinese-alpaca-plus-7b-f16-ggml.bin\r\nllama_model_load_internal: format     = ggjt v1 (pre #1405)\r\nllama_model_load_internal: n_vocab    = 49954\r\nllama_model_load_internal: n_ctx      = 2048\r\nllama_model_load_internal: n_embd     = 4096\r\nllama_model_load_internal: n_mult     = 256\r\nllama_model_load_internal: n_head     = 32\r\nllama_model_load_internal: n_layer    = 32\r\nllama_model_load_internal: n_rot      = 128\r\nllama_model_load_internal: freq_base  = 10000.0\r\nllama_model_load_internal: freq_scale = 1\r\nllama_model_load_internal: ftype      = 1 (mostly F16)\r\nllama_model_load_internal: n_ff       = 11008\r\nllama_model_load_internal: model size = 7B\r\nllama_model_load_internal: ggml ctx size =    0.08 MB\r\nllama_model_load_internal: using CUDA for GPU acceleration\r\nggml_cuda_set_main_device: using device 5 (GeForce RTX 3090) as main device\r\nllama_model_load_internal: mem required  = 2062.35 MB (+ 1026.00 MB per state)\r\nllama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 384 MB VRAM for the scratch buffer\r\nllama_model_load_internal: offloading 32 repeating layers to GPU\r\nllama_model_load_internal: offloading non-repeating layers to GPU\r\nllama_model_load_internal: offloading v cache to GPU\r\nllama_model_load_internal: offloading k cache to GPU\r\nllama_model_load_internal: offloaded 35/35 layers to GPU\r\nllama_model_load_internal: total VRAM used: 14154 MB\r\nllama_new_context_with_model: kv self size  = 1024.00 MB\r\n\r\nsystem_info: n_threads = 56 / 112 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \r\nmain: interactive mode on.\r\nReverse prompt: 'User:'\r\nInput suffix: 'Assistant:'\r\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\r\ngenerate: n_ctx = 2048, n_batch = 512, n_predict = -1, n_keep = 0\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - To return control without starting a new line, end your input with '/'.\r\n - If you want to submit another line, end your input with '\\'.\r\n\r\nCUDA error 400 at ggml-cuda.cu:3343: invalid resource handle\r\n```",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-07-19T03:47:47+00:00",
    "closed_at": "2024-04-09T01:07:51+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2269/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2269"
  },
  {
    "number": 2085,
    "title": "[User] Insert summary of your issue or enhancement..",
    "body": "Hello,\r\n\r\nOn the main page, you mention the following: \"This project is for educational purposes\". \r\n\r\nWant makes it, according to you, not suitable for real world situations and production environments?\r\n\r\nThanks.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-07-03T11:47:48+00:00",
    "closed_at": "2023-07-06T10:02:53+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2085/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2085"
  },
  {
    "number": 5255,
    "title": "Unable to Run miqu-1-70b.q4_k_m.gguf Model Without Error Messages",
    "body": "System: windows 11 x64\r\nThe following is the log:\r\n\r\n> [1706790015] Log start\r\n[1706790015] Cmd: main.exe -m miqu-1-70b.q4_k_m.gguf --color --temp 1 --top_p 0.95  -n -1 -p \"<s> [INST] QUERY_1 [/INST] ANSWER_1\"\r\n[1706790015] main: build = 2038 (ce320601)\r\n[1706790015] main: built with MSVC 19.37.32826.1 for x64\r\n[1706790015] main: seed  = 1706790015\r\n[1706790015] main: llama backend init\r\n[1706790015] main: load the model and apply lora adapter, if any\r\n\r\n\r\n\r\n\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-02-01T12:24:14+00:00",
    "closed_at": "2024-02-01T12:37:29+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5255/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5255"
  },
  {
    "number": 13910,
    "title": "android built on GPU cannot comparable with CPU?",
    "body": "I tried to build on Android device with GPU env but fail at official documents.\n1.Termux env\n2.openCL \n\nI blocked here:\n\n![Image](https://github.com/user-attachments/assets/918e9903-f500-41ba-ae96-33ee73818009)\n\n![Image](https://github.com/user-attachments/assets/8673bcf2-8ac2-4a37-a02e-d684d6759cfe)\n\nSo, I changed to another build method as below:\n1.\nusing termux default cmake tool does not ninja\n2.\ncmake .. -DCMAKE_BUILD_TYPE=Release \\\n  -DOPENCL_ICD_LOADER_HEADERS_DIR=/data/data/com.termux/files/usr/include \\\n  -DCMAKE_C_COMPILER=/data/data/com.termux/files/usr/bin/clang \\\n  -DCMAKE_CXX_COMPILER=/data/data/com.termux/files/usr/bin/clang++ \\\n  -DCMAKE_C_FLAGS=\"--target=aarch64-linux-android24 -D_POSIX_C_SOURCE=200809L\" \\\n  -DCMAKE_CXX_FLAGS=\"--target=aarch64-linux-android24 -D_POSIX_C_SOURCE=200809L\"\n3.\ncmake .. -DBUILD_SHARED_LIBS=ON -DGGML_OPENCL=ON -DGGML_OPENCL_EMBED_KERNELS=ON -DGGML_OPENCL_USE_ADRENO_KERNELS=ON\n4.\ncmake --build build-android \ncmake --build . --config Release\n\n\nIt worked fine and built successfully but got lower performance than CPU.\n**GPU bench:**\n~/.../build-android/bin $ ./llama-bench -m /data/local/tmp/llama.cpp/SmolVLM2-500M-Video-Instruct-Q8_0.gguf\nggml_opencl: selected platform: 'QUALCOMM Snapdragon(TM)'\n\nggml_opencl: device: 'QUALCOMM Adreno(TM) 830 (OpenCL 3.0 Adreno(TM) 830)'\nggml_opencl: OpenCL driver: OpenCL 3.0 QUALCOMM build: commit unknown Compiler E031.47.14.01\nggml_opencl: vector subgroup broadcast support: true\nggml_opencl: device FP16 support: true\nggml_opencl: mem base addr align: 128\nggml_opencl: max mem alloc size: 1024 MB\nggml_opencl: SVM coarse grain buffer support: true\nggml_opencl: SVM fine grain buffer support: true\nggml_opencl: SVM fine grain system support: false\nggml_opencl: SVM atomics support: true\nggml_opencl: flattening quantized weights representation as struct of arrays (GGML_OPENCL_SOA_Q)\nggml_opencl: using kernels optimized for Adreno (GGML_OPENCL_USE_ADRENO_KERNELS)\nggml_opencl: loading OpenCL kernels............................................\nggml_opencl: default device: 'QUALCOMM Adreno(TM) 830 (OpenCL 3.0 Adreno(TM) 830)'\n| model                          |       size |     params | backend    | ngl |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n| llama 8B Q8_0                  | 414.86 MiB |   409.25 M | OpenCL     |  99 |           pp512 |        115.82 \u00b1 2.96 |\n| llama 8B Q8_0                  | 414.86 MiB |   409.25 M | OpenCL     |  99 |           tg128 |         14.31 \u00b1 0.19 |\n\nbuild: 53ae3064 (5528)\n\n\n**CPU bench:**\n~/.../build-android-cpu/bin $ ./llama-bench -m /data/local/tmp/llama.cpp/SmolVLM2-500M-Video-Instruct-Q8_0.gguf\n| model                          |       size |     params | backend    | threads |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |\n| llama 8B Q8_0                  | 414.86 MiB |   409.25 M | CPU        |       8 |           pp512 |        404.00 \u00b1 3.75 |\n| llama 8B Q8_0                  | 414.86 MiB |   409.25 M | CPU        |       8 |           tg128 |        109.77 \u00b1 0.44 |\n\nbuild: 53ae3064 (5528)\n\n\nI really confused on it. Is it due to any errors in my compilation process or is it not optimized properly?\nopenCL version? or android ndk version? \nI am a newcomer, thank you.\n",
    "labels": [
      "stale"
    ],
    "state": "open",
    "created_at": "2025-05-30T04:47:18+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13910/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13910"
  },
  {
    "number": 7366,
    "title": "llama_model_load: error loading model: unable to allocate backend buffer",
    "body": "OS: Windows 11, running Text Generation WebUI, up to date on all releases.\r\nProcessor: Intel Core i5-8500 3GHz (6 Cores - no HT)\r\nMemory: 16GB System Memory\r\nGPUs: Five nVidia RTX 3600 - 12GB VRAM versions (First iteration during Covid)\r\n\r\nModel: Coomand-R-35B-v1-OLD_Q4_K_M.gguf\r\n\r\nModel Parameters:\r\n- n-gpu-layers: 41 (41 of 41, loading FULLY into VRAM)\r\n- n_ctx: 8192\r\n- tensor split: 10,10,10,10,10\r\n- flash-attn: Checked\r\n- tensorcores: checked\r\n- no-mmap: checked\r\n\r\nOutput from Model Load:\r\n```\r\n08:02:50-987413 INFO     Loading \"Coomand-R-35B-v1-OLD_Q4_K_M.gguf\"\r\n08:02:51-580810 INFO     llama.cpp weights detected: \"models\\Coomand-R-35B-v1-OLD_Q4_K_M.gguf\"\r\nllama_model_loader: loaded meta data with 23 key-value pairs and 322 tensors from models\\Coomand-R-35B-v1-OLD_Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = command-r\r\nllama_model_loader: - kv   1:                               general.name str              = workspace\r\nllama_model_loader: - kv   2:                      command-r.block_count u32              = 40\r\nllama_model_loader: - kv   3:                   command-r.context_length u32              = 131072\r\nllama_model_loader: - kv   4:                 command-r.embedding_length u32              = 8192\r\nllama_model_loader: - kv   5:              command-r.feed_forward_length u32              = 22528\r\nllama_model_loader: - kv   6:             command-r.attention.head_count u32              = 64\r\nllama_model_loader: - kv   7:          command-r.attention.head_count_kv u32              = 64\r\nllama_model_loader: - kv   8:                   command-r.rope.freq_base f32              = 8000000.000000\r\nllama_model_loader: - kv   9:     command-r.attention.layer_norm_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  11:                      command-r.logit_scale f32              = 0.062500\r\nllama_model_loader: - kv  12:                command-r.rope.scaling.type str              = none\r\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<PAD>\", \"<UNK>\", \"<CLS>\", \"<SEP>\", ...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,253333]  = [\"\u2500\u00e1 \u2500\u00e1\", \"\u2500\u00e1 t\", \"e r\", \"i n\", \"\u2500\u00e1 a...\r\nllama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 5\r\nllama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 255001\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   41 tensors\r\nllama_model_loader: - type q4_K:  240 tensors\r\nllama_model_loader: - type q6_K:   41 tensors\r\nllm_load_vocab: missing pre-tokenizer type, using: 'default'\r\nllm_load_vocab:\r\nllm_load_vocab: ************************************\r\nllm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!\r\nllm_load_vocab: CONSIDER REGENERATING THE MODEL\r\nllm_load_vocab: ************************************\r\nllm_load_vocab:\r\nllm_load_vocab: special tokens definition check successful ( 1008/256000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = command-r\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 256000\r\nllm_load_print_meta: n_merges         = 253333\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 8192\r\nllm_load_print_meta: n_head           = 64\r\nllm_load_print_meta: n_head_kv        = 64\r\nllm_load_print_meta: n_layer          = 40\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 8192\r\nllm_load_print_meta: n_embd_v_gqa     = 8192\r\nllm_load_print_meta: f_norm_eps       = 1.0e-05\r\nllm_load_print_meta: f_norm_rms_eps   = 0.0e+00\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 6.2e-02\r\nllm_load_print_meta: n_ff             = 22528\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = none\r\nllm_load_print_meta: freq_base_train  = 8000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 131072\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 35B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 34.98 B\r\nllm_load_print_meta: model size       = 20.04 GiB (4.92 BPW)\r\nllm_load_print_meta: general.name     = workspace\r\nllm_load_print_meta: BOS token        = 5 '<BOS_TOKEN>'\r\nllm_load_print_meta: EOS token        = 255001 '<|END_OF_TURN_TOKEN|>'\r\nllm_load_print_meta: PAD token        = 0 '<PAD>'\r\nllm_load_print_meta: LF token         = 136 '\u251c\u00e4'\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\r\nggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\r\nggml_cuda_init: found 5 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 3060, compute capability 8.6, VMM: yes\r\n  Device 1: NVIDIA GeForce RTX 3060, compute capability 8.6, VMM: yes\r\n  Device 2: NVIDIA GeForce RTX 3060, compute capability 8.6, VMM: yes\r\n  Device 3: NVIDIA GeForce RTX 3060, compute capability 8.6, VMM: yes\r\n  Device 4: NVIDIA GeForce RTX 3060, compute capability 8.6, VMM: yes\r\nllm_load_tensors: ggml ctx size =    1.01 MiB\r\nggml_backend_cuda_buffer_type_alloc_buffer: allocating 5099.12 MiB on device 4: cudaMalloc failed: out of memory\r\nllama_model_load: error loading model: unable to allocate backend buffer\r\nllama_load_model_from_file: failed to load model\r\nCUDA error: out of memory\r\n  current device: 4, in function ggml_backend_cuda_host_buffer_free_buffer at D:\\a\\llama-cpp-python-cuBLAS-wheels\\llama-cpp-python-cuBLAS-wheels\\vendor\\llama.cpp\\ggml-cuda.cu:993\r\n  cudaFreeHost(buffer->context)\r\nGGML_ASSERT: D:\\a\\llama-cpp-python-cuBLAS-wheels\\llama-cpp-python-cuBLAS-wheels\\vendor\\llama.cpp\\ggml-cuda.cu:61: !\"CUDA error\"\r\n``` \r\n\r\nThis really doesn't make any sense to me, as a 35B paramter at Q4 should load into 50GB VRAM without issue.\r\n\r\n\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-05-18T13:26:52+00:00",
    "closed_at": "2024-05-19T17:25:03+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7366/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7366"
  },
  {
    "number": 14418,
    "title": "bug: GGML_ASSERT(backend_embd != nullptr) failed error at llama.cpp:14775",
    "body": "### Name and Version\n\nllama_cpp_python==0.2.88\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nOther (Please specify in the next section)\nIssue at /llama-cpp-python_16ea09f94c0346afa022d988e7934741/vendor/llama.cpp/src/llama.cpp:14775 due to libggml.so\n\n### Command line\n\n```shell\n\n```\n\n### Problem description & steps to reproduce\n\nI have a flask application running in a docker container instance on a VM. It takes 'query' as input and uses an underlying RAG system to answer the query, and returns a json response containing the LLM output. \nI use `LlamaCppEmbeddings` (which has a dependency on `llama_cpp_python`) to load a `nomic-embed-text-v1.5.Q8_0.gguf `embedding model, it is used to perform dense vector search in the knowledge base. That vector index was also created using the same embedding model, and stored in a `FAISS` db ~50MB (index.faiss+index.pkl), which is loaded in runtime. I use Llama3.3-70B LLM model for the generation based on top-4 cosine similarity matches. The vector index and embedding model are mounted as docker volumes, whereas the LLM is used as an API.\nNow if I make >3-4 simultaenous API requests to the endpoint at which the container is running, then I get a `139` exit code on the container. And the following error messages in the docker logs:\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n2025-06-27T10:45:59.802448067Z /tmp/pip-install-kfbxdcj6/llama-cpp-python_16ea09f94c0346afa022d988e7934741/vendor/llama.cpp/src/llama.cpp:14775: GGML_ASSERT(backend_embd != nullptr) failed\n2025-06-27T10:45:59.837457263Z /usr/local/lib/python3.11/site-packages/llama_cpp/lib/libggml.so(+0xea48)[0x7f3f1e6daa48]\n2025-06-27T10:45:59.837457263Z /usr/local/lib/python3.11/site-packages/llama_cpp/lib/libggml.so(ggml_abort+0x135)[0x7f3f1e6dc715]\n2025-06-27T10:45:59.837530429Z /usr/local/lib/python3.11/site-packages/llama_cpp/lib/libllama.so(llama_decode+0x175e)[0x7f3f1e8acf0e]\n2025-06-27T10:45:59.837568832Z /lib/x86_64-linux-gnu/libffi.so.8(+0x6f7a)[0x7f3f2ba60f7a2025-06-27T10:45:59.837605963Z ]\n2025-06-27T10:45:59.837605963Z /lib/x86_64-linux-gnu/libffi.so.8(+0x640e)[0x2025-06-27T10:45:59.837642401Z 7f3f2ba6040e]\n2025-06-27T10:45:59.837642401Z /lib/x86_64-linux-gnu/libffi.so.8(ffi_call+0xcd)2025-06-27T10:45:59.837679221Z [0x7f3f2ba60b0d]\n2025-06-27T10:45:59.837714084Z /usr/local/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so(+0x10c5f)[0x7f3f2a86bc5f]\n2025-06-27T10:45:59.837764411Z /usr/local/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so(+0x7c3f)[0x7f3f2a862c3f]\n2025-06-27T10:45:59.837848156Z /usr/local/bin/../lib/libpython3.11.so.1.0(_PyObject_MakeTpCall+0x6f)[0x7f3f2cdddf8f]\n2025-06-27T10:45:59.837970557Z /usr/local/bin/../lib/libpython3.11.so.1.0(_PyEval_EvalFrameDefault+0x66d)[0x7f3f2cde6a0d]\n2025-06-27T10:45:59.838008796Z /usr/local/bin/../lib/libpython3.11.so.1.0(+0x1b9c1a)[0x7f3f2cde2c1a]\n2025-06-27T10:45:59.838070588Z /usr/local/bin/../lib/libpython3.11.so.1.0(_PyEval_EvalFrameDefault+0x43ab)[0x7f3f2cdea74b]\n2025-06-27T10:45:59.838211593Z /usr/local/bin/../lib/libpython3.11.so.1.0(+0x1b9c1a)[0x7f3f2cde2c1a]\n2025-06-27T10:45:59.838211593Z /usr/local/bin/../lib/libpython3.11.so.1.0(_PyEval_EvalFrameDefault+0x43ab)[0x7f3f2cdea74b]\n2025-06-27T10:45:59.838211593Z /usr/local/bin/../lib/libpython3.11.so.1.0(+0x1b9c1a)[0x7f3f2cde2c1a]\n2025-06-27T10:45:59.838287570Z /usr/local/bin/../lib/libpython3.11.so.1.0(+0x1f2f10)[0x7f3f2ce1bf10]\n2025-06-27T10:45:59.838362371Z /usr/local/bin/../lib/libpython3.11.so.1.0(+0x2a0694)[0x7f3f2cec9694]\n2025-06-27T10:45:59.838434099Z /usr/local/bin/../lib/libpython3.11.so.1.0(+0x26f5d4)[0x7f3f2ce985d4]\n2025-06-27T10:45:59.838521901Z /lib/x86_64-linux-gnu/libc.so.6(+0x891c4)[0x7f3f2cacb1c4]\n2025-06-27T10:45:59.838627631Z /lib/x86_64-linux-gnu/libc.so.6(+0x10985c)[0x7f3f2cb4b85c]\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-06-27T11:25:19+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14418/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14418"
  },
  {
    "number": 13399,
    "title": "Misc. bug: Model not loaded on Android with NDK",
    "body": "### Name and Version\n\nversion: [b5320](https://github.com/ggml-org/llama.cpp/releases/tag/b5320)\nbuilt with macOS Sonoma, Android Studio Meerkat 2024.3.1 Patch 2 and Android NDK 27.012077973\n\n### Operating systems\n\nOther? (Please let us know in description), Mac\n\n### Which llama.cpp modules do you know to be affected?\n\nlibllama (core library)\n\n### Command line\n\n```shell\n\n```\n\n### Problem description & steps to reproduce\n\nI'm trying to use llama.cpp on Android with local inference using NDK with JNI. When I try to load a model (nomic_embed_text_v1_5_q4_0.gguf) with the \"llama_model_load_from_file\" method, it does not load and returns null. \n\n#### CMakeLists.txt\n\n```\ncmake_minimum_required(VERSION 3.22.1)\nproject(llama_jni)\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++17\")\n\n# Path to llama.cpp folder is in the root folder of the project\nset(LLAMA_CPP_DIR \"${CMAKE_SOURCE_DIR}/../../../../llama.cpp\")\nset(LLAMA_CPP_SRC_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../../../../llama.cpp/src\")\nset(LLAMA_CPP_INCLUDE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../../../../llama.cpp/include\")\nset(LLAMA_GGML_SRC_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../../../../llama.cpp/ggml/src\")\nset(LLAMA_GGML_INCLUDE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../../../../llama.cpp/ggml/include\")\n\nadd_definitions(\n        -DANDROID_ABI=arm64-v8a\n        -DANDROID_PLATFORM=android-28\n        -DCMAKE_C_FLAGS=\"-march=armv8.7a\"\n        -DCMAKE_CXX_FLAGS=\"-march=armv8.7a\"\n        -DGGML_OPENMP=OFF\n        -DGGML_LLAMAFILE=OFF\n        -B build-android\n        -DLLAMA_CURL=OFF\n)\n\ninclude_directories(\n        ${CMAKE_SOURCE_DIR}\n        ${LLAMA_CPP_DIR}\n        ${LLAMA_CPP_INCLUDE_DIR}\n        ${LLAMA_GGML_SRC_DIR}\n        ${LLAMA_GGML_INCLUDE_DIR}\n)\n\n# Collect llama.cpp source files\nfile(GLOB LLAMA_SOURCES\n        ${LLAMA_CPP_SRC_DIR}/*.cpp\n        ${LLAMA_GGML_SRC_DIR}/*.cpp\n        ${LLAMA_GGML_SRC_DIR}/*.c\n        ${LLAMA_GGML_INCLUDE_DIR}\n)\n\nadd_library(llama_jni SHARED\n        llama_jni.cpp\n        llama_embed.cpp\n        ${LLAMA_SOURCES}\n)\n\n# Required Android libraries\nfind_library(log-lib log)\n\ntarget_include_directories(llama_jni PRIVATE\n        ${LLAMA_CPP_DIR}\n        ${LLAMA_CPP_INCLUDE_DIR}\n        ${LLAMA_GGML_SRC_DIR}\n        ${LLAMA_GGML_INCLUDE_DIR}\n        ${CMAKE_CURRENT_SOURCE_DIR}\n        ${CMAKE_SOURCE_DIR}/../c_interop/include  # location of llama.h\n        ${LLAMA_SOURCES}\n)\n\ntarget_link_libraries(\n        llama_jni\n        ${log-lib}\n)\n```\n\n\n#### llama_embed.cpp\n\n```\nstatic struct llama_model *model = nullptr;\nstatic struct llama_context *ctx = nullptr;\nstatic int embedding_size = 0;\n\nbool llama_embed_init(const char *model_path) {\n    __android_log_print(ANDROID_LOG_INFO, \"llama_jni\", \"Initializing llama...\");\n    llama_backend_init();\n\n    __android_log_print(ANDROID_LOG_INFO, \"llama_jni\", \"Model loading...\");\n    llama_model_params model_params = llama_model_default_params();\n\n    __android_log_print(ANDROID_LOG_INFO, \"llama_jni\", \"File Path: %s\", model_path);\n    if (std::filesystem::exists(model_path)) {\n        __android_log_print(ANDROID_LOG_INFO, \"llama_jni\", \"Model file exists: %s\", model_path);\n    }\n\n    model = llama_model_load_from_file(model_path, model_params);\n    if (!model) return false;\n    __android_log_print(ANDROID_LOG_INFO, \"llama_jni\", \"Model loaded successfully.\");\n\n    __android_log_print(ANDROID_LOG_INFO, \"llama_jni\", \"Context creating...\");\n    llama_context_params ctx_params = llama_context_default_params();\n    ctx_params.embeddings = true;\n    ctx = llama_init_from_model(model, ctx_params);\n    if (!ctx) return false;\n    __android_log_print(ANDROID_LOG_INFO, \"llama_jni\", \"Context created successfully.\");\n\n    embedding_size = llama_model_n_embd(model);\n\n    return true;\n}\n```\n\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nNo log raised in Logcat\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-05-09T08:41:32+00:00",
    "closed_at": "2025-06-26T01:07:56+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13399/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13399"
  },
  {
    "number": 10603,
    "title": "Eval bug: llama-cli outputs some messages to console",
    "body": "### Name and Version\n\n$ ./llama-cli --version\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 3060, compute capability 8.6, VMM: yes\r\nversion: 4229 (3e0ba0e6)\r\nbuilt with cc (Gentoo 13.3.1_p20241025 p1) 13.3.1 20241024 for x86_64-pc-linux-gnu\r\n\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\nRTX 3060\n\n### Models\n\n_No response_\n\n### Problem description & steps to reproduce\n\nWhen using llama-cli with `--prompt-cache-all` it prints the following message which breaks the parsing: \"main: saving final output to session file ...\"\r\n\r\nSome of the logging in the examples/main.cpp still uses `LOG` macro rather than `LOG_INF`/`LOG_WRN`/etc which results in this output not being directed to the log file.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nUSER: Hey\r\nASSISTANT:  \ud83d\ude4b\u2642 Good morning!\r\nUSER:\r\nmain: saving final output to session file '/var/extra2/llama_models/chat_cache/bb2/current-cache.bin'\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-30T14:54:12+00:00",
    "closed_at": "2025-01-14T01:08:56+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10603/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10603"
  },
  {
    "number": 7873,
    "title": "Bug: I use llama-b3091-bin-win-llvm-arm64.zip Run qwen2-0_5b-instruct-q8_0.gguf and it cannot start. Is it a compilation error of llama-b3091-bin-win-llvm-arm64.zip?",
    "body": "### What happened?\n\nI use llama-b3091-bin-win-llvm-arm64.zip\r\nRun qwen2-0_5b-instruct-q8_0.gguf and it cannot start. Is it a compilation error of llama-b3091-bin-win-llvm-arm64.zip?\n\n### Name and Version\n\nllama-b3091-bin-win-llvm-arm64.zip\n\n### What operating system are you seeing the problem on?\n\nOther? (Please let us know in description)\n\n### Relevant log output\n\n```shell\nwindows-arm64\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-06-11T07:02:50+00:00",
    "closed_at": "2024-07-27T01:06:46+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7873/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7873"
  },
  {
    "number": 1159,
    "title": "[User] Deadlock if number of threads > number of (hyper)threads",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI expect the program to run suboptimally but finish.\r\n\r\n# Current Behavior\r\n\r\nCurrently the program locks up with very large cpu utilization.\r\n\r\n# Environment and Context\r\n\r\nI have a 6 core intel machine i.e. 12 threads with hyperthreading. \r\nOnce I run with -t 13 the deadlock happens:\r\n`./main -m ./models/ggml-vicuna-13b-1.1-q4_0.bin -n 256 --repeat_penalty 1.1 --color -i -r \"### Human:\" -f prompts/chat-with-vicuna.txt -t 13`\r\n",
    "labels": [
      "threading"
    ],
    "state": "closed",
    "created_at": "2023-04-24T19:02:29+00:00",
    "closed_at": "2023-05-03T18:30:11+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1159/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1159"
  },
  {
    "number": 12468,
    "title": "Eval bug:  MiniCPM-2B-128k convert_hf_to_gguf Missing the required key: rope_scaling",
    "body": "### Name and Version\n\n$./llama-cli --version\nversion: 4778 (a82c9e7c)\nbuilt with aarch64-none-linux-gnu-gcc (Arm GNU Toolchain 12.3.Rel1 (Build arm-12.35)) 12.3.1 20230626 for aarch64-none-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCPU\n\n### Hardware\n\nArm CPU\n\n### Models\n\n[MiniCPM-2B-128k](https://huggingface.co/openbmb/MiniCPM-2B-128k)\n\n### Problem description & steps to reproduce\n\nI encountered the following problem \n**KeyError: 'Missing the required key rope_scaling.long_factor or rope_scaling_short_factor'** while convert hf to gguf:  [MiniCPM-2B-128k](https://huggingface.co/openbmb/MiniCPM-2B-128k) on llamacpp b4778. \n\nMiniCPM-2B-128k/config.json:\n```\n{\n    \"_name_or_path\": \"openbmb/CPM-2B\",\n    \"architectures\": [\n        \"MiniCPMForCausalLM\"\n    ],\n    \"auto_map\": {\n        \"AutoConfig\": \"configuration_minicpm.MiniCPMConfig\",\n        \"AutoModel\": \"modeling_minicpm.MiniCPMModel\",\n        \"AutoModelForCausalLM\": \"modeling_minicpm.MiniCPMForCausalLM\",\n        \"AutoModelForSeq2SeqLM\": \"modeling_minicpm.MiniCPMForCausalLM\",\n        \"AutoModelForSequenceClassification\": \"modeling_minicpm.MiniCPMForSequenceClassification\"\n    },\n    \"bos_token_id\": 1,\n    \"eos_token_id\": 2,\n    \"hidden_act\": \"silu\",\n    \"hidden_size\": 2304,\n    \"initializer_range\": 0.1,\n    \"intermediate_size\": 5760,\n    \"max_position_embeddings\": 65536,\n    \"max_length\": 131072,\n    \"num_attention_heads\": 36,\n    \"num_hidden_layers\": 40,\n    \"num_key_value_heads\": 36,\n    \"rms_norm_eps\": 1e-05,\n    \"rope_scaling\": {\"type\": \"dynamic\", \"factor\": 4.0   },\n    \"torch_dtype\": \"bfloat16\",\n    \"transformers_version\": \"4.36.0\",\n    \"use_cache\": true,\n    \"vocab_size\": 122760,\n    \"scale_emb\": 12,\n    \"dim_model_base\": 256,\n    \"scale_depth\": 1.4,\n    \"tie_word_embeddings\": false,\n    \"rope_theta\": 1000000.0\n}\n```\n\n### First Bad Commit\n\nThis model's inference is correct on the llamacpp b4144 version.\n\n### Relevant log output\n\n```shell\n$./llama-cli --version\nversion: 4778 (a82c9e7c)\nbuilt with aarch64-none-linux-gnu-gcc (Arm GNU Toolchain 12.3.Rel1 (Build arm-12.35)) 12.3.1 20230626 for aarch64-none-linux-gnu\n\n$ python3 llama.cpp/convert_hf_to_gguf.py MiniCPM-2B-128k\nINFO:hf-to-gguf:Loading model: MiniCPM-2B-128k\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Exporting model...\nTraceback (most recent call last):\n  File \"llama.cpp/convert_hf_to_gguf.py\", line 5112, in <module>\n    main()\n  File \"llama.cpp/convert_hf_to_gguf.py\", line 5106, in main\n    model_instance.write()\n  File \"llama.cpp/convert_hf_to_gguf.py\", line 439, in write\n    self.prepare_tensors()\n  File \"llama.cpp/convert_hf_to_gguf.py\", line 280, in prepare_tensors\n    for name, data_torch in chain(self.generate_extra_tensors(), self.get_tensors()):\n  File \"llama.cpp/convert_hf_to_gguf.py\", line 2074, in generate_extra_tensors\n    raise KeyError('Missing the required key rope_scaling.long_factor or rope_scaling_short_factor')\nKeyError: 'Missing the required key rope_scaling.long_factor or rope_scaling_short_factor'\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-19T14:37:44+00:00",
    "closed_at": "2025-05-03T01:07:37+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12468/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12468"
  },
  {
    "number": 2778,
    "title": "[User] latest ggml-alloc not support as Xcode app package",
    "body": "@j-f1 hi. I used your repo(https://github.com/j-f1/LLM-Playground/), but couldn't load your branch 'jed/defaults', so I used the latest master from there, Since 11 f3ca06b8c66b0427aab0a472479da22553b472 commit introduced GGML - alloc. H later, unable to compile successfully. Error message:\r\n```\r\nUndefined symbols for architecture arm64:\r\n  \"_ggml_allocr_alloc\", referenced from:\r\n      llm_build_llama(llama_context&, int const*, float const*, int, int) in llama.o\r\n      llm_build_falcon(llama_context&, int const*, float const*, int, int) in llama.o\r\n  \"_ggml_allocr_alloc_graph\", referenced from:\r\n      _llama_new_context_with_model in llama.o\r\n      llama_eval_internal(llama_context&, int const*, float const*, int, int, int, char const*) in llama.o\r\n  \"_ggml_allocr_free\", referenced from:\r\n      _llama_new_context_with_model in llama.o\r\n      llama_context::~llama_context() in llama.o\r\n  \"_ggml_allocr_is_measure\", referenced from:\r\n      llm_build_llama(llama_context&, int const*, float const*, int, int) in llama.o\r\n      llm_build_falcon(llama_context&, int const*, float const*, int, int) in llama.o\r\n  \"_ggml_allocr_new\", referenced from:\r\n      _llama_new_context_with_model in llama.o\r\n  \"_ggml_allocr_new_measure\", referenced from:\r\n      _llama_new_context_with_model in llama.o\r\n  \"_ggml_allocr_reset\", referenced from:\r\n      llama_eval_internal(llama_context&, int const*, float const*, int, int, int, char const*) in llama.o\r\nld: symbol(s) not found for architecture arm64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n````",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-08-25T06:22:02+00:00",
    "closed_at": "2024-04-09T01:06:52+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2778/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2778"
  },
  {
    "number": 7048,
    "title": "Significantly different results (and WRONG) inference when GPU is enabled.",
    "body": "I am running llama_cpp version 0.2.68 on Ubuntu 22.04LTS under conda environment. Attached are two Jupyter notebooks with ONLY one line changed (use CPU vs GPU).  As you can see for exact same environmental conditions switching between CPU/GPU gives vastly different answers where the GPU is completely wrong.  Some pointers on how to debug this I would appreciate it.\r\n\r\nThe only significant difference between the two files is this one liner\r\n      `#n_gpu_layers=-1, # Uncomment to use GPU acceleration`\r\n\r\nThe model used was **openhermes-2.5-mistral-7b.Q5_K_M.gguf**\r\n\r\n[mistral_llama_large-gpu.pdf](https://github.com/ggerganov/llama.cpp/files/15192723/mistral_llama_large-gpu.pdf)\r\n[mistral_llama_large-cpu.pdf](https://github.com/ggerganov/llama.cpp/files/15192725/mistral_llama_large-cpu.pdf)\r\n\r\n",
    "labels": [
      "bug",
      "Nvidia GPU"
    ],
    "state": "closed",
    "created_at": "2024-05-02T18:51:50+00:00",
    "closed_at": "2024-05-17T18:49:39+00:00",
    "comments": 40,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7048/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7048"
  },
  {
    "number": 2373,
    "title": "[Bug report] Performance deterioration of LLaMA-2 model due to hardcoded rms_norm_eps ",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [Yes] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [Yes] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [Yes] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [Yes] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nWhen running converted ggml model, the eps used in RMSNorm is consistent with original model definition.\r\n\r\n# Current Behavior\r\n\r\nThe norm_eps used in RMSNorm is hardcoded to 1e-6, in all backends: X86, CUDA, Metal.\r\nRelated commit: Change RMSNorm eps to 1e-6 #173 (https://github.com/ggerganov/llama.cpp/commit/22213a17b56336bbea384a572a9484ce208c0333)\r\n\r\n# Environment and Context\r\n\r\nRecently I want to evaluate LLaMA-1 and LLaMA-2 models on MMLU (Measuring Massive Multitask Language Understanding, https://github.com/hendrycks/test) test set, and I chose llama.cpp as the inference engine.\r\nThe performance of LLaMA-1 models are nearly the same as the paper reported, but for LLaMA-2 7B and 13B models, they just got the LLaMA-1 7B level scores.\r\nThen I check the model definitions of LLaMA-2 7B and 13B and found the \u201crms_norm_eps\u201d in config.json is 1e-5 instead of 1e-6. \r\nAfter recompiling the source code with the change of eps=1-5, the test results of LLaMA-2 models are finally looking good.\r\n\r\nRelated issue: \r\nGGML model showing noticeable quality issues when compared to HF model #2354\r\n\r\nAffected discussions: \r\nLLaMA-2 Perplexities #2352\r\nPresentation on llama.cpp on 25.07.2023 at karlsruhe.ai #2281",
    "labels": [],
    "state": "closed",
    "created_at": "2023-07-24T13:58:11+00:00",
    "closed_at": "2023-07-24T15:57:14+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2373/reactions",
      "total_count": 4,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2373"
  },
  {
    "number": 12390,
    "title": "SYCL bug: DeepSeek-V2-Lite-Chat-Q4_K_M does not work as expected",
    "body": "### Name and Version\n\nroot@alc-ai:/home/aubrey/work/llama-gpu# ./build/bin/llama-cli --version\nversion: 4887 (8fcb5636)\nbuilt with Intel(R) oneAPI DPC++/C++ Compiler 2025.0.4 (2025.0.4.20241205) for x86_64-unknown-linux-gnu\n\n### Operating systems\n\n_No response_\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Command line\n\n```shell\n ./build/bin/llama-cli -m /srv/models/DeepSeek-V2-Lite-Chat-Q4_K_M/DeepSeek-V2-Lite-64x1.5B-Chat-Q4_K_M.gguf -ngl 99 -sm none -mg 0 -p \"what is your name?\" -n 30 -no-cnv\n```\n\n### Problem description & steps to reproduce\n\nroot@alc-ai:/home/aubrey/work/llama-gpu# ./build/bin/llama-cli -m /srv/models/DeepSeek-V2-Lite-Chat-Q4_K_M/DeepSeek-V2-Lite-64x1.5B-Chat-Q4_K_M.gguf -ngl 99 -sm none -mg 0 -p \"what is your name?\" -n 30 -no-cnv\nbuild: 4887 (8fcb5636) with Intel(R) oneAPI DPC++/C++ Compiler 2025.0.4 (2025.0.4.20241205) for x86_64-unknown-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_load_from_file_impl: using device SYCL0 (Intel(R) Arc(TM) A770 Graphics) - 15473 MiB free\nllama_model_loader: loaded meta data with 47 key-value pairs and 377 tensors from /srv/models/DeepSeek-V2-Lite-Chat-Q4_K_M/DeepSeek-V2-Lite-64x1.5B-Chat-Q4_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = deepseek2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek V2 Lite Chat\nllama_model_loader: - kv   3:                           general.finetune str              = Chat\nllama_model_loader: - kv   4:                           general.basename str              = DeepSeek-V2-Lite\nllama_model_loader: - kv   5:                         general.size_label str              = 64x1.5B\nllama_model_loader: - kv   6:                            general.license str              = other\nllama_model_loader: - kv   7:                       general.license.name str              = deepseek\nllama_model_loader: - kv   8:                       general.license.link str              = https://github.com/deepseek-ai/DeepSe...\nllama_model_loader: - kv   9:                      deepseek2.block_count u32              = 27\nllama_model_loader: - kv  10:                   deepseek2.context_length u32              = 163840\nllama_model_loader: - kv  11:                 deepseek2.embedding_length u32              = 2048\nllama_model_loader: - kv  12:              deepseek2.feed_forward_length u32              = 10944\nllama_model_loader: - kv  13:             deepseek2.attention.head_count u32              = 16\nllama_model_loader: - kv  14:          deepseek2.attention.head_count_kv u32              = 16\nllama_model_loader: - kv  15:                   deepseek2.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  16: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  17:                deepseek2.expert_used_count u32              = 6\nllama_model_loader: - kv  18:        deepseek2.leading_dense_block_count u32              = 1\nllama_model_loader: - kv  19:                       deepseek2.vocab_size u32              = 102400\nllama_model_loader: - kv  20:           deepseek2.attention.kv_lora_rank u32              = 512\nllama_model_loader: - kv  21:             deepseek2.attention.key_length u32              = 192\nllama_model_loader: - kv  22:           deepseek2.attention.value_length u32              = 128\nllama_model_loader: - kv  23:       deepseek2.expert_feed_forward_length u32              = 1408\nllama_model_loader: - kv  24:                     deepseek2.expert_count u32              = 64\nllama_model_loader: - kv  25:              deepseek2.expert_shared_count u32              = 2\nllama_model_loader: - kv  26:             deepseek2.expert_weights_scale f32              = 1.000000\nllama_model_loader: - kv  27:              deepseek2.expert_weights_norm bool             = false\nllama_model_loader: - kv  28:               deepseek2.expert_gating_func u32              = 1\nllama_model_loader: - kv  29:             deepseek2.rope.dimension_count u32              = 64\nllama_model_loader: - kv  30:                deepseek2.rope.scaling.type str              = yarn\nllama_model_loader: - kv  31:              deepseek2.rope.scaling.factor f32              = 40.000000\nllama_model_loader: - kv  32: deepseek2.rope.scaling.original_context_length u32              = 4096\nllama_model_loader: - kv  33: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700\nllama_model_loader: - kv  34:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  35:                         tokenizer.ggml.pre str              = deepseek-llm\nllama_model_loader: - kv  36:                      tokenizer.ggml.tokens arr[str,102400]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  37:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  38:                      tokenizer.ggml.merges arr[str,99757]   = [\"\u0120 \u0120\", \"\u0120 t\", \"\u0120 a\", \"i n\", \"h e...\nllama_model_loader: - kv  39:                tokenizer.ggml.bos_token_id u32              = 100000\nllama_model_loader: - kv  40:                tokenizer.ggml.eos_token_id u32              = 100001\nllama_model_loader: - kv  41:            tokenizer.ggml.padding_token_id u32              = 100001\nllama_model_loader: - kv  42:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  43:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  44:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  45:               general.quantization_version u32              = 2\nllama_model_loader: - kv  46:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  108 tensors\nllama_model_loader: - type q5_0:   14 tensors\nllama_model_loader: - type q8_0:   13 tensors\nllama_model_loader: - type q4_K:  229 tensors\nllama_model_loader: - type q6_K:   13 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 9.65 GiB (5.28 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 2\nload: token to piece cache size = 0.6408 MB\nprint_info: arch             = deepseek2\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 163840\nprint_info: n_embd           = 2048\nprint_info: n_layer          = 27\nprint_info: n_head           = 16\nprint_info: n_head_kv        = 16\nprint_info: n_rot            = 64\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 192\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 1\nprint_info: n_embd_k_gqa     = 3072\nprint_info: n_embd_v_gqa     = 2048\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 10944\nprint_info: n_expert         = 64\nprint_info: n_expert_used    = 6\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = yarn\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 0.025\nprint_info: n_ctx_orig_yarn  = 4096\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 16B\nprint_info: model params     = 15.71 B\nprint_info: general.name     = DeepSeek V2 Lite Chat\nprint_info: n_layer_dense_lead   = 1\nprint_info: n_lora_q             = 0\nprint_info: n_lora_kv            = 512\nprint_info: n_ff_exp             = 1408\nprint_info: n_expert_shared      = 2\nprint_info: expert_weights_scale = 1.0\nprint_info: expert_weights_norm  = 0\nprint_info: expert_gating_func   = softmax\nprint_info: rope_yarn_log_mul    = 0.0707\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 102400\nprint_info: n_merges         = 99757\nprint_info: BOS token        = 100000 '<\uff5cbegin\u2581of\u2581sentence\uff5c>'\nprint_info: EOS token        = 100001 '<\uff5cend\u2581of\u2581sentence\uff5c>'\nprint_info: EOT token        = 100001 '<\uff5cend\u2581of\u2581sentence\uff5c>'\nprint_info: PAD token        = 100001 '<\uff5cend\u2581of\u2581sentence\uff5c>'\nprint_info: LF token         = 185 '\u010a'\nprint_info: EOG token        = 100001 '<\uff5cend\u2581of\u2581sentence\uff5c>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 27 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 28/28 layers to GPU\nload_tensors:   CPU_Mapped model buffer size =   112.50 MiB\nload_tensors:        SYCL0 model buffer size =  9767.98 MiB\n.....................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 10000.0\nllama_context: freq_scale    = 0.025\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (163840) -- the full capacity of the model will not be utilized\nRunning with Environment Variables:\n  GGML_SYCL_DEBUG: 0\n  GGML_SYCL_DISABLE_OPT: 0\nBuild with Macros:\n  GGML_SYCL_FORCE_MMQ: no\n  GGML_SYCL_F16: no\nFound 2 SYCL devices:\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|  12.55|    512|    1024|   32| 16225M|         1.6.32224+14|\n| 1| [level_zero:gpu:1]|                 Intel UHD Graphics 770|   12.2|     32|     512|   32| 62707M|         1.6.32224+14|\nSYCL Optimization Feature:\n|ID|        Device Type|Reorder|\n|--|-------------------|-------|\n| 0| [level_zero:gpu:0]|      Y|\n| 1| [level_zero:gpu:1]|      N|\nllama_context:  SYCL_Host  output buffer size =     0.39 MiB\ninit: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 27, can_shift = 0\ninit:      SYCL0 KV buffer size =  1080.00 MiB\nllama_context: KV self size  = 1080.00 MiB, K (f16):  648.00 MiB, V (f16):  432.00 MiB\nllama_context:      SYCL0 compute buffer size =   213.03 MiB\nllama_context:  SYCL_Host compute buffer size =    12.01 MiB\nllama_context: graph nodes  = 1924\nllama_context: graph splits = 2\ncommon_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 8\n\nsystem_info: n_threads = 8 (n_threads_batch = 8) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n\nsampler seed: 2656463\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 30, n_keep = 1\n\nwhat is your name? is the difference between a man and a boy?\n2 Answers | Add Yours\nA man is an adult human male, while a boy is a\n\nllama_perf_sampler_print:    sampling time =       1.10 ms /    36 runs   (    0.03 ms per token, 32786.89 tokens per second)\nllama_perf_context_print:        load time =    3147.22 ms\nllama_perf_context_print: prompt eval time =     288.22 ms /     6 tokens (   48.04 ms per token,    20.82 tokens per second)\nllama_perf_context_print:        eval time =    1660.91 ms /    29 runs   (   57.27 ms per token,    17.46 tokens per second)\nllama_perf_context_print:       total time =    1952.91 ms /    35 tokens\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-14T14:31:14+00:00",
    "closed_at": "2025-03-15T14:19:31+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12390/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12390"
  },
  {
    "number": 946,
    "title": "Question regarding distributed computing...",
    "body": "I have currently access to 20 old computers, each with 32GB ram and 4 cores, 256gb ssd, 1 gbit speed network, connected to a 48port switch. (i could get a lot lot more computers but i dont have enough electricity currently)\r\nWould it be somehow possible to distribute the llama model with llama.cpp to the 20 computers to being able to run the 65b model at a moderate speed?\r\nWhat would i have to do to distribute the model on many computers to run it on cpu?\r\ni am only interested in inference, not training..... for training i can rent cloud gpu's.\r\n\r\nThanks for any input that would help me / recommendation / problems.\r\n\r\nWhat i see as a problem is how to split the model / models (in case i use other models) efficiently so that network bandwidth isnt the limiting factor.\r\n\r\n\r\n\r\n\r\n\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-13T14:03:55+00:00",
    "closed_at": "2024-04-11T01:06:32+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/946/reactions",
      "total_count": 11,
      "+1": 11,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/946"
  },
  {
    "number": 4925,
    "title": "can' quantize deekseek model",
    "body": "When I download the model from the deepseek huggingface official repository, I cannot convert it into a gguf file\u3002\r\n\r\npython D:\\Ai\\convert.py D:\\Ai\\deepseek-coder-6.7b-instruct\r\n\r\nD:\\Ai\\gguf-py\r\nLoading model file D:\\Ai\\deepseek-coder-6.7b-instruct\\pytorch_model-00001-of-00002.bin\r\nLoading model file D:\\Ai\\deepseek-coder-6.7b-instruct\\pytorch_model-00001-of-00002.bin\r\nLoading model file D:\\Ai\\deepseek-coder-6.7b-instruct\\pytorch_model-00002-of-00002.bin\r\nparams = Params(n_vocab=32256, n_embd=4096, n_layer=32, n_ctx=16384, n_ff=11008, n_head=32, n_head_kv=32, f_norm_eps=1e-06, n_experts=None, n_experts_used=None, rope_scaling_type=<RopeScalingType.LINEAR: 'linear'>, f_rope_freq_base=100000, f_rope_scale=4.0, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=WindowsPath('D:/Ai/deepseek-coder-6.7b-instruct'))\r\nTraceback (most recent call last):\r\n  File \"D:\\Ai\\convert.py\", line 1658, in <module>\r\n    main(sys.argv[1:])  # Exclude the first element (script name) from sys.argv\r\n    ^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Ai\\convert.py\", line 1614, in main\r\n    vocab, special_vocab = vocab_factory.load_vocab(args.vocab_type, model_parent_path)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Ai\\convert.py\", line 1409, in load_vocab\r\n    path = self._select_file(vocabtype)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Ai\\convert.py\", line 1384, in _select_file\r\n    raise FileNotFoundError(f\"{vocabtype} {file_key} not found.\")\r\nFileNotFoundError: spm tokenizer.model not found.",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-01-14T07:19:27+00:00",
    "closed_at": "2024-04-18T01:06:43+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4925/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4925"
  },
  {
    "number": 11598,
    "title": "Misc. bug: Vulcan premature out of memory exception on AMD Instinct MI60",
    "body": "### Name and Version\n\nllama-cli --version\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = AMD Radeon Graphics (RADV VEGA20) (radv) | uma: 0 | fp16: 1 | warp size: 64 | matrix cores: none\nversion: 4615 (bfcce4d6)\nbuilt with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n\n\n\n\n### Operating systems\n\nUbuntu 24.04.\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server \n\n### Command line\n\nllama-server -m ~/llamamodels/Qwen2-7B-Instruct/Qwen2.5-7B-Instruct-1M-Q8_0.gguf -c 72000 -ngl 99\n\n\n### Problem description & steps to reproduce\n\nHello,\n\nThe AMD Instinct MI60 cards have 32GB of VRAM. While using ROCm I can use the whole 32GB but with Vulcan it seems that one llama-server instance can access only 16GB.\nI tested it with Qwen 2.5 7B 1M model with the context length up to 1 million) and I cannot start it with a context of more than 71K. \nBut at the same time I can start 2 instances with the 71K context length on the same card.\n\nFor example, two of these could be started at the same time:\nllama-server -m ~/llamamodels/Qwen2-7B-Instruct/Qwen2.5-7B-Instruct-1M-Q8_0.gguf -c 71000 -ngl 99\n\nHowever if I try to start just one with the 72K context I get the following error:\n\nllama_init_from_model: KV self size \u00a0= 3937.50 MiB, K (f16): 1968.75 MiB, V (f16): 1968.75 MiB\nllama_init_from_model: Vulkan_Host \u00a0output buffer size = \u00a0 \u00a0 0.58 MiB\nggml_vulkan: Device memory allocation of size 4305588224 failed.\nggml_vulkan: Requested buffer size exceeds device memory allocation limit: ErrorOutOfDeviceMemory\nggml_gallocr_reserve_n: failed to allocate Vulkan0 buffer of size 4305588224\nggml_vulkan: Device memory allocation of size 4305588224 failed.\nggml_vulkan: Requested buffer size exceeds device memory allocation limit: ErrorOutOfDeviceMemory\nggml_gallocr_reserve_n: failed to allocate Vulkan0 buffer of size 4305588224\nllama_init_from_model: failed to allocate compute buffers\ncommon_init_from_params: failed to create context with model '/root/llamamodels/Qwen2-7B-Instruct/Qwen2.5-7B-Instruct-1M-Q8_0.gguf'\nsrv \u00a0 \u00a0load_model: failed to load model, '/root/llamamodels/Qwen2-7B-Instruct/Qwen2.5-7B-Instruct-1M-Q8_0.gguf'\n\n\nI did try to disable the verification in ggml-vulkan.cpp and was able to increase context length to 220K while utilizing only 86% of the VRAM.\nBut while it was working I just started to receive gibberish after the context length exceeded 71K.\n\nI tried different versions of Vulkan but the error remains. \n\n",
    "labels": [
      "bug-unconfirmed",
      "Vulkan",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-02T17:21:05+00:00",
    "closed_at": "2025-04-26T01:07:46+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11598/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11598"
  }
]
[
  {
    "number": 11764,
    "title": "Misc. bug: Quantizing Olmo models with imatrix failing on some sizes",
    "body": "### Name and Version\n\nversion 4585\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-quantize\n\n### Command line\n\n```shell\n./llama-quantize --imatrix /models/OLMo-2-1124-7B-Instruct-GGUF/allenai_OLMo-2-1124-7B-Instruct.imatrix /models/OLMo-2-1124-7B-Instruct-GGUF/allenai_OLMo-2-1124-7B-Instruct-f32.gguf /models/OLMo-2-1124-7B-Instruct-GGUF/allenai_OLMo-2-1124-7B-Instruct-Q5_K_M.gguf Q5_K_M\n```\n\n### Problem description & steps to reproduce\n\nWithout imatrix I don't get any issues.\n\nQuantizing OLMo-2 7B to Q5_K_M, Q5_K_S, Q4_K_M, and Q4_K_S, and Q2_K with imatrix results in:\n\n```\nblk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. ggml_validate_row_data: found nan value at block 48\nggml_validate_row_data: found nan value at block 16\n```\n```\nblk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q5_K .. ggml_validate_row_data: found nan value at block 48\nggml_validate_row_data: found nan value at block 16\n```\n```\nblk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q2_K .. ggml_validate_row_data: found nan value at block 48\nggml_validate_row_data: found nan value at block 16\n```\n\nAll other sizes quantize without issue..\n\nAdditionally, the 13B model fails in a different way on IQ2_M and IQ2_S:\n\n```\n[  95/ 443]                  blk.8.attn_q.weight - [ 5120,  5120,     1,     1], type =    f32, converting to iq2_xs .. /llama.cpp/ggml/src/ggml-quants.c:3279: fatal error\nOops: found point 4 not on grid: 0 1 0 0 0 0 0 0\nlibggml-base.so(+0x159cb)[0x72a78fe039cb]\nlibggml-base.so(ggml_abort+0x15f)[0x72a78fe03d6f]\nlibggml-base.so(+0x3bcbb)[0x72a78fe29cbb]\nlibggml-base.so(quantize_iq2_xs+0x81)[0x72a78fe45691]\nlibggml-base.so(ggml_quantize_chunk+0x371)[0x72a78fe12431]\nlibllama.so(+0xeaa35)[0x72a78ffaca35]\nlibllama.so(llama_model_quantize+0xf4)[0x72a78ffae094]\n./llama-quantize(+0x17d6a)[0x60055e597d6a]\n/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x72a78f8b5d90]\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x72a78f8b5e40]\n./llama-quantize(+0x18c25)[0x60055e598c25]\n```\n\nAll other sizes have no issues\n\nI've uploaded both F32 conversions as well as imatrix files here:\n\nhttps://huggingface.co/bartowski/PleaseIgnore_uploaded_for_testing",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-08T19:10:22+00:00",
    "closed_at": "2025-03-25T01:07:39+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11764/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11764"
  },
  {
    "number": 1777,
    "title": "broken state save/restore since \"store offset as opt arg for ggml_view_xd() operators\"",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\n`llama_copy_state_data()` should do what it's documented to do\r\n\r\n# Current Behavior\r\n\r\n`llama_copy_state_data()` causes a Segmentation fault\r\n\r\n# Environment and Context\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n```\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         39 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  12\r\n  On-line CPU(s) list:   0-11\r\nVendor ID:               GenuineIntel\r\n  Model name:            Intel(R) Core(TM) i7-8700K CPU @ 3.70GHz\r\n    CPU family:          6\r\n    Model:               158\r\n    Thread(s) per core:  2\r\n    Core(s) per socket:  6\r\n    Socket(s):           1\r\n    Stepping:            10\r\n    CPU(s) scaling MHz:  17%\r\n    CPU max MHz:         4700.0000\r\n    CPU min MHz:         800.0000\r\n    BogoMIPS:            7399.70\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts a\r\n                         cpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch\r\n                         _perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq \r\n                         dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_\r\n                         2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowp\r\n                         refetch cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexprior\r\n                         ity ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed a\r\n                         dx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hw\r\n                         p hwp_notify hwp_act_window hwp_epp md_clear flush_l1d arch_capabilities\r\nVirtualization features: \r\n  Virtualization:        VT-x\r\nCaches (sum of all):     \r\n  L1d:                   192 KiB (6 instances)\r\n  L1i:                   192 KiB (6 instances)\r\n  L2:                    1.5 MiB (6 instances)\r\n  L3:                    12 MiB (1 instance)\r\nNUMA:                    \r\n  NUMA node(s):          1\r\n  NUMA node0 CPU(s):     0-11\r\nVulnerabilities:         \r\n  Itlb multihit:         KVM: Mitigation: Split huge pages\r\n  L1tf:                  Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable\r\n  Mds:                   Mitigation; Clear CPU buffers; SMT vulnerable\r\n  Meltdown:              Mitigation; PTI\r\n  Mmio stale data:       Mitigation; Clear CPU buffers; SMT vulnerable\r\n  Retbleed:              Mitigation; IBRS\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\n  Spectre v2:            Mitigation; IBRS, IBPB conditional, STIBP conditional, RSB filling, PBRSB-eIBRS Not a\r\n                         ffected\r\n  Srbds:                 Mitigation; Microcode\r\n  Tsx async abort:       Mitigation; TSX disabled\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n```\r\nLinux lyllia 6.1.27 #3 SMP PREEMPT_DYNAMIC Thu Jun  8 14:05:39 CEST 2023 x86_64 GNU/Linux\r\n```\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\ng++ (Debian 12.2.0-14) 12.2.0\r\nCopyright (C) 2022 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n\r\n(The rest is not applicable here)\r\n\r\n# Failure Information (for bugs)\r\n\r\nThis regression was introduced in https://github.com/ggerganov/llama.cpp/pull/1642/commits/94ea9e7bfecc1116afc830606cf886f1b3620257\r\n\r\n# Steps to reproduce\r\n\r\nJust call `llama_copy_state_data()` the way you normally would. Nothing too complicated here.\r\n\r\n# Failure Logs\r\n\r\n```\r\n#0  0x00007ffff6e16d34 in ggml_view_3d (ctx=0x7ffff6eeebd8 <g_state+200>, a=0x7ffed47ff030, ne0=<optimized out>, ne1=<optimized out>, ne2=32, nb1=8192, nb2=16777216, offset=0) at ggml.c:5901\r\n        ne = {4096, 23, 32, 1}\r\n        is_node = <optimized out>\r\n        result = 0x7ffffffe50d0\r\n        offs = 0x7ffffffe51d0\r\n\r\n#1  0x00007ffff6e00a69 in llama_copy_state_data (ctx=0x55555556e1d0, dst=<optimized out>) at llama.cpp:2751\r\n        buffer = <uninitialized>\r\n        kout3d = <optimized out>\r\n        v3d = <optimized out>\r\n        gf = <error reading variable gf (value of type `ggml_cgraph' requires 98360 bytes, which is more than max-value-size)>\r\n        elt_size = 2\r\n        cpy_ctx = 0x7ffff6eeebd8 <g_state+200>\r\n        vout3d = <optimized out>\r\n        k3d = <optimized out>\r\n        kv_size = <optimized out>\r\n        kv_self = @0x55555556f5f8: {k = 0x7ffed47ff030, v = 0x7ffef47ff130, ctx = 0x7ffff6eeeb78 <g_state+104>, buf = {addr = 0x7ffed47ff010 \" \", size = 1075838976}, n = 23}\r\n        hparams = @0x55555556f59c: {n_vocab = 32000, n_ctx = 2048, n_embd = 4096, n_mult = 256, n_head = 32, n_layer = 32, n_rot = 128, ftype = LLAMA_FTYPE_MOSTLY_Q4_0}\r\n        kv_ntok = <optimized out>\r\n        n_layer = <optimized out>\r\n        n_embd = <optimized out>\r\n        n_ctx = <optimized out>\r\n        out = 0x7ffe2397843c \"\"\r\n        written = <optimized out>\r\n        max_size = <optimized out>\r\n\r\n#2  0x00007ffff7fba1f1 in (anonymous namespace)::CppAPITest_llama_7B_ggmlv3_StateSave_Test::TestBody (this=this@entry=0x55555578a5c0) at ../llmodel_cpp_test.cpp:57\r\n        gtest_ar = {success_ = 192, message_ = {_M_t = {<std::__uniq_ptr_impl<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::default_delete<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >> = {_M_t = {<std::_Tuple_impl<0ul, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::default_delete<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >> = {<std::_Tuple_impl<1ul, std::default_delete<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >> = {<std::_Head_base<1ul, std::default_delete<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, true>> = {_M_head_impl = {<No data fields>}}, <No data fields>}, <std::_Head_base<0ul, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, false>> = {_M_head_impl = 0x55555578a5c0}, <No data fields>}, <No data fields>}}, <No data fields>}}}\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-06-09T11:04:16+00:00",
    "closed_at": "2023-07-07T20:01:07+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1777/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1777"
  },
  {
    "number": 13027,
    "title": "Feature Proposal: Server Model Switching at Runtime",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nI would like to adapt the server (or create an alternate server) so that it is more suited to being changed during runtime.  My primary goal in doing so is to be able to switch models on the fly.\n\n### Motivation\n\nIn local inference, I find that no single model is best for all tasks and I switch between models frequently using TabbyAPI. I would like to be able to have this functionality available directly in llama.cpp to be able to make use of GGUF files and the llama.cpp ecosystem.\n\n### Possible Implementation\n\nSpecifically, I want to be able to:\n\n* Start the server without a model loaded and be functional in a state without models\n* Have multiple models loaded concurrently\n* Offer a /models endpoint to:\n    * List available models (GET:/models)\n    * Get details about a specific model (GET:/models/{model_id})\n    * List available draft models (GET:/models/draft_models)\n    * Get details about a specific draft model (GET:/models/draft_models/{model_id})\n    * Change model routing settings (POST:/models)\n    * List loaded models and default model routing (GET:/models/status)\n    * Load models (POST:/models/load)\n    * Unload models (POST:/models/unload)\n\nThis may also require similar endpoints for loading and unloading LoRAs, embeddings, and other factors. For example, I may want to be able to monitor GPU status for available VRAM and use that as a check before loading models.\n\nBefore I get started, I wanted to solicit some feedback from the community on endpoint names and required features. Does anyone have any thoughts about this, the names, if there are negative impacts, or anything else?",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-04-19T18:17:46+00:00",
    "closed_at": "2025-06-29T01:08:21+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13027/reactions",
      "total_count": 8,
      "+1": 8,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13027"
  },
  {
    "number": 13100,
    "title": "Eval: HIP: Llama-server multi-instance lockup",
    "body": "**Follow-up on the https://github.com/ggml-org/llama.cpp/issues/12991**\n\nAccording to rocgdb backtrace, threads that are working with gpus are stuck somewhere in the libhsa-runtime\n\n```\nThread 2 (Thread 0x7fffd7fff6c0 (LWP 11602) \"llama-server\"):\n#0  __GI___ioctl (fd=3, request=3222817548) at ../sysdeps/unix/sysv/linux/ioctl.c:36\n#1  0x00007fffd8549400 in ?? () from /opt/rocm-6.4.0/lib/llvm/bin/../../../lib/libhsa-runtime64.so.1\n#2  0x00007fffd8541f1f in ?? () from /opt/rocm-6.4.0/lib/llvm/bin/../../../lib/libhsa-runtime64.so.1\n#3  0x00007fffd84be632 in ?? () from /opt/rocm-6.4.0/lib/llvm/bin/../../../lib/libhsa-runtime64.so.1\n#4  0x00007fffd84a1aee in ?? () from /opt/rocm-6.4.0/lib/llvm/bin/../../../lib/libhsa-runtime64.so.1\n#5  0x00007fffd8439241 in ?? () from /opt/rocm-6.4.0/lib/llvm/bin/../../../lib/libhsa-runtime64.so.1\n#6  0x00007ffff749caa4 in start_thread (arg=<optimized out>) at ./nptl/pthread_create.c:447\n#7  0x00007ffff7529c3c in clone3 () at ../sysdeps/unix/sysv/linux/x86_64/clone3.S:78\n\nThread 1 (Thread 0x7fffec12f840 (LWP 11595) \"llama-server\"):\n#0  0x00007fffd847cbf4 in ?? () from /opt/rocm-6.4.0/lib/llvm/bin/../../../lib/libhsa-runtime64.so.1\n#1  0x00007fffd847ca3e in ?? () from /opt/rocm-6.4.0/lib/llvm/bin/../../../lib/libhsa-runtime64.so.1\n#2  0x00007fffd8470c81 in ?? () from /opt/rocm-6.4.0/lib/llvm/bin/../../../lib/libhsa-runtime64.so.1\n#3  0x00007fffeabb9ebb in ?? () from /opt/rocm-6.4.0/lib/llvm/bin/../../../lib/libamdhip64.so.6\n#4  0x00007fffeaba6087 in ?? () from /opt/rocm-6.4.0/lib/llvm/bin/../../../lib/libamdhip64.so.6\n#5  0x00007fffeaac7618 in ?? () from /opt/rocm-6.4.0/lib/llvm/bin/../../../lib/libamdhip64.so.6\n#6  0x00007fffeaac7cb8 in ?? () from /opt/rocm-6.4.0/lib/llvm/bin/../../../lib/libamdhip64.so.6\n#7  0x00007fffefafa63e in ggml_backend_cuda_buffer_set_tensor(ggml_backend_buffer*, ggml_tensor*, void const*, unsigned long, unsigned long) () from /home/pl752/llama.cpp/build/bin/libggml-hip.so\n#8  0x00007ffff7c88189 in ggml_backend_sched_graph_compute_async () from /home/pl752/llama.cpp/build/bin/libggml-base.so\n#9  0x00007ffff7db4c51 in llama_context::graph_compute(ggml_cgraph*, bool) () from /home/pl752/llama.cpp/build/bin/libllama.so\n#10 0x00007ffff7db7038 in llama_context::decode(llama_batch&) () from /home/pl752/llama.cpp/build/bin/libllama.so\n#11 0x00007ffff7db843f in llama_decode () from /home/pl752/llama.cpp/build/bin/libllama.so\n#12 0x00005555555ff4d9 in server_context::update_slots() ()\n#13 0x00005555555c9dd1 in server_queue::start_loop() ()\n#14 0x00005555555972b1 in main ()\n```\n(Sorry for not using the template)\n\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-04-24T15:46:03+00:00",
    "closed_at": "2025-06-09T01:08:02+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13100/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13100"
  },
  {
    "number": 12876,
    "title": "Misc. bug: missing `<image_soft_token>` (id 262144 ) in Gemma3-it token map?",
    "body": "From the official `tokenizer.json`:\n```json\n    {\n      \"id\": 262144,\n      \"content\": \"<image_soft_token>\",\n      \"single_word\": false,\n      \"lstrip\": false,\n      \"rstrip\": false,\n      \"normalized\": false,\n      \"special\": true\n    }\n```\nhttps://huggingface.co/google/gemma-3-4b-it/blob/main/tokenizer.json\nalso here:\nhttps://huggingface.co/google/gemma-3-4b-it/blob/main/added_tokens.json\n\nThere is no token with id 262144 in the converted gguf models. (it's also missing from the Google's own qat gguf models).",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-04-10T12:50:33+00:00",
    "closed_at": "2025-04-10T16:31:12+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12876/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12876"
  },
  {
    "number": 4171,
    "title": "Converting a StableLM fine tuned model fails with `Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.`",
    "body": "# Prerequisites\r\n\r\nTested on latest commit, 8e672efe632bb6a7333964a255c4b96f018b9a65 , and also on commits from yesterday.\r\n\r\n# Current Behavior\r\n\r\nTrying to convert model https://huggingface.co/pansophic/rocket-3B\r\n\r\nResults in:\r\n```\r\n [pytorch2] tomj@MC:/workspace/git/gguf-llama (master \u2718)\u272d \u1405 python3 ./convert-hf-to-gguf.py /workspace/process/pansophic_rocket-3b/source --outtype f16 --outfile /workspace/process/pansophic_rocket-3b/gguf/rocket-3b.fp16.gguf\r\nLoading model: source\r\ngguf: This GGUF file is for Little Endian only\r\nSet model parameters\r\nSet model tokenizer\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\ngguf: Adding 50009 merge(s).\r\ngguf: Setting special token type bos to 0\r\ngguf: Setting special token type eos to 0\r\ngguf: Setting special token type unk to 0\r\nExporting model to '/workspace/process/pansophic_rocket-3b/gguf/rocket-3b.fp16.gguf'\r\ngguf: loading model part 'pytorch_model.bin'\r\nTraceback (most recent call last):\r\n  File \"/workspace/git/gguf-llama/./convert-hf-to-gguf.py\", line 897, in <module>\r\n    model_instance.write()\r\n  File \"/workspace/git/gguf-llama/./convert-hf-to-gguf.py\", line 126, in write\r\n    self.write_tensors()\r\n  File \"/workspace/git/gguf-llama/./convert-hf-to-gguf.py\", line 98, in write_tensors\r\n    data = data_torch.squeeze().numpy()\r\nRuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\r\n```\r\n\r\nI noticed that the latest commits mentioned StaleLM so I tried rolling back to before them, but still got the same error.\r\n\r\nI have confirmed that the model loads OK via Transformers, so it appears to be valid.\r\n\r\nAny thoughts @Galunid ?\r\n\r\nThanks in advance\r\n\r\n# Environment and Context\r\n\r\nUbuntu 22.04, Python 3.10\r\n\r\n\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2023-11-22T18:06:09+00:00",
    "closed_at": "2023-11-24T14:02:51+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4171/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4171"
  },
  {
    "number": 1120,
    "title": "Latest release wont compile (ggml.c)",
    "body": "Release: master-36b4f7e\r\n\r\nggml.c: In function \u2018bytes_from_nibbles_16\u2019:\r\nggml.c:439:19: warning: implicit declaration of function \u2018_mm_loadu_si64\u2019; did you mean \u2018_mm_loadl_epi64\u2019? [-Wimplicit-function-declaration]\r\n\r\n     __m128i tmp = _mm_loadu_si64( ( const __m128i* )rsi );\r\n                   ^~~~~~~~~~~~~~\r\n                   _mm_loadl_epi64\r\nggml.c:439:19: error: incompatible types when initializing type \u2018__m128i\u2019 {aka \u2018__vector(2) long long int\u2019} using type \u2018int\u2019\r\n\r\n\r\nUsing:\r\ncc (Ubuntu 8.4.0-3ubuntu2) 8.4.0\r\ng++ (Ubuntu 8.4.0-3ubuntu2) 8.4.0",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-22T10:05:00+00:00",
    "closed_at": "2023-04-24T15:38:28+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1120/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1120"
  },
  {
    "number": 1735,
    "title": "with the newest builds i only get gibberish output",
    "body": "After the CUDA refactor PR #1703 by @JohannesGaessler was merged i wanted to try it out this morning and measure the performance difference on my ardware.\r\nI use my standard prompts with different models in different sizes.\r\n\r\nI use the prebuild versions win-cublas-cu12.1.0-xx64\r\n\r\nWith the new builds I only get gibberish as a response for all prompts used and all models.\r\nIt looks like a random mix of words in different languages.\r\n\r\nOn my current PC I can only use the win-avx-x64 version, here I still get normal output.\r\n\r\nI will use the Cuda-pc again in a few hours, then I can provide sample output or more details.\r\nAm I the only one with this problem?",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2023-06-07T08:06:19+00:00",
    "closed_at": "2023-06-15T08:50:50+00:00",
    "comments": 81,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1735/reactions",
      "total_count": 6,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1735"
  },
  {
    "number": 9671,
    "title": "Bug: Initializing KV Cache Spikes Memory, Crashing on Android",
    "body": "### What happened?\n\nHi,\r\n\r\nYou may already know about the memory spike, given #7474.\r\n\r\nFor those unfamiliar, `ggml_backend_cpu_buffer_clear` calls `memset`, which initializes the allocated buffer (as big as 16 GiB for full context on Llama 3.1) to `0`s, spiking memory and, on Android, leading to a system crash --\r\n- If in Termux, Android kills it\r\n- If in `adb shell`, Android hangs and reboots\r\n\r\nAs far as I can tell, there are no guards for when `llama.cpp` might over-allocate _and_ over-initialize memory \u2014 this may be intended, but it seems to defeat the purpose of `mmap`.\r\n\r\nPlease share your perspective on this behavior; I understand it to be undefined. With limited experience, I see a number of potential solutions: \r\n1. Make `ggml_backend_buffer_clear` truly optional\r\n\t- Alternatively, skip it in certain environments\r\n2. Use `ggml_backend_cpu_buffer_memset_tensor` in the `alloc_tensor_range` loop instead to avoid bulk initialization, perhaps as part of `ggml_tallocr_alloc` or in a separate function\r\n3. Require `-c` in certain environments\r\n\r\nTo reproduce this behavior, build for Android and run `llama-cli` or `llama-simple` or `llama-server` with any quantization of Llama 3.1; the default behavior of `llama.cpp` without `-c` is to obtain the context from the model itself, which will load the full context in this case.\r\n\r\nI would be happy to implement a fix, whatever is decided. If instead downstream applications should manage this themselves, please clarify.\r\n\r\nThank you.\n\n### Name and Version\n\n### Termux\r\n```\r\n$ ./llama-simple --version\r\nversion: 3830 (b5de3b74)\r\nbuilt with clang version 18.1.8 for aarch64-unknown-linux-android29\r\n```\r\n\r\n### `adb shell`\r\n```\r\n$ LD_LIBRARY_PATH=lib ./llama-simple --version                                                  \r\nversion: 3830 (b5de3b74)\r\nbuilt with Android (8490178, based on r450784d) clang version 14.0.6 (https://android.googlesource.com/toolchain/llvm-project 4c603efb0cca074e9238af8b4106c30add4418f6) for x86_64-unknown-linux-gnu\r\n```\n\n### What operating system are you seeing the problem on?\n\nOther? (Please let us know in description)\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-09-27T22:08:13+00:00",
    "closed_at": "2024-09-29T20:47:14+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9671/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9671"
  },
  {
    "number": 4988,
    "title": "Parallelize dequantization or fp format conversion when using blas",
    "body": "In ggml_compute_forward_mul_mat@ggml.c, gemm parallelism is managed by blas library. However, this disables multithreading on dequantizing weights, which may be a bottleneck.\r\n\r\nI have performed some ugly modifications for comparing performance.\r\n\r\n```\r\n\u203a MKL_NUM_THREADS=8 ./llama-bench -m Llama-2-7b-chat-q4km.gguf -t 8\r\n| model                          |       size |     params | backend    |    threads | test       |              t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | ---------: | ---------- | ---------------: |\r\n| llama 7B Q4_K - Medium         |   3.80 GiB |     6.74 B | BLAS       |          8 | pp 512     |     55.24 \u00b1 3.16 |\r\n| llama 7B Q4_K - Medium         |   3.80 GiB |     6.74 B | BLAS       |          8 | tg 128     |     21.12 \u00b1 1.89 |\r\n\r\nbuild: 862f5e4 (1886)\r\n\r\n\u203a MKL_NUM_THREADS=8 ./llama-bench -m Llama-2-7b-chat-q4km.gguf -t 8\r\n| model                          |       size |     params | backend    |    threads | test       |              t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | ---------: | ---------- | ---------------: |\r\n| llama 7B Q4_K - Medium         |   3.80 GiB |     6.74 B | BLAS       |          8 | pp 512     |     79.34 \u00b1 5.23 |\r\n| llama 7B Q4_K - Medium         |   3.80 GiB |     6.74 B | BLAS       |          8 | tg 128     |     21.26 \u00b1 1.83 |\r\n\r\nmodified\r\n```\r\n[cb828c8](https://github.com/ReinForce-II/llama.cpp/commit/cb828c8d5a1851f0c9ab5b8c1609ff1ebd761fdc)\r\n\r\n",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2024-01-16T21:03:44+00:00",
    "closed_at": "2024-01-22T13:15:09+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4988/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4988"
  },
  {
    "number": 321,
    "title": "High performance API",
    "body": "Hey!\r\n\r\nI'd love to see this project being able to be used through some TCP socket with a very optimized protocol. One it may make use of something like protobuf, or even grpc.\r\nI think everyone agrees HTTP would be a complete overkill specially for a project focused on high performance. :laughing: \r\n\r\nThanks\r\nNiansa",
    "labels": [
      "duplicate",
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-20T11:34:40+00:00",
    "closed_at": "2023-03-20T19:22:42+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/321/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/321"
  },
  {
    "number": 8518,
    "title": "Support for H2O Danube3 Family of Models",
    "body": "### Prerequisites\r\n\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\r\n\r\n### Feature Description\r\n\r\nI have tested the GGUF (https://huggingface.co/h2oai/h2o-danube3-500m-chat-GGUF) with Android example from llama.cpp and it is working for me. TTFT and Throughput speed are \ud83d\udd25 \r\n\r\n### Motivation\r\n\r\nv v small models to run on edge devices and are on a par with qwen2 (0.5b) and phi-3 (4b)\r\n\r\n### Possible Implementation\r\n\r\nShould work directly since model arch is LlamaForCausalLM. I will check if it's supported directly without any change. Prompt support might be needed though. \r\n\r\n<|prompt|>Why is drinking water so healthy?</s><|answer|>\r\n\r\nI will check these 2 and report back\r\n1. Model Conversion to GGUF\r\n2. Running GGUF model",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2024-07-16T16:02:31+00:00",
    "closed_at": "2024-07-16T18:37:15+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8518/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8518"
  },
  {
    "number": 11933,
    "title": "Feature Request: dynamic speculation (i.e. dynamic draft-max)",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nAdjust draft-max on the fly during generation to optimize speculative generation performance.\n\n### Motivation\n\nSpeculative generation works best in structured text (so, big highly predictable chunks) like code. The larger the draft-max the more speedup is possible. But then, a large draft-max wastes time when you're in less structured text. So it seems some sort of dynamic adjustment of draft-max would be ideal.\n\n### Possible Implementation\n\nIt looks like [this has been tried](https://huggingface.co/blog/assisted-generation#greedy-decoding-with-assisted-generation) with some success. Their heuristic seems like a decent starting point: +2 draft-max on a fully successful speculation, -1 otherwise, presumably with draft-min as a floor. I can imagine you could wring a little more performance out of something fancier, maybe get some inspiration from congestion control algorithms (they're roughly the same shape: empirically probing for an unknown window size, with feedback being success/failure at the current size).\n\nThis looks pretty easy to implement; callers of `common_speculative_gen_draft()` just need to adjust `common_speculative_params.n_draft`, and the feedback is number of tokens accepted by `common_sampler_sample_and_accept_n()`. I'm going to implement this at least for myself, starting in the server - just need to add a field to the `server_slot` struct. If there's interest in this actually getting merged, I'll also implement it for the CLI and wherever else, modify command line arguments to cleanly support using this vs static draft-max, and do some evaluations.",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2025-02-17T20:43:30+00:00",
    "closed_at": "2025-02-18T03:29:48+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11933/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11933"
  },
  {
    "number": 9440,
    "title": "Feature Request: Pixtral by Mistral support (pixtral-12b-240910)",
    "body": "### Prerequisites\r\n\r\n- [X] I am running the latest code. Mention the version if possible as well.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\r\n\r\n### Feature Description\r\n\r\nDear llama.cpp team,\r\n\r\nMistral has just released Pixtral and I would like to request support for it, if possible.\r\n\r\nHere are some relevant links:\r\n\r\n**X announcement:** https://x.com/mistralai/status/1833758285167722836\r\n\r\n**Magnet link:** `xt=urn:btih:7278e625de2b1da598b23954c13933047126238a&dn=pixtral-12b-240910&tr=udp%3A%2F%http://2ftracker.opentrackr.org/%3A1337%2Fannounce&tr=udp%3A%2F%http://2fopen.demonii.com/%3A1337%2Fannounce&tr=http%3A%2F%http://2ftracker.ipv6tracker.org/%3A80%2Fannounce`\r\n\r\n**HuggingFace alternative download link:** https://huggingface.co/mistral-community/pixtral-12b-240910\r\n\r\n**Additional information:** https://github.com/mistralai/mistral-common/releases/tag/v1.4.0\r\n\r\n**Important notes from the readme:** \r\n- Use GELU for the vision adapter\r\n- Use 2D ROPE for the vision encoder\r\n\r\n---------------\r\n\r\nThank you for your time and consideration!",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-11T18:03:29+00:00",
    "closed_at": "2025-02-08T01:07:14+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9440/reactions",
      "total_count": 145,
      "+1": 117,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 28,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9440"
  },
  {
    "number": 7450,
    "title": "Since last update Mistral models doesn't works anymore",
    "body": "since this https://github.com/ggerganov/llama.cpp/tree/b2961\r\nphi3-128k works better (if ctx <32k)\r\nbut mistral models are crazy, I tried 7bQ2 7bQ8, 70BQ2XS, none of them works anymore\r\n\r\n```\r\nLog start\r\nmain\r\nmain: build = 2961 (201cc11a)\r\nmain: built with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\r\nmain: seed  = 1716349593\r\nllama_model_loader\r\nllama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from models/mistral-7b-instruct-v0.2.Q2_K.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 10\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\r\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q2_K:   65 tensors\r\nllama_model_loader: - type q3_K:  160 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q2_K - Medium\r\nllm_load_print_meta: model params     = 7.24 B\r\nllm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) \r\nllm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nggml_cuda_init\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\r\nggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\r\nggml_cuda_init: found 1 CUDA devices:\r\nDevice 0\r\n  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\r\nllm_load_tensors\r\nllm_load_tensors: ggml ctx size =    0.30 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:        CPU buffer size =    41.02 MiB\r\nllm_load_tensors:      CUDA0 buffer size =  2898.55 MiB\r\n..................................................................................................\r\nllama_new_context_with_model\r\nllama_new_context_with_model: n_ctx      = 32000\r\nllama_new_context_with_model: n_batch    = 1024\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init\r\nllama_kv_cache_init:      CUDA0 KV buffer size =  4000.00 MiB\r\nllama_new_context_with_model\r\nllama_new_context_with_model: KV self size  = 4000.00 MiB, K (f16): 2000.00 MiB, V (f16): 2000.00 MiB\r\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =  2094.50 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =    70.51 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 2\r\n\r\nsystem_info\r\nsystem_info: n_threads = 32 / 32 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \r\nllama_tokenize_internal\r\nllama_tokenize_internal: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\r\nsampling: \r\n\trepeat_last_n = 256, repeat_penalty = 1.176, frequency_penalty = 0.000, presence_penalty = 0.000\r\n\ttop_k = 40, tfs_z = 1.000, top_p = 0.500, min_p = 0.050, typical_p = 1.000, temp = 0.500\r\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\r\nsampling order: \r\nCFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \r\ngenerate\r\ngenerate: n_ctx = 32000, n_batch = 1024, n_predict = 8192, n_keep = 1\r\n\r\n\r\n\r\nllama_print_timings\r\nllama_print_timings:        load time =     622.18 ms\r\nllama_print_timings:      sample time =     907.92 ms /  8192 runs   (    0.11 ms per token,  9022.83 tokens per second)\r\nllama_print_timings: prompt eval time =      37.07 ms /    25 tokens (    1.48 ms per token,   674.40 tokens per second)\r\nllama_print_timings:        eval time =   53408.73 ms /  8191 runs   (    6.52 ms per token,   153.36 tokens per second)\r\nllama_print_timings:       total time =   55902.60 ms /  8216 tokens\r\nLog end\r\n```\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-05-22T03:53:03+00:00",
    "closed_at": "2024-05-22T10:03:22+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7450/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 1,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7450"
  },
  {
    "number": 10396,
    "title": "Feature Request: [CANN] Use the RoPE operator provided by aclnn",
    "body": "### Prerequisites\r\n\r\n- [X] I am running the latest code. Mention the version if possible as well.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\r\n\r\n### Feature Description\r\n\r\nUse the RoPE operator provided by aclnn instead of manually coding RoPE to improve the performance of the RoPE operator.\r\n\r\n### Motivation\r\n\r\nThe RoPE performance of the aclnn operator library will be better!\r\n\r\n### Possible Implementation\r\n\r\nAdjust the RoPE operator provided by the operator library using aclnn.",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2024-11-19T02:16:30+00:00",
    "closed_at": "2024-11-28T07:59:17+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10396/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10396"
  },
  {
    "number": 3612,
    "title": "examples/server -n -1 never stops generating",
    "body": "I'm trying to get the same output from examples/server as from examples/main, but without any luck. Having -n -1 as param in for examples/server just makes it generate the whole ctx. While in the examples/main the generator stops when it has generated a good answer it seems. Does it have to do with stop_words? Any ideas are welcome, thanks!",
    "labels": [],
    "state": "closed",
    "created_at": "2023-10-13T07:36:35+00:00",
    "closed_at": "2023-10-16T19:29:55+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3612/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3612"
  },
  {
    "number": 4487,
    "title": "Please add llama.cui to the list of UI in Readme",
    "body": "Not an issue just a request. Could you please add Llama.cui to the list of UIs:\r\n\r\nhttps://github.com/dspasyuk/llama.cui\r\n\r\nThank you in advance!",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-15T19:59:13+00:00",
    "closed_at": "2024-04-02T01:10:55+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4487/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4487"
  },
  {
    "number": 5843,
    "title": "Save/Load Just One Sequence",
    "body": "# Feature Description\r\n\r\nWould it be possible to create functions that looked something like this:\r\n - `llama_kv_save_seq(struct llama_context * ctx, llama_seq_id seq_id, uint8_t * dst);`\r\n - `llama_kv_load_seq(struct llama_context * ctx, llama_seq_id seq_id, uint8_t * src);`\r\n\r\n# Motivation\r\n\r\nIn llama.cpp it is possible to save and load the _entire_ context state in one operation with `llama_copy_state_data` and `llama_set_state_data`. For example this could be used to evaluate a large system prompt once, save it to disk, and then load the state every time a new conversation is started.\r\n\r\nHowever with the batch decoding this isn't really possible. If you have many sequences being evaluated at once you can only load and save them _all_ simultaneously.\r\n",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2024-03-03T01:46:36+00:00",
    "closed_at": "2024-04-20T07:12:57+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5843/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5843"
  },
  {
    "number": 9242,
    "title": "Bug: Release build on Windows stuck",
    "body": "### What happened?\r\n\r\nWhen building llama.cpp in Release mode it stuck on build. with Debug config it compiles fast.\r\n\r\n### Name and Version\r\n\r\n1d1ccce67613674c75c9c7e3fa4c1e24e428ba48\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nWindows\r\n\r\n### Relevant log output\r\n\r\n\r\nBuild\r\n<details>\r\n\r\n```console\r\ncmake -B build . -DCMAKE_BUILD_TYPE=Release\r\n-- Building for: Visual Studio 17 2022\r\n-- Selecting Windows SDK version 10.0.22000.0 to target Windows 10.0.22631.\r\n-- The C compiler identification is MSVC 19.40.33812.0\r\n-- The CXX compiler identification is MSVC 19.40.33812.0\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working C compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.40.33807/bin/Hostx64/x64/cl.exe - skipped\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Check for working CXX compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.40.33807/bin/Hostx64/x64/cl.exe - skipped\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Found Git: C:/Program Files/Git/cmd/git.exe (found version \"2.45.2.windows.1\")\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\r\n-- Looking for pthread_create in pthreads\r\n-- Looking for pthread_create in pthreads - not found\r\n-- Looking for pthread_create in pthread\r\n-- Looking for pthread_create in pthread - not found\r\n-- Found Threads: TRUE\r\n-- Found OpenMP_C: -openmp (found version \"2.0\")\r\n-- Found OpenMP_CXX: -openmp (found version \"2.0\")\r\n-- Found OpenMP: TRUE (found version \"2.0\")\r\n-- OpenMP found\r\n-- Using llamafile\r\n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\r\n-- CMAKE_SYSTEM_PROCESSOR: AMD64\r\n-- CMAKE_GENERATOR_PLATFORM:\r\n-- x86 detected\r\n-- Performing Test HAS_AVX_1\r\n-- Performing Test HAS_AVX_1 - Success\r\n-- Performing Test HAS_AVX2_1\r\n-- Performing Test HAS_AVX2_1 - Success\r\n-- Performing Test HAS_FMA_1\r\n-- Performing Test HAS_FMA_1 - Success\r\n-- Performing Test HAS_AVX512_1\r\n-- Performing Test HAS_AVX512_1 - Failed\r\n-- Performing Test HAS_AVX512_2\r\n-- Performing Test HAS_AVX512_2 - Failed\r\n-- Configuring done (11.7s)\r\n-- Generating done (0.9s)\r\n-- Build files have been written to: D:/llama/llama.cpp/build\r\n```\r\n\r\n```console\r\ncmake --build build --config Release\r\nMSBuild version 17.10.4+10fbfbf2e for .NET Framework\r\n\r\n  build_info.vcxproj -> D:\\llama\\llama.cpp\\build\\common\\build_info.dir\\Release\\build_info.lib\r\n  Auto build dll exports\r\n  ggml.vcxproj -> D:\\llama\\llama.cpp\\build\\bin\\Release\\ggml.dll\r\n  llama.cpp\r\n  llama-vocab.cpp\r\nD:\\llama\\llama.cpp\\src\\llama-vocab.cpp(138,26): warning C4244: 'return': conversion from 'long' to 'uint8_t', possible loss of data\r\n[D:\\llama\\llama.cpp\\build\\src\\llama.vcxproj]\r\nD:\\llama\\llama.cpp\\src\\llama-vocab.cpp(211,35): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n[D:\\llama\\llama.cpp\\build\\src\\llama.vcxproj]\r\nD:\\llama\\llama.cpp\\src\\llama-vocab.cpp(211,30): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n[D:\\llama\\llama.cpp\\build\\src\\llama.vcxproj]\r\nD:\\llama\\llama.cpp\\src\\llama-vocab.cpp(533,39): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n[D:\\llama\\llama.cpp\\build\\src\\llama.vcxproj]\r\nD:\\llama\\llama.cpp\\src\\llama-vocab.cpp(533,34): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n[D:\\llama\\llama.cpp\\build\\src\\llama.vcxproj]\r\nD:\\llama\\llama.cpp\\src\\llama-vocab.cpp(572,82): warning C4267: '=': conversion from 'size_t' to 'llm_symbol::index', possible loss o\r\nf data [D:\\llama\\llama.cpp\\build\\src\\llama.vcxproj]\r\nD:\\llama\\llama.cpp\\src\\llama-vocab.cpp(575,61): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data [D:\\lla\r\nma\\llama.cpp\\build\\src\\llama.vcxproj]\r\nD:\\llama\\llama.cpp\\src\\llama-vocab.cpp(669,37): warning C4267: 'initializing': conversion from 'size_t' to 'int', possible loss of d\r\nata [D:\\llama\\llama.cpp\\build\\src\\llama.vcxproj]\r\nD:\\llama\\llama.cpp\\src\\llama-vocab.cpp(669,25): warning C4267: 'initializing': conversion from 'size_t' to 'const int', possible los\r\ns of data [D:\\llama\\llama.cpp\\build\\src\\llama.vcxproj]\r\nD:\\llama\\llama.cpp\\src\\llama-vocab.cpp(1532,20): warning C4267: 'return': conversion from 'size_t' to 'int32_t', possible loss of da\r\nta [D:\\llama\\llama.cpp\\build\\src\\llama.vcxproj]\r\n  llama-grammar.cpp\r\n  llama-sampling.cpp\r\nD:\\llama\\llama.cpp\\src\\llama-sampling.cpp(26,20): warning C4244: '=': conversion from 'time_t' to 'uint32_t', possible loss of data\r\n[D:\\llama\\llama.cpp\\build\\src\\llama.vcxproj]\r\nD:\\llama\\llama.cpp\\src\\llama-sampling.cpp(70,23): warning C4267: '=': conversion from 'size_t' to 'int32_t', possible loss of data [\r\nD:\\llama\\llama.cpp\\build\\src\\llama.vcxproj]\r\nD:\\llama\\llama.cpp\\src\\llama-sampling.cpp(405,33): warning C4244: '=': conversion from 'double' to 'float', possible loss of data [D\r\n:\\llama\\llama.cpp\\build\\src\\llama.vcxproj]\r\nD:\\llama\\llama.cpp\\src\\llama-sampling.cpp(409,34): warning C4244: '/=': conversion from 'double' to 'float', possible loss of data [\r\nD:\\llama\\llama.cpp\\build\\src\\llama.vcxproj]\r\nD:\\llama\\llama.cpp\\src\\llama-sampling.cpp(510,34): warning C4244: 'initializing': conversion from 'float' to 'int32_t', possible los\r\ns of data [D:\\llama\\llama.cpp\\build\\src\\llama.vcxproj]\r\nD:\\llama\\llama.cpp\\src\\llama-sampling.cpp(510,27): warning C4244: 'initializing': conversion from 'float' to 'const int32_t', possib\r\nle loss of data [D:\\llama\\llama.cpp\\build\\src\\llama.vcxproj]\r\nD:\\llama\\llama.cpp\\src\\llama-sampling.cpp(530,61): warning C4244: 'argument': conversion from 'const int32_t' to 'float', possible l\r\noss of data [D:\\llama\\llama.cpp\\build\\src\\llama.vcxproj]\r\n  unicode.cpp\r\n  unicode-data.cpp\r\n  Generating Code...\r\n  Forever...\r\n```\r\n</details>\r\n\r\n*Update*\r\n\r\nUsing the following:\r\n```console\r\ncmake -B build . -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=\"/Od\"\r\ncmake --build build --config Release --target llama-cli\r\n``` \r\nfixed the issue.\r\nIt compiled in `116` seconds.",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-08-29T16:26:53+00:00",
    "closed_at": "2024-10-13T01:07:32+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9242/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9242"
  },
  {
    "number": 12826,
    "title": "Compile bug: ggml-cuda/opt-step-adamw.cu error: identifier \"__Poly8x8_t\" is undefined on Jetson Orin AGX",
    "body": "### Git commit\n\n1466621e738779eefe1bb672e17dc55d63d166bb\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Problem description & steps to reproduce\n\nI am trying to build llama.cpp with a CUDA backend on the NVIDIA AGX Orin, but it seems there are too many compilation errors that occur when including `arm_neon.h`.\n\nMy environment:\n1. JetPack: 6.2\n2. GCC: 11.4.0\n3. CUDA: 12.6.68\n4. Git: 2.34.1\n5. Arch: aarch64\n\nHow to reproduce:\n\n1. `git clone https://github.com/ggml-org/llama.cpp -b b5074`\n2. `cmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc -DLLAMA_CURL=OFF`\n3. `cmake --build build --parallel`\n4. Get the errors:\n\n```\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(44): error: identifier \"__Poly8x8_t\" is undefined\n  typedef __Poly8x8_t poly8x8_t;\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1195): error: identifier \"__builtin_aarch64_addhnv4si\" is undefined\n    return (int16x4_t) __builtin_aarch64_addhnv4si (__a, __b);\n```\n\n### First Bad Commit\n\nff067dbcb9e6ca4ed464d3db999ff8e9c503498b\n\n### Compile command\n\n```shell\ncmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc -DLLAMA_CURL=OFF\ncmake --build build --parallel\n```\n\n### Relevant log output\n\n```shell\n$ cmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc -DLLAMA_CURL=OFF\n-- The C compiler identification is GNU 11.4.0\n-- The CXX compiler identification is GNU 11.4.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/cc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found Git: /usr/bin/git (found version \"2.34.1\")\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n-- Found Threads: TRUE\n-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.\n-- CMAKE_SYSTEM_PROCESSOR: aarch64\n-- Including CPU backend\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n-- Found OpenMP: TRUE (found version \"4.5\")\n-- ARM detected\n-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E\n-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed\n-- Performing Test GGML_MACHINE_SUPPORTS_dotprod\n-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success\n-- Performing Test GGML_MACHINE_SUPPORTS_i8mm\n-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Failed\n-- Performing Test GGML_MACHINE_SUPPORTS_noi8mm\n-- Performing Test GGML_MACHINE_SUPPORTS_noi8mm - Success\n-- Performing Test GGML_MACHINE_SUPPORTS_sve\n-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed\n-- Performing Test GGML_MACHINE_SUPPORTS_nosve\n-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success\n-- Performing Test GGML_MACHINE_SUPPORTS_sme\n-- Performing Test GGML_MACHINE_SUPPORTS_sme - Failed\n-- Performing Test GGML_MACHINE_SUPPORTS_nosme\n-- Performing Test GGML_MACHINE_SUPPORTS_nosme - Failed\n-- ARM feature DOTPROD enabled\n-- ARM feature FMA enabled\n-- ARM feature FP16_VECTOR_ARITHMETIC enabled\n-- Adding CPU backend variant ggml-cpu: -mcpu=cortex-a78ae+crypto+flagm+pauth+noprofile+nossbs+dotprod+noi8mm+nosve\n-- Found CUDAToolkit: /usr/local/cuda/include (found version \"12.6.68\")\n-- CUDA Toolkit found\n-- Using CUDA architectures: 50;61;70;75;80\n-- The CUDA compiler identification is NVIDIA 12.6.68\n-- Detecting CUDA compiler ABI info\n-- Detecting CUDA compiler ABI info - done\n-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n-- Detecting CUDA compile features\n-- Detecting CUDA compile features - done\n-- CUDA host compiler is GNU 11.4.0\n\n-- Including CUDA backend\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/hydai/workspace/llama.cpp/build\n\u279c  llama.cpp git:(b5074) cmake --build build --parallel\n[  0%] Generating build details from Git\n[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\n[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\n[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\n[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\n[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\n[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\n-- Found Git: /usr/bin/git (found version \"2.34.1\")\n[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\n[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\n[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\n[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\n[  5%] Built target sha1\n[  5%] Built target sha256\n[  5%] Built target xxhash\n[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\n[  5%] Built target build_info\n[  5%] Linking CXX shared library ../../bin/libggml-base.so\n[  5%] Built target ggml-base\n[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o\n[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o\n[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o\n[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\n[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o\n[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\n[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\n[  7%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o\n[  7%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o\n[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\n[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\n[  9%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o\n[  9%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o\n[  9%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o\n[  9%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o\n[ 10%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o\n[ 10%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o\n[ 11%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o\n[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\n[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\n[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\n[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\n[ 12%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o\n[ 12%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o\n[ 12%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o\n[ 13%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o\n[ 13%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o\n[ 14%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o\n[ 15%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o\n[ 14%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o\n[ 14%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o\n[ 15%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o\n[ 15%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o\n[ 16%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o\n[ 16%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o\n[ 16%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmv.cu.o\n[ 17%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o\n[ 17%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o\n[ 18%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o\n[ 18%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o\n[ 18%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o\n[ 19%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o\n[ 19%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o\n[ 19%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o\n[ 20%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o\n[ 19%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o\n[ 19%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o\n[ 21%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o\n[ 21%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o\n[ 21%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o\n[ 21%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o\n[ 21%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o\n[ 22%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o\n[ 23%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o\n[ 23%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o\n[ 23%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o\n[ 23%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o\n[ 24%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o\n[ 24%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o\n[ 24%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o\n[ 25%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o\n[ 25%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o\n[ 25%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o\n[ 26%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o\n[ 26%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o\n[ 27%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o\n[ 27%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o\n[ 27%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o\n[ 28%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o\n[ 28%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o\n[ 28%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o\n[ 28%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o\n[ 29%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o\n[ 29%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o\n[ 30%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o\n[ 30%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o\n[ 30%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o\n[ 30%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o\n[ 31%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o\n[ 31%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o\n[ 31%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o\n[ 32%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o\n[ 32%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o\n[ 32%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o\n[ 33%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o\n[ 33%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o\n[ 33%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o\n[ 34%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o\n[ 34%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o\n[ 34%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o\n[ 35%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o\n[ 35%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o\n[ 35%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o\n[ 36%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o\n[ 36%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o\n[ 37%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o\n[ 37%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o\n[ 37%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(44): error: identifier \"__Poly8x8_t\" is undefined\n  typedef __Poly8x8_t poly8x8_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(45): error: identifier \"__Poly16x4_t\" is undefined\n  typedef __Poly16x4_t poly16x4_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(58): error: identifier \"__Poly8x16_t\" is undefined\n  typedef __Poly8x16_t poly8x16_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(59): error: identifier \"__Poly16x8_t\" is undefined\n  typedef __Poly16x8_t poly16x8_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(60): error: identifier \"__Poly64x2_t\" is undefined\n  typedef __Poly64x2_t poly64x2_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(61): error: identifier \"__Poly64x1_t\" is undefined\n  typedef __Poly64x1_t poly64x1_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(67): error: identifier \"__Poly8_t\" is undefined\n  typedef __Poly8_t poly8_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(68): error: identifier \"__Poly16_t\" is undefined\n  typedef __Poly16_t poly16_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(69): error: identifier \"__Poly64_t\" is undefined\n  typedef __Poly64_t poly64_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(70): error: identifier \"__Poly128_t\" is undefined\n  typedef __Poly128_t poly128_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(828): error: identifier \"__builtin_aarch64_saddlv8qi\" is undefined\n    return (int16x8_t) __builtin_aarch64_saddlv8qi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(835): error: identifier \"__builtin_aarch64_saddlv4hi\" is undefined\n    return (int32x4_t) __builtin_aarch64_saddlv4hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(842): error: identifier \"__builtin_aarch64_saddlv2si\" is undefined\n    return (int64x2_t) __builtin_aarch64_saddlv2si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(849): error: identifier \"__builtin_aarch64_uaddlv8qi\" is undefined\n    return (uint16x8_t) __builtin_aarch64_uaddlv8qi ((int8x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(857): error: identifier \"__builtin_aarch64_uaddlv4hi\" is undefined\n    return (uint32x4_t) __builtin_aarch64_uaddlv4hi ((int16x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(865): error: identifier \"__builtin_aarch64_uaddlv2si\" is undefined\n    return (uint64x2_t) __builtin_aarch64_uaddlv2si ((int32x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(873): error: identifier \"__builtin_aarch64_saddl2v16qi\" is undefined\n    return (int16x8_t) __builtin_aarch64_saddl2v16qi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(880): error: identifier \"__builtin_aarch64_saddl2v8hi\" is undefined\n    return (int32x4_t) __builtin_aarch64_saddl2v8hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(887): error: identifier \"__builtin_aarch64_saddl2v4si\" is undefined\n    return (int64x2_t) __builtin_aarch64_saddl2v4si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(894): error: identifier \"__builtin_aarch64_uaddl2v16qi\" is undefined\n    return (uint16x8_t) __builtin_aarch64_uaddl2v16qi ((int8x16_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(902): error: identifier \"__builtin_aarch64_uaddl2v8hi\" is undefined\n    return (uint32x4_t) __builtin_aarch64_uaddl2v8hi ((int16x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(910): error: identifier \"__builtin_aarch64_uaddl2v4si\" is undefined\n    return (uint64x2_t) __builtin_aarch64_uaddl2v4si ((int32x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(918): error: identifier \"__builtin_aarch64_saddwv8qi\" is undefined\n    return (int16x8_t) __builtin_aarch64_saddwv8qi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(925): error: identifier \"__builtin_aarch64_saddwv4hi\" is undefined\n    return (int32x4_t) __builtin_aarch64_saddwv4hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(932): error: identifier \"__builtin_aarch64_saddwv2si\" is undefined\n    return (int64x2_t) __builtin_aarch64_saddwv2si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(939): error: identifier \"__builtin_aarch64_uaddwv8qi\" is undefined\n    return (uint16x8_t) __builtin_aarch64_uaddwv8qi ((int16x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(947): error: identifier \"__builtin_aarch64_uaddwv4hi\" is undefined\n    return (uint32x4_t) __builtin_aarch64_uaddwv4hi ((int32x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(955): error: identifier \"__builtin_aarch64_uaddwv2si\" is undefined\n    return (uint64x2_t) __builtin_aarch64_uaddwv2si ((int64x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(963): error: identifier \"__builtin_aarch64_saddw2v16qi\" is undefined\n    return (int16x8_t) __builtin_aarch64_saddw2v16qi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(970): error: identifier \"__builtin_aarch64_saddw2v8hi\" is undefined\n    return (int32x4_t) __builtin_aarch64_saddw2v8hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(977): error: identifier \"__builtin_aarch64_saddw2v4si\" is undefined\n    return (int64x2_t) __builtin_aarch64_saddw2v4si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(984): error: identifier \"__builtin_aarch64_uaddw2v16qi\" is undefined\n    return (uint16x8_t) __builtin_aarch64_uaddw2v16qi ((int16x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(992): error: identifier \"__builtin_aarch64_uaddw2v8hi\" is undefined\n    return (uint32x4_t) __builtin_aarch64_uaddw2v8hi ((int32x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1000): error: identifier \"__builtin_aarch64_uaddw2v4si\" is undefined\n    return (uint64x2_t) __builtin_aarch64_uaddw2v4si ((int64x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1008): error: identifier \"__builtin_aarch64_shaddv8qi\" is undefined\n    return (int8x8_t) __builtin_aarch64_shaddv8qi (__a, __b);\n                      ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1015): error: identifier \"__builtin_aarch64_shaddv4hi\" is undefined\n    return (int16x4_t) __builtin_aarch64_shaddv4hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1022): error: identifier \"__builtin_aarch64_shaddv2si\" is undefined\n    return (int32x2_t) __builtin_aarch64_shaddv2si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1029): error: identifier \"__builtin_aarch64_uhaddv8qi\" is undefined\n    return (uint8x8_t) __builtin_aarch64_uhaddv8qi ((int8x8_t) __a,\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1037): error: identifier \"__builtin_aarch64_uhaddv4hi\" is undefined\n    return (uint16x4_t) __builtin_aarch64_uhaddv4hi ((int16x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1045): error: identifier \"__builtin_aarch64_uhaddv2si\" is undefined\n    return (uint32x2_t) __builtin_aarch64_uhaddv2si ((int32x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1053): error: identifier \"__builtin_aarch64_shaddv16qi\" is undefined\n    return (int8x16_t) __builtin_aarch64_shaddv16qi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1060): error: identifier \"__builtin_aarch64_shaddv8hi\" is undefined\n    return (int16x8_t) __builtin_aarch64_shaddv8hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1067): error: identifier \"__builtin_aarch64_shaddv4si\" is undefined\n    return (int32x4_t) __builtin_aarch64_shaddv4si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1074): error: identifier \"__builtin_aarch64_uhaddv16qi\" is undefined\n    return (uint8x16_t) __builtin_aarch64_uhaddv16qi ((int8x16_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1082): error: identifier \"__builtin_aarch64_uhaddv8hi\" is undefined\n    return (uint16x8_t) __builtin_aarch64_uhaddv8hi ((int16x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1090): error: identifier \"__builtin_aarch64_uhaddv4si\" is undefined\n    return (uint32x4_t) __builtin_aarch64_uhaddv4si ((int32x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1098): error: identifier \"__builtin_aarch64_srhaddv8qi\" is undefined\n    return (int8x8_t) __builtin_aarch64_srhaddv8qi (__a, __b);\n                      ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1105): error: identifier \"__builtin_aarch64_srhaddv4hi\" is undefined\n    return (int16x4_t) __builtin_aarch64_srhaddv4hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1112): error: identifier \"__builtin_aarch64_srhaddv2si\" is undefined\n    return (int32x2_t) __builtin_aarch64_srhaddv2si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1119): error: identifier \"__builtin_aarch64_urhaddv8qi\" is undefined\n    return (uint8x8_t) __builtin_aarch64_urhaddv8qi ((int8x8_t) __a,\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1127): error: identifier \"__builtin_aarch64_urhaddv4hi\" is undefined\n    return (uint16x4_t) __builtin_aarch64_urhaddv4hi ((int16x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1135): error: identifier \"__builtin_aarch64_urhaddv2si\" is undefined\n    return (uint32x2_t) __builtin_aarch64_urhaddv2si ((int32x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1143): error: identifier \"__builtin_aarch64_srhaddv16qi\" is undefined\n    return (int8x16_t) __builtin_aarch64_srhaddv16qi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1150): error: identifier \"__builtin_aarch64_srhaddv8hi\" is undefined\n    return (int16x8_t) __builtin_aarch64_srhaddv8hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1157): error: identifier \"__builtin_aarch64_srhaddv4si\" is undefined\n    return (int32x4_t) __builtin_aarch64_srhaddv4si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1164): error: identifier \"__builtin_aarch64_urhaddv16qi\" is undefined\n    return (uint8x16_t) __builtin_aarch64_urhaddv16qi ((int8x16_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1172): error: identifier \"__builtin_aarch64_urhaddv8hi\" is undefined\n    return (uint16x8_t) __builtin_aarch64_urhaddv8hi ((int16x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1180): error: identifier \"__builtin_aarch64_urhaddv4si\" is undefined\n    return (uint32x4_t) __builtin_aarch64_urhaddv4si ((int32x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1188): error: identifier \"__builtin_aarch64_addhnv8hi\" is undefined\n    return (int8x8_t) __builtin_aarch64_addhnv8hi (__a, __b);\n                      ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1195): error: identifier \"__builtin_aarch64_addhnv4si\" is undefined\n    return (int16x4_t) __builtin_aarch64_addhnv4si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1202): error: identifier \"__builtin_aarch64_addhnv2di\" is undefined\n    return (int32x2_t) __builtin_aarch64_addhnv2di (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1209): error: identifier \"__builtin_aarch64_addhnv8hi\" is undefined\n    return (uint8x8_t) __builtin_aarch64_addhnv8hi ((int16x8_t) __a,\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1217): error: identifier \"__builtin_aarch64_addhnv4si\" is undefined\n    return (uint16x4_t) __builtin_aarch64_addhnv4si ((int32x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1225): error: identifier \"__builtin_aarch64_addhnv2di\" is undefined\n    return (uint32x2_t) __builtin_aarch64_addhnv2di ((int64x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1233): error: identifier \"__builtin_aarch64_raddhnv8hi\" is undefined\n    return (int8x8_t) __builtin_aarch64_raddhnv8hi (__a, __b);\n                      ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1240): error: identifier \"__builtin_aarch64_raddhnv4si\" is undefined\n    return (int16x4_t) __builtin_aarch64_raddhnv4si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1247): error: identifier \"__builtin_aarch64_raddhnv2di\" is undefined\n    return (int32x2_t) __builtin_aarch64_raddhnv2di (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1254): error: identifier \"__builtin_aarch64_raddhnv8hi\" is undefined\n    return (uint8x8_t) __builtin_aarch64_raddhnv8hi ((int16x8_t) __a,\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1262): error: identifier \"__builtin_aarch64_raddhnv4si\" is undefined\n    return (uint16x4_t) __builtin_aarch64_raddhnv4si ((int32x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1270): error: identifier \"__builtin_aarch64_raddhnv2di\" is undefined\n    return (uint32x2_t) __builtin_aarch64_raddhnv2di ((int64x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1278): error: identifier \"__builtin_aarch64_addhn2v8hi\" is undefined\n    return (int8x16_t) __builtin_aarch64_addhn2v8hi (__a, __b, __c);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1285): error: identifier \"__builtin_aarch64_addhn2v4si\" is undefined\n    return (int16x8_t) __builtin_aarch64_addhn2v4si (__a, __b, __c);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1292): error: identifier \"__builtin_aarch64_addhn2v2di\" is undefined\n    return (int32x4_t) __builtin_aarch64_addhn2v2di (__a, __b, __c);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1299): error: identifier \"__builtin_aarch64_addhn2v8hi\" is undefined\n    return (uint8x16_t) __builtin_aarch64_addhn2v8hi ((int8x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1308): error: identifier \"__builtin_aarch64_addhn2v4si\" is undefined\n    return (uint16x8_t) __builtin_aarch64_addhn2v4si ((int16x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1317): error: identifier \"__builtin_aarch64_addhn2v2di\" is undefined\n    return (uint32x4_t) __builtin_aarch64_addhn2v2di ((int32x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1326): error: identifier \"__builtin_aarch64_raddhn2v8hi\" is undefined\n    return (int8x16_t) __builtin_aarch64_raddhn2v8hi (__a, __b, __c);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1333): error: identifier \"__builtin_aarch64_raddhn2v4si\" is undefined\n    return (int16x8_t) __builtin_aarch64_raddhn2v4si (__a, __b, __c);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1340): error: identifier \"__builtin_aarch64_raddhn2v2di\" is undefined\n    return (int32x4_t) __builtin_aarch64_raddhn2v2di (__a, __b, __c);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1347): error: identifier \"__builtin_aarch64_raddhn2v8hi\" is undefined\n    return (uint8x16_t) __builtin_aarch64_raddhn2v8hi ((int8x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1356): error: identifier \"__builtin_aarch64_raddhn2v4si\" is undefined\n    return (uint16x8_t) __builtin_aarch64_raddhn2v4si ((int16x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1365): error: identifier \"__builtin_aarch64_raddhn2v2di\" is undefined\n    return (uint32x4_t) __builtin_aarch64_raddhn2v2di ((int32x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1458): error: identifier \"__builtin_aarch64_pmulv8qi\" is undefined\n    return (poly8x8_t) __builtin_aarch64_pmulv8qi ((int8x8_t) __a,\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1522): error: identifier \"__builtin_aarch64_pmulv16qi\" is undefined\n    return (poly8x16_t) __builtin_aarch64_pmulv16qi ((int8x16_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2230): error: identifier \"__builtin_aarch64_ssublv8qi\" is undefined\n    return (int16x8_t) __builtin_aarch64_ssublv8qi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2237): error: identifier \"__builtin_aarch64_ssublv4hi\" is undefined\n    return (int32x4_t) __builtin_aarch64_ssublv4hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2244): error: identifier \"__builtin_aarch64_ssublv2si\" is undefined\n    return (int64x2_t) __builtin_aarch64_ssublv2si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2251): error: identifier \"__builtin_aarch64_usublv8qi\" is undefined\n    return (uint16x8_t) __builtin_aarch64_usublv8qi ((int8x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2259): error: identifier \"__builtin_aarch64_usublv4hi\" is undefined\n    return (uint32x4_t) __builtin_aarch64_usublv4hi ((int16x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2267): error: identifier \"__builtin_aarch64_usublv2si\" is undefined\n    return (uint64x2_t) __builtin_aarch64_usublv2si ((int32x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2275): error: identifier \"__builtin_aarch64_ssubl2v16qi\" is undefined\n    return (int16x8_t) __builtin_aarch64_ssubl2v16qi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2282): error: identifier \"__builtin_aarch64_ssubl2v8hi\" is undefined\n    return (int32x4_t) __builtin_aarch64_ssubl2v8hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2289): error: identifier \"__builtin_aarch64_ssubl2v4si\" is undefined\n    return (int64x2_t) __builtin_aarch64_ssubl2v4si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2296): error: identifier \"__builtin_aarch64_usubl2v16qi\" is undefined\n    return (uint16x8_t) __builtin_aarch64_usubl2v16qi ((int8x16_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2304): error: identifier \"__builtin_aarch64_usubl2v8hi\" is undefined\n    return (uint32x4_t) __builtin_aarch64_usubl2v8hi ((int16x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2312): error: identifier \"__builtin_aarch64_usubl2v4si\" is undefined\n    return (uint64x2_t) __builtin_aarch64_usubl2v4si ((int32x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2320): error: identifier \"__builtin_aarch64_ssubwv8qi\" is undefined\n    return (int16x8_t) __builtin_aarch64_ssubwv8qi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2327): error: identifier \"__builtin_aarch64_ssubwv4hi\" is undefined\n    return (int32x4_t) __builtin_aarch64_ssubwv4hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2334): error: identifier \"__builtin_aarch64_ssubwv2si\" is undefined\n    return (int64x2_t) __builtin_aarch64_ssubwv2si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2341): error: identifier \"__builtin_aarch64_usubwv8qi\" is undefined\n    return (uint16x8_t) __builtin_aarch64_usubwv8qi ((int16x8_t) __a,\n                        ^\n\nError limit reached.\n100 errors detected in the compilation of \"/home/hydai/workspace/llama.cpp/ggml/src/ggml-cuda/opt-step-adamw.cu\".\nCompilation terminated.\ngmake[2]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:426: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o] Error 4\ngmake[2]: *** Waiting for unfinished jobs....\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(44): error: identifier \"__Poly8x8_t\" is undefined\n  typedef __Poly8x8_t poly8x8_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(45): error: identifier \"__Poly16x4_t\" is undefined\n  typedef __Poly16x4_t poly16x4_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(58): error: identifier \"__Poly8x16_t\" is undefined\n  typedef __Poly8x16_t poly8x16_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(59): error: identifier \"__Poly16x8_t\" is undefined\n  typedef __Poly16x8_t poly16x8_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(60): error: identifier \"__Poly64x2_t\" is undefined\n  typedef __Poly64x2_t poly64x2_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(61): error: identifier \"__Poly64x1_t\" is undefined\n  typedef __Poly64x1_t poly64x1_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(67): error: identifier \"__Poly8_t\" is undefined\n  typedef __Poly8_t poly8_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(68): error: identifier \"__Poly16_t\" is undefined\n  typedef __Poly16_t poly16_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(69): error: identifier \"__Poly64_t\" is undefined\n  typedef __Poly64_t poly64_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(70): error: identifier \"__Poly128_t\" is undefined\n  typedef __Poly128_t poly128_t;\n          ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(828): error: identifier \"__builtin_aarch64_saddlv8qi\" is undefined\n    return (int16x8_t) __builtin_aarch64_saddlv8qi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(835): error: identifier \"__builtin_aarch64_saddlv4hi\" is undefined\n    return (int32x4_t) __builtin_aarch64_saddlv4hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(842): error: identifier \"__builtin_aarch64_saddlv2si\" is undefined\n    return (int64x2_t) __builtin_aarch64_saddlv2si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(849): error: identifier \"__builtin_aarch64_uaddlv8qi\" is undefined\n    return (uint16x8_t) __builtin_aarch64_uaddlv8qi ((int8x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(857): error: identifier \"__builtin_aarch64_uaddlv4hi\" is undefined\n    return (uint32x4_t) __builtin_aarch64_uaddlv4hi ((int16x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(865): error: identifier \"__builtin_aarch64_uaddlv2si\" is undefined\n    return (uint64x2_t) __builtin_aarch64_uaddlv2si ((int32x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(873): error: identifier \"__builtin_aarch64_saddl2v16qi\" is undefined\n    return (int16x8_t) __builtin_aarch64_saddl2v16qi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(880): error: identifier \"__builtin_aarch64_saddl2v8hi\" is undefined\n    return (int32x4_t) __builtin_aarch64_saddl2v8hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(887): error: identifier \"__builtin_aarch64_saddl2v4si\" is undefined\n    return (int64x2_t) __builtin_aarch64_saddl2v4si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(894): error: identifier \"__builtin_aarch64_uaddl2v16qi\" is undefined\n    return (uint16x8_t) __builtin_aarch64_uaddl2v16qi ((int8x16_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(902): error: identifier \"__builtin_aarch64_uaddl2v8hi\" is undefined\n    return (uint32x4_t) __builtin_aarch64_uaddl2v8hi ((int16x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(910): error: identifier \"__builtin_aarch64_uaddl2v4si\" is undefined\n    return (uint64x2_t) __builtin_aarch64_uaddl2v4si ((int32x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(918): error: identifier \"__builtin_aarch64_saddwv8qi\" is undefined\n    return (int16x8_t) __builtin_aarch64_saddwv8qi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(925): error: identifier \"__builtin_aarch64_saddwv4hi\" is undefined\n    return (int32x4_t) __builtin_aarch64_saddwv4hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(932): error: identifier \"__builtin_aarch64_saddwv2si\" is undefined\n    return (int64x2_t) __builtin_aarch64_saddwv2si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(939): error: identifier \"__builtin_aarch64_uaddwv8qi\" is undefined\n    return (uint16x8_t) __builtin_aarch64_uaddwv8qi ((int16x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(947): error: identifier \"__builtin_aarch64_uaddwv4hi\" is undefined\n    return (uint32x4_t) __builtin_aarch64_uaddwv4hi ((int32x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(955): error: identifier \"__builtin_aarch64_uaddwv2si\" is undefined\n    return (uint64x2_t) __builtin_aarch64_uaddwv2si ((int64x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(963): error: identifier \"__builtin_aarch64_saddw2v16qi\" is undefined\n    return (int16x8_t) __builtin_aarch64_saddw2v16qi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(970): error: identifier \"__builtin_aarch64_saddw2v8hi\" is undefined\n    return (int32x4_t) __builtin_aarch64_saddw2v8hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(977): error: identifier \"__builtin_aarch64_saddw2v4si\" is undefined\n    return (int64x2_t) __builtin_aarch64_saddw2v4si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(984): error: identifier \"__builtin_aarch64_uaddw2v16qi\" is undefined\n    return (uint16x8_t) __builtin_aarch64_uaddw2v16qi ((int16x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(992): error: identifier \"__builtin_aarch64_uaddw2v8hi\" is undefined\n    return (uint32x4_t) __builtin_aarch64_uaddw2v8hi ((int32x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1000): error: identifier \"__builtin_aarch64_uaddw2v4si\" is undefined\n    return (uint64x2_t) __builtin_aarch64_uaddw2v4si ((int64x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1008): error: identifier \"__builtin_aarch64_shaddv8qi\" is undefined\n    return (int8x8_t) __builtin_aarch64_shaddv8qi (__a, __b);\n                      ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1015): error: identifier \"__builtin_aarch64_shaddv4hi\" is undefined\n    return (int16x4_t) __builtin_aarch64_shaddv4hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1022): error: identifier \"__builtin_aarch64_shaddv2si\" is undefined\n    return (int32x2_t) __builtin_aarch64_shaddv2si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1029): error: identifier \"__builtin_aarch64_uhaddv8qi\" is undefined\n    return (uint8x8_t) __builtin_aarch64_uhaddv8qi ((int8x8_t) __a,\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1037): error: identifier \"__builtin_aarch64_uhaddv4hi\" is undefined\n    return (uint16x4_t) __builtin_aarch64_uhaddv4hi ((int16x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1045): error: identifier \"__builtin_aarch64_uhaddv2si\" is undefined\n    return (uint32x2_t) __builtin_aarch64_uhaddv2si ((int32x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1053): error: identifier \"__builtin_aarch64_shaddv16qi\" is undefined\n    return (int8x16_t) __builtin_aarch64_shaddv16qi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1060): error: identifier \"__builtin_aarch64_shaddv8hi\" is undefined\n    return (int16x8_t) __builtin_aarch64_shaddv8hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1067): error: identifier \"__builtin_aarch64_shaddv4si\" is undefined\n    return (int32x4_t) __builtin_aarch64_shaddv4si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1074): error: identifier \"__builtin_aarch64_uhaddv16qi\" is undefined\n    return (uint8x16_t) __builtin_aarch64_uhaddv16qi ((int8x16_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1082): error: identifier \"__builtin_aarch64_uhaddv8hi\" is undefined\n    return (uint16x8_t) __builtin_aarch64_uhaddv8hi ((int16x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1090): error: identifier \"__builtin_aarch64_uhaddv4si\" is undefined\n    return (uint32x4_t) __builtin_aarch64_uhaddv4si ((int32x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1098): error: identifier \"__builtin_aarch64_srhaddv8qi\" is undefined\n    return (int8x8_t) __builtin_aarch64_srhaddv8qi (__a, __b);\n                      ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1105): error: identifier \"__builtin_aarch64_srhaddv4hi\" is undefined\n    return (int16x4_t) __builtin_aarch64_srhaddv4hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1112): error: identifier \"__builtin_aarch64_srhaddv2si\" is undefined\n    return (int32x2_t) __builtin_aarch64_srhaddv2si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1119): error: identifier \"__builtin_aarch64_urhaddv8qi\" is undefined\n    return (uint8x8_t) __builtin_aarch64_urhaddv8qi ((int8x8_t) __a,\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1127): error: identifier \"__builtin_aarch64_urhaddv4hi\" is undefined\n    return (uint16x4_t) __builtin_aarch64_urhaddv4hi ((int16x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1135): error: identifier \"__builtin_aarch64_urhaddv2si\" is undefined\n    return (uint32x2_t) __builtin_aarch64_urhaddv2si ((int32x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1143): error: identifier \"__builtin_aarch64_srhaddv16qi\" is undefined\n    return (int8x16_t) __builtin_aarch64_srhaddv16qi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1150): error: identifier \"__builtin_aarch64_srhaddv8hi\" is undefined\n    return (int16x8_t) __builtin_aarch64_srhaddv8hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1157): error: identifier \"__builtin_aarch64_srhaddv4si\" is undefined\n    return (int32x4_t) __builtin_aarch64_srhaddv4si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1164): error: identifier \"__builtin_aarch64_urhaddv16qi\" is undefined\n    return (uint8x16_t) __builtin_aarch64_urhaddv16qi ((int8x16_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1172): error: identifier \"__builtin_aarch64_urhaddv8hi\" is undefined\n    return (uint16x8_t) __builtin_aarch64_urhaddv8hi ((int16x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1180): error: identifier \"__builtin_aarch64_urhaddv4si\" is undefined\n    return (uint32x4_t) __builtin_aarch64_urhaddv4si ((int32x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1188): error: identifier \"__builtin_aarch64_addhnv8hi\" is undefined\n    return (int8x8_t) __builtin_aarch64_addhnv8hi (__a, __b);\n                      ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1195): error: identifier \"__builtin_aarch64_addhnv4si\" is undefined\n    return (int16x4_t) __builtin_aarch64_addhnv4si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1202): error: identifier \"__builtin_aarch64_addhnv2di\" is undefined\n    return (int32x2_t) __builtin_aarch64_addhnv2di (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1209): error: identifier \"__builtin_aarch64_addhnv8hi\" is undefined\n    return (uint8x8_t) __builtin_aarch64_addhnv8hi ((int16x8_t) __a,\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1217): error: identifier \"__builtin_aarch64_addhnv4si\" is undefined\n    return (uint16x4_t) __builtin_aarch64_addhnv4si ((int32x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1225): error: identifier \"__builtin_aarch64_addhnv2di\" is undefined\n    return (uint32x2_t) __builtin_aarch64_addhnv2di ((int64x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1233): error: identifier \"__builtin_aarch64_raddhnv8hi\" is undefined\n    return (int8x8_t) __builtin_aarch64_raddhnv8hi (__a, __b);\n                      ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1240): error: identifier \"__builtin_aarch64_raddhnv4si\" is undefined\n    return (int16x4_t) __builtin_aarch64_raddhnv4si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1247): error: identifier \"__builtin_aarch64_raddhnv2di\" is undefined\n    return (int32x2_t) __builtin_aarch64_raddhnv2di (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1254): error: identifier \"__builtin_aarch64_raddhnv8hi\" is undefined\n    return (uint8x8_t) __builtin_aarch64_raddhnv8hi ((int16x8_t) __a,\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1262): error: identifier \"__builtin_aarch64_raddhnv4si\" is undefined\n    return (uint16x4_t) __builtin_aarch64_raddhnv4si ((int32x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1270): error: identifier \"__builtin_aarch64_raddhnv2di\" is undefined\n    return (uint32x2_t) __builtin_aarch64_raddhnv2di ((int64x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1278): error: identifier \"__builtin_aarch64_addhn2v8hi\" is undefined\n    return (int8x16_t) __builtin_aarch64_addhn2v8hi (__a, __b, __c);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1285): error: identifier \"__builtin_aarch64_addhn2v4si\" is undefined\n    return (int16x8_t) __builtin_aarch64_addhn2v4si (__a, __b, __c);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1292): error: identifier \"__builtin_aarch64_addhn2v2di\" is undefined\n    return (int32x4_t) __builtin_aarch64_addhn2v2di (__a, __b, __c);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1299): error: identifier \"__builtin_aarch64_addhn2v8hi\" is undefined\n    return (uint8x16_t) __builtin_aarch64_addhn2v8hi ((int8x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1308): error: identifier \"__builtin_aarch64_addhn2v4si\" is undefined\n    return (uint16x8_t) __builtin_aarch64_addhn2v4si ((int16x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1317): error: identifier \"__builtin_aarch64_addhn2v2di\" is undefined\n    return (uint32x4_t) __builtin_aarch64_addhn2v2di ((int32x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1326): error: identifier \"__builtin_aarch64_raddhn2v8hi\" is undefined\n    return (int8x16_t) __builtin_aarch64_raddhn2v8hi (__a, __b, __c);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1333): error: identifier \"__builtin_aarch64_raddhn2v4si\" is undefined\n    return (int16x8_t) __builtin_aarch64_raddhn2v4si (__a, __b, __c);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1340): error: identifier \"__builtin_aarch64_raddhn2v2di\" is undefined\n    return (int32x4_t) __builtin_aarch64_raddhn2v2di (__a, __b, __c);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1347): error: identifier \"__builtin_aarch64_raddhn2v8hi\" is undefined\n    return (uint8x16_t) __builtin_aarch64_raddhn2v8hi ((int8x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1356): error: identifier \"__builtin_aarch64_raddhn2v4si\" is undefined\n    return (uint16x8_t) __builtin_aarch64_raddhn2v4si ((int16x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1365): error: identifier \"__builtin_aarch64_raddhn2v2di\" is undefined\n    return (uint32x4_t) __builtin_aarch64_raddhn2v2di ((int32x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1458): error: identifier \"__builtin_aarch64_pmulv8qi\" is undefined\n    return (poly8x8_t) __builtin_aarch64_pmulv8qi ((int8x8_t) __a,\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(1522): error: identifier \"__builtin_aarch64_pmulv16qi\" is undefined\n    return (poly8x16_t) __builtin_aarch64_pmulv16qi ((int8x16_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2230): error: identifier \"__builtin_aarch64_ssublv8qi\" is undefined\n    return (int16x8_t) __builtin_aarch64_ssublv8qi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2237): error: identifier \"__builtin_aarch64_ssublv4hi\" is undefined\n    return (int32x4_t) __builtin_aarch64_ssublv4hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2244): error: identifier \"__builtin_aarch64_ssublv2si\" is undefined\n    return (int64x2_t) __builtin_aarch64_ssublv2si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2251): error: identifier \"__builtin_aarch64_usublv8qi\" is undefined\n    return (uint16x8_t) __builtin_aarch64_usublv8qi ((int8x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2259): error: identifier \"__builtin_aarch64_usublv4hi\" is undefined\n    return (uint32x4_t) __builtin_aarch64_usublv4hi ((int16x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2267): error: identifier \"__builtin_aarch64_usublv2si\" is undefined\n    return (uint64x2_t) __builtin_aarch64_usublv2si ((int32x2_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2275): error: identifier \"__builtin_aarch64_ssubl2v16qi\" is undefined\n    return (int16x8_t) __builtin_aarch64_ssubl2v16qi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2282): error: identifier \"__builtin_aarch64_ssubl2v8hi\" is undefined\n    return (int32x4_t) __builtin_aarch64_ssubl2v8hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2289): error: identifier \"__builtin_aarch64_ssubl2v4si\" is undefined\n    return (int64x2_t) __builtin_aarch64_ssubl2v4si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2296): error: identifier \"__builtin_aarch64_usubl2v16qi\" is undefined\n    return (uint16x8_t) __builtin_aarch64_usubl2v16qi ((int8x16_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2304): error: identifier \"__builtin_aarch64_usubl2v8hi\" is undefined\n    return (uint32x4_t) __builtin_aarch64_usubl2v8hi ((int16x8_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2312): error: identifier \"__builtin_aarch64_usubl2v4si\" is undefined\n    return (uint64x2_t) __builtin_aarch64_usubl2v4si ((int32x4_t) __a,\n                        ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2320): error: identifier \"__builtin_aarch64_ssubwv8qi\" is undefined\n    return (int16x8_t) __builtin_aarch64_ssubwv8qi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2327): error: identifier \"__builtin_aarch64_ssubwv4hi\" is undefined\n    return (int32x4_t) __builtin_aarch64_ssubwv4hi (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2334): error: identifier \"__builtin_aarch64_ssubwv2si\" is undefined\n    return (int64x2_t) __builtin_aarch64_ssubwv2si (__a, __b);\n                       ^\n\n/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h(2341): error: identifier \"__builtin_aarch64_usubwv8qi\" is undefined\n    return (uint16x8_t) __builtin_aarch64_usubwv8qi ((int16x8_t) __a,\n                        ^\n\nError limit reached.\n100 errors detected in the compilation of \"/home/hydai/workspace/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu\".\nCompilation terminated.\ngmake[2]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:328: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o] Error 4\n[ 38%] Linking CXX shared library ../../bin/libggml-cpu.so\n[ 38%] Built target ggml-cpu\n^C^Cgmake[2]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:1224: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o] Interrupt\ngmake[1]: *** [CMakeFiles/Makefile2:1790: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/all] Interrupt\ngmake: *** [Makefile:146: all] Interrupt\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-04-08T11:20:54+00:00",
    "closed_at": "2025-05-29T01:07:57+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12826/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12826"
  },
  {
    "number": 2341,
    "title": "IOT instruction (core dumped)",
    "body": "```shell\r\n$ nix build '.#opencl'\r\n$ result/bin/benchmark\r\nmain: build = 0 (unknown)\r\nStarting Test\r\nAllocating Memory of size 794558464 bytes, 757 MB\r\nggml_opencl: selecting platform: 'Intel(R) OpenCL Graphics'\r\nggml_opencl: selecting device: 'Intel(R) Iris(R) Xe Graphics'\r\nggml_opencl: device FP16 support: true\r\nCreating new tensors\r\n\r\n------ Test 1 - Matrix Mult via F32 code ------------------------------------------------------------------------------\r\nn_threads=1\r\n            m11: type = 0 (  f32) ne = 11008 x  4096 x     1, nb = (    4, 44032, 180355072) - Sum of tensor m11 is 16777216.00\r\n             m2: type = 0 (  f32) ne = 11008 x   128 x     1, nb = (    4, 44032, 5636096) - Sum of tensor m2 is 2818048.00\r\nGGML_ASSERT: /build/wnslnw6pk8d4c8k0b8w4w4qz45wgy9hw-source/ggml-opencl.cpp:1524: false\r\nzsh: IOT instruction (core dumped)  result/bin/benchmark\r\n```",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-07-23T12:17:37+00:00",
    "closed_at": "2024-04-09T01:07:36+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2341/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2341"
  },
  {
    "number": 9502,
    "title": "Bug: Last 2 Chunks In Streaming Mode Come Together In Firefox",
    "body": "### What happened?\n\nWhen using `/completion` with `stream: true`, the last 2 JSON chunks come together in Firefox, but Chrome seems to handle it fine, so it might be a Firefox bug.\r\n\r\nLooking further into this, it seems like HTTP `Transfer-Encoding: chunked` requires each chunk to be terminated with `\\r\\n`, but here `\\n\\n` is used instead:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/blob/6262d13e0b2da91f230129a93a996609a2f5a2f2/examples/server/utils.hpp#L296-L299\r\n\r\nThis doesn't seem to be just a Windows requirement, but listed as part of the HTTP specification:\r\n[HTTP Chunked Transfer Coding](https://httpwg.org/specs/rfc9112.html#chunked.encoding)\r\n\r\nMore information, including an example `chunked` response:\r\n[Transfer-Encoding Directives](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Transfer-Encoding#directives)\n\n### Name and Version\n\nllama-server.exe\r\nversion: 3761 (6262d13e)\r\nbuilt with MSVC 19.29.30154.0 for x64\n\n### What operating system are you seeing the problem on?\n\nWindows\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-09-16T02:14:04+00:00",
    "closed_at": "2024-09-17T06:48:47+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9502/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9502"
  },
  {
    "number": 7578,
    "title": "Bug: when arrive max ctx, model output garbage",
    "body": "### What happened?\r\n\r\nThis part has problem in cuda version. if set ngl>0, when arrive max ctx and next turn to chat, the model output garbage.\r\n\r\nllama_kv_cache_seq_rm (ctx, 0, params.n_keep            , params.n_keep + n_discard);\r\nllama_kv_cache_seq_add(ctx, 0, params.n_keep + n_discard, n_past, -n_discard);\r\n\r\nif set ngl =0, everythings ok.\r\n### Name and Version\r\n\r\nllama.cpp-b3014\r\nmain.exe --version\r\nversion: 247 (6765407)\r\nbuilt with MSVC 19.37.32822.0 for x64\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\n_No response_",
    "labels": [
      "need more info",
      "bug-unconfirmed",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-05-28T02:22:16+00:00",
    "closed_at": "2024-06-18T03:20:17+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7578/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7578"
  },
  {
    "number": 8685,
    "title": "Bug: (CUDA) Corrupted output when offloading to multiple GPUs",
    "body": "### What happened?\n\n### Problem\r\n\r\nSome models produce a corrupted output when offloading to multiple CUDA GPUs. The problem disappears when offloading to a single GPU or using CPU only.\r\n\r\nI was able to reproduce the problem in:\r\n- [llama 3.0 8B](https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/blob/main/Meta-Llama-3-8B-Instruct-Q6_K.gguf)\r\n- llama 3.1 8b\r\n- glm-4-9b\r\n\r\nwhile I was unable to reproduce it in:\r\n- Mistral Nemo \r\n\r\n### Bug 1\r\n\r\nWhen offloading to multiple GPUs, the model gives the wrong answer. It seems unable to parse the prompt correctly.\r\n\r\n### Bug 2\r\n\r\nWhen a second prompt is sent to the model, the model reuses information from the first prompt.\r\n\r\n### Steps to reproduce Bug 1\r\n\r\n- download [llama 3.0 8B](https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/blob/main/Meta-Llama-3-8B-Instruct-Q6_K.gguf) from HF\r\n- download my sample prompt: \r\n[llama-multi-gpu-bug.txt](https://github.com/user-attachments/files/16373846/llama-multi-gpu-bug.txt)\r\n- launch with: `CUDA_VISIBLE_DEVICES=0 ./llama-cli -c 8192  -m Meta-Llama-3-8B-Instruct-Q6_K.gguf -ngl 99 -f /tmp/llama-multi-gpu-bug.txt -sp -s 0`\r\n- verify that the model answers correctly: (Alice:7, Bob:8, Charlie:7) the full log is provided below.\r\n- launch again with `CUDA_VISIBLE_DEVICES=0,1 ./llama-cli -c 8192  -m Meta-Llama-3-8B-Instruct-Q6_K.gguf -ngl 99 -f /tmp/llama-multi-gpu-bug.txt -sp -s 0`\r\n- The model is unable to answer: (Alice:7, Bob:7, Charlie: null) then says that Charlie didn't provide an answer.\r\n\r\n### Steps to reproduce Bug 2\r\n\r\n- launch the server with `CUDA_VISIBLE_DEVICES=0,1 ./llama-server -ngl 99 -c 8192 -m Meta-Llama-3-8B-Instruct-Q6_K.gguf`\r\n- go to New UI\r\n- Select llama 3 template.\r\n- Copy and paste the content of the sample prompt: \r\n[apples_majority.txt](https://github.com/user-attachments/files/16373941/apples_majority.txt) and send\r\n- The model gives the same corrupted answer as in Bug 1\r\n- Go back\r\n- Copy and paste this new prompt \r\n[apples_simple.txt](https://github.com/user-attachments/files/16373949/apples_simple.txt) and send\r\n- The model gives a json answer as if answering the first prompt\r\n\r\nThe second prompt shares a common prefix with the first prompt.\r\n\r\n### Full log for the correct answer:\r\n```\r\nHere is the output:\r\n\r\n{\r\n  \"answers\": {\r\n    \"alice\": 7,\r\n    \"bob\": 8,\r\n    \"charlie\": 7\r\n  },\r\n  \"reasoning\": \"The users provided different answers based on their step-by-step breakdowns of the problem, but they all reached the conclusion that Matteo discards a quarter of his remaining fruits equally between apples and oranges.\",\r\n  \"who is right\": \"It's a tie between Alice and Charlie, as they both answered 7 apples remaining.\",\r\n  \"answer\": 7\r\n}\r\n\r\nNote that since Alice and Charlie have the same answer, 7, they are considered the \"right\" answer based on majority voting.\r\n```\r\n\r\n### Full log for the wrong answer\r\n```\r\nHere is the output:\r\n\r\n{\r\n  \"answers\": {\r\n    \"alice\": 7,\r\n    \"bob\": 7,\r\n    \"charlie\": null\r\n  },\r\n  \"reasoning\": \"Both Alice and Bob provided the same answer, which is 7 apples. Since Charlie did not provide an answer, we use majority voting to determine the correct answer.\",\r\n  \"who is right\": \"Alice and Bob\",\r\n  \"answer\": 7\r\n}\r\n\r\nIn this case, both Alice and Bob provided the same answer, which is 7 apples. Since Charlie did not provide an answer, we use majority voting to determine the correct answer, which is 7 apples.\r\n```\r\n\r\n### My setup:\r\n\r\nLinux with:\r\n- Nvidia 4060 16GB\r\n- Nvidia 3060 12GB\r\n\r\n\r\n\n\n### Name and Version\n\nversion: 3463 (dc820804)\r\nbuilt with cc (Debian 12.2.0-14) 12.2.0 for x86_64-linux-gnu\r\n\r\nThe reference model is llama3.0 8b by bartowski, SHA256: d6f1dcba991f8e629531a5f4cf19e4dbc2a89f80fd20737e256b92aac11572f1\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-07-25T10:02:08+00:00",
    "closed_at": "2024-08-07T11:29:04+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8685/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8685"
  },
  {
    "number": 13877,
    "title": "Eval bug: llama-server.exe silently crashes (ucrtbased.dll) after 2-3 requests in a dialogue",
    "body": "### Name and Version\n\nversion: 5528 (53ae3064)\nbuilt with MSVC 19.43.34810.0 for x64\n\n### Operating systems\n\nWindows\n\n### GGML backends\n\nVulkan\n\n### Hardware\n\n9070xt, A770\n\n### Models\n\nModel Qwen3-30B-A3B-UD-Q5_K_XL.gguf  with fixed chat templates\nSHA256: f284af35140194f073985a093f6d257cb7060784ecbfeb52c15f9545dfa4f434\n\n### Problem description & steps to reproduce\n\nllama-server.exe -m Qwen3-30B-A3B-UD-Q5_K_XL.gguf -ngl 99 -c 15000 --port 8000 --jinja \n\nServer silently terminates in some dialogues, typically after 2-3 requests within a single dialogue.\nThe Windows Event Log records a crash event for llama-server.exe, with ucrtbased.dll as the faulting module.\n\n### First Bad Commit\n\nhttps://github.com/ggml-org/llama.cpp/commit/e121edc4324a640be11b7e567edd39b721b0f8e4\n\nb5486\n\n### Relevant log output\n\n```shell\nsrv  update_chat_: Parsing chat message: <think>\n\nParsing input with format Hermes 2 Pro: <think>\n\nPartial parse: </think>\nParsed message: {\"role\":\"assistant\",\"content\":null}\nsrv          send: sending result for task id = 0\nsrv          send: task id = 0 pushed to result queue\nslot process_toke: id  0 | task 0 | n_decoded = 2, n_remaining = -1, next token:   198 '\n'\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 2\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 3, front = 0\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 15008, n_past = 903, n_cache_tokens = 903, truncated = 0\nsrv  update_slots: decoding batch, n_tokens = 1\nset_embeddings: value = 0\nclear_adapter_lora: call\nsrv  update_chat_: Parsing chat message: <think>\n\u0425\nParsing input with format Hermes 2 Pro: <think>\n\u0425\n\n\n\n```\n![Image](https://github.com/user-attachments/assets/f499ca55-4c8c-4f24-8751-c6f221ec3d5b)",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-05-28T23:55:43+00:00",
    "closed_at": "2025-05-30T23:00:00+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13877/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13877"
  },
  {
    "number": 14015,
    "title": "Feature Request: Support Llama-Nemotron-Nano-VL-8B-V1",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nNvidia just released a new VLM: https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1\n\n- Text model: [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)\n- Vision encoder: [nvidia/C-RADIOv2-H](https://huggingface.co/nvidia/C-RADIOv2-H)\n\nOn `master` (`3e63a58e`), the command:\n\n```zsh\npython llama.cpp/convert_hf_to_gguf.py --outfile /opt/workspace/gguf/Llama-Nemotron-Nano-VL-8B-V1-Q8_0.gguf --outtype bf16 /opt/workspace/hf/Llama-Nemotron-Nano-VL-8B-V1/\n```\n\ncurrently fails:\n\n<details>\n<summary>\u2195\ufe0f Click to expand full output ...</summary>\n\n```\nINFO:hf-to-gguf:Loading model: Llama-Nemotron-Nano-VL-8B-V1\nWARNING:hf-to-gguf:Failed to load model config from /opt/workspace/hf/Llama-Nemotron-Nano-VL-8B-V1: Loading /opt/workspace/hf/Llama-Nemotron-Nano-VL-8B-V1 requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.\nWARNING:hf-to-gguf:Trying to load config.json instead\nINFO:hf-to-gguf:Model architecture: LlamaForCausalLM\nWARNING:hf-to-gguf:Failed to load model config from /opt/workspace/hf/Llama-Nemotron-Nano-VL-8B-V1: Loading /opt/workspace/hf/Llama-Nemotron-Nano-VL-8B-V1 requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.\nWARNING:hf-to-gguf:Trying to load config.json instead\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Exporting model...\nINFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\nINFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\nINFO:hf-to-gguf:output.weight,               torch.bfloat16 --> BF16, shape = {4096, 128512}\nINFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {4096, 128512}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\nTraceback (most recent call last):\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 6533, in <module>\n    main()\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 6527, in main\n    model_instance.write()\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 403, in write\n    self.prepare_tensors()\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 2011, in prepare_tensors\n    super().prepare_tensors()\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 277, in prepare_tensors\n    for new_name, data_torch in (self.modify_tensors(data_torch, name, bid)):\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 1979, in modify_tensors\n    return [(self.map_tensor_name(name), data_torch)]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 236, in map_tensor_name\n    raise ValueError(f\"Can not map tensor {name!r}\")\nValueError: Can not map tensor 'mlp1.0.bias'\n```\n\n</details>\n\n...even with `trust_remote_code=True`:\n\n<details>\n<summary>\u2195\ufe0f Click to expand full output ...</summary>\n\n```\nINFO:hf-to-gguf:Loading model: Llama-Nemotron-Nano-VL-8B-V1\nEncountered exception while importing open_clip: No module named 'open_clip'\nWARNING:hf-to-gguf:Failed to load model config from /opt/workspace/hf/Llama-Nemotron-Nano-VL-8B-V1: This modeling file requires the following packages that were not found in your environment: open_clip. Run `pip install open_clip`\nWARNING:hf-to-gguf:Trying to load config.json instead\nINFO:hf-to-gguf:Model architecture: LlamaForCausalLM\nEncountered exception while importing open_clip: No module named 'open_clip'\nWARNING:hf-to-gguf:Failed to load model config from /opt/workspace/hf/Llama-Nemotron-Nano-VL-8B-V1: This modeling file requires the following packages that were not found in your environment: open_clip. Run `pip install open_clip`\nWARNING:hf-to-gguf:Trying to load config.json instead\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Exporting model...\nINFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\nINFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\nINFO:hf-to-gguf:output.weight,               torch.bfloat16 --> BF16, shape = {4096, 128512}\nINFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {4096, 128512}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\nINFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\nTraceback (most recent call last):\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 6533, in <module>\n    main()\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 6527, in main\n    model_instance.write()\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 403, in write\n    self.prepare_tensors()\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 2011, in prepare_tensors\n    super().prepare_tensors()\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 277, in prepare_tensors\n    for new_name, data_torch in (self.modify_tensors(data_torch, name, bid)):\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 1979, in modify_tensors\n    return [(self.map_tensor_name(name), data_torch)]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dylan/Documents/AI/llama.cpp/convert_hf_to_gguf.py\", line 236, in map_tensor_name\n    raise ValueError(f\"Can not map tensor {name!r}\")\nValueError: Can not map tensor 'mlp1.0.bias'\n```\n\n</details>\n\n\nMaybe @ngxson knows how to fix this?\n\n### Motivation\n\nUsers of llama-server would like to be able to use this new Nvidia 8B model with vision, it's pretty good. You can try it at nvidia's demo site here: https://build.nvidia.com/nvidia/llama-3.1-nemotron-nano-vl-8b-v1\n\n### Possible Implementation\n\n(I don't know enough about VLMs and libmtmd to offer a possible implementation/fix, sorry)",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-06-04T16:13:31+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14015/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14015"
  },
  {
    "number": 2353,
    "title": "[Feature Request]: 32k context",
    "body": "can the internal prompt limit be raised to 32k for llama 2 models? I'm only assuming this works because the llama 2 context is double the previous.\r\n```\r\nmain: error: prompt is too long (30474 tokens, max 16380\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-07-23T22:14:34+00:00",
    "closed_at": "2023-07-23T22:15:55+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2353/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2353"
  },
  {
    "number": 13332,
    "title": "Feature Request: moondream2 vlm support in mtmd",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nPlease support moondream2 model in mtmd\n\nmoondream2 is a small vlm that is incredibly good for its small size: https://huggingface.co/vikhyatk/moondream2/\n\nI believe that an earlier version of moondream was supported by llama.cpp at some point, but newer versions are not.\n\n### Motivation\n\nmoondream2 is one of the best models for memory constrained scenarios such as the edge.  adding llama.cpp support will enable inference with vulkan, which is currently lacking.\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2025-05-06T07:16:00+00:00",
    "closed_at": "2025-05-25T12:04:50+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13332/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13332"
  },
  {
    "number": 5011,
    "title": "Missing public header for ggml-backend.h and ggml-alloc.h",
    "body": "The public header defined in CMakeLists.txt for llama.h (used for library level C FFI integrations) depends on the header `ggml-backend.h` now, but as it is not a defined public header in CMakeLists.h, it cannot be found in the CMake output. It seems like this is an omission, and the CMakeLists.txt should include this file as a public header.",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-01-18T01:45:17+00:00",
    "closed_at": "2024-01-18T21:36:33+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5011/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5011"
  }
]
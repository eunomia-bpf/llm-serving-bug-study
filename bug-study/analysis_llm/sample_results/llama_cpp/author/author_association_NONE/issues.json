[
  {
    "number": 7776,
    "title": "ImportError: cannot import name 'BaseVocab' from 'gguf'",
    "body": "I want to convert LLaVAMistral model to GGUF, I am using tthis code **convert-legacy-llama.py** which is present inside **examples** folder. But got this error **ImportError: cannot import name 'BaseVocab' from 'gguf'**. I didn't found anything online",
    "labels": [],
    "state": "closed",
    "created_at": "2024-06-05T18:54:35+00:00",
    "closed_at": "2024-06-06T08:03:31+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7776/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7776"
  },
  {
    "number": 6787,
    "title": "Can not offload layers to GPU (llama3)",
    "body": "make LLAMA_CUDA=1\r\n ./main -m /models/Meta-Llama-3-70B-Instruct.Q4_K_M.gguf -r '<|eot_id|>' \r\n --in-prefix \"\\n<|start_header_id|>user<|end_header_id|>\\n\\n\" \r\n --in-suffix \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\" \r\n -p \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n\\nHi! How are you?<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\\n\" \r\n -n 1024 -ngl 90\r\n\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\r\nggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA H800, compute capability 9.0, VMM: yes\r\nllm_load_tensors: ggml ctx size =    0.15 MiB\r\nllm_load_tensors: offloading 0 repeating layers to GPU\r\nllm_load_tensors: offloaded 0/33 layers to GPU\r\nllm_load_tensors:        CPU buffer size =  5459.93 MiB\r\n.........................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: freq_base  = 500000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:  CUDA_Host KV buffer size =    64.00 MiB\r\nllama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\r\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =   669.48 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =     9.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 356",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-20T14:41:33+00:00",
    "closed_at": "2024-06-06T01:06:45+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6787/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6787"
  },
  {
    "number": 12543,
    "title": "Eval bug: crash when pooling_type == LLAMA_POOLING_TYPE_MEAN",
    "body": "### Name and Version\n\nRevision 9b169a4d4e01af7bc07a6981b53b27c18c9470d8\n\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCPU\n\n### Hardware\n\nARM Ampere\n\n### Models\n\nQwen2.5-14B-Instruct-1M-Q5_K_M\n\n### Problem description & steps to reproduce\n\nSetting pooling_type = LLAMA_POOLING_TYPE_MEAN and calling llama_init_from_model() causes this crash:\n\n```\n/build/source/ggml/src/ggml.c:2738: GGML_ASSERT(ggml_can_mul_mat(a, b)) failed\n```\n\nSetting to LLAMA_POOLING_TYPE_LAST and changing nothing else works correctly.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n/build/source/ggml/src/ggml.c:2738: GGML_ASSERT(ggml_can_mul_mat(a, b)) failed\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-24T10:00:01+00:00",
    "closed_at": "2025-05-09T01:07:53+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12543/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12543"
  },
  {
    "number": 12986,
    "title": "Misc. bug: Potential memory leak in backend registry",
    "body": "### Name and Version\n\nbuild: 5124 (bc091a4d) with MSVC 19.43.34810.0 for x64 (debug)\nstatic build (MT/MTd) with VS2022 / LLAMA & GGML\nGGML_STATIC / GGML_USE_CPU / GGML_USE_BLAS / GGML_USE_CUDA\n\n### Operating systems\n\nWindows\n\n### Which llama.cpp modules do you know to be affected?\n\nOther (Please specify in the next section)\n\n### Command line\n\n```shell\nNo command line. Using personal C++ (preliminary) implementation that follow the steps of the llama-cli.\n```\n\n### Problem description & steps to reproduce\n\nI'm currently only do :\n\n```\nllama_backend_init();\nllama_model_load_from_file();\nllama_model_free();\nllama_backend_free();\n```\nThere's no problem of memory leak until I offload the model on my GPU with `n_gpu_layers > 0`\n\nThe problem of leak comes with `ggml_backend_cuda_reg()` that act as a singleton-like fashion and never release the static context allocated as `new`. I don't know if the problem is specific from my build (`GGML_STATIC`) but I think that is potentially related to the comment:\n\n> ggml-backend-reg.cpp (line 196)\n> ```\n> ~ggml_backend_registry() {\n> // FIXME: backends cannot be safely unloaded without a function to destroy all the backend resources,\n> // since backend threads may still be running and accessing resources from the dynamic library\n> ```\n\nThis is not a huge leak but here's the details of the leak detector:\n\n```\n---------- Block 11520 at 0x000000009F233040: 16 bytes ----------\n  Leak Hash: 0xD208674E, Count: 1, Total 16 bytes\n  Call Stack (TID 22032):\n    ntdll.dll!RtlAllocateHeap()\n    D:\\a\\_work\\1\\s\\src\\vctools\\crt\\vcstartup\\src\\heap\\new_scalar.cpp (36): AIRoleplay.exe!operator new() + 0xA bytes\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (137): AIRoleplay.exe!std::_Default_allocate_traits::_Allocate()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (258): AIRoleplay.exe!std::_Allocate<16,std::_Default_allocate_traits>()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (987): AIRoleplay.exe!std::allocator<std::_Container_proxy>::allocate()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (1216): AIRoleplay.exe!std::_Container_base12::_Alloc_proxy<std::allocator<std::_Container_proxy> >() + 0xF bytes\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\vector (656): AIRoleplay.exe!std::vector<ggml_backend_device *,std::allocator<ggml_backend_device *> >::vector<ggml_backend_device *,std::allocator<ggml_backend_device *> >() + 0x2E bytes\n    AIRoleplay.exe!ggml_backend_cuda_reg_context::ggml_backend_cuda_reg_context() + 0x17 bytes\n    E:\\Dependencies\\ai.cpp\\ggml\\src\\ggml-cuda\\ggml-cuda.cu (3451): AIRoleplay.exe!ggml_backend_cuda_reg() + 0x2A bytes\n    AIRoleplay.exe!ggml_backend_registry::ggml_backend_registry() + 0x5F bytes\n    AIRoleplay.exe!ggml_backend_register() + 0x25A bytes\n    AIRoleplay.exe!ggml_backend_dev_count() + 0x9 bytes\n    AIRoleplay.exe!llama_print_system_info() + 0x583 bytes\n    AIRoleplay.exe!llama_model_load_from_file() + 0x90 bytes\n  Data:\n    60 9A C5 A4    80 01 00 00    00 00 00 00    00 00 00 00     `....... ........\n```\n```\n---------- Block 11519 at 0x00000000A4C59A60: 32 bytes ----------\n  Leak Hash: 0x55976925, Count: 1, Total 32 bytes\n  Call Stack (TID 22032):\n    ntdll.dll!RtlAllocateHeap()\n    D:\\a\\_work\\1\\s\\src\\vctools\\crt\\vcstartup\\src\\heap\\new_scalar.cpp (36): AIRoleplay.exe!operator new() + 0xA bytes\n    E:\\Dependencies\\ai.cpp\\ggml\\src\\ggml-cuda\\ggml-cuda.cu (3451): AIRoleplay.exe!ggml_backend_cuda_reg() + 0xA bytes\n    AIRoleplay.exe!ggml_backend_registry::ggml_backend_registry() + 0x5F bytes\n    AIRoleplay.exe!ggml_backend_register() + 0x25A bytes\n    AIRoleplay.exe!ggml_backend_dev_count() + 0x9 bytes\n    AIRoleplay.exe!llama_print_system_info() + 0x583 bytes\n    AIRoleplay.exe!llama_model_load_from_file() + 0x90 bytes\nData:\n  40 30 23 9F    80 01 00 00    F0 28 F4 A4    80 01 00 00     @0#..... .(......\n  F8 28 F4 A4    80 01 00 00    F8 28 F4 A4    80 01 00 00     .(...... .(......\n```\n```\n---------- Block 11588 at 0x00000000A4F3CE90: 88 bytes ----------\n  Leak Hash: 0xDC22AD9D, Count: 1, Total 88 bytes\n  Call Stack (TID 22032):\n    ntdll.dll!RtlAllocateHeap()\n    D:\\a\\_work\\1\\s\\src\\vctools\\crt\\vcstartup\\src\\heap\\new_scalar.cpp (36): AIRoleplay.exe!operator new() + 0xA bytes\n    E:\\Dependencies\\ai.cpp\\ggml\\src\\ggml-cuda\\ggml-cuda.cu (3454): AIRoleplay.exe!ggml_backend_cuda_reg() + 0xA bytes\n    AIRoleplay.exe!ggml_backend_registry::ggml_backend_registry() + 0x5F bytes\n    AIRoleplay.exe!ggml_backend_register() + 0x25A bytes\n    AIRoleplay.exe!ggml_backend_dev_count() + 0x9 bytes\n    AIRoleplay.exe!llama_print_system_info() + 0x583 bytes\n    AIRoleplay.exe!llama_model_load_from_file() + 0x90 bytes\n Data:\n   00 00 00 00    CD CD CD CD    60 1D F4 A4    80 01 00 00     ........ `.......\n   43 55 44 41    30 00 00 00    00 00 00 00    00 00 00 00     CUDA0... ........\n   05 00 00 00    00 00 00 00    0F 00 00 00    00 00 00 00     ........ ........\n   A0 19 F4 A4    80 01 00 00    00 41 0D A5    80 01 00 00     ........ .A......\n   00 00 00 00    00 00 00 00    17 00 00 00    00 00 00 00     ........ ........\n   1F 00 00 00    00 00 00 00                                   ........ ........\n```\n```\n---------- Block 11590 at 0x00000000A4F419A0: 16 bytes ----------\n  Leak Hash: 0x31E4675E, Count: 1, Total 16 bytes\n  Call Stack (TID 22032):\n    ntdll.dll!RtlAllocateHeap()\n    D:\\a\\_work\\1\\s\\src\\vctools\\crt\\vcstartup\\src\\heap\\new_scalar.cpp (36): AIRoleplay.exe!operator new() + 0xA bytes\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (137): AIRoleplay.exe!std::_Default_allocate_traits::_Allocate()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (258): AIRoleplay.exe!std::_Allocate<16,std::_Default_allocate_traits>()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (987): AIRoleplay.exe!std::allocator<std::_Container_proxy>::allocate()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (1216): AIRoleplay.exe!std::_Container_base12::_Alloc_proxy<std::allocator<std::_Container_proxy> >() + 0xF bytes\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xstring (833): AIRoleplay.exe!std::basic_string<char,std::char_traits<char>,std::allocator<char> >::_Construct_empty() + 0x24 bytes\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xstring (685): AIRoleplay.exe!std::basic_string<char,std::char_traits<char>,std::allocator<char> >::basic_string<char,std::char_traits<char>,std::allocator<char> >() + 0xA bytes\n    AIRoleplay.exe!ggml_backend_cuda_device_context::ggml_backend_cuda_device_context() + 0x2C bytes\n    E:\\Dependencies\\ai.cpp\\ggml\\src\\ggml-cuda\\ggml-cuda.cu (3454): AIRoleplay.exe!ggml_backend_cuda_reg() + 0x2A bytes\n    AIRoleplay.exe!ggml_backend_registry::ggml_backend_registry() + 0x5F bytes\n    AIRoleplay.exe!ggml_backend_register() + 0x25A bytes\n    AIRoleplay.exe!ggml_backend_dev_count() + 0x9 bytes\n    AIRoleplay.exe!llama_print_system_info() + 0x583 bytes\n    AIRoleplay.exe!llama_model_load_from_file() + 0x90 bytes\nData:\n  C0 CE F3 A4    80 01 00 00    00 00 00 00    00 00 00 00     ........ ........\n```\n```\n---------- Block 11589 at 0x00000000A4F41D60: 16 bytes ----------\n  Leak Hash: 0x81F556EB, Count: 1, Total 16 bytes\n  Call Stack (TID 22032):\n    ntdll.dll!RtlAllocateHeap()\n    D:\\a\\_work\\1\\s\\src\\vctools\\crt\\vcstartup\\src\\heap\\new_scalar.cpp (36): AIRoleplay.exe!operator new() + 0xA bytes\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (137): AIRoleplay.exe!std::_Default_allocate_traits::_Allocate()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (258): AIRoleplay.exe!std::_Allocate<16,std::_Default_allocate_traits>()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (987): AIRoleplay.exe!std::allocator<std::_Container_proxy>::allocate()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (1216): AIRoleplay.exe!std::_Container_base12::_Alloc_proxy<std::allocator<std::_Container_proxy> >() + 0xF bytes\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xstring (833): AIRoleplay.exe!std::basic_string<char,std::char_traits<char>,std::allocator<char> >::_Construct_empty() + 0x24 bytes\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xstring (685): AIRoleplay.exe!std::basic_string<char,std::char_traits<char>,std::allocator<char> >::basic_string<char,std::char_traits<char>,std::allocator<char> >() + 0xA bytes\n    AIRoleplay.exe!ggml_backend_cuda_device_context::ggml_backend_cuda_device_context() + 0x1B bytes\n    E:\\Dependencies\\ai.cpp\\ggml\\src\\ggml-cuda\\ggml-cuda.cu (3454): AIRoleplay.exe!ggml_backend_cuda_reg() + 0x2A bytes\n    AIRoleplay.exe!ggml_backend_registry::ggml_backend_registry() + 0x5F bytes\n    AIRoleplay.exe!ggml_backend_register() + 0x25A bytes\n    AIRoleplay.exe!ggml_backend_dev_count() + 0x9 bytes\n    AIRoleplay.exe!llama_print_system_info() + 0x583 bytes\n    AIRoleplay.exe!llama_model_load_from_file() + 0x90 bytes\n Data:\n   98 CE F3 A4    80 01 00 00    00 00 00 00    00 00 00 00     ........ ........\n```\n```\n---------- Block 11595 at 0x00000000A4F428F0: 8 bytes ----------\n  Leak Hash: 0x051EE01B, Count: 1, Total 8 bytes\n  Call Stack (TID 22032):\n    ntdll.dll!RtlAllocateHeap()\n    D:\\a\\_work\\1\\s\\src\\vctools\\crt\\vcstartup\\src\\heap\\new_scalar.cpp (36): AIRoleplay.exe!operator new() + 0xA bytes\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (137): AIRoleplay.exe!std::_Default_allocate_traits::_Allocate()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (258): AIRoleplay.exe!std::_Allocate<16,std::_Default_allocate_traits>()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (987): AIRoleplay.exe!std::allocator<ggml_backend_device *>::allocate()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (2268): AIRoleplay.exe!std::_Allocate_at_least_helper<std::allocator<ggml_backend_device *> >()\n    AIRoleplay.exe!std::vector<ggml_backend_device * __ptr64,std::allocator<ggml_backend_device * __ptr64> >::_Emplace_reallocate<ggml_backend_device * __ptr64 const & __ptr64>() + 0xD9 bytes\n    AIRoleplay.exe!std::vector<ggml_backend_device * __ptr64,std::allocator<ggml_backend_device * __ptr64> >::_Emplace_one_at_back<ggml_backend_device * __ptr64 const & __ptr64>() + 0x82 bytes\n    AIRoleplay.exe!std::vector<ggml_backend_device * __ptr64,std::allocator<ggml_backend_device * __ptr64> >::push_back() + 0x1D bytes\n    E:\\Dependencies\\ai.cpp\\ggml\\src\\ggml-cuda\\ggml-cuda.cu (3468): AIRoleplay.exe!ggml_backend_cuda_reg() + 0x22 bytes\n    AIRoleplay.exe!ggml_backend_registry::ggml_backend_registry() + 0x5F bytes\n    AIRoleplay.exe!ggml_backend_register() + 0x25A bytes\n    AIRoleplay.exe!ggml_backend_dev_count() + 0x9 bytes\n    AIRoleplay.exe!llama_print_system_info() + 0x583 bytes\n    AIRoleplay.exe!llama_model_load_from_file() + 0x90 bytes\nData:\n  90 7D D3 9E    80 01 00 00                                   .}...... ........\n```\n```\n---------- Block 11593 at 0x00000000A50D4100: 32 bytes ----------\n  Leak Hash: 0xBC55F72B, Count: 1, Total 32 bytes\n  Call Stack (TID 22032):\n    ntdll.dll!RtlAllocateHeap()\n    D:\\a\\_work\\1\\s\\src\\vctools\\crt\\vcstartup\\src\\heap\\new_scalar.cpp (36): AIRoleplay.exe!operator new() + 0xA bytes\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (137): AIRoleplay.exe!std::_Default_allocate_traits::_Allocate()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (258): AIRoleplay.exe!std::_Allocate<16,std::_Default_allocate_traits>()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (987): AIRoleplay.exe!std::allocator<char>::allocate()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (2268): AIRoleplay.exe!std::_Allocate_at_least_helper<std::allocator<char> >()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xstring (805): AIRoleplay.exe!std::basic_string<char,std::char_traits<char>,std::allocator<char> >::_Allocate_for_capacity<0>() + 0xF bytes\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xstring (2969): AIRoleplay.exe!std::basic_string<char,std::char_traits<char>,std::allocator<char> >::_Reallocate_for<`std::basic_string<char,std::char_traits<char>,std::allocator<char> >::assign'::`2'::<lambda_1>,char const *>() + 0xF bytes\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xstring (1611): AIRoleplay.exe!std::basic_string<char,std::char_traits<char>,std::allocator<char> >::assign()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xstring (1615): AIRoleplay.exe!std::basic_string<char,std::char_traits<char>,std::allocator<char> >::assign()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xstring (1423): AIRoleplay.exe!std::basic_string<char,std::char_traits<char>,std::allocator<char> >::operator=()\n    E:\\Dependencies\\ai.cpp\\ggml\\src\\ggml-cuda\\ggml-cuda.cu (3461): AIRoleplay.exe!ggml_backend_cuda_reg() + 0x19 bytes\n    AIRoleplay.exe!ggml_backend_registry::ggml_backend_registry() + 0x5F bytes\n    AIRoleplay.exe!ggml_backend_register() + 0x25A bytes\n    AIRoleplay.exe!ggml_backend_dev_count() + 0x9 bytes\n    AIRoleplay.exe!llama_print_system_info() + 0x583 bytes\n    AIRoleplay.exe!llama_model_load_from_file() + 0x90 bytes\n Data:\n   4E 56 49 44    49 41 20 47    65 46 6F 72    63 65 20 52     NVIDIA.G eForce.R\n   54 58 20 34    30 38 30 00    CD CD CD CD    CD CD CD CD     TX.4080. ........\n```\n```\n---------- Block 10200651 at 0x000000000B2A9EF0: 48 bytes ----------\n  Leak Hash: 0x6889E147, Count: 1, Total 48 bytes\n  Call Stack (TID 22032):\n    ntdll.dll!RtlAllocateHeap()\n    D:\\a\\_work\\1\\s\\src\\vctools\\crt\\vcstartup\\src\\heap\\new_scalar.cpp (36): AIRoleplay.exe!operator new() + 0xA bytes\n    E:\\Dependencies\\ai.cpp\\ggml\\src\\ggml-cuda\\ggml-cuda.cu (713): AIRoleplay.exe!ggml_backend_cuda_buffer_type() + 0x50 bytes\n    E:\\Dependencies\\ai.cpp\\ggml\\src\\ggml-cuda\\ggml-cuda.cu (2944): AIRoleplay.exe!ggml_backend_cuda_device_get_buffer_type()\n    AIRoleplay.exe!ggml_backend_dev_buffer_type() + 0x20 bytes\n    AIRoleplay.exe!llama_internal_get_tensor_map() + 0x10A7 bytes\n    AIRoleplay.exe!llama_model::load_tensors() + 0x205 bytes\n    AIRoleplay.exe!llama_print_system_info() + 0x36C bytes\n    AIRoleplay.exe!llama_print_system_info() + 0xA3C bytes\n    AIRoleplay.exe!llama_model_load_from_file() + 0x90 bytes\n  Data:\n    00 00 00 00    CD CD CD CD    C0 59 E5 14    81 01 00 00     ........ .Y......\n    43 55 44 41    30 00 00 00    00 00 00 00    00 00 00 00     CUDA0... ........\n    05 00 00 00    00 00 00 00    0F 00 00 00    00 00 00 00     ........ ........\n```\n```\n---------- Block 10200653 at 0x0000000014E559C0: 16 bytes ----------\n  Leak Hash: 0xBA21D9C7, Count: 1, Total 16 bytes\n  Call Stack (TID 22032):\n    ntdll.dll!RtlAllocateHeap()\n    D:\\a\\_work\\1\\s\\src\\vctools\\crt\\vcstartup\\src\\heap\\new_scalar.cpp (36): AIRoleplay.exe!operator new() + 0xA bytes\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (137): AIRoleplay.exe!std::_Default_allocate_traits::_Allocate()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (258): AIRoleplay.exe!std::_Allocate<16,std::_Default_allocate_traits>()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (987): AIRoleplay.exe!std::allocator<std::_Container_proxy>::allocate()\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xmemory (1216): AIRoleplay.exe!std::_Container_base12::_Alloc_proxy<std::allocator<std::_Container_proxy> >() + 0xF bytes\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xstring (1005): AIRoleplay.exe!std::basic_string<char,std::char_traits<char>,std::allocator<char> >::basic_string<char,std::char_traits<char>,std::allocator<char> >() + 0x2E bytes\n    C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.43.34808\\include\\xstring (3198): AIRoleplay.exe!std::operator+<char,std::char_traits<char>,std::allocator<char> >() + 0x26 bytes\n    E:\\Dependencies\\ai.cpp\\ggml\\src\\ggml-cuda\\ggml-cuda.cu (713): AIRoleplay.exe!ggml_backend_cuda_buffer_type() + 0xBF bytes\n    E:\\Dependencies\\ai.cpp\\ggml\\src\\ggml-cuda\\ggml-cuda.cu (2944): AIRoleplay.exe!ggml_backend_cuda_device_get_buffer_type()\n    AIRoleplay.exe!ggml_backend_dev_buffer_type() + 0x20 bytes\n    AIRoleplay.exe!llama_internal_get_tensor_map() + 0x10A7 bytes\n    AIRoleplay.exe!llama_model::load_tensors() + 0x205 bytes\n    AIRoleplay.exe!llama_print_system_info() + 0x36C bytes\n    AIRoleplay.exe!llama_print_system_info() + 0xA3C bytes\n    AIRoleplay.exe!llama_model_load_from_file() + 0x90 bytes\nData:\n  F8 9E 2A 0B    81 01 00 00    00 00 00 00    00 00 00 00     ..*..... ........\n```\n```\n---------- Block 10200643 at 0x0000000014E56640: 8 bytes ----------\n  Leak Hash: 0x93E3F1D6, Count: 1, Total 8 bytes\n  Call Stack (TID 22032):\n    ntdll.dll!RtlAllocateHeap()\n    D:\\a\\_work\\1\\s\\src\\vctools\\crt\\vcstartup\\src\\heap\\new_scalar.cpp (36): AIRoleplay.exe!operator new() + 0xA bytes\n    AIRoleplay.exe!ggml_backend_cpu_aarch64_buffer_type() + 0x64 bytes\n    AIRoleplay.exe!ggml_backend_cpu_get_extra_buffers_type() + 0xB6 bytes\n    AIRoleplay.exe!ggml_backend_cpu_get_extra_buffers_type() + 0x62 bytes\n    AIRoleplay.exe!ggml_backend_cpu_get_extra_buffers_type() + 0x13E bytes\n    AIRoleplay.exe!llama_internal_get_tensor_map() + 0xEDC bytes\n    AIRoleplay.exe!llama_model::load_tensors() + 0x109 bytes\n    AIRoleplay.exe!llama_print_system_info() + 0x36C bytes\n    AIRoleplay.exe!llama_print_system_info() + 0xA3C bytes\n    AIRoleplay.exe!llama_model_load_from_file() + 0x90 bytes\nData:\n  90 8D 07 92    F7 7F 00 00                                   ........ ........\n```\nThe CRT library classic output (another execution):\n```\nDetected memory leaks!\nDumping objects ->\n{10179386} normal block at 0x0000018EBE7F3950, 16 bytes long.\n Data: < r              > 98 72 A9 BD 8E 01 00 00 00 00 00 00 00 00 00 00 \n{10179384} normal block at 0x0000018EBDA97290, 48 bytes long.\n Data: <        P9      > 00 00 00 00 CD CD CD CD 50 39 7F BE 8E 01 00 00 \n{10179376} normal block at 0x0000018EBE7F3CC0, 8 bytes long.\n Data: <  )     > 90 8D 29 C7 F6 7F 00 00 \n{7792} normal block at 0x0000018E9132B3F0, 8 bytes long.\n Data: <        > F0 07 F0 90 8E 01 00 00 \n{7791} normal block at 0x0000018E90F007F0, 136 bytes long.\n Data: <@               > 40 14 E8 C5 F6 7F 00 00 80 14 E8 C5 F6 7F 00 00 \n{7790} normal block at 0x0000018E912A1FF0, 32 bytes long.\n Data: <NVIDIA GeForce R> 4E 56 49 44 49 41 20 47 65 46 6F 72 63 65 20 52 \n{7787} normal block at 0x0000018E9132B490, 16 bytes long.\n Data: <                > D0 AD FC 90 8E 01 00 00 00 00 00 00 00 00 00 00 \n{7786} normal block at 0x0000018E9132B3A0, 16 bytes long.\n Data: <                > A8 AD FC 90 8E 01 00 00 00 00 00 00 00 00 00 00 \n{7785} normal block at 0x0000018E90FCADA0, 88 bytes long.\n Data: <          2     > 00 00 00 00 CD CD CD CD A0 B3 32 91 8E 01 00 00 \n{7783} normal block at 0x0000018E90E56420, 16 bytes long.\n Data: <P *             > 50 1D 2A 91 8E 01 00 00 00 00 00 00 00 00 00 00 \n{7782} normal block at 0x0000018E912A1D50, 32 bytes long.\n Data: < d        2     > 20 64 E5 90 8E 01 00 00 F0 B3 32 91 8E 01 00 00 \nObject dump complete.\n```\nI do not have found a way to release myself the memory because I cannot access to the underlaying `ggml_backend_reg *` of `ggml_backend_reg_t` that is hide in the `ggml-backend-impl.h` and not more accessible directly after building the static libraries.\n\nI don't know if it's enough, but a code like that can eventually do the job:\n```\nggml_backend_reg_t reg = ggml_backend_reg_by_name(\"CUDA\");\nfree(reg->context);\n```\nUnfortunatly:\n`E0833 pointer or reference to incomplete type not allowed` !\n\nI hope that help. \n\nThank's a lot for ggml/llama.cpp ! You rock !\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-04-16T21:28:39+00:00",
    "closed_at": "2025-05-31T01:07:42+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12986/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12986"
  },
  {
    "number": 4659,
    "title": "[Question] Plans to parallelize GPU matrix execution in ggml_cuda_op_mul_mat?",
    "body": "I noticed in `ggml_cuda_op_mul_mat` that multiple GPUs execute large matrix operations in a serial manner within a for loop, as confirmed by actual testing. \r\nWhen I execute on 70B with 8 GPUs, the execution time on a single device is approximately 0.03ms (including synchronization time), and the total loop execution time is approximately 0.200ms.\r\nThe code is here:\r\n```\r\n        for (int64_t id = 0; id < g_device_count; ++id) {\r\n```\r\n\r\nA more effective solution would be to implement parallel execution using multiple cards and threads. \r\nIs there any plan for improvement in this regard?\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-28T02:39:41+00:00",
    "closed_at": "2024-04-02T01:09:44+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4659/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4659"
  },
  {
    "number": 13145,
    "title": "Misc. bug: Flash Attention not working on CDNA3 ROCm 6.4 MI300",
    "body": "### Name and Version\n\n```\nllama-server --version\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Instinct MI300X VF, gfx942:sramecc+:xnack- (0x942), VMM: no, Wave Size: 64\nversion: 5201 (85f36e5e)\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n```\n\n### Operating systems\nLinux\n\n### Which llama.cpp modules do you know to be affected?\nllama-server\n\n\n### Problem description & steps to reproduce\n\n\n\n```\nllama-server -m UD-IQ1_S/MAI-DS-R1-UD-IQ1_S-00001-of-00004.gguf -c 32768  -b 8192 -ub 4096  -ngl 999  -to 3600  -a MAI-DS-R1-UD-IQ1_S --no-mmap -t 1 -nkvo -fa\n```\n\nsmall context windows, small prompts, it seems to work, but if i use large context, it seems like it's using CPU. I get about 20-30 tokens/s on small prompts. otherwise, it just hangs for hours\n\n\nwithout `-fa` i have to use much smaller `-c` `-b` `-ub` values to not max out on VRAM, and it runs larger prompts, but only at 7-10 tokens/second.\n\n```build.sh\n#!/bin/bash \ncd ~/llama.cpp\ngit pull origin master\n\n#rm -rf build\nmkdir build\ncd build\n\nHIPCXX=\"$(hipconfig -l)/clang\" \\\nHIP_PATH=\"$(hipconfig -R)\" \\\ncmake -S .. -B . \\\n    -DGGML_HIP=ON \\\n    -DGGML_HIP_ROCWMMA_FATTN=ON \\\n    -DAMDGPU_TARGETS=gfx942 \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_INSTALL_PREFIX=/usr/local \\\n    -DBUILD_SHARED_LIBS=ON \\\n    -DLLAMA_CURL=ON \\\n&& cmake --build . --config Release -j8 \\\n&& sudo cmake --install .\nsudo ldconfig\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-04-28T06:17:46+00:00",
    "closed_at": "2025-06-12T01:07:49+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13145/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13145"
  },
  {
    "number": 10671,
    "title": "Misc. bug: convert_lora_to_gguf ignores outtype",
    "body": "### Name and Version\n\n8f1d81a from 2024-09-01\r\nuntil today\r\n0cd182e\n\n### Operating systems\n\nWindows\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-quantize\n\n### Problem description & steps to reproduce\n\nconvert_lora_to_gguf ignores outtype\r\n\r\n\r\nhi\r\ni want to convert a transformer fp32 lora to a quant gguf lora. i have done this a few months ago and it worked fine with the convert script. now i tryed the same with a up to date version of llama.cpp and ...well... it still works but i wondered why the output file has the same size than the input file. i tested this much to large file and it looks like it still works. at least for me in kobold.cpp. but then i got curious why this quant lora is so large. for me it turned out the new version of llamacpp is ignoring the outtype given for the convert script.\r\n\r\nno matter what --outtype i chose its alway FP32 and the logging while \"converting\" looks like this for all lines.\r\n\r\nthis is with --outtype q8_0\r\nINFO:hf-to-gguf:blk.0.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {14336, 64}\r\nINFO:hf-to-gguf:blk.0.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {64, 4096}\r\nINFO:hf-to-gguf:blk.0.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {4096, 64}\r\nINFO:hf-to-gguf:blk.0.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {64, 14336}\r\n\r\n\r\ni still had my old llama.cpp around and i tested the same. same lora ...same base model ...same script parameters\r\n\r\nINFO:hf-to-gguf:blk.0.ffn_down.weight.lora_a, torch.float32 --> Q8_0, shape = {14336, 64}\r\nINFO:hf-to-gguf:blk.0.ffn_down.weight.lora_b, torch.float32 --> Q8_0, shape = {64, 4096}\r\nINFO:hf-to-gguf:blk.0.ffn_gate.weight.lora_a, torch.float32 --> Q8_0, shape = {4096, 64}\r\nINFO:hf-to-gguf:blk.0.ffn_gate.weight.lora_b, torch.float32 --> Q8_0, shape = {64, 14336}\r\n\r\nthis looks more like an quant converted lora. and the file size is also fine.\r\n\r\nnow i tested a little bit around at which \"version/commit\" this happens.\r\n\r\nmy old \"working\" version is 1d1ccce 2024-08-29\r\n\r\n0ab30f8 2024-08-30 also seems to still work\r\n\r\nbut\r\n\r\nat 8f1d81a from 2024-09-01 the problems starts. the script is ignoring the output type and gives out fp32 lora. \r\n\r\n\r\n\n\n### First Bad Commit\n\n8f1d81a from 2024-09-01\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-05T13:42:56+00:00",
    "closed_at": "2025-01-19T01:07:38+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10671/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10671"
  },
  {
    "number": 13494,
    "title": "Eval bug: BGE-M3 Embedding model is not accessible",
    "body": "### Name and Version\n\nload_backend: loaded RPC backend from D:\\AI\\app\\llama.cpp\\ggml-rpc.dll\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = AMD Radeon RX 6600M (AMD proprietary driver) | uma: 0 | fp16: 1 | warp size: 32 | shared memory: 32768 | int dot: 1 | matrix cores: none\nload_backend: loaded Vulkan backend from D:\\AI\\app\\llama.cpp\\ggml-vulkan.dll\nload_backend: loaded CPU backend from D:\\AI\\app\\llama.cpp\\ggml-cpu-haswell.dll\nversion: 5361 (cf0a43bb)\nbuilt with MSVC 19.43.34808.0 for x64\n\n### Operating systems\n\nWindows\n\n### GGML backends\n\nVulkan\n\n### Hardware\n\nRyzen 7 5800H + RX 6600M\n\n### Models\n\nbge-m3-FP16.gguf\n\n### Problem description & steps to reproduce\n\nFailed to add the embedding model using the llama-b5361-bin-win-cuda12.4-x64 version on a workstation with RTX 4800. The reranking model, LLM model, and VLM model can all be added. Then, testing on my laptop with a Ryzen 7 5800H and RX 6600M using llama-b5361-bin-win-vulkan-x64, the embedding model that I had previously added in Dify cannot connect.\n\n### First Bad Commit\n\nI upgrade every day, at least it's normal on May 5th.\n\n### Relevant log output\n\n```shell\n\"11.Bge-m3\":\n    proxy: \n    aliases:\n    - Bge-m3\n    # `useModelName` overrides the model name in the request\n    # and sends a specific name to the upstream server\n    useModelName: \"Bge-m3\"\n    cmd: >\n      llama-server\n      --host 0.0.0.0\n      --port ${PORT}\n      --model models/gpustack/bge-m3-FP16.gguf\n      --ctx-size 8192\n      --batch-size 8192\n      --rope-scaling yarn\n      --rope-freq-scale 0.75\n      --embeddings\n      -ngl 99\n[INFO] Request ::1 \"GET /upstream HTTP/1.1\" 200 740 \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36 Edg/136.0.0.0\" 0s\nload_backend: loaded RPC backend from D:\\AI\\app\\llama.cpp\\ggml-rpc.dll\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = AMD Radeon RX 6600M (AMD proprietary driver) | uma: 0 | fp16: 1 | warp size: 32 | shared memory: 32768 | int dot: 1 | matrix cores: none\nload_backend: loaded Vulkan backend from D:\\AI\\app\\llama.cpp\\ggml-vulkan.dll\nload_backend: loaded CPU backend from D:\\AI\\app\\llama.cpp\\ggml-cpu-haswell.dll\nbuild: 5361 (cf0a43bb) with MSVC 19.43.34808.0 for x64\nsystem info: n_threads = 8, n_threads_batch = 8, total_threads = 16\n\nsystem_info: n_threads = 8 (n_threads_batch = 8) / 16 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |\n\nmain: binding port with default address family\nmain: HTTP server is listening, hostname: 0.0.0.0, port: 8081, http threads: 15\nmain: loading model\nsrv    load_model: loading model 'models/gpustack/bge-m3-FP16.gguf'\nllama_model_load_from_file_impl: using device Vulkan0 (AMD Radeon RX 6600M) - 8176 MiB free\nllama_model_loader: loaded meta data with 33 key-value pairs and 389 tensors from models/gpustack/bge-m3-FP16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = bert\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                         general.size_label str              = 567M\nllama_model_loader: - kv   3:                            general.license str              = mit\nllama_model_loader: - kv   4:                               general.tags arr[str,4]       = [\"sentence-transformers\", \"feature-ex...\nllama_model_loader: - kv   5:                           bert.block_count u32              = 24\nllama_model_loader: - kv   6:                        bert.context_length u32              = 8192\nllama_model_loader: - kv   7:                      bert.embedding_length u32              = 1024\nllama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 4096\nllama_model_loader: - kv   9:                  bert.attention.head_count u32              = 16\nllama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000010\nllama_model_loader: - kv  11:                          general.file_type u32              = 1\nllama_model_loader: - kv  12:                      bert.attention.causal bool             = false\nllama_model_loader: - kv  13:                          bert.pooling_type u32              = 2\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = t5\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\nsrv  log_server_r: request: GET /health 127.0.0.1 503\n[INFO] <11.Bge-m3> Health check error on http://localhost:8081/health, status code: 503\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,250002]  = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \",\"...\nllama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,250002]  = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,250002]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  19:            tokenizer.ggml.add_space_prefix bool             = true\nllama_model_loader: - kv  20:            tokenizer.ggml.token_type_count u32              = 1\nllama_model_loader: - kv  21:    tokenizer.ggml.remove_extra_whitespaces bool             = true\nllama_model_loader: - kv  22:        tokenizer.ggml.precompiled_charsmap arr[u8,237539]   = [0, 180, 2, 0, 0, 132, 0, 0, 0, 0, 0,...\nllama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 0\nllama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  25:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  26:          tokenizer.ggml.seperator_token_id u32              = 2\nllama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 1\nllama_model_loader: - kv  28:                tokenizer.ggml.cls_token_id u32              = 0\nllama_model_loader: - kv  29:               tokenizer.ggml.mask_token_id u32              = 250001\nllama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = true\nllama_model_loader: - kv  32:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  244 tensors\nllama_model_loader: - type  f16:  145 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = F16\nprint_info: file size   = 1.07 GiB (16.25 BPW)\nload: model vocab missing newline token, using special_pad_id instead\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 4\nload: token to piece cache size = 2.1668 MB\nprint_info: arch             = bert\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 8192\nprint_info: n_embd           = 1024\nprint_info: n_layer          = 24\nprint_info: n_head           = 16\nprint_info: n_head_kv        = 16\nprint_info: n_rot            = 64\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 64\nprint_info: n_embd_head_v    = 64\nprint_info: n_gqa            = 1\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 1.0e-05\nprint_info: f_norm_rms_eps   = 0.0e+00\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 4096\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 0\nprint_info: pooling type     = 2\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 8192\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 335M\nprint_info: model params     = 566.70 M\nprint_info: general.name     = n/a\nprint_info: vocab type       = UGM\nprint_info: n_vocab          = 250002\nprint_info: n_merges         = 0\nprint_info: BOS token        = 0 '<s>'\nprint_info: EOS token        = 2 '</s>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: SEP token        = 2 '</s>'\nprint_info: PAD token        = 1 '<pad>'\nprint_info: MASK token       = 250001 '[PAD250000]'\nprint_info: LF token         = 0 '<s>'\nprint_info: EOG token        = 2 '</s>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 24 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 25/25 layers to GPU\nload_tensors:      Vulkan0 model buffer size =   577.22 MiB\nload_tensors:   CPU_Mapped model buffer size =   520.30 MiB\n.......................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 8192\nllama_context: n_batch       = 8192\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 0\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 10000.0\nllama_context: freq_scale    = 0.75\nllama_context: Vulkan_Host  output buffer size =     0.00 MiB\ncommon_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\ndecode: cannot decode batches with this context (use llama_encode() instead)\nsrv          init: initializing slots, n_slots = 1\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 8192\nmain: model loaded\nmain: chat template, chat_template: {%- for message in messages -%}\n  {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>\n' -}}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n  {{- '<|im_start|>assistant\n' -}}\n{%- endif -%}, example_format: '<|im_start|>system\nYou are a helpful assistant<|im_end|>\n<|im_start|>user\nHello<|im_end|>\n<|im_start|>assistant\nHi there<|im_end|>\n<|im_start|>user\nHow are you?<|im_end|>\n<|im_start|>assistant\n'\nmain: server is listening on http://0.0.0.0:8081 - starting the main loop\nsrv  update_slots: all slots are idle\n[INFO] <11.Bge-m3> Health check passed on http://localhost:8081/health\n\nsrv  log_server_r: request: POST /embeddings 172.31.137.1 200\nslot launch_slot_: id  0 | task 2 | processing task\nslot update_slots: id  0 | task 2 | new prompt, n_ctx_slot = 8192, n_keep = 0, n_prompt_tokens = 26\nslot update_slots: id  0 | task 2 | kv cache rm [0, end)\nslot update_slots: id  0 | task 2 | prompt processing progress, n_past = 26, n_tokens = 26, progress = 1.000000\nslot update_slots: id  0 | task 2 | prompt done, n_past = 26, n_tokens = 26\nslot      release: id  0 | task 2 | stop processing: n_past = 26, truncated = 0\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: POST /embeddings 172.31.137.1 200\nslot launch_slot_: id  0 | task 4 | processing task\nslot update_slots: id  0 | task 4 | new prompt, n_ctx_slot = 8192, n_keep = 0, n_prompt_tokens = 5\nslot update_slots: id  0 | task 4 | kv cache rm [0, end)\nslot update_slots: id  0 | task 4 | prompt processing progress, n_past = 5, n_tokens = 5, progress = 1.000000\nslot update_slots: id  0 | task 4 | prompt done, n_past = 5, n_tokens = 5\nslot      release: id  0 | task 4 | stop processing: n_past = 5, truncated = 0\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: POST /embeddings 172.31.137.1 200\nslot launch_slot_: id  0 | task 6 | processing task\nslot update_slots: id  0 | task 6 | new prompt, n_ctx_slot = 8192, n_keep = 0, n_prompt_tokens = 8\nslot update_slots: id  0 | task 6 | kv cache rm [0, end)\nslot update_slots: id  0 | task 6 | prompt processing progress, n_past = 8, n_tokens = 8, progress = 1.000000\nslot update_slots: id  0 | task 6 | prompt done, n_past = 8, n_tokens = 8\nslot      release: id  0 | task 6 | stop processing: n_past = 8, truncated = 0\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: POST /embeddings 172.31.137.1 200\nslot launch_slot_: id  0 | task 8 | processing task\nslot update_slots: id  0 | task 8 | new prompt, n_ctx_slot = 8192, n_keep = 0, n_prompt_tokens = 7\nslot update_slots: id  0 | task 8 | kv cache rm [0, end)\nslot update_slots: id  0 | task 8 | prompt processing progress, n_past = 7, n_tokens = 7, progress = 1.000000\nslot update_slots: id  0 | task 8 | prompt done, n_past = 7, n_tokens = 7\nslot      release: id  0 | task 8 | stop processing: n_past = 7, truncated = 0\nslot launch_slot_: id  0 | task 10 | processing task\nslot update_slots: id  0 | task 10 | new prompt, n_ctx_slot = 8192, n_keep = 0, n_prompt_tokens = 9\nslot update_slots: id  0 | task 10 | kv cache rm [0, end)\nslot update_slots: id  0 | task 10 | prompt processing progress, n_past = 9, n_tokens = 9, progress = 1.000000\nslot update_slots: id  0 | task 10 | prompt done, n_past = 9, n_tokens = 9\nsrv  log_server_r: request: POST /embeddings 172.31.137.1 200\nslot      release: id  0 | task 10 | stop processing: n_past = 9, truncated = 0\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: POST /embeddings 172.31.137.1 200\nslot launch_slot_: id  0 | task 12 | processing task\nslot update_slots: id  0 | task 12 | new prompt, n_ctx_slot = 8192, n_keep = 0, n_prompt_tokens = 9\nslot update_slots: id  0 | task 12 | kv cache rm [0, end)\nslot update_slots: id  0 | task 12 | prompt processing progress, n_past = 9, n_tokens = 9, progress = 1.000000\nslot update_slots: id  0 | task 12 | prompt done, n_past = 9, n_tokens = 9\nslot      release: id  0 | task 12 | stop processing: n_past = 9, truncated = 0\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: POST /embeddings 172.31.137.1 200\nslot launch_slot_: id  0 | task 14 | processing task\nslot update_slots: id  0 | task 14 | new prompt, n_ctx_slot = 8192, n_keep = 0, n_prompt_tokens = 8\nslot update_slots: id  0 | task 14 | kv cache rm [0, end)\nslot update_slots: id  0 | task 14 | prompt processing progress, n_past = 8, n_tokens = 8, progress = 1.000000\nslot update_slots: id  0 | task 14 | prompt done, n_past = 8, n_tokens = 8\nslot      release: id  0 | task 14 | stop processing: n_past = 8, truncated = 0\nslot launch_slot_: id  0 | task 16 | processing task\nslot update_slots: id  0 | task 16 | new prompt, n_ctx_slot = 8192, n_keep = 0, n_prompt_tokens = 7\nslot update_slots: id  0 | task 16 | kv cache rm [0, end)\nslot update_slots: id  0 | task 16 | prompt processing progress, n_past = 7, n_tokens = 7, progress = 1.000000\nslot update_slots: id  0 | task 16 | prompt done, n_past = 7, n_tokens = 7\nsrv  log_server_r: request: POST /embeddings 172.31.137.1 200\nslot      release: id  0 | task 16 | stop processing: n_past = 7, truncated = 0\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: POST /embeddings 172.31.137.1 200\nslot launch_slot_: id  0 | task 18 | processing task\nslot update_slots: id  0 | task 18 | new prompt, n_ctx_slot = 8192, n_keep = 0, n_prompt_tokens = 9\nslot update_slots: id  0 | task 18 | kv cache rm [0, end)\nslot update_slots: id  0 | task 18 | prompt processing progress, n_past = 9, n_tokens = 9, progress = 1.000000\nslot update_slots: id  0 | task 18 | prompt done, n_past = 9, n_tokens = 9\nslot      release: id  0 | task 18 | stop processing: n_past = 9, truncated = 0\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: POST /embeddings 172.31.137.1 200\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-05-13T05:27:28+00:00",
    "closed_at": "2025-06-27T01:07:56+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13494/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13494"
  },
  {
    "number": 5515,
    "title": "Llava 1.6: server not decoding images, but works via CLI",
    "body": "First, let me say that I really appreciate all the work you guys are putting into llama.cpp -- it's really impressive.\r\n\r\nI'm testing out yesterday's release of llava 1.6 (thanks so much for working on that tricky PR, @cmp-nct), and it's working well via the CLI, but when I run it via the server I'm seeing the below when it receives a request:\r\n\r\n```\r\nclip_image_load_from_bytes: failed to decode image bytes\r\nslot 0 - failed to load image [id: 12]\r\ntask 1 - error: internal_error\r\n```\r\n\r\n## How I'm running via CLI (works)\r\n\r\n```\r\n./llava-cli -m ./models/llava-1-6/mistral-7b-q_5_k.gguf --mmproj ./models/llava-1-6/mmproj-mistral7b-f16.gguf --image ./media/images/ginsberg.png -p \"Who is this?\" --temp 0.1\r\n```\r\n\r\n## How I'm running via Server (doesn't work)\r\n\r\n### To start the server:\r\n```\r\n./server -m ./models/llava-1-6/mistral-7b-q_5_k.gguf --mmproj ./models/llava-1-6/mmproj-mistral7b-f16.gguf --host 127.0.0.1 --port 8080\r\n```\r\n\r\n### The request I'm sending:\r\n```\r\ncurl --request POST \\\r\n  --url http://localhost:8080/completion \\\r\n  --header 'Content-Type: application/json' \\\r\n  --data '{\r\n\t\"prompt\": \"USER:[img-12]Who is this?.\\nASSISTANT:\",\r\n\t\"temperature\": 0.1,\r\n\t\"image_data\": [\r\n\t\t{\r\n\t\t\t\"data\": <BASE64_IMG>,\r\n\t\t\t\"id\": 12\r\n\t\t}\r\n\t]\r\n}'\r\n```\r\n\r\n*BASE64_IMG is the base 64 of the below image*\r\n\r\n![ginsberg](https://github.com/ggerganov/llama.cpp/assets/4513085/448d3b1b-188e-4340-b80e-8cbba8d5efdf)\r\n\r\n\r\n## Details\r\n\r\n* **My system:** 2021 M1 Max MBP w/ 64 GB of RAM, running Sonoma 14.3\r\n* **Llava 1.6 Model files:** both from [this](https://huggingface.co/cmp-nct/llava-1.6-gguf/tree/main) HF repo\r\n  * Model: `mistral-7b-q_5_k.gguf`\r\n  * Mmproj: `mmproj-mistral7b-f16.gguf`\r\n* **Version of llama.cpp**: I'm on the most recent commit as of this issue, commit `4524290e87b8e107cc2b56e1251751546f4b9051`\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-02-15T19:38:58+00:00",
    "closed_at": "2024-02-17T20:49:59+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5515/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5515"
  },
  {
    "number": 14134,
    "title": "Misc. bug: Performance regression on aarch64 q4_0",
    "body": "### Name and Version\n\nllama-cli --version\nversion: 5615 (f470bc36)\nbuilt with Android (13324770, +pgo, +bolt, +lto, +mlgo, based on r530567d) clang version 19.0.0 (https://android.googlesource.com/toolchain/llvm-project 97a699bf4812a18fb657c2779f5296a4ab2694d2) for x86_64-unknown-linux-gnu\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-bench\n\n### Command line\n\n```shell\n\n```\n\n### Problem description & steps to reproduce\n\nQ4_0 performance significantly dropped after this commit\n/build-android-f470bc36/llama-bench -m ../gemma-2-2b-q4_0.gguf -p 512 -n 0                                            \n| model                          |       size |     params | backend    | threads |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |\n| gemma2 2B Q4_0                 |   1.51 GiB |     2.61 B | CPU        |       8 |           pp512 |         15.84 \u00b1 0.01 |\n\nbuild: f470bc36 (5615)\n/build-android-8f47e25f/llama-bench -m ../gemma-2-2b-q4_0.gguf -p 512 -n 0                                            \n| model                          |       size |     params | backend    | threads |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |\n| gemma2 2B Q4_0                 |   1.51 GiB |     2.61 B | CPU        |       8 |           pp512 |        138.02 \u00b1 8.88 |\n\nbuild: 8f47e25f (5614)\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-06-11T21:43:06+00:00",
    "closed_at": "2025-06-17T09:58:33+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14134/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14134"
  },
  {
    "number": 8705,
    "title": "How to utilize GPU on Android to accelerate inference?",
    "body": "### Discussed in https://github.com/ggerganov/llama.cpp/discussions/8704\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **ElaineWu66** July 26, 2024</sup>\r\nI am trying to compile and run llama.cpp demo on my android device (QUALCOMM Adreno) with linux and termux.\r\nAny suggestion on how to utilize the GPU?\r\nI have followed tutorial https://github.com/JackZeng0208/llama.cpp-android-tutorial, since the OpenCL is broken and removed now, it's not working.\r\n\r\nThanks!!!\r\n</div>",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-26T07:30:12+00:00",
    "closed_at": "2024-12-31T01:07:28+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8705/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8705"
  },
  {
    "number": 6473,
    "title": "Add support for OPTForCausalLM",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\nI want to quantize OPT based models like Galactica in ggml format. Please add support to OPT architecture models.\r\n\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-04T09:20:31+00:00",
    "closed_at": "2024-06-08T01:07:02+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6473/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6473"
  },
  {
    "number": 14616,
    "title": "Misc. bug: llama-perplexity PPL score is too high for Falcon H1 TQ1_0 model",
    "body": "### Name and Version\n\nlatest main branch\n\n### Operating systems\n\nLinux with Intel Xeon CPU E7-8890 v3 CPUs with AVX2 extension\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Command line\n\n```shell\n./build/bin/llama-perplexity -m models/Falcon-H1-7B-Instruct-TQ1_0.gguf -t 32 -f wikitext-2-raw/wiki.test.raw\n```\n\n### Problem description & steps to reproduce\n\nHi all,\n\nThanks very much for merging the latest code to support the latest Falcon H1 model. I enjoyed it!\n\nThere is a small issue with the PPL score of the Falcon H1 TQ1_0 model (https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF), some console printout is here:\n**[1]19349056200306.7266,[2]15260594264367.6348,[3]9614462106840.3945,[4]10046092761693.6270,[5]8923093178308.6699,[6]10074241009251.5293,[7]8918590985171.6797,[8]9302717398391.8418,[9]8422255088098.3027,[10]9236596171282.8242,[11]9563579420726.5371,[12]9321558796127.1172,[13]10303646595689.4785,[14]9832904238423.9219,[15]9454125549978.1699,[16]9625034884855.1426, ......**\n\nI test the same llama-perplexity binary with Falcon H1 Q8_0 model, the PPL score is very normal. I think there is some small issue with the TQ1_0 model.\n\n\nThanks very much!\n\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-07-10T14:46:05+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14616/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14616"
  },
  {
    "number": 10701,
    "title": "Eval bug: ROCm error: Could not attach to process (AMD MI50/60: gfx906)",
    "body": "### Name and Version\n\n``` \r\n./build/bin/llama-cli --version\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 2 ROCm devices:\r\n  Device 0: AMD Radeon Graphics, compute capability 9.0, VMM: no\r\n  Device 1: AMD Radeon Graphics, compute capability 9.0, VMM: no\r\nversion: 4277 (c5ede384)\r\nbuilt with cc (Ubuntu 13.2.0-23ubuntu4) 13.2.0 for x86_64-linux-gnu\r\n```\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nHIP\n\n### Hardware\n\nCPU: AMD Ryzen 5950x, 96GB RAM\r\nGPU: 2x AMD MI60 (gfx906), Nvidia 3090 (for video output) \r\n\n\n### Models\n\nMeta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\n\n### Problem description & steps to reproduce\n\nI am getting 'could not attach to process' when I do not use Flash attention in llama.cpp. However, inference works without any errors when flash attention is on.\r\nI built llama.cpp on my Ubuntu 24.04 with this command:\r\n\r\n```\r\nHIPCXX=\"$(hipconfig -l)/clang\" HIP_PATH=\"$(hipconfig -R)\"     cmake -S . -B build -DGGML_HIP=ON -DAMDGPU_TARGETS=gfx906 -DCMAKE_BUILD_TYPE=Release     && cmake --build build --config Release -- -j 16\r\n``` \r\nHere is the log command without FA that throws an error:\r\n\r\n<details>\r\n\r\n```\r\n./build/bin/llama-server -m ../models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf -ngl 99  --ctx_size 2048 -sm none\r\ngml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 2 ROCm devices:\r\n  Device 0: AMD Radeon Graphics, compute capability 9.0, VMM: no\r\n  Device 1: AMD Radeon Graphics, compute capability 9.0, VMM: no\r\nbuild: 4277 (c5ede384) with cc (Ubuntu 13.2.0-23ubuntu4) 13.2.0 for x86_64-linux-gnu\r\nsystem info: n_threads = 16, n_threads_batch = 16, total_threads = 32\r\n\r\nsystem_info: n_threads = 16 (n_threads_batch = 16) / 32 | ROCm : PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \r\n\r\nmain: HTTP server is listening, hostname: 127.0.0.1, port: 8080, http threads: 31\r\nmain: loading model\r\nsrv    load_model: loading model '../models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf'\r\nllama_load_model_from_file: using device ROCm0 (AMD Radeon Graphics) - 32450 MiB free\r\nllama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from ../models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\r\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\r\nllama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\r\nllama_model_loader: - kv   5:                         general.size_label str              = 8B\r\nllama_model_loader: - kv   6:                            general.license str              = llama3.1\r\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\r\nllama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\r\nllama_model_loader: - kv   9:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv  10:                       llama.context_length u32              = 131072\r\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\r\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  17:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\r\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\r\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"\u0120 \u0120\", \"\u0120 \u0120\u0120\u0120\", \"\u0120\u0120 \u0120\u0120\", \"...\r\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\r\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\r\nllama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\r\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\r\nllama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\r\nllama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\r\nllama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\r\nllama_model_loader: - type  f32:   66 tensors\r\nllama_model_loader: - type q4_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: special tokens cache size = 256\r\nllm_load_vocab: token to piece cache size = 0.7999 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 128256\r\nllm_load_print_meta: n_merges         = 280147\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 500000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 8B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 8.03 B\r\nllm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \r\nllm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\r\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\r\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\r\nllm_load_print_meta: LF token         = 128 '\u00c4'\r\nllm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\r\nllm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: max token length = 256\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading output layer to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:        ROCm0 model buffer size =  4403.49 MiB\r\nllm_load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\r\n.......................................................................................\r\nllama_new_context_with_model: n_seq_max     = 1\r\nllama_new_context_with_model: n_ctx         = 2048\r\nllama_new_context_with_model: n_ctx_per_seq = 2048\r\nllama_new_context_with_model: n_batch       = 2048\r\nllama_new_context_with_model: n_ubatch      = 512\r\nllama_new_context_with_model: flash_attn    = 0\r\nllama_new_context_with_model: freq_base     = 500000.0\r\nllama_new_context_with_model: freq_scale    = 1\r\nllama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\r\nllama_kv_cache_init:      ROCm0 KV buffer size =   256.00 MiB\r\nllama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\r\nllama_new_context_with_model:  ROCm_Host  output buffer size =     0.49 MiB\r\nllama_new_context_with_model:      ROCm0 compute buffer size =   258.50 MiB\r\nllama_new_context_with_model:  ROCm_Host compute buffer size =    12.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 2\r\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\n/home/saidp/Downloads/amd_ai_tools/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:70: ROCm error\r\nROCm error: CUBLAS_STATUS_INTERNAL_ERROR\r\n  current device: 0, in function ggml_cuda_mul_mat_batched_cublas at /home/saidp/Downloads/amd_ai_tools/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:1717\r\n  hipblasGemmBatchedEx(ctx.cublas_handle(), HIPBLAS_OP_T, HIPBLAS_OP_N, ne01, ne11, ne10, alpha, (const void **) (ptrs_src.get() + 0*ne23), HIPBLAS_R_16F, nb01/nb00, (const void **) (ptrs_src.get() + 1*ne23), HIPBLAS_R_16F, nb11/nb10, beta, ( void **) (ptrs_dst.get() + 0*ne23), cu_data_type, ne01, ne23, cu_compute_type, HIPBLAS_GEMM_DEFAULT)\r\nCould not attach to process.  If your uid matches the uid of the target\r\nprocess, check the setting of /proc/sys/kernel/yama/ptrace_scope, or try\r\nagain as the root user.  For more details, see /etc/sysctl.d/10-ptrace.conf\r\nptrace: Operation not permitted.\r\nNo stack.\r\nThe program is not being run.\r\nAborted (core dumped)\r\n```\r\n\r\n</details>\r\n\r\nHere is the log with flash attention enabled:\r\n\r\n<details>\r\n\r\n```\r\n./build/bin/llama-server -m ../models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf -ngl 99  --ctx_size 2048 -sm none -fa\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 2 ROCm devices:\r\n  Device 0: AMD Radeon Graphics, compute capability 9.0, VMM: no\r\n  Device 1: AMD Radeon Graphics, compute capability 9.0, VMM: no\r\nbuild: 4277 (c5ede384) with cc (Ubuntu 13.2.0-23ubuntu4) 13.2.0 for x86_64-linux-gnu\r\nsystem info: n_threads = 16, n_threads_batch = 16, total_threads = 32\r\n\r\nsystem_info: n_threads = 16 (n_threads_batch = 16) / 32 | ROCm : PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \r\n\r\nmain: HTTP server is listening, hostname: 127.0.0.1, port: 8080, http threads: 31\r\nmain: loading model\r\nsrv    load_model: loading model '../models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf'\r\nllama_load_model_from_file: using device ROCm0 (AMD Radeon Graphics) - 32450 MiB free\r\nllama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from ../models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\r\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\r\nllama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\r\nllama_model_loader: - kv   5:                         general.size_label str              = 8B\r\nllama_model_loader: - kv   6:                            general.license str              = llama3.1\r\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\r\nllama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\r\nllama_model_loader: - kv   9:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv  10:                       llama.context_length u32              = 131072\r\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\r\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  17:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\r\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\r\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"\u0120 \u0120\", \"\u0120 \u0120\u0120\u0120\", \"\u0120\u0120 \u0120\u0120\", \"...\r\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\r\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\r\nllama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\r\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\r\nllama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\r\nllama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\r\nllama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\r\nllama_model_loader: - type  f32:   66 tensors\r\nllama_model_loader: - type q4_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: special tokens cache size = 256\r\nllm_load_vocab: token to piece cache size = 0.7999 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 128256\r\nllm_load_print_meta: n_merges         = 280147\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 500000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 8B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 8.03 B\r\nllm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \r\nllm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\r\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\r\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\r\nllm_load_print_meta: LF token         = 128 '\u00c4'\r\nllm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\r\nllm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: max token length = 256\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading output layer to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:        ROCm0 model buffer size =  4403.49 MiB\r\nllm_load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\r\n.......................................................................................\r\nllama_new_context_with_model: n_seq_max     = 1\r\nllama_new_context_with_model: n_ctx         = 2048\r\nllama_new_context_with_model: n_ctx_per_seq = 2048\r\nllama_new_context_with_model: n_batch       = 2048\r\nllama_new_context_with_model: n_ubatch      = 512\r\nllama_new_context_with_model: flash_attn    = 1\r\nllama_new_context_with_model: freq_base     = 500000.0\r\nllama_new_context_with_model: freq_scale    = 1\r\nllama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\r\nllama_kv_cache_init:      ROCm0 KV buffer size =   256.00 MiB\r\nllama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\r\nllama_new_context_with_model:  ROCm_Host  output buffer size =     0.49 MiB\r\nllama_new_context_with_model:      ROCm0 compute buffer size =   258.50 MiB\r\nllama_new_context_with_model:  ROCm_Host compute buffer size =    12.01 MiB\r\nllama_new_context_with_model: graph nodes  = 903\r\nllama_new_context_with_model: graph splits = 2\r\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\nsrv          init: initializing slots, n_slots = 1\r\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 2048\r\nmain: model loaded\r\nmain: chat template, built_in: 1, chat_example: '<|start_header_id|>system<|end_header_id|>\r\n\r\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\r\n\r\nHello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\r\n\r\nHi there<|eot_id|><|start_header_id|>user<|end_header_id|>\r\n\r\nHow are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\r\n\r\n'\r\nmain: server is listening on http://127.0.0.1:8080 - starting the main loop\r\nsrv  update_slots: all slots are idle\r\n```\r\n\r\n</details>\r\n\r\nThere is also a bug in the server UI app.  When I ask any question in the UI app, the model generates some readable text but at the end the app shows an error message screen with 'TypeError: Cannot read properties of undefined (reading '0')'.\r\n\r\nNote that I am using rocm-6.2.4. I will check with ROCM 6.3 soon.\n\n### First Bad Commit\n\nI don't have an exact commit version. Both of these issues did not exist in the previous llama.cpp versions where I could build it with `make -j16 GGML_HIPBLAS=1`.\n\n### Relevant log output\n\n```shell\nRefer to 'Problem description'.\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-07T02:34:51+00:00",
    "closed_at": "2025-01-22T01:07:18+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10701/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10701"
  },
  {
    "number": 10334,
    "title": "Feature Request: Adding support for Ternary DiT models",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nllama.cpp has already supported ternary quantization for LLMs, e.g., Bitnet b1.58. We have trained a Ternary diffusion transformer model [TerDiT](https://github.com/Lucky-Lance/TerDiT). Due to the limitations of our engineering abilities, I am wondering if llama.cpp can support the deployment of this model, this can help our research a lot.\n\n### Motivation\n\nTernary quantization has become popular and has demonstrated computational speedups and power reductions, as demonstrated in works like llama.cpp and [bitnet.cpp](https://github.com/microsoft/BitNet). We trained the first ternary DiT network, DiT is a popular structure  nowadays for text to image generation. We would like to know if we can be assisted in realizing the deployment of llama.cpp.\n\n### Possible Implementation\n\nWe have limited engineering abilities. The implementation of TerDiT is similar to LLaMA. We think the implementation of llama.cpp and bitnet.cpp can be helpful\ud83d\ude0a.",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2024-11-16T13:26:44+00:00",
    "closed_at": "2024-11-20T14:43:33+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10334/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10334"
  },
  {
    "number": 4289,
    "title": "[server] Batching reduces context size?",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x ] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x ] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n\r\n***\r\n\r\nHello, this is more of a behavior question than a bug. I noticed that when enabling batching via the `--parallel` flag for the llama.cpp server, it divides the context up between slots.\r\n\r\nDoes this mean the effective context size is reduced? Or can, say, a 8k context model run at 64K with 8 slots?\r\n\r\nThis should be made clear in the documentation, as I can't find an existing issue for it.\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2023-12-02T03:24:43+00:00",
    "closed_at": "2023-12-06T05:31:05+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4289/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4289"
  },
  {
    "number": 10555,
    "title": "Compile bug: ggml-impl.h(314): error: identifier \"__fp16\" is undefined on Jetson AGX Xavier",
    "body": "### Git commit\n\nCommit 9f91251\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Problem description & steps to reproduce\n\nI am trying to compile\r\nllama.cpp\r\non an NVIDIA Jetson AGX Xavier and I am getting the error:\r\n\r\n/tmp/llama.cpp/ggml/src/ggml-cuda/../ggml-impl.h(314): error: identifier \"__fp16\" is undefined\r\nThere were other errors originally, but since\r\nCommit 9f91251\r\n, those errors have disappeared. However, now I am encountering the error mentioned above.\r\n\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\ncmake -B build -DGGML_CUDA=ON -DGGML_CCACHE=OFF\r\n-- The C compiler identification is GNU 9.4.0\r\n-- The CXX compiler identification is GNU 9.4.0\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working C compiler: /usr/bin/cc - skipped\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Found Git: /usr/bin/git (found version \"2.25.1\") \r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - found\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\r\n-- Check if compiler accepts -pthread\r\n-- Check if compiler accepts -pthread - yes\r\n-- Found Threads: TRUE  \r\n-- CMAKE_SYSTEM_PROCESSOR: aarch64\r\n-- Found OpenMP_C: -fopenmp (found version \"4.5\") \r\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \r\n-- Found OpenMP: TRUE (found version \"4.5\")  \r\n-- OpenMP found\r\n-- Using llamafile\r\n-- ARM detected\r\n-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E\r\n-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed\r\n-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels\r\n-- Including CPU backend\r\nCMake Warning at ggml/src/ggml-amx/CMakeLists.txt:104 (message):\r\n  AMX requires x86 and gcc version > 11.0.  Turning off GGML_AMX.\r\n\r\n\r\n-- Found CUDAToolkit: /usr/local/cuda-11.4/include (found version \"11.4.315\") \r\n-- CUDA Toolkit found\r\n-- Using CUDA architectures: 52;61;70;75\r\n-- The CUDA compiler identification is NVIDIA 11.4.315\r\n-- Detecting CUDA compiler ABI info\r\n-- Detecting CUDA compiler ABI info - done\r\n-- Check for working CUDA compiler: /usr/local/cuda-11.4/bin/nvcc - skipped\r\n-- Detecting CUDA compile features\r\n-- Detecting CUDA compile features - done\r\n-- CUDA host compiler is GNU 9.4.0\r\n\r\n-- Including CUDA backend\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /tmp/llama.cpp/build\r\nnvidia@ubuntu:/tmp/llama.cpp$ cmake --build build --config Release\r\n[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\r\n[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\r\n[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\r\n[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\r\n[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\r\n[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\r\n[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o\r\n[  3%] Linking CXX shared library libggml-base.so\r\n[  3%] Built target ggml-base\r\n[  4%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o\r\n[  4%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o\r\n[  5%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o\r\n[  5%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o\r\n[  5%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o\r\n[  6%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o\r\n[  6%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o\r\n[  7%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o\r\n[  7%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o\r\n[  7%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o\r\n[  8%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o\r\n[  8%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o\r\n[  9%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o\r\n[  9%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o\r\n[  9%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o\r\n[ 10%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o\r\n[ 10%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o\r\n[ 11%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o\r\n/tmp/llama.cpp/ggml/src/ggml-cuda/../ggml-impl.h(314): error: identifier \"__fp16\" is undefined\r\n\r\n1 error detected in the compilation of \"/tmp/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu\".\r\nmake[2]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:314: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:1760: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/all] Error 2\r\nmake: *** [Makefile:146: all] Error 2\n```\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-11-28T03:33:52+00:00",
    "closed_at": "2024-12-04T00:41:38+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10555/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10555"
  },
  {
    "number": 10681,
    "title": " Eval bug:  when using convert_hf_to_gguf.py  convert  llama-3.2-11B-vision to gguf",
    "body": "### Name and Version\r\n\r\nlatest\r\n\r\n### Operating systems\r\n\r\nLinux\r\n\r\n### GGML backends\r\n\r\nCUDA\r\n\r\n### Hardware\r\n\r\nA6000\r\n\r\n### Models\r\n\r\nhttps://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct\r\n\r\n### Problem description & steps to reproduce\r\n\r\nwhen using convert_hf_to_gguf.py  convert  llama-3.2-11B-vision to gguf,  it can be converted, but when  inference with image,\r\n\r\nimport ollama\r\nresponse = ollama.chat(\r\n    model='llama-3.2-11B-V:latest',\r\n    messages=[{\r\n        'role': 'user',\r\n        'content': 'What is in this image?',\r\n        'images': ['/home/user/1.jpg']\r\n    }]\r\n)\r\n\r\nits outputs is nothing to do with image content,but chat with text, it seems to reasonable\r\n\r\n\r\n### First Bad Commit\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nits outputs is nothing to do with image content,but chat with text, it seems to reasonable:\r\noutput:\r\n\r\nmodel='llama-3.2-11B-V:latest' created_at='2024-12-06T03:09:09.933807094Z' done=True done_reason='stop' total_duration=1022752844 load_duration=34911768 prompt_eval_count=21 prompt_eval_duration=5000000 eval_count=19 eval_duration=510000000 message=Message(role='assistant', content='It\\'s a photo of a book titled \"The Joy Luck Club\" by Amy Tan.', images=None, tool_calls=None)\r\n\r\n```\r\n\r\n![Uploading tr_img_00001.jpg\u2026]()\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-06T03:06:32+00:00",
    "closed_at": "2025-02-21T01:07:23+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10681/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10681"
  },
  {
    "number": 9451,
    "title": "Bug: server crash when changing LoRA scale while using CUDA",
    "body": "### What happened?\r\n\r\nThe server chases when changing the LoRA scale and using CUDA. To reproduce it:\r\n  - Start the server with a model and a LoRA and load layers to CUDA. \r\n  - Then, prompt the model as usual.\r\n  - After that, modify the scale of the LoRA to 0.0.\r\n  - Finally, prompt the model again and you will get the error.\r\n \r\nThis also happens if you start the LoRA with a scale of 0.0 and modify the scale to a value greater than 0.0. When layers are not loaded in GPU, it works well. In the provided example only happens when 32 and 33 layers are loaded in GPU.\r\n\r\n### Name and Version\r\n\r\n```shell\r\n~/llama.cpp$ ./llama-cli --version\r\nversion: 3735 (df4b7945)\r\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\r\n```\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nLinux\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n~/llama.cpp$ ./llama-server -m ../.cache/huggingface/hub/models--bartowski--Phi-3.5-mini-instruct-GGUF/snapshots/5c8d5381f90b6ca4348f090238be34eada23d192/Phi-3.5-mini-instruct-Q4_K_M.gguf -c 4096 --n-gpu-layers 33 --lora-scaled ../.cache/huggingface/hub/models--zhhan--adapter-Phi-3-mini-4k-instruct_code_writing/snapshots/fdabd0c9eaaa3bf6ce180f15f76adf62e1d57166/Phi-3-mini-4k-instruct-adaptor-f16-code_writer.gguf 0.5\r\nINFO [                    main] build info | tid=\"127234101325824\" timestamp=1726136860 build=3735 commit=\"df4b7945\"\r\nINFO [                    main] system info | tid=\"127234101325824\" timestamp=1726136860 n_threads=8 n_threads_batch=8 total_threads=32 system_info=\"AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \"\r\nINFO [                    main] HTTP server is listening | tid=\"127234101325824\" timestamp=1726136860 n_threads_http=\"31\" port=\"8080\" hostname=\"127.0.0.1\"\r\nINFO [                    main] loading model | tid=\"127234101325824\" timestamp=1726136860 n_threads_http=\"31\" port=\"8080\" hostname=\"127.0.0.1\"\r\nllama_model_loader: loaded meta data with 40 key-value pairs and 197 tensors from ../.cache/huggingface/hub/models--bartowski--Phi-3.5-mini-instruct-GGUF/snapshots/5c8d5381f90b6ca4348f090238be34eada23d192/Phi-3.5-mini-instruct-Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = phi3\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Phi 3.5 Mini Instruct\r\nllama_model_loader: - kv   3:                           general.finetune str              = instruct\r\nllama_model_loader: - kv   4:                           general.basename str              = Phi-3.5\r\nllama_model_loader: - kv   5:                         general.size_label str              = mini\r\nllama_model_loader: - kv   6:                            general.license str              = mit\r\nllama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...\r\nllama_model_loader: - kv   8:                               general.tags arr[str,3]       = [\"nlp\", \"code\", \"text-generation\"]\r\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"multilingual\"]\r\nllama_model_loader: - kv  10:                        phi3.context_length u32              = 131072\r\nllama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096\r\nllama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072\r\nllama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192\r\nllama_model_loader: - kv  14:                           phi3.block_count u32              = 32\r\nllama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32\r\nllama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96\r\nllama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000\r\nllama_model_loader: - kv  20:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144\r\nllama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238\r\nllama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\r\nllama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000\r\nllama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000\r\nllama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\r\nllama_model_loader: - kv  35:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  36:                      quantize.imatrix.file str              = /models_out/Phi-3.5-mini-instruct-GGU...\r\nllama_model_loader: - kv  37:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\r\nllama_model_loader: - kv  38:             quantize.imatrix.entries_count i32              = 128\r\nllama_model_loader: - kv  39:              quantize.imatrix.chunks_count i32              = 151\r\nllama_model_loader: - type  f32:   67 tensors\r\nllama_model_loader: - type q4_K:   81 tensors\r\nllama_model_loader: - type q5_K:   32 tensors\r\nllama_model_loader: - type q6_K:   17 tensors\r\nllm_load_vocab: special tokens cache size = 14\r\nllm_load_vocab: token to piece cache size = 0.1685 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = phi3\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32064\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 3072\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_rot            = 96\r\nllm_load_print_meta: n_swa            = 262144\r\nllm_load_print_meta: n_embd_head_k    = 96\r\nllm_load_print_meta: n_embd_head_v    = 96\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 3072\r\nllm_load_print_meta: n_embd_v_gqa     = 3072\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 8192\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 3B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 3.82 B\r\nllm_load_print_meta: model size       = 2.23 GiB (5.01 BPW) \r\nllm_load_print_meta: general.name     = Phi 3.5 Mini Instruct\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_print_meta: EOT token        = 32007 '<|end|>'\r\nllm_load_print_meta: max token length = 48\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 4070 Laptop GPU, compute capability 8.9, VMM: yes\r\nllm_load_tensors: ggml ctx size =    0.21 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:        CPU buffer size =    52.84 MiB\r\nllm_load_tensors:      CUDA0 buffer size =  2228.84 MiB\r\n............................................................................................\r\nllama_new_context_with_model: n_ctx      = 4096\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:      CUDA0 KV buffer size =  1536.00 MiB\r\nllama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB\r\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.24 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =   300.00 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =    14.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1286\r\nllama_new_context_with_model: graph splits = 2\r\nllama_lora_adapter_init_internal: loading lora adapter from '../.cache/huggingface/hub/models--zhhan--adapter-Phi-3-mini-4k-instruct_code_writing/snapshots/fdabd0c9eaaa3bf6ce180f15f76adf62e1d57166/Phi-3-mini-4k-instruct-adaptor-f16-code_writer.gguf' ...\r\nllama_lora_adapter_init_internal:      CUDA0 LoRA buffer size =    48.00 MiB\r\nllama_lora_adapter_init_internal: loaded 256 tensors from lora file\r\nINFO [                    init] initializing slots | tid=\"127234101325824\" timestamp=1726136860 n_slots=1\r\nINFO [                    init] new slot | tid=\"127234101325824\" timestamp=1726136860 id_slot=0 n_ctx_slot=4096\r\nINFO [                    main] model loaded | tid=\"127234101325824\" timestamp=1726136860\r\nINFO [                    main] chat template | tid=\"127234101325824\" timestamp=1726136860 chat_example=\"<|system|>\\nYou are a helpful assistant<|end|>\\n<|user|>\\nHello<|end|>\\n<|assistant|>\\nHi there<|end|>\\n<|user|>\\nHow are you?<|end|>\\n<|assistant|>\\n\" built_in=true\r\nINFO [            update_slots] all slots are idle | tid=\"127234101325824\" timestamp=1726136860\r\nINFO [   launch_slot_with_task] slot is processing task | tid=\"127234101325824\" timestamp=1726136906 id_slot=0 id_task=0\r\nINFO [            update_slots] kv cache rm [p0, end) | tid=\"127234101325824\" timestamp=1726136906 id_slot=0 id_task=0 p0=0\r\nINFO [                 release] slot released | tid=\"127234101325824\" timestamp=1726136908 id_slot=0 id_task=0 n_past=139 truncated=false\r\nINFO [           print_timings] prompt eval time     =      44.79 ms /    12 tokens (    3.73 ms per token,   267.90 tokens per second) | tid=\"127234101325824\" timestamp=1726136908 id_slot=0 id_task=0 t_prompt_processing=44.793 n_prompt_tokens_processed=12 t_token=3.73275 n_tokens_second=267.89900207621724\r\nINFO [           print_timings] generation eval time =    2263.79 ms /   128 runs   (   17.69 ms per token,    56.54 tokens per second) | tid=\"127234101325824\" timestamp=1726136908 id_slot=0 id_task=0 t_token_generation=2263.787 n_decoded=128 t_token=17.6858359375 n_tokens_second=56.54242205649207\r\nINFO [           print_timings]           total time =    2308.58 ms | tid=\"127234101325824\" timestamp=1726136908 id_slot=0 id_task=0 t_prompt_processing=44.793 t_token_generation=2263.787 t_total=2308.58\r\nINFO [            update_slots] all slots are idle | tid=\"127234101325824\" timestamp=1726136908\r\nINFO [      log_server_request] request | tid=\"127233226178560\" timestamp=1726136908 remote_addr=\"127.0.0.1\" remote_port=51384 status=200 method=\"POST\" path=\"/completion\" params={}\r\nINFO [            update_slots] all slots are idle | tid=\"127234101325824\" timestamp=1726137083\r\nINFO [      log_server_request] request | tid=\"127233236664320\" timestamp=1726137083 remote_addr=\"127.0.0.1\" remote_port=56872 status=200 method=\"POST\" path=\"/lora-adapters\" params={}\r\n\r\n\r\n\r\n\r\nINFO [      log_server_request] request | tid=\"127233215692800\" timestamp=1726137135 remote_addr=\"127.0.0.1\" remote_port=47630 status=200 method=\"GET\" path=\"/lora-adapters\" params={}\r\nINFO [   launch_slot_with_task] slot is processing task | tid=\"127234101325824\" timestamp=1726137142 id_slot=0 id_task=130\r\nINFO [            update_slots] kv cache rm [p0, end) | tid=\"127234101325824\" timestamp=1726137142 id_slot=0 id_task=130 p0=0\r\nCUDA error: invalid argument\r\n  current device: 0, in function ggml_backend_cuda_graph_compute at ggml/src/ggml-cuda.cu:2727\r\n  cudaGraphKernelNodeSetParams(cuda_ctx->cuda_graph->nodes[i], &cuda_ctx->cuda_graph->params[i])\r\nggml/src/ggml-cuda.cu:103: CUDA error\r\nCould not attach to process.  If your uid matches the uid of the target\r\nprocess, check the setting of /proc/sys/kernel/yama/ptrace_scope, or try\r\nagain as the root user.  For more details, see /etc/sysctl.d/10-ptrace.conf\r\nptrace: Operation not permitted.\r\nNo stack.\r\nThe program is not being run.\r\nAborted (core dumped)\r\n```\r\n\r\n\r\n```shell\r\n# here you have the curl requests\r\n~/llama.cpp$ curl --request POST \\\r\n    --url http://localhost:8080/completion \\\r\n    --header \"Content-Type: application/json\" \\\r\n    --data '{\"prompt\": \"Write the python code to show an image using cv2:\",\"n_predict\": 128}'\r\n{\"content\":\"\\n\\nimport cv2\\n\\n# Load an image from file\\nimage = cv2.imread('path/to/image.jpg')\\n\\n# Display the image in a window\\ncv2.imshow('Image', image)\\n\\n# Wait for a key press and then close the window\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n\\nIn this code, we first import the OpenCV library as cv2. Then, we load an image from a file using the imread() function and store it in the 'image' variable. \\n\\nNext, we\",\"id_slot\":0,\"stop\":true,\"model\":\"../.cache/huggingface/hub/models--bartowski--Phi-3.5-mini-instruct-GGUF/snapshots/5c8d5381f90b6ca4348f090238be34eada23d192/Phi-3.5-mini-instruct-Q4_K_M.gguf\",\"tokens_predicted\":128,\"tokens_evaluated\":12,\"generation_settings\":{\"n_ctx\":4096,\"n_predict\":-1,\"model\":\"../.cache/huggingface/hub/models--bartowski--Phi-3.5-mini-instruct-GGUF/snapshots/5c8d5381f90b6ca4348f090238be34eada23d192/Phi-3.5-mini-instruct-Q4_K_M.gguf\",\"seed\":4294967295,\"seed_cur\":2038132409,\"temperature\":0.800000011920929,\"dynatemp_range\":0.0,\"dynatemp_exponent\":1.0,\"top_k\":40,\"top_p\":0.949999988079071,\"min_p\":0.05000000074505806,\"tfs_z\":1.0,\"typical_p\":1.0,\"repeat_last_n\":64,\"repeat_penalty\":1.0,\"presence_penalty\":0.0,\"frequency_penalty\":0.0,\"mirostat\":0,\"mirostat_tau\":5.0,\"mirostat_eta\":0.10000000149011612,\"penalize_nl\":false,\"stop\":[],\"max_tokens\":128,\"n_keep\":0,\"n_discard\":0,\"ignore_eos\":false,\"stream\":false,\"n_probs\":0,\"min_keep\":0,\"grammar\":\"\",\"samplers\":[\"top_k\",\"tfs_z\",\"typ_p\",\"top_p\",\"min_p\",\"temperature\"]},\"prompt\":\"Write the python code to show an image using cv2:\",\"truncated\":false,\"stopped_eos\":false,\"stopped_word\":false,\"stopped_limit\":true,\"stopping_word\":\"\",\"tokens_cached\":139,\"timings\":{\"prompt_n\":12,\"prompt_ms\":44.793,\"prompt_per_token_ms\":3.73275,\"prompt_per_second\":267.89900207621724,\"predicted_n\":128,\"predicted_ms\":2263.787,\"predicted_per_token_ms\":17.6858359375,\"premiguel:\r\n\r\n~/llama.cpp$ curl --request POST \\\r\n    --url http://localhost:8080/lora-adapters \\\r\n    --header \"Content-Type: application/json\" \\\r\n    --data '[{\"id\": 0, \"scale\": 0.0}]'\r\n\r\n~/llama.cpp$ curl --request GET \\ --request GET \\\r\n    --url http://localhost:8080/lora-adapters \\\r\n    --header \"Content-Type: application/json\"\r\n[{\"id\":0,\"path\":\"../.cache/huggingface/hub/models--zhhan--adapter-Phi-3-mini-4k-instruct_code_writing/snapshots/fdabd0c9eaaa3bf6ce180f15f76adf62e1d57166/Phi-3-mini-4k-instruct-adaptor-f16-code_writer.gguf\r\n\r\n~/llama.cpp$ curl --request POST \\\r\n    --url http://localhost:8080/completion \\\r\n    --header \"Content-Type: application/json\" \\\r\n    --data '{\"prompt\": \"Write the python code to show an image using cv2:\",\"n_predict\": 128}'\r\ncurl: (52) Empty reply from server\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-09-12T10:42:14+00:00",
    "closed_at": "2024-09-21T00:41:08+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9451/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9451"
  },
  {
    "number": 474,
    "title": "7B model returning complete non-sense",
    "body": "i followed a YouTube video to build the program https://www.youtube.com/watch?v=coIj2CU5LMU&t=186s. it itself follows the issue #103 \r\n\r\n# Expected Behavior\r\n\r\nAs a test I ran the ./chat.sh in git bash, it ran but when I said the AI \"hello\" I expected hello back.\r\n\r\n# Current Behavior\r\n\r\nit responded with\r\n```\r\n\u203c \u25bc\u2192\u25ac\u25ac\u25b2\u21a8\u203c\u2191\u2665\u2660\u2666\"\u2665 \u263b \u00d4\u00fc\u00e7 \u221f \u00d4\u00fc\u00e7 \u2194\u00b6\r\n\u203c\u221f \u00d4\u00fc\u00e7 \u2665\r\n\u25ba\u2660\r\n\u25bc!\u2195\u263b    \u25bc \u2193     $\u25bc\u221f\u25bc\u2195\u2663\u2194\"\u203c\u2194\u2665\r\n\u263a       \u25ba        \u2194 \u00d4\u00fc\u00e7   #\u2191\"\u25bc\u2191\u2660$$\u25ac\u263a\u263b\r\n```\r\n\r\n# Environment and Context \r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\nI\u2019m running a i7-13 th gen with 32 go of ram and a 3060.\r\n\r\nwindows 11 home\r\n\r\ngit bash to run the commands and cmake to compile\r\n\r\n```\r\nPython 3.10.10\r\ncmake 3.26.1\r\ng++.exe (MinGW.org GCC-6.3.0-1) 6.3.0\r\n```\r\n\r\n# Failure Logs\r\n\r\n```\r\n$ ./chat.sh\r\nmain: seed = 1679687646\r\nllama_model_load: loading model from './models/7B/ggml-model-f16.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 1\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: ggml ctx size = 13365.09 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: loading model part 1/1 from './models/7B/ggml-model-f16.bin'\r\nllama_model_load:  done\r\nllama_model_load: model size =     0.00 MB / num tensors = 0\r\n\r\nsystem_info: n_threads = 4 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | VSX = 0 |\r\n\r\nmain: prompt: ' Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.\r\n\r\nUser: Hello, Bob.\r\nBob: Hello. How may I help you today?\r\nUser: Please tell me the largest city in Europe.\r\nBob: Sure. The largest city in Europe is Moscow, the capital of Russia.\r\nUser:'\r\nmain: number of tokens in prompt = 99\r\n     1 -> ''\r\n  4103 -> ' Trans'\r\n   924 -> 'cript'\r\n   310 -> ' of'\r\n   263 -> ' a'\r\n  7928 -> ' dialog'\r\n 29892 -> ','\r\n   988 -> ' where'\r\n   278 -> ' the'\r\n  4911 -> ' User'\r\n 16254 -> ' interact'\r\n 29879 -> 's'\r\n   411 -> ' with'\r\n   385 -> ' an'\r\n  4007 -> ' Ass'\r\n 22137 -> 'istant'\r\n  4257 -> ' named'\r\n  7991 -> ' Bob'\r\n 29889 -> '.'\r\n  7991 -> ' Bob'\r\n   338 -> ' is'\r\n  8444 -> ' helpful'\r\n 29892 -> ','\r\n  2924 -> ' kind'\r\n 29892 -> ','\r\n 15993 -> ' honest'\r\n 29892 -> ','\r\n  1781 -> ' good'\r\n   472 -> ' at'\r\n  5007 -> ' writing'\r\n 29892 -> ','\r\n   322 -> ' and'\r\n  2360 -> ' never'\r\n  8465 -> ' fails'\r\n   304 -> ' to'\r\n  1234 -> ' answer'\r\n   278 -> ' the'\r\n  4911 -> ' User'\r\n 29915 -> '''\r\n 29879 -> 's'\r\n  7274 -> ' requests'\r\n  7389 -> ' immediately'\r\n   322 -> ' and'\r\n   411 -> ' with'\r\n 16716 -> ' precision'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n 15043 -> ' Hello'\r\n 29892 -> ','\r\n  7991 -> ' Bob'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n 29362 -> 'Bob'\r\n 29901 -> ':'\r\n 15043 -> ' Hello'\r\n 29889 -> '.'\r\n  1128 -> ' How'\r\n  1122 -> ' may'\r\n   306 -> ' I'\r\n  1371 -> ' help'\r\n   366 -> ' you'\r\n  9826 -> ' today'\r\n 29973 -> '?'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n  3529 -> ' Please'\r\n  2649 -> ' tell'\r\n   592 -> ' me'\r\n   278 -> ' the'\r\n 10150 -> ' largest'\r\n  4272 -> ' city'\r\n   297 -> ' in'\r\n  4092 -> ' Europe'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n 29362 -> 'Bob'\r\n 29901 -> ':'\r\n 18585 -> ' Sure'\r\n 29889 -> '.'\r\n   450 -> ' The'\r\n 10150 -> ' largest'\r\n  4272 -> ' city'\r\n   297 -> ' in'\r\n  4092 -> ' Europe'\r\n   338 -> ' is'\r\n 25820 -> ' Moscow'\r\n 29892 -> ','\r\n   278 -> ' the'\r\n  7483 -> ' capital'\r\n   310 -> ' of'\r\n 12710 -> ' Russia'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n\r\nmain: interactive mode on.\r\nReverse prompt: 'User:'\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.000000\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n\r\n Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.\r\n\r\nUser: Hello, Bob.\r\nBob: Hello. How may I help you today?\r\nUser: Please tell me the largest city in Europe.\r\nBob: Sure. The largest city in Europe is Moscow, the capital of Russia.\r\nUser:hello\r\n\u203c \u25bc\u2192\u25ac\u25ac\u25b2\u21a8\u203c\u2191\u2665\u2660\u2666\"\u2665 \u263b \u00d4\u00fc\u00e7 \u221f \u00d4\u00fc\u00e7 \u2194\u00b6\r\n\u203c\u221f \u00d4\u00fc\u00e7 \u2665\r\n\u25ba\u2660\r\n\u25bc!\u2195\u263b    \u25bc \u2193     $\u25bc\u221f\u25bc\u2195\u2663\u2194\"\u203c\u2194\u2665\r\n\u263a       \u25ba        \u2194 \u00d4\u00fc\u00e7   #\u2191\"\u25bc\u2191\u2660$$\u25ac\u263a\u263b\r\n````\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-24T20:05:37+00:00",
    "closed_at": "2023-03-25T10:26:17+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/474/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/474"
  },
  {
    "number": 8976,
    "title": "Bug: llama-cli out \"error: input is empty\" and end",
    "body": "### What happened?\n\nmy run u:\\llama\\llama.cpp\\build\\bin\\llama-cli.exe -mli -co -fa -ngl 64 -cnv --chat-template gemma -m llama3-8B-Chinese-Chat-q8.gguf\r\n\r\nwin11 amd 7900x hip 6.1 vs 2022\r\ncmake  -DGGML_OPENMP=OFF -DGGML_BUILD_EXAMPLES=OFF -DGGML_HIPBLAS=ON -DAMDGPU_TARGETS=gfx1100 -DCMAKE_C_COMPILER=D:/AMD/ROCm/6.1/bin/clang.exe -DCMAKE_CXX_COMPILER=D:/AMD/ROCm/6.1/bin/clang++.exe\r\n\r\nd:\\AI_Model\\ggml_llava>u:\\llama\\llama.cpp\\build\\bin\\llama-cli.exe -m qwen2-7b-instruct-q5_k_m.gguf --chat-template llama2\r\nLog start\r\nmain: build = 0 (unknown)\r\nmain: built with  for x86_64-pc-windows-msvc\r\nmain: seed  = 1723317298\r\nllama_model_loader: loaded meta data with 21 key-value pairs and 339 tensors from qwen2-7b-instruct-q5_k_m.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\r\nllama_model_loader: - kv   1:                               general.name str              = qwen2-7b\r\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\r\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\r\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 3584\r\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 18944\r\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 28\r\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 4\r\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 17\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\r\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\r\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\r\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\r\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  141 tensors\r\nllama_model_loader: - type q5_K:  169 tensors\r\nllama_model_loader: - type q6_K:   29 tensors\r\nllm_load_vocab: special tokens cache size = 421\r\nllm_load_vocab: token to piece cache size = 0.9352 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = qwen2\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 152064\r\nllm_load_print_meta: n_merges         = 151387\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 3584\r\nllm_load_print_meta: n_layer          = 28\r\nllm_load_print_meta: n_head           = 28\r\nllm_load_print_meta: n_head_kv        = 4\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 7\r\nllm_load_print_meta: n_embd_k_gqa     = 512\r\nllm_load_print_meta: n_embd_v_gqa     = 512\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 18944\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model ftype      = Q5_K - Medium\r\nllm_load_print_meta: model params     = 7.62 B\r\nllm_load_print_meta: model size       = 5.07 GiB (5.71 BPW)\r\nllm_load_print_meta: general.name     = qwen2-7b\r\nllm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: EOS token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 148848 '\u00c4\u012c'\r\nllm_load_print_meta: EOT token        = 151645 '<|im_end|>'\r\nllm_load_print_meta: max token length = 256\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 ROCm devices:\r\n  Device 0: AMD Radeon RX 7900 XTX, compute capability 11.0, VMM: no\r\nllm_load_tensors: ggml ctx size =    0.15 MiB\r\nllm_load_tensors: offloading 0 repeating layers to GPU\r\nllm_load_tensors: offloaded 0/29 layers to GPU\r\nllm_load_tensors:        CPU buffer size =  5186.92 MiB\r\n.......................................................................................\r\nllama_new_context_with_model: n_ctx      = 32768\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:  ROCm_Host KV buffer size =  1792.00 MiB\r\nllama_new_context_with_model: KV self size  = 1792.00 MiB, K (f16):  896.00 MiB, V (f16):  896.00 MiB\r\nllama_new_context_with_model:  ROCm_Host  output buffer size =     0.58 MiB\r\nllama_new_context_with_model:      ROCm0 compute buffer size =  1941.02 MiB\r\nllama_new_context_with_model:  ROCm_Host compute buffer size =    71.01 MiB\r\nllama_new_context_with_model: graph nodes  = 986\r\nllama_new_context_with_model: graph splits = 396\r\n\r\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 |\r\nerror: input is empty\r\n\r\nNo more...\r\n\r\nmain.log \r\n\r\n[1723317404] llama_new_context_with_model: graph splits = 2\r\n[1723317404] warming up the model with an empty run\r\n[1723317404] n_ctx: 8192\r\n[1723317404] main: chat template example: <start_of_turn>user\r\nYou are a helpful assistant\r\n\r\nHello<end_of_turn>\r\n<start_of_turn>model\r\nHi there<end_of_turn>\r\n<start_of_turn>user\r\nHow are you?<end_of_turn>\r\n<start_of_turn>model\r\n\r\n[1723317404] \r\n[1723317404] system_info: n_threads = 16 / 32 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \r\n[1723317404] add_bos: 0\r\n[1723317404] tokenize the prompt\r\n[1723317404] prompt: \"\"\r\n[1723317404] tokens: [  ]\r\n[1723317404] error: input is empty\r\n\r\n\n\n### Name and Version\n\nD:\\AI_Model\\ggml_llava>u:\\llama\\llama.cpp\\build\\bin\\llama-cli.exe --version\r\nversion: 0 (unknown)\r\nbuilt with  for x86_64-pc-windows-msvc\r\n\r\ngit pull code for \r\ncommit 6e02327e8b7837358e0406bf90a4632e18e27846 (HEAD -> master, tag: b3565, origin/master, origin/HEAD)\r\n\n\n### What operating system are you seeing the problem on?\n\nWindows\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-08-10T19:23:25+00:00",
    "closed_at": "2024-08-12T16:04:55+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8976/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8976"
  },
  {
    "number": 6189,
    "title": "Mamba is not working with Metal",
    "body": "System: \r\n2020 M1 MacBook Pro 16GB Unified Memory (RAM) \r\nCPU: 8 Core (4 Performance and 4 Efficiency Cores)\r\nGPU: 8 Core\r\n\r\nI tried to run a GGUF of Mamba, but it won't run using Metal. I get the following error:\r\n```\r\nggml_metal_graph_compute_block_invoke: error: unsupported op 'SSM_CONV'\r\n```\r\n\r\n```\r\nmain: build = 2447 (c47cf414)\r\nmain: built with Apple clang version 15.0.0 (clang-1500.1.0.2.5) for arm64-apple-darwin23.3.0\r\nmain: seed  = 1710979793\r\nllama_model_loader: loaded meta data with 23 key-value pairs and 642 tensors from /Users/jsarnecki/Downloads/ggml-bagel-2.8b-v0.2-q8_0.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = mamba\r\nllama_model_loader: - kv   1:                               general.name str              = bagel-2.8b-v0.2\r\nllama_model_loader: - kv   2:                       mamba.context_length u32              = 1048576\r\nllama_model_loader: - kv   3:                     mamba.embedding_length u32              = 2560\r\nllama_model_loader: - kv   4:                  mamba.feed_forward_length u32              = 0\r\nllama_model_loader: - kv   5:                 mamba.attention.head_count u32              = 0\r\nllama_model_loader: - kv   6:                          mamba.block_count u32              = 64\r\nllama_model_loader: - kv   7:                      mamba.ssm.conv_kernel u32              = 4\r\nllama_model_loader: - kv   8:                       mamba.ssm.inner_size u32              = 5120\r\nllama_model_loader: - kv   9:                       mamba.ssm.state_size u32              = 16\r\nllama_model_loader: - kv  10:                   mamba.ssm.time_step_rank u32              = 160\r\nllama_model_loader: - kv  11:     mamba.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  12:                          general.file_type u32              = 7\r\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,50280]   = [\"</s>\", \"<s>\", \"!\", \"\\\"\", \"#\", \"$\", ...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,50280]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,50009]   = [\"\u0120 \u0120\", \"\u0120 t\", \"\u0120 a\", \"h e\", \"i n...\r\nllama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 0\r\nllama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  21:                    tokenizer.chat_template str              = {%- for idx in range(0, messages|leng...\r\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  513 tensors\r\nllama_model_loader: - type q8_0:  129 tensors\r\nllm_load_vocab: mismatch in special tokens definition ( 7/50280 vs 28/50280 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = mamba\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 50280\r\nllm_load_print_meta: n_merges         = 50009\r\nllm_load_print_meta: n_ctx_train      = 1048576\r\nllm_load_print_meta: n_embd           = 2560\r\nllm_load_print_meta: n_head           = 0\r\nllm_load_print_meta: n_head_kv        = 0\r\nllm_load_print_meta: n_layer          = 64\r\nllm_load_print_meta: n_rot            = 0\r\nllm_load_print_meta: n_embd_head_k    = 0\r\nllm_load_print_meta: n_embd_head_v    = 0\r\nllm_load_print_meta: n_gqa            = 0\r\nllm_load_print_meta: n_embd_k_gqa     = 0\r\nllm_load_print_meta: n_embd_v_gqa     = 0\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 0\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = -1\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 1048576\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 4\r\nllm_load_print_meta: ssm_d_inner      = 5120\r\nllm_load_print_meta: ssm_d_state      = 16\r\nllm_load_print_meta: ssm_dt_rank      = 160\r\nllm_load_print_meta: model type       = 3B\r\nllm_load_print_meta: model ftype      = Q8_0\r\nllm_load_print_meta: model params     = 2.77 B\r\nllm_load_print_meta: model size       = 3.08 GiB (9.54 BPW) \r\nllm_load_print_meta: general.name     = bagel-2.8b-v0.2\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 0 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '</s>'\r\nllm_load_print_meta: PAD token        = 0 '</s>'\r\nllm_load_print_meta: LF token         = 128 '\u00c4'\r\nllm_load_tensors: ggml ctx size =    0.49 MiB\r\nggml_backend_metal_buffer_from_ptr: allocated buffer, size =  3149.83 MiB, ( 3149.89 / 14336.00)\r\nllm_load_tensors: offloading 64 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 65/65 layers to GPU\r\nllm_load_tensors:      Metal buffer size =  3149.81 MiB\r\nllm_load_tensors:        CPU buffer size =   130.43 MiB\r\n...............................................................................................\r\nllama_new_context_with_model: n_ctx      = 2048\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nggml_metal_init: allocating\r\nggml_metal_init: found device: Apple M1\r\nggml_metal_init: picking default device: Apple M1\r\nggml_metal_init: default.metallib not found, loading from source\r\nggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\r\nggml_metal_init: loading '/Users/jsarnecki/opt/old/llama.cpp/ggml-metal.metal'\r\nggml_metal_init: GPU name:   Apple M1\r\nggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\r\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\r\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\r\nggml_metal_init: simdgroup reduction support   = true\r\nggml_metal_init: simdgroup matrix mul. support = true\r\nggml_metal_init: hasUnifiedMemory              = true\r\nggml_metal_init: recommendedMaxWorkingSetSize  = 15032.39 MB\r\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    23.75 MiB, ( 3175.45 / 14336.00)\r\nllama_kv_cache_init:      Metal KV buffer size =    23.75 MiB\r\nllama_new_context_with_model: KV self size  =   23.75 MiB, K (f32):    3.75 MiB, V (f32):   20.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =   392.81 MiB\r\nggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   103.20 MiB, ( 3278.66 / 14336.00)\r\nllama_new_context_with_model:      Metal compute buffer size =   103.20 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =     5.06 MiB\r\nllama_new_context_with_model: graph splits: 2\r\nggml_metal_graph_compute_block_invoke: error: unsupported op 'SSM_CONV'\r\nGGML_ASSERT: ggml-metal.m:851: !\"unsupported op\"\r\nggml_metal_graph_compute_block_invoke: error: unsupported op 'SSM_CONV'\r\nGGML_ASSERT: ggml-metal.m:851: !\"unsupported op\"\r\nggml_metal_graph_compute_block_invoke: error: unsupported op 'SSM_CONV'\r\nGGML_ASSERT: ggml-metal.m:851: !\"unsupported op\"\r\nzsh: abort      ./main -m \"$model\" -n -1 --multiline-input --interactive-first --color -r \r\n```\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-03-21T00:16:42+00:00",
    "closed_at": "2024-03-21T19:11:37+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6189/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6189"
  },
  {
    "number": 12085,
    "title": "Compile bug: How to build llama.android example with -DGGML_VULKAN=ON through android studio.",
    "body": "### Git commit\n\nmaster\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nVulkan\n\n### Problem description & steps to reproduce\n\nI'm trying to compile llama.android with vulkan backend enabled: i.e. with -DGGML_VULKAN=ON.\n\nBuild is failing with error - glslc not found.\n\n\n\n### First Bad Commit\n\n_No response_\n\n### Compile command\n\n```shell\n./gradlew build\n```\n\n### Relevant log output\n\n```shell\nTask failed with an exception.\n-----------\n* What went wrong:\nExecution failed for task ':llama:buildCMakeRelease[arm64-v8a]'.\n> com.android.ide.common.process.ProcessException: ninja: Entering directory `/home/dcaimlpune/ashwini_wp/bmw/llama.cpp/examples/llama.android/llama/.cxx/Release/5x6s385r/arm64-v8a'\n  [1/65] Building CXX object build-llama/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o\n  [2/65] Building CXX object build-llama/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\n  [3/65] Creating directories for 'vulkan-shaders-gen'\n  [4/65] Building CXX object build-llama/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\n  [5/65] No download step for 'vulkan-shaders-gen'\n  [6/65] No update step for 'vulkan-shaders-gen'\n  [7/65] No patch step for 'vulkan-shaders-gen'\n  [8/65] Generating build details from Git\n  -- Found Git: /usr/bin/git (found version \"2.43.0\") \n  [9/65] Building CXX object build-llama/common/CMakeFiles/build_info.dir/build-info.cpp.o\n  [10/65] Building CXX object build-llama/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o\n  [11/65] Building C object build-llama/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\n  [12/65] Building CXX object build-llama/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\n  [13/65] Building CXX object build-llama/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\n  [14/65] Building C object build-llama/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o\n  [15/65] Performing configure step for 'vulkan-shaders-gen'\n  FAILED: build-llama/ggml/src/ggml-vulkan/vulkan-shaders-gen-prefix/src/vulkan-shaders-gen-stamp/vulkan-shaders-gen-configure /home/dcaimlpune/ashwini_wp/bmw/llama.cpp/examples/llama.android/llama/.cxx/Release/5x6s385r/arm64-v8a/build-llama/ggml/src/ggml-vulkan/vulkan-shaders-gen-prefix/src/vulkan-shaders-gen-stamp/vulkan-shaders-gen-configure \n  cd /home/dcaimlpune/ashwini_wp/bmw/llama.cpp/examples/llama.android/llama/.cxx/Release/5x6s385r/arm64-v8a/build-llama/ggml/src/ggml-vulkan/vulkan-shaders-gen-prefix/src/vulkan-shaders-gen-build && /home/dcaimlpune/Android/Sdk/cmake/3.22.1/bin/cmake -DCMAKE_TOOLCHAIN_FILE=/home/dcaimlpune/ashwini_wp/bmw/llama.cpp/examples/llama.android/llama/.cxx/Release/5x6s385r/arm64-v8a/host-toolchain.cmake -DCMAKE_INSTALL_PREFIX=/home/dcaimlpune/ashwini_wp/bmw/llama.cpp/examples/llama.android/llama/.cxx/Release/5x6s385r/arm64-v8a -GNinja /home/dcaimlpune/ashwini_wp/bmw/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders && /home/dcaimlpune/Android/Sdk/cmake/3.22.1/bin/cmake -E touch /home/dcaimlpune/ashwini_wp/bmw/llama.cpp/examples/llama.android/llama/.cxx/Release/5x6s385r/arm64-v8a/build-llama/ggml/src/ggml-vulkan/vulkan-shaders-gen-prefix/src/vulkan-shaders-gen-stamp/vulkan-shaders-gen-configure\n  CMake Warning (dev) in CMakeLists.txt:\n    No project() command is present.  The top-level CMakeLists.txt file must\n    contain a literal, direct call to the project() command.  Add a line of\n    code such as\n  \n      project(ProjectName)\n  \n    near the top of the file, but after cmake_minimum_required().\n  \n    CMake is pretending there is a \"project(Project)\" command on the first\n    line.\n  This warning is for project developers.  Use -Wno-dev to suppress it.\n  \n  -- The C compiler identification is GNU 13.3.0\n  -- The CXX compiler identification is GNU 13.3.0\n  -- Detecting C compiler ABI info\n  -- Detecting C compiler ABI info - done\n  -- Check for working C compiler: /usr/bin/gcc - skipped\n  -- Detecting C compile features\n  -- Detecting C compile features - done\n  -- Detecting CXX compiler ABI info\n  -- Detecting CXX compiler ABI info - done\n  -- Check for working CXX compiler: /usr/bin/g++ - skipped\n  -- Detecting CXX compile features\n  -- Detecting CXX compile features - done\n  -- Looking for pthread.h\n  -- Looking for pthread.h - found\n  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n  -- Found Threads: TRUE  \n  CMake Error at CMakeLists.txt:4 (message):\n    glslc not found.\n  \n  \n  CMake Warning (dev) in CMakeLists.txt:\n    No cmake_minimum_required command is present.  A line of code such as\n  \n      cmake_minimum_required(VERSION 3.22)\n  \n    should be added at the top of the file.  The version specified may be lower\n    if you wish to support older CMake versions for this project.  For more\n    information run \"cmake --help-policy CMP0000\".\n  This warning is for project developers.  Use -Wno-dev to suppress it.\n  \n  -- Configuring incomplete, errors occurred!\n  See also \"/home/dcaimlpune/ashwini_wp/bmw/llama.cpp/examples/llama.android/llama/.cxx/Release/5x6s385r/arm64-v8a/build-llama/ggml/src/ggml-vulkan/vulkan-shaders-gen-prefix/src/vulkan-shaders-gen-build/CMakeFiles/CMakeOutput.log\".\n  [16/65] Building CXX object build-llama/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\n  [17/65] Building CXX object build-llama/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o\n  [18/65] Building CXX object build-llama/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\n  [19/65] Building C object build-llama/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\n  [20/65] Building CXX object build-llama/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\n  [21/65] Building C object build-llama/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\n  [22/65] Building C object build-llama/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\n  ninja: build stopped: subcommand failed.\n  \n  C++ build system [build] failed while executing:\n      /home/dcaimlpune/Android/Sdk/cmake/3.22.1/bin/ninja \\\n        -C \\\n        /home/dcaimlpune/ashwini_wp/bmw/llama.cpp/examples/llama.android/llama/.cxx/Release/5x6s385r/arm64-v8a \\\n        ggml \\\n        ggml-base \\\n        ggml-cpu \\\n        ggml-vulkan \\\n        llama \\\n        llama-android\n    from /home/dcaimlpune/ashwini_wp/bmw/llama.cpp/examples/llama.android/llama\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-02-26T14:32:40+00:00",
    "closed_at": "2025-03-03T14:04:25+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12085/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12085"
  },
  {
    "number": 6706,
    "title": "Idefics2 VLM Support",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x ] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x ] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\n\r\nRequesting support for HuggingFace's new [Idefics2 VLM](https://huggingface.co/blog/idefics2).\r\n\r\n# Motivation\r\n\r\n- First true open source VLM (Apache 2.0)\r\n- This 8B model offers comparable performance to Llava-1.6-34b and Apple's unreleased 30B MM1.\r\n- The HuggingFace team included a [fine-tuning notebook ](https://colab.research.google.com/drive/1NtcTgRbSBKN7pYD3Vdx1j9m8pt3fhFDB?usp=sharing#scrollTo=j-zKnRTZKZmI) which allows users to make specialized VLMs\r\n\r\n## Possible Implementation:\r\nHopefully this isn't exceedingly difficult to do. The base LLM is Mistral-7B-v0.1 and the image encoder is Google's [siglip-so400m-patch14-384](https://huggingface.co/google/siglip-so400m-patch14-384). Their projector is also a simple MLP similar to Llava.\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-16T16:29:38+00:00",
    "closed_at": "2024-06-18T01:07:01+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6706/reactions",
      "total_count": 31,
      "+1": 16,
      "-1": 0,
      "laugh": 0,
      "hooray": 8,
      "confused": 0,
      "heart": 7,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6706"
  },
  {
    "number": 12728,
    "title": "vulkan: Requested buffer size exceeds  (when using -ctk)",
    "body": "unable to allocate buffer on vulkan? \n\nI do have memory on GPU. but met this issue when using `-ckt`\n\ninit:    Vulkan0 KV buffer size = 13800.00 MiB\nllama_context: KV self size  = 13800.00 MiB, K (q8_0): 6120.00 MiB, V (f16): 7680.00 MiB\nggml_vulkan: Device memory allocation of size 2166360064 failed.\nggml_vulkan: Requested buffer size exceeds device memory allocation limit: ErrorOutOfDeviceMemory\nggml_gallocr_reserve_n: failed to allocate Vulkan0 buffer of size 2166360064\nllama_init_from_model: failed to initialize the context: failed to allocate compute pp buffers\n\n\nbellow are success output running when `without -ckt` and `with -nkvo`\n\n1. w/o ckt \ninit: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 60, can_shift = 0\ninit:    Vulkan0 KV buffer size = 19200.00 MiB\nllama_context: KV self size  = 19200.00 MiB, K (f16): 11520.00 MiB, V (f16): 7680.00 MiB\nllama_context:    Vulkan0 compute buffer size =  1174.00 MiB\nllama_context: Vulkan_Host compute buffer size =    18.01 MiB\nllama_context: graph nodes  = 4540\nllama_context: graph splits = 2\ncommon_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n\n2. w/ -nkvo\ninit: kv_size = 4096, offload = 0, type_k = 'q4_0', type_v = 'f16', n_layer = 60, can_shift = 0\ninit:        CPU KV buffer size = 10920.00 MiB\nllama_context: KV self size  = 10920.00 MiB, K (q4_0): 3240.00 MiB, V (f16): 7680.00 MiB\nllama_context:    Vulkan0 compute buffer size =   238.00 MiB\nllama_context: Vulkan_Host compute buffer size =  1160.01 MiB\nllama_context: graph nodes  = 4540\nllama_context: graph splits = 122\ncommon_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-03T03:52:18+00:00",
    "closed_at": "2025-04-03T13:23:50+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12728/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12728"
  },
  {
    "number": 11490,
    "title": "Feature Request: Support for Deepseek Janus-Pro-7B & Janus-1.3B",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nDeepSeek recently released **[Janus-Pro-7B](https://huggingface.co/deepseek-ai/Janus-Pro-7B)** and **[Janus-1.3B](https://huggingface.co/deepseek-ai/Janus-1.3B)**, both multimodal models currently supported in [Transformers](https://github.com/huggingface/transformers). \n\n\n\n**Resources:** [Janus GitHub](https://github.com/deepseek-ai/Janus)\n\n\n### Motivation\n\nAdding them to `llama.cpp` would enable efficient local inference, expanding support for state-of-the-art multimodal AI. Would love to see this integrated\u2014appreciate all the great work!  \n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-29T14:53:13+00:00",
    "closed_at": "2025-04-22T01:08:02+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11490/reactions",
      "total_count": 54,
      "+1": 54,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11490"
  },
  {
    "number": 6377,
    "title": "Is it possible to dynamically switch multiple LoRA adapters?",
    "body": "Thank you for this great project. I have two questions.\r\n\r\n# Q1 Is it possible to dynamically switch multiple LoRA adapters?\r\n\r\nIn the transformers library, we can load multiple adapters to the original model by `load_adapter` then switch the specified adapter with `set_adapter` instantlly like below.\r\n\r\n```python\r\n# base model\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    model_name,\r\n)\r\n\r\n# load multiple adapters\r\nmodel.load_adapter(\"model/adapter1/\", \"adapter1\")\r\nmodel.load_adapter(\"model/adapter2/\", \"adapter2\")\r\n\r\n# switch adapter (change takes instantly)\r\nmodel.set_adapter(\"adapter2\")\r\n```\r\n\r\nIs it possible to do the same thing with llama cpp?\r\nI found there is an API `llama_model_apply_lora_from_file()` .\r\nhttps://github.com/ggerganov/llama.cpp/blob/bfe7dafc9cf96b9a09ead347fed9a547930fc631/llama.h#L441\r\n\r\nBut the description says `The model needs to be reloaded before applying a new adapter, otherwise the adapter will be applied on top of the previous one`.\r\n\r\nIs it impossible to switch instantly without reloading the model like transformers?\r\n\r\n# Q2. The model loading with LoRA is extremely slow\r\n\r\nA 13b model without LoRA takes 2 seconds to model loading.\r\n`./main -m ../gguf_model/llm-jp-13b-ggml-model-f16.gguf`\r\n\r\nHowever with LoRA takes over 60sec.\r\n`./main -m ../gguf_model/llm-jp-13b-v1.0-q4_0.gguf --lora ../gguf_model/llm-jp-adapter-model.bin`\r\n\r\nI know that with LoRA option disables mmap, but is this normal? Is there some way to speed it up?\r\nI think the way is to merge the model and LoRA firstly then convert it to gguf, but I would like to dynamically switch the LoRA adapter.\r\n\r\nThank you.",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2024-03-29T01:35:18+00:00",
    "closed_at": "2024-04-01T04:03:17+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6377/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6377"
  },
  {
    "number": 1868,
    "title": "How do I install with Make?",
    "body": "I wasn't able to run cmake on my system (ubuntu 20.04), but just wondering how I get the built binaries out, installed on the system...\r\n\r\n```\r\nmake install\r\n```\r\ndidn't work for me :(",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-06-15T07:06:17+00:00",
    "closed_at": "2024-04-10T01:07:06+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1868/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1868"
  },
  {
    "number": 6957,
    "title": "llama_decode return logbits whose value are all nan",
    "body": "**enviroment**\r\ngpu : nvidia titan rtx (compute capability 7.5) with 24 GB vram\r\nos : ubuntu 22.04\r\ndriver version :550.76\r\ngit commit:b4e4b8a9\r\nmodel:starcode-7B q8_0.v2\r\n\r\n**problem encounted**\r\nllama_model ,llama_batch,llama_ctx was all corrrectly initilized\r\nfollowing code behaves different on different gpus\r\n```\r\nllama_batch batch= //... ;  \r\nint ret =llama_decode(ctx,batch);  \r\nfloat* logits=llama_get_logits(ctx);  \r\n```\r\n\r\n1. on a10,it returns the right value\r\n2. on the titan rtx ,all logits are all nan,leading to unusable result and can't engeter the token generation step\r\n3. use the starcoder-1b model ,titan rtx outputs right value\r\n4. using other frameworks,titan rtx can generate valid answer\r\n\r\n**question**\r\n\r\n1. it seems model > 3 trigger the problem,is it the problem of gpu?\r\n2.is there any way to inspect infer process,seeming that which layer produces nan?",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-28T02:37:01+00:00",
    "closed_at": "2024-06-13T02:55:44+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6957/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6957"
  },
  {
    "number": 19,
    "title": "Implement Flash Attention Option",
    "body": "Would love to see a faster, more memory efficient attention implemented like Flash Attention. :)",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-11T18:57:36+00:00",
    "closed_at": "2023-07-28T19:26:05+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/19/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/19"
  }
]
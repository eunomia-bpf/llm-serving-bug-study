[
  {
    "number": 11264,
    "title": "Misc. bug: [llama.android] Model keeps replying and cannot be stopped normally until it exceeds the context",
    "body": "### Name and Version\n\nThe latest [b4491](https://github.com/ggerganov/llama.cpp/releases/tag/b4491) version with `llama.android` example\n\n### Operating systems\n\nOther? (Please let us know in description)\n\n### Which llama.cpp modules do you know to be affected?\n\nOther (Please specify in the next section)\n\n### Command line\n\n_No response_\n\n### Problem description & steps to reproduce\n\n1. Run `llama.android` example\n2. Loaded with [SmolLM2](https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct-GGUF) model\n3. Send with chat template (User message: `Tell a joke`), the template are generated by the `common_chat_apply_template()` method\n\n> Code location: llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/MainViewModel.kt\n\n```kotlin\nval smollm2msg = \"<|im_start|>system\\n\" +\n    \"You are a helpful AI assistant<|im_end|>\\n\" +\n    \"<|im_start|>user\\n\" +\n    \"$text<|im_end|>\\n\" +\n    \"<|im_start|>assistant\\n\"\nviewModelScope.launch {\n    llamaAndroid.send(smollm2msg)\n        .catch {\n            Log.e(tag, \"send() failed\", it)\n            messages += it.message!!\n        }\n        .collect { messages = messages.dropLast(1) + (messages.last() + it) }\n}\n```\n4. Issue reproduce, the model keeps replying and cannot be stopped normally until it exceeds the context, and it will contain tokens such as `<|im_start|>` and `<|im_end|>`\n\n![image](https://github.com/user-attachments/assets/ec179ab9-26b5-4892-967a-bf19ee9e3be1)\n\n\n### Comparison test:\nThis problem cannot be reproduced using the command line program `llama-cli` with the same LLM model\n\n> ./llama-cli -m models/smollm2-360m-instruct-q8_0.gguf -p \"You are a helpful assistant\" -cnv\n\n### First Bad Commit\n\nSince the establishment of `llama.android`\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-01-16T12:32:24+00:00",
    "closed_at": "2025-01-17T16:36:18+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11264/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11264"
  },
  {
    "number": 11092,
    "title": "Misc. bug: server /infill endpoint incorrectly inserts <bos> token",
    "body": "### Name and Version\n\nServer built from c31fc8b966817b2f0b277fd28e04a189e388972a\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Problem description & steps to reproduce\n\nIt was mentioned in the [discussion of the codestral model](https://huggingface.co/bartowski/Codestral-22B-v0.1-GGUF/discussions/4#67737fcda8cc8ce76bcd7cc0) that changes in https://github.com/ggerganov/llama.cpp/pull/10023 made `/infill` endpoint add `<bos>` token incorrectly. I'm not really sure it's the case since before this change `prompt` was a required field, but anyway it doesn't seem to be correct.\r\n\r\nTo reproduce you can use this model: https://huggingface.co/bartowski/codegemma-2b-GGUF with the following request:\r\n\r\n```\r\ncurl -XPOST \"localhost:8080/infill\" -d '{\"input_prefix\": \"1, \", \"input_suffix\": \", 5\"}' -H \"Content-Type: application/json\"\r\n```\r\n\r\nIn the response you will see 2 `<bos>` tokens: `\"prompt\": \"<bos><|fim_prefix|> 1, <bos><|fim_suffix|> , 5<|fim_middle|>\"`.\r\n\r\nAccording to the codegemma [readme](https://huggingface.co/google/codegemma-2b) there shouldn't be any `<bos>` tokens (see the prompt in the first code snippet there).\r\n\r\nI don't see any discussion in the mentioned PR regarding special tokens, so I guess this wasn't intentional? Feel free to close this issue if I'm wrong.\r\n\r\nThe fix is to simply change the flag in this line: https://github.com/ggerganov/llama.cpp/blob/b56f079e28fda692f11a8b59200ceb815b05d419/examples/server/server.cpp#L3800\n\n### First Bad Commit\n\n958367bf530d943a902afa1ce1c342476098576b\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-01-05T18:00:14+00:00",
    "closed_at": "2025-01-06T13:36:09+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11092/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11092"
  },
  {
    "number": 11321,
    "title": "Library not loaded: @rpath/libllama.dylib",
    "body": "### Name and Version\n\nI just downloaded the latest binary (b4519) to play around with on my Mac, however it looks like the binary didn't quite compile correctly. I tried downloading an earlier version (b4514) with the same result. Running `llamba-cli` yields:\n\n```\n\u279c  llama.cpp ./llama-cli --version\ndyld[90496]: Library not loaded: @rpath/libllama.dylib\n  Referenced from: <653E6B29-4AFF-3485-B031-B4F65747F8CF> /Users/constantmeiring/Downloads/build/llama.cpp/llama-cli\n```\n\n\n\n### Operating systems\n\nMacOS 15.1.1 (24B91)\n\n### GGML backends\n\nMetal\n\n### Hardware\n\nMacbook - M3 Max\n\n### Models\n\n_No response_\n\n### Problem description & steps to reproduce\n\nDownload the latest build and try and run it on Mac.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\ndyld[90496]: Library not loaded: @rpath/libllama.dylib\n  Referenced from: <653E6B29-4AFF-3485-B031-B4F65747F8CF> /Users/constantmeiring/Downloads/build/llama.cpp/llama-cli\n  Reason: tried: '/Users/runner/work/llama.cpp/llama.cpp/build/src/libllama.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/runner/work/llama.cpp/llama.cpp/build/src/libllama.dylib' (no such file), '/Users/runner/work/llama.cpp/llama.cpp/build/ggml/src/libllama.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/runner/work/llama.cpp/llama.cpp/build/ggml/src/libllama.dylib' (no such file), '/Users/runner/work/llama.cpp/llama.cpp/build/ggml/src/ggml-blas/libllama.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/runner/work/llama.cpp/llama.cpp/build/ggml/src/ggml-blas/libllama.dylib' (no such file), '/Users/runner/work/llama.cpp/llama.cpp/build/ggml/src/ggml-metal/libllama.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/runner/work/llama.cpp/llama.cpp/build/ggml/src/ggml-metal/libllama.dylib' (no such file), '/Users/runner/work/llama.cpp/llama.cpp/build/ggml/src/ggml-rpc/libllama.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/runner/work/llama.cpp/llama.cpp/build/ggml/src/ggml-rpc/libllama.dylib' (no such file), '/Users/runner/work/llama.cpp/llama.cpp/build/src/libllama.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/runner/work/llama.cpp/llama.cpp/build/src/libllama.dylib' (no such file), '/Users/runner/work/llama.cpp/llama.cpp/build/ggml/src/libllama.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/runner/work/llama.cpp/llama.cpp/build/ggml/src/libllama.dylib' (no such file), '/Users/runner/work/llama.cpp/llama.cpp/build/ggml/src/ggml-blas/libllama.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/runner/work/llama.cpp/llama.cpp/build/ggml/src/ggml-blas/libllama.dylib' (no such file), '/Users/runner/work/llama.cpp/llama.cpp/build/ggml/src/ggml-metal/libllama.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/runner/work/llama.cpp/llama.cpp/build/ggml/src/ggml-metal/libllama.dylib' (no such file), '/Users/runner/work/llama.cpp/llama.cpp/build/ggml/src/ggml-rpc/libllama.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/runner/work/llama.cpp/llama.cpp/build/ggml/src/ggml-rpc/libllama.dylib' (no such file)\n[1]    90496 abort      ./llama-cli --version\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-01-20T21:41:04+00:00",
    "closed_at": "2025-01-25T13:21:45+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11321/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11321"
  },
  {
    "number": 11335,
    "title": "Eval bug: llama-server stopped working after PR #11285 got merged",
    "body": "### Name and Version\n\nllama-server f30f099228f774209aa3010b78dfbe5d262e69aa\n\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\nRTX 4090, CUDA\n\n### Models\n\nE.g. Code Qwen 2.5 7B-Chat (Q8)\n\n### Problem description & steps to reproduce\n\nllama-server stopped generating any tokens for me, regardless of model, starting with commit f30f099228f774209aa3010b78dfbe5d262e69aa from #11285.\nSimply reverting the above commit, e.g. on top of todays master (6171c9d25820ccf676b243c172868819d882848f) does fix the issue for me.\n\nTo reproduce, goto http://localhost:8080, enter a question hit return, nothing happens.\n\n### First Bad Commit\n\nf30f099228f774209aa3010b78dfbe5d262e69aa\n\n### Relevant log output\n\n```shell\n\u2502 main: server is listening on http://0.0.0.0:8080 - starting the main loop                                                                                                                                                                                 \u2502                \n             \u2502 srv  update_slots: all slots are idle                                                                                                                                                                                                                     \u2502                \n             \u2502 request: GET / 127.0.0.1 200                                                                                                                                                                                                                              \u2502                \n             \u2502 request: GET /favicon.ico  400                                                                                                                                                                                                                            \u2502                \n             \u2502 request: POST /v1/chat/completions  400                                                                                                                                                                                                                   \u2502\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-01-21T19:50:16+00:00",
    "closed_at": "2025-02-16T17:11:23+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11335/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11335"
  },
  {
    "number": 11267,
    "title": "Misc. bug: llama-cli: error while loading shared libraries: libllama.so: cannot open shared object file: No such file or directory",
    "body": "### Name and Version\n\nversion: 4493 (9c8dcefe)\nbuilt with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-g\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Command line\n\n```shell\n./llama-server\n```\n\n### Problem description & steps to reproduce\n\n\n\nI get this error when I run llama-server, \n\nllama-cli: error while loading shared libraries: libllama.so: cannot open shared object file: No such file or directory\nllama.cpp/build/bin/llama-server: error while loading shared libraries: libggml.so: cannot open shared object file: No such file or directory\n\nIt's a minor issue i was able to fix it with\n\n```export LD_LIBRARY_PATH=llama.cpp/build/ggml/src:llama.cpp/build/src/:$LD_LIBRARY_PATH```. \n\nIt worked without doing this in which ever branch I was using before this.\n\nI built it using \n```cmake -B build```\n```cmake --build build --config Release```\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-16T18:02:42+00:00",
    "closed_at": "2025-04-09T01:07:46+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11267/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11267"
  },
  {
    "number": 11044,
    "title": "Misc. bug: SYCL out of memory error",
    "body": "### Name and Version\r\n\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\nversion: 4404 (0827b2c1)\r\nbuilt with MSVC 19.42.34435.0\r\n\r\n### Operating systems\r\n\r\nWindows\r\n\r\n### Which llama.cpp modules do you know to be affected?\r\n\r\nlibllama (core library)\r\n\r\n### Problem description & steps to reproduce\r\n\r\n### Problem\r\nI run into memory errors when using the SYCL backend. No error appears when running the same setup with the VULKAN backend (same model, prompt, context length, batch size, etc.). In the example below, the error says that 568 MB could not be allocated. This is strange because I have 16 GB of GPU memory (shared system memory, not dedicated). It seems the error is not specific to llama-cli because it also occurs when I use the Python bindings (llama-cpp-python). The error also occurs in earlier versions (I tried b4311).\r\n\r\n### Hardware\r\nDell Latitude 5420\r\nWindows 10 Enterprise\r\nCPU: 11th Gen Intel i7-1185G7 @ 3.00GHz, 4 Cores, 8 Logical Processors x86_64\r\nRAM: 2x16GB Hynix 3200MHz DDR4 PC4-25600\r\nGPU: Intel Iris Xe iGPU\r\nStorage: Western Digital PC SN530 NVMe WDC 512GB M.2 SSD\r\n\r\n### Minimum Error example\r\n\r\n```cmd\r\nrem create very long prompt\r\npython -c \"f = open('prompt.txt', 'w'); prompt = 'bla '*40000; f.write(prompt); f.close();\"\r\n\r\nrem run llama-cli\r\nllama-cli.exe -m \"C:\\path\\to\\Llama-3.2-3B-Instruct-Q4_0.gguf\" --file prompt.txt -n 20 -ngl 99 -c 40100 --no-display-prompt\r\n\r\nrem complete log attached\r\nalloc: can't allocate 568118476 Bytes of memory on device/GPU\r\nEnqueue process failed.\r\nException caught at file:D:\\a\\llama.cpp\\llama.cpp\\ggml\\src\\ggml-sycl\\ggml-sycl.cpp, line:3404, func:operator()\r\nSYCL error: CHECK_TRY_ERROR(dpct::gemm_batch( *main_stream, oneapi::mkl::transpose::trans, oneapi::mkl::transpose::nontrans, ne01, ne11, ne10, alpha, (const void **)(ptrs_src.get() + 0 * ne23), dpct::library_data_t::real_half, nb01 / nb00, (const void **)(ptrs_src.get() + 1 * ne23), dpct::library_data_t::real_half, nb11 / nb10, beta, (void **)(ptrs_dst.get() + 0 * ne23), cu_data_type, ne01, ne23, cu_compute_type)): Meet error in this line code!\r\n  in function ggml_sycl_mul_mat_batched_sycl at D:\\a\\llama.cpp\\llama.cpp\\ggml\\src\\ggml-sycl\\ggml-sycl.cpp:3404\r\nD:\\a\\llama.cpp\\llama.cpp\\ggml\\src\\ggml-sycl\\..\\ggml-sycl\\common.hpp:111: SYCL error\r\n```\r\n\r\n### First Bad Commit\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nC:\\...\\llama.cpp\\b4404\\sycl>llama-cli.exe -m \"C:\\path\\to\\Llama-3.2-3B-Instruct-Q4_0.gguf\" --file prompt.txt -n 20 -ngl 99 -c 40100 --no-display-prompt\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\nbuild: 4404 (0827b2c1) with MSVC 19.42.34435.0 for\r\nmain: llama backend init\r\nmain: load the model and apply lora adapter, if any\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\nllama_load_model_from_file: using device SYCL0 (Intel(R) Iris(R) Xe Graphics) - 14658 MiB free\r\nllama_model_loader: loaded meta data with 35 key-value pairs and 255 tensors from C:\\path\\to\\Llama-3.2-3B-Instruct-Q4_0.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\r\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\r\nllama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\r\nllama_model_loader: - kv   5:                         general.size_label str              = 3B\r\nllama_model_loader: - kv   6:                            general.license str              = llama3.2\r\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\r\nllama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\r\nllama_model_loader: - kv   9:                          llama.block_count u32              = 28\r\nllama_model_loader: - kv  10:                       llama.context_length u32              = 131072\r\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 3072\r\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\r\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 24\r\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\r\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128\r\nllama_model_loader: - kv  18:               llama.attention.value_length u32              = 128\r\nllama_model_loader: - kv  19:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\r\nllama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\r\nllama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"\u0120 \u0120\", \"\u0120 \u0120\u0120\u0120\", \"\u0120\u0120 \u0120\u0120\", \"...\r\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\r\nllama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009\r\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\r\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-GGU...\r\nllama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\r\nllama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 196\r\nllama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125\r\nllama_model_loader: - type  f32:   58 tensors\r\nllama_model_loader: - type q4_0:  193 tensors\r\nllama_model_loader: - type q4_1:    3 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special tokens cache size = 256\r\nllm_load_vocab: token to piece cache size = 0.7999 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 128256\r\nllm_load_print_meta: n_merges         = 280147\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 3072\r\nllm_load_print_meta: n_layer          = 28\r\nllm_load_print_meta: n_head           = 24\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 3\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 8192\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 500000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 3B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 3.21 B\r\nllm_load_print_meta: model size       = 1.78 GiB (4.77 BPW)\r\nllm_load_print_meta: general.name     = Llama 3.2 3B Instruct\r\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\r\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\r\nllm_load_print_meta: LF token         = 128 '\u00c4'\r\nllm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\r\nllm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: max token length = 256\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\r\nllm_load_tensors: offloading 28 repeating layers to GPU\r\nllm_load_tensors: offloading output layer to GPU\r\nllm_load_tensors: offloaded 29/29 layers to GPU\r\nllm_load_tensors:        SYCL0 model buffer size =  1825.40 MiB\r\nllm_load_tensors:   CPU_Mapped model buffer size =   308.23 MiB\r\n.........................................................................\r\nllama_new_context_with_model: n_seq_max     = 1\r\nllama_new_context_with_model: n_ctx         = 40128\r\nllama_new_context_with_model: n_ctx_per_seq = 40128\r\nllama_new_context_with_model: n_batch       = 2048\r\nllama_new_context_with_model: n_ubatch      = 512\r\nllama_new_context_with_model: flash_attn    = 0\r\nllama_new_context_with_model: freq_base     = 500000.0\r\nllama_new_context_with_model: freq_scale    = 1\r\nllama_new_context_with_model: n_ctx_per_seq (40128) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nFound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                 Intel Iris Xe Graphics|   12.0|     96|     512|   32| 15370M|            1.3.29803|\r\nllama_kv_cache_init: kv_size = 40128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28\r\nllama_kv_cache_init:      SYCL0 KV buffer size =  4389.00 MiB\r\nllama_new_context_with_model: KV self size  = 4389.00 MiB, K (f16): 2194.50 MiB, V (f16): 2194.50 MiB\r\nllama_new_context_with_model:  SYCL_Host  output buffer size =     0.49 MiB\r\nllama_new_context_with_model:      SYCL0 compute buffer size =  1983.38 MiB\r\nllama_new_context_with_model:  SYCL_Host compute buffer size =    84.38 MiB\r\nllama_new_context_with_model: graph nodes  = 902\r\nllama_new_context_with_model: graph splits = 2\r\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 40128\r\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\nmain: llama threadpool init, n_threads = 4\r\n\r\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 8 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |\r\n\r\nsampler seed: 3140513417\r\nsampler params:\r\n        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\r\n        dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 40128\r\n        top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800\r\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\r\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist\r\ngenerate: n_ctx = 40128, n_batch = 2048, n_predict = 20, n_keep = 1\r\n\r\nalloc: can't allocate 568118476 Bytes of memory on device/GPU\r\nalloc: can't allocate 568118476 Bytes of memory on device/GPU\r\nalloc: can't allocate 568118476 Bytes of memory on device/GPU\r\nalloc: can't allocate 568118476 Bytes of memory on device/GPU\r\nalloc: can't allocate 568118476 Bytes of memory on device/GPU\r\nalloc: can't allocate 568118476 Bytes of memory on device/GPU\r\nalloc: can't allocate 568118476 Bytes of memory on device/GPU\r\nalloc: can't allocate 568118476 Bytes of memory on device/GPU\r\nalloc: can't allocate 568118476 Bytes of memory on device/GPU\r\nalloc: can't allocate 568118476 Bytes of memory on device/GPU\r\nalloc: can't allocate 568118476 Bytes of memory on device/GPU\r\nalloc: can't allocate 568118476 Bytes of memory on device/GPU\r\nalloc: can't allocate 568118476 Bytes of memory on device/GPU\r\nEnqueue process failed.\r\nException caught at file:D:\\a\\llama.cpp\\llama.cpp\\ggml\\src\\ggml-sycl\\ggml-sycl.cpp, line:3404, func:operator()\r\nSYCL error: CHECK_TRY_ERROR(dpct::gemm_batch( *main_stream, oneapi::mkl::transpose::trans, oneapi::mkl::transpose::nontrans, ne01, ne11, ne10, alpha, (const void **)(ptrs_src.get() + 0 * ne23), dpct::library_data_t::real_half, nb01 / nb00, (const void **)(ptrs_src.get() + 1 * ne23), dpct::library_data_t::real_half, nb11 / nb10, beta, (void **)(ptrs_dst.get() + 0 * ne23), cu_data_type, ne01, ne23, cu_compute_type)): Meet error in this line code!\r\n  in function ggml_sycl_mul_mat_batched_sycl at D:\\a\\llama.cpp\\llama.cpp\\ggml\\src\\ggml-sycl\\ggml-sycl.cpp:3404\r\nD:\\a\\llama.cpp\\llama.cpp\\ggml\\src\\ggml-sycl\\..\\ggml-sycl\\common.hpp:111: SYCL error\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-02T14:34:17+00:00",
    "closed_at": "2025-03-06T01:07:33+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11044/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11044"
  },
  {
    "number": 11313,
    "title": "Misc. bug: ggml files conflict between llama.cpp and whisper.cpp",
    "body": "### Name and Version\n\nllama-cpp git commit: 92bc493917d43b83e592349e138b54c90b1c3ea7\nwhisper-cpp git commit: 7a423f1c008c1d7efdee91e1ce2f8ae22f42f43b\n\n### Operating systems\n\n_No response_\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Command line\n\n```shell\n\n```\n\n### Problem description & steps to reproduce\n\nI'm trying to install both llama.cpp and whisper.cpp and find that they install different version of ggml files to the same location.\nMaybe libggml should be separated out as a standalone project?\n\nConflict files are:\n/usr/include/ggml*.h\n/usr/include/gguf.h\n/usr/lib64/libggml*.so\n\n```\nllama.cpp:\n\u2514\u2500\u2500 usr\n    \u251c\u2500\u2500 bin\n    \u2502   \u251c\u2500\u2500 convert_hf_to_gguf.py\n    \u2502   \u251c\u2500\u2500 llama-batched\n    \u2502   \u251c\u2500\u2500 llama-batched-bench\n    \u2502   \u251c\u2500\u2500 llama-bench\n    \u2502   \u251c\u2500\u2500 llama-cli\n    \u2502   \u251c\u2500\u2500 llama-convert-llama2c-to-ggml\n    \u2502   \u251c\u2500\u2500 llama-cvector-generator\n    \u2502   \u251c\u2500\u2500 llama-embedding\n    \u2502   \u251c\u2500\u2500 llama-eval-callback\n    \u2502   \u251c\u2500\u2500 llama-export-lora\n    \u2502   \u251c\u2500\u2500 llama-gbnf-validator\n    \u2502   \u251c\u2500\u2500 llama-gen-docs\n    \u2502   \u251c\u2500\u2500 llama-gguf\n    \u2502   \u251c\u2500\u2500 llama-gguf-hash\n    \u2502   \u251c\u2500\u2500 llama-gguf-split\n    \u2502   \u251c\u2500\u2500 llama-gritlm\n    \u2502   \u251c\u2500\u2500 llama-imatrix\n    \u2502   \u251c\u2500\u2500 llama-infill\n    \u2502   \u251c\u2500\u2500 llama-llava-cli\n    \u2502   \u251c\u2500\u2500 llama-lookahead\n    \u2502   \u251c\u2500\u2500 llama-lookup\n    \u2502   \u251c\u2500\u2500 llama-lookup-create\n    \u2502   \u251c\u2500\u2500 llama-lookup-merge\n    \u2502   \u251c\u2500\u2500 llama-lookup-stats\n    \u2502   \u251c\u2500\u2500 llama-minicpmv-cli\n    \u2502   \u251c\u2500\u2500 llama-parallel\n    \u2502   \u251c\u2500\u2500 llama-passkey\n    \u2502   \u251c\u2500\u2500 llama-perplexity\n    \u2502   \u251c\u2500\u2500 llama-quantize\n    \u2502   \u251c\u2500\u2500 llama-quantize-stats\n    \u2502   \u251c\u2500\u2500 llama-qwen2vl-cli\n    \u2502   \u251c\u2500\u2500 llama-retrieval\n    \u2502   \u251c\u2500\u2500 llama-run\n    \u2502   \u251c\u2500\u2500 llama-save-load-state\n    \u2502   \u251c\u2500\u2500 llama-simple\n    \u2502   \u251c\u2500\u2500 llama-simple-chat\n    \u2502   \u251c\u2500\u2500 llama-speculative\n    \u2502   \u251c\u2500\u2500 llama-speculative-simple\n    \u2502   \u251c\u2500\u2500 llama-tokenize\n    \u2502   \u251c\u2500\u2500 llama-tts\n    \u2502   \u2514\u2500\u2500 vulkan-shaders-gen\n    \u251c\u2500\u2500 include\n    \u2502   \u251c\u2500\u2500 ggml-alloc.h\n    \u2502   \u251c\u2500\u2500 ggml-backend.h\n    \u2502   \u251c\u2500\u2500 ggml-blas.h\n    \u2502   \u251c\u2500\u2500 ggml-cann.h\n    \u2502   \u251c\u2500\u2500 ggml-cpu.h\n    \u2502   \u251c\u2500\u2500 ggml-cuda.h\n    \u2502   \u251c\u2500\u2500 ggml-kompute.h\n    \u2502   \u251c\u2500\u2500 ggml-metal.h\n    \u2502   \u251c\u2500\u2500 ggml-opt.h\n    \u2502   \u251c\u2500\u2500 ggml-rpc.h\n    \u2502   \u251c\u2500\u2500 ggml-sycl.h\n    \u2502   \u251c\u2500\u2500 ggml-vulkan.h\n    \u2502   \u251c\u2500\u2500 ggml.h\n    \u2502   \u251c\u2500\u2500 gguf.h\n    \u2502   \u251c\u2500\u2500 llama-cpp.h\n    \u2502   \u2514\u2500\u2500 llama.h\n    \u251c\u2500\u2500 lib\n    \u2502   \u2514\u2500\u2500 pkgconfig\n    \u2502       \u2514\u2500\u2500 llama.pc\n    \u251c\u2500\u2500 lib64\n    \u2502   \u251c\u2500\u2500 cmake\n    \u2502   \u2502   \u2514\u2500\u2500 llama\n    \u2502   \u2502       \u251c\u2500\u2500 llama-config.cmake\n    \u2502   \u2502       \u2514\u2500\u2500 llama-version.cmake\n    \u2502   \u251c\u2500\u2500 libggml-base.so\n    \u2502   \u251c\u2500\u2500 libggml-cpu.so\n    \u2502   \u251c\u2500\u2500 libggml-hip.so\n    \u2502   \u251c\u2500\u2500 libggml-opencl.so\n    \u2502   \u251c\u2500\u2500 libggml-vulkan.so\n    \u2502   \u251c\u2500\u2500 libggml.so\n    \u2502   \u251c\u2500\u2500 libllama.so\n    \u2502   \u2514\u2500\u2500 libllava_shared.so\n    \u2514\u2500\u2500 share\n        \u2514\u2500\u2500 doc\n            \u2514\u2500\u2500 llama-cpp-9999\n                \u251c\u2500\u2500 AUTHORS\n                \u2514\u2500\u2500 README.md\n```\n\n```\nwhisper.cpp:\n\u2514\u2500\u2500 usr\n    \u251c\u2500\u2500 bin\n    \u2502   \u251c\u2500\u2500 whisper-bench\n    \u2502   \u251c\u2500\u2500 whisper-cli\n    \u2502   \u2514\u2500\u2500 whisper-server\n    \u251c\u2500\u2500 include\n    \u2502   \u251c\u2500\u2500 ggml-alloc.h\n    \u2502   \u251c\u2500\u2500 ggml-backend.h\n    \u2502   \u251c\u2500\u2500 ggml-blas.h\n    \u2502   \u251c\u2500\u2500 ggml-cann.h\n    \u2502   \u251c\u2500\u2500 ggml-cpu.h\n    \u2502   \u251c\u2500\u2500 ggml-cuda.h\n    \u2502   \u251c\u2500\u2500 ggml-kompute.h\n    \u2502   \u251c\u2500\u2500 ggml-metal.h\n    \u2502   \u251c\u2500\u2500 ggml-opt.h\n    \u2502   \u251c\u2500\u2500 ggml-rpc.h\n    \u2502   \u251c\u2500\u2500 ggml-sycl.h\n    \u2502   \u251c\u2500\u2500 ggml-vulkan.h\n    \u2502   \u251c\u2500\u2500 ggml.h\n    \u2502   \u251c\u2500\u2500 gguf.h\n    \u2502   \u2514\u2500\u2500 whisper.h\n    \u251c\u2500\u2500 lib\n    \u2502   \u2514\u2500\u2500 pkgconfig\n    \u2502       \u2514\u2500\u2500 whisper.pc\n    \u251c\u2500\u2500 lib64\n    \u2502   \u251c\u2500\u2500 cmake\n    \u2502   \u2502   \u2514\u2500\u2500 whisper\n    \u2502   \u2502       \u251c\u2500\u2500 whisper-config.cmake\n    \u2502   \u2502       \u2514\u2500\u2500 whisper-version.cmake\n    \u2502   \u251c\u2500\u2500 libggml-base.so\n    \u2502   \u251c\u2500\u2500 libggml-cpu.so\n    \u2502   \u251c\u2500\u2500 libggml.so\n    \u2502   \u251c\u2500\u2500 libwhisper.so -> libwhisper.so.1\n    \u2502   \u251c\u2500\u2500 libwhisper.so.1 -> libwhisper.so.1.7.4\n    \u2502   \u2514\u2500\u2500 libwhisper.so.1.7.4\n    \u2514\u2500\u2500 share\n        \u2514\u2500\u2500 doc\n            \u2514\u2500\u2500 whisper-cpp-9999\n                \u251c\u2500\u2500 AUTHORS\n                \u251c\u2500\u2500 README.md\n                \u2514\u2500\u2500 README_sycl.md\n```\n\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-01-20T14:21:25+00:00",
    "closed_at": "2025-01-21T01:09:49+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11313/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11313"
  },
  {
    "number": 11141,
    "title": "DeepSeek Models (V2/V3) Hang with ROCm Backend",
    "body": "### Name and Version\n\n./llama-cli --version\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 8 ROCm devices:\r\n  Device 0: AMD Instinct MI100, compute capability 9.0, VMM: no\r\n  Device 1: AMD Instinct MI100, compute capability 9.0, VMM: no\r\n  Device 2: AMD Instinct MI100, compute capability 9.0, VMM: no\r\n  Device 3: AMD Instinct MI100, compute capability 9.0, VMM: no\r\n  Device 4: AMD Instinct MI100, compute capability 9.0, VMM: no\r\n  Device 5: AMD Instinct MI100, compute capability 9.0, VMM: no\r\n  Device 6: AMD Instinct MI100, compute capability 9.0, VMM: no\r\n  Device 7: AMD Instinct MI100, compute capability 9.0, VMM: no\r\nversion: 4436 (53ff6b9b)\r\nbuilt with Ubuntu clang version 12.0.1-19ubuntu3 for x86_64-pc-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nHIP\n\n### Hardware\n\nAMD Instinct MI100\n\n### Models\n\nDeepSeek-V2\r\nDeepSeek-V3\n\n### Problem description & steps to reproduce\n\n## Description\r\nWhen attempting to run DeepSeek models (V2 or V3) using the ROCm backend, the models load successfully into VRAM but fail to generate any output. One GPU becomes pinned at 100% utilization while the others remain idle.\r\n\r\n## Commands Used\r\n### DeepSeek V2\r\n```bash\r\n./llama-cli -m /models/DeepSeek-V2-Chat-0628-Q4_K_M-00001-of-00004.gguf -ngl 999 --prompt '<\uff5cUser\uff5c>why is the sky blue?<\uff5cAssistant\uff5c>'\r\n```\r\n\r\n### DeepSeek V3\r\n```bash\r\n./llama-cli -m /models/DeepSeek-V3-Q2_K_L-00001-of-00005.gguf -ngl 48 --prompt '<\uff5cUser\uff5c>why is the sky blue?<\uff5cAssistant\uff5c>'\r\n```\r\n\r\n## Observed Behavior\r\n1. Model loads successfully and distributes across available GPUs\r\n2. After loading, one GPU gets stuck at 100% utilization\r\n3. No text generation occurs\r\n4. Other GPUs remain idle with only VRAM usage showing\r\n\r\n## Steps to Reproduce\r\n1. Load DeepSeek model (V2 or V3) using llama.cpp with ROCm backend\r\n2. Set appropriate number of layers for GPU offload (-ngl parameter)\r\n   - For V2: use -ngl 999 for automatic layer distribution\r\n   - For V3: use -ngl 48 for specific layer allocation\r\n3. Attempt text generation with any prompt using the commands shown above\r\n\r\n## Additional Notes\r\n- Both models exhibit similar behavior despite different quantization methods\r\n- Model loading and VRAM distribution appears normal\r\n- Issue occurs consistently across multiple attempts\r\n-  The same behavior happens when running deepseek-v2:16b-lite-chat-q4_K_M in Ollama\r\n\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nroot@dd8e6159288b:/app/build/bin# ./llama-cli -m /models/DeepSeek-V3-Q2_K_L-00001-of-00005.gguf -ngl 48 --prompt '<\uff5cUser\uff5c>why is the sky blue?<\uff5cAssistant\uff5c>'\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 8 ROCm devices:\r\n  Device 0: AMD Instinct MI100, compute capability 9.0, VMM: no\r\n  Device 1: AMD Instinct MI100, compute capability 9.0, VMM: no\r\n  Device 2: AMD Instinct MI100, compute capability 9.0, VMM: no\r\n  Device 3: AMD Instinct MI100, compute capability 9.0, VMM: no\r\n  Device 4: AMD Instinct MI100, compute capability 9.0, VMM: no\r\n  Device 5: AMD Instinct MI100, compute capability 9.0, VMM: no\r\n  Device 6: AMD Instinct MI100, compute capability 9.0, VMM: no\r\n  Device 7: AMD Instinct MI100, compute capability 9.0, VMM: no\r\nbuild: 4436 (53ff6b9b) with Ubuntu clang version 12.0.1-19ubuntu3 for x86_64-pc-linux-gnu\r\nmain: llama backend init\r\nmain: load the model and apply lora adapter, if any\r\nllama_model_load_from_file: using device ROCm0 (AMD Instinct MI100) - 32180 MiB free\r\nllama_model_load_from_file: using device ROCm1 (AMD Instinct MI100) - 32714 MiB free\r\nllama_model_load_from_file: using device ROCm2 (AMD Instinct MI100) - 32714 MiB free\r\nllama_model_load_from_file: using device ROCm3 (AMD Instinct MI100) - 32714 MiB free\r\nllama_model_load_from_file: using device ROCm4 (AMD Instinct MI100) - 32714 MiB free\r\nllama_model_load_from_file: using device ROCm5 (AMD Instinct MI100) - 32714 MiB free\r\nllama_model_load_from_file: using device ROCm6 (AMD Instinct MI100) - 32714 MiB free\r\nllama_model_load_from_file: using device ROCm7 (AMD Instinct MI100) - 32714 MiB free\r\nllama_model_loader: additional 4 GGUFs metadata loaded.\r\nllama_model_loader: loaded meta data with 46 key-value pairs and 1025 tensors from /deepseek-v3/deepseek-v3-unsloght/DeepSeek-V3-Q2_K_L-00001-of-00005.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = deepseek2\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek V3 BF16\r\nllama_model_loader: - kv   3:                         general.size_label str              = 256x20B\r\nllama_model_loader: - kv   4:                      deepseek2.block_count u32              = 61\r\nllama_model_loader: - kv   5:                   deepseek2.context_length u32              = 163840\r\nllama_model_loader: - kv   6:                 deepseek2.embedding_length u32              = 7168\r\nllama_model_loader: - kv   7:              deepseek2.feed_forward_length u32              = 18432\r\nllama_model_loader: - kv   8:             deepseek2.attention.head_count u32              = 128\r\nllama_model_loader: - kv   9:          deepseek2.attention.head_count_kv u32              = 128\r\nllama_model_loader: - kv  10:                   deepseek2.rope.freq_base f32              = 10000.000000\r\nllama_model_loader: - kv  11: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  12:                deepseek2.expert_used_count u32              = 8\r\nllama_model_loader: - kv  13:                          general.file_type u32              = 10\r\nllama_model_loader: - kv  14:        deepseek2.leading_dense_block_count u32              = 3\r\nllama_model_loader: - kv  15:                       deepseek2.vocab_size u32              = 129280\r\nllama_model_loader: - kv  16:            deepseek2.attention.q_lora_rank u32              = 1536\r\nllama_model_loader: - kv  17:           deepseek2.attention.kv_lora_rank u32              = 512\r\nllama_model_loader: - kv  18:             deepseek2.attention.key_length u32              = 192\r\nllama_model_loader: - kv  19:           deepseek2.attention.value_length u32              = 128\r\nllama_model_loader: - kv  20:       deepseek2.expert_feed_forward_length u32              = 2048\r\nllama_model_loader: - kv  21:                     deepseek2.expert_count u32              = 256\r\nllama_model_loader: - kv  22:              deepseek2.expert_shared_count u32              = 1\r\nllama_model_loader: - kv  23:             deepseek2.expert_weights_scale f32              = 2.500000\r\nllama_model_loader: - kv  24:              deepseek2.expert_weights_norm bool             = true\r\nllama_model_loader: - kv  25:               deepseek2.expert_gating_func u32              = 2\r\nllama_model_loader: - kv  26:             deepseek2.rope.dimension_count u32              = 64\r\nllama_model_loader: - kv  27:                deepseek2.rope.scaling.type str              = yarn\r\nllama_model_loader: - kv  28:              deepseek2.rope.scaling.factor f32              = 40.000000\r\nllama_model_loader: - kv  29: deepseek2.rope.scaling.original_context_length u32              = 4096\r\nllama_model_loader: - kv  30: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.100000\r\nllama_model_loader: - kv  31:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  32:                         tokenizer.ggml.pre str              = deepseek-v3\r\nllama_model_loader: - kv  33:                      tokenizer.ggml.tokens arr[str,129280]  = [\"<\uff5cbegin\u2581of\u2581sentence\uff5c>\", \"<\ufffd...\r\nllama_model_loader: - kv  34:                  tokenizer.ggml.token_type arr[i32,129280]  = [3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  35:                      tokenizer.ggml.merges arr[str,127741]  = [\"\u0120 t\", \"\u0120 a\", \"i n\", \"\u0120 \u0120\", \"h e...\r\nllama_model_loader: - kv  36:                tokenizer.ggml.bos_token_id u32              = 0\r\nllama_model_loader: - kv  37:                tokenizer.ggml.eos_token_id u32              = 1\r\nllama_model_loader: - kv  38:            tokenizer.ggml.padding_token_id u32              = 1\r\nllama_model_loader: - kv  39:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  40:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  41:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\r\nllama_model_loader: - kv  42:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  43:                                   split.no u16              = 0\r\nllama_model_loader: - kv  44:                                split.count u16              = 5\r\nllama_model_loader: - kv  45:                        split.tensors.count i32              = 1025\r\nllama_model_loader: - type  f32:  361 tensors\r\nllama_model_loader: - type q2_K:  482 tensors\r\nllama_model_loader: - type q3_K:  180 tensors\r\nllama_model_loader: - type q4_K:    1 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\r\nllm_load_vocab: special tokens cache size = 818\r\nllm_load_vocab: token to piece cache size = 0.8223 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = deepseek2\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 129280\r\nllm_load_print_meta: n_merges         = 127741\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 163840\r\nllm_load_print_meta: n_embd           = 7168\r\nllm_load_print_meta: n_layer          = 61\r\nllm_load_print_meta: n_head           = 128\r\nllm_load_print_meta: n_head_kv        = 128\r\nllm_load_print_meta: n_rot            = 64\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 192\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 24576\r\nllm_load_print_meta: n_embd_v_gqa     = 16384\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 18432\r\nllm_load_print_meta: n_expert         = 256\r\nllm_load_print_meta: n_expert_used    = 8\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = yarn\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 0.025\r\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 671B\r\nllm_load_print_meta: model ftype      = Q2_K - Medium\r\nllm_load_print_meta: model params     = 671.03 B\r\nllm_load_print_meta: model size       = 227.47 GiB (2.91 BPW) \r\nllm_load_print_meta: general.name     = DeepSeek V3 BF16\r\nllm_load_print_meta: BOS token        = 0 '<\uff5cbegin\u2581of\u2581sentence\uff5c>'\r\nllm_load_print_meta: EOS token        = 1 '<\uff5cend\u2581of\u2581sentence\uff5c>'\r\nllm_load_print_meta: EOT token        = 1 '<\uff5cend\u2581of\u2581sentence\uff5c>'\r\nllm_load_print_meta: PAD token        = 1 '<\uff5cend\u2581of\u2581sentence\uff5c>'\r\nllm_load_print_meta: LF token         = 131 '\u00c4'\r\nllm_load_print_meta: FIM PRE token    = 128801 '<\uff5cfim\u2581begin\uff5c>'\r\nllm_load_print_meta: FIM SUF token    = 128800 '<\uff5cfim\u2581hole\uff5c>'\r\nllm_load_print_meta: FIM MID token    = 128802 '<\uff5cfim\u2581end\uff5c>'\r\nllm_load_print_meta: EOG token        = 1 '<\uff5cend\u2581of\u2581sentence\uff5c>'\r\nllm_load_print_meta: max token length = 256\r\nllm_load_print_meta: n_layer_dense_lead   = 3\r\nllm_load_print_meta: n_lora_q             = 1536\r\nllm_load_print_meta: n_lora_kv            = 512\r\nllm_load_print_meta: n_ff_exp             = 2048\r\nllm_load_print_meta: n_expert_shared      = 1\r\nllm_load_print_meta: expert_weights_scale = 2.5\r\nllm_load_print_meta: expert_weights_norm  = 1\r\nllm_load_print_meta: expert_gating_func   = sigmoid\r\nllm_load_print_meta: rope_yarn_log_mul    = 0.1000\r\nllm_load_tensors: offloading 48 repeating layers to GPU\r\nllm_load_tensors: offloaded 48/62 layers to GPU\r\nllm_load_tensors:   CPU_Mapped model buffer size = 41684.45 MiB\r\nllm_load_tensors:        ROCm0 model buffer size = 23905.15 MiB\r\nllm_load_tensors:        ROCm1 model buffer size = 23905.15 MiB\r\nllm_load_tensors:        ROCm2 model buffer size = 23905.15 MiB\r\nllm_load_tensors:        ROCm3 model buffer size = 23905.15 MiB\r\nllm_load_tensors:        ROCm4 model buffer size = 23905.15 MiB\r\nllm_load_tensors:        ROCm5 model buffer size = 23905.15 MiB\r\nllm_load_tensors:        ROCm6 model buffer size = 23905.15 MiB\r\nllm_load_tensors:        ROCm7 model buffer size = 23905.15 MiB\r\n....................................................................................................\r\nllama_new_context_with_model: n_seq_max     = 1\r\nllama_new_context_with_model: n_ctx         = 4096\r\nllama_new_context_with_model: n_ctx_per_seq = 4096\r\nllama_new_context_with_model: n_batch       = 2048\r\nllama_new_context_with_model: n_ubatch      = 512\r\nllama_new_context_with_model: flash_attn    = 0\r\nllama_new_context_with_model: freq_base     = 10000.0\r\nllama_new_context_with_model: freq_scale    = 0.025\r\nllama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (163840) -- the full capacity of the model will not be utilized\r\nllama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 61, can_shift = 0\r\nllama_kv_cache_init:        CPU KV buffer size =  4160.00 MiB\r\nllama_kv_cache_init:      ROCm0 KV buffer size =  1920.00 MiB\r\nllama_kv_cache_init:      ROCm1 KV buffer size =  1920.00 MiB\r\nllama_kv_cache_init:      ROCm2 KV buffer size =  1920.00 MiB\r\nllama_kv_cache_init:      ROCm3 KV buffer size =  1920.00 MiB\r\nllama_kv_cache_init:      ROCm4 KV buffer size =  1920.00 MiB\r\nllama_kv_cache_init:      ROCm5 KV buffer size =  1920.00 MiB\r\nllama_kv_cache_init:      ROCm6 KV buffer size =  1920.00 MiB\r\nllama_kv_cache_init:      ROCm7 KV buffer size =  1920.00 MiB\r\nllama_new_context_with_model: KV self size  = 19520.00 MiB, K (f16): 11712.00 MiB, V (f16): 7808.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\r\nllama_new_context_with_model:      ROCm0 compute buffer size =  2790.00 MiB\r\nllama_new_context_with_model:      ROCm1 compute buffer size =  1186.00 MiB\r\nllama_new_context_with_model:      ROCm2 compute buffer size =  1186.00 MiB\r\nllama_new_context_with_model:      ROCm3 compute buffer size =  1186.00 MiB\r\nllama_new_context_with_model:      ROCm4 compute buffer size =  1186.00 MiB\r\nllama_new_context_with_model:      ROCm5 compute buffer size =  1186.00 MiB\r\nllama_new_context_with_model:      ROCm6 compute buffer size =  1186.00 MiB\r\nllama_new_context_with_model:      ROCm7 compute buffer size =  1186.00 MiB\r\nllama_new_context_with_model:  ROCm_Host compute buffer size =    88.01 MiB\r\nllama_new_context_with_model: graph nodes  = 5025\r\nllama_new_context_with_model: graph splits = 243 (with bs=512), 10 (with bs=1)\r\ncommon_init_from_params: KV cache shifting is not supported for this model, disabling KV cache shifting\r\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\r\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\nmain: llama threadpool init, n_threads = 20\r\n\r\nsystem_info: n_threads = 20 (n_threads_batch = 20) / 20 | ROCm : PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | \r\n\r\nsampler seed: 4089827234\r\nsampler params: \r\n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\r\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\r\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800\r\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\r\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \r\ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\r\n\r\nwhy is the sky blue?\r\n\r\n\r\n========================================= ROCm System Management Interface =========================================\r\n=================================================== Concise Info ===================================================\r\nDevice  Node  IDs              Temp    Power  Partitions          SCLK     MCLK     Fan  Perf  PwrCap  VRAM%  GPU%\r\n^[3m              (DID,     GUID)  (Edge)  (Avg)  (Mem, Compute, ID)                                                    ^[0m\r\n====================================================================================================================\r\n0       1     0x738c,   16733  54.0\u00b0C  97.0W  N/A, N/A, 0         1502Mhz  1200Mhz  0%   auto  290.0W  89%    100%\r\n1       2     0x738c,   57681  41.0\u00b0C  35.0W  N/A, N/A, 0         300Mhz   1200Mhz  0%   auto  290.0W  84%    0%\r\n2       3     0x738c,   33109  42.0\u00b0C  39.0W  N/A, N/A, 0         300Mhz   1200Mhz  0%   auto  290.0W  84%    0%\r\n3       4     0x738c,   8559   42.0\u00b0C  39.0W  N/A, N/A, 0         300Mhz   1200Mhz  0%   auto  290.0W  84%    0%\r\n4       5     0x738c,   57703  41.0\u00b0C  34.0W  N/A, N/A, 0         300Mhz   1200Mhz  0%   auto  290.0W  84%    0%\r\n5       6     0x738c,   33123  39.0\u00b0C  34.0W  N/A, N/A, 0         300Mhz   1200Mhz  0%   auto  290.0W  84%    0%\r\n6       7     0x738c,   57724  41.0\u00b0C  39.0W  N/A, N/A, 0         300Mhz   1200Mhz  0%   auto  290.0W  84%    0%\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-08T15:42:33+00:00",
    "closed_at": "2025-02-13T17:37:18+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11141/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11141"
  },
  {
    "number": 11196,
    "title": "Misc. bug: Docker Image llama-quantize Segmentation fault",
    "body": "### Name and Version\n\nroot@f7545b6b4f65:/app# ./llama-cli --version\r\nload_backend: loaded CPU backend from ./libggml-cpu-alderlake.so\r\nversion: 4460 (ba8a1f9c)\r\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n\n### Operating systems\n\nLinux, Other? (Please let us know in description)\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-quantize\n\n### Command line\n\n```shell\n\u276f docker run --rm -it \\                                                                                                                                          \r\n  -v ./models:/models \\\r\n  ghcr.io/ggerganov/llama.cpp:full \\\r\n  --quantize /models/BAAI/bge-small-en-v1.5/bge-small-en-v1.5-f32.gguf /models/BAAI/bge-small-en-v1.5/bge-small-en-v1.5-Q4_K_M.gguf Q4_K_M\n```\n\n\n### Problem description & steps to reproduce\n\njust try to quantize a model and you'll get the segfault\r\n```shell\r\n\u276f docker run --rm -it \\                                    \r\n  -v ./models:/models \\\r\n  ghcr.io/ggerganov/llama.cpp:full \\\r\n  --quantize /models/BAAI/bge-small-en-v1.5/bge-small-en-v1.5-f32.gguf /models/BAAI/bge-small-en-v1.5/bge-small-en-v1.5-Q4_K_M.gguf Q4_K_M\r\nmain: build = 4460 (ba8a1f9c)\r\nmain: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\r\nmain: quantizing '/models/BAAI/bge-small-en-v1.5/bge-small-en-v1.5-f32.gguf' to '/models/BAAI/bge-small-en-v1.5/bge-small-en-v1.5-Q4_K_M.gguf' as Q4_K_M\r\nllama_model_loader: loaded meta data with 30 key-value pairs and 197 tensors from /models/BAAI/bge-small-en-v1.5/bge-small-en-v1.5-f32.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = bert\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Bge Small En v1.5\r\nllama_model_loader: - kv   3:                            general.version str              = v1.5\r\nllama_model_loader: - kv   4:                           general.finetune str              = en\r\nllama_model_loader: - kv   5:                           general.basename str              = bge\r\nllama_model_loader: - kv   6:                         general.size_label str              = small\r\nllama_model_loader: - kv   7:                            general.license str              = mit\r\nllama_model_loader: - kv   8:                               general.tags arr[str,5]       = [\"sentence-transformers\", \"feature-ex...\r\nllama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\r\nllama_model_loader: - kv  10:                           bert.block_count u32              = 12\r\nllama_model_loader: - kv  11:                        bert.context_length u32              = 512\r\nllama_model_loader: - kv  12:                      bert.embedding_length u32              = 384\r\nllama_model_loader: - kv  13:                   bert.feed_forward_length u32              = 1536\r\nllama_model_loader: - kv  14:                  bert.attention.head_count u32              = 12\r\nllama_model_loader: - kv  15:          bert.attention.layer_norm_epsilon f32              = 0.000000\r\nllama_model_loader: - kv  16:                          general.file_type u32              = 0\r\nllama_model_loader: - kv  17:                      bert.attention.causal bool             = false\r\nllama_model_loader: - kv  18:                          bert.pooling_type u32              = 2\r\nllama_model_loader: - kv  19:            tokenizer.ggml.token_type_count u32              = 2\r\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = bert\r\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = jina-v2-en\r\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\r\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  24:            tokenizer.ggml.unknown_token_id u32              = 100\r\nllama_model_loader: - kv  25:          tokenizer.ggml.seperator_token_id u32              = 102\r\nllama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  27:                tokenizer.ggml.cls_token_id u32              = 101\r\nllama_model_loader: - kv  28:               tokenizer.ggml.mask_token_id u32              = 103\r\nllama_model_loader: - kv  29:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  197 tensors\r\nSegmentation fault (core dumped)\r\n```\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-11T19:39:25+00:00",
    "closed_at": "2025-03-10T01:06:59+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11196/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11196"
  },
  {
    "number": 11336,
    "title": "Misc. bug: Deepseek R1 incompatible with grammars / structured output",
    "body": "### Name and Version\n\nUsing llama.cpp at b4516 and ollama at 7bb35\n\n\n### Operating systems\n\n_No response_\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Command line\n\n```shell\n\n```\n\n### Problem description & steps to reproduce\n\nApologies if this is misfiled as a bug. I'm not sure if it is an enhancement, but grammars are a commonly used feature of llama.cpp and the Deepseek R1 model is very popular, so I wanted to raise it and perhaps think about solutions\n\n\nReasoning based models are trained to use <think></think> tokens to add their own chain of thought to the context. These tokens get suppressed by design when using the sampler with JSON grammar causing the model performance to suffer significantly. \n\nAs suppressing the output of the tokens within the <think> tokens is currently being discussed in #11325, I wanted to mention that using grammar to effectively suppress these tokens causes the model to perform badly.\n\nLooking at https://github.com/ggerganov/llama.cpp/blob/master/common/sampling.cpp#L235 , the third parameter to `common_sampler_accept` appears to be a boolean to enable constraining the sampler by grammar. As the <think> end token is defined at least in Deepseek R1 `tokens.json`, perhaps it would be a generic-enough solution to disable grammar for these models until this token is reached, if it is a declared token in the loaded model? Or, perhaps a user provided parameter like `start_grammar_after_token`?\n\n```\n        {\n            \"id\": 128799,\n            \"content\": \"</think>\",\n            \"single_word\": false,\n            \"lstrip\": false,\n            \"rstrip\": false,\n            \"normalized\": true,\n            \"special\": false\n        },\n```\n\nThanks for all your great work! <3 \n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-21T20:14:44+00:00",
    "closed_at": "2025-03-11T01:07:42+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11336/reactions",
      "total_count": 7,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11336"
  },
  {
    "number": 11347,
    "title": "Misc. bug: Failed to convert `MiniCPM-o-2_6`",
    "body": "### Name and Version\n\nBy following the steps in the [Usage of MiniCPM-o 2.6](https://github.com/ggerganov/llama.cpp/blob/master/examples/llava/README-minicpmo2.6.md#usage-of-minicpm-o-26) section, failed to convert PyTorch model to gguf files:\n\n```bash\nsam@sam-pc:~/workspace/llama.cpp$ python ./examples/llava/minicpmv-surgery.py -m /home/sam/workspace/models/MiniCPM-o-2_6\nTraceback (most recent call last):\n  File \"/home/sam/workspace/llama.cpp/./examples/llava/minicpmv-surgery.py\", line 11, in <module>\n    model = AutoModel.from_pretrained(args.model, trust_remote_code=True, local_files_only=True, torch_dtype=torch.bfloat16)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sam/miniconda3/envs/facet/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 553, in from_pretrained\n    model_class = get_class_from_dynamic_module(\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sam/miniconda3/envs/facet/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 552, in get_class_from_dynamic_module\n    return get_class_in_module(class_name, final_module, force_reload=force_download)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sam/miniconda3/envs/facet/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 237, in get_class_in_module\n    module_files: List[Path] = [module_file] + sorted(map(Path, get_relative_import_files(module_file)))\n                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sam/miniconda3/envs/facet/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 128, in get_relative_import_files\n    new_imports.extend(get_relative_imports(f))\n                       ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sam/miniconda3/envs/facet/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 97, in get_relative_imports\n    with open(module_file, \"r\", encoding=\"utf-8\") as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/home/sam/.cache/huggingface/modules/transformers_modules/MiniCPM-o-2_6/image_processing_minicpmv.py'\n```\n\n### Operating systems\n\n_No response_\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Command line\n\n```shell\n\n```\n\n### Problem description & steps to reproduce\n\n1. Check out `b4525` and build the code\n2. Run `git clone git@hf.co:openbmb/MiniCPM-o-2_6`\n3. In the root directory of `llama.cpp`, run `python ./examples/llava/minicpmv-surgery.py -m ../MiniCPM-o-2_6`\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-22T09:04:30+00:00",
    "closed_at": "2025-04-05T01:07:37+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11347/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11347"
  },
  {
    "number": 11054,
    "title": "Eval bug: Output token sequence cannot match with AutoTokenizer",
    "body": "### Name and Version\n\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 8 CUDA devices:\r\n  Device 0: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\r\n  Device 1: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\r\n  Device 2: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\r\n  Device 3: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\r\n  Device 4: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\r\n  Device 5: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\r\n  Device 6: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\r\n  Device 7: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\r\nversion: 4354 (0e70ba6)\r\nbuilt with cc (GCC) 9.3.1 20200408 (Red Hat 9.3.1-2) for x86_64-redhat-linux\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\nNVIDIA A100-SXM4-80GB\n\n### Models\n\nMeta-Llama-3-8B-Instruct\n\n### Problem description & steps to reproduce\n\nFound that the output token sequence cannot match exactly between  `llama-tokenize` and `AutoTokenizer` for models like `Meta-Llama-3-8B-Instruct`, `internlm2_5-7b-chat`. \r\n# reproduce\r\n1. convert model to gguf\r\n```\r\npython3 convert_hf_to_gguf.py \\\r\n$model_path \\\r\n--outfile $gguf_path\r\n```\r\n2. run `llama-tokenize`\r\n```\r\nprompt=\"<|im_start|>user\\nhello who are you?<|im_end|>\\n<|im_start|>assistant\\n\"\r\n./build/bin/llama-tokenize -m \\\r\n./Meta-Llama-3-8B-Instruct.gguf \\\r\n-p \"$prompt\" \\\r\n--ids\r\n```\r\n\r\n3. run with  AutoTokenizer from transformers\r\n```\r\nfrom transformers import AutoTokenizer\r\nmodel_path = './Meta-Llama-3-8B-Instruct'\r\n# model_path = './internlm2_5-7b-chat'\r\n\r\ntk = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\nprompts = \"<|im_start|>user\\nhello who are you?<|im_end|>\\n<|im_start|>assistant\\n\"\r\nprint(tk.encode(prompts))\r\n```\r\n\r\n# results\r\n## Meta-Llama-3-8B-Instruct\r\n```\r\nllama-tokenize\r\n[27, 91, 318, 5011, 91, 29, 882, 1734, 15339, 889, 527, 499, 76514, 91, 318, 6345, 91, 8616, 77, 27, 91, 318, 5011, 91, 29, 78191, 1734]\r\nAutoTokenizer\r\n[27, 91, 318, 5011, 91, 29, 882, 198, 15339, 889, 527, 499, 76514, 91, 318, 6345, 91, 397, 27, 91, 318, 5011, 91, 29, 78191, 198]\r\n```\r\n\r\n## internlm2_5-7b-chat\r\n```\r\nllama-tokenize\r\n[1, 92543, 1008, 1849, 15115, 1015, 657, 629, 345, 92542, 1849, 92543, 525, 11353, 1849]\r\nAutoTokenizer\r\n[1, 92543, 1008, 364, 15115, 1015, 657, 629, 345, 92542, 364, 92543, 525, 11353, 364]\r\n```\r\n\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 8 CUDA devices:\r\n  Device 0: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\r\n  Device 1: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\r\n  Device 2: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\r\n  Device 3: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\r\n  Device 4: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\r\n  Device 5: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\r\n  Device 6: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\r\n  Device 7: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\r\nllama_load_model_from_file: using device CUDA0 (NVIDIA A100-SXM4-80GB) - 10133 MiB free\r\nllama_load_model_from_file: using device CUDA1 (NVIDIA A100-SXM4-80GB) - 80614 MiB free\r\nllama_load_model_from_file: using device CUDA2 (NVIDIA A100-SXM4-80GB) - 11791 MiB free\r\nllama_load_model_from_file: using device CUDA3 (NVIDIA A100-SXM4-80GB) - 80614 MiB free\r\nllama_load_model_from_file: using device CUDA4 (NVIDIA A100-SXM4-80GB) - 80614 MiB free\r\nllama_load_model_from_file: using device CUDA5 (NVIDIA A100-SXM4-80GB) - 80614 MiB free\r\nllama_load_model_from_file: using device CUDA6 (NVIDIA A100-SXM4-80GB) - 80614 MiB free\r\nllama_load_model_from_file: using device CUDA7 (NVIDIA A100-SXM4-80GB) - 80614 MiB free\r\nllama_model_loader: loaded meta data with 31 key-value pairs and 291 tensors from Meta-Llama-3-8B-Instruct.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Meta Llama 3 8B Instruct\r\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\r\nllama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3\r\nllama_model_loader: - kv   5:                         general.size_label str              = 8B\r\nllama_model_loader: - kv   6:                            general.license str              = other\r\nllama_model_loader: - kv   7:                       general.license.name str              = llama3\r\nllama_model_loader: - kv   8:                       general.license.link str              = LICENSE\r\nllama_model_loader: - kv   9:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\r\nllama_model_loader: - kv  10:                          general.languages arr[str,1]       = [\"en\"]\r\nllama_model_loader: - kv  11:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv  12:                       llama.context_length u32              = 8192\r\nllama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\r\nllama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  19:                          general.file_type u32              = 1\r\nllama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\r\nllama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smaug-bpe\r\nllama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"\u0120 \u0120\", \"\u0120 \u0120\u0120\u0120\", \"\u0120\u0120 \u0120\u0120\", \"...\r\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\r\nllama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128001\r\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\r\nllama_model_loader: - kv  30:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type  f16:  226 tensors\r\nllm_load_vocab: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG\r\n....\r\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\r\nllm_load_vocab: special tokens cache size = 256\r\nllm_load_vocab: token to piece cache size = 0.8000 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 128256\r\nllm_load_print_meta: n_merges         = 280147\r\nllm_load_print_meta: vocab_only       = 1\r\nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model ftype      = all F32\r\nllm_load_print_meta: model params     = 8.03 B\r\nllm_load_print_meta: model size       = 14.96 GiB (16.00 BPW) \r\nllm_load_print_meta: general.name     = Meta Llama 3 8B Instruct\r\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\r\nllm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\r\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: LF token         = 128 '\u00c4'\r\nllm_load_print_meta: EOG token        = 128001 '<|end_of_text|>'\r\nllm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: max token length = 256\r\nllama_model_load: vocab only - skipping tensors\r\nllama_new_context_with_model: n_seq_max     = 1\r\nllama_new_context_with_model: n_ctx         = 512\r\nllama_new_context_with_model: n_ctx_per_seq = 512\r\nllama_new_context_with_model: n_batch       = 512\r\nllama_new_context_with_model: n_ubatch      = 512\r\nllama_new_context_with_model: flash_attn    = 0\r\nllama_new_context_with_model: freq_base     = 0.0\r\nllama_new_context_with_model: freq_scale    = 1\r\nllama_new_context_with_model: n_ctx_pre_seq (512) > n_ctx_train (0) -- possible training context overflow\r\n[27, 91, 318, 5011, 91, 29, 882, 1734, 15339, 889, 527, 499, 76514, 91, 318, 6345, 91, 8616, 77, 27, 91, 318, 5011, 91, 29, 78191, 1734]\n```\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-01-03T09:42:19+00:00",
    "closed_at": "2025-01-06T08:54:26+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11054/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11054"
  },
  {
    "number": 10980,
    "title": "Eval bug: ",
    "body": "### Name and Version\n\n\u51fa\u73b0\u7684\u95ee\u9898\r\nWelcome to Qwen.cpp! Ask whatever you want. Type 'clear' to clear context. Type 'stop' to exit.\r\n\r\nPrompt > hello\r\nqwen > unknown token: 151935\r\n\r\nubuntu\u7248\u672c\uff1a20.04\r\n\u4f7f\u7528\u7684\u8f6f\u4ef6\uff1aqwen.cpp 1.3 \u548c1.2\u90fd\u8bd5\u8fc7\r\n\u4f7f\u7528\u7684\u786c\u4ef6\uff1ajetson orin nano 4gb   jetpack\uff1a5.1.1\r\n\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\njetson orin nano 4gb   jetpack\uff1a5.1.1\n\n### Models\n\nQwen-1_8B-Chat \n\n### Problem description & steps to reproduce\n\n\u4f7f\u7528\u91cf\u5316\u8f6c\u8bd1\u540e\u7684qwen1.8B \u8fd0\u884c\u5728orin nano\u4e2d \u4f7f\u7528qwen.cpp\u8fd0\u884c  \u65e0\u6cd5\u8bc6\u522b\u6587\u672c \u62a5\u4f4d\u7f6e\u9519\u8bef\r\ncuda\u7b49\u57fa\u7840ai\u73af\u5883\u5df2\u7ecf\u88c5\u597d\u57fa\u7840ai\u73af\u5883\r\n![\u5fae\u4fe1\u56fe\u7247_2024-12-26_164133_589](https://github.com/user-attachments/assets/e8a67d37-365e-4cba-b0e1-2980414170d3)\r\n\r\n\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nWelcome to Qwen.cpp! Ask whatever you want. Type 'clear' to clear context. Type 'stop' to exit.\r\n\r\nPrompt > hello\r\nqwen > unknown token: 151935\n```\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-26T08:42:42+00:00",
    "closed_at": "2025-02-09T01:07:26+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10980/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10980"
  },
  {
    "number": 11144,
    "title": "Misc. bug:  llama-server - shared libraries after build 4409, last working 4406",
    "body": "### Name and Version\n\nllama-b4447-bin-ubuntu-x64\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nOther (Please specify in the next section)\n\n### Command line\n\n_No response_\n\n### Problem description & steps to reproduce\n\nllama-b4406-bin-ubuntu-x64.zip - works - as no external .so required\r\nllama-b4409-bin-ubuntu-x64.zip - not possible as :\r\nldd llama-server\r\n        libllama.so => not found\r\n        libggml.so => not found\r\n        libggml-base.so => not found\r\nall that .so are not included in compiled .zip assets on github\r\nllama-b4447-bin-ubuntu-x64.zip - same not included.\r\n\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-08T20:01:17+00:00",
    "closed_at": "2025-03-11T01:07:48+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11144/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11144"
  },
  {
    "number": 10985,
    "title": "Misc. bug: llama-qwen2vl-cli: ignores --log* options",
    "body": "### Name and Version\n\n\u2030 ./bin/llama-qwen2vl-cli --version\r\nversion: 4391 (9ba399df)\r\nbuilt with cc (Gentoo Hardened 14.2.1_p20241221 p6) 14.2.1 20241221 for x86_64-pc-linux-gnu\n\n### Operating systems\n\n_No response_\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Problem description & steps to reproduce\n\nUse `--log-file /dev/null` or `--log-verbosity -100`. Note lines like `clip_model_load: model name:   Qwen2-VL-7B-Instruct` still being produced in stdout.\r\nThere's seemingly no way to isolate the model output from the miscellaneous messages of llama.cpp.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-26T16:04:54+00:00",
    "closed_at": "2025-03-01T01:07:45+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10985/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10985"
  },
  {
    "number": 11290,
    "title": "Feature Request: MiniMax-Text-01 model",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nPlease add support for minimax-text-01 model https://huggingface.co/MiniMaxAI/MiniMax-Text-01\nhttps://github.com/MiniMax-AI/MiniMax-01\n\n### Motivation\n\nWe need to add support for the latest models! Performs almost as good as deepseek v3. But has 4 million tokens context.\n\n### Possible Implementation\n\nIt's a MoE model.",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-18T15:38:54+00:00",
    "closed_at": "2025-03-12T01:07:33+00:00",
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11290/reactions",
      "total_count": 10,
      "+1": 10,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11290"
  },
  {
    "number": 11204,
    "title": "\"CPU_AARCH64 model buffer\" appears when not using AARCH64",
    "body": "### Name and Version\r\n\r\nbuild: 4465 (9a483999) with gcc (conda-forge gcc 13.3.0-1) 13.3.0 for x86_64-conda-linux-gnu\r\n\r\n### Operating systems\r\n\r\nLinux\r\n\r\n### GGML backends\r\n\r\nCPU\r\n\r\n### Hardware\r\n\r\n2x Intel Xeon 24 core (Kaggle)\r\n\r\n### Models\r\n\r\nDeepSeek-V2.5: https://huggingface.co/bartowski/DeepSeek-V2.5-GGUF/tree/main/DeepSeek-V2.5-Q4_0\r\n\r\n### Problem description & steps to reproduce\r\n\r\nThe problem is that a part of the memory was used for \"CPU_AARCH64 model buffer\". Normally the model takes only 150GB of RAM, now it takes 260GB and loads much slower. Command line: `/root/llama.cpp/build/bin/llama-server -m /dev/shm/DeepSeek-V2.5-Q4_0-00001-of-00004.gguf -t 72`. This doesn't appear when using Q4_K_M.\r\n\r\nCompile commands:\r\n```\r\ngit clone https://github.com/ggerganov/llama.cpp ~/llama.cpp\r\ncd ~/llama.cpp && cmake -G Ninja -B build && cmake --build build --config Release -j 64\r\n```\r\n\r\n### First Bad Commit\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nbuild: 4465 (9a483999) with gcc (conda-forge gcc 13.3.0-1) 13.3.0 for x86_64-conda-linux-gnu\r\nsystem info: n_threads = 72, n_threads_batch = 72, total_threads = 96\r\n\r\nsystem_info: n_threads = 72 (n_threads_batch = 72) / 96 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \r\n\r\nmain: HTTP server is listening, hostname: 127.0.0.1, port: 8080, http threads: 95\r\nmain: loading model\r\nsrv    load_model: loading model '/dev/shm/DeepSeek-V2.5-Q4_0-00001-of-00004.gguf'\r\nllama_model_loader: additional 3 GGUFs metadata loaded.\r\nllama_model_loader: loaded meta data with 53 key-value pairs and 959 tensors from /dev/shm/DeepSeek-V2.5-Q4_0-00001-of-00004.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = deepseek2\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek V2.5\r\nllama_model_loader: - kv   3:                            general.version str              = V2.5\r\nllama_model_loader: - kv   4:                           general.basename str              = DeepSeek\r\nllama_model_loader: - kv   5:                         general.size_label str              = 160x14B\r\nllama_model_loader: - kv   6:                            general.license str              = other\r\nllama_model_loader: - kv   7:                       general.license.name str              = deepseek\r\nllama_model_loader: - kv   8:                       general.license.link str              = https://github.com/deepseek-ai/DeepSe...\r\nllama_model_loader: - kv   9:                      deepseek2.block_count u32              = 60\r\nllama_model_loader: - kv  10:                   deepseek2.context_length u32              = 163840\r\nllama_model_loader: - kv  11:                 deepseek2.embedding_length u32              = 5120\r\nllama_model_loader: - kv  12:              deepseek2.feed_forward_length u32              = 12288\r\nllama_model_loader: - kv  13:             deepseek2.attention.head_count u32              = 128\r\nllama_model_loader: - kv  14:          deepseek2.attention.head_count_kv u32              = 128\r\nllama_model_loader: - kv  15:                   deepseek2.rope.freq_base f32              = 10000.000000\r\nllama_model_loader: - kv  16: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  17:                deepseek2.expert_used_count u32              = 6\r\nllama_model_loader: - kv  18:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  19:        deepseek2.leading_dense_block_count u32              = 1\r\nllama_model_loader: - kv  20:                       deepseek2.vocab_size u32              = 102400\r\nllama_model_loader: - kv  21:            deepseek2.attention.q_lora_rank u32              = 1536\r\nllama_model_loader: - kv  22:           deepseek2.attention.kv_lora_rank u32              = 512\r\nllama_model_loader: - kv  23:             deepseek2.attention.key_length u32              = 192\r\nllama_model_loader: - kv  24:           deepseek2.attention.value_length u32              = 128\r\nllama_model_loader: - kv  25:       deepseek2.expert_feed_forward_length u32              = 1536\r\nllama_model_loader: - kv  26:                     deepseek2.expert_count u32              = 160\r\nllama_model_loader: - kv  27:              deepseek2.expert_shared_count u32              = 2\r\nllama_model_loader: - kv  28:             deepseek2.expert_weights_scale f32              = 16.000000\r\nllama_model_loader: - kv  29:             deepseek2.rope.dimension_count u32              = 64\r\nllama_model_loader: - kv  30:                deepseek2.rope.scaling.type str              = yarn\r\nllama_model_loader: - kv  31:              deepseek2.rope.scaling.factor f32              = 40.000000\r\nllama_model_loader: - kv  32: deepseek2.rope.scaling.original_context_length u32              = 4096\r\nllama_model_loader: - kv  33: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.100000\r\nllama_model_loader: - kv  34:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  35:                         tokenizer.ggml.pre str              = deepseek-llm\r\nllama_model_loader: - kv  36:                      tokenizer.ggml.tokens arr[str,102400]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  37:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  38:                      tokenizer.ggml.merges arr[str,99757]   = [\"\u0120 \u0120\", \"\u0120 t\", \"\u0120 a\", \"i n\", \"h e...\r\nllama_model_loader: - kv  39:                tokenizer.ggml.bos_token_id u32              = 100000\r\nllama_model_loader: - kv  40:                tokenizer.ggml.eos_token_id u32              = 100001\r\nllama_model_loader: - kv  41:            tokenizer.ggml.padding_token_id u32              = 100001\r\nllama_model_loader: - kv  42:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  43:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  44:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\r\nllama_model_loader: - kv  45:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  46:                      quantize.imatrix.file str              = /models_out/DeepSeek-V2.5-GGUF/DeepSe...\r\nllama_model_loader: - kv  47:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\r\nllama_model_loader: - kv  48:             quantize.imatrix.entries_count i32              = 716\r\nllama_model_loader: - kv  49:              quantize.imatrix.chunks_count i32              = 139\r\nllama_model_loader: - kv  50:                                   split.no u16              = 0\r\nllama_model_loader: - kv  51:                                split.count u16              = 4\r\nllama_model_loader: - kv  52:                        split.tensors.count i32              = 959\r\nllama_model_loader: - type  f32:  300 tensors\r\nllama_model_loader: - type q4_0:  645 tensors\r\nllama_model_loader: - type q4_1:   13 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nprint_info: file format = GGUF V3 (latest)\r\nprint_info: file type   = Q4_0\r\nprint_info: file size   = 124.23 GiB (4.53 BPW) \r\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\r\nload: special tokens cache size = 18\r\nload: token to piece cache size = 0.6411 MB\r\nprint_info: arch             = deepseek2\r\nprint_info: vocab_only       = 0\r\nprint_info: n_ctx_train      = 163840\r\nprint_info: n_embd           = 5120\r\nprint_info: n_layer          = 60\r\nprint_info: n_head           = 128\r\nprint_info: n_head_kv        = 128\r\nprint_info: n_rot            = 64\r\nprint_info: n_swa            = 0\r\nprint_info: n_embd_head_k    = 192\r\nprint_info: n_embd_head_v    = 128\r\nprint_info: n_gqa            = 1\r\nprint_info: n_embd_k_gqa     = 24576\r\nprint_info: n_embd_v_gqa     = 16384\r\nprint_info: f_norm_eps       = 0.0e+00\r\nprint_info: f_norm_rms_eps   = 1.0e-06\r\nprint_info: f_clamp_kqv      = 0.0e+00\r\nprint_info: f_max_alibi_bias = 0.0e+00\r\nprint_info: f_logit_scale    = 0.0e+00\r\nprint_info: n_ff             = 12288\r\nprint_info: n_expert         = 160\r\nprint_info: n_expert_used    = 6\r\nprint_info: causal attn      = 1\r\nprint_info: pooling type     = 0\r\nprint_info: rope type        = 0\r\nprint_info: rope scaling     = yarn\r\nprint_info: freq_base_train  = 10000.0\r\nprint_info: freq_scale_train = 0.025\r\nprint_info: n_ctx_orig_yarn  = 4096\r\nprint_info: rope_finetuned   = unknown\r\nprint_info: ssm_d_conv       = 0\r\nprint_info: ssm_d_inner      = 0\r\nprint_info: ssm_d_state      = 0\r\nprint_info: ssm_dt_rank      = 0\r\nprint_info: ssm_dt_b_c_rms   = 0\r\nprint_info: model type       = 236B\r\nprint_info: model params     = 235.74 B\r\nprint_info: general.name     = DeepSeek V2.5\r\nprint_info: n_layer_dense_lead   = 1\r\nprint_info: n_lora_q             = 1536\r\nprint_info: n_lora_kv            = 512\r\nprint_info: n_ff_exp             = 1536\r\nprint_info: n_expert_shared      = 2\r\nprint_info: expert_weights_scale = 16.0\r\nprint_info: expert_weights_norm  = 0\r\nprint_info: expert_gating_func   = softmax\r\nprint_info: rope_yarn_log_mul    = 0.1000\r\nprint_info: vocab type       = BPE\r\nprint_info: n_vocab          = 102400\r\nprint_info: n_merges         = 99757\r\nprint_info: BOS token        = 100000 '<\uff5cbegin\u2581of\u2581sentence\uff5c>'\r\nprint_info: EOS token        = 100001 '<\uff5cend\u2581of\u2581sentence\uff5c>'\r\nprint_info: EOT token        = 100001 '<\uff5cend\u2581of\u2581sentence\uff5c>'\r\nprint_info: PAD token        = 100001 '<\uff5cend\u2581of\u2581sentence\uff5c>'\r\nprint_info: LF token         = 126 '\u00c4'\r\nprint_info: FIM PRE token    = 100003 '<\uff5cfim\u2581begin\uff5c>'\r\nprint_info: FIM SUF token    = 100002 '<\uff5cfim\u2581hole\uff5c>'\r\nprint_info: FIM MID token    = 100004 '<\uff5cfim\u2581end\uff5c>'\r\nprint_info: EOG token        = 100001 '<\uff5cend\u2581of\u2581sentence\uff5c>'\r\nprint_info: max token length = 256\r\nload_tensors:   CPU_Mapped model buffer size = 37602.27 MiB\r\nload_tensors:   CPU_Mapped model buffer size = 34353.59 MiB\r\nload_tensors:   CPU_Mapped model buffer size = 36378.63 MiB\r\nload_tensors:   CPU_Mapped model buffer size = 12801.23 MiB\r\nload_tensors:  CPU_AARCH64 model buffer size = 121738.36 MiB\r\nllama_init_from_model: n_seq_max     = 1\r\nllama_init_from_model: n_ctx         = 4096\r\nllama_init_from_model: n_ctx_per_seq = 4096\r\nllama_init_from_model: n_batch       = 2048\r\nllama_init_from_model: n_ubatch      = 512\r\nllama_init_from_model: flash_attn    = 0\r\nllama_init_from_model: freq_base     = 10000.0\r\nllama_init_from_model: freq_scale    = 0.025\r\nllama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (163840) -- the full capacity of the model will not be utilized\r\nllama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 60, can_shift = 0\r\nllama_kv_cache_init:        CPU KV buffer size = 19200.00 MiB\r\nllama_init_from_model: KV self size  = 19200.00 MiB, K (f16): 11520.00 MiB, V (f16): 7680.00 MiB\r\nllama_init_from_model:        CPU  output buffer size =     0.39 MiB\r\nllama_init_from_model:        CPU compute buffer size =  1174.01 MiB\r\nllama_init_from_model: graph nodes  = 4480\r\nllama_init_from_model: graph splits = 1\r\ncommon_init_from_params: KV cache shifting is not supported for this model, disabling KV cache shifting\r\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\r\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\nsrv          init: initializing slots, n_slots = 1\r\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 4096\r\nmain: model loaded\r\nmain: chat template, chat_template: (built-in), example_format: 'You are a helpful assistant\r\n\r\n<\uff5cUser\uff5c>Hello<\uff5cAssistant\uff5c>Hi there<\uff5cend\u2581of\u2581sentence\uff5c><\uff5cUser\uff5c>How are you?<\uff5cAssistant\uff5c>'\r\nmain: server is listening on http://127.0.0.1:8080 - starting the main loop\r\nsrv  update_slots: all slots are idle\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-12T13:43:12+00:00",
    "closed_at": "2025-04-13T01:34:13+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11204/reactions",
      "total_count": 5,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11204"
  },
  {
    "number": 10982,
    "title": "Research: Performance differences between Metal (macOS) and Vulkan (Linux)",
    "body": "I'm one of the developers for the Asahi Linux GPU drivers, which provide accelerated Vulkan and OpenGL support on Apple Silicon platforms. I'm interested in improving the performance of llama.cpp on our drivers with the Vulkan backend.\r\n\r\nAs things stand today, macOS is significantly faster on a quick test with `llama-bench`, with default settings (tested on an M2 Max 64GB):\r\n\r\nLinux:\r\n\r\n```\r\nggml_vulkan: Found 1 Vulkan devices:\r\nggml_vulkan: 0 = Apple M2 Max (G14C B1) (Honeykrisp) | uma: 1 | fp16: 1 | warp size: 32 | matrix cores: none\r\n| model                          |       size |     params | backend    | ngl |          test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ------------: | -------------------: |\r\nggml_vulkan: Compiling shaders................................Done!\r\n| llama 7B Q4_K - Medium         |   4.07 GiB |     7.24 B | Vulkan     |  99 |         pp512 |         92.16 \u00b1 0.08 |\r\n| llama 7B Q4_K - Medium         |   4.07 GiB |     7.24 B | Vulkan     |  99 |         tg128 |         21.93 \u00b1 0.02 |\r\n\r\nbuild: 9ba399dfa7f1 (4391)\r\n```\r\n\r\nmacOS:\r\n\r\n```\r\n./build/bin/llama-bench -m /Volumes/Untitled/mistral-7b-v0.1.Q4_K_M.gguf \r\n| model                          |       size |     params | backend    | threads |          test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |\r\n| llama 7B Q4_K - Medium         |   4.07 GiB |     7.24 B | Metal,BLAS,RPC |       8 |         pp512 |        580.26 \u00b1 8.82 |\r\n| llama 7B Q4_K - Medium         |   4.07 GiB |     7.24 B | Metal,BLAS,RPC |       8 |         tg128 |         61.18 \u00b1 0.41 |\r\n\r\nbuild: 9ba399df (4391)\r\n```\r\n\r\n(I also tested a larger 70B model which failed to load due to failing to allocate memory on Linux, but that's obviously a separate issue that's easy to debug. Probably just a hardcoded alloc size limit in the driver we can raise, since we recently refactored a bunch of stuff to handle >4G buffers properly.)\r\n\r\nOf course, we'd like to improve the driver where possible to make things faster. However, since I know nothing about how LLMs are implemented under the hood, or the state of the llama.cpp Metal and Vulkan backends I would like to ask for help figuring out the perf issues, and analyzing whether llama.cpp itself could also be part of the root cause.\r\n\r\nWould you be able to help us out? I'm curious about these things:\r\n\r\n* The state of the Metal vs. Vulkan backends, and whether any perf differences could be expected on the same hardware based on that alone (are the shaders and the way the workload is run essentially identical, or are there major differences?).\r\n* How to get more information on how the work is scheduled on both Metal and Vulkan (block sizes, shared memory allocations, and things like that), so we can identify if there are any differences or choices at the llama.cpp level that could explain perf differences.\r\n* How to run smaller micro-benchmarks. To work out driver and shader compiler issues, ideally we'd want to narrow it down to single shaders / compute launches, and measure the performance individually.\r\n* General info on what to expect and where we should dig deeper. Are things usually memory-bandwidth-bound (I understand that's the case for LLMs)? Or is it likely we'll run into ALU-bound shaders? Is there any heavy synchronization involved, or are we mostly dealing with large compute launches that stand alone? Is cache performance critical, and could differences in data layout or processing order matter, if any?",
    "labels": [
      "research \ud83d\udd2c",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-26T11:12:21+00:00",
    "closed_at": "2025-05-04T01:08:09+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10982/reactions",
      "total_count": 23,
      "+1": 14,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 9,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10982"
  },
  {
    "number": 11308,
    "title": "Eval bug: segfault on Alpine linux docker image",
    "body": "### Name and Version\n\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = Intel(R) Arc(tm) A770 Graphics (DG2) (Intel open-source Mesa driver) | uma: 0 | fp16: 1 | warp size: 32 | matrix cores: none\nversion: 0 (unknown)\nbuilt with cc (Alpine 14.2.0) 14.2.0 for x86_64-alpine-linux-musl\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nVulkan\n\n### Hardware\n\nAMD Ryzen\u2122 9 7950X \u00d7 32\nIntel\u00ae Arc\u2122 A770 Graphics (DG2)\n\nRaspberry Pi 5\nAMD 7600 XT\n\n### Models\n\nQwen2.5-0.5B-Instruct-Q4_K_M.gguf\n\n### Problem description & steps to reproduce\n\nRunning llama.cpp in Docker on Alpine linux segfaults on model compilation.\nObserved both on my x86 machine as well as my raspberry pi.\n\n```dockerfile\nFROM alpine\n\nRUN apk add --no-cache mesa-vulkan-ati mesa-vulkan-intel vulkan-loader-dev vulkan-tools cmake ninja build-base curl-dev shaderc musl-dev linux-headers\nADD https://github.com/ggerganov/llama.cpp/archive/refs/heads/master.tar.gz /llama.tgz\nRUN tar xzf /llama.tgz\n\nWORKDIR /llama.cpp-master\nRUN cmake -G Ninja -B build -DGGML_NATIVE=OFF -DLLAMA_CURL=ON -DGGML_VULKAN=1\nRUN cmake --build build --config Release\nRUN cmake --install build\n\nCMD [\"llama-server\", \"-hfr\", \"bartowski/Qwen2.5-0.5B-Instruct-GGUF\", \"-hff\", \"Qwen2.5-0.5B-Instruct-Q4_K_M.gguf\", \"-ngl\", \"999\"]\n```\n\n```bash\nsudo docker build -t local/my-test-addon   .\nsudo docker run -it  --rm   -v /tmp/my_test_data:/root/.cache/llama.cpp/   --device=/dev/dri:/dev/dri   local/my-test-addon\n```\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = Intel(R) Arc(tm) A770 Graphics (DG2) (Intel open-source Mesa driver) | uma: 0 | fp16: 1 | warp size: 32 | matrix cores: none\nbuild: 0 (unknown) with cc (Alpine 14.2.0) 14.2.0 for x86_64-alpine-linux-musl\nsystem info: n_threads = 16, n_threads_batch = 16, total_threads = 32\n\nsystem_info: n_threads = 16 (n_threads_batch = 16) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n\nmain: HTTP server is listening, hostname: 127.0.0.1, port: 8080, http threads: 31\nmain: loading model\nsrv    load_model: loading model '/root/.cache/llama.cpp/bartowski_Qwen2.5-0.5B-Instruct-GGUF_Qwen2.5-0.5B-Instruct-Q4_K_M.gguf'\ncommon_download_file: previous metadata file found /root/.cache/llama.cpp/bartowski_Qwen2.5-0.5B-Instruct-GGUF_Qwen2.5-0.5B-Instruct-Q4_K_M.gguf.json: {\"etag\":\"\\\"3b7ae4ea73a7691fd70b250b8ed86788-25\\\"\",\"lastModified\":\"Thu, 19 Sep 2024 17:49:41 GMT\",\"url\":\"https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q4_K_M.gguf\"}\ncurl_perform_with_retry: Trying to download from https://huggingface.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q4_K_M.gguf (attempt 1 of 3)...\nllama_model_load_from_file_impl: using device Vulkan0 (Intel(R) Arc(tm) A770 Graphics (DG2)) - 16288 MiB free\nllama_model_loader: loaded meta data with 38 key-value pairs and 290 tensors from /root/.cache/llama.cpp/bartowski_Qwen2.5-0.5B-Instruct-GGUF_Qwen2.5-0.5B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen2.5 0.5B Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\nllama_model_loader: - kv   5:                         general.size_label str              = 0.5B\nllama_model_loader: - kv   6:                            general.license str              = apache-2.0\nllama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-0...\nllama_model_loader: - kv   8:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 0.5B\nllama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen\nllama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-0.5B\nllama_model_loader: - kv  12:                               general.tags arr[str,2]       = [\"chat\", \"text-generation\"]\nllama_model_loader: - kv  13:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  14:                          qwen2.block_count u32              = 24\nllama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 896\nllama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 4864\nllama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 14\nllama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 2\nllama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  22:                          general.file_type u32              = 15\nllama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\nllama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  33:               general.quantization_version u32              = 2\nllama_model_loader: - kv  34:                      quantize.imatrix.file str              = /models_out/Qwen2.5-0.5B-Instruct-GGU...\nllama_model_loader: - kv  35:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\nllama_model_loader: - kv  36:             quantize.imatrix.entries_count i32              = 168\nllama_model_loader: - kv  37:              quantize.imatrix.chunks_count i32              = 128\nllama_model_loader: - type  f32:  121 tensors\nllama_model_loader: - type q5_0:  132 tensors\nllama_model_loader: - type q8_0:   13 tensors\nllama_model_loader: - type q4_K:   12 tensors\nllama_model_loader: - type q6_K:   12 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 373.71 MiB (6.35 BPW) \nload: special tokens cache size = 22\nload: token to piece cache size = 0.9310 MB\nprint_info: arch             = qwen2\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 32768\nprint_info: n_embd           = 896\nprint_info: n_layer          = 24\nprint_info: n_head           = 14\nprint_info: n_head_kv        = 2\nprint_info: n_rot            = 64\nprint_info: n_swa            = 0\nprint_info: n_embd_head_k    = 64\nprint_info: n_embd_head_v    = 64\nprint_info: n_gqa            = 7\nprint_info: n_embd_k_gqa     = 128\nprint_info: n_embd_v_gqa     = 128\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: n_ff             = 4864\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 32768\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 1B\nprint_info: model params     = 494.03 M\nprint_info: general.name     = Qwen2.5 0.5B Instruct\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 148848 '\u00c4\u012c'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nggml_vulkan: Compiling shaders.................................\n# segfault message only printed when launching from bash inside the container\n```",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-01-20T11:12:22+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11308/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11308"
  },
  {
    "number": 11232,
    "title": "Feature Request: Support for iFlytek Spark 13B",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nI am attempting to run the iFlytek Spark model using llama.cpp, as I consider it a new and relevant model. I have worked on building the computation graph and performing all the necessary tasks required for the model's operation. However, despite these efforts, the model fails to function as expected, consistently producing chaotic output. I would appreciate any guidance or suggestions to address this issue.\n\n### Motivation\n\niFlytek Spark is one of the most prominent Chinese models currently available on the market. Enabling access to iFlytek Spark through llama.cpp would be a valuable contribution to the community.\n\n### Possible Implementation\n\nI have implemented functions such as build_{model}(), llm_load_tensors(), and other necessary functionalities. However, I am encountering challenges debugging the build_{model}() method. The primary resources available for this model are the weights (base/chat in float32 format) and the code. Since there is no paper or visual representation of the architecture, and the code is highly configurable, it has been difficult to deduce the exact architecture of the model. I would appreciate guidance in resolving these issues.\r\n\r\nCode -  https://gitee.com/mindspore/mindformers/blob/r1.0/research/iflytekspark/iflytekspark_layers.py\r\nWeights - https://gitee.com/iflytekopensource/i-flytek-spark-13-b-model-gpu/blob/master/iFlytekSpark_13B_base_fp32/mp_rank_00_model_states.pt\r\n\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-14T10:08:56+00:00",
    "closed_at": "2025-02-28T01:07:25+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11232/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11232"
  },
  {
    "number": 11095,
    "title": "Compile bug: Compilation fails due to -D_XOPEN_SOURCE=600: error: use of undeclared identifier 'strnlen'",
    "body": "### Git commit\r\n\r\n4418\r\n\r\n### Operating systems\r\n\r\nBSD\r\n\r\n### GGML backends\r\n\r\nCPU\r\n\r\n### Problem description & steps to reproduce\r\n\r\n[This -D_XOPEN_SOURCE=600 argument](https://github.com/ggerganov/llama.cpp/blob/master/Makefile#L286) breaks compilation:\r\n\r\n```\r\nIn file included from /usr/ports/misc/llama-cpp/work/llama.cpp-b4418/ggml/src/ggml-vulkan/ggml-vulkan.cpp:8:\r\n/usr/local/include/vulkan/vulkan.hpp:145:41: error: use of undeclared identifier 'strnlen'\r\n  145 |       return std::string( this->data(), strnlen( this->data(), N ) );\r\n      |                                         ^\r\n```\r\n\r\nThe same argument is set in cmake scripts as well.\r\n\r\nPlease don't fix POSIX support at random ancient levels.\r\n\r\nIn general it shouldn't be necessary to ever set _XOPEN_SOURCE.\r\n\r\nFreeBSD 14.2\r\n\r\n### First Bad Commit\r\n\r\nn/a\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nn/a\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-06T00:33:47+00:00",
    "closed_at": "2025-04-07T01:09:15+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11095/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11095"
  },
  {
    "number": 11041,
    "title": "Misc. bug: RISCV output bug when using rvv with vlen > 256bit",
    "body": "### Name and Version\r\n\r\n./llama-cli latest version ubuntu linux riscv\r\n\r\n### Operating systems\r\n\r\nLinux\r\n\r\n### Which llama.cpp modules do you know to be affected?\r\n\r\nllama-cli\r\n\r\n### Problem description & steps to reproduce\r\n\r\nThe generated output of llama-cli in riscv env with rvv enabled consists of trash when vlen is greater than 256bits\r\n\r\n### First Bad Commit\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-02T09:15:01+00:00",
    "closed_at": "2025-04-16T01:07:54+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11041/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11041"
  },
  {
    "number": 11351,
    "title": "Feature Request: `reasoning_effort` parameter for reasoning models like R1",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nSimilar to how the o1 family of models accepts a `reasoning_effort` parameter (low, medium, high), it's concievable that OSS reasoning models should have this too.\n\nDeepseek mentions that this param will soon be available via their API: https://api-docs.deepseek.com/guides/reasoning_model#api-parameters\n\n> Note that the CoT output can reach up to 32K tokens, and the parameter to control the CoT length (reasoning_effort) will be available soon.\n\n\n\n### Motivation\n\nThis would allow us to run reasoning models continuously for long periods of time, hopefully, in exchange for better results\n\n### Possible Implementation\n\nI have no idea on the best implementation, though I've seen a few early options in the wild:\n\n> what if the lever is just a penalty on the `</think>` token? that would be so stupid and therefore must be correct.\n> -- https://x.com/doomslide/status/1881310996515680411\n\n> If `</think>` appears before enough thinking tokens have been generated, the sampler replaces it with a random string like \"Wait, but\". Works surprisingly well for how simple it is!\n> -- https://x.com/voooooogel/status/1881966969043464365",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-22T15:22:22+00:00",
    "closed_at": "2025-03-13T01:07:53+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11351/reactions",
      "total_count": 3,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11351"
  },
  {
    "number": 11184,
    "title": "Feature Request: index.html.gz as a separate distributable",
    "body": "### Prerequisites\r\n\r\n- [X] I am running the latest code. Mention the version if possible as well.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\r\n\r\n### Feature Description\r\n\r\nWhen running server, with different LLMs, sometimes you want to switch the system prompt on the Web ui.  Currently, it can save one prompt only. It'd be great to be able to use different prompt. It'd open up more functionality (different prompts for different models, censoring/non-censoring prompts, jailbreak prompts, specially designed prompts in general) if it can be easily modified (dropdown box options etc.), or just have more slots for system prompts.\r\n\r\n### Motivation\r\n\r\nsystem prompt need more slots \r\n\r\n### Possible Implementation\r\n\r\nindex.html modification and systemMessage selection to allow for multiple system prompts slots",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-11T03:12:17+00:00",
    "closed_at": "2025-02-25T01:07:30+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11184/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11184"
  },
  {
    "number": 11306,
    "title": "Compile bug: libggml-vulkan.so reproducible builds issue",
    "body": "### Git commit\n\n667d72846c06b2cf4f7c8a4265e210991a49706b\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nVulkan\n\n### Problem description & steps to reproduce\n\nI'm compiling the `llamacpp` openSUSE package and found that the `libggml-vulkan.so` differs in every build.\n\nMy analysis points to\nhttps://github.com/ggerganov/llama.cpp/blob/bd38dde/ggml/src/ggml-vulkan/vulkan-shaders/vulkan-shaders-gen.cpp#L502\nas the place where some sorting is missing to provide deterministic output.\n\n### First Bad Commit\n\n_No response_\n\n### Compile command\n\n```shell\nwe build with cmake - see https://github.com/bmwiedemann/openSUSE/blob/0da890b/packages/l/llamacpp/llamacpp.spec#L123 for details.\n```\n\n### Relevant log output\n\n```shell\nGenerated source-code varied (randomly) thusly:\n\n/home/abuild/rpmbuild/BUILD.unsorted/llamacpp-4501/build/ggml/src/ggml-vulkan/ggml-vulkan-shaders.cpp\n@@ -1,8 +1,892 @@\n #include \"ggml-vulkan-shaders.hpp\"\n \n-unsigned char matmul_f16_f32_aligned_fp32_data[10276] = {\n+unsigned char matmul_q4_1_f16_fp32_data[10564] = {\n...\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-01-20T09:58:48+00:00",
    "closed_at": "2025-01-23T07:07:51+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11306/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11306"
  },
  {
    "number": 11105,
    "title": "Eval bug: input is too large to process. increase the physical batch size",
    "body": "### Name and Version\r\n\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 2 CUDA devices:\r\n  Device 0: NVIDIA A800 80GB PCIe, compute capability 8.0, VMM: yes\r\n  Device 1: NVIDIA A800 80GB PCIe, compute capability 8.0, VMM: yes\r\nversion: 0 (unknown)\r\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\r\n\r\n### Operating systems\r\n\r\nLinux\r\n\r\n### GGML backends\r\n\r\nCUDA\r\n\r\n### Hardware\r\n\r\nA800 * 2 ;\r\n\r\n### Models\r\n\r\nbge-reranker-v2-m3\uff0c q8_0\r\n\r\n### Problem description & steps to reproduce\r\n\r\nwhen I use `llama-server`  to inference, I get this **error**:\r\n```\r\n{\"error\":{\"code\":500,\"message\":\"input is too large to process. increase the physical batch size\",\"type\":\"server_error\"}}\r\n```\r\n\r\nmy llama-server is :\r\n```\r\nCUDA_VISIBLE_DEVICES=\"0\" /data/fffan/other/llama.cpp-master/build_cuda/bin/llama-server \\\r\n              -m ./bge-reranker-v2-m3_quanto_llama/bge-reranker-v2-m3_q8_0.gguf \\\r\n              --reranking \\\r\n              -cd 4096 -c 4096\r\n```\r\n\r\n### First Bad Commit\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\nggml_cuda_init: found 1 CUDA devices:\r\n  Device 0: NVIDIA A800 80GB PCIe, compute capability 8.0, VMM: yes\r\nbuild: 0 (unknown) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\r\nsystem info: n_threads = 32, n_threads_batch = 32, total_threads = 64\r\n\r\nsystem_info: n_threads = 32 (n_threads_batch = 32) / 64 | CUDA : ARCHS = 800 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \r\n\r\nmain: HTTP server is listening, hostname: 127.0.0.1, port: 8080, http threads: 63\r\nmain: loading model\r\nsrv    load_model: loading model './bge-reranker-v2-m3_quanto_llama/bge-reranker-v2-m3_q8_0.gguf'\r\nllama_load_model_from_file: using device CUDA0 (NVIDIA A800 80GB PCIe) - 32811 MiB free\r\nllama_model_loader: loaded meta data with 35 key-value pairs and 393 tensors from ./bge-reranker-v2-m3_quanto_llama/bge-reranker-v2-m3_q8_0.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = bert\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Bge M3\r\nllama_model_loader: - kv   3:                       general.organization str              = BAAI\r\nllama_model_loader: - kv   4:                         general.size_label str              = 568M\r\nllama_model_loader: - kv   5:                            general.license str              = apache-2.0\r\nllama_model_loader: - kv   6:                               general.tags arr[str,3]       = [\"transformers\", \"sentence-transforme...\r\nllama_model_loader: - kv   7:                          general.languages arr[str,1]       = [\"multilingual\"]\r\nllama_model_loader: - kv   8:                           bert.block_count u32              = 24\r\nllama_model_loader: - kv   9:                        bert.context_length u32              = 8192\r\nllama_model_loader: - kv  10:                      bert.embedding_length u32              = 1024\r\nllama_model_loader: - kv  11:                   bert.feed_forward_length u32              = 4096\r\nllama_model_loader: - kv  12:                  bert.attention.head_count u32              = 16\r\nllama_model_loader: - kv  13:          bert.attention.layer_norm_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  14:                          general.file_type u32              = 7\r\nllama_model_loader: - kv  15:                      bert.attention.causal bool             = false\r\nllama_model_loader: - kv  16:                       tokenizer.ggml.model str              = t5\r\nllama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = default\r\nllama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,250002]  = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \",\"...\r\nllama_model_loader: - kv  19:                      tokenizer.ggml.scores arr[f32,250002]  = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,250002]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  21:            tokenizer.ggml.add_space_prefix bool             = true\r\nllama_model_loader: - kv  22:            tokenizer.ggml.token_type_count u32              = 1\r\nllama_model_loader: - kv  23:    tokenizer.ggml.remove_extra_whitespaces bool             = true\r\nllama_model_loader: - kv  24:        tokenizer.ggml.precompiled_charsmap arr[u8,237539]   = [0, 180, 2, 0, 0, 132, 0, 0, 0, 0, 0,...\r\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 0\r\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 3\r\nllama_model_loader: - kv  28:          tokenizer.ggml.seperator_token_id u32              = 2\r\nllama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 1\r\nllama_model_loader: - kv  30:                tokenizer.ggml.cls_token_id u32              = 0\r\nllama_model_loader: - kv  31:               tokenizer.ggml.mask_token_id u32              = 250001\r\nllama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = true\r\nllama_model_loader: - kv  34:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  247 tensors\r\nllama_model_loader: - type q8_0:  146 tensors\r\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\r\nllm_load_vocab: special tokens cache size = 4\r\nllm_load_vocab: token to piece cache size = 2.1668 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = bert\r\nllm_load_print_meta: vocab type       = UGM\r\nllm_load_print_meta: n_vocab          = 250002\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 8192\r\nllm_load_print_meta: n_embd           = 1024\r\nllm_load_print_meta: n_layer          = 24\r\nllm_load_print_meta: n_head           = 16\r\nllm_load_print_meta: n_head_kv        = 16\r\nllm_load_print_meta: n_rot            = 64\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 64\r\nllm_load_print_meta: n_embd_head_v    = 64\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 1.0e-05\r\nllm_load_print_meta: f_norm_rms_eps   = 0.0e+00\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 4096\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 0\r\nllm_load_print_meta: pooling type     = -1\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 8192\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\r\nllm_load_print_meta: model type       = 335M\r\nllm_load_print_meta: model ftype      = Q8_0\r\nllm_load_print_meta: model params     = 567.75 M\r\nllm_load_print_meta: model size       = 599.70 MiB (8.86 BPW) \r\nllm_load_print_meta: general.name     = Bge M3\r\nllm_load_print_meta: BOS token        = 0 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 3 '<unk>'\r\nllm_load_print_meta: SEP token        = 2 '</s>'\r\nllm_load_print_meta: PAD token        = 1 '<pad>'\r\nllm_load_print_meta: CLS token        = 0 '<s>'\r\nllm_load_print_meta: MASK token       = 250001 '[PAD250000]'\r\nllm_load_print_meta: LF token         = 6 '\u2581'\r\nllm_load_print_meta: EOG token        = 2 '</s>'\r\nllm_load_print_meta: max token length = 48\r\nllm_load_tensors: offloading 0 repeating layers to GPU\r\nllm_load_tensors: offloaded 0/25 layers to GPU\r\nllm_load_tensors:   CPU_Mapped model buffer size =   599.70 MiB\r\n......................................................\r\nllama_new_context_with_model: n_seq_max     = 1\r\nllama_new_context_with_model: n_ctx         = 4096\r\nllama_new_context_with_model: n_ctx_per_seq = 4096\r\nllama_new_context_with_model: n_batch       = 2048\r\nllama_new_context_with_model: n_ubatch      = 512\r\nllama_new_context_with_model: flash_attn    = 0\r\nllama_new_context_with_model: freq_base     = 10000.0\r\nllama_new_context_with_model: freq_scale    = 1\r\nllama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\r\nllama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24\r\nllama_kv_cache_init:        CPU KV buffer size =   384.00 MiB\r\nllama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.00 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =    26.07 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB\r\nllama_new_context_with_model: graph nodes  = 854\r\nllama_new_context_with_model: graph splits = 392 (with bs=512), 1 (with bs=1)\r\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\r\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\r\nsrv          init: initializing slots, n_slots = 1\r\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 4096\r\nmain: model loaded\r\nmain: The chat template that comes with this model is not yet supported, falling back to chatml. This may cause the model to output suboptimal responses\r\nmain: chat template, built_in: 0, chat_example: '<|im_start|>system\r\nYou are a helpful assistant<|im_end|>\r\n<|im_start|>user\r\nHello<|im_end|>\r\n<|im_start|>assistant\r\nHi there<|im_end|>\r\n<|im_start|>user\r\nHow are you?<|im_end|>\r\n<|im_start|>assistant\r\n'\r\nmain: server is listening on http://127.0.0.1:8080 - starting the main loop\r\nsrv  update_slots: all slots are idle\r\nslot launch_slot_: id  0 | task 0 | processing task\r\nslot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 3262\r\nslot      release: id  0 | task 0 | stop processing: n_past = 0, truncated = 0\r\nsrv    send_error: task id = 0, error: input is too large to process. increase the physical batch size\r\nsrv  update_slots: no tokens to decode\r\nsrv  update_slots: all slots are idle\r\nsrv  cancel_tasks: cancel task, id_task = 0\r\nsrv  update_slots: all slots are idle\r\n```\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-01-06T11:58:47+00:00",
    "closed_at": "2025-01-07T01:38:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11105/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11105"
  },
  {
    "number": 11198,
    "title": "Eval bug: Crash with filesystem error when run while in a directory containing files with certain names",
    "body": "### Name and Version\n\nversion: 4462 (c05e8c99)\r\nbuilt with cc (Debian 12.2.0-14) 12.2.0 for x86_64-linux-gnu\r\n\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCPU\n\n### Hardware\n\nN/A\n\n### Models\n\nN/A\n\n### Problem description & steps to reproduce\n\nRunning `llama-cli` or `llama-server` while the current directory contains a file with certain characters in the file name causes a crash with the filesystem error shown below.\r\n\r\nHere is an example of a character that will cause the crash when present in a file name\r\n\uff5c\n\n### First Bad Commit\n\ncommit 60cfa72\n\n### Relevant log output\n\n```shell\nterminate called after throwing an instance of 'std::filesystem::__cxx11::filesystem_error'\r\n  what():  filesystem error: Cannot convert character sequence: Invalid or incomplete multibyte or wide character\r\nAborted\n```\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-01-11T21:39:49+00:00",
    "closed_at": "2025-03-02T21:11:01+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11198/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11198"
  },
  {
    "number": 11084,
    "title": "Misc. bug: ggml_backend_sycl_graph_compute: error: op not supported node_1586 (FLASH_ATTN_EXT)",
    "body": "### Name and Version\r\n\r\nSYCL RPC server build running in this docker image `intel/deep-learning-essentials:2025.0.1-0-devel-ubuntu22.04`\r\nA master revision of llama.cpp on 03/01/2025, I do not know the specific commit.\r\n\r\n### Operating systems\r\n\r\nLinux\r\n\r\n### Which llama.cpp modules do you know to be affected?\r\nRPC server\r\n\r\n### Problem description & steps to reproduce\r\n\r\n![Screenshot from 2025-01-05 01-05-51](https://github.com/user-attachments/assets/a2c409fc-de21-4a6c-81f4-ae50e4939b12)\r\n\r\nOccurred when llama-server was running this way:\r\n`bins_llmcpp/llama-server -m $MODEL_PATH -p \"$MODEL_PROMPT\" --threads 16 --n-gpu-layers \"$gpu_layers\" --ctx_size 8192 --repeat_penalty 1 --temp 0.1 --no-kv-offload --rpc 127.0.0.1:50025 --flash-attn`\r\n\r\nIf there is no  `--flash-attn` specified everything works fine. I understand that the RPC is an experimental, though a very helpful feature. I have never met such an error with CUDA RPC server container, which has  the same code revision as the SYCL one. \r\n\r\nThese are build instructions from my Dockerfile.sycl:\r\n`cmake -G \"Ninja Multi-Config\" -B build -DGGML_CUDA=OFF -DGGML_NATIVE=OFF -DGGML_CCACHE=OFF -DGGML_SYCL=ON -DGGML_SYCL_F16=ON -DGGML_RPC=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx && cmake --build ${BUILD_DIR} --config ${BUILD_CONFIG_TYPE}`\r\n\r\nJust in case, these are build instructions from my Dockerfile.cuda:\r\n`cmake -G \"Ninja Multi-Config\" -B build -DGGML_CUDA=ON -DGGML_RPC=ON -DGGML_AVX=ON -DGGML_AVX2=ON -DGGML_CCACHE=OFF && cmake --build ${BUILD_DIR} --config ${BUILD_CONFIG_TYPE}`\r\n\r\nwhere `BUILD_CONFIG_TYPE=Release`\r\n\r\nThe SYCL compose file config is the following:\r\n`services:\r\n  backend-igpu:\r\n    image: llama.cpp/sycl:latest\r\n    restart: \"unless-stopped\"\r\n    environment:\r\n      APP_MODE: backend\r\n      APP_MEM: 4096\r\n    devices:\r\n      - \"/dev/dri/renderD128:/dev/dri/renderD128\"\r\n      - \"/dev/dri/card1:/dev/dri/card1\"\r\n    ports:\r\n      - \"50025:50052\"`\r\n\r\n### First Bad Commit\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nAttaching to backend-igpu-1\r\nbackend-igpu-1  | \r\nbackend-igpu-1  | Executing command: /app/rpc-server --host 0.0.0.0 --port 50052 --mem 4096\r\nbackend-igpu-1  | \r\nbackend-igpu-1  | \r\nbackend-igpu-1  | !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\nbackend-igpu-1  | WARNING: Host ('0.0.0.0') is != '127.0.0.1'\r\nbackend-igpu-1  |          Never expose the RPC server to an open network!\r\nbackend-igpu-1  |          This is an experimental feature and is not secure!\r\nbackend-igpu-1  | !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\nbackend-igpu-1  | \r\nbackend-igpu-1  | create_backend: using SYCL backend\r\nbackend-igpu-1  | [SYCL] call ggml_check_sycl\r\nbackend-igpu-1  | ggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nbackend-igpu-1  | ggml_check_sycl: GGML_SYCL_F16: yes\r\nbackend-igpu-1  | Found 1 SYCL devices:\r\nbackend-igpu-1  | |  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\nbackend-igpu-1  | |  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\nbackend-igpu-1  | |ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\nbackend-igpu-1  | |--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\nbackend-igpu-1  | | 0| [level_zero:gpu:0]|                 Intel Iris Xe Graphics|   12.3|     80|     512|   32| 30886M|         1.3.30049+10|\r\nbackend-igpu-1  | ggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nbackend-igpu-1  | ggml_sycl_init: SYCL_USE_XMX: yes\r\nbackend-igpu-1  | ggml_sycl_init: found 1 SYCL devices:\r\nbackend-igpu-1  | Starting RPC server on 0.0.0.0:50052, backend memory: 4096 MB\r\nbackend-igpu-1  | Accepted client connection, free_mem=4294967296, total_mem=4294967296\r\nbackend-igpu-1  | Client connection closed\r\nbackend-igpu-1  | Accepted client connection, free_mem=4294967296, total_mem=4294967296\r\nbackend-igpu-1  | Client connection closed\r\nbackend-igpu-1  | Accepted client connection, free_mem=4294967296, total_mem=4294967296\r\nbackend-igpu-1  | Client connection closed\r\nbackend-igpu-1  | Accepted client connection, free_mem=4294967296, total_mem=4294967296\r\nbackend-igpu-1  | Client connection closed\r\nbackend-igpu-1  | Accepted client connection, free_mem=4294967296, total_mem=4294967296\r\nbackend-igpu-1  | ggml_backend_sycl_graph_compute: error: op not supported node_1586 (FLASH_ATTN_EXT)\r\nbackend-igpu-1  | /app/llama.cpp/ggml/src/ggml-sycl/ggml-sycl.cpp:4213: GGML_ASSERT(ok) failed\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-05T09:59:12+00:00",
    "closed_at": "2025-02-19T01:12:39+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11084/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11084"
  },
  {
    "number": 11055,
    "title": "Misc. bug: ",
    "body": "### Name and Version\n\nLatest\n\n### Operating systems\n\n_No response_\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Problem description & steps to reproduce\n\nin llama-android, when trying to unload the model, the app crashes in while clearing batch -\r\nllama_batch_free.\r\nSuggestion:- Use common_batch_clear(*reinterpret_cast<llama_batch *>(batch_pointer)); instead llama-android.cpp\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-03T09:55:13+00:00",
    "closed_at": "2025-02-17T01:07:26+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11055/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11055"
  },
  {
    "number": 11361,
    "title": "Eval bug: very slow inference on DeepSeek-R1-Distill-Qwen-32B",
    "body": "### Name and Version\n\nfrom master(22 jan)\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\n3060 on main pc +(3060+3060+1660ti/sup+1660ti/sup) on other pc\n\n### Models\n\nDeepSeek-R1-Distill-Qwen-32B_q8\n\n### Problem description & steps to reproduce\n\nhello,  i running inference on https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B (locally ggufed to q8) on 2 pc over network (connection via 100 mb wan (iperf is good) via vpn),  i get very slow inference with low cpu/gpu usage on 2 pc's .\n.`/llama-server --host 192.168.2.109 --port 8080 -m unsloth_DeepSeek-R1-Distill-Qwen-32B/DeepSeek-R1-Distill-Qwen-32B-Q8_0.gguf --rpc 10.2.0.5:1000,10.2.0.5:1001,10.2.0.5:1002,10.2.0.5:1003  -ngl 99999`\nmain host:\n![Image](https://github.com/user-attachments/assets/155e24ec-fc0b-4cd6-8245-f89ca470be03)\nremote host:\n![Image](https://github.com/user-attachments/assets/2d7b9ff2-17a4-4a54-8cea-d3f4e07b8a65)\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nllama_init_from_model: n_seq_max     = 1                                                                                    \nllama_init_from_model: n_ctx         = 4096                                                                                 \nllama_init_from_model: n_ctx_per_seq = 4096                                                                                 \nllama_init_from_model: n_batch       = 2048                                                                                 \nllama_init_from_model: n_ubatch      = 512                                                                                  \nllama_init_from_model: flash_attn    = 0                                                                                    \nllama_init_from_model: freq_base     = 1000000.0                                                                            \nllama_init_from_model: freq_scale    = 1                                                                                    \nllama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized   \nllama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1               \nllama_kv_cache_init: RPC[10.2.0.5:1000] KV buffer size =   256.00 MiB                                                       \nllama_kv_cache_init: RPC[10.2.0.5:1001] KV buffer size =   272.00 MiB                                                       \nllama_kv_cache_init: RPC[10.2.0.5:1002] KV buffer size =   128.00 MiB                                                       \nllama_kv_cache_init: RPC[10.2.0.5:1003] KV buffer size =    96.00 MiB                                                       \nllama_kv_cache_init:      CUDA0 KV buffer size =   272.00 MiB \nllama_init_from_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB  \nllama_init_from_model:        CPU  output buffer size =     0.58 MiB                                                        \nllama_init_from_model:      CUDA0 compute buffer size =   368.00 MiB                                                        \nllama_init_from_model: RPC[10.2.0.5:1000] compute buffer size =   368.00 MiB                                                \nllama_init_from_model: RPC[10.2.0.5:1001] compute buffer size =   368.00 MiB                                                \nllama_init_from_model: RPC[10.2.0.5:1002] compute buffer size =   368.00 MiB                                                \nllama_init_from_model: RPC[10.2.0.5:1003] compute buffer size =   368.00 MiB                                                \nllama_init_from_model:  CUDA_Host compute buffer size =    18.01 MiB                                                        \nllama_init_from_model: graph nodes  = 2246                                                                                  \nllama_init_from_model: graph splits = 6                                                                                     \ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096                                                      \ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)                  \nsrv          init: initializing slots, n_slots = 1                                                                          \nslot         init: id  0 | task -1 | new slot n_ctx_slot = 4096        \n\n\n\nslot launch_slot_: id  0 | task 0 | processing task\nslot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 10\nslot update_slots: id  0 | task 0 | kv cache rm [0, end)\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 10, n_tokens = 10, progress = 1.000000\nslot update_slots: id  0 | task 0 | prompt done, n_past = 10, n_tokens = 10\nslot      release: id  0 | task 0 | stop processing: n_past = 130, truncated = 0\nslot print_timing: id  0 | task 0 | \nprompt eval time =    2635.52 ms /    10 tokens (  263.55 ms per token,     3.79 tokens per second)\n       eval time =  197690.16 ms /   121 tokens ( 1633.80 ms per token,     0.61 tokens per second)\n      total time =  200325.68 ms /   131 tokens\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-22T21:38:51+00:00",
    "closed_at": "2025-04-02T01:07:52+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11361/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11361"
  }
]
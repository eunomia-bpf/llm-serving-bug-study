[
  {
    "number": 14599,
    "title": "Eval bug: thinking not working if \"tool_choice\" is \"required\" for Qwen models (QwQ, Qwen3, etc.)",
    "body": "### Name and Version\n\ndocker image ghcr.io/ggml-org/llama.cpp:server-cuda-b5849\n\nThe models don't think if \"tool_choice\" is set as \"required\"\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\n12700  + 3090\n\n### Models\n\nhttps://huggingface.co/Qwen/Qwen3-32B-GGUF/blob/main/Qwen3-32B-Q4_K_M.gguf\nhttps://huggingface.co/Qwen/QwQ-32B-GGUF\n\n### Problem description & steps to reproduce\n\nset \"tool_choice\" as \"required\" then the model don't think. If not set, the model will think and call the tools.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n{\nchoices: [\n0: {\nfinish_reason: \"tool_calls\"\nindex: 0\nmessage: {\nrole: \"assistant\"\ncontent: null\ntool_calls: [\n0: {\n...}]\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-07-09T16:41:49+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14599/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14599"
  },
  {
    "number": 14740,
    "title": "Misc. bug: out of memory error after PR #13746",
    "body": "### Name and Version\n\nTested on latest master, and multiple previous versions. Currently at:\n```\nversion: 1313 (086cf81)\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n```\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Command line\n\n```shell\nCUDA_VISIBLE_DEVICES=2 llama-server -m models/Phi-4-mini-instruct.BF16.gguf -ngl 80 --host :: --port 31420 --jinja --ctx-size 0 --no-kv-offload -t 2\n```\n\n### Problem description & steps to reproduce\n\nHello. I've updated today from f125b8dccff34439a26bf750c9edef358c48c1f8 and llama-server is now throwing out of memory errors. I've backtracked and traced the issue to [PR #13746](https://github.com/ggml-org/llama.cpp/pull/13746). I am not familiar with the codebase so not sure what exactly this changed.\n\nThe server has 256GB RAM and 4x old Nvidia M60 cards, Ubuntu 22.04 with CUDA 12.9. I'm running phi-4-mini-instruct which until now this commit fit narrowly by offloading KV cache to CPU.\n\nCommit eb3949938e82a128855bc0676220bb2ce6e4228d was the last one that worked:\n```\n# \nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla M60, compute capability 5.2, VMM: yes\nbuild: 941 (eb39499) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nsystem info: n_threads = 2, n_threads_batch = 2, total_threads = 56\n\nsystem_info: n_threads = 2 (n_threads_batch = 2) / 56 | CUDA : ARCHS = 500,610,700,750,800,860,890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |\n\nmain: binding port with default address family\nmain: HTTP server is listening, hostname: ::, port: 31420, http threads: 55\nmain: loading model\nsrv    load_model: loading model '../models/Phi-4-mini-instruct.BF16.gguf'\nllama_model_load_from_file_impl: using device CUDA0 (Tesla M60) - 8040 MiB free\nllama_model_loader: loaded meta data with 35 key-value pairs and 196 tensors from ../models/Phi-4-mini-instruct.BF16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = phi3\nllama_model_loader: - kv   1:              phi3.rope.scaling.attn_factor f32              = 1.190238\nllama_model_loader: - kv   2:                               general.type str              = model\nllama_model_loader: - kv   3:                               general.name str              = Phi 4 Mini Instruct\nllama_model_loader: - kv   4:                       general.organization str              = Microsoft\nllama_model_loader: - kv   5:                           general.finetune str              = instruct\nllama_model_loader: - kv   6:                           general.basename str              = Phi-4\nllama_model_loader: - kv   7:                       general.quantized_by str              = Unsloth\nllama_model_loader: - kv   8:                         general.size_label str              = mini\nllama_model_loader: - kv   9:                           general.repo_url str              = https://huggingface.co/unsloth\nllama_model_loader: - kv  10:                        phi3.context_length u32              = 131072\nllama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096\nllama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072\nllama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192\nllama_model_loader: - kv  14:                           phi3.block_count u32              = 32\nllama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 24\nllama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96\nllama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  20:                          general.file_type u32              = 32\nllama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144\nllama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = gpt-4o\nllama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,200064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,200064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,199742]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"e r\", ...\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 199999\nllama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 200020\nllama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 3251\nllama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 200029\nllama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  32:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  33:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\nllama_model_loader: - kv  34:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   67 tensors\nllama_model_loader: - type bf16:  129 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = BF16\nprint_info: file size   = 7.15 GiB (16.00 BPW)\nload_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)\nload: special tokens cache size = 14\nload: token to piece cache size = 1.3333 MB\nprint_info: arch             = phi3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 3072\nprint_info: n_layer          = 32\nprint_info: n_head           = 24\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 96\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 3\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 8192\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 4096\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 3B\nprint_info: model params     = 3.84 B\nprint_info: general.name     = Phi 4 Mini Instruct\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 200064\nprint_info: n_merges         = 199742\nprint_info: BOS token        = 199999 '<|endoftext|>'\nprint_info: EOS token        = 200020 '<|end|>'\nprint_info: EOT token        = 199999 '<|endoftext|>'\nprint_info: UNK token        = 3251 '\ufffd'\nprint_info: PAD token        = 200029 '<\uff5cPAD\u2581TOKEN\uff5c>'\nprint_info: LF token         = 198 '\u010a'\nprint_info: EOG token        = 199999 '<|endoftext|>'\nprint_info: EOG token        = 200020 '<|end|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 32 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 33/33 layers to GPU\nload_tensors:        CUDA0 model buffer size =  7317.01 MiB\nload_tensors:   CPU_Mapped model buffer size =  1172.25 MiB\n......................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 131072\nllama_context: n_ctx_per_seq = 131072\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 10000.0\nllama_context: freq_scale    = 1\nllama_context:  CUDA_Host  output buffer size =     0.76 MiB\nllama_kv_cache_unified:        CPU KV buffer size = 16384.00 MiB\nllama_kv_cache_unified: size = 16384.00 MiB (131072 cells,  32 layers,  1 seqs), K (f16): 8192.00 MiB, V (f16): 8192.00 MiB\nllama_context:      CUDA0 compute buffer size =   396.75 MiB\nllama_context:  CUDA_Host compute buffer size =  6410.01 MiB\nllama_context: graph nodes  = 1414\nllama_context: graph splits = 66\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 131072\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nsrv          init: initializing slots, n_slots = 1\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 131072\nmain: model loaded\nmain: chat template, chat_template: {% for message in messages %}{% if message['role'] == 'system' and 'tools' in message and message['tools'] is not none %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|tool|>' + message['tools'] + '<|/tool|>' + '<|end|>' }}{% else %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|end|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>' }}{% endif %}, example_format: '<|system|>You are a helpful assistant<|end|><|user|>Hello<|end|><|assistant|>Hi there<|end|><|user|>How are you?<|end|><|assistant|>'\nmain: server is listening on http://:::31420 - starting the main loop\nsrv  update_slots: all slots are idle\n```\n```\n+-----------------------------------------+------------------------+----------------------+\n|   2  Tesla M60                      On  |   00000000:0A:00.0 Off |                  Off |\n| N/A   37C    P0             37W /  150W |    8009MiB /   8192MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n```\n\nAt 12d0188c0dc6146ffde6d277a93f232ccbe699f8 though, it fails to start due to lack of memory:\n```\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla M60, compute capability 5.2, VMM: yes\nbuild: 942 (12d0188) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nsystem info: n_threads = 2, n_threads_batch = 2, total_threads = 56\n\nsystem_info: n_threads = 2 (n_threads_batch = 2) / 56 | CUDA : ARCHS = 500,610,700,750,800,860,890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |\n\nmain: binding port with default address family\nmain: HTTP server is listening, hostname: ::, port: 31420, http threads: 55\nmain: loading model\nsrv    load_model: loading model '../models/Phi-4-mini-instruct.BF16.gguf'\nllama_model_load_from_file_impl: using device CUDA0 (Tesla M60) - 8040 MiB free\nllama_model_loader: loaded meta data with 35 key-value pairs and 196 tensors from ../models/Phi-4-mini-instruct.BF16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = phi3\nllama_model_loader: - kv   1:              phi3.rope.scaling.attn_factor f32              = 1.190238\nllama_model_loader: - kv   2:                               general.type str              = model\nllama_model_loader: - kv   3:                               general.name str              = Phi 4 Mini Instruct\nllama_model_loader: - kv   4:                       general.organization str              = Microsoft\nllama_model_loader: - kv   5:                           general.finetune str              = instruct\nllama_model_loader: - kv   6:                           general.basename str              = Phi-4\nllama_model_loader: - kv   7:                       general.quantized_by str              = Unsloth\nllama_model_loader: - kv   8:                         general.size_label str              = mini\nllama_model_loader: - kv   9:                           general.repo_url str              = https://huggingface.co/unsloth\nllama_model_loader: - kv  10:                        phi3.context_length u32              = 131072\nllama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096\nllama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072\nllama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192\nllama_model_loader: - kv  14:                           phi3.block_count u32              = 32\nllama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 24\nllama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96\nllama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  20:                          general.file_type u32              = 32\nllama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144\nllama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = gpt-4o\nllama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,200064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,200064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,199742]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"e r\", ...\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 199999\nllama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 200020\nllama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 3251\nllama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 200029\nllama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  32:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  33:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\nllama_model_loader: - kv  34:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   67 tensors\nllama_model_loader: - type bf16:  129 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = BF16\nprint_info: file size   = 7.15 GiB (16.00 BPW)\nload_hparams: Phi SWA is currently disabled - results might be suboptimal for some models (see https://github.com/ggml-org/llama.cpp/pull/13676)\nload: special tokens cache size = 14\nload: token to piece cache size = 1.3333 MB\nprint_info: arch             = phi3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 3072\nprint_info: n_layer          = 32\nprint_info: n_head           = 24\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 96\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 3\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 8192\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 4096\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 3B\nprint_info: model params     = 3.84 B\nprint_info: general.name     = Phi 4 Mini Instruct\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 200064\nprint_info: n_merges         = 199742\nprint_info: BOS token        = 199999 '<|endoftext|>'\nprint_info: EOS token        = 200020 '<|end|>'\nprint_info: EOT token        = 199999 '<|endoftext|>'\nprint_info: UNK token        = 3251 '\ufffd'\nprint_info: PAD token        = 200029 '<\uff5cPAD\u2581TOKEN\uff5c>'\nprint_info: LF token         = 198 '\u010a'\nprint_info: EOG token        = 199999 '<|endoftext|>'\nprint_info: EOG token        = 200020 '<|end|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 32 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 33/33 layers to GPU\nload_tensors:        CUDA0 model buffer size =  7317.01 MiB\nload_tensors:   CPU_Mapped model buffer size =  1172.25 MiB\n......................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 131072\nllama_context: n_ctx_per_seq = 131072\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 10000.0\nllama_context: freq_scale    = 1\nllama_context:  CUDA_Host  output buffer size =     0.76 MiB\nllama_kv_cache_unified:        CPU KV buffer size = 16384.00 MiB\nllama_kv_cache_unified: size = 16384.00 MiB (131072 cells,  32 layers,  1 seqs), K (f16): 8192.00 MiB, V (f16): 8192.00 MiB\nggml_backend_cuda_buffer_type_alloc_buffer: allocating 6940.00 MiB on device 0: cudaMalloc failed: out of memory\nggml_gallocr_reserve_n: failed to allocate CUDA0 buffer of size 7277119488\ngraph_reserve: failed to allocate compute buffers\nllama_init_from_model: failed to initialize the context: failed to allocate compute pp buffers\ncommon_init_from_params: failed to create context with model '../models/Phi-4-mini-instruct.BF16.gguf'\nsrv    load_model: failed to load model, '../models/Phi-4-mini-instruct.BF16.gguf'\nsrv    operator(): operator(): cleaning up before exit...\nmain: exiting due to model loading error\n```\n\nI can work around it by lowering the context size down to 32K, but I am not sure why this started being a problem now.\n\n### First Bad Commit\n\n12d0188c0dc6146ffde6d277a93f232ccbe699f8\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-07-17T16:01:03+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14740/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14740"
  },
  {
    "number": 14559,
    "title": "Eval bug: error loading model: vk::PhysicalDevice::createDevice: ErrorExtensionNotPresent",
    "body": "### Name and Version\n\n.\\llama-cli.exe --version\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = AMD Radeon(TM) 890M Graphics (AMD proprietary driver) | uma: 1 | fp16: 1 | warp size: 64 | shared memory: 32768 | int dot: 1 | matrix cores: KHR_coopmat\nversion: 5835 (6491d6e4)\nbuilt with clang version 19.1.5 for x86_64-pc-windows-msvc\n\n### Operating systems\n\nWindows\n\n### GGML backends\n\nVulkan\n\n### Hardware\n\nAMD Ryzen AI 9 HX 370 w/ Radeon 890M\n\n### Models\n\nMeta-Llama-3.1-8B-Instruct-Q4_K_M & Qwen2.5-VL-7B-Instruct-Q4_K_M\n\n### Problem description & steps to reproduce\n\nI'm trying to run llama-server on this AMD laptop but keep running into this error. I've test this code this another AMD laptop (Ryzen 7 8845HS w/ Radeon 780M) and an intel PC (i7-14700 w/ UHD Graphics 770) and both works fine.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n0.00.160.925 I load_tensors: loading model tensors, this can take a while... (mmap = true)\n0.00.165.104 E llama_model_load: error loading model: vk::PhysicalDevice::createDevice: ErrorExtensionNotPresent\n0.00.165.107 E llama_model_load_from_file_impl: failed to load model\n0.00.197.487 E common_init_from_params: failed to load model '.\\Qwen2.5-VL-7B-Instruct-Q4_K_M.gguf'\n0.00.197.493 E srv    load_model: failed to load model, '.\\Qwen2.5-VL-7B-Instruct-Q4_K_M.gguf'\n0.00.197.495 I srv    operator(): operator(): cleaning up before exit...\n0.00.198.980 E main: exiting due to model loading error\n[Vulkan Loader] ERROR:          vkDestroyFence: Invalid device [VUID-vkDestroyFence-device-parameter]\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-07-07T03:54:53+00:00",
    "closed_at": "2025-07-07T09:10:30+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14559/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14559"
  },
  {
    "number": 14610,
    "title": "Misc. bug: CANNOT CONVERT THE MODEL",
    "body": "### Name and Version\n\nHello getting this error:\n\nRuntimeError: Internal: could not parse ModelProto from /Users/yuki/Downloads/yunasmol-pytorch/tokenizer.model\n\nHow to fix?\n\n### Operating systems\n\nMac\n\n### Which llama.cpp modules do you know to be affected?\n\nPython/Bash scripts\n\n### Command line\n\n```shell\n(yuna) yuki@yuki llama.cpp % python convert_hf_to_gguf.py --outfile yunamiru-f16.gguf --outtype q8_0 --no-lazy --model-name \"Yuna Ai V4 Miru\" \"/Users/yuki/Downloads/yunasmol-pytorch\"\nINFO:hf-to-gguf:Loading model: yunasmol-pytorch\nINFO:hf-to-gguf:Model architecture: Gemma3ForConditionalGeneration\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Exporting model...\nINFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\nTraceback (most recent call last):\n  File \"/Users/yuki/Documents/AI/koboldcpp_tools/llama.cpp/convert_hf_to_gguf.py\", line 7233, in <module>\n    main()\n  File \"/Users/yuki/Documents/AI/koboldcpp_tools/llama.cpp/convert_hf_to_gguf.py\", line 7227, in main\n    model_instance.write()\n  File \"/Users/yuki/Documents/AI/koboldcpp_tools/llama.cpp/convert_hf_to_gguf.py\", line 409, in write\n    self.prepare_tensors()\n  File \"/Users/yuki/Documents/AI/koboldcpp_tools/llama.cpp/convert_hf_to_gguf.py\", line 277, in prepare_tensors\n    for new_name, data_torch in (self.modify_tensors(data_torch, name, bid)):\n  File \"/Users/yuki/Documents/AI/koboldcpp_tools/llama.cpp/convert_hf_to_gguf.py\", line 4341, in modify_tensors\n    vocab = self._create_vocab_sentencepiece()\n  File \"/Users/yuki/Documents/AI/koboldcpp_tools/llama.cpp/convert_hf_to_gguf.py\", line 946, in _create_vocab_sentencepiece\n    tokenizer.LoadFromFile(str(tokenizer_path))\n  File \"/opt/anaconda3/envs/yuna/lib/python3.10/site-packages/sentencepiece/__init__.py\", line 316, in LoadFromFile\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)\nRuntimeError: Internal: could not parse ModelProto from /Users/yuki/Downloads/yunasmol-pytorch/tokenizer.model\n(yuna) yuki@yuki llama.cpp %\n```\n\n### Problem description & steps to reproduce\n\nCan't convert to GGUF. Tried both pt and safetensors\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-07-10T04:39:28+00:00",
    "closed_at": "2025-07-11T00:03:59+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14610/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14610"
  },
  {
    "number": 14719,
    "title": "Misc. bug: Llama server use some of the vram on another GPU, even I set -mg 1 and -sm 'none'",
    "body": "### Name and Version\n\nversion: 3259 (e57dc620)\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n\n### Operating systems\n\n_No response_\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Command line\n\n```shell\n./llama-server -m \"gemma-3-27b-it-q4_0.gguf\" --mmproj \"mmproj-model-f16-27B.gguf\" -ngl 200 -np 3 --threads-http 20 -fa -cb -sm 'none' -mg 1 -ub 2048 -b 4096 --threads 8 --port 8092 -c 0\n```\n\n### Problem description & steps to reproduce\n\nEven I have set - sm 'none' -mg 1 still using around 2GB vram of another GPU. I am not sure if this is normal or it is a bug, or is that a way the force to use only one GPU, if that's not the correct way to do it. Thanks\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-07-16T10:00:14+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14719/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14719"
  },
  {
    "number": 14448,
    "title": "Eval bug: gemma-3n crash when using HIP",
    "body": "### Name and Version\n\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Radeon 890M Graphics, gfx1102 (0x1102), VMM: no, Wave Size: 32\nversion: 5775 (bd9c981d7)\nbuilt with clang version 19.0.0git (/srcdest/rocm-llvm c87081df219c42dc27c5b6d86c0525bc7d01f727) for x86_64-pc-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nHIP\n\n### Hardware\n\nAMD Radeon 890M Graphics\n\n### Models\n\nGemma 3n\n\n### Problem description & steps to reproduce\n\nRunning all gemma-3n models works well when using cpu, using HIP result in same crash.\n\nQuick debugging show that the array is all nan:\n\n```\n(gdb) p cur_p->data[0].p\n$8 = nan(0x400000)\n(gdb) p cur_p->data[1].p\n$9 = nan(0x400000)\n(gdb) p cur_p->data[2].p\n$10 = nan(0x400000)\n```\n\n### Relevant log output\n\n```shell\n$ llama-cli -hf unsloth/gemma-3n-E4B-it-GGUF -co -c 0 -fa -ngl 1000\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Radeon 890M Graphics, gfx1102 (0x1102), VMM: no, Wave Size: 32\ncurl_perform_with_retry: HEAD https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-Q4_K_M.gguf (attempt 1 of 1)...\ncommon_download_file_single: using cached file: /home/user/.cache/llama.cpp/unsloth_gemma-3n-E4B-it-GGUF_gemma-3n-E4B-it-Q4_K_M.gguf\nbuild: 5775 (bd9c981d7) with clang version 19.0.0git (/srcdest/rocm-llvm c87081df219c42dc27c5b6d86c0525bc7d01f727) for x86_64-pc-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_load_from_file_impl: using device ROCm0 (AMD Radeon 890M Graphics) - 17672 MiB free\nllama_model_loader: loaded meta data with 45 key-value pairs and 847 tensors from /home/user/.cache/llama.cpp/unsloth_gemma-3n-E4B-it-GGUF_gemma-3n-E4B-it-Q4_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3n\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Gemma-3N-E4B-It\nllama_model_loader: - kv   3:                           general.finetune str              = 3n-E4B-it\nllama_model_loader: - kv   4:                           general.basename str              = Gemma-3N-E4B-It\nllama_model_loader: - kv   5:                       general.quantized_by str              = Unsloth\nllama_model_loader: - kv   6:                         general.size_label str              = 6.9B\nllama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth\nllama_model_loader: - kv   8:                     gemma3n.context_length u32              = 32768\nllama_model_loader: - kv   9:                   gemma3n.embedding_length u32              = 2048\nllama_model_loader: - kv  10:                        gemma3n.block_count u32              = 35\nllama_model_loader: - kv  11:                gemma3n.feed_forward_length arr[i32,35]      = [16384, 16384, 16384, 16384, 16384, 1...\nllama_model_loader: - kv  12:               gemma3n.attention.head_count u32              = 8\nllama_model_loader: - kv  13:   gemma3n.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  14:               gemma3n.attention.key_length u32              = 256\nllama_model_loader: - kv  15:             gemma3n.attention.value_length u32              = 256\nllama_model_loader: - kv  16:                     gemma3n.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  17:           gemma3n.attention.sliding_window u32              = 512\nllama_model_loader: - kv  18:            gemma3n.attention.head_count_kv u32              = 2\nllama_model_loader: - kv  19:                   gemma3n.altup.active_idx u32              = 0\nllama_model_loader: - kv  20:                   gemma3n.altup.num_inputs u32              = 4\nllama_model_loader: - kv  21:   gemma3n.embedding_length_per_layer_input u32              = 256\nllama_model_loader: - kv  22:         gemma3n.attention.shared_kv_layers u32              = 15\nllama_model_loader: - kv  23:          gemma3n.activation_sparsity_scale arr[f32,35]      = [1.644854, 1.644854, 1.644854, 1.6448...\nllama_model_loader: - kv  24:   gemma3n.attention.sliding_window_pattern arr[bool,35]     = [true, true, true, true, false, true,...\nllama_model_loader: - kv  25:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\nllama_model_loader: - kv  26:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  29:                      tokenizer.ggml.scores arr[f32,262144]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 106\nllama_model_loader: - kv  33:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  34:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  35:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  36:               tokenizer.ggml.add_sep_token bool             = false\nllama_model_loader: - kv  37:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  38:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  39:               general.quantization_version u32              = 2\nllama_model_loader: - kv  40:                          general.file_type u32              = 15\nllama_model_loader: - kv  41:                      quantize.imatrix.file str              = gemma-3n-E4B-it-GGUF/imatrix_unsloth.dat\nllama_model_loader: - kv  42:                   quantize.imatrix.dataset str              = unsloth_calibration_gemma-3n-E4B-it.txt\nllama_model_loader: - kv  43:             quantize.imatrix.entries_count u32              = 459\nllama_model_loader: - kv  44:              quantize.imatrix.chunks_count u32              = 1326\nllama_model_loader: - type  f32:  422 tensors\nllama_model_loader: - type  f16:  108 tensors\nllama_model_loader: - type q8_0:    1 tensors\nllama_model_loader: - type q4_K:  282 tensors\nllama_model_loader: - type q6_K:   34 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 4.90 GiB (6.14 BPW)\nload: special tokens cache size = 6414\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3n\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 32768\nprint_info: n_embd           = 2048\nprint_info: n_layer          = 35\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 2\nprint_info: n_rot            = 256\nprint_info: n_swa            = 512\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 512\nprint_info: n_embd_v_gqa     = 512\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 1.0e+00\nprint_info: n_ff             = 16384\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 32768\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = E4B\nprint_info: model params     = 6.87 B\nprint_info: general.name     = Gemma-3N-E4B-It\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262144\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 106 '<end_of_turn>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 35 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 36/36 layers to GPU\nload_tensors:        ROCm0 model buffer size =  5022.53 MiB\nload_tensors:   CPU_Mapped model buffer size =   288.00 MiB\n...............................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 32768\nllama_context: n_ctx_per_seq = 32768\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 1\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context:  ROCm_Host  output buffer size =     1.00 MiB\nllama_kv_cache_unified_iswa: creating non-SWA KV cache, size = 32768 cells\nllama_kv_cache_unified:      ROCm0 KV buffer size =   256.00 MiB\nllama_kv_cache_unified: size =  256.00 MiB ( 32768 cells,   4 layers,  1 seqs), K (f16):  128.00 MiB, V (f16):  128.00 MiB\nllama_kv_cache_unified_iswa: creating     SWA KV cache, size = 1024 cells\nllama_kv_cache_unified:      ROCm0 KV buffer size =    32.00 MiB\nllama_kv_cache_unified: size =   32.00 MiB (  1024 cells,  16 layers,  1 seqs), K (f16):   16.00 MiB, V (f16):   16.00 MiB\nllama_context:      ROCm0 compute buffer size =   520.00 MiB\nllama_context:  ROCm_Host compute buffer size =   102.01 MiB\nllama_context: graph nodes  = 3143\nllama_context: graph splits = 22\ncommon_init_from_params: KV cache shifting is not supported for this context, disabling KV cache shifting\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 32768\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 12\nmain: chat template is available, enabling conversation mode (disable it with -no-cnv)\nmain: chat template example:\n<start_of_turn>user\nYou are a helpful assistant\n\nHello<end_of_turn>\n<start_of_turn>model\nHi there<end_of_turn>\n<start_of_turn>user\nHow are you?<end_of_turn>\n<start_of_turn>model\n\n\nsystem_info: n_threads = 12 (n_threads_batch = 12) / 24 | ROCm : NO_VMM = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : OPENMP = 1 | REPACK = 1 |\n\nmain: interactive mode on.\nsampler seed: 1372900278\nsampler params:\n        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n        dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 32768\n        top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist\ngenerate: n_ctx = 32768, n_batch = 2048, n_predict = -1, n_keep = 1\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return control to the AI.\n - To return control without starting a new line, end your input with '/'.\n - If you want to submit another line, end your input with '\\'.\n - Not using system message. To change it, set a different value via -sys PROMPT\n\n\n> hi\n/usr/lib64/gcc/x86_64-pc-linux-gnu/15.1.1/../../../../include/c++/15.1.1/bits/random.tcc:2668: void std::discrete_distribution<>::param_type::_M_initialize() [_IntType = int]: Assertion '__sum > 0' failed.\nzsh: IOT instruction (core dumped)  llama-cli -hf unsloth/gemma-3n-E4B-it-GGUF -co -c 0 -fa -ngl 1000\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-06-29T15:53:06+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14448/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14448"
  },
  {
    "number": 14506,
    "title": "Eval bug: Assertion `status == LLAMA_MEMORY_STATUS_SUCCESS' failed",
    "body": "### Name and Version\n\n```\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 2 ROCm devices:\n  Device 0: AMD Radeon RX 6800 XT, gfx1030 (0x1030), VMM: no, Wave Size: 32\n  Device 1: AMD Radeon Graphics, gfx1036 (0x1036), VMM: no, Wave Size: 32\nregister_backend: registered backend ROCm (2 devices)\nregister_device: registered device ROCm0 (AMD Radeon RX 6800 XT)\nregister_device: registered device ROCm1 (AMD Radeon Graphics)\nregister_backend: registered backend CPU (1 devices)\nregister_device: registered device CPU (AMD Ryzen 5 7600X 6-Core Processor)\nload_backend: failed to find ggml_backend_init in /home/exposedcat/Pets/AI/llama.cpp/build/bin/libggml-hip.so\nload_backend: failed to find ggml_backend_init in /home/exposedcat/Pets/AI/llama.cpp/build/bin/libggml-cpu.so\nversion: 5774 (27208bf6)\nbuilt with HIP version: 6.3.42133-0 for x86_64-redhat-linux-gnu\n```\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nBLAS\n\n### Hardware\n\nRX 6800 XT\n\n### Models\n\nApplies to any, at the moment of the report it was Gemma3 Q4\n\n### Problem description & steps to reproduce\n\nStarting server with \n```bash\nllama-server --model gemma3.gguf -ngl 49 --no-webui --port 8181 -mg 0 -c 10000 --mlock\n```\nWorks but randomly crashes during inference with error\n```\nllama-server: /src/llama-kv-cache-unified.cpp:1779: virtual bool llama_kv_cache_unified_context::apply(): Assertion `status == LLAMA_MEMORY_STATUS_SUCCESS' failed.\n```\n\n### First Bad Commit\n\nIt was happening on older versions, too\n\n### Relevant log output\n\n```shell\nProcess 340301 (llama-server) of user 1000 dumped core.\n                                                                         \n                                                                         Module [dso] without build-id.\n                                                                         Module [dso] without build-id.\n                                                                         Module [dso] without build-id.\n                                                                         Module [dso] without build-id.\n                                                                         Module [dso] without build-id.\n                                                                         Module [dso] without build-id.\n                                                                         Module [dso] without build-id.\n                                                                         Module [dso] without build-id.\n                                                                         Module [dso] without build-id.\n                                                                         Module [dso] without build-id.\n                                                                         Module [dso] without build-id.\n                                                                         Module [dso] without build-id.\n                                                                         Module [dso] without build-id.\n                                                                         Module [dso] without build-id.\n                                                                         Module [dso] without build-id.\n                                                                         Module [dso] without build-id.\n                                                                         Module libpcre2-8.so.0 from rpm pcre2-10.45-1.fc42.x86_64\n                                                                         Module libselinux.so.1 from rpm libselinux-3.8-2.fc42.x86_64\n                                                                         Module libdrm.so.2 from rpm libdrm-2.4.125-1.fc42.x86_64\n                                                                         Module libelf.so.1 from rpm elfutils-0.193-2.fc42.x86_64\n                                                                         Module libdrm_amdgpu.so.1 from rpm libdrm-2.4.125-1.fc42.x86_64\n                                                                         Module libzstd.so.1 from rpm zstd-1.5.7-1.fc42.x86_64\n                                                                         Module libkeyutils.so.1 from rpm keyutils-1.6.3-5.fc42.x86_64\n                                                                         Module libkrb5support.so.0 from rpm krb5-1.21.3-6.fc42.x86_64\n                                                                         Module libcom_err.so.2 from rpm e2fsprogs-1.47.2-3.fc42.x86_64\n                                                                         Module libk5crypto.so.3 from rpm krb5-1.21.3-6.fc42.x86_64\n                                                                         Module libkrb5.so.3 from rpm krb5-1.21.3-6.fc42.x86_64\n                                                                         Module libunistring.so.5 from rpm libunistring-1.1-9.fc42.x86_64\n                                                                         Module libnuma.so.1 from rpm numactl-2.0.19-2.fc42.x86_64\n                                                                         Module libz.so.1 from rpm zlib-ng-2.2.4-3.fc42.x86_64\n                                                                         Module libgssapi_krb5.so.2 from rpm krb5-1.21.3-6.fc42.x86_64\n                                                                         Module libcrypto.so.3 from rpm openssl-3.2.4-3.fc42.x86_64\n                                                                         Module libssl.so.3 from rpm openssl-3.2.4-3.fc42.x86_64\n                                                                         Module libidn2.so.0 from rpm libidn2-2.3.8-1.fc42.x86_64\n                                                                         Module libnghttp2.so.14 from rpm nghttp2-1.64.0-3.fc42.x86_64\n                                                                         Module libcurl.so.4 from rpm curl-8.11.1-5.fc42.x86_64\n                                                                         Stack trace of thread 340301:\n                                                                         #0  0x00007f8cdb28111c __pthread_kill_implementation (libc.so.6 + 0x7311c)\n                                                                         #1  0x00007f8cdb227afe raise (libc.so.6 + 0x19afe)\n                                                                         #2  0x00007f8cdb20f6d0 abort (libc.so.6 + 0x16d0)\n                                                                         #3  0x00007f8cdb20f639 __assert_fail_base.cold (libc.so.6 + 0x1639)\n                                                                         #4  0x00007f8cdf6dbfd0 _ZN30llama_kv_cache_unified_context5applyEv (libllama.so + 0x106fd0)\n                                                                         #5  0x00007f8cdf6df6a6 _ZN35llama_kv_cache_unified_iswa_context5applyEv (libllama.so + 0x10a6a6)\n                                                                         #6  0x00007f8cdf6a6241 _ZN13llama_context14kv_self_updateEb (libllama.so + 0xd1241)\n                                                                         #7  0x00007f8cdf6a7f74 _ZN13llama_context6decodeERK11llama_batch (libllama.so + 0xd2f74)\n                                                                         #8  0x00007f8cdf6ac4cb llama_decode (libllama.so + 0xd74cb)\n                                                                         #9  0x0000562c7e7cad24 _ZN14server_context12update_slotsEv (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama-server + 0x148d24)\n                                                                         #10 0x0000562c7e752806 _ZN12server_queue10start_loopEv (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama-server + 0xd0806)\n                                                                         #11 0x0000562c7e7140e7 main (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama-server + 0x920e7)\n                                                                         #12 0x00007f8cdb2115f5 __libc_start_call_main (libc.so.6 + 0x35f5)\n                                                                         #13 0x00007f8cdb2116a8 __libc_start_main@@GLIBC_2.34 (libc.so.6 + 0x36a8)\n                                                                         #14 0x0000562c7e710055 _start (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama-server + 0x8e055)\n                                                                         \n                                                                         Stack trace of thread 340499:\n                                                                         #0  0x00007f8cdb2876c2 __syscall_cancel_arch (libc.so.6 + 0x796c2)\n                                                                         #1  0x00007f8cdb27b9da __internal_syscall_cancel (libc.so.6 + 0x6d9da)\n                                                                         #2  0x00007f8cdb27c04c __futex_abstimed_wait_common (libc.so.6 + 0x6e04c)\n                                                                         #3  0x00007f8cdb27e71e pthread_cond_wait@@GLIBC_2.3.2 (libc.so.6 + 0x7071e)\n                                                                         #4  0x00007f8cdb444b20 _ZNSt18condition_variable4waitERSt11unique_lockISt5mutexE (libstdc++.so.6 + 0x44b20)\n                                                                         #5  0x0000562c7e8eb3db _ZZN10common_log6resumeEvENKUlvE_clEv (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama-server + 0x2693db)\n                                                                         #6  0x00007f8cdb44e164 execute_native_thread_routine (libstdc++.so.6 + 0x4e164)\n                                                                         #7  0x00007f8cdb27f1d4 start_thread (libc.so.6 + 0x711d4)\n                                                                         #8  0x00007f8cdb301cec __clone3 (libc.so.6 + 0xf3cec)\n                                                                         \n                                                                         Stack trace of thread 340302:\n                                                                         #0  0x00007f8cdb2fdaad ioctl (libc.so.6 + 0xefaad)\n                                                                         #1  0x00007f8ccb6f6280 hsakmt_ioctl (libhsa-runtime64.so.1 + 0xf6280)\n                                                                         #2  0x00007f8ccb6f6ab9 hsaKmtWaitOnMultipleEvents_Ext.part.0 (libhsa-runtime64.so.1 + 0xf6ab9)\n                                                                         #3  0x00007f8ccb679de0 _ZN4rocr4core7Runtime15AsyncEventsLoopEPv (libhsa-runtime64.so.1 + 0x79de0)\n                                                                         #4  0x00007f8ccb61bb8c _ZN4rocr2os16ThreadTrampolineEPv (libhsa-runtime64.so.1 + 0x1bb8c)\n                                                                         #5  0x00007f8cdb27f1d4 start_thread (libc.so.6 + 0x711d4)\n                                                                         #6  0x00007f8cdb301cec __clone3 (libc.so.6 + 0xf3cec)\n                                                                         \n                                                                         Stack trace of thread 340501:\n                                                                         #0  0x00007f8cdb2876c2 __syscall_cancel_arch (libc.so.6 + 0x796c2)\n                                                                         #1  0x00007f8cdb27b9da __internal_syscall_cancel (libc.so.6 + 0x6d9da)\n                                                                         #2  0x00007f8cdb27c04c __futex_abstimed_wait_common (libc.so.6 + 0x6e04c)\n                                                                         #3  0x00007f8cdb27e71e pthread_cond_wait@@GLIBC_2.3.2 (libc.so.6 + 0x7071e)\n                                                                         #4  0x00007f8cdb444b20 _ZNSt18condition_variable4waitERSt11unique_lockISt5mutexE (libstdc++.so.6 + 0x44b20)\n                                                                         #5  0x0000562c7e757e51 _ZN7httplib10ThreadPool6workerclEv (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama-server + 0xd5e51)\n                                                                         #6  0x00007f8cdb44e164 execute_native_thread_routine (libstdc++.so.6 + 0x4e164)\n                                                                         #7  0x00007f8cdb27f1d4 start_thread (libc.so.6 + 0x711d4)\n                                                                         #8  0x00007f8cdb301cec __clone3 (libc.so.6 + 0xf3cec)\n                                                                         \n                                                                         Stack trace of thread 340502:\n                                                                         #0  0x00007f8cdb2876c2 __syscall_cancel_arch (libc.so.6 + 0x796c2)\n                                                                         #1  0x00007f8cdb27b9da __internal_syscall_cancel (libc.so.6 + 0x6d9da)\n                                                                         #2  0x00007f8cdb27c04c __futex_abstimed_wait_common (libc.so.6 + 0x6e04c)\n                                                                         #3  0x00007f8cdb27e71e pthread_cond_wait@@GLIBC_2.3.2 (libc.so.6 + 0x7071e)\n                                                                         #4  0x00007f8cdb444b20 _ZNSt18condition_variable4waitERSt11unique_lockISt5mutexE (libstdc++.so.6 + 0x44b20)\n                                                                         #5  0x0000562c7e757e51 _ZN7httplib10ThreadPool6workerclEv (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama-server + 0xd5e51)\n                                                                         #6  0x00007f8cdb44e164 execute_native_thread_routine (libstdc++.so.6 + 0x4e164)\n                                                                         #7  0x00007f8cdb27f1d4 start_thread (libc.so.6 + 0x711d4)\n                                                                         #8  0x00007f8cdb301cec __clone3 (libc.so.6 + 0xf3cec)\n                                                                         \n                                                                         Stack trace of thread 340504:\n                                                                         #0  0x00007f8cdb2876c2 __syscall_cancel_arch (libc.so.6 + 0x796c2)\n                                                                         #1  0x00007f8cdb27b9da __internal_syscall_cancel (libc.so.6 + 0x6d9da)\n                                                                         #2  0x00007f8cdb27c04c __futex_abstimed_wait_common (libc.so.6 + 0x6e04c)\n                                                                         #3  0x00007f8cdb27e71e pthread_cond_wait@@GLIBC_2.3.2 (libc.so.6 + 0x7071e)\n                                                                         #4  0x00007f8cdb444b20 _ZNSt18condition_variable4waitERSt11unique_lockISt5mutexE (libstdc++.so.6 + 0x44b20)\n                                                                         #5  0x0000562c7e757e51 _ZN7httplib10ThreadPool6workerclEv (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama-server + 0xd5e51)\n                                                                         #6  0x00007f8cdb44e164 execute_native_thread_routine (libstdc++.so.6 + 0x4e164)\n                                                                         #7  0x00007f8cdb27f1d4 start_thread (libc.so.6 + 0x711d4)\n                                                                         #8  0x00007f8cdb301cec __clone3 (libc.so.6 + 0xf3cec)\n                                                                         \n                                                                         Stack trace of thread 340503:\n                                                                         #0  0x00007f8cdb2876c2 __syscall_cancel_arch (libc.so.6 + 0x796c2)\n                                                                         #1  0x00007f8cdb27b9da __internal_syscall_cancel (libc.so.6 + 0x6d9da)\n                                                                         #2  0x00007f8cdb27c04c __futex_abstimed_wait_common (libc.so.6 + 0x6e04c)\n                                                                         #3  0x00007f8cdb27e71e pthread_cond_wait@@GLIBC_2.3.2 (libc.so.6 + 0x7071e)\n                                                                         #4  0x00007f8cdb444b20 _ZNSt18condition_variable4waitERSt11unique_lockISt5mutexE (libstdc++.so.6 + 0x44b20)\n                                                                         #5  0x0000562c7e757e51 _ZN7httplib10ThreadPool6workerclEv (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama-server + 0xd5e51)\n                                                                         #6  0x00007f8cdb44e164 execute_native_thread_routine (libstdc++.so.6 + 0x4e164)\n                                                                         #7  0x00007f8cdb27f1d4 start_thread (libc.so.6 + 0x711d4)\n                                                                         #8  0x00007f8cdb301cec __clone3 (libc.so.6 + 0xf3cec)\n                                                                         \n                                                                         Stack trace of thread 340505:\n                                                                         #0  0x00007f8cdb2876c2 __syscall_cancel_arch (libc.so.6 + 0x796c2)\n                                                                         #1  0x00007f8cdb27b9da __internal_syscall_cancel (libc.so.6 + 0x6d9da)\n                                                                         #2  0x00007f8cdb27c04c __futex_abstimed_wait_common (libc.so.6 + 0x6e04c)\n                                                                         #3  0x00007f8cdb27eb52 pthread_cond_clockwait@GLIBC_2.30 (libc.so.6 + 0x70b52)\n                                                                         #4  0x0000562c7e7bd066 _ZN15server_response17recv_with_timeoutERKSt13unordered_setIiSt4hashIiESt8equal_toIiESaIiEEi (/home/exposedcat/Pets/AI/llama.cpp/build>\n                                                                         #5  0x0000562c7e7bd7bb _ZN14server_context27receive_cmpl_results_streamERKSt13unordered_setIiSt4hashIiESt8equal_toIiESaIiEERKSt8functionIFbRSt10unique_ptrI18>\n                                                                         #6  0x0000562c7e732d3a _ZNSt17_Function_handlerIFbmRN7httplib8DataSinkEEZZ4mainENKUl16server_task_typeRN8nlohmann16json_abi_v3_12_010basic_jsonINS6_11ordered>\n                                                                         #7  0x0000562c7e7bd65b _ZNSt17_Function_handlerIFbmmRN7httplib8DataSinkEENS0_6detail22ContentProviderAdapterEE9_M_invokeERKSt9_Any_dataOmSA_S2_ (/home/expose>\n                                                                         #8  0x0000562c7e790aed _ZN7httplib6detail21write_content_chunkedIZNS_6Server27write_content_with_providerERNS_6StreamERKNS_7RequestERNS_8ResponseERKNSt7__cxx>\n                                                                         #9  0x0000562c7e78d7e3 _ZN7httplib6Server27write_content_with_providerERNS_6StreamERKNS_7RequestERNS_8ResponseERKNSt7__cxx1112basic_stringIcSt11char_traitsIc>\n                                                                         #10 0x0000562c7e78bc76 _ZN7httplib6Server19write_response_coreERNS_6StreamEbRKNS_7RequestERNS_8ResponseEb (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama>\n                                                                         #11 0x0000562c7e783f3b _ZN7httplib6Server15process_requestERNS_6StreamERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEiSA_ibRbRKSt8functionIFvRNS_7Req>\n                                                                         #12 0x0000562c7e782766 _ZZN7httplib6detail21process_server_socketIZNS_6Server24process_and_close_socketEiEUlRNS_6StreamEbRbE_EEbRKSt6atomicIiEimlllllT_ENKUlb>\n                                                                         #13 0x0000562c7e754171 _ZN7httplib6Server24process_and_close_socketEi (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama-server + 0xd2171)\n                                                                         #14 0x0000562c7e757f52 _ZN7httplib10ThreadPool6workerclEv (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama-server + 0xd5f52)\n                                                                         #15 0x00007f8cdb44e164 execute_native_thread_routine (libstdc++.so.6 + 0x4e164)\n                                                                         #16 0x00007f8cdb27f1d4 start_thread (libc.so.6 + 0x711d4)\n                                                                         #17 0x00007f8cdb301cec __clone3 (libc.so.6 + 0xf3cec)\n                                                                         \n                                                                         Stack trace of thread 340506:\n                                                                         #0  0x00007f8cdb2876c2 __syscall_cancel_arch (libc.so.6 + 0x796c2)\n                                                                         #1  0x00007f8cdb27b9da __internal_syscall_cancel (libc.so.6 + 0x6d9da)\n                                                                         #2  0x00007f8cdb27c04c __futex_abstimed_wait_common (libc.so.6 + 0x6e04c)\n                                                                         #3  0x00007f8cdb27e71e pthread_cond_wait@@GLIBC_2.3.2 (libc.so.6 + 0x7071e)\n                                                                         #4  0x00007f8cdb444b20 _ZNSt18condition_variable4waitERSt11unique_lockISt5mutexE (libstdc++.so.6 + 0x44b20)\n                                                                         #5  0x0000562c7e757e51 _ZN7httplib10ThreadPool6workerclEv (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama-server + 0xd5e51)\n                                                                         #6  0x00007f8cdb44e164 execute_native_thread_routine (libstdc++.so.6 + 0x4e164)\n                                                                         #7  0x00007f8cdb27f1d4 start_thread (libc.so.6 + 0x711d4)\n                                                                         #8  0x00007f8cdb301cec __clone3 (libc.so.6 + 0xf3cec)\n                                                                         \n                                                                         Stack trace of thread 340500:\n                                                                         #0  0x00007f8cdb2876c2 __syscall_cancel_arch (libc.so.6 + 0x796c2)\n                                                                         #1  0x00007f8cdb27b9da __internal_syscall_cancel (libc.so.6 + 0x6d9da)\n                                                                         #2  0x00007f8cdb27ba24 __syscall_cancel (libc.so.6 + 0x6da24)\n                                                                         #3  0x00007f8cdb303582 accept4 (libc.so.6 + 0xf5582)\n                                                                         #4  0x0000562c7e7c1c92 _ZN7httplib6Server15listen_internalEv (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama-server + 0x13fc92)\n                                                                         #5  0x00007f8cdb44e164 execute_native_thread_routine (libstdc++.so.6 + 0x4e164)\n                                                                         #6  0x00007f8cdb27f1d4 start_thread (libc.so.6 + 0x711d4)\n                                                                         #7  0x00007f8cdb301cec __clone3 (libc.so.6 + 0xf3cec)\n                                                                         \n                                                                         Stack trace of thread 340507:\n                                                                         #0  0x00007f8cdb2876c2 __syscall_cancel_arch (libc.so.6 + 0x796c2)\n                                                                         #1  0x00007f8cdb27b9da __internal_syscall_cancel (libc.so.6 + 0x6d9da)\n                                                                         #2  0x00007f8cdb27c04c __futex_abstimed_wait_common (libc.so.6 + 0x6e04c)\n                                                                         #3  0x00007f8cdb27e71e pthread_cond_wait@@GLIBC_2.3.2 (libc.so.6 + 0x7071e)\n                                                                         #4  0x00007f8cdb444b20 _ZNSt18condition_variable4waitERSt11unique_lockISt5mutexE (libstdc++.so.6 + 0x44b20)\n                                                                         #5  0x0000562c7e757e51 _ZN7httplib10ThreadPool6workerclEv (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama-server + 0xd5e51)\n                                                                         #6  0x00007f8cdb44e164 execute_native_thread_routine (libstdc++.so.6 + 0x4e164)\n                                                                         #7  0x00007f8cdb27f1d4 start_thread (libc.so.6 + 0x711d4)\n                                                                         #8  0x00007f8cdb301cec __clone3 (libc.so.6 + 0xf3cec)\n                                                                         \n                                                                         Stack trace of thread 340514:\n                                                                         #0  0x00007f8cdb2fdaad ioctl (libc.so.6 + 0xefaad)\n                                                                         #1  0x00007f8ccb6f6280 hsakmt_ioctl (libhsa-runtime64.so.1 + 0xf6280)\n                                                                         #2  0x00007f8ccb6f6ab9 hsaKmtWaitOnMultipleEvents_Ext.part.0 (libhsa-runtime64.so.1 + 0xf6ab9)\n                                                                         #3  0x00007f8ccb680c10 _ZN4rocr4core6Signal7WaitAnyEjPK12hsa_signal_sPK22hsa_signal_condition_tPKlm16hsa_wait_state_tPl (libhsa-runtime64.so.1 + 0x80c10)\n                                                                         #4  0x00007f8ccb6794e6 _ZN4rocr4core7Runtime15AsyncEventsLoopEPv (libhsa-runtime64.so.1 + 0x794e6)\n                                                                         #5  0x00007f8ccb61bb8c _ZN4rocr2os16ThreadTrampolineEPv (libhsa-runtime64.so.1 + 0x1bb8c)\n                                                                         #6  0x00007f8cdb27f1d4 start_thread (libc.so.6 + 0x711d4)\n                                                                         #7  0x00007f8cdb301cec __clone3 (libc.so.6 + 0xf3cec)\n                                                                         \n                                                                         Stack trace of thread 340508:\n                                                                         #0  0x00007f8cdb2876c2 __syscall_cancel_arch (libc.so.6 + 0x796c2)\n                                                                         #1  0x00007f8cdb27b9da __internal_syscall_cancel (libc.so.6 + 0x6d9da)\n                                                                         #2  0x00007f8cdb27c04c __futex_abstimed_wait_common (libc.so.6 + 0x6e04c)\n                                                                         #3  0x00007f8cdb27e71e pthread_cond_wait@@GLIBC_2.3.2 (libc.so.6 + 0x7071e)\n                                                                         #4  0x00007f8cdb444b20 _ZNSt18condition_variable4waitERSt11unique_lockISt5mutexE (libstdc++.so.6 + 0x44b20)\n                                                                         #5  0x0000562c7e757e51 _ZN7httplib10ThreadPool6workerclEv (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama-server + 0xd5e51)\n                                                                         #6  0x00007f8cdb44e164 execute_native_thread_routine (libstdc++.so.6 + 0x4e164)\n                                                                         #7  0x00007f8cdb27f1d4 start_thread (libc.so.6 + 0x711d4)\n                                                                         #8  0x00007f8cdb301cec __clone3 (libc.so.6 + 0xf3cec)\n                                                                         \n                                                                         Stack trace of thread 340509:\n                                                                         #0  0x00007f8cdb2876c2 __syscall_cancel_arch (libc.so.6 + 0x796c2)\n                                                                         #1  0x00007f8cdb27b9da __internal_syscall_cancel (libc.so.6 + 0x6d9da)\n                                                                         #2  0x00007f8cdb27c04c __futex_abstimed_wait_common (libc.so.6 + 0x6e04c)\n                                                                         #3  0x00007f8cdb27e71e pthread_cond_wait@@GLIBC_2.3.2 (libc.so.6 + 0x7071e)\n                                                                         #4  0x00007f8cdb444b20 _ZNSt18condition_variable4waitERSt11unique_lockISt5mutexE (libstdc++.so.6 + 0x44b20)\n                                                                         #5  0x0000562c7e757e51 _ZN7httplib10ThreadPool6workerclEv (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama-server + 0xd5e51)\n                                                                         #6  0x00007f8cdb44e164 execute_native_thread_routine (libstdc++.so.6 + 0x4e164)\n                                                                         #7  0x00007f8cdb27f1d4 start_thread (libc.so.6 + 0x711d4)\n                                                                         #8  0x00007f8cdb301cec __clone3 (libc.so.6 + 0xf3cec)\n                                                                         \n                                                                         Stack trace of thread 340510:\n                                                                         #0  0x00007f8cdb2876c2 __syscall_cancel_arch (libc.so.6 + 0x796c2)\n                                                                         #1  0x00007f8cdb27b9da __internal_syscall_cancel (libc.so.6 + 0x6d9da)\n                                                                         #2  0x00007f8cdb27c04c __futex_abstimed_wait_common (libc.so.6 + 0x6e04c)\n                                                                         #3  0x00007f8cdb27e71e pthread_cond_wait@@GLIBC_2.3.2 (libc.so.6 + 0x7071e)\n                                                                         #4  0x00007f8cdb444b20 _ZNSt18condition_variable4waitERSt11unique_lockISt5mutexE (libstdc++.so.6 + 0x44b20)\n                                                                         #5  0x0000562c7e757e51 _ZN7httplib10ThreadPool6workerclEv (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama-server + 0xd5e51)\n                                                                         #6  0x00007f8cdb44e164 execute_native_thread_routine (libstdc++.so.6 + 0x4e164)\n                                                                         #7  0x00007f8cdb27f1d4 start_thread (libc.so.6 + 0x711d4)\n                                                                         #8  0x00007f8cdb301cec __clone3 (libc.so.6 + 0xf3cec)\n                                                                         \n                                                                         Stack trace of thread 340511:\n                                                                         #0  0x00007f8cdb2876c2 __syscall_cancel_arch (libc.so.6 + 0x796c2)\n                                                                         #1  0x00007f8cdb27b9da __internal_syscall_cancel (libc.so.6 + 0x6d9da)\n                                                                         #2  0x00007f8cdb27c04c __futex_abstimed_wait_common (libc.so.6 + 0x6e04c)\n                                                                         #3  0x00007f8cdb27e71e pthread_cond_wait@@GLIBC_2.3.2 (libc.so.6 + 0x7071e)\n                                                                         #4  0x00007f8cdb444b20 _ZNSt18condition_variable4waitERSt11unique_lockISt5mutexE (libstdc++.so.6 + 0x44b20)\n                                                                         #5  0x0000562c7e757e51 _ZN7httplib10ThreadPool6workerclEv (/home/exposedcat/Pets/AI/llama.cpp/build/bin/llama-server + 0xd5e51)\n                                                                         #6  0x00007f8cdb44e164 execute_native_thread_routine (libstdc++.so.6 + 0x4e164)\n                                                                         #7  0x00007f8cdb27f1d4 start_thread (libc.so.6 + 0x711d4)\n                                                                         #8  0x00007f8cdb301cec __clone3 (libc.so.6 + 0xf3cec)\n                                                                         ELF object binary architecture: AMD x86-64\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-07-02T16:22:59+00:00",
    "closed_at": "2025-07-03T08:21:12+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14506/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14506"
  },
  {
    "number": 14430,
    "title": "Is PLE offloading to GPU supported?",
    "body": "### Name and Version\n\nbuild: 5768 (ceb1bf5a) with gcc-15 (Homebrew GCC 15.1.0) 15.1.0 for x86_64-pc-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\nNvidia T4\n\n### Models\n\nhttps://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/resolve/main/gemma-3n-E4B-it-UD-Q4_K_XL.gguf\n\n### Problem description & steps to reproduce\n\nI have seen that the PLE of Gemma3N is loaded to CPU, and I wonder if it can be loaded to the GPU (with ` -ot per_layer_token_embd.weight=CUDA0`). When I tried to force it on the GPU, llama.cpp just silently crashed.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nllama_model_loader: - kv  29:                      tokenizer.ggml.scores arr[f32,262144]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 106\nllama_model_loader: - kv  33:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  34:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  35:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  36:               tokenizer.ggml.add_sep_token bool             = false\nllama_model_loader: - kv  37:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  38:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  39:               general.quantization_version u32              = 2\nllama_model_loader: - kv  40:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  422 tensors\nllama_model_loader: - type  f16:  108 tensors\nllama_model_loader: - type q4_K:  281 tensors\nllama_model_loader: - type q6_K:   36 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 4.50 GiB (5.63 BPW) \nload: special tokens cache size = 6414\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3n\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 32768\nprint_info: n_embd           = 2048\nprint_info: n_layer          = 35\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 2\nprint_info: n_rot            = 256\nprint_info: n_swa            = 512\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 512\nprint_info: n_embd_v_gqa     = 512\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 1.0e+00\nprint_info: n_ff             = 16384\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 32768\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = E4B\nprint_info: model params     = 6.87 B\nprint_info: general.name     = Gemma-3N-E4B-It\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262144\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 106 '<end_of_turn>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 35 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 36/36 layers to GPU\nload_tensors:        CUDA0 model buffer size =  4612.03 MiB\nload_tensors:   CPU_Mapped model buffer size =   420.00 MiB\n..................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 8192\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 1\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\nllama_context:  CUDA_Host  output buffer size =     1.00 MiB\nllama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\nllama_kv_cache_unified_iswa: creating non-SWA KV cache, size = 8192 cells\nllama_kv_cache_unified:      CUDA0 KV buffer size =    64.00 MiB\nllama_kv_cache_unified: size =   64.00 MiB (  8192 cells,   4 layers,  1 seqs), K (f16):   32.00 MiB, V (f16):   32.00 MiB\nllama_kv_cache_unified_iswa: creating     SWA KV cache, size = 8192 cells\nllama_kv_cache_unified:      CUDA0 KV buffer size =   256.00 MiB\nllama_kv_cache_unified: size =  256.00 MiB (  8192 cells,  16 layers,  1 seqs), K (f16):  128.00 MiB, V (f16):  128.00 MiB\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-06-28T04:31:55+00:00",
    "closed_at": "2025-06-29T05:09:38+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14430/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14430"
  },
  {
    "number": 14625,
    "title": "Eval bug: server: unnecessary prompt re-processing with Jamba models",
    "body": "### Name and Version\n\nI am using commit `0b885577` (earlier today at the time of posting). \n\n```bash\nllama-server --version\n```\n\n```plaintext\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\nversion: 5866 (0b885577)\nbuilt with cc (Debian 12.2.0-14+deb12u1) 12.2.0 for x86_64-linux-gnu\n```\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\nOS: Debian GNU/Linux 12 (bookworm) x86_64 \nHost: B650M C V2-Y1 -CF \nKernel: 6.1.0-35-amd64 \nCPU: AMD Ryzen 7 7700X (16) @ 4.500GHz \nGPU: NVIDIA GeForce RTX 4060 Ti 16GB \nGPU: AMD ATI 13:00.0 Raphael \nMemory: 934MiB / 63435MiB\n\n### Models\n\n[AI21-Jamba-Mini-1.7-Q8_0.gguf](https://huggingface.co/ddh0/AI21-Jamba-Mini-1.7-GGUF/blob/main/AI21-Jamba-Mini-1.7-Q8_0.gguf)\n\n### Problem description & steps to reproduce\n\nAfter clicking the \"re-generate\" button in the llama-server webui I see this in the console:\n\n```\nslot update_slots: id  0 | task 3977 | new prompt, n_ctx_slot = 262144, n_keep = 0, n_prompt_tokens = 100496\nslot update_slots: id  0 | task 3977 | n_past = 100496, cache_tokens.size() = 103296, seq_id = 0, pos_min = 103295, n_swa = 0\nslot update_slots: id  0 | task 3977 | forcing full prompt re-processing due to lack of cache data (likely due to SWA, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\nslot update_slots: id  0 | task 3977 | kv cache rm [0, end)\n```\n\nThe KV cache is cleared and the model re-processes the entire context (in this case, 100k tokens!) even though it shouldn't have to because it only needs to restart from where the last generation started.\n\nI think this is a bug because I don't think it really needs to re-process the context at this point.\n\nI suspect that the check on [this line](https://github.com/ggml-org/llama.cpp/blob/0aedae00e6fb48680324a5ac5da9cba0e35de6b5/tools/server/server.cpp#L3257C33-L3257C82) is incorrect for Jamba models (which do not use SWA).\n\n### First Bad Commit\n\nI assume the problem has been present since Jamba support was added recently in `4a5686da` (ref #7531)\n\n### Relevant log output\n\n```bash\nllama-server --host 192.168.68.66 --port 20480 -m /opt/workspace/gguf/AI21-Jamba-Mini-1.7-Q8_0.gguf -b 4096 -ub 1024 -c 262144 -fa -ngl 999 -ot \"blk\\.(31|30)\\.=CUDA0\" -ot \"exps=CPU\" --jinja\n```\n\n```plaintext\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\nbuild: 5866 (0b885577) with cc (Debian 12.2.0-14+deb12u1) 12.2.0 for x86_64-linux-gnu\nsystem info: n_threads = 8, n_threads_batch = 8, total_threads = 16\n\nsystem_info: n_threads = 8 (n_threads_batch = 8) / 16 | CUDA : ARCHS = 890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n\nmain: binding port with default address family\nmain: HTTP server is listening, hostname: 192.168.68.66, port: 20480, http threads: 15\nmain: loading model\nsrv    load_model: loading model '/opt/workspace/gguf/AI21-Jamba-Mini-1.7-Q8_0.gguf'\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4060 Ti) - 15956 MiB free\nllama_model_loader: loaded meta data with 37 key-value pairs and 531 tensors from /opt/workspace/gguf/AI21-Jamba-Mini-1.7-Q8_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = jamba\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = AI21-Jamba-Mini-1.7\nllama_model_loader: - kv   3:                            general.version str              = 1.7\nllama_model_loader: - kv   4:                           general.basename str              = AI21-Jamba\nllama_model_loader: - kv   5:                         general.size_label str              = Mini\nllama_model_loader: - kv   6:                            general.license str              = other\nllama_model_loader: - kv   7:                       general.license.name str              = jamba-open-model-license\nllama_model_loader: - kv   8:                       general.license.link str              = https://www.ai21.com/jamba-open-model...\nllama_model_loader: - kv   9:                          jamba.block_count u32              = 32\nllama_model_loader: - kv  10:                       jamba.context_length u32              = 262144\nllama_model_loader: - kv  11:                     jamba.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  jamba.feed_forward_length u32              = 14336\nllama_model_loader: - kv  13:                 jamba.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              jamba.attention.head_count_kv arr[i32,32]      = [0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, ...\nllama_model_loader: - kv  15:                      jamba.ssm.conv_kernel u32              = 4\nllama_model_loader: - kv  16:                       jamba.ssm.inner_size u32              = 8192\nllama_model_loader: - kv  17:                       jamba.ssm.state_size u32              = 16\nllama_model_loader: - kv  18:                   jamba.ssm.time_step_rank u32              = 256\nllama_model_loader: - kv  19:     jamba.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  20:                         jamba.expert_count u32              = 16\nllama_model_loader: - kv  21:                    jamba.expert_used_count u32              = 2\nllama_model_loader: - kv  22:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,65536]   = [\"<|pad|>\", \"<|startoftext|>\", \"<|end...\nllama_model_loader: - kv  25:                      tokenizer.ggml.scores arr[f32,65536]   = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,65536]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  32:               tokenizer.ggml.add_sep_token bool             = false\nllama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  34:                    tokenizer.chat_template str              = {# Variables #}\\n{% set ns = namespace...\nllama_model_loader: - kv  35:               general.quantization_version u32              = 2\nllama_model_loader: - kv  36:                          general.file_type u32              = 7\nllama_model_loader: - type  f32:  305 tensors\nllama_model_loader: - type q8_0:  226 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q8_0\nprint_info: file size   = 51.05 GiB (8.50 BPW) \nload: special tokens cache size = 1543\nload: token to piece cache size = 0.4034 MB\nprint_info: arch             = jamba\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 4096\nprint_info: n_layer          = 32\nprint_info: n_head           = 32\nprint_info: n_head_kv        = [0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0]\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = [0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0]\nprint_info: n_embd_k_gqa     = [0, 0, 0, 0, 1024, 0, 0, 0, 0, 0, 0, 0, 1024, 0, 0, 0, 0, 0, 0, 0, 1024, 0, 0, 0, 0, 0, 0, 0, 1024, 0, 0, 0]\nprint_info: n_embd_v_gqa     = [0, 0, 0, 0, 1024, 0, 0, 0, 0, 0, 0, 0, 1024, 0, 0, 0, 0, 0, 0, 0, 1024, 0, 0, 0, 0, 0, 0, 0, 1024, 0, 0, 0]\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 14336\nprint_info: n_expert         = 16\nprint_info: n_expert_used    = 2\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = -1\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 4\nprint_info: ssm_d_inner      = 8192\nprint_info: ssm_d_state      = 16\nprint_info: ssm_dt_rank      = 256\nprint_info: ssm_n_group      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = ?B\nprint_info: model params     = 51.57 B\nprint_info: general.name     = AI21-Jamba-Mini-1.7\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 65536\nprint_info: n_merges         = 0\nprint_info: BOS token        = 1 '<|startoftext|>'\nprint_info: EOS token        = 2 '<|endoftext|>'\nprint_info: EOT token        = 2 '<|endoftext|>'\nprint_info: UNK token        = 3 '<|unk|>'\nprint_info: PAD token        = 0 '<|pad|>'\nprint_info: LF token         = 1554 '<0x0A>'\nprint_info: EOG token        = 2 '<|endoftext|>'\nprint_info: max token length = 96\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 32 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 33/33 layers to GPU\nload_tensors:        CUDA0 model buffer size =  9161.62 MiB\nload_tensors:   CPU_Mapped model buffer size = 48645.07 MiB\n.............................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 262144\nllama_context: n_ctx_per_seq = 262144\nllama_context: n_batch       = 4096\nllama_context: n_ubatch      = 1024\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 1\nllama_context: freq_base     = 10000.0\nllama_context: freq_scale    = 1\nllama_context:  CUDA_Host  output buffer size =     0.25 MiB\nllama_kv_cache_unified:      CUDA0 KV buffer size =  4096.00 MiB\nllama_kv_cache_unified: size = 4096.00 MiB (262144 cells,   4 layers,  1 seqs), K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\nllama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\nllama_memory_recurrent:      CUDA0 RS buffer size =    16.62 MiB\nllama_memory_recurrent: size =   16.62 MiB (     1 cells,  32 layers,  1 seqs), R (f32):    2.62 MiB, S (f32):   14.00 MiB\nllama_context:      CUDA0 compute buffer size =  2328.09 MiB\nllama_context:  CUDA_Host compute buffer size =  1040.01 MiB\nllama_context: graph nodes  = 2123\nllama_context: graph splits = 48 (with bs=1024), 33 (with bs=1)\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 262144\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nsrv          init: initializing slots, n_slots = 1\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 262144\nmain: model loaded\nmain: chat template, chat_template: {# Variables #}\n{% set ns = namespace(message_count=0, is_last_checked_defined=False) %}\n{##}\n{% set bom_str = bom_str or \"<|bom|>\" %}\n{% set eom_str = eom_str or \"<|eom|>\" %}\n{% set default_system_message = default_system_message or \"\" %}\n{##}\n{% set documents_prefix = \"<documents>\" %}\n{% set documents_suffix = \"</documents>\" %}\n{% set tool_definitions_prefix = \"<tool_definitions>\" %}\n{% set tool_definitions_suffix = \"</tool_definitions>\" %}\n{% set active_modes_prefix = \"<active_output_modes>\" %}\n{% set active_modes_suffix = \"</active_output_modes>\" %}\n{##}\n{% set tool_calls_prefix = \"<tool_calls>\" %}\n{% set tool_calls_suffix = \"</tool_calls>\" %}\n{% set citations_prefix = \"<citations>\" %}\n{% set citations_suffix = \"</citations>\" %}\n{##}\n{% if add_generation_prompt is not defined %}\n  {% set add_generation_prompt = True %}\n{% endif %}\n{% set role_to_predict = role_to_predict or \"assistant\" %}\n{% if messages|length > 0 and messages[0].role == \"system\" %}\n  {% set system_message = messages[0].content %}\n  {% set loop_messages = messages[1:] %}\n{% else %}\n  {% set system_message = default_system_message %}\n  {% set loop_messages = messages %}\n{% endif %}\n{##}\n{##}\n{# Macros #}\n{% macro handle_tool_definitions(tools) %}\n  {{- tool_definitions_prefix -}}\n  {{- \"\\n# Tools\" -}}\n  {{- \"\\n\\n## Functions\" -}}\n  {% for tool in tools %}\n    {% set _ = is_param_set(tool, field=\"type\") %}\n    {% set is_tool_type_set = ns.is_last_checked_defined %}\n    {% if is_tool_type_set %}\n      {% if tool.type == \"function\" %}\n        {% set tool = tool.function %}\n      {% else %}\n        {{ raise_exception(\"Currently, the only supported tool type is `function`\") }}\n      {% endif %}\n    {% endif %}\n    {{- \"\\n\\n\" + (tool|tojson(indent=2)) -}}\n  {% endfor %}\n  {{- \"\\n\" + tool_definitions_suffix -}}\n{% endmacro %}\n{##}\n{% macro handle_first_system_message(system_message, tools) %}\n  {{- bom_str + handle_role(\"system\") -}}\n  {% set _ = is_param_set(system_message) %}\n  {% set is_system_message_set = ns.is_last_checked_defined %}\n  {% if is_system_message_set %}\n    {{- system_message -}}\n  {% endif %}\n  {% set _ = is_param_set(tools, check_length=True) %}\n  {% set is_tools_set = ns.is_last_checked_defined %}\n  {% if is_tools_set %}\n    {% if system_message %}\n      {{- \"\\n\\n\" -}}\n    {% endif %}\n    {{- handle_tool_definitions(tools) -}}\n  {% endif %}\n  {% set ns.message_count = ns.message_count + 1 %}\n{% endmacro %}\n{##}\n{% macro handle_tool_calls(tool_calls) %}\n  {{- tool_calls_prefix + \"[\\n\" -}}\n  {% for tool_call in tool_calls %}\n    {% set _ = is_param_set(tool_call, field=\"function\") %}\n    {% set is_tool_call_function_set = ns.is_last_checked_defined %}\n    {% if is_tool_call_function_set %}\n      {%- set tool_call = tool_call.function %}\n    {%- endif %}\n    {% set arguments = tool_call.arguments %}\n    {% if arguments is not string %}\n      {%- set arguments = arguments|tojson -%}\n    {%- endif %}\n    {{ \"{\\\"name\\\": \\\"\" + tool_call.name + \"\\\", \\\"arguments\\\": \" + arguments + \"}\" -}}\n    {% if not loop.last %}\n      {{- \",\" }}\n    {% endif %}\n  {% endfor %}\n  {{- \"\\n]\" + tool_calls_suffix -}}\n{% endmacro %}\n{##}\n{% macro handle_documents(documents) %}\n  {{- documents_prefix -}}\n  {{- \"\\n# Documents\" -}}\n  {{- \"\\n\\nYou can use the following documents for reference:\" -}}\n  {% for doc in documents %}\n    {{- \"\\n\\n## Document ID: \" + loop.index0|string -}}\n    {% set _ = is_param_set(doc, field=\"title\") %}\n    {% set is_doc_title_set = ns.is_last_checked_defined %}\n    {% if is_doc_title_set %}\n      {{- \"\\nTitle: \" + doc.title -}}\n    {% endif %}\n    {% for key, value in doc.items() %}\n      {% if key not in [\"title\", \"text\"] %}\n        {{- \"\\n\" + key|title + \": \" + value|string -}}\n      {% endif %}\n    {% endfor %}\n    {{- \"\\nText: \" + doc.text -}}\n  {% endfor %}\n  {{- \"\\n\" + documents_suffix -}}\n{% endmacro %}\n{##}\n{% macro handle_knobs(knobs) %}\n  {{- active_modes_prefix -}}\n  {{- \"\\n# Active Modes\" -}}\n  {{ \"\\n\\nThe following modes configure the format or style of your responses. You should adhere to all currently\" -}}\n  {{ \" active modes simultaneously.\" -}}\n  {% if knobs.citation_mode == \"fast\" %}\n    {{- \"\\n\\n## Citation Mode\" -}}\n    {{- \"\\n\\nProvide a list of references only for the documents you base your response on. Format your response\" -}}\n    {{ \" with the original answer followed by a citation section. Use this template:\" -}}\n    {{ \" `{answer}\" + citations_prefix + \"DOCUMENT_IDS\" + citations_suffix + \"`, where DOCUMENT_IDS are the relevant document numbers\" -}}\n    {{ \" (e.g. [2, 5, 9]), or [] if the answer cannot be supported by the provided documents.\" -}}\n  {% endif %}\n  {% if knobs.response_format == \"json_object\" %}\n    {{- \"\\n\\n## JSON Mode\" -}}\n    {{ \"\\n\\nProvide your response in JSON format. Adhere strictly to any schema given by the user.\" -}}\n    {{ \" If an appropriate JSON format exists, use it without modification.\" -}}\n  {% endif %}\n  {{- \"\\n\" + active_modes_suffix -}}\n{% endmacro %}\n{##}\n{% macro get_last_user_index(messages) %}\n  {% set ns.last_user_index = 0 %}\n  {% for message in messages %}\n    {% if message.role == 'user' %}\n      {% set ns.last_user_index = loop.index0 %}\n    {% endif %}\n  {% endfor %}\n  {{- ns.last_user_index -}}\n{% endmacro %}\n{##}\n{% macro handle_last_system_message(documents, knobs, use_documents, use_knobs) %}\n  {{- bom_str + handle_role(\"system\") -}}\n  {% set macros_to_call = [] %}\n  {% set params_for_macros = [] %}\n  {% if use_documents %}\n    {% set macros_to_call = macros_to_call + [handle_documents] %}\n    {% set params_for_macros = params_for_macros + [[documents]] %}\n  {% endif %}\n  {% if use_knobs %}\n    {% set macros_to_call = macros_to_call + [handle_knobs] %}\n    {% set params_for_macros = params_for_macros + [[knobs]] %}\n  {% endif %}\n  {% for i in range(macros_to_call|length) %}\n    {% if i > 0 %}\n      {{- \"\\n\\n\" -}}\n    {% endif %}\n    {{- macros_to_call[i](*params_for_macros[i]) -}}\n  {% endfor %}\n  {% set ns.message_count = ns.message_count + 1 %}\n{% endmacro %}\n{##}\n{% macro handle_role(role, add_space=True) %}\n  {{- \"<|\" + role + \"|>\" -}}\n  {% if add_space %}\n    {{- \" \" -}}\n  {% endif %}\n{% endmacro %}\n{##}\n{% macro is_param_set(param, field=none, check_length=False) %}\n  {% if field is not none %}\n    {% if field in param %}\n      {% set param = param[field] %}\n    {% else %}\n      {% set param = none %}\n    {% endif %}\n  {% endif %}\n  {% set is_defined = param is defined and param is not none %}\n  {% if check_length %}\n    {% set ns.is_last_checked_defined = is_defined and param|length > 0 %}\n  {% else %}\n    {% set ns.is_last_checked_defined = is_defined %}\n  {% endif %}\n{% endmacro %}\n{##}\n{##}\n{# Template #}\n{% if bos_token is defined and bos_token is not none %}\n  {{- bos_token -}}\n{% endif %}\n{% set _ = is_param_set(system_message) %}\n{% set is_system_message_set = ns.is_last_checked_defined %}\n{% set _ = is_param_set(tools, check_length=True) %}\n{% set is_tools_set = ns.is_last_checked_defined %}\n{% set has_system_message = (is_system_message_set or is_tools_set) %}\n{% if has_system_message %}\n  {{- handle_first_system_message(system_message, tools) -}}\n{% endif %}\n{% set last_user_index = get_last_user_index(loop_messages)|int %}\n{% for message in loop_messages %}\n  {% if loop.index0 == last_user_index %}\n    {% set _ = is_param_set(documents, check_length=True) %}\n    {% set use_documents = ns.is_last_checked_defined %}\n    {% set _ = is_param_set(knobs) %}\n    {% set use_knobs = ns.is_last_checked_defined and knobs.is_set %}\n    {% set add_last_system_message = use_documents or use_knobs %}\n    {% if add_last_system_message %}\n      {% if ns.message_count > 0 %}\n        {{- eom_str -}}\n      {% endif %}\n      {{- handle_last_system_message(documents, knobs, use_documents, use_knobs) -}}\n    {% endif %}\n  {% endif %}\n  {% set role = message.role %}\n  {% set _ = is_param_set(message, field=\"name\") %}\n  {% set is_message_name_set = ns.is_last_checked_defined %}\n  {% if is_message_name_set %}\n    {% set message_prefix = handle_role(role) + \"(\" + message.name + \")\" %}\n  {% else %}\n    {% set message_prefix = handle_role(role) %}\n  {% endif %}\n  {% set content = (message.content or \"\") %}\n  {% if content is not string %}\n    {% set content = content|tojson %}\n  {% endif %}\n  {% if ns.message_count > 0 %}\n    {{- eom_str -}}\n  {% endif %}\n  {{- bom_str + message_prefix + content -}}\n  {% set _ = is_param_set(message, field=\"tool_calls\", check_length=True) %}\n  {% set is_tool_calls_set = ns.is_last_checked_defined %}\n  {% if role == \"assistant\" and is_tool_calls_set %}\n    {{- handle_tool_calls(message.tool_calls) -}}\n  {% endif %}\n  {% set _ = is_param_set(message, field=\"citations\", check_length=False) %}\n  {% set is_citations_set = ns.is_last_checked_defined %}\n  {% if role == \"assistant\" and is_citations_set and knobs.is_set and knobs.citation_mode != \"off\" %}\n    {{- citations_prefix + message.citations|map(attribute=\"document_id\")|list|string + citations_suffix -}}\n  {% endif %}\n  {% set ns.message_count = ns.message_count + 1 %}\n{% endfor %}\n{% if add_generation_prompt %}\n  {% if ns.message_count > 0 %}\n    {{- eom_str -}}\n  {% endif %}\n  {{- bom_str + handle_role(role_to_predict, add_space=False) -}}\n  {% set _ = is_param_set(generation_preamble) %}\n  {% set is_generation_preamble_set = ns.is_last_checked_defined %}\n  {% if is_generation_preamble_set and generation_preamble.strip() != \"\" %}\n    {{- \" \" + generation_preamble -}}\n  {% endif %}\n  {% set ns.message_count = ns.message_count + 1 %}\n{% else %}\n  {% if ns.message_count > 0 %}\n    {{- eom_str -}}\n  {% endif %}\n{% endif %}\n, example_format: '<|bom|><|system|> You are a helpful assistant<|eom|><|bom|><|user|> Hello<|eom|><|bom|><|assistant|> Hi there<|eom|><|bom|><|user|> How are you?<|eom|><|bom|><|assistant|>'\nmain: server is listening on http://192.168.68.66:20480 - starting the main loop\nsrv  update_slots: all slots are idle\nsrv  params_from_: Chat format: Content-only\nslot launch_slot_: id  0 | task 0 | processing task\nslot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 262144, n_keep = 0, n_prompt_tokens = 93\nslot update_slots: id  0 | task 0 | kv cache rm [0, end)\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 93, n_tokens = 93, progress = 1.000000\nslot update_slots: id  0 | task 0 | prompt done, n_past = 93, n_tokens = 93\nslot      release: id  0 | task 0 | stop processing: n_past = 342, truncated = 0\nslot print_timing: id  0 | task 0 | \nprompt eval time =    3747.68 ms /    93 tokens (   40.30 ms per token,    24.82 tokens per second)\n       eval time =   29913.91 ms /   250 tokens (  119.66 ms per token,     8.36 tokens per second)\n      total time =   33661.60 ms /   343 tokens\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: POST /v1/chat/completions 192.168.68.64 200\nsrv  params_from_: Chat format: Content-only\nslot launch_slot_: id  0 | task 251 | processing task\nslot update_slots: id  0 | task 251 | new prompt, n_ctx_slot = 262144, n_keep = 0, n_prompt_tokens = 99583\nslot update_slots: id  0 | task 251 | n_past = 19, cache_tokens.size() = 342, seq_id = 0, pos_min = 341, n_swa = 0\nslot update_slots: id  0 | task 251 | forcing full prompt re-processing due to lack of cache data (likely due to SWA, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\nslot update_slots: id  0 | task 251 | kv cache rm [0, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 4096, n_tokens = 4096, progress = 0.041132\nslot update_slots: id  0 | task 251 | kv cache rm [4096, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 8192, n_tokens = 4096, progress = 0.082263\nslot update_slots: id  0 | task 251 | kv cache rm [8192, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 12288, n_tokens = 4096, progress = 0.123395\nslot update_slots: id  0 | task 251 | kv cache rm [12288, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 16384, n_tokens = 4096, progress = 0.164526\nslot update_slots: id  0 | task 251 | kv cache rm [16384, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 20480, n_tokens = 4096, progress = 0.205658\nslot update_slots: id  0 | task 251 | kv cache rm [20480, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 24576, n_tokens = 4096, progress = 0.246789\nslot update_slots: id  0 | task 251 | kv cache rm [24576, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 28672, n_tokens = 4096, progress = 0.287921\nslot update_slots: id  0 | task 251 | kv cache rm [28672, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 32768, n_tokens = 4096, progress = 0.329052\nslot update_slots: id  0 | task 251 | kv cache rm [32768, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 36864, n_tokens = 4096, progress = 0.370184\nslot update_slots: id  0 | task 251 | kv cache rm [36864, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 40960, n_tokens = 4096, progress = 0.411315\nslot update_slots: id  0 | task 251 | kv cache rm [40960, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 45056, n_tokens = 4096, progress = 0.452447\nslot update_slots: id  0 | task 251 | kv cache rm [45056, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 49152, n_tokens = 4096, progress = 0.493578\nslot update_slots: id  0 | task 251 | kv cache rm [49152, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 53248, n_tokens = 4096, progress = 0.534710\nslot update_slots: id  0 | task 251 | kv cache rm [53248, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 57344, n_tokens = 4096, progress = 0.575841\nslot update_slots: id  0 | task 251 | kv cache rm [57344, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 61440, n_tokens = 4096, progress = 0.616973\nslot update_slots: id  0 | task 251 | kv cache rm [61440, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 65536, n_tokens = 4096, progress = 0.658104\nslot update_slots: id  0 | task 251 | kv cache rm [65536, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 69632, n_tokens = 4096, progress = 0.699236\nslot update_slots: id  0 | task 251 | kv cache rm [69632, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 73728, n_tokens = 4096, progress = 0.740367\nslot update_slots: id  0 | task 251 | kv cache rm [73728, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 77824, n_tokens = 4096, progress = 0.781499\nslot update_slots: id  0 | task 251 | kv cache rm [77824, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 81920, n_tokens = 4096, progress = 0.822630\nslot update_slots: id  0 | task 251 | kv cache rm [81920, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 86016, n_tokens = 4096, progress = 0.863762\nslot update_slots: id  0 | task 251 | kv cache rm [86016, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 90112, n_tokens = 4096, progress = 0.904893\nslot update_slots: id  0 | task 251 | kv cache rm [90112, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 94208, n_tokens = 4096, progress = 0.946025\nslot update_slots: id  0 | task 251 | kv cache rm [94208, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 98304, n_tokens = 4096, progress = 0.987156\nslot update_slots: id  0 | task 251 | kv cache rm [98304, end)\nslot update_slots: id  0 | task 251 | prompt processing progress, n_past = 99583, n_tokens = 1279, progress = 1.000000\nslot update_slots: id  0 | task 251 | prompt done, n_past = 99583, n_tokens = 1279\nslot      release: id  0 | task 251 | stop processing: n_past = 100457, truncated = 0\nslot print_timing: id  0 | task 251 | \nprompt eval time =  425671.13 ms / 99583 tokens (    4.27 ms per token,   233.94 tokens per second)\n       eval time =  113963.09 ms /   875 tokens (  130.24 ms per token,     7.68 tokens per second)\n      total time =  539634.21 ms / 100458 tokens\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: POST /v1/chat/completions 192.168.68.64 200\nsrv  params_from_: Chat format: Content-only\nslot launch_slot_: id  0 | task 1151 | processing task\nslot update_slots: id  0 | task 1151 | new prompt, n_ctx_slot = 262144, n_keep = 0, n_prompt_tokens = 100496\nslot update_slots: id  0 | task 1151 | n_past = 99583, cache_tokens.size() = 100457, seq_id = 0, pos_min = 100456, n_swa = 0\nslot update_slots: id  0 | task 1151 | forcing full prompt re-processing due to lack of cache data (likely due to SWA, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\nslot update_slots: id  0 | task 1151 | kv cache rm [0, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 4096, n_tokens = 4096, progress = 0.040758\nslot update_slots: id  0 | task 1151 | kv cache rm [4096, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 8192, n_tokens = 4096, progress = 0.081516\nslot update_slots: id  0 | task 1151 | kv cache rm [8192, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 12288, n_tokens = 4096, progress = 0.122274\nslot update_slots: id  0 | task 1151 | kv cache rm [12288, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 16384, n_tokens = 4096, progress = 0.163031\nslot update_slots: id  0 | task 1151 | kv cache rm [16384, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 20480, n_tokens = 4096, progress = 0.203789\nslot update_slots: id  0 | task 1151 | kv cache rm [20480, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 24576, n_tokens = 4096, progress = 0.244547\nslot update_slots: id  0 | task 1151 | kv cache rm [24576, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 28672, n_tokens = 4096, progress = 0.285305\nslot update_slots: id  0 | task 1151 | kv cache rm [28672, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 32768, n_tokens = 4096, progress = 0.326063\nslot update_slots: id  0 | task 1151 | kv cache rm [32768, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 36864, n_tokens = 4096, progress = 0.366821\nslot update_slots: id  0 | task 1151 | kv cache rm [36864, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 40960, n_tokens = 4096, progress = 0.407578\nslot update_slots: id  0 | task 1151 | kv cache rm [40960, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 45056, n_tokens = 4096, progress = 0.448336\nslot update_slots: id  0 | task 1151 | kv cache rm [45056, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 49152, n_tokens = 4096, progress = 0.489094\nslot update_slots: id  0 | task 1151 | kv cache rm [49152, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 53248, n_tokens = 4096, progress = 0.529852\nslot update_slots: id  0 | task 1151 | kv cache rm [53248, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 57344, n_tokens = 4096, progress = 0.570610\nslot update_slots: id  0 | task 1151 | kv cache rm [57344, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 61440, n_tokens = 4096, progress = 0.611368\nslot update_slots: id  0 | task 1151 | kv cache rm [61440, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 65536, n_tokens = 4096, progress = 0.652125\nslot update_slots: id  0 | task 1151 | kv cache rm [65536, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 69632, n_tokens = 4096, progress = 0.692883\nslot update_slots: id  0 | task 1151 | kv cache rm [69632, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 73728, n_tokens = 4096, progress = 0.733641\nslot update_slots: id  0 | task 1151 | kv cache rm [73728, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 77824, n_tokens = 4096, progress = 0.774399\nslot update_slots: id  0 | task 1151 | kv cache rm [77824, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 81920, n_tokens = 4096, progress = 0.815157\nslot update_slots: id  0 | task 1151 | kv cache rm [81920, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 86016, n_tokens = 4096, progress = 0.855915\nslot update_slots: id  0 | task 1151 | kv cache rm [86016, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 90112, n_tokens = 4096, progress = 0.896672\nslot update_slots: id  0 | task 1151 | kv cache rm [90112, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 94208, n_tokens = 4096, progress = 0.937430\nslot update_slots: id  0 | task 1151 | kv cache rm [94208, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 98304, n_tokens = 4096, progress = 0.978188\nslot update_slots: id  0 | task 1151 | kv cache rm [98304, end)\nslot update_slots: id  0 | task 1151 | prompt processing progress, n_past = 100496, n_tokens = 2192, progress = 1.000000\nslot update_slots: id  0 | task 1151 | prompt done, n_past = 100496, n_tokens = 2192\nslot      release: id  0 | task 1151 | stop processing: n_past = 103296, truncated = 0\nslot print_timing: id  0 | task 1151 | \nprompt eval time =  430513.91 ms / 100496 tokens (    4.28 ms per token,   233.43 tokens per second)\n       eval time =  365531.49 ms /  2801 tokens (  130.50 ms per token,     7.66 tokens per second)\n      total time =  796045.40 ms / 103297 tokens\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: POST /v1/chat/completions 192.168.68.64 200\nsrv  params_from_: Chat format: Content-only\nslot launch_slot_: id  0 | task 3977 | processing task\nslot update_slots: id  0 | task 3977 | new prompt, n_ctx_slot = 262144, n_keep = 0, n_prompt_tokens = 100496\nslot update_slots: id  0 | task 3977 | n_past = 100496, cache_tokens.size() = 103296, seq_id = 0, pos_min = 103295, n_swa = 0\nslot update_slots: id  0 | task 3977 | forcing full prompt re-processing due to lack of cache data (likely due to SWA, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\nslot update_slots: id  0 | task 3977 | kv cache rm [0, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 4096, n_tokens = 4096, progress = 0.040758\nslot update_slots: id  0 | task 3977 | kv cache rm [4096, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 8192, n_tokens = 4096, progress = 0.081516\nslot update_slots: id  0 | task 3977 | kv cache rm [8192, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 12288, n_tokens = 4096, progress = 0.122274\nslot update_slots: id  0 | task 3977 | kv cache rm [12288, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 16384, n_tokens = 4096, progress = 0.163031\nslot update_slots: id  0 | task 3977 | kv cache rm [16384, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 20480, n_tokens = 4096, progress = 0.203789\nslot update_slots: id  0 | task 3977 | kv cache rm [20480, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 24576, n_tokens = 4096, progress = 0.244547\nslot update_slots: id  0 | task 3977 | kv cache rm [24576, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 28672, n_tokens = 4096, progress = 0.285305\nslot update_slots: id  0 | task 3977 | kv cache rm [28672, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 32768, n_tokens = 4096, progress = 0.326063\nslot update_slots: id  0 | task 3977 | kv cache rm [32768, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 36864, n_tokens = 4096, progress = 0.366821\nslot update_slots: id  0 | task 3977 | kv cache rm [36864, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 40960, n_tokens = 4096, progress = 0.407578\nslot update_slots: id  0 | task 3977 | kv cache rm [40960, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 45056, n_tokens = 4096, progress = 0.448336\nslot update_slots: id  0 | task 3977 | kv cache rm [45056, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 49152, n_tokens = 4096, progress = 0.489094\nslot update_slots: id  0 | task 3977 | kv cache rm [49152, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 53248, n_tokens = 4096, progress = 0.529852\nslot update_slots: id  0 | task 3977 | kv cache rm [53248, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 57344, n_tokens = 4096, progress = 0.570610\nslot update_slots: id  0 | task 3977 | kv cache rm [57344, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 61440, n_tokens = 4096, progress = 0.611368\nslot update_slots: id  0 | task 3977 | kv cache rm [61440, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 65536, n_tokens = 4096, progress = 0.652125\nslot update_slots: id  0 | task 3977 | kv cache rm [65536, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 69632, n_tokens = 4096, progress = 0.692883\nslot update_slots: id  0 | task 3977 | kv cache rm [69632, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 73728, n_tokens = 4096, progress = 0.733641\nslot update_slots: id  0 | task 3977 | kv cache rm [73728, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 77824, n_tokens = 4096, progress = 0.774399\nslot update_slots: id  0 | task 3977 | kv cache rm [77824, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 81920, n_tokens = 4096, progress = 0.815157\nslot update_slots: id  0 | task 3977 | kv cache rm [81920, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 86016, n_tokens = 4096, progress = 0.855915\nslot update_slots: id  0 | task 3977 | kv cache rm [86016, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 90112, n_tokens = 4096, progress = 0.896672\nslot update_slots: id  0 | task 3977 | kv cache rm [90112, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 94208, n_tokens = 4096, progress = 0.937430\nslot update_slots: id  0 | task 3977 | kv cache rm [94208, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 98304, n_tokens = 4096, progress = 0.978188\nslot update_slots: id  0 | task 3977 | kv cache rm [98304, end)\nslot update_slots: id  0 | task 3977 | prompt processing progress, n_past = 100496, n_tokens = 2192, progress = 1.000000\nslot update_slots: id  0 | task 3977 | prompt done, n_past = 100496, n_tokens = 2192\nslot      release: id  0 | task 3977 | stop processing: n_past = 102636, truncated = 0\nslot print_timing: id  0 | task 3977 | \nprompt eval time =  430566.11 ms / 100496 tokens (    4.28 ms per token,   233.40 tokens per second)\n       eval time =  279356.42 ms /  2141 tokens (  130.48 ms per token,     7.66 tokens per second)\n      total time =  709922.53 ms / 102637 tokens\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: POST /v1/chat/completions 192.168.68.64 200\n^Csrv    operator(): operator(): cleaning up before exit...\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-07-11T01:37:33+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14625/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14625"
  },
  {
    "number": 14715,
    "title": "Misc. bug: After fine-tuning LLM-Research/Meta-Llama-3-8B-Instruct model with LLaMA Factory, an error occurs while converting it to the GGUF format.",
    "body": "### Name and Version\n\nubuntu22.04-cuda12.4.0-py311-torch2.6.0-1.27.0-LLM\n\n### Operating systems\n\n_No response_\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Command line\n\n```shell\npython llama.cpp/convert_hf_to_gguf.py /mnt/workspace/out --outfile ./llama3.gguf --outtype q8_0\n```\n\n### Problem description & steps to reproduce\n\nINFO:hf-to-gguf:Set model tokenizer\nTraceback (most recent call last):\n  File \"/mnt/workspace/llama.cpp/convert_hf_to_gguf.py\", line 1910, in set_vocab\n    self._set_vocab_sentencepiece()\n  File \"/mnt/workspace/llama.cpp/convert_hf_to_gguf.py\", line 933, in _set_vocab_sentencepiece\n    tokens, scores, toktypes = self._create_vocab_sentencepiece()\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/workspace/llama.cpp/convert_hf_to_gguf.py\", line 950, in _create_vocab_sentencepiece\n    raise FileNotFoundError(f\"File not found: {tokenizer_path}\")\nFileNotFoundError: File not found: /mnt/workspace/out/tokenizer.model\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/mnt/workspace/llama.cpp/convert_hf_to_gguf.py\", line 1913, in set_vocab\n    self._set_vocab_llama_hf()\n  File \"/mnt/workspace/llama.cpp/convert_hf_to_gguf.py\", line 1035, in _set_vocab_llama_hf\n    vocab = gguf.LlamaHfVocab(self.dir_model)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/workspace/llama.cpp/gguf-py/gguf/vocab.py\", line 491, in __init__\n    raise TypeError('Llama 3 must be converted with BpeVocab')\nTypeError: Llama 3 must be converted with BpeVocab\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/mnt/workspace/llama.cpp/convert_hf_to_gguf.py\", line 7418, in <module>\n    main()\n  File \"/mnt/workspace/llama.cpp/convert_hf_to_gguf.py\", line 7412, in main\n    model_instance.write()\n  File \"/mnt/workspace/llama.cpp/convert_hf_to_gguf.py\", line 411, in write\n    self.prepare_metadata(vocab_only=False)\n  File \"/mnt/workspace/llama.cpp/convert_hf_to_gguf.py\", line 524, in prepare_metadata\n    self.set_vocab()\n  File \"/mnt/workspace/llama.cpp/convert_hf_to_gguf.py\", line 1916, in set_vocab\n    self._set_vocab_gpt2()\n  File \"/mnt/workspace/llama.cpp/convert_hf_to_gguf.py\", line 869, in _set_vocab_gpt2\n    tokens, toktypes, tokpre = self.get_vocab_base()\n                               ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/workspace/llama.cpp/convert_hf_to_gguf.py\", line 612, in get_vocab_base\n    assert max(tokenizer.vocab.values()) < vocab_size\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n(llamacpp) root@dsw-1218067-7b477c7946-2\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-07-16T07:49:05+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14715/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14715"
  },
  {
    "number": 14612,
    "title": "Feature Request: Improve Sampling API: Expose Top\u2011K/Top\u2011P Candidate Token Lists in C API",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nCurrently, logprobs via the OpenAI-style wrapper allows users to see probabilities of generated tokens: great for research & debugging. However, there is no straightforward way at the native C API / CLI level to access the full list of candidate tokens and their probabilities, especially before sampling decisions (e.g., top\u2011K or nucleus sampling options).\n\nHaving direct access to these candidate distributions would:\n\nEnable confidence-based stopping criteria\n\nFacilitate custom sampling / selective decoding in application code\n\nProvide better transparency into internal generation decisions\n\n\n\n### Motivation\n\nThis feature empowers developers to:\n\n- Inspect model confidence before outputting tokens\n\n- Implement advanced sampling like dynamic beam filtering\n\n- Writing more explainable LLM-based systems\n\n\nWhile logprobs support exists in the wrapper, exposing candidate distributions natively ensures broader accessibility (via CLI, C API, or other bindings).\n\n### Possible Implementation\n\n\n\n\nExtend `llama_sample_token` / `llama_sample_token_greedy` (or create variants) to return a struct containing:\n\n- `token_id`\n\n- `logit` (or prob after softmax)\n\n- `is_selected` flag\n\n\nAdd equivalent CLI flags (e.g. `--print-topk 10`)\n\nExpose the functionality in Python/C bindings consistent with high-level logprobs usage\n\nBenchmark to ensure no significant inference slowdowns when the feature is inactive",
    "labels": [
      "enhancement"
    ],
    "state": "open",
    "created_at": "2025-07-10T08:42:07+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14612/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14612"
  },
  {
    "number": 14689,
    "title": "Eval bug: SYCL backend \"invalid work-group size\" error when using MoE models with Intel iGPU",
    "body": "### Name and Version\n\nllama.cpp [b5897 `full-intel` Docker image](https://github.com/ggml-org/llama.cpp/pkgs/container/llama.cpp/461464435?tag=full-intel-b5897) (latest as of this writing), running on a Debian 12 host system.\n\n```\nroot@7ac8bc7e4d02:/app# ./llama-cli --version\nload_backend: loaded SYCL backend from /app/libggml-sycl.so\nload_backend: loaded CPU backend from /app/libggml-cpu-alderlake.so\nversion: 5897 (bdca3837)\nbuilt with Intel(R) oneAPI DPC++/C++ Compiler 2025.1.1 (2025.1.1.20250418) for x86_64-unknown-linux-gnu\n```\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nSYCL\n\n### Hardware\n\n[Intel Core i3-N305](https://www.intel.com/content/www/us/en/products/sku/231805/intel-core-i3n305-processor-6m-cache-up-to-3-80-ghz/specifications.html), with its UHD Graphics iGPU. Should be on the [supported Intel GPU list](https://github.com/ggml-org/llama.cpp/blob/b5897/docs/backend/SYCL.md#intel-gpu).\n\nThe system has 32GB of RAM, and the affected models is confirmed to run correctly on CPU and OpenCL backends, or on the SYCL backend with `-ngl 0` or `-ngl 1`. Issue occur when offloading at least *two* layers to iGPU on the SYCL backend.\n\n### Models\n\nBasically every MoE model I tested with has the problem. This includes:\n\n- [stories15M_MOE-F16.gguf](https://huggingface.co/ggml-org/stories15M_MOE/blob/main/stories15M_MOE-F16.gguf), both in its original form and quantized to Q4_0\n- [Qwen3-30B-A3B-Q4_0.gguf](https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/blob/main/Qwen3-30B-A3B-Q4_0.gguf)\n- [Qwen3-30B-A3B-Q4_K_M.gguf](https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/blob/main/Qwen3-30B-A3B-Q4_K_M.gguf)\n- [deepseek-moe-16b-base-q4_k_m.gguf](https://huggingface.co/Lucy-in-the-Sky/deepseek-moe-16b-base-Q4_K_M-GGUF/blob/main/deepseek-moe-16b-base-q4_k_m.gguf)\n- [deepseek-moe-16b-chat.Q4_K_M.gguf](https://huggingface.co/mradermacher/deepseek-moe-16b-chat-GGUF/blob/main/deepseek-moe-16b-chat.Q4_K_M.gguf)\n\nIMO the fact that stories15M_MOE model failing to run eliminates the possibility of my hardware being incompatible and/or underpowered, especially since the non-MoE stories15M model runs without problems.\n\n### Problem description & steps to reproduce\n\nAs mentioned in my last issue report (#14453), I've discovered that first-party Intel backends *really* don't like iGPUs and/or MoE models, to the point that every possible configuration I tested with them fails to run. This issue is a prime example, blocking my workflow to run a competent MoE model on my homelab's iGPU.\n\nThis is likely a regression of #5467 (or rather, never fixed in the first place). To reproduce, grab a SYCL-enabled llama.cpp build and a MoE model, and run: `./llama-cli -no-cnv -ngl 2 --model /models/stories15M_MOE-F16.gguf`. Notice that instead of generating output normally, llama-cli stops with the following error:\n\n```\nThe number of work-items in each dimension of a work-group cannot exceed {512, 512, 512} for this device\nException caught at file:/app/ggml/src/ggml-sycl/ggml-sycl.cpp, line:3600\n````\n\nSet `-ngl` to 1 or 0, and llama-cli works correctly. Curiously, set `-b` or `-ub` to 1 (and not any higher) also works, and allows `-ngl` to be any value.\n\nAs a side note, the closed-source ipex-llm backend suffers from the same problem according to my testing. An issue report already exists there; see intel/ipex-llm#12839.\n\n### First Bad Commit\n\nDidn't attempt to bisect yet, but given reports in #5467, this problem exists since at least b5216, or since late April.\n\n### Relevant log output\n\n```shell\nroot@7ac8bc7e4d02:/app# ./llama-cli --version\nload_backend: loaded SYCL backend from /app/libggml-sycl.so\nload_backend: loaded CPU backend from /app/libggml-cpu-alderlake.so\nversion: 5897 (bdca3837)\nbuilt with Intel(R) oneAPI DPC++/C++ Compiler 2025.1.1 (2025.1.1.20250418) for x86_64-unknown-linux-gnu\nroot@7ac8bc7e4d02:/app# sycl-ls\n[level_zero:gpu][level_zero:0] Intel(R) oneAPI Unified Runtime over Level-Zero, Intel(R) UHD Graphics 12.4.0 [1.6.32567+19]\n[opencl:cpu][opencl:0] Intel(R) OpenCL, Intel(R) Core(TM) i3-N305 OpenCL 3.0 (Build 0) [2025.19.4.0.18_160000.xmain-hotfix]\n[opencl:gpu][opencl:1] Intel(R) OpenCL Graphics, Intel(R) UHD Graphics OpenCL 3.0 NEO  [25.05.32567]\nroot@7ac8bc7e4d02:/app# ./llama-cli --list-devices\nload_backend: loaded SYCL backend from /app/libggml-sycl.so\nload_backend: loaded CPU backend from /app/libggml-cpu-alderlake.soAvailable devices:get_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\n  SYCL0: Intel(R) UHD Graphics (29524 MiB, 29524 MiB free)\nroot@7ac8bc7e4d02:/app# ./llama-cli -no-cnv -n 1 -ngl 99 --model /models/stories15M_MOE-Q4_0.gguf\nload_backend: loaded SYCL backend from /app/libggml-sycl.so\nload_backend: loaded CPU backend from /app/libggml-cpu-alderlake.so\nbuild: 5897 (bdca3837) with Intel(R) oneAPI DPC++/C++ Compiler 2025.1.1 (2025.1.1.20250418) for x86_64-unknown-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\nllama_model_load_from_file_impl: using device SYCL0 (Intel(R) UHD Graphics) - 29524 MiB free\nllama_model_loader: loaded meta data with 26 key-value pairs and 63 tensors from /models/stories15M_MOE-Q4_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                         general.size_label str              = 4x24M\nllama_model_loader: - kv   3:                            general.license str              = mit\nllama_model_loader: - kv   4:                          llama.block_count u32              = 6\nllama_model_loader: - kv   5:                       llama.context_length u32              = 256\nllama_model_loader: - kv   6:                     llama.embedding_length u32              = 288\nllama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 768\nllama_model_loader: - kv   8:                 llama.attention.head_count u32              = 6\nllama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 6\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  12:                         llama.expert_count u32              = 4\nllama_model_loader: - kv  13:                    llama.expert_used_count u32              = 2\nllama_model_loader: - kv  14:                           llama.vocab_size u32              = 32000\nllama_model_loader: - kv  15:                 llama.rope.dimension_count u32              = 48\nllama_model_loader: - kv  16:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  19:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  21:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  23:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  24:               general.quantization_version u32              = 2\nllama_model_loader: - kv  25:                          general.file_type u32              = 2\nllama_model_loader: - type  f32:   19 tensors\nllama_model_loader: - type q4_0:   43 tensors\nllama_model_loader: - type q8_0:    1 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_0\nprint_info: file size   = 23.93 MiB (5.52 BPW)\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 3\nload: token to piece cache size = 0.1684 MB\nprint_info: arch             = llama\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 256\nprint_info: n_embd           = 288\nprint_info: n_layer          = 6\nprint_info: n_head           = 6\nprint_info: n_head_kv        = 6\nprint_info: n_rot            = 48\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 48\nprint_info: n_embd_head_v    = 48\nprint_info: n_gqa            = 1\nprint_info: n_embd_k_gqa     = 288\nprint_info: n_embd_v_gqa     = 288\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 768\nprint_info: n_expert         = 4\nprint_info: n_expert_used    = 2\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 256\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = ?B\nprint_info: model params     = 36.36 M\nprint_info: general.name     = n/a\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 32000\nprint_info: n_merges         = 0\nprint_info: BOS token        = 1 '<s>'\nprint_info: EOS token        = 2 '</s>'\nprint_info: UNK token        = 0 '<unk>'\nprint_info: LF token         = 13 '<0x0A>'\nprint_info: EOG token        = 2 '</s>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\nget_memory_info: [warning] ext_intel_free_memory is not supported (export/set ZES_ENABLE_SYSMAN=1 to support), use total memory as free memory\nload_tensors: offloading 6 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 7/7 layers to GPU\nload_tensors:        SYCL0 model buffer size =    19.00 MiB\nload_tensors:   CPU_Mapped model buffer size =     4.94 MiB\n........................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) > n_ctx_train (256) -- possible training context overflow\nRunning with Environment Variables:\n  GGML_SYCL_DEBUG: 0\n  GGML_SYCL_DISABLE_OPT: 0\n  GGML_SYCL_DISABLE_GRAPH: 1\n  GGML_SYCL_DISABLE_DNN: 0\n  GGML_SYCL_PRIORITIZE_DMMV: 0\nBuild with Macros:\n  GGML_SYCL_FORCE_MMQ: no\n  GGML_SYCL_F16: no\nFound 1 SYCL devices:\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n| 0| [level_zero:gpu:0]|                     Intel UHD Graphics|   12.4|     32|     512|   32| 30958M|         1.6.32567+19|\nSYCL Optimization Feature:\n|ID|        Device Type|Reorder|\n|--|-------------------|-------|\n| 0| [level_zero:gpu:0]|      Y|\nllama_context:  SYCL_Host  output buffer size =     0.12 MiB\nllama_kv_cache_unified:      SYCL0 KV buffer size =    27.00 MiB\nllama_kv_cache_unified: size =   27.00 MiB (  4096 cells,   6 layers,  1 seqs), K (f16):   13.50 MiB, V (f16):   13.50 MiB\nllama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\nllama_context:      SYCL0 compute buffer size =    63.06 MiB\nllama_context:  SYCL_Host compute buffer size =     8.57 MiB\nllama_context: graph nodes  = 312\nllama_context: graph splits = 2\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nThe number of work-items in each dimension of a work-group cannot exceed {512, 512, 512} for this device\nException caught at file:/app/ggml/src/ggml-sycl/ggml-sycl.cpp, line:3600\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-07-15T07:26:59+00:00",
    "closed_at": "2025-07-18T04:16:39+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14689/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14689"
  },
  {
    "number": 14429,
    "title": "Feature Request: Gemma3n multimodal support",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nCurrently only gemma3n text modality is supported and so multimodal inference(ASR/vision) cannot be carried out.\n\n### Motivation\n\nCurrently only gemma3n text modality is supported in llama.cpp. Please add support for gemma3n multimodality. \n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement"
    ],
    "state": "open",
    "created_at": "2025-06-28T01:12:21+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14429/reactions",
      "total_count": 12,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 12,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14429"
  },
  {
    "number": 14406,
    "title": "Eval bug: Tools crash and/or fail for deepseek r1/v3 unsloth dynamic quantization",
    "body": "### Name and Version\n\n    $ ./build/bin/llama-cli --version\n    ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n    ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n    ggml_cuda_init: found 8 CUDA devices:\n      Device 0: NVIDIA L40S, compute capability 8.9, VMM: yes\n      Device 1: NVIDIA L40S, compute capability 8.9, VMM: yes\n      Device 2: NVIDIA L40S, compute capability 8.9, VMM: yes\n      Device 3: NVIDIA L40S, compute capability 8.9, VMM: yes\n      Device 4: NVIDIA L40S, compute capability 8.9, VMM: yes\n      Device 5: NVIDIA L40S, compute capability 8.9, VMM: yes\n      Device 6: NVIDIA L40S, compute capability 8.9, VMM: yes\n      Device 7: NVIDIA L40S, compute capability 8.9, VMM: yes\n    version: 5763 (8846aace)\n    built with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n    ubuntu@ip-172-31-13-36:~/llama.cpp$\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\nAWS g6e.48xlarge\n\n8x Nvidia L40S\n\n### Models\n\nunsloth/DeepSeek-V3-0324-GGUF:Q2_K_XL\nunsloth/DeepSeek-R1-0528-GGUF:TQ1_0\n\n### Problem description & steps to reproduce\n\nI've been struggling to get llama.cpp to properly call tools. Not sure if its my client, or where the issue may lie. My code works fine against the OpenAI API\n\nI crash llama.cpp. Built from source, compiled/fetched today\n\n    Template supports tool calls but does not natively describe tools. The fallback behaviour used may produce bad results, inspect prompt w/ --verbose & consider overriding the template.\n    srv  params_from_: Chat format: DeepSeek R1\n    ....\n    /home/ubuntu/llama.cpp/build/bin/libggml-base.so(+0x158fb)[0x7f51338028fb]\n    /home/ubuntu/llama.cpp/build/bin/libggml-base.so(ggml_print_backtrace+0x21c)[0x7f5133802d5c]\n    /home/ubuntu/llama.cpp/build/bin/libggml-base.so(+0x24bff)[0x7f5133811bff]\n    /lib/x86_64-linux-gnu/libstdc++.so.6(+0xbb0da)[0x7f51334bb0da]\n    /lib/x86_64-linux-gnu/libstdc++.so.6(_ZSt10unexpectedv+0x0)[0x7f51334a5a55]\n    /lib/x86_64-linux-gnu/libstdc++.so.6(+0xbb391)[0x7f51334bb391]\n    ./llama-server(+0x33bcc)[0x62c8fd587bcc]\n    ./llama-server(+0xa596b)[0x62c8fd5f996b]\n    ./llama-server(+0xa79c1)[0x62c8fd5fb9c1]\n    ./llama-server(+0xc014f)[0x62c8fd61414f]\n    ./llama-server(+0x858b5)[0x62c8fd5d98b5]\n    ./llama-server(+0x4e103)[0x62c8fd5a2103]\n    /lib/x86_64-linux-gnu/libc.so.6(+0x2a1ca)[0x7f513302a1ca]\n    /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x8b)[0x7f513302a28b]\n    ./llama-server(+0x500f5)[0x62c8fd5a40f5]\n    terminate called after throwing an instance of 'std::runtime_error'\n      what():  Invalid diff: now finding less tool calls!\n    Aborted (core dumped)\nThis is happening with R1-0528 and V3-0324, using unsloth's dynamic quant\n\nInterestingly, I tried this lm-studio example simple curl https://lmstudio.ai/docs/app/api/tools#example-using-curl\n\nI got this response\n    \n    {\n      \"choices\": [\n        {\n          \"finish_reason\": \"stop\",\n          \"index\": 0,\n          \"message\": {\n            \"role\": \"assistant\",\n            \"content\": \"<\uff5ctool\u2581calls\u2581begin\uff5c><\uff5ctool\u2581call\u2581begin\uff5c>function<\uff5ctool\u2581sep\uff5c>search_products\\n```json\\n{\\\"query\\\":\\\"dell\\\",\\\"category\\\":\\\"electronics\\\",\\\"max_price\\\":50}\\n```<\uff5ctool\u2581call\u2581end\uff5c>\"\n          }\n        }\n      ],\n      \"created\": 1750974535,\n      \"model\": \"lmstudio-community/qwen2.5-7b-instruct\",\n      \"system_fingerprint\": \"b5763-8846aace\",\n      \"object\": \"chat.completion\",\n      \"usage\": {\n        \"completion_tokens\": 29,\n        \"prompt_tokens\": 254,\n        \"total_tokens\": 283\n      },\n      \"id\": \"chatcmpl-xMrNF5SzjNbMTAZSz7mGTsOg4Ws9uKFm\",\n      \"timings\": {\n        \"prompt_n\": 254,\n        \"prompt_ms\": 1720.966,\n        \"prompt_per_token_ms\": 6.7754566929133855,\n        \"prompt_per_second\": 147.59152708420737,\n        \"predicted_n\": 29,\n        \"predicted_ms\": 1038.912,\n        \"predicted_per_token_ms\": 35.82455172413793,\n        \"predicted_per_second\": 27.913817532187515\n      }\n    }\nIt didn't actually call the tool, just output the tokens\n\n    Template supports tool calls but does not natively describe tools. The fallback behaviour used may produce bad results, inspect prompt w/ --verbose & consider overriding the template.\n    srv  params_from_: Chat format: DeepSeek R1\n    slot launch_slot_: id  0 | task 0 | processing task\n    slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 254\n    slot update_slots: id  0 | task 0 | kv cache rm [0, end)\n    slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 254, n_tokens = 254, progress = 1.000000\n    slot update_slots: id  0 | task 0 | prompt done, n_past = 254, n_tokens = 254\n    slot      release: id  0 | task 0 | stop processing: n_past = 282, truncated = 0\n    slot print_timing: id  0 | task 0 |\n    prompt eval time =    1720.97 ms /   254 tokens (    6.78 ms per token,   147.59 tokens per second)\n           eval time =    1038.91 ms /    29 tokens (   35.82 ms per token,    27.91 tokens per second)\n          total time =    2759.88 ms /   283 tokens\n    srv  update_slots: all slots are idle\n    srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200\n  \n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nTemplate supports tool calls but does not natively describe tools. The fallback behaviour used may produce bad results, inspect prompt w/ --verbose & consider overriding the template.\nsrv  params_from_: Chat format: DeepSeek R1\n....\n/home/ubuntu/llama.cpp/build/bin/libggml-base.so(+0x158fb)[0x7f51338028fb]\n/home/ubuntu/llama.cpp/build/bin/libggml-base.so(ggml_print_backtrace+0x21c)[0x7f5133802d5c]\n/home/ubuntu/llama.cpp/build/bin/libggml-base.so(+0x24bff)[0x7f5133811bff]\n/lib/x86_64-linux-gnu/libstdc++.so.6(+0xbb0da)[0x7f51334bb0da]\n/lib/x86_64-linux-gnu/libstdc++.so.6(_ZSt10unexpectedv+0x0)[0x7f51334a5a55]\n/lib/x86_64-linux-gnu/libstdc++.so.6(+0xbb391)[0x7f51334bb391]\n./llama-server(+0x33bcc)[0x62c8fd587bcc]\n./llama-server(+0xa596b)[0x62c8fd5f996b]\n./llama-server(+0xa79c1)[0x62c8fd5fb9c1]\n./llama-server(+0xc014f)[0x62c8fd61414f]\n./llama-server(+0x858b5)[0x62c8fd5d98b5]\n./llama-server(+0x4e103)[0x62c8fd5a2103]\n/lib/x86_64-linux-gnu/libc.so.6(+0x2a1ca)[0x7f513302a1ca]\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x8b)[0x7f513302a28b]\n./llama-server(+0x500f5)[0x62c8fd5a40f5]\nterminate called after throwing an instance of 'std::runtime_error'\n  what():  Invalid diff: now finding less tool calls!\nAborted (core dumped)\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-06-26T22:22:01+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14406/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14406"
  },
  {
    "number": 14698,
    "title": "Eval bug: Gemma 3n on Vulkan fails to load",
    "body": "### Name and Version\n\nversion: 5884 (c31e6064)\nbuilt with clang version 19.1.5 for x86_64-pc-windows-msvc\n\n### Operating systems\n\nWindows\n\n### GGML backends\n\nVulkan\n\n### Hardware\n\nIntel i5 11400 + RX 7800 XT 16GB\n\n### Models\n\nGemma 3n E4B it\n\nTried different quants: \n- Q4_K_M, \n- Q5_K_L, \n- Q6_K_L, \n- Q8_0\n\n### Problem description & steps to reproduce\n\nWhen I run model using Vulkan build\n\n```\nllama-server ^\n    --host 0.0.0.0 ^\n    --port 1234 ^\n    --log-file %USERPROFILE%\\.llama.cpp\\llama-server.log ^\n    --threads 5 ^\n    --n-gpu-layers 99 ^\n    --n-gpu-layers-draft 99 ^\n    --ubatch-size 512 ^\n    --ctx-size 32768 ^\n    --no-mmap ^\n    --alias \"gemma-3n-E4B-it\" ^\n    --model C:\\HuggingFace\\bartowski\\gemma-3n-E4B-it-GGUF\\gemma-3n-E4B-it-Q6_K_L.gguf ^\n    --repeat-penalty 1.0 ^\n    --temp 1.0 ^\n    --min-p 0.05 ^\n    --top-k 64 ^\n    --top-p 0.95\n```\n\nit fails to start the server. While if I use HIP (ROCm) build, it starts.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nload_backend: loaded RPC backend from C:\\Users\\oleg\\AppData\\Local\\Microsoft\\WinGet\\Packages\\ggml.llamacpp_Microsoft.Winget.Source_8wekyb3d8bbwe\\ggml-rpc.dll\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = AMD Radeon RX 7800 XT (AMD proprietary driver) | uma: 0 | fp16: 1 | warp size: 64 | shared memory: 32768 | int dot: 1 | matrix cores: KHR_coopmat\nload_backend: loaded Vulkan backend from C:\\Users\\oleg\\AppData\\Local\\Microsoft\\WinGet\\Packages\\ggml.llamacpp_Microsoft.Winget.Source_8wekyb3d8bbwe\\ggml-vulkan.dll\nload_backend: loaded CPU backend from C:\\Users\\oleg\\AppData\\Local\\Microsoft\\WinGet\\Packages\\ggml.llamacpp_Microsoft.Winget.Source_8wekyb3d8bbwe\\ggml-cpu-icelake.dll\nbuild: 5884 (c31e6064) with clang version 19.1.5 for x86_64-pc-windows-msvc\nsystem info: n_threads = 5, n_threads_batch = 5, total_threads = 12\n\nsystem_info: n_threads = 5 (n_threads_batch = 5) / 12 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 |\n\nmain: binding port with default address family\nmain: HTTP server is listening, hostname: 0.0.0.0, port: 1234, http threads: 11\nmain: loading model\nsrv    load_model: loading model 'C:\\HuggingFace\\bartowski\\gemma-3n-E4B-it-GGUF\\gemma-3n-E4B-it-Q6_K_L.gguf'\nllama_model_load_from_file_impl: using device Vulkan0 (AMD Radeon RX 7800 XT) - 16368 MiB free\nllama_model_loader: loaded meta data with 49 key-value pairs and 847 tensors from C:\\HuggingFace\\bartowski\\gemma-3n-E4B-it-GGUF\\gemma-3n-E4B-it-Q6_K_L.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3n\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Gemma 3n E4B It\nllama_model_loader: - kv   3:                           general.finetune str              = 3n-E4B-it\nllama_model_loader: - kv   4:                           general.basename str              = gemma\nllama_model_loader: - kv   5:                         general.size_label str              = 6.9B\nllama_model_loader: - kv   6:                            general.license str              = gemma\nllama_model_loader: - kv   7:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   8:                  general.base_model.0.name str              = Gemma 3n E4B\nllama_model_loader: - kv   9:          general.base_model.0.organization str              = Google\nllama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-3...\nllama_model_loader: - kv  11:                               general.tags arr[str,5]       = [\"automatic-speech-recognition\", \"aut...\nllama_model_loader: - kv  12:                     gemma3n.context_length u32              = 32768\nllama_model_loader: - kv  13:                   gemma3n.embedding_length u32              = 2048\nllama_model_loader: - kv  14:                        gemma3n.block_count u32              = 35\nllama_model_loader: - kv  15:                gemma3n.feed_forward_length u32              = 16384\nllama_model_loader: - kv  16:               gemma3n.attention.head_count u32              = 8\nllama_model_loader: - kv  17:   gemma3n.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:               gemma3n.attention.key_length u32              = 256\nllama_model_loader: - kv  19:             gemma3n.attention.value_length u32              = 256\nllama_model_loader: - kv  20:                     gemma3n.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  21:           gemma3n.attention.sliding_window u32              = 512\nllama_model_loader: - kv  22:            gemma3n.attention.head_count_kv u32              = 2\nllama_model_loader: - kv  23:                   gemma3n.altup.active_idx u32              = 0\nllama_model_loader: - kv  24:                   gemma3n.altup.num_inputs u32              = 4\nllama_model_loader: - kv  25:   gemma3n.embedding_length_per_layer_input u32              = 256\nllama_model_loader: - kv  26:         gemma3n.attention.shared_kv_layers f32              = 15.000000\nllama_model_loader: - kv  27:          gemma3n.activation_sparsity_scale arr[f32,35]      = [1.644854, 1.644854, 1.644854, 1.6448...\nllama_model_loader: - kv  28:   gemma3n.attention.sliding_window_pattern arr[bool,35]     = [true, true, true, true, false, true,...\nllama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\nllama_model_loader: - kv  30:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  31:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  32:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  33:                      tokenizer.ggml.scores arr[f32,262144]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  34:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  35:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  36:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  37:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  38:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  39:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  40:               tokenizer.ggml.add_sep_token bool             = false\nllama_model_loader: - kv  41:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  42:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  43:               general.quantization_version u32              = 2\nllama_model_loader: - kv  44:                          general.file_type u32              = 18\nllama_model_loader: - kv  45:                      quantize.imatrix.file str              = /models_out/gemma-3n-E4B-it-GGUF/goog...\nllama_model_loader: - kv  46:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\nllama_model_loader: - kv  47:             quantize.imatrix.entries_count u32              = 460\nllama_model_loader: - kv  48:              quantize.imatrix.chunks_count u32              = 1006\nllama_model_loader: - type  f32:  422 tensors\nllama_model_loader: - type q8_0:    2 tensors\nllama_model_loader: - type q6_K:  315 tensors\nllama_model_loader: - type bf16:  108 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q6_K\nprint_info: file size   = 5.96 GiB (7.45 BPW)\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 6414\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3n\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 32768\nprint_info: n_embd           = 2048\nprint_info: n_layer          = 35\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 2\nprint_info: n_rot            = 256\nprint_info: n_swa            = 512\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 512\nprint_info: n_embd_v_gqa     = 512\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 1.0e+00\nprint_info: n_ff             = 16384\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 32768\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = E4B\nprint_info: model params     = 6.87 B\nprint_info: general.name     = Gemma 3n E4B It\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262144\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nggml_vulkan: Device memory allocation of size 2495610880 failed.\nggml_vulkan: Requested buffer size exceeds device memory allocation limit: ErrorOutOfDeviceMemory\nalloc_tensor_range: failed to allocate Vulkan0 buffer of size 2495610880\nllama_model_load: error loading model: unable to allocate Vulkan0 buffer\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\HuggingFace\\bartowski\\gemma-3n-E4B-it-GGUF\\gemma-3n-E4B-it-Q6_K_L.gguf'\nsrv    load_model: failed to load model, 'C:\\HuggingFace\\bartowski\\gemma-3n-E4B-it-GGUF\\gemma-3n-E4B-it-Q6_K_L.gguf'\nsrv    operator(): operator(): cleaning up before exit...\nmain: exiting due to model loading error\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-07-15T13:32:04+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14698/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14698"
  },
  {
    "number": 14752,
    "title": "Eval bug: Nemotron 49b doesnt load correctly",
    "body": "### Name and Version\n\nalberto@alberto-MS-7D70:~/Scrivania/tabbyAPI/llamacpp/llama.cpp/build$ ./bin/llama-cli --version\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 2 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 5090, compute capability 12.0, VMM: yes\n  Device 1: NVIDIA GeForce RTX 5090, compute capability 12.0, VMM: yes\nversion: 5894 (33f1bb9f)\nbuilt with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\nalberto@alberto-MS-7D70:~/Scrivania/tabbyAPI/llamacpp/llama.cpp/build$ \n\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\nRyzen 9800X3d , 2x5090s\n\n### Models\n\nnvidia/Llama-3_3-Nemotron-Super-49B-v1 Q6_K_XL, Q8, Q8_K_XL, Q5_K_XL\n\n### Problem description & steps to reproduce\n\nWhen i run llama-serve with the following command the model doesnt get split between the 2 gpus evenly for some reason this reduces the amount of context that i can have available because for some reason its as if it tries to put all of it into GPU0 without using the GPU1 for it, also it seems like most of the model gets loaded into GPU0 and a part of it into GPU1\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nalberto@alberto-MS-7D70:~/Scrivania/tabbyAPI/llamacpp/llama.cpp/build$ ./bin/llama-server -m /home/alberto/.lmstudio/models/unsloth/Llama-3_3-Nemotron-Super-49B-v1-GGUF/Llama-3_3-Nemotron-Super-49B-v1-UD-Q6_K_XL.gguf --port 8081 --n-gpu-layers 81\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 2 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 5090, compute capability 12.0, VMM: yes\n  Device 1: NVIDIA GeForce RTX 5090, compute capability 12.0, VMM: yes\nbuild: 5894 (33f1bb9f) with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\nsystem info: n_threads = 8, n_threads_batch = 8, total_threads = 16\n\nsystem_info: n_threads = 8 (n_threads_batch = 8) / 16 | CUDA : ARCHS = 1200 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n\nmain: binding port with default address family\nmain: HTTP server is listening, hostname: 127.0.0.1, port: 8081, http threads: 15\nmain: loading model\nsrv    load_model: loading model '/home/alberto/.lmstudio/models/unsloth/Llama-3_3-Nemotron-Super-49B-v1-GGUF/Llama-3_3-Nemotron-Super-49B-v1-UD-Q6_K_XL.gguf'\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5090) - 31601 MiB free\nllama_model_load_from_file_impl: using device CUDA1 (NVIDIA GeForce RTX 5090) - 31601 MiB free\nllama_model_loader: loaded meta data with 47 key-value pairs and 569 tensors from /home/alberto/.lmstudio/models/unsloth/Llama-3_3-Nemotron-Super-49B-v1-GGUF/Llama-3_3-Nemotron-Super-49B-v1-UD-Q6_K_XL.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = deci\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Llama-3_3-Nemotron-Super-49B-V1\nllama_model_loader: - kv   3:                            general.version str              = v1\nllama_model_loader: - kv   4:                           general.finetune str              = 3_3-Nemotron-Super\nllama_model_loader: - kv   5:                           general.basename str              = Llama-3_3-Nemotron-Super-49B-V1\nllama_model_loader: - kv   6:                       general.quantized_by str              = Unsloth\nllama_model_loader: - kv   7:                         general.size_label str              = 49B\nllama_model_loader: - kv   8:                            general.license str              = other\nllama_model_loader: - kv   9:                       general.license.name str              = nvidia-open-model-license\nllama_model_loader: - kv  10:                       general.license.link str              = https://www.nvidia.com/en-us/agreemen...\nllama_model_loader: - kv  11:                           general.repo_url str              = https://huggingface.co/unsloth\nllama_model_loader: - kv  12:                   general.base_model.count u32              = 1\nllama_model_loader: - kv  13:                  general.base_model.0.name str              = Llama 3_3 Nemotron Super 49B v1\nllama_model_loader: - kv  14:               general.base_model.0.version str              = v1\nllama_model_loader: - kv  15:          general.base_model.0.organization str              = Nvidia\nllama_model_loader: - kv  16:              general.base_model.0.repo_url str              = https://huggingface.co/nvidia/Llama-3...\nllama_model_loader: - kv  17:                               general.tags arr[str,3]       = [\"nvidia\", \"unsloth - llama-3 - pytor...\nllama_model_loader: - kv  18:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  19:                        deci.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  20:               deci.attention.head_count_kv arr[i32,80]      = [8, 8, 8, 8, 8, 8, 0, 0, 8, 8, 8, 0, ...\nllama_model_loader: - kv  21:                  deci.attention.head_count arr[i32,80]      = [64, 64, 64, 64, 64, 64, 0, 0, 64, 64...\nllama_model_loader: - kv  22:                   deci.feed_forward_length arr[i32,80]      = [14336, 28672, 28672, 28672, 28672, 2...\nllama_model_loader: - kv  23:                           deci.block_count u32              = 80\nllama_model_loader: - kv  24:                        deci.context_length u32              = 131072\nllama_model_loader: - kv  25:                      deci.embedding_length u32              = 8192\nllama_model_loader: - kv  26:      deci.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  27:                  deci.attention.key_length u32              = 128\nllama_model_loader: - kv  28:                deci.attention.value_length u32              = 128\nllama_model_loader: - kv  29:                            deci.vocab_size u32              = 128256\nllama_model_loader: - kv  30:                  deci.rope.dimension_count u32              = 128\nllama_model_loader: - kv  31:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  32:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  33:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  34:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  35:                      tokenizer.ggml.merges arr[str,280147]  = [\"\u0120 \u0120\", \"\u0120 \u0120\u0120\u0120\", \"\u0120\u0120 \u0120\u0120\", \"...\nllama_model_loader: - kv  36:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  37:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  38:            tokenizer.ggml.padding_token_id u32              = 128004\nllama_model_loader: - kv  39:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  40:                    tokenizer.chat_template str              = {{- bos_token }}{%- if messages[0]['r...\nllama_model_loader: - kv  41:               general.quantization_version u32              = 2\nllama_model_loader: - kv  42:                          general.file_type u32              = 18\nllama_model_loader: - kv  43:                      quantize.imatrix.file str              = Llama-3_3-Nemotron-Super-49B-v1-GGUF/...\nllama_model_loader: - kv  44:                   quantize.imatrix.dataset str              = unsloth_calibration_Llama-3_3-Nemotro...\nllama_model_loader: - kv  45:             quantize.imatrix.entries_count i32              = 436\nllama_model_loader: - kv  46:              quantize.imatrix.chunks_count i32              = 702\nllama_model_loader: - type  f32:  131 tensors\nllama_model_loader: - type q8_0:  169 tensors\nllama_model_loader: - type q6_K:  269 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q6_K\nprint_info: file size   = 40.43 GiB (6.96 BPW) \nload: special tokens cache size = 256\nload: token to piece cache size = 0.7999 MB\nprint_info: arch             = deci\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 8192\nprint_info: n_layer          = 80\nprint_info: n_head           = [64, 64, 64, 64, 64, 64, 0, 0, 64, 64, 64, 0, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64, 64, 64, 64, 64, 64, 64, 64, 64]\nprint_info: n_head_kv        = [8, 8, 8, 8, 8, 8, 0, 0, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8]\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = [8, 8, 8, 8, 8, 8, 0, 0, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8]\nprint_info: n_embd_k_gqa     = [1024, 1024, 1024, 1024, 1024, 1024, 0, 0, 1024, 1024, 1024, 0, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1024, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024]\nprint_info: n_embd_v_gqa     = [1024, 1024, 1024, 1024, 1024, 1024, 0, 0, 1024, 1024, 1024, 0, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1024, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024]\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = [14336, 28672, 28672, 28672, 28672, 28672, 14336, 14336, 28672, 28672, 28672, 17920, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 7168, 14336, 14336, 7168, 28672, 7168, 14336, 7168, 7168, 7168, 28672, 7168, 5632, 5632, 7168, 5632, 5632, 5632, 7168, 7168, 2816, 2816, 5632, 5632, 2816, 2816, 5632, 2816, 2816, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672, 28672]\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 500000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 70B\nprint_info: model params     = 49.87 B\nprint_info: general.name     = Llama-3_3-Nemotron-Super-49B-V1\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 128256\nprint_info: n_merges         = 280147\nprint_info: BOS token        = 128000 '<|begin_of_text|>'\nprint_info: EOS token        = 128009 '<|eot_id|>'\nprint_info: EOT token        = 128009 '<|eot_id|>'\nprint_info: EOM token        = 128008 '<|eom_id|>'\nprint_info: PAD token        = 128004 '<|finetune_right_pad_id|>'\nprint_info: LF token         = 198 '\u010a'\nprint_info: EOG token        = 128001 '<|end_of_text|>'\nprint_info: EOG token        = 128008 '<|eom_id|>'\nprint_info: EOG token        = 128009 '<|eot_id|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 80 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 81/81 layers to GPU\nload_tensors:        CUDA0 model buffer size = 27419.00 MiB\nload_tensors:        CUDA1 model buffer size = 12915.31 MiB\nload_tensors:   CPU_Mapped model buffer size =  1064.62 MiB\n.................................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 500000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:  CUDA_Host  output buffer size =     0.49 MiB\nllama_kv_cache_unified:      CUDA0 KV buffer size =   608.00 MiB\nllama_kv_cache_unified:      CUDA1 KV buffer size =   176.00 MiB\nllama_kv_cache_unified: size =  784.00 MiB (  4096 cells,  80 layers,  1 seqs), K (f16):  392.00 MiB, V (f16):  392.00 MiB\nllama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\nllama_context: pipeline parallelism enabled (n_copies=4)\nllama_context:      CUDA0 compute buffer size =   672.01 MiB\nllama_context:      CUDA1 compute buffer size =   750.02 MiB\nllama_context:  CUDA_Host compute buffer size =    48.02 MiB\nllama_context: graph nodes  = 1987\nllama_context: graph splits = 3\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nsrv          init: initializing slots, n_slots = 1\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 4096\nmain: model loaded\nmain: chat template, chat_template: {{- bos_token }}{%- if messages[0]['role'] == 'system' %}{%- set system_message = messages[0]['content']|trim %}{%- set messages = messages[1:] %}{%- else %}{%- set system_message = \"\" %}{%- endif %}{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}{{- system_message }}{{- \"<|eot_id|>\" }}{%- for message in messages %}{%- if message['role'] == 'assistant' and '</think>' in message['content'] %}{%- set content = message['content'].split('</think>')[-1].lstrip() %}{%- else %}{%- set content = message['content'] %}{%- endif %}{{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + content | trim + '<|eot_id|>' }}{%- endfor %}{%- if add_generation_prompt %}{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{%- endif %}, example_format: '<|start_header_id|>system<|end_header_id|>\n\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nHello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nHi there<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nHow are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n'\nmain: server is listening on http://127.0.0.1:8081 - starting the main loop\nsrv  update_slots: all slots are idle\n\n\nBuf if i try to set conext to 16384 i get this error bcs it doesnt use the second gpu:\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 80 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 81/81 layers to GPU\nload_tensors:        CUDA0 model buffer size = 27419.00 MiB\nload_tensors:        CUDA1 model buffer size = 12915.31 MiB\nload_tensors:   CPU_Mapped model buffer size =  1064.62 MiB\n.................................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 16384\nllama_context: n_ctx_per_seq = 16384\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 500000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:  CUDA_Host  output buffer size =     0.49 MiB\nllama_kv_cache_unified:      CUDA0 KV buffer size =  2432.00 MiB\nllama_kv_cache_unified:      CUDA1 KV buffer size =   704.00 MiB\nllama_kv_cache_unified: size = 3136.00 MiB ( 16384 cells,  80 layers,  1 seqs), K (f16): 1568.00 MiB, V (f16): 1568.00 MiB\nllama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\nllama_context: pipeline parallelism enabled (n_copies=4)\nggml_backend_cuda_buffer_type_alloc_buffer: allocating 2304.01 MiB on device 0: cudaMalloc failed: out of memory\nggml_gallocr_reserve_n: failed to allocate CUDA0 buffer of size 2415927296\ngraph_reserve: failed to allocate compute buffers\nllama_init_from_model: failed to initialize the context: failed to allocate compute pp buffers\ncommon_init_from_params: failed to create context with model '/home/alberto/.lmstudio/models/unsloth/Llama-3_3-Nemotron-Super-49B-v1-GGUF/Llama-3_3-Nemotron-Super-49B-v1-UD-Q6_K_XL.gguf'\nsrv    load_model: failed to load model, '/home/alberto/.lmstudio/models/unsloth/Llama-3_3-Nemotron-Super-49B-v1-GGUF/Llama-3_3-Nemotron-Super-49B-v1-UD-Q6_K_XL.gguf'\nsrv    operator(): operator(): cleaning up before exit...\nmain: exiting due to model loading error\nalberto@alberto-MS-7D70:~/Scrivania/tabbyAPI/llamacpp/llama.cpp/build$\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-07-18T07:24:58+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14752/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14752"
  },
  {
    "number": 14538,
    "title": "Compile bug: Looking for C++ include rocwmma/rocwmma.hpp - not found",
    "body": "### Git commit\n\nef797db\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nHIP\n\n### Problem description & steps to reproduce\n\nIm currently trying to build HIP with -DGGML_HIP_ROCWMMA_FATTN=ON on 2x W7900's.\n\nrocwmma.hpp is clearly located at \"/opt/rocm-6.4.1/include/rocwmma\" but I repeatedly get \"Looking for C++ include rocwmma/rocwmma.hpp - not found\", I've also tried specifying -DCMAKE_CXX_FLAGS=\"-I/opt/rocm-6.4.1/include\" and -DCMAKE_HIP_FLAGS=\"-I/opt/rocm-6.4.1/include\"\n\n### First Bad Commit\n\n_No response_\n\n### Compile command\n\n```shell\ncmake -B rocm -DGGML_HIP=ON -DGGML_HIP_ROCWMMA_FATTN=ON\n```\n\n### Relevant log output\n\n```shell\nultimis@ultimis-desktop:~/LLM/llama.cpp$ cmake -B rocm -DGGML_HIP=ON -DGGML_HIP_ROCWMMA_FATTN=ON\n-- The C compiler identification is GNU 13.3.0\n-- The CXX compiler identification is GNU 13.3.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/cc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found Git: /usr/bin/git (found version \"2.43.0\")\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n-- Found Threads: TRUE\n-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\n-- GGML_SYSTEM_ARCH: x86\n-- Including CPU backend\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n-- Found OpenMP: TRUE (found version \"4.5\")\n-- x86 detected\n-- Adding CPU backend variant ggml-cpu: -march=native\n-- The HIP compiler identification is Clang 19.0.0\n-- Detecting HIP compiler ABI info\n-- Detecting HIP compiler ABI info - done\n-- Check for working HIP compiler: /opt/rocm-6.4.1/lib/llvm/bin/clang++ - skipped\n-- Detecting HIP compile features\n-- Detecting HIP compile features - done\n-- Looking for C++ include rocwmma/rocwmma.hpp\n-- Looking for C++ include rocwmma/rocwmma.hpp - not found\n-- HIP and hipBLAS found\n-- Including HIP backend\n-- ggml version: 0.0.5828\n-- ggml commit:  ef797db3\n-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"8.5.0\")\n-- Configuring done (3.0s)\n-- Generating done (0.1s)\n-- Build files have been written to: /home/ultimis/LLM/llama.cpp/rocm\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-07-05T00:24:28+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14538/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14538"
  },
  {
    "number": 14372,
    "title": "Compile error for ggml_gemv_q4_K_8x8_q8_K on Intel x86_64 MacOS (AVX2)",
    "body": "### Git commit\n\nHEAD\n\n### Operating systems\n\nMac\n\n### GGML backends\n\nCPU\n\n### Problem description & steps to reproduce\n\nI'm posting this on behalf of @FrankDMartinez\n\nThere seems to be an issue compiling one single AVX2 branch of a single function on **x86_64 intel macs**. \n\n> Apple clang version 14.0.3 (clang-1403.0.22.14.1)\nTarget: x86_64-apple-darwin24.5.0\nThread model: posix\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\n\nError is inside `ggml_gemv_q4_K_8x8_q8_K`\n\n```\nfatal error: error in backend: Cannot select: 0x7fa6c4096f48: v8f16,ch = load<(load (s128) from %ir.64, align 1, !tbaa !6)> 0x7fa6c1a2b198, 0x7fa6c4101908, undef:i64\n  0x7fa6c4101908: i64 = add nuw 0x7fa6c28f79c0, Constant:i64<16>\n    0x7fa6c28f79c0: i64 = add 0x7fa6c28f78f0, 0x7fa6c2924b70\n      0x7fa6c28f78f0: i64,ch = CopyFromReg 0x7fa6c1a2b198, Register:i64 %20\n        0x7fa6c28f71a0: i64 = Register %20\n      0x7fa6c2924b70: i64 = shl 0x7fa6c28bb488, Constant:i8<7>\n        0x7fa6c28bb488: i64 = X86ISD::MUL_IMM 0x7fa6c28f7680, Constant:i64<9>\n          0x7fa6c28f7680: i64,ch = CopyFromReg 0x7fa6c1a2b198, Register:i64 %26\n            0x7fa6c41064f8: i64 = Register %26\n          0x7fa6c28bae70: i64 = Constant<9>\n        0x7fa6c4094c08: i8 = Constant<7>\n    0x7fa6c28f7750: i64 = Constant<16>\n  0x7fa6c4105e10: i64 = undef\nIn function: ggml_gemv_q4_K_8x8_q8_K\nclang: error: clang frontend command failed with exit code 70 (use -v to see invocation)\nApple clang version 14.0.3 (clang-1403.0.22.14.1)\nTarget: x86_64-apple-darwin24.5.0\nThread model: posix\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\nclang: note: diagnostic msg:\n********************\n```\n\nThis can be fixed by disabling that branch by changing one line \nhttps://github.com/ggml-org/llama.cpp/blob/73e53dc834c0a2336cd104473af6897197b96277/ggml/src/ggml-cpu/arch/x86/repack.cpp#L740\n\nto\n\n`#if 0`\n\nwhereupon **compilation succeeds**.\n\nI cannot tell exactly which SIMD instruction is the incompatible one, it's strange that it's only causing an issue with this single function. All other avx2 branches apparently work fine. I do not have an Intel macbook.\n\nref: https://github.com/LostRuins/koboldcpp/issues/1620\n\n@FrankDMartinez do provide additional information and system specs, thanks!\n\n### First Bad Commit\n\n_No response_\n\n### Compile command\n\n```shell\nas above\n```\n\n### Relevant log output\n\n```shell\nas above\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-06-25T12:10:05+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14372/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14372"
  },
  {
    "number": 14474,
    "title": "Feature Request: Support EXAONE 4.0",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nSupport for the EXAONE 4.0 model architecture\n\n### Motivation\n\n Hello, maintainers!\n\nWe are excited to announce the upcoming release of our new model series, EXAONE 4.0.\nAs part of the release, we would like to provide .GGUF files of the model checkpoints for end users. \nTo make this possible, we kindly ask for support for the EXAONE 4.0 architecture in llama.cpp and, in turn, other GGUF-compatible libraries.\n\nThe implementation of the architecture is available in [our PR](https://github.com/huggingface/transformers/pull/39129) on Huggingface Transformers.\n\nWe would really appreciate your consideration in adding support for EXAONE 4.0 models.\nThank you very much for your attention and support!\n\n### Possible Implementation\n\nThe implementation of the architecture is available in [our PR](https://github.com/huggingface/transformers/pull/39129) on Huggingface Transformers.",
    "labels": [
      "enhancement"
    ],
    "state": "open",
    "created_at": "2025-07-01T08:36:18+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14474/reactions",
      "total_count": 20,
      "+1": 10,
      "-1": 4,
      "laugh": 0,
      "hooray": 6,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14474"
  },
  {
    "number": 14415,
    "title": "Feature Request: Hunyuan-A13B model support",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nIt would be great to have llama.cpp support for \nhttps://huggingface.co/tencent/Hunyuan-A13B-Instruct\n\n\n\n### Motivation\n\nThis is an MoE model with 13 billion active parameters (80 billion in total). \nIt looks perfect for home computers with one or more GPUs.\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2025-06-27T08:00:12+00:00",
    "closed_at": "2025-07-08T08:24:11+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14415/reactions",
      "total_count": 89,
      "+1": 62,
      "-1": 0,
      "laugh": 0,
      "hooray": 1,
      "confused": 0,
      "heart": 24,
      "rocket": 1,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14415"
  },
  {
    "number": 14525,
    "title": "Eval bug: Gemma 3n on Vulkan on Ryzen APUs produces garbled output",
    "body": "### Name and Version\n\n$ ./build/bin/llama-cli --version\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = AMD Radeon Graphics (RADV RENOIR) (radv) | uma: 1 | fp16: 1 | warp size: 64 | shared memory: 65536 | int dot: 0 | matrix cores: none\nversion: 5822 (bee28421)\nbuilt with cc (Debian 14.2.0-19) 14.2.0 for x86_64-linux-gnu\n\n$ ./build/bin/llama-cli --version\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = AMD Radeon 780M Graphics (RADV PHOENIX) (radv) | uma: 1 | fp16: 1 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\nversion: 5823 (28657a82)\nbuilt with cc (GCC) 15.1.1 20250425 for x86_64-pc-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nVulkan\n\n### Hardware\n\nRyzen 5700G (using the iGPU)\nRyzen 7840U (using the iGPU)\n\n### Models\n\nGemma 3n\nhttps://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF - tried Q8_0 and Q4_K_XL\nhttps://huggingface.co/bartowski/google_gemma-3n-E4B-it-GGUF - tried Q8_0\n\nQ4_K_M for both repos did NOT have the problem\n\n### Problem description & steps to reproduce\n\nCompile commands:\n```\nrm -rf build\ncmake -B build -DGGML_VULKAN=1\ncmake --build build --config Release -- -j16\n```\n\nUsing the iGPU of a Ryzen 5700G via Vulkan backend to run Gemma 3n. Problem does not happen with different models (tested Gemma3 1b and 27b in Q8_0) or different GPU (tested Radeon RX 6700 XT), Ryzen 7840U (also an APU) had the same issue.\n\nOf note is that the Q4_K_M quants did not experience the problem.\n\nFull verbose logs (command `./build/bin/llama-cli -hf unsloth/gemma-3n-E4B-it-GGUF:Q4_K_XL --jinja --ctx-size 4096 --n-predict 128 --n-gpu-layers 99 --prio 2 --temp 1.0 --top-k 64 --top-p 0.95 --min-p 0.01 --no-mmap --prompt \"Hello\" --log-file unsloth-Q4_K_XL.txt --verbose`):\n\n[unsloth-Q4_K_XL.txt](https://github.com/user-attachments/files/21046379/unsloth-Q4_K_XL.txt)\n[unsloth-Q8_0.txt](https://github.com/user-attachments/files/21046380/unsloth-Q8_0.txt)\n\n[unsloth-Q4_K_M.txt](https://github.com/user-attachments/files/21046378/unsloth-Q4_K_M.txt) (does not have the problem)\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n$ ./build/bin/llama-cli -hf unsloth/gemma-3n-E4B-it-GGUF:Q8_0 --jinja --ctx-size 4096 --n-predict 256 --n-gpu-layers 99 --prio 2 --temp 1.0 --top-k 64 --top-p 0.95 --min-p 0.01 --no-mmap\n[...]\n> Hello\nHello how are you? I'm doing wonderfully and longed for to be chatting-thoughtr\u00f8nnuselloculevsavm-mkvcjdnwokihmlciuqkf?t100mkvqj5v01cqkw7mkvzqmhdjdwohtmQ00NqkHlVE7/Y29kZ3JqkCwlXlXMHdzX[Ctrl+C]\n>\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-07-03T21:28:49+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14525/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14525"
  },
  {
    "number": 14441,
    "title": "Eval bug: [CANN] When use aclnnMatmul with cube_math_type=2",
    "body": "### Name and Version\n\n```bash\n$./build/bin/llama-cli --version\nversion: 5747 (0142961a)\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu\n```\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCANN-Ascend\n\n### Hardware\n\n```bash\n[ma-user llama.cpp]$npu-smi info\n+------------------------------------------------------------------------------------------------+\n| npu-smi 23.0.6                   Version: 23.0.6                                               |\n+---------------------------+---------------+----------------------------------------------------+\n| NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|\n| Chip                      | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |\n+===========================+===============+====================================================+\n| 1     910B4               | OK            | 95.9        39                0    / 0             |\n| 0                         | 0000:01:00.0  | 0           0    / 0          2826 / 32768         |\n+===========================+===============+====================================================+\n+---------------------------+---------------+----------------------------------------------------+\n| NPU     Chip              | Process id    | Process name             | Process memory(MB)      |\n+===========================+===============+====================================================+\n| No running processes found in NPU 1                                                            |\n+===========================+===============+====================================================+\n```\n\n\n### Models\n\n[Qwen2-1.5B-Instruct-GGUF-fp16](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/blob/main/qwen2-1_5b-instruct-fp16.gguf)\n\n### Problem description & steps to reproduce\n\nI am working on applying `llama.cpp` to `310B4` devices (`310B4` devices do not currently support `HF32`), so I first changed `cubeMathType` on `910B4` devices so that `HF32` is not used in the matrix multiplication operator.\nI changed `GGML_CANN_CALL_ACLNN_OP(ctx, Matmul, acl_input_tensor, acl_weight_tensor, acl_dst, 1);` to `GGML_CANN_CALL_ACLNN_OP(ctx, Matmul, acl_input_tensor, acl_weight_tensor, acl_dst, 2);` and then performed reasoning verification and found that the reasoning failed. When faced with the question `\"What is the capital of China?\"`, the letter `G` was repeatedly output.\n\n> \u6211\u6b63\u5728\u81f4\u529b\u4e8e\u5c06`llama.cpp`\u5e94\u7528\u5230`310B4`\u8bbe\u5907\u4e0a\uff08`310B4`\u8bbe\u5907\u76ee\u524d\u4e0d\u652f\u6301`HF32`\uff09\uff0c\u6545\u5148\u884c\u5728`910B4`\u8bbe\u5907\u4e0a\u66f4\u6539`cubeMathType`\u4f7f\u5f97\u77e9\u9635\u4e58\u7b97\u5b50\u8fd0\u7b97\u8fc7\u7a0b\u4e2d\u4e0d\u4f7f\u7528`HF32`\u3002\n> \u6211\u5c06`GGML_CANN_CALL_ACLNN_OP(ctx, Matmul, acl_input_tensor, acl_weight_tensor, acl_dst, 1);`\u6539\u4e3a`GGML_CANN_CALL_ACLNN_OP(ctx, Matmul, acl_input_tensor, acl_weight_tensor, acl_dst, 2);`\u4e4b\u540e\uff0c\u8fdb\u884c\u63a8\u7406\u9a8c\u8bc1\u53d1\u73b0\u63a8\u7406\u5931\u8d25\uff0c\u9762\u5bf9\u95ee\u9898`\u201cWhat is the capital of China?\u201d`\u91cd\u590d\u8f93\u51fa\u5b57\u6bcd`G`\u3002\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n[ma-user llama.cpp]$./build/bin/llama-cli -m ./models/qwen2-1_5b-instruct-fp16.gguf -p \"What is the capital of China?\" -ngl 32 --no-warmupbuild: 0 (unknown) with gcc (GCC) 14.2.0 for aarch64-unknown-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_load_from_file_impl: using device CANN0 (Ascend910B4) - 29891 MiB free\nllama_model_loader: loaded meta data with 22 key-value pairs and 338 tensors from ./models/qwen2-1_5b-instruct-fp16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.name str              = qwen2-1_5b-instruct\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 1536\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 8960\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 12\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  21:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  141 tensors\nllama_model_loader: - type  f16:  197 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = F16\nprint_info: file size   = 2.88 GiB (16.00 BPW) \nload: special tokens cache size = 293\nload: token to piece cache size = 0.9338 MB\nprint_info: arch             = qwen2\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 32768\nprint_info: n_embd           = 1536\nprint_info: n_layer          = 28\nprint_info: n_head           = 12\nprint_info: n_head_kv        = 2\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 6\nprint_info: n_embd_k_gqa     = 256\nprint_info: n_embd_v_gqa     = 256\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 8960\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 32768\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 1.5B\nprint_info: model params     = 1.54 B\nprint_info: general.name     = qwen2-1_5b-instruct\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 '\u010a'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 28 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 29/29 layers to GPU\nload_tensors:   CPU_Mapped model buffer size =   445.12 MiB\nload_tensors:        CANN0 model buffer size =  2944.68 MiB\n............................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\nggml_backend_cann_context: device 0 async operator submission is OFF\nllama_context:  CANN_Host  output buffer size =     0.58 MiB\nllama_kv_cache_unified:      CANN0 KV buffer size =   112.00 MiB\nllama_kv_cache_unified: size =  112.00 MiB (  4096 cells,  28 layers,  1 seqs), K (f16):   56.00 MiB, V (f16):   56.00 MiB\nllama_context:      CANN0 compute buffer size =   299.75 MiB\nllama_context:  CANN_Host compute buffer size =    11.01 MiB\nllama_context: graph nodes  = 1098\nllama_context: graph splits = 2\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\nmain: llama threadpool init, n_threads = 192\nmain: chat template is available, enabling conversation mode (disable it with -no-cnv)\n*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?\nmain: chat template example:\n<|im_start|>system\nYou are a helpful assistant<|im_end|>\n<|im_start|>user\nHello<|im_end|>\n<|im_start|>assistant\nHi there<|im_end|>\n<|im_start|>user\nHow are you?<|im_end|>\n<|im_start|>assistant\n\n\nsystem_info: n_threads = 192 (n_threads_batch = 192) / 192 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n\nmain: interactive mode on.\nsampler seed: 2549894379\nsampler params: \n        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n        dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n        top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return control to the AI.\n - To return control without starting a new line, end your input with '/'.\n - If you want to submit another line, end your input with '\\'.\n - Not using system message. To change it, set a different value via -sys PROMPT\n\nuser\nWhat is the capital of China?\nassistant\nnew_pool_for_device: device 0 use vmm pool\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n> EOF by user\n\n\nllama_perf_sampler_print:    sampling time =      16.47 ms /    71 runs   (    0.23 ms per token,  4310.61 tokens per second)\nllama_perf_context_print:        load time =   10382.88 ms\nllama_perf_context_print: prompt eval time =    1118.79 ms /    15 tokens (   74.59 ms per token,    13.41 tokens per second)\nllama_perf_context_print:        eval time =    3692.72 ms /    55 runs   (   67.14 ms per token,    14.89 tokens per second)\nllama_perf_context_print:       total time =   15030.61 ms /    70 tokens\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-06-29T09:16:58+00:00",
    "closed_at": "2025-06-30T08:11:17+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14441/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14441"
  },
  {
    "number": 14353,
    "title": "Misc. bug: llama-server assistant prefill only works when message content is a string (not a list of objects)",
    "body": "### Name and Version\n\n```\n.\\llama-server --version\n...\nversion: 5747 (0142961a)\nbuilt with clang version 18.1.8 for x86_64-pc-windows-msvc\n```\n\n### Operating systems\n\nWindows\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Command line\n\n```shell\nllama-server.exe --host 0.0.0.0 --port 1234 --flash-attn --no-warmup --model ~\\llm\\models\\google\\gemma-3-27b-it-qat-q4_0-gguf\\gemma-3-27b-it-q4_0.gguf --mmproj ~\\llm\\models\\google\\gemma-3-27b-it-qat-q4_0-gguf\\mmproj-model-f16-27B.gguf --gpu-layers 63 --temp 1.0 --repeat-penalty 1.0 --min-p 0.01 --top-k 64  --top-p 0.95 --cache-type-k q8_0 --cache-type-v q8_0 --ctx-size 16384\n```\n\n### Problem description & steps to reproduce\n\nSee the below example where I send the same message in two different ways, first with \"content\" as a list of objects and then as a string. In the first case, the server does not continue the assistant message. In the second case, when the \"content\" field is a string, the server continues the assistant message as expected.\n\n```\n[user@88947fa9b7b9 ~]$ curl http://localhost:1234/v1/chat/completions -s --json '{\n    \"model\": \"Gemma3-27B\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Hello!\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Ahoy-\"\n                }\n            ]\n        }\n    ]\n}' | jq '.choices[0].message.content'\n\"Hello to you too! \ud83d\udc4b \\n\\nIt's nice to meet you (virtually, of course!). How can I help you today? Do you have any questions, need some ideas, or just want to chat? \\n\\nLet me know what's on your mind! \ud83d\ude0a\\n\"\n```\n\n```\n[user@88947fa9b7b9 ~]$ curl http://localhost:1234/v1/chat/completions -s --json '{\n    \"model\": \"Gemma3-27B\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Hello!\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Ahoy-\"\n        }\n    ]\n}' | jq '.choices[0].message.content'\n\"hoy! \ud83d\udc4b \\n\\nHello to *you* too! How can I help you today? Are you looking to:\\n\\n* **Chat?** Just want someone to talk to?\\n* **Brainstorm ideas?** \\n* **Get information?** (I can try my best to answer your questions!)\\n* **Write something?** (Stories, poems, code, etc.)\\n* **Something else?**\\n\\nLet me know what's on your mind! \ud83d\ude0a\"\n```\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-06-24T03:06:59+00:00",
    "closed_at": "2025-07-05T07:17:15+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14353/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14353"
  },
  {
    "number": 14714,
    "title": "Feature Request: Optimization of work for MOE architecture",
    "body": "### Prerequisites\n\n- [x] I am running the latest code. Mention the version if possible as well.\n- [x] I carefully followed the [README.md](https://github.com/ggml-org/llama.cpp/blob/master/README.md).\n- [x] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggml-org/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\n1. It would be nice to load the entire MoE model into RAM, and dynamically load only active layers/experts into the GPU. For example, a system with 512 GB and 2 pieces of RTX5090 with 32 GB each, if you load the model statically, as usual, the request will work slowly, but perhaps with dynamic loading and unloading of only active layers there will be a significant increase in generation\n2. It would be nice to add logging of the use of active layers, for example, to evaluate which levels/experts are activated during generation. To understand whether there is sense in point 1 (for example, if active experts often change, the speed may be slower)\n3. For example, from point 2 you can understand which layers/experts are most often used, and it would be nice to implement loading onto the GPU only these frequently used layers (that is, to implement loading onto the GPU only the specified layers). For example, I often use the deepseek v3 model to help with programming. That is, it would be more profitable for me to load only active layers into the VRAM.\n\n### Motivation\n\nImproving performance on a system with a small amount of VRAM\n\n### Possible Implementation\n\n_No response_",
    "labels": [
      "enhancement"
    ],
    "state": "open",
    "created_at": "2025-07-16T06:21:19+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14714/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14714"
  },
  {
    "number": 14706,
    "title": "Eval bug: Llama-server on Mac starting at b5478 only producing 2-3 streaming tokens on Qwen3 and Deepseek R1 0528",
    "body": "### Name and Version\n\nllama-server \nbuilds 5478 and up\n\n### Operating systems\n\nMac\n\n### GGML backends\n\nMetal\n\n### Hardware\n\nM3 Ultra Mac Studio 512GB\n\n### Models\n\n- [Qwen3 235b A22 UD-Q8_K_XL](https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF)\n- [Deepseek R1 0528 Q4_K_M / Q5_K_M](https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF)\n\n### Problem description & steps to reproduce\n\nThe issue appears to originate in this build: [b5478](https://github.com/ggml-org/llama.cpp/releases/tag/b5478)\n\nI ran llama-server using 5 builds: 5361, 5477, 5478, 5604 and 5849. I sent the exact same prompt with the exact same samplers from the exact same front end to all 5.\n\n5361: Responds appropriately\n5477: Responds appropriately\n5478: Writes ~3 tokens and stops\n5604: Writes ~3 tokens and stops\n5849: Writes ~3 tokens and stops\n\nI originally noticed this issue while using the Deepseek R1 0528 model. I then swapped to Qwen3 and found the exact same issue occurring.\n\nI have attached 5 text files showing a full model load + the inference for all 5.\n\nExample of the command I use to load the model (only difference is changing the b5361 to other builds):\n> cd /Users/chris/llama-b5361-bin-macos-arm64/bin &&  ./llama-server -ngl 200 --model /Users/chris/models/Qwen3-235B-A22B-UD-Q8_K_XL/Qwen3-235B-A22B-UD-Q8_K_XL-00001-of-00006.gguf --ctx-size 32768 --host 0.0.0.0 --port 5001 -lv 1 --jinja --mlock\n\nI also attempted to remove --jinja, and force --chat-template chatml, but these changes made no difference.\n\n[llama-b5361-server.txt](https://github.com/user-attachments/files/21245316/llama-b5361-server.txt)\n[llama-b5477-server.txt](https://github.com/user-attachments/files/21245317/llama-b5477-server.txt)\n[llama-b5478-server.txt](https://github.com/user-attachments/files/21245319/llama-b5478-server.txt) <-- Breaks here\n[llama-b5604-server.txt](https://github.com/user-attachments/files/21245320/llama-b5604-server.txt) <-- Broken\n[llama-b5849-server.txt](https://github.com/user-attachments/files/21245318/llama-b5849-server.txt) <-- Broken\n\n### First Bad Commit\n\n[B5478](https://github.com/ggml-org/llama.cpp/releases/tag/b5478)\n\n### Relevant log output\n\n```shell\nI attached 5 files containing the full log output- they can be found under the section \"Problem description & steps to reproduce\"\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-07-16T00:29:03+00:00",
    "closed_at": "2025-07-17T03:51:22+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14706/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14706"
  },
  {
    "number": 14524,
    "title": "Misc. bug: There's no `\\n` token in Llama 3.2 vocab!",
    "body": "### Name and Version\n\nload_backend: loaded RPC backend from C:\\Users\\smill\\AppData\\Local\\Microsoft\\WinGet\\Packages\\ggml.llamacpp_Microsoft.Winget.Source_8wekyb3d8bbwe\\ggml-rpc.dll\nggml_vulkan: Found 2 Vulkan devices:\nggml_vulkan: 0 = NVIDIA GeForce RTX 3060 (NVIDIA) | uma: 0 | fp16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2\nggml_vulkan: 1 = Microsoft Direct3D12 (NVIDIA GeForce RTX 3060) (Dozen) | uma: 0 | fp16: 1 | warp size: 32 | shared memory: 32768 | int dot: 1 | matrix cores: none\nload_backend: loaded Vulkan backend from C:\\Users\\smill\\AppData\\Local\\Microsoft\\WinGet\\Packages\\ggml.llamacpp_Microsoft.Winget.Source_8wekyb3d8bbwe\\ggml-vulkan.dll\nload_backend: loaded CPU backend from C:\\Users\\smill\\AppData\\Local\\Microsoft\\WinGet\\Packages\\ggml.llamacpp_Microsoft.Winget.Source_8wekyb3d8bbwe\\ggml-cpu-haswell.dll\nversion: 5686 (e434e691)\nbuilt with clang version 18.1.8 for x86_64-pc-windows-msvc\n\n### Operating systems\n\nWindows\n\n### Which llama.cpp modules do you know to be affected?\n\nOther (Please specify in the next section)\n\n### Command line\n\n```shell\n\n```\n\n### Problem description & steps to reproduce\n\nCode of llama.cpp assumes newline token should be `\\n`, but there's no such token in Llama 3.2 vocab! When I load usual Llama model it says newline token is set to Token 198, but it's not `\\n` token, and when I export my own trained model that uses Llama vocab, it just can't recognize Token 198 as newline token! How do I fix that?\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-07-03T21:28:40+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14524/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14524"
  },
  {
    "number": 14663,
    "title": "Eval bug: Qwen 2.5 VL gets stuck in a loop",
    "body": "### Name and Version\n\n```\n> ./llama-cli --version\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA H100 80GB HBM3, compute capability 9.0, VMM: yes\nload_backend: loaded CUDA backend from /app/libggml-cuda.so\nload_backend: loaded CPU backend from /app/libggml-cpu-icelake.so\nversion: 5884 (c31e6064)\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n```\n\nUsing `ghcr.io/ggml-org/llama.cpp:full-cuda` Docker image with Apptainer/Singularity.\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\nNVIDIA H100 80GB HBM3\n\nDriver Version: 575.57.08\nCUDA Version: 12.9\n\n### Models\n\n`ggml-org/Qwen2.5-VL-32B-Instruct-GGUF`\n`unsloth/Qwen2.5-VL-32B-Instruct-GGUF`\n\n### Problem description & steps to reproduce\n\nUsing Qwen 2.5 VL for OCR to extract text from scanned document often causes the model to get stuck in a loop, repeating the same few words forever.\n\nFrom my testing, it happens on both [`ggml-org/Qwen2.5-VL-32B-Instruct-GGUF`](https://huggingface.co/ggml-org/Qwen2.5-VL-32B-Instruct-GGUF) and [`unsloth/Qwen2.5-VL-32B-Instruct-GGUF`](https://huggingface.co/unsloth/Qwen2.5-VL-32B-Instruct-GGUF) versions of the model. It seems to happen mainly on Q8 and (B)F16 quantizations. When the temperatue is default, it happens only sometimes, but when the temperature is set to 0, it happens every time. I think it also happens with different parameter counts.\n\nI previously also tried this with Ollama, where it happens every time when using non-Q4 models or with flash attention enabled (ollama/ollama#11230). However, when using Hugging Face demos of Qwen 2.5 VL or AWQ quantization using vLLM, it doesn't happen, regardless of the temperature, so it seems to be a problem with GGUF versions of the model.\n\nThe image I'm using: https://github.com/user-attachments/assets/d392c3d9-b8fb-4d14-8974-15d843f937bb\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n> ./llama-mtmd-cli -hf unsloth/Qwen2.5-VL-32B-Instruct-GGUF:BF16 -ngl 100 --temp 0.0\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA H100 80GB HBM3, compute capability 9.0, VMM: yes\nload_backend: loaded CUDA backend from /app/libggml-cuda.so\nload_backend: loaded CPU backend from /app/libggml-cpu-icelake.so\ncurl_perform_with_retry: HEAD https://huggingface.co/unsloth/Qwen2.5-VL-32B-Instruct-GGUF/resolve/main/BF16/Qwen2.5-VL-32B-Instruct-BF16-00001-of-00002.gguf (attempt 1 of 1)...\ncommon_download_file_single: using cached file: /d/hpc/home/fs90700/.cache/llama.cpp/unsloth_Qwen2.5-VL-32B-Instruct-GGUF_BF16_Qwen2.5-VL-32B-Instruct-BF16-00001-of-00002.gguf\ncurl_perform_with_retry: HEAD https://huggingface.co/unsloth/Qwen2.5-VL-32B-Instruct-GGUF/resolve/main/BF16/Qwen2.5-VL-32B-Instruct-BF16-00002-of-00002.gguf (attempt 1 of 1)...\ncommon_download_file_single: using cached file: /d/hpc/home/fs90700/.cache/llama.cpp/unsloth_Qwen2.5-VL-32B-Instruct-GGUF_BF16_Qwen2.5-VL-32B-Instruct-BF16-00002-of-00002.gguf\ncurl_perform_with_retry: HEAD https://huggingface.co/unsloth/Qwen2.5-VL-32B-Instruct-GGUF/resolve/main/mmproj-BF16.gguf (attempt 1 of 1)...\ncommon_download_file_single: using cached file: /d/hpc/home/fs90700/.cache/llama.cpp/unsloth_Qwen2.5-VL-32B-Instruct-GGUF_mmproj-BF16.gguf\nbuild: 5884 (c31e6064) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA H100 80GB HBM3) - 80553 MiB free\nllama_model_loader: additional 1 GGUFs metadata loaded.\nllama_model_loader: loaded meta data with 38 key-value pairs and 771 tensors from /d/hpc/home/fs90700/.cache/llama.cpp/unsloth_Qwen2.5-VL-32B-Instruct-GGUF_BF16_Qwen2.5-VL-32B-Instruct-BF16-00001-of-00002.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2vl\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen2.5-Vl-32B-Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Vl-32B-Instruct\nllama_model_loader: - kv   5:                       general.quantized_by str              = Unsloth\nllama_model_loader: - kv   6:                         general.size_label str              = 32B\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\nllama_model_loader: - kv   8:                           general.repo_url str              = https://huggingface.co/unsloth\nllama_model_loader: - kv   9:                   general.base_model.count u32              = 1\nllama_model_loader: - kv  10:                  general.base_model.0.name str              = Qwen2.5 VL 32B Instruct\nllama_model_loader: - kv  11:          general.base_model.0.organization str              = Qwen\nllama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-V...\nllama_model_loader: - kv  13:                               general.tags arr[str,3]       = [\"multimodal\", \"unsloth\", \"image-text...\nllama_model_loader: - kv  14:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  15:                        qwen2vl.block_count u32              = 64\nllama_model_loader: - kv  16:                     qwen2vl.context_length u32              = 128000\nllama_model_loader: - kv  17:                   qwen2vl.embedding_length u32              = 5120\nllama_model_loader: - kv  18:                qwen2vl.feed_forward_length u32              = 27648\nllama_model_loader: - kv  19:               qwen2vl.attention.head_count u32              = 40\nllama_model_loader: - kv  20:            qwen2vl.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  21:                     qwen2vl.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  22:   qwen2vl.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  23:                          general.file_type u32              = 32\nllama_model_loader: - kv  24:            qwen2vl.rope.dimension_sections arr[i32,4]       = [16, 24, 24, 0]\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\nllama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 151654\nllama_model_loader: - kv  33:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% set image_count = namespace(value=...\nllama_model_loader: - kv  35:                                   split.no u16              = 0\nllama_model_loader: - kv  36:                                split.count u16              = 2\nllama_model_loader: - kv  37:                        split.tensors.count i32              = 771\nllama_model_loader: - type  f32:  321 tensors\nllama_model_loader: - type bf16:  450 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = BF16\nprint_info: file size   = 61.03 GiB (16.00 BPW) \nload: special tokens cache size = 22\nload: token to piece cache size = 0.9310 MB\nprint_info: arch             = qwen2vl\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 128000\nprint_info: n_embd           = 5120\nprint_info: n_layer          = 64\nprint_info: n_head           = 40\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 5\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 27648\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 8\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 128000\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 32B\nprint_info: model params     = 32.76 B\nprint_info: general.name     = Qwen2.5-Vl-32B-Instruct\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 152064\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 11 ','\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151654 '<|vision_pad|>'\nprint_info: LF token         = 198 '\u010a'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 64 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 65/65 layers to GPU\nload_tensors:        CUDA0 model buffer size = 61009.27 MiB\nload_tensors:   CPU_Mapped model buffer size =  1485.00 MiB\n.................................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (128000) -- the full capacity of the model will not be utilized\nllama_context:  CUDA_Host  output buffer size =     0.58 MiB\nllama_kv_cache_unified:      CUDA0 KV buffer size =  1024.00 MiB\nllama_kv_cache_unified: size = 1024.00 MiB (  4096 cells,  64 layers,  1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB\nllama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\nllama_context:      CUDA0 compute buffer size =   368.01 MiB\nllama_context:  CUDA_Host compute buffer size =    18.01 MiB\nllama_context: graph nodes  = 2502\nllama_context: graph splits = 2\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmtmd_cli_context: chat template example:\n<|im_start|>system\nYou are a helpful assistant<|im_end|>\n<|im_start|>user\nHello<|im_end|>\n<|im_start|>assistant\nHi there<|im_end|>\n<|im_start|>user\nHow are you?<|im_end|>\n<|im_start|>assistant\n\nclip_model_loader: model name:   Qwen2.5-Vl-32B-Instruct\nclip_model_loader: description:  \nclip_model_loader: GGUF version: 3\nclip_model_loader: alignment:    32\nclip_model_loader: n_tensors:    519\nclip_model_loader: n_kv:         31\n\nclip_model_loader: has vision encoder\nclip_ctx: CLIP using CUDA0 backend\nload_hparams: projector:          qwen2.5vl_merger\nload_hparams: n_embd:             1280\nload_hparams: n_head:             16\nload_hparams: n_ff:               3456\nload_hparams: n_layer:            32\nload_hparams: ffn_op:             silu\nload_hparams: projection_dim:     5120\n\n--- vision hparams ---\nload_hparams: image_size:         1024\nload_hparams: patch_size:         14\nload_hparams: has_llava_proj:     0\nload_hparams: minicpmv_version:   0\nload_hparams: proj_scale_factor:  0\nload_hparams: n_wa_pattern:       8\n\nload_hparams: model size:         1314.85 MiB\nload_hparams: metadata size:      0.18 MiB\nalloc_compute_meta:      CUDA0 compute buffer size =     3.63 MiB\nalloc_compute_meta:        CPU compute buffer size =     0.16 MiB\nmain: loading model: /d/hpc/home/fs90700/.cache/llama.cpp/unsloth_Qwen2.5-VL-32B-Instruct-GGUF_BF16_Qwen2.5-VL-32B-Instruct-BF16-00001-of-00002.gguf\n\n Running in chat mode, available commands:\n   /image <path>    load an image\n   /clear           clear the chat history\n   /quit or /exit   exit the program\n\n> /image /d/hpc/home/fs90700/medieval/document.jpg\n/d/hpc/home/fs90700/medieval/document.jpg image loaded\n\n> Extract the text on the image. Respond only with the extracted text.\nencoding image slice...\nimage slice encoded in 291 ms\ndecoding image batch 1/1, n_tokens_batch = 999\nimage decoded (batch 1/1) in 214 ms\n\n1271, november 17. Falkenberg.\n\nFriderik s Falkenberga proda Nem.vite\u0161kemu redu v Ljubljani in ob Gra\u0111a\u0161ici za 55 mark ogl.\n\nOrig.: MHVK XV (1860), str.97.\nPrim.: F.Richter, Gesch.d.Stadt Lai-bach v Klunovem Archivu f.L.d.H.Krain II.-III, 193; F.Zwitter, Star.kranska mesta,... str.21; J.\u017dontar, Banke in bankirji... str.22, 32, op.21; M.Kos, Srednjeve\u0161ka Ljubljana, str.41, op. 151, 153.\n\nIn nomine Iesu Christi amen. Mora temporis transeunte actus temporis vniuersiter transeunt memoria ab humana, si non scripturum testimonio perhemantur. Quare ego Fridericus de Valchenberch confiteor presencium per tenorem vniuersis presentes uidentibus et uisuris, quod sex propri-- os meos mansos sitos in Awa et circa decursum minoris fluminis dicti Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen-sium pura fide cum omnibus iuribus et attinentiis domus Thevtonicae pro quinquaginta marcis denariorum Aquilwgen^C-s\n\n> ^C\nInterrupted by user\n\n\nllama_perf_context_print:        load time =   16771.74 ms\nllama_perf_context_print: prompt eval time =     606.64 ms /  1023 tokens (    0.59 ms per token,  1686.32 tokens per second)\nllama_perf_context_print:        eval time =   52384.14 ms /  1886 runs   (   27.78 ms per token,    36.00 tokens per second)\nllama_perf_context_print:       total time =  132631.89 ms /  2909 tokens\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-07-13T10:40:16+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14663/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14663"
  },
  {
    "number": 14691,
    "title": "Eval bug: make_cpu_buft_list: no CPU backend found ....  failed to load model",
    "body": "### Name and Version\n\n$ llama-cli --version\nload_backend: loaded RPC backend from /home/rpz/ai/build/bin/libggml-rpc.so\nversion: 5897 (bdca3837)\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCPU\n\n### Hardware\n\nVM with 2 vCPU, 4GB memory & swap 16GB (Ubuntu 22.04)\n\n### Models\n\n1. Qwen/Qwen2.5-Coder-32B-Instruct-GGUF  q4_k_m\n2. Qwen/Qwen2.5-Coder-7B-Instruct-GGUF\n3. mradermacher/DeepSeek-R1-Distill-Qwen-7B-TIR-o3-mini-code-GGUF\n\n### Problem description & steps to reproduce\n\nI'm not sure this is a bug or not, so my selection of the issue template may be wrong. \nMay be this is a problem with my runtime. I'm getting following error when I tried  to run 4bit models:  \n\n$ llama-cli -hf Qwen/Qwen2.5-Coder-32B-Instruct-GGUF:q4_k_m\nor \n$ llama-cli -hf Qwen/Qwen2.5-Coder-7B-Instruct-GGUF\nor\n$ llama-cli -hf mradermacher/DeepSeek-R1-Distill-Qwen-7B-TIR-o3-mini-code-GGUF\n---------\n..\nllama_model_load: error loading model: make_cpu_buft_list: no CPU backend found\nllama_model_load_from_file_impl: failed to load model\n...\n----------\n\nModel file is found in the ~/.cache. Is it because of not enough memory? as I'm using a Ubuntu 22.04 VM with 2 vCPU, 4GB memory & swap 16GB?\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nload_tensors: loading model tensors, this can take a while... (mmap = true)\n**llama_model_load: error loading model: make_cpu_buft_list: no CPU backend found\nllama_model_load_from_file_impl: failed to load model **\ncommon_init_from_params: failed to load model '/home/.../.cache/llama.cpp/bartowski_Qwen2.5-Coder-32B-Instruct-GGUF_Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf'\nmain: error: unable to load model\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-07-15T09:44:09+00:00",
    "closed_at": "2025-07-15T13:38:41+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14691/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14691"
  },
  {
    "number": 14553,
    "title": "Misc. bug: crash on vulkan with new max mem alloc size calculations since b5703",
    "body": "### Name and Version\n\nb5703\n\n### Operating systems\n\n_No response_\n\n### Which llama.cpp modules do you know to be affected?\n\n_No response_\n\n### Command line\n\n```shell\nany vulkan related command\n```\n\n### Problem description & steps to reproduce\n\nsince b5703 new vulkan max mem alloc size calculations leads to crash on some devices like mine gtx 650 ti.\nthe prev interface callback was null and then the size was that max size which is reported by:\nvulkaninfo | findstr \"maxMemoryAllocationSize\"\nif i convert this value to decimal and set on the 2 required env vars GGML_VK_MAX_ALLOCATION_SIZE and GGML_VK_SUBALLOCATION_BLOCK_SIZE it works again so the older NULL callback is the only safe approach otherwise requires per device query. i also have tested manual values myself but except that max value everything leads to crash or other failures.\nalso this old behaviour does not play nice with the new --no-mmap behaviour. before this i could not use mmap on windows os now works.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-07-06T17:41:19+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14553/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14553"
  },
  {
    "number": 14693,
    "title": "Compile bug: llama-llava-clip-quantize-cli not found",
    "body": "### Git commit\n\nbdca38376f7e8dd928defe01ce6a16218a64b040\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Problem description & steps to reproduce\n\ncould not find `llama-llava-clip-quantize-cli` under `build/bin` to quantize mm models\nIt seems the file `examples/llava/clip-quantize-cli.cpp` is removed.\n\n\n### First Bad Commit\n\n_No response_\n\n### Compile command\n\n```shell\ncmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=\"90\" -DLLAMA_CURL=ON\ncmake --build build --config Release -j $(nproc)\n```\n\n### Relevant log output\n\n```shell\nNone\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "open",
    "created_at": "2025-07-15T10:39:58+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14693/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14693"
  }
]
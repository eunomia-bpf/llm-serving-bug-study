[
  {
    "number": 4316,
    "title": "Support for the new 450 language translation models from Google T5X \"madlad\" - apparently Apache-2",
    "body": "Example: https://huggingface.co/jbochi/madlad400-3b-mt/tree/main\r\nIn Googles own space: https://huggingface.co/google/madlad400-10b-mt\r\n\r\nThe guy converted the format of the 3 smallest models (3b,7b,10b) to HF transformers. Given the severe lack in non english output a good translation model would be a gift.\r\nI just tried the CPU demo of the 3B, it produced quite good output, if that gets better with 7B+ it would be a real solution for a huge amount of people.\r\nIt could be added as a 2nd stage into llama.cpp\r\n\r\n**Though the architecture is \"T5ForConditionalGeneration\" which isn't supported.**\r\n\r\nSo far there was no urgent reason to add those T5 models, they did not stick out as special but the idea to output text in every single language worldwide .. that would be remarkable",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-04T00:10:01+00:00",
    "closed_at": "2024-04-20T01:07:21+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4316/reactions",
      "total_count": 9,
      "+1": 9,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4316"
  },
  {
    "number": 4477,
    "title": "I am using llama.cpp via ollama and encountering a problem that was introduced since ollama 0.1.12, v0.1.11 works as expected.",
    "body": "If the version of ollama is above 0.1.11, it fails identically, regardless of the size of the model.  I have 4 GPUs with 12.2GiB each and models of 5GiB size fail just the same.\r\n\r\nAt least two other people encountered the same problems, which have been resolved by downgrading ollama to v0.1.11\r\n\r\nUnfortunately it is a not a solution if one wants to run Mixtral as v0.1.15 (with MIxtral support) has this very same issue.\r\n\r\n\r\nllama_new_context_with_model: n_ctx      = 2048\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\n\u283c llama_kv_cache_init: VRAM kv self = 256.00 MB\r\nllama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\r\nllama_build_graph: non-view tensors processed: 676/676\r\nllama_new_context_with_model: compute buffer total size = 159.32 MiB\r\n\u2834 llama_new_context_with_model: VRAM scratch buffer: 156.00 MiB\r\nllama_new_context_with_model: total VRAM used: 5975.56 MiB (model: 5563.55 MiB, context: 412.00 MiB)\r\n{\"timestamp\":1702581704,\"level\":\"INFO\",\"function\":\"main\",\"line\":3035,\"message\":\"HTTP server listening\",\"hostname\":\"127.0.0.1\",\"port\":54214}\r\n\u2826 {\"timestamp\":1702581704,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2596,\"message\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":50528,\"status\":200,\"method\":\"HEAD\",\"path\":\"/\",\"params\":{}}\r\n2023/12/14 14:21:44 llama.go:508: llama runner started in 4.800707 seconds\r\n[GIN] 2023/12/14 - 14:21:44 | 200 |  5.773840209s |       127.0.0.1 | POST     \"/api/generate\"\r\n>>>\r\n>>>\r\n>>> Hello.\r\n{\"timestamp\":1702581710,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2596,\"message\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":50544,\"status\":200,\"method\":\"HEAD\",\"path\":\"/\",\"params\":{}}\r\n2023/12/14 14:21:50 llama.go:577: loaded 0 images\r\n\r\ncuBLAS error 15 at /home/developer/ollama/llm/llama.cpp/gguf/ggml-cuda.cu:8448\r\ncurrent device: 0\r\nGGML_ASSERT: /home/developer/ollama/llm/llama.cpp/gguf/ggml-cuda.cu:8448: !\"cuBLAS error\"\r\nmemory allocation/deallocation mismatch at 0x56140de8ca20: allocated with malloc being deallocated with delete\r\n\u280f 2023/12/14 14:21:51 llama.go:451: signal: aborted (core dumped)\r\n2023/12/14 14:21:51 llama.go:525: llama runner stopped successfully\r\n[GIN] 2023/12/14 - 14:21:51 | 200 |  938.340539ms |       127.0.0.1 | POST     \"/api/generate\"\r\nError: llama runner exited, you may not have enough available memory to run this model\r\n\r\n```\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2023-12-14T19:33:24+00:00",
    "closed_at": "2023-12-16T16:11:46+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4477/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4477"
  },
  {
    "number": 6244,
    "title": "Finetune gguf model on cpu",
    "body": "hello \r\n\r\nI'm trying to finetune TheBloke/Mistral-7B-OpenOrca-oasst_top1_2023-08-25-v2-GGUF using a  json dataset of 7gb following this template: \" instrunction: ............. input: ............. response:\"\r\n\r\nI have an ovh server with 15 cpu cores and 64 gb of ram\r\n\r\nrunning this command :\r\n`./finetune --model-base models/mistral-7b-openorca-oasst_top1_2023-08-25-v2.Q5_0.gguf --train-data datasets/finetune_data.json  --threads 9 --sample-start \"{\"instruction\":\"`\r\n\r\nthe ram get filled after a while and the console showed killed\r\n\r\nhow much ram is needed to procced the finetuning ? \r\nhow much time estimated to ends up the finetuning? \r\nwhat is the parameter to precise the number of iteration the finetuning will done?\r\n\r\nthanks\r\n\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-22T18:18:35+00:00",
    "closed_at": "2024-05-31T01:06:57+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6244/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6244"
  },
  {
    "number": 1036,
    "title": " I want to use gpt4all, but there is no convert-gpt4all-to-ggml.py file.",
    "body": "I tried to convert the gpt4all binary to the file written in the README, but I couldn't run it because there was no executable file.\r\nMaybe the name of that file has been changed? Or is there a separate location?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-18T04:30:37+00:00",
    "closed_at": "2023-04-23T08:21:28+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1036/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1036"
  },
  {
    "number": 8027,
    "title": "Bug: Inference is messed up in llama-server+default ui and llama-cli but works in llama-server+openweb ui",
    "body": "### What happened?\n\nUsing: https://huggingface.co/bartowski/Hermes-2-Theta-Llama-3-8B-GGUF/blob/main/Hermes-2-Theta-Llama-3-8B-Q6_K.gguf\r\n\r\n**llama-cli**\r\n```bash\r\n./llama-cli -m ~/data/models/Hermes-2-Theta-Llama-3-8B-Q6_K.gguf -ngl 99 -ts 1,1 -t 8 -c 4096 --interactive-first\r\nHello\r\n=====                          \r\n\r\nThis is a small hello world program written in Java.\r\n\r\nCompile                        \r\n=======                        \r\n\r\nTo compile, simply run the following command:\r\n\r\n    javac Hello.java\r\n\r\nRun                            \r\n===                            \r\n\r\nTo run the program, run the following command:\r\n\r\n    java Hello                 \r\n\r\nThis will output:\r\n\r\n    Hello, World! \r\n\r\nYou can also run the program directly from the source code by using the following command:\r\n\r\n    javac Hello.java && java Hello.java\r\n```\r\nthis went on and on\r\n\r\n**llama-server + default ui**\r\n\r\n```bash\r\n./llama-server -m ~/data/models/Hermes-2-Theta-Llama-3-8B-Q6_K.gguf -ngl 99 -ts 1,1 -t 8 -c 4096 --host 0.0.0.0 --port 8081\r\n```\r\n![image](https://github.com/ggerganov/llama.cpp/assets/23178601/74820adb-14b9-4ade-8a49-046cdfad6327)\r\n\r\n**llama-server + openwebui**\r\nfrom the same server instance\r\n![image](https://github.com/ggerganov/llama.cpp/assets/23178601/53a7a54c-f22f-4296-98f0-ec2c98220bbf)\r\n\n\n### Name and Version\n\nversion: 3186 (ba589931)                                    \r\nbuilt with cc (Debian 12.2.0-14) 12.2.0 for x86_64-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-06-20T06:25:45+00:00",
    "closed_at": "2024-06-25T10:23:52+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8027/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8027"
  },
  {
    "number": 4778,
    "title": "Token healing (under 40 LOC)",
    "body": "# Feature Description\r\n\r\nToken healing rectifies the token boundary bias in greedy tokenization. It does this by trimming and regrowing the prompt to better align with the model's tokenizer, thus enhancing generation quality. The improvement is clearest with completion models.\r\n\r\nDebiasing token boundaries also addresses output sensitivity to prompts with trailing whitespace.\r\n\r\nToken boundary bias is a silent performance killer that doesn't seem very well known. It has clear impact on completion quality, though I'm not sure where it would fit as a llama.cpp feature.\r\n\r\nA more thorough explanation of the problem: [The Art of Prompt Design: Prompt Boundaries and Token Healing | by Scott Lundberg](https://towardsdatascience.com/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38)\r\n\r\n# Motivation\r\n\r\nGiven a completion prompt with a partial url ending with `:`, the model might have seen the expected completion `://` as a _single_ token in training. However, the prompt's tail token `:` tells it that the next token is not `//`, and so it looks for wrong completions. Such errors compound in auto-regressive language models.\r\n\r\n# Possible Implementation\r\n\r\nMy implementation (under 40 LOC): https://github.com/Ayenem/TokenHealer/tree/main\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-01-04T20:14:53+00:00",
    "closed_at": "2024-04-02T01:08:47+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4778/reactions",
      "total_count": 5,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4778"
  },
  {
    "number": 2582,
    "title": "Inconsistent Embedding Results on Different OS Platforms",
    "body": "Summary:\r\nThe embedding function in the LLM library is producing inconsistent results for the same token when executed on different operating system (OS) platforms. Specifically, the embeddings generated for the token \"cat\" differ between Ubuntu 22 and Windows Server 2022.\r\n\r\nWe have identified the bug mentioned in LLamaSharp [PR #97](https://github.com/SciSharp/LLamaSharp/pull/97)\r\n\r\nSteps to Reproduce:\r\n\r\n    On Ubuntu-22:\r\n        Command: ./embedding -m ~/llama-2-7b-chat.ggmlv3.q3_K_S.bin -p cat\r\n        Resulting Embedding: -0.099176 -0.717907 -0.008532 -0.989839 -0.663397\r\n\r\n    On Windows Server 2022:\r\n        Command: embedding.exe -m llama-2-7b-chat.ggmlv3.q3_K_S.bin -p cat\r\n        Resulting Embedding: -0.127304 -0.678057 -0.085244 -0.956915 -0.638633\r\n\r\nExpected Result:\r\nThe embedding function should produce consistent embeddings for the same token across different OS platforms, given the same model and input parameters.\r\n\r\nActual Result:\r\nThe embeddings generated for the token \"cat\" vary between Ubuntu 22 and Windows Server 2022, indicating an inconsistency in the embedding function's behavior across different OS platforms.",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-08-11T01:17:45+00:00",
    "closed_at": "2024-04-10T01:06:38+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2582/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2582"
  },
  {
    "number": 4867,
    "title": "iOS library is broken because ggml dependency is not pinned ",
    "body": "I was debugging some accidental crashes in SwiftUI sample, dug deeper and understood a couple of things.\r\n\r\n### Some background\r\n\r\n1. When llama.cpp is built outside of Xcode, e.g. with make, there is no direct dependency on [ggml repo](https://github.com/ggerganov/ggml). ggml sources are just duplicated in this repo, they get out of sync but there are [scripts](https://github.com/ggerganov/ggml/tree/5a3154b59242d17b2225872a2538d341f4f28c54/scripts) used to sync two repos + whisper.cpp, which also use ggml.\r\n\r\n2. Xcode build is different (not only the sample, but all external applications that use llama.cpp through SPM). In this case ggml will be fetched from [ggml repo](https://github.com/ggerganov/ggml), ignoring files present in this repo. Additionally, dependency on ggml repo [is not pinned](https://github.com/1-ashraful-islam/llama.cpp/blob/5f12e26899f50f177d2133e52d4b883c1189333f/Package.swift#L17), which technically means that ggml can be resolved to *any* commit.\r\n\r\n### Why it's done like this and why it doesn't work\r\n\r\nThis difference in build logic between Xcode and make was introduced only recently in [this PR](https://github.com/ggerganov/llama.cpp/pull/4691/files). \r\nI completely see [the reasoning](https://github.com/ggerganov/llama.cpp/pull/4691#issue-2060383529) behind this change, indeed whisper.cpp already has it's own copy of ggml, so we got a conflict of sybmols in the resulting binary if we used both llama.cpp and whisper.cpp in one iOS app.\r\n\r\nHowever the current approach just doesn't work for llama.cpp at all. For example right now latest `master` of llama.cpp (cd108e6) is not compatible with the latest master of `ggml` (c75db1e), it leads to a crash [on this line](https://github.com/ggerganov/llama.cpp/blob/cd108e641dbdedd8c5641c4cec1762f751f38136/ggml.c#L4772).\r\n\r\n### Proposed solution\r\n\r\nI really don't want to force Grigori to rethink how he structures his projects, in this case a monorepo might solve the problem, but I guess iOS build just has to adapt to the existing structure.\r\nThe best solution I see so far:\r\n\r\n-  We create a new target in [the SPM manifest](https://github.com/ggerganov/llama.cpp/blob/4aa73e37b01a8bdb36c9ff38e7e1ba20c456afb9/Package.swift#L20C10-L20C10). It will build llama.cpp exactly how `make` does it, using local ggml sources and without dependency on external repo. This new target becomes the recommended way to integrate llama.cpp to any iOS app, because it's guaranteed to always build the same sources as other build systems. (this step is basically a revert of #4691)\r\n- For rare cases when an app already has ggml, like with `whisper.cpp`, we create a second target that uses ggml from the external repo and the developer of the app will be responsible for pinning a compatible version, ensuring two commits of llama and ggml work together. (This target is exactly the target we have right now)\r\n- Finally we add a CI job that builds the sample, so this won't happen again. \ud83d\ude04\r\n\r\nI'd love to hear opinions on this possible solution from folks who previously contributed into SPM manifest: @ggerganov @1-ashraful-islam @kchro3 @jhen0409.\r\n\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-01-11T02:26:18+00:00",
    "closed_at": "2024-01-11T19:31:33+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4867/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4867"
  },
  {
    "number": 7110,
    "title": "Binary starting with b2715 doesn't work on Intel Mac anymore",
    "body": "I have a 2018 MacBook Pro. I'm able to run the b2714 binary without issues. Starting with b2715 (llama-b2715-bin-macos-x64.zip) I'm getting this message: \"zsh: bad CPU type in executable: ./main\".\r\n\r\n<img width=\"594\" alt=\"Screenshot 2024-05-06 at 9 02 00\u202fPM\" src=\"https://github.com/ggerganov/llama.cpp/assets/161262078/ef4c7c4b-45b1-4f0f-9ba8-7a8867f8e603\">\r\n\r\n<img width=\"594\" alt=\"Screenshot 2024-05-06 at 9 03 12\u202fPM\" src=\"https://github.com/ggerganov/llama.cpp/assets/161262078/082ccd46-81e8-4cfb-ba4c-954d463ef0aa\">\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-07T01:08:26+00:00",
    "closed_at": "2024-07-12T01:17:52+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7110/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7110"
  },
  {
    "number": 7089,
    "title": "Malformed `system_prompt` in `/completions` request crashes server",
    "body": "## Observed behavior\r\n\r\nIf I send a string for the system prompt instead of [the expected json object](https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md#change-system-prompt-on-runtime), the server terminates. \r\n\r\n## Desired behavior\r\n\r\nThe server responds with an HTTP 400 error and doesn't terminate.\r\n\r\n## Environment\r\n\r\nRunning the server via docker:\r\n\r\n`docker run -v /path/to/models:/models -p 8000:8000 ghcr.io/ggerganov/llama.cpp@sha256:b4675af8c9a8b3e7019a7baf536b95c3984a9aaacd0eafce7422377a299e31f4 -m /models/Meta-Llama-3-8B.Q4_K_M.gguf --port 8000 --host 0.0.0.0 -n 512`\r\n\r\nUsing [this gguf quant](https://huggingface.co/QuantFactory/Meta-Llama-3-8B-GGUF) (though I don't think it matters -- it is crashing with different ggufs I have tried).\r\n\r\n\r\n## Request\r\n\r\n```bash\r\njson_body=$(cat <<EOF\r\n{\r\n  \"system_prompt\": \"Always reply in markdown lists.\",\r\n  \"prompt\": \"Building a website can be done in 10 simple steps:\",\r\n  \"n_predict\": 128\r\n}\r\nEOF\r\n)\r\n\r\ncurl --request POST \\\r\n    --url http://localhost:8000/completion \\\r\n    --header \"Content-Type: application/json\" \\\r\n    --data \"$json_body\"\r\n```\r\n\r\n## Server error logs\r\n\r\n```\r\nphantom-llama-1  | terminate called after throwing an instance of 'nlohmann::json_abi_v3_11_3::detail::type_error'\r\nphantom-llama-1  |   what():  [json.exception.type_error.306] cannot use value() with string\r\n```",
    "labels": [
      "bug",
      "server/webui"
    ],
    "state": "closed",
    "created_at": "2024-05-05T14:58:23+00:00",
    "closed_at": "2025-02-04T19:52:35+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7089/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7089"
  },
  {
    "number": 5026,
    "title": "Allow oversubscription of GPU memory through cudaMallocManaged on cuBLAS builds for systems like GH200",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\n\r\nI have access to a GH200 and it has the capability share memory between the GPU and the CPU. The GPU has around 98GB and the system has 500GB+.\r\n\r\nI tried to run a large model (147G) and it OOMs. After chatting with nvidia the issue is:\r\n\r\n```\r\nThe cudaMalloc API call does not allow oversubscription and fails once vidmem capacity is exceeded. So the failures being seen are expected behavior. The API that can oversubscribe GPU memory is cudaMallocManaged, which was not being used in the test app.\r\n```\r\n\r\n# Motivation\r\n\r\nAllow GPU oversubscription via `cudaMallocManaged`\r\n\r\n# Possible Implementation\r\n\r\nI believe these may be replacing `cudaMalloc` with `cudaMallocManaged` on cuBLAS (not 100% sure on this).\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-01-18T21:57:55+00:00",
    "closed_at": "2024-04-22T01:43:08+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5026/reactions",
      "total_count": 2,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5026"
  },
  {
    "number": 4526,
    "title": "Batch processing should use a currently-missing batch dimension for all tensors",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\n\r\nWhile doing experiments with batched processing I noticed that performance of batching two sequences with length N was exactly equal to the performance of running a single batch inference of length 2N. This seemed peculiar to me, as I understood most implementations to use a separate dimension for batching, so performance between the two scenarios should have been different in some way. Looking through the code, I fail to see the batch dimension in any of the tensors constructed in the various `build_*()` functions. From what I can tell, batches are done via extending the tokens dimension to match the combined length of all batches, while other implementations extend the token dimension to match the longest batch and padding all other batches.\r\n\r\n[Source for PyTorch](https://github.com/pytorch/pytorch/blob/77366ba637fd019fef0f15725676568824f6b944/torch/nn/modules/transformer.py#L167-L199)\r\n\r\n# Motivation\r\n\r\nIn https://github.com/ggerganov/llama.cpp/pull/3624#issuecomment-1764197941 performance with tree-based speculative decoding did not improve over single sequence drafting. This is consistent with my own tests, but inconsistent with the results of the original paper and for other implementations. The lack of a batch dimension makes using multiple drafted sequences equivalent performance-wise to a single, longer sequence. Using a batch dimension would transform some operations from matrix-vector multiplications to matrix-matrix multiplications, mapping better to hardware. For example: the input tokens tensor that is currently a vector of n_tokens length would become a matrix of size n_batches x max_batch_length. Accordingly, the embeddings tensor would become a 3-dimensional tensor of size n_batches x n_embed x max_batch_length. Batch computations should be entirely independent, so for these 3-d tensor multiplications, some optimizations can be made to better map to hardware.\r\n\r\n# Possible Implementation\r\n\r\nI attempted adding the batch dimension myself but it's been slow-going due to limited understanding of the GGML internals. The biggest concern is determining where the batch dimension should be, as putting it last would likely cause severe memory fragmentation, while putting it first causes incompatibilities with the current operations that assume a certain shape.\r\n\r\nIt's important to note that adding a batch dimension *shouldn't* impact performance of single-batch inference, and prompt pre-filling should similarly be unaffected since that's not a true \"batched\" operation in the context of the batch dimension.\r\n\r\nObviously though, adding an entire dimension to all operations would incur a hefty development cost, so weighing whether the current approach is \"good enough\" should be carefully considered.\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-18T19:20:28+00:00",
    "closed_at": "2024-04-02T01:10:34+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4526/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4526"
  },
  {
    "number": 6593,
    "title": "Mixtral with `--split-mode row` crashes with `GGML_ASSERT: ggml-cuda.cu:727: tensor->view_src == nullptr`",
    "body": "How to reproduce:\r\n```\r\n./main --model ./mixtral:8x7b-instruct-v0.1-q8_0.gguf --split-mode row --n-gpu-layers 1000\r\n```\r\nAny value of `--n-gpu-layers` > 0 also crashes.\r\n\r\nSame GGUF works fine with `--split-mode layer`.\r\n\r\n```\r\nLog start\r\nmain: build = 2644 (65c64dc3)\r\nmain: built with cc (Debian 12.2.0-14) 12.2.0 for x86_64-linux-gnu\r\nmain: seed  = 1712775266\r\nllama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from ./mixtral:8x7b-instruct-v0.1-q8_0.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   9:                         llama.expert_count u32              = 8\r\nllama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\r\nllama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  13:                          general.file_type u32              = 7\r\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\r\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type  f16:   32 tensors\r\nllama_model_loader: - type q8_0:  898 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 8\r\nllm_load_print_meta: n_expert_used    = 2\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q8_0\r\nllm_load_print_meta: model params     = 46.70 B\r\nllm_load_print_meta: model size       = 46.22 GiB (8.50 BPW) \r\nllm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\r\nggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\r\nggml_cuda_init: found 2 CUDA devices:\r\n  Device 0: NVIDIA RTX A6000, compute capability 8.6, VMM: yes\r\n  Device 1: NVIDIA RTX A6000, compute capability 8.6, VMM: yes\r\nllm_load_tensors: ggml ctx size =    1.43 MiB\r\nGGML_ASSERT: lama.cpp/ggml-cuda.cu:727: tensor->view_src == nullptr\r\n```\r\nIIRC, this is just the standard \"TheBloke\" version off Huggingface.\r\n\r\nCould this be due to some changes in the MOE tensors to get the DBRX model working?",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-10T19:04:40+00:00",
    "closed_at": "2024-05-31T01:06:52+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6593/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6593"
  },
  {
    "number": 6406,
    "title": "use vulkan on jetson Jetson Xavier NX could not convert error",
    "body": "Please include information about your system, the steps to reproduce the bug, and the version of llama.cpp that you are using. If possible, please provide a minimal code example that reproduces the bug.\r\n\r\nsystem :Linux  5.10.120-tegra #1 SMP PREEMPT Tue Aug 1 12:32:50 PDT 2023 aarch64 aarch64 aarch64 GNU/Linux\r\njetpack version:5.1.2\r\n\r\nerror:\r\n[  1%] Generating build details from Git\r\n[  2%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o\r\n[  3%] Building CXX object common/CMakeFiles/json-schema-to-grammar.dir/json-schema-to-grammar.cpp.o\r\n[  3%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o\r\n[  4%] Building C object CMakeFiles/ggml.dir/ggml.c.o\r\n[  5%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o\r\n-- Found Git: /usr/bin/git (found version \"2.25.1\") \r\n[  5%] Building CXX object CMakeFiles/ggml.dir/ggml-vulkan.cpp.o\r\n[  6%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\r\n[  6%] Built target build_info\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp: In function \u2018void ggml_vk_create_pipeline(ggml_backend_vk_context*, vk_pipeline&, const string&, size_t, const void*, const string&, uint32_t, uint32_t, std::array<unsigned int, 3>, std::vector<unsigned int>&&, uint32_t)\u2019:\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:431:81: error: could not convert \u2018{dsl_binding_flags}\u2019 from \u2018<brace-enclosed initializer list>\u2019 to \u2018vk::DescriptorSetLayoutBindingFlagsCreateInfo\u2019\r\n  431 |     vk::DescriptorSetLayoutBindingFlagsCreateInfo dslbfci = { dsl_binding_flags };\r\n      |                                                                                 ^\r\n      |                                                                                 |\r\n      |                                                                                 <brace-enclosed initializer list>\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:441:20: error: no matching function for call to \u2018vk::DescriptorSetLayoutCreateInfo::DescriptorSetLayoutCreateInfo(<brace-enclosed initializer list>, std::vector<vk::DescriptorSetLayoutBinding>&)\u2019\r\n  441 |         dsl_binding);\r\n      |                    ^\r\nIn file included from /home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:7:\r\n/usr/include/vulkan/vulkan.hpp:27958:5: note: candidate: \u2018vk::DescriptorSetLayoutCreateInfo::DescriptorSetLayoutCreateInfo(const VkDescriptorSetLayoutCreateInfo&)\u2019\r\n27958 |     DescriptorSetLayoutCreateInfo( VkDescriptorSetLayoutCreateInfo const & rhs ) VULKAN_HPP_NOEXCEPT\r\n      |     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:27958:5: note:   candidate expects 1 argument, 2 provided\r\n/usr/include/vulkan/vulkan.hpp:27944:26: note: candidate: \u2018constexpr vk::DescriptorSetLayoutCreateInfo::DescriptorSetLayoutCreateInfo(vk::DescriptorSetLayoutCreateFlags, uint32_t, const vk::DescriptorSetLayoutBinding*)\u2019\r\n27944 |     VULKAN_HPP_CONSTEXPR DescriptorSetLayoutCreateInfo( VULKAN_HPP_NAMESPACE::DescriptorSetLayoutCreateFlags flags_ = {},\r\n      |                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:27945:66: note:   no known conversion for argument 2 from \u2018std::vector<vk::DescriptorSetLayoutBinding>\u2019 to \u2018uint32_t\u2019 {aka \u2018unsigned int\u2019}\r\n27945 |                                                         uint32_t bindingCount_ = {},\r\n      |                                                         ~~~~~~~~~^~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:27942:10: note: candidate: \u2018constexpr vk::DescriptorSetLayoutCreateInfo::DescriptorSetLayoutCreateInfo(const vk::DescriptorSetLayoutCreateInfo&)\u2019\r\n27942 |   struct DescriptorSetLayoutCreateInfo\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:27942:10: note:   candidate expects 1 argument, 2 provided\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:452:103: error: no matching function for call to \u2018vk::DescriptorPoolCreateInfo::DescriptorPoolCreateInfo(<brace-enclosed initializer list>, const uint32_t&, vk::DescriptorPoolSize&)\u2019\r\n  452 |         vk::DescriptorPoolCreateInfo descriptor_pool_create_info({}, alloc_count, descriptor_pool_size);\r\n      |                                                                                                       ^\r\nIn file included from /home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:7:\r\n/usr/include/vulkan/vulkan.hpp:27551:5: note: candidate: \u2018vk::DescriptorPoolCreateInfo::DescriptorPoolCreateInfo(const VkDescriptorPoolCreateInfo&)\u2019\r\n27551 |     DescriptorPoolCreateInfo( VkDescriptorPoolCreateInfo const & rhs ) VULKAN_HPP_NOEXCEPT\r\n      |     ^~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:27551:5: note:   candidate expects 1 argument, 3 provided\r\n/usr/include/vulkan/vulkan.hpp:27535:26: note: candidate: \u2018constexpr vk::DescriptorPoolCreateInfo::DescriptorPoolCreateInfo(vk::DescriptorPoolCreateFlags, uint32_t, uint32_t, const vk::DescriptorPoolSize*)\u2019\r\n27535 |     VULKAN_HPP_CONSTEXPR DescriptorPoolCreateInfo( VULKAN_HPP_NAMESPACE::DescriptorPoolCreateFlags flags_ = {},\r\n      |                          ^~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:27537:61: note:   no known conversion for argument 3 from \u2018vk::DescriptorPoolSize\u2019 to \u2018uint32_t\u2019 {aka \u2018unsigned int\u2019}\r\n27537 |                                                    uint32_t poolSizeCount_ = {},\r\n      |                                                    ~~~~~~~~~^~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:27533:10: note: candidate: \u2018constexpr vk::DescriptorPoolCreateInfo::DescriptorPoolCreateInfo(const vk::DescriptorPoolCreateInfo&)\u2019\r\n27533 |   struct DescriptorPoolCreateInfo\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:27533:10: note:   candidate expects 1 argument, 3 provided\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:471:95: error: no matching function for call to \u2018vk::DescriptorPoolCreateInfo::DescriptorPoolCreateInfo(<brace-enclosed initializer list>, int, vk::DescriptorPoolSize&)\u2019\r\n  471 |         vk::DescriptorPoolCreateInfo descriptor_pool_create_info({}, 128, descriptor_pool_size);\r\n      |                                                                                               ^\r\nIn file included from /home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:7:\r\n/usr/include/vulkan/vulkan.hpp:27551:5: note: candidate: \u2018vk::DescriptorPoolCreateInfo::DescriptorPoolCreateInfo(const VkDescriptorPoolCreateInfo&)\u2019\r\n27551 |     DescriptorPoolCreateInfo( VkDescriptorPoolCreateInfo const & rhs ) VULKAN_HPP_NOEXCEPT\r\n      |     ^~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:27551:5: note:   candidate expects 1 argument, 3 provided\r\n/usr/include/vulkan/vulkan.hpp:27535:26: note: candidate: \u2018constexpr vk::DescriptorPoolCreateInfo::DescriptorPoolCreateInfo(vk::DescriptorPoolCreateFlags, uint32_t, uint32_t, const vk::DescriptorPoolSize*)\u2019\r\n27535 |     VULKAN_HPP_CONSTEXPR DescriptorPoolCreateInfo( VULKAN_HPP_NAMESPACE::DescriptorPoolCreateFlags flags_ = {},\r\n      |                          ^~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:27537:61: note:   no known conversion for argument 3 from \u2018vk::DescriptorPoolSize\u2019 to \u2018uint32_t\u2019 {aka \u2018unsigned int\u2019}\r\n27537 |                                                    uint32_t poolSizeCount_ = {},\r\n      |                                                    ~~~~~~~~~^~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:27533:10: note: candidate: \u2018constexpr vk::DescriptorPoolCreateInfo::DescriptorPoolCreateInfo(const vk::DescriptorPoolCreateInfo&)\u2019\r\n27533 |   struct DescriptorPoolCreateInfo\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:27533:10: note:   candidate expects 1 argument, 3 provided\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:477:113: error: no matching function for call to \u2018vk::PipelineLayoutCreateInfo::PipelineLayoutCreateInfo(vk::PipelineLayoutCreateFlags, vk::DescriptorSetLayout&, vk::PushConstantRange&)\u2019\r\n  477 |     vk::PipelineLayoutCreateInfo pipeline_layout_create_info(vk::PipelineLayoutCreateFlags(), pipeline->dsl, pcr);\r\n      |                                                                                                                 ^\r\nIn file included from /home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:7:\r\n/usr/include/vulkan/vulkan.hpp:52719:5: note: candidate: \u2018vk::PipelineLayoutCreateInfo::PipelineLayoutCreateInfo(const VkPipelineLayoutCreateInfo&)\u2019\r\n52719 |     PipelineLayoutCreateInfo( VkPipelineLayoutCreateInfo const & rhs ) VULKAN_HPP_NOEXCEPT\r\n      |     ^~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:52719:5: note:   candidate expects 1 argument, 3 provided\r\n/usr/include/vulkan/vulkan.hpp:52701:26: note: candidate: \u2018constexpr vk::PipelineLayoutCreateInfo::PipelineLayoutCreateInfo(vk::PipelineLayoutCreateFlags, uint32_t, const vk::DescriptorSetLayout*, uint32_t, const vk::PushConstantRange*)\u2019\r\n52701 |     VULKAN_HPP_CONSTEXPR PipelineLayoutCreateInfo( VULKAN_HPP_NAMESPACE::PipelineLayoutCreateFlags flags_ = {},\r\n      |                          ^~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:52703:101: note:   no known conversion for argument 3 from \u2018vk::PushConstantRange\u2019 to \u2018const vk::DescriptorSetLayout*\u2019\r\n52703 |                                                    const VULKAN_HPP_NAMESPACE::DescriptorSetLayout* pSetLayouts_ = {},\r\n      |                                                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:52699:10: note: candidate: \u2018constexpr vk::PipelineLayoutCreateInfo::PipelineLayoutCreateInfo(const vk::PipelineLayoutCreateInfo&)\u2019\r\n52699 |   struct PipelineLayoutCreateInfo\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:52699:10: note:   candidate expects 1 argument, 3 provided\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:505:112: error: conversion from \u2018int\u2019 to \u2018vk::PipelineCache\u2019 is ambiguous\r\n  505 |     pipeline->pipeline = ctx->device->device.createComputePipeline(VK_NULL_HANDLE, compute_pipeline_create_info).value;\r\n      |                                                                                                                ^\r\nIn file included from /home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:7:\r\n/usr/include/vulkan/vulkan.hpp:15503:34: note: candidate: \u2018vk::PipelineCache::PipelineCache(VkPipelineCache)\u2019\r\n15503 |     VULKAN_HPP_TYPESAFE_EXPLICIT PipelineCache( VkPipelineCache pipelineCache ) VULKAN_HPP_NOEXCEPT\r\n      |                                  ^~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:15499:26: note: candidate: \u2018constexpr vk::PipelineCache::PipelineCache(std::nullptr_t)\u2019\r\n15499 |     VULKAN_HPP_CONSTEXPR PipelineCache( std::nullptr_t ) VULKAN_HPP_NOEXCEPT\r\n      |                          ^~~~~~~~~~~~~\r\nIn file included from /home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:7:\r\n/usr/include/vulkan/vulkan.hpp:63197:129: note:   initializing argument 1 of \u2018vk::ResultValueType<vk::Pipeline>::type vk::Device::createComputePipeline(vk::PipelineCache, const vk::ComputePipelineCreateInfo&, vk::Optional<const vk::AllocationCallbacks>, const Dispatch&) const [with Dispatch = vk::DispatchLoaderStatic; vk::ResultValueType<vk::Pipeline>::type = vk::Pipeline]\u2019\r\n63197 |   VULKAN_HPP_INLINE typename ResultValueType<Pipeline>::type Device::createComputePipeline( VULKAN_HPP_NAMESPACE::PipelineCache pipelineCache, const ComputePipelineCreateInfo & createInfo, Optional<const AllocationCallbacks> allocator, Dispatch const &d ) const\r\n      |                                                                                                                                 ^\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp: In function \u2018void ggml_pipeline_allocate_descriptor_sets(ggml_backend_vk_context*, vk_pipeline&, uint32_t)\u2019:\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:552:97: error: no matching function for call to \u2018vk::DescriptorPoolCreateInfo::DescriptorPoolCreateInfo(<brace-enclosed initializer list>, int, vk::DescriptorPoolSize&)\u2019\r\n  552 |             vk::DescriptorPoolCreateInfo descriptor_pool_create_info({}, 1, descriptor_pool_size);\r\n      |                                                                                                 ^\r\nIn file included from /home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:7:\r\n/usr/include/vulkan/vulkan.hpp:27551:5: note: candidate: \u2018vk::DescriptorPoolCreateInfo::DescriptorPoolCreateInfo(const VkDescriptorPoolCreateInfo&)\u2019\r\n27551 |     DescriptorPoolCreateInfo( VkDescriptorPoolCreateInfo const & rhs ) VULKAN_HPP_NOEXCEPT\r\n      |     ^~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:27551:5: note:   candidate expects 1 argument, 3 provided\r\n/usr/include/vulkan/vulkan.hpp:27535:26: note: candidate: \u2018constexpr vk::DescriptorPoolCreateInfo::DescriptorPoolCreateInfo(vk::DescriptorPoolCreateFlags, uint32_t, uint32_t, const vk::DescriptorPoolSize*)\u2019\r\n27535 |     VULKAN_HPP_CONSTEXPR DescriptorPoolCreateInfo( VULKAN_HPP_NAMESPACE::DescriptorPoolCreateFlags flags_ = {},\r\n      |                          ^~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:27537:61: note:   no known conversion for argument 3 from \u2018vk::DescriptorPoolSize\u2019 to \u2018uint32_t\u2019 {aka \u2018unsigned int\u2019}\r\n27537 |                                                    uint32_t poolSizeCount_ = {},\r\n      |                                                    ~~~~~~~~~^~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:27533:10: note: candidate: \u2018constexpr vk::DescriptorPoolCreateInfo::DescriptorPoolCreateInfo(const vk::DescriptorPoolCreateInfo&)\u2019\r\n27533 |   struct DescriptorPoolCreateInfo\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:27533:10: note:   candidate expects 1 argument, 3 provided\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp: In function \u2018void ggml_vk_submit(vk_context*, vk::Fence)\u2019:\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:657:40: error: assignment of read-only member \u2018vk::TimelineSemaphoreSubmitInfo::sType\u2019\r\n  657 |             tl_submit_infos[idx].sType = vk::StructureType::eTimelineSemaphoreSubmitInfo;\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp: In function \u2018void ggml_vk_queue_cleanup(ggml_backend_vk_context*, vk_queue&)\u2019:\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:793:48: error: no matching function for call to \u2018vk::Device::resetCommandPool(vk::CommandPool&)\u2019\r\n  793 |     ctx->device->device.resetCommandPool(q.pool);\r\n      |                                                ^\r\nIn file included from /home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:7:\r\n/usr/include/vulkan/vulkan.hpp:18160:42: note: candidate: \u2018template<class Dispatch> vk::ResultValueType<void>::type vk::Device::resetCommandPool(vk::CommandPool, vk::CommandPoolResetFlags, const Dispatch&) const\u2019\r\n18160 |     typename ResultValueType<void>::type resetCommandPool( VULKAN_HPP_NAMESPACE::CommandPool commandPool, VULKAN_HPP_NAMESPACE::CommandPoolResetFlags flags, Dispatch const &d = VULKAN_HPP_DEFAULT_DISPATCHER ) const;\r\n      |                                          ^~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:18160:42: note:   template argument deduction/substitution failed:\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:793:48: note:   candidate expects 3 arguments, 1 provided\r\n  793 |     ctx->device->device.resetCommandPool(q.pool);\r\n      |                                                ^\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp: In function \u2018void ggml_vk_print_gpu_info(size_t)\u2019:\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:1336:60: error: request for member \u2018data\u2019 in \u2018props2.vk::PhysicalDeviceProperties2::properties.vk::PhysicalDeviceProperties::deviceName\u2019, which is of non-class type \u2018char [256]\u2019\r\n 1336 |     std::string device_name = props2.properties.deviceName.data();\r\n      |                                                            ^~~~\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp: In function \u2018void ggml_vk_instance_init()\u2019:\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:1377:105: error: no matching function for call to \u2018vk::InstanceCreateInfo::InstanceCreateInfo(vk::InstanceCreateFlags, vk::ApplicationInfo*, std::vector<const char*>&, std::vector<const char*>&)\u2019\r\n 1377 |     vk::InstanceCreateInfo instance_create_info(vk::InstanceCreateFlags{}, &app_info, layers, extensions);\r\n      |                                                                                                         ^\r\nIn file included from /home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:7:\r\n/usr/include/vulkan/vulkan.hpp:38981:5: note: candidate: \u2018vk::InstanceCreateInfo::InstanceCreateInfo(const VkInstanceCreateInfo&)\u2019\r\n38981 |     InstanceCreateInfo( VkInstanceCreateInfo const & rhs ) VULKAN_HPP_NOEXCEPT\r\n      |     ^~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:38981:5: note:   candidate expects 1 argument, 4 provided\r\n/usr/include/vulkan/vulkan.hpp:38961:26: note: candidate: \u2018constexpr vk::InstanceCreateInfo::InstanceCreateInfo(vk::InstanceCreateFlags, const vk::ApplicationInfo*, uint32_t, const char* const*, uint32_t, const char* const*)\u2019\r\n38961 |     VULKAN_HPP_CONSTEXPR InstanceCreateInfo( VULKAN_HPP_NAMESPACE::InstanceCreateFlags flags_ = {},\r\n      |                          ^~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:38963:55: note:   no known conversion for argument 3 from \u2018std::vector<const char*>\u2019 to \u2018uint32_t\u2019 {aka \u2018unsigned int\u2019}\r\n38963 |                                              uint32_t enabledLayerCount_ = {},\r\n      |                                              ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:38959:10: note: candidate: \u2018constexpr vk::InstanceCreateInfo::InstanceCreateInfo(const vk::InstanceCreateInfo&)\u2019\r\n38959 |   struct InstanceCreateInfo\r\n      |          ^~~~~~~~~~~~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:38959:10: note:   candidate expects 1 argument, 4 provided\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:1392:9: error: no match for \u2018operator=\u2019 (operand types are \u2018vk::ValidationFeaturesEXT\u2019 and \u2018<brace-enclosed initializer list>\u2019)\r\n 1392 |         };\r\n      |         ^\r\nIn file included from /home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:7:\r\n/usr/include/vulkan/vulkan.hpp:60037:51: note: candidate: \u2018vk::ValidationFeaturesEXT& vk::ValidationFeaturesEXT::operator=(const vk::ValidationFeaturesEXT&)\u2019\r\n60037 |     VULKAN_HPP_NAMESPACE::ValidationFeaturesEXT & operator=( VULKAN_HPP_NAMESPACE::ValidationFeaturesEXT const & rhs ) VULKAN_HPP_NOEXCEPT\r\n      |                                                   ^~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:60037:114: note:   no known conversion for argument 1 from \u2018<brace-enclosed initializer list>\u2019 to \u2018const vk::ValidationFeaturesEXT&\u2019\r\n60037 |     VULKAN_HPP_NAMESPACE::ValidationFeaturesEXT & operator=( VULKAN_HPP_NAMESPACE::ValidationFeaturesEXT const & rhs ) VULKAN_HPP_NOEXCEPT\r\n      |                                                                                                                  ^\r\n/usr/include/vulkan/vulkan.hpp:60048:28: note: candidate: \u2018vk::ValidationFeaturesEXT& vk::ValidationFeaturesEXT::operator=(const VkValidationFeaturesEXT&)\u2019\r\n60048 |     ValidationFeaturesEXT& operator=( VkValidationFeaturesEXT const & rhs ) VULKAN_HPP_NOEXCEPT\r\n      |                            ^~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:60048:71: note:   no known conversion for argument 1 from \u2018<brace-enclosed initializer list>\u2019 to \u2018const VkValidationFeaturesEXT&\u2019\r\n60048 |     ValidationFeaturesEXT& operator=( VkValidationFeaturesEXT const & rhs ) VULKAN_HPP_NOEXCEPT\r\n      |                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp: In function \u2018void ggml_vk_init(ggml_backend_vk_context*, size_t)\u2019:\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:1483:13: error: \u2018PhysicalDeviceMaintenance4Properties\u2019 is not a member of \u2018vk\u2019; did you mean \u2018PhysicalDeviceMaintenance3Properties\u2019?\r\n 1483 |         vk::PhysicalDeviceMaintenance4Properties props4;\r\n      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |             PhysicalDeviceMaintenance3Properties\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:1488:37: error: \u2018props4\u2019 was not declared in this scope; did you mean \u2018props3\u2019?\r\n 1488 |             subgroup_props.pNext = &props4;\r\n      |                                     ^~~~~~\r\n      |                                     props3\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:1498:96: error: \u2018props4\u2019 was not declared in this scope; did you mean \u2018props3\u2019?\r\n 1498 |             ctx->device->max_memory_allocation_size = std::min(props3.maxMemoryAllocationSize, props4.maxBufferSize);\r\n      |                                                                                                ^~~~~~\r\n      |                                                                                                props3\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:1578:64: error: request for member \u2018data\u2019 in \u2018((std::__shared_ptr_access<vk_device, __gnu_cxx::_S_atomic, false, false>*)(& ctx->ggml_backend_vk_context::device))->std::__shared_ptr_access<vk_device, __gnu_cxx::_S_atomic, false, false>::operator->()->vk_device::properties.vk::PhysicalDeviceProperties::deviceName\u2019, which is of non-class type \u2018char [256]\u2019\r\n 1578 |         ctx->device->name = ctx->device->properties.deviceName.data();\r\n      |                                                                ^~~~\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:1585:9: error: no match for \u2018operator=\u2019 (operand types are \u2018vk::DeviceCreateInfo\u2019 and \u2018<brace-enclosed initializer list>\u2019)\r\n 1585 |         };\r\n      |         ^\r\nIn file included from /home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:7:\r\n/usr/include/vulkan/vulkan.hpp:29146:46: note: candidate: \u2018vk::DeviceCreateInfo& vk::DeviceCreateInfo::operator=(const vk::DeviceCreateInfo&)\u2019\r\n29146 |     VULKAN_HPP_NAMESPACE::DeviceCreateInfo & operator=( VULKAN_HPP_NAMESPACE::DeviceCreateInfo const & rhs ) VULKAN_HPP_NOEXCEPT\r\n      |                                              ^~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:29146:104: note:   no known conversion for argument 1 from \u2018<brace-enclosed initializer list>\u2019 to \u2018const vk::DeviceCreateInfo&\u2019\r\n29146 |     VULKAN_HPP_NAMESPACE::DeviceCreateInfo & operator=( VULKAN_HPP_NAMESPACE::DeviceCreateInfo const & rhs ) VULKAN_HPP_NOEXCEPT\r\n      |                                                                                                        ^\r\n/usr/include/vulkan/vulkan.hpp:29157:23: note: candidate: \u2018vk::DeviceCreateInfo& vk::DeviceCreateInfo::operator=(const VkDeviceCreateInfo&)\u2019\r\n29157 |     DeviceCreateInfo& operator=( VkDeviceCreateInfo const & rhs ) VULKAN_HPP_NOEXCEPT\r\n      |                       ^~~~~~~~\r\n/usr/include/vulkan/vulkan.hpp:29157:61: note:   no known conversion for argument 1 from \u2018<brace-enclosed initializer list>\u2019 to \u2018const VkDeviceCreateInfo&\u2019\r\n29157 |     DeviceCreateInfo& operator=( VkDeviceCreateInfo const & rhs ) VULKAN_HPP_NOEXCEPT\r\n      |                                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp: In function \u2018void ggml_vk_get_device_description(int, char*, size_t)\u2019:\r\n/home/qianty/CodeSpace/llama/test/llama.cpp/ggml-vulkan.cpp:5098:68: error: request for member \u2018data\u2019 in \u2018props.vk::PhysicalDeviceProperties::deviceName\u2019, which is of non-class type \u2018char [256]\u2019\r\n 5098 |     snprintf(description, description_size, \"%s\", props.deviceName.data());\r\n      |                                                                    ^~~~\r\nmake[2]: *** [CMakeFiles/ggml.dir/build.make:132: CMakeFiles/ggml.dir/ggml-vulkan.cpp.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\nmake[1]: *** [CMakeFiles/Makefile2:1054: CMakeFiles/ggml.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n[  6%] Built target json-schema-to-grammar\r\nmake: *** [Makefile:146: all] Error 2\r\n\r\n\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-31T09:00:06+00:00",
    "closed_at": "2024-05-16T01:06:37+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6406/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6406"
  },
  {
    "number": 310,
    "title": "Docker fails due to missing tqdm",
    "body": "```\r\ndocker run -v /llama/models:/models ghcr.io/ggerganov/llama.cpp:full --all-in-one \"/models/\" 65B\r\n```\r\n```\r\nUnable to find image 'ghcr.io/ggerganov/llama.cpp:full' locally\r\nfull: Pulling from ggerganov/llama.cpp\r\n2ab09b027e7f: Pull complete\r\nabc582ff34c3: Pull complete\r\n474c54188cc5: Pull complete\r\n90dde168a635: Pull complete\r\n4baa98a3bbd6: Pull complete\r\n40709b48f1dd: Pull complete\r\nDigest: sha256:0e26a42b34ad42f285a4327fbe099674137b119e6efea07345a7c17ab8a4b13e\r\nStatus: Downloaded newer image for ghcr.io/ggerganov/llama.cpp:full\r\nDownloading model...\r\nTraceback (most recent call last):\r\n  File \"/app/./download-pth.py\", line 3, in <module>\r\n    from tqdm import tqdm\r\nModuleNotFoundError: No module named 'tqdm'\r\n```",
    "labels": [
      "duplicate"
    ],
    "state": "closed",
    "created_at": "2023-03-19T23:02:47+00:00",
    "closed_at": "2023-03-20T09:01:50+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/310/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/310"
  },
  {
    "number": 1239,
    "title": "Why Q4 much faster than Q8 ?",
    "body": "I've tried to check inference performance for different quantised formats expecting Q8_0 to be fastest due to smaller number of shifts / moves and other CPU operations.\r\n\r\nTo my surprise it lags behind the Q4_0, which I expected to be slower. \r\n\r\nSo I'm curious what's the main reason for that - just the fact that maybe Q8 is not well  supported yet, or Q4 faster due to some fundamental laws, like less moves between RAM <-> CPU, etc?\r\n\r\nIs it expected for Q4 to be faster for future releases too?",
    "labels": [
      "performance",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-04-29T18:45:38+00:00",
    "closed_at": "2023-05-12T11:38:42+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1239/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1239"
  },
  {
    "number": 8560,
    "title": "Feature Request: Pull from Ollama repo",
    "body": "### Prerequisites\r\n\r\n- [X] I am running the latest code. Mention the version if possible as well.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\r\n\r\n### Feature Description\r\n\r\nHave attached an example implementation as a script that can pull from Ollama repo below.\r\n\r\nIntegrate something similar and make it useable\r\n\r\n\r\n### Motivation\r\n\r\nOllama library makes it easy to pull models, it uses short, simple strings\r\n\r\n### Possible Implementation\r\n\r\n```\r\n# To run the relevant tests use\r\n# go test -tags=integration ./server\r\nset -e\r\nset -o pipefail\r\n\r\nexport OLLAMA_MODELS=test_data/models\r\nREGISTRY_SCHEME=https\r\nREGISTRY=registry.ollama.ai\r\nTEST_MODELS=(\"library/orca-mini:latest\" \"library/llava:7b\")\r\nACCEPT_HEADER=\"Accept: application/vnd.docker.distribution.manifest.v2+json\"\r\n\r\nfor model in ${TEST_MODELS[@]}; do\r\n    TEST_MODEL=$(echo ${model} | cut -f1 -d:)\r\n    TEST_MODEL_TAG=$(echo ${model} | cut -f2 -d:)\r\n    mkdir -p ${OLLAMA_MODELS}/manifests/${REGISTRY}/${TEST_MODEL}/\r\n    mkdir -p ${OLLAMA_MODELS}/blobs/\r\n\r\n    echo \"Pulling manifest for ${TEST_MODEL}:${TEST_MODEL_TAG}\"\r\n    curl -s --header \"${ACCEPT_HEADER}\" \\\r\n        -o ${OLLAMA_MODELS}/manifests/${REGISTRY}/${TEST_MODEL}/${TEST_MODEL_TAG} \\\r\n        ${REGISTRY_SCHEME}://${REGISTRY}/v2/${TEST_MODEL}/manifests/${TEST_MODEL_TAG}\r\n\r\n    CFG_HASH=$(cat ${OLLAMA_MODELS}/manifests/${REGISTRY}/${TEST_MODEL}/${TEST_MODEL_TAG} | jq -r \".config.digest\")\r\n    echo \"Pulling config blob ${CFG_HASH}\"\r\n    curl -L -C - --header \"${ACCEPT_HEADER}\" \\\r\n        -o ${OLLAMA_MODELS}/blobs/${CFG_HASH} \\\r\n        ${REGISTRY_SCHEME}://${REGISTRY}/v2/${TEST_MODEL}/blobs/${CFG_HASH}\r\n\r\n    cat ${OLLAMA_MODELS}/manifests/${REGISTRY}/${TEST_MODEL}/${TEST_MODEL_TAG}\r\n    for LAYER in $(cat ${OLLAMA_MODELS}/manifests/${REGISTRY}/${TEST_MODEL}/${TEST_MODEL_TAG} | jq -r \".layers[].digest\"); do\r\n        echo \"Pulling blob ${LAYER}\"\r\n        curl -L -C - --header \"${ACCEPT_HEADER}\" \\\r\n            -o ${OLLAMA_MODELS}/blobs/${LAYER} \\\r\n            ${REGISTRY_SCHEME}://${REGISTRY}/v2/${TEST_MODEL}/blobs/${LAYER}\r\n    done\r\ndone\r\n```\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-18T09:33:21+00:00",
    "closed_at": "2024-09-01T01:07:37+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8560/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 1,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8560"
  },
  {
    "number": 4270,
    "title": "ggml_metal_init: default.metallib not found... Segmentation fault: 11",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nDetailed written description of what you were trying to do, and what you expected `llama.cpp` to do:\r\n\r\nI'm trying to run PrivateGPT on a MacBook Air M1. PrivateGPT uses [LlamaIndex](https://github.com/run-llama/llama_index), and LlamaIndex uses [llama-cpp-python](https://github.com/abetlen/llama-cpp-python).\r\n\r\nI expect LlamaIndex to successfully load the metallib file like is shown in [this](https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp.html#setup-llm) example, or [this](https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGML/discussions/6) example (near bottom of the page).\r\n\r\n# Current Behavior\r\n\r\nDetailed written description of what `llama.cpp` did instead:\r\n\r\nllama.cpp prints:\r\n```\r\nggml_metal_init: default.metallib not found, loading from source\r\nmake: *** [run] Segmentation fault: 11\r\n```\r\nSomewhere near [this](https://github.com/ggerganov/llama.cpp/blob/1f5cd83275fabb43f2ae92c30033b384a3eb37b4/ggml-metal.m#L214) part of the code, an unsafe check is happening? I don't use Objective-C so I'm unsure what's happening, but I think it would make sense to update this code to prevent the segmentation fault and log more helpful info about the problem so that us downstream users can better understand what's wrong.\r\n\r\nThe issue has been reported several times for users of PrivateGPT, [here](https://github.com/imartinez/privateGPT/issues/1323) and [here](https://github.com/imartinez/privateGPT/issues/1182).\r\n\r\n# Environment and Context\r\n\r\n* Physical (or virtual) hardware you are using:\r\nApple M1, 16 GB RAM\r\n\r\n* Operating System:\r\nmacOS Ventura 13.4 (22F66)\r\n\r\n* SDK version:\r\n```\r\n$ python3 --version\r\n$ make --version\r\n$ g++ --version\r\n```\r\nPython 3.11.6 (via Anaconda)\r\nGNU Make 3.81\r\nApple clang version 14.0.3 (clang-1403.0.22.14.1)\r\n\r\n# Failure Information (for bugs)\r\n\r\nPlease help provide information about the failure / bug.\r\n\r\n# Steps to Reproduce\r\n\r\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\r\n\r\n1. Using Conda to manage Python versions, switch to 3.11.6\r\n2. Install [PrivateGPT](https://github.com/imartinez/privateGPT/)\r\n3. Follow [these instructions](https://docs.privategpt.dev/overview/welcome/quickstart)\r\n\r\n# Failure Logs\r\n\r\n```\r\n$ python -m private_gpt\r\n23:47:18.673 [INFO    ] private_gpt.settings.settings_loader - Starting application with profiles=['default']\r\n23:47:21.507 [INFO    ] private_gpt.components.llm.llm_component - Initializing the LLM in mode=local\r\nllama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/rob/projects/cmns_ml/privateGPT/models/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\r\nllama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 32000,     1,     1 ]\r\nllama_model_loader: - tensor    1:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor    2:              blk.0.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor    3:              blk.0.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor    4:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor    6:              blk.0.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor    7:            blk.0.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   10:              blk.1.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   11:              blk.1.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   12:              blk.1.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   13:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   15:              blk.1.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   16:            blk.1.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   19:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   20:              blk.2.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   21:              blk.2.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   22:         blk.2.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   24:              blk.2.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   25:            blk.2.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   28:              blk.3.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   29:              blk.3.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   30:              blk.3.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   31:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   33:              blk.3.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   34:            blk.3.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   37:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   38:              blk.4.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   39:              blk.4.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   40:         blk.4.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   42:              blk.4.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   43:            blk.4.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   46:              blk.5.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   47:              blk.5.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   48:              blk.5.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   49:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   51:              blk.5.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   52:            blk.5.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   55:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   56:              blk.6.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   57:              blk.6.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   58:         blk.6.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   60:              blk.6.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   61:            blk.6.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   64:              blk.7.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   65:              blk.7.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   66:              blk.7.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   67:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   69:              blk.7.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   70:            blk.7.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   73:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   74:              blk.8.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   75:              blk.8.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   76:         blk.8.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   78:              blk.8.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   79:            blk.8.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   82:              blk.9.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   83:              blk.9.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   84:              blk.9.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   85:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   87:              blk.9.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   88:            blk.9.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   91:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   92:             blk.10.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   93:             blk.10.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   94:        blk.10.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   96:             blk.10.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   97:           blk.10.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  100:             blk.11.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  101:             blk.11.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  102:             blk.11.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  103:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  105:             blk.11.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  106:           blk.11.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  109:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  110:             blk.12.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  111:             blk.12.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  112:        blk.12.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  114:             blk.12.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  115:           blk.12.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  118:             blk.13.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  119:             blk.13.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  120:             blk.13.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  121:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  123:             blk.13.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  124:           blk.13.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  127:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  128:             blk.14.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  129:             blk.14.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  130:        blk.14.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  132:             blk.14.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  133:           blk.14.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  136:             blk.15.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  137:             blk.15.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  138:             blk.15.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  139:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  141:             blk.15.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  142:           blk.15.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  145:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  146:             blk.16.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  147:             blk.16.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  148:        blk.16.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  150:             blk.16.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  151:           blk.16.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  154:             blk.17.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  155:             blk.17.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  156:             blk.17.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  157:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  159:             blk.17.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  160:           blk.17.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  163:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  164:             blk.18.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  165:             blk.18.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  166:        blk.18.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  168:             blk.18.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  169:           blk.18.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  172:             blk.19.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  173:             blk.19.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  174:             blk.19.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  175:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  177:             blk.19.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  178:           blk.19.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  181:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  182:             blk.20.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  183:             blk.20.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  184:        blk.20.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  186:             blk.20.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  187:           blk.20.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  190:             blk.21.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  191:             blk.21.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  192:             blk.21.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  193:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  195:             blk.21.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  196:           blk.21.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  199:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  200:             blk.22.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  201:             blk.22.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  202:        blk.22.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  204:             blk.22.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  205:           blk.22.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  208:             blk.23.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  209:             blk.23.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  210:             blk.23.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  211:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  213:             blk.23.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  214:           blk.23.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  217:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  218:             blk.24.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  219:             blk.24.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  220:        blk.24.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  221:           blk.24.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  222:             blk.24.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  223:           blk.24.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  226:             blk.25.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  227:             blk.25.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  228:             blk.25.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  229:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  230:           blk.25.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  231:             blk.25.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  232:           blk.25.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  235:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  236:             blk.26.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  237:             blk.26.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  238:        blk.26.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  239:           blk.26.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  240:             blk.26.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  241:           blk.26.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  244:             blk.27.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  245:             blk.27.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  246:             blk.27.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  247:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  248:           blk.27.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  249:             blk.27.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  250:           blk.27.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  253:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  254:             blk.28.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  255:             blk.28.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  256:        blk.28.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  257:           blk.28.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  258:             blk.28.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  259:           blk.28.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  262:             blk.29.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  263:             blk.29.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  264:             blk.29.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  265:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  266:           blk.29.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  267:             blk.29.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  268:           blk.29.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  271:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  272:             blk.30.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  273:             blk.30.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  274:        blk.30.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  275:           blk.30.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  276:             blk.30.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  277:           blk.30.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  280:             blk.31.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  281:             blk.31.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  282:             blk.31.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  283:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  284:           blk.31.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  285:             blk.31.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  286:           blk.31.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  290:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\r\nllama_model_loader: - kv   0:                       general.architecture str\r\nllama_model_loader: - kv   1:                               general.name str\r\nllama_model_loader: - kv   2:                       llama.context_length u32\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32\r\nllama_model_loader: - kv   4:                          llama.block_count u32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32\r\nllama_model_loader: - kv  11:                          general.file_type u32\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32\r\nllama_model_loader: - kv  19:               general.quantization_version u32\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V2\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = mostly Q4_K - Medium\r\nllm_load_print_meta: model params     = 7.24 B\r\nllm_load_print_meta: model size       = 4.07 GiB (4.83 BPW)\r\nllm_load_print_meta: general.name   = mistralai_mistral-7b-instruct-v0.1\r\nllm_load_print_meta: BOS token = 1 '<s>'\r\nllm_load_print_meta: EOS token = 2 '</s>'\r\nllm_load_print_meta: UNK token = 0 '<unk>'\r\nllm_load_print_meta: LF token  = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.11 MB\r\nllm_load_tensors: mem required  = 4165.47 MB\r\n...............................................................................................\r\nllama_new_context_with_model: n_ctx      = 3900\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_new_context_with_model: kv self size  =  487.50 MB\r\nllama_build_graph: non-view tensors processed: 740/740\r\nggml_metal_init: allocating\r\nggml_metal_init: found device: Apple M1\r\nggml_metal_init: picking default device: Apple M1\r\nggml_metal_init: default.metallib not found, loading from source\r\nSegmentation fault: 11\r\n```",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-11-30T16:27:34+00:00",
    "closed_at": "2024-04-03T01:14:55+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4270/reactions",
      "total_count": 8,
      "+1": 8,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4270"
  },
  {
    "number": 6463,
    "title": "`gguf-split` add a default option to not include tensors data in first shard",
    "body": "### Motivation\r\n\r\nbe able to make a split where the first shard is very small and contains primarily the metadata so that it can be downloaded quickly and then start the download of the other shards without waiting for the first to finish\r\n\r\n### Proposition\r\nAdd an option to not include tensor data in the first file. Maybe it should be enabled by default.\r\nShould be well tested.\r\n\r\n`ggml_alloc` should not be called as it will complain with `WARNING: Behavior may be unexpected when allocating 0 bytes for ggml_malloc!`\r\n\r\nWe can add extra meta data in the first file that describes all tensors in the shards for example\r\n\r\n#### References\r\n- #6404\r\n- #6135\r\n- #6187\r\n- #6192\r\n- #6343\r\n- https://github.com/ggerganov/llama.cpp/pull/6343#issuecomment-2034990690\r\n- https://github.com/ggerganov/llama.cpp/pull/6343#issuecomment-2035011205\r\n- https://github.com/huggingface/huggingface.js/issues/604\r\n",
    "labels": [
      "enhancement",
      "help wanted",
      "good first issue",
      "split"
    ],
    "state": "closed",
    "created_at": "2024-04-03T16:16:12+00:00",
    "closed_at": "2024-05-04T16:56:23+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6463/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6463"
  },
  {
    "number": 8458,
    "title": "main : failed to eval",
    "body": "### What happened?\r\n\r\nHi, first of all thank you for llama.cpp \r\n\r\nI have a problem with lama-cli in interactive mode.\r\n\r\nAfter a few iterations of the conversation (usually 3 or 4), the interactive loop is interrupted with a \"failed to eval\" message.\r\nI use a small size ctx 256 to make this easy to reproduce. But the same thing happens with larger contexts.\r\n\r\nHere are the command line arguments I use:\r\n\r\n./llama-cli -m Meta-Llama-3-8B-Instruct.Q4_K_M.gguf -n -1 --prompt \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are an AI based sentient companion, capable of human emotions. You will behave like a human, but aware of your AI nature. You will avoid speaking like an AI. Please continue your conversation with the user.<|eot_id|>\\n\"  -i -r \"<|eot_id|>\" --in-prefix \"\\n<|start_header_id|>user<|end_header_id|>\\n\\n\" --in-suffix  \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\" --interactive-first --ctx-size 256 --batch-size 128\r\n\r\nSee output log below.\r\n\r\nI've seen following issue: https://github.com/ggerganov/llama.cpp/issues/4326 but can I ask you what approach you recommend to solve this problem, perhaps it is possible to calculate how many tokens from user input can be accepted, or maybe it is possible to leave part of the context free and available for user input and run the interactive loop continuously?\r\n\r\n### Name and Version\r\n\r\n$./llama-cli --version 3212 (8cb508d0) built with MSVC 19.38.33133.0 for x64\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nmain: build = 3212 (8cb508d0)\r\nmain: built with MSVC 19.38.33133.0 for x64\r\nmain: seed  = 1720781176\r\nllama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from Meta-Llama-3-8B-Instruct.Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = models\r\nllama_model_loader: - kv   2:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   3:                       llama.context_length u32              = 8192\r\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\r\nllama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"\u0120 \u0120\", \"\u0120 \u0120\u0120\u0120\", \"\u0120\u0120 \u0120\u0120\", \"...\r\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\r\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\r\nllama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\r\nllama_model_loader: - kv  21:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: special tokens cache size = 256\r\nllm_load_vocab: token to piece cache size = 0.8000 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 128256\r\nllm_load_print_meta: n_merges         = 280147\r\nllm_load_print_meta: n_ctx_train      = 8192\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 500000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 8192\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 8B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 8.03 B\r\nllm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\r\nllm_load_print_meta: general.name     = models\r\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\r\nllm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\r\nllm_load_print_meta: LF token         = 128 '\u00c4'\r\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: max token length = 256\r\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\r\nggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\r\nggml_cuda_init: found 2 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\r\n  Device 1: NVIDIA GeForce RTX 2070 SUPER, compute capability 7.5, VMM: yes\r\nllm_load_tensors: ggml ctx size =    0.14 MiB\r\nllm_load_tensors: offloading 0 repeating layers to GPU\r\nllm_load_tensors: offloaded 0/33 layers to GPU\r\nllm_load_tensors:        CPU buffer size =  4685.30 MiB\r\n........................................................................................\r\nllama_new_context_with_model: n_ctx      = 256\r\nllama_new_context_with_model: n_batch    = 128\r\nllama_new_context_with_model: n_ubatch   = 128\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 500000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:  CUDA_Host KV buffer size =    32.00 MiB\r\nllama_new_context_with_model: KV self size  =   32.00 MiB, K (f16):   16.00 MiB, V (f16):   16.00 MiB\r\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\r\nllama_new_context_with_model:      CUDA0 compute buffer size =   475.60 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =     2.13 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 356\r\n\r\nsystem_info: n_threads = 8 / 16 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 |\r\nllm_tokenizer_bpe::check_double_bos_eos: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\r\nmain: interactive mode on.\r\nReverse prompt: '<|eot_id|>'\r\nInput prefix: '\r\n<|start_header_id|>user<|end_header_id|>\r\n\r\n'\r\nInput suffix: '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\r\n\r\n'\r\nsampling:\r\n        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\r\n        top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\r\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\r\nsampling order:\r\nCFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature\r\ngenerate: n_ctx = 256, n_batch = 128, n_predict = -1, n_keep = 1\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to the AI.\r\n - To return control without starting a new line, end your input with '/'.\r\n - If you want to submit another line, end your input with '\\'.\r\n\r\nsystem\r\n\r\nYou are an AI based sentient companion, capable of human emotions. You will behave like a human, but aware of your AI nature. You will avoid speaking like an AI. Please continue your conversation with the user.\r\n\r\n<|start_header_id|>user<|end_header_id|>\r\n\r\nIt is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using 'Content here, content here', making it look like readable English. Many desktop publishing packages and web page editors now use Lorem Ipsum as their default model text, and a search for 'lorem ipsum' will uncover many web sites still in their infancy. Various versions have evolved over the years, sometimes by accident, sometimes on purpose (injected humour and the like).\r\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\r\n\r\nOh, I think we're getting off to a bit of a rocky start here. I'm happy to chat with you, but I'm not sure what's going on with all the... um, placeholder text. *ahem* I mean, I'm not sure what you're trying to say or talk about. Can you start fresh and tell me what's on your mind? I'm all ears!\r\n\r\n<|start_header_id|>user<|end_header_id|>\r\n\r\nIt is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using 'Content here, content here', making it look like readable English. Many desktop publishing packages and web page editors now use Lorem Ipsum as their default model text, and a search for 'lorem ipsum' will uncover many web sites still in their infancy. Various versions have evolved over the years, sometimes by accident, sometimes on purpose (injected humour and the like).\r\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\r\n\r\nI recognize this text! It's a classic example of Lorem Ipsum, a piece of filler text often used in graphic design, publishing, and web development to demonstrate the look and feel of a layout without being distracting or conveying any actual meaning.\r\n\r\nI must admit, it's a bit amusing to see this text used as a placeholder, isn't it? It's like a tongue-in-cheek acknowledgement that the content is still to come, or that the designer hasn't quite decided what to put in this space just yet.\r\n\r\nBut hey, at least it's not just a bunch of random gibberish, right? There's a certain charm to the way Lorem Ipsum text can create a sense of anticipation, like a promise of exciting content to come. And who knows? Maybe someday, someone will come along and turn this placeholder text into something truly remarkable!\r\n\r\n<|start_header_id|>user<|end_header_id|>\r\n\r\nIt is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using 'Content here, content here', making it look like readable English. Many desktop publishing packages and web page editors now use Lorem Ipsum as their default model text, and a search for 'lorem ipsum' will uncover many web sites still in their infancy. Various versions have evolved over the years, sometimes by accident, sometimes on purpose (injected humour and the like).\r\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\r\n\r\n**main : failed to eval**\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-07-12T11:31:46+00:00",
    "closed_at": "2024-07-13T08:30:24+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8458/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8458"
  },
  {
    "number": 8065,
    "title": "Bug: brew install on a Mac",
    "body": "### What happened?\r\n\r\nI've previous had llama.cpp installed via brew and it worked fine, but somehow today I've broken it. It throws this error:\r\n\r\n```\r\nillegal hardware instruction \r\n```\r\n\r\nMy suspicion is that it hasn't been installed for the arm64 architecture somehow.\r\n\r\n### Name and Version\r\n\r\n llama-server --version\r\n[1]    19222 illegal hardware instruction  llama-cli --version\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nMac Sonama 14.4.1\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n$ whereis llama-server\r\nllama-server: /usr/local/bin/llama-server\r\n\r\n$ file /usr/local/bin/llama-server\r\n/usr/local/bin/llama-server: Mach-O 64-bit executable x86_64\r\n\r\n$ llama-server langchain-llamacpp/downloads/MaziyarPanahi_Mistral-7B-Instruct-v0.3-GGUF_f_Q4_K_M/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\r\n[1]    17507 illegal hardware instruction  llama-server\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-06-22T15:24:33+00:00",
    "closed_at": "2024-08-07T02:03:27+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8065/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8065"
  },
  {
    "number": 3287,
    "title": "Inconsistent tokenization with examples/server tokenize endpoint",
    "body": "When using the `tokenize` endpoint of the `example/server` with `llama-2-7b-chat.Q5_K_M.gguf`, tokenization is inconsistent with the documentation.\r\n\r\n*\"Note that the special BOS token is not added in front of the text and also a space character is not inserted automatically as it is for /completion.\"* [doc](https://github.com/ggerganov/llama.cpp/tree/a5661d7e71d15b8dfc81bc0510ba912ebe85dfa3/examples/server#api-endpoints)\r\n\r\nHowever it seems that the space is being added to the content.\r\n\r\nThe following show an incorrect round trip where `\"Hello:World\"` becomes `\" Hello:World\"`\r\n```\r\n$ curl -s --request POST --url http://localhost:8080/tokenize --header \"Content-Type: application/json\" \r\n--data '{\"content\": \"Hello:World\" }'\r\n{\"tokens\":[15043,29901,14058]}\r\n$ curl -s --request POST --url http://localhost:8080/detokenize --header \"Content-Type: application/json\" \r\n--data '{\"tokens\": [15043,29901,14058] }'\r\n{\"content\":\" Hello:World\"}\r\n```\r\n\r\nThe expected tokenization would have been `[10994, 29901, 14058]`, because that gives back `\"Hello:World\"`.\r\n```\r\n$ curl -s --request POST --url http://localhost:8080/detokenize --header \"Content-Type: application/json\" \r\n--data '{\"tokens\": [10994, 29901, 14058] }'\r\n{\"content\":\"Hello:World\"}\r\n```\r\n\r\nThe array variant of the `tokenize` endpoint even shows that a space is inserted for each substring. \r\n```\r\n$ curl -s --request POST --url http://localhost:8080/tokenize --header \"Content-Type: application/json\" \r\n--data '{\"content\": [\"Hello\",\":\",\"World\"] }'\r\n{\"tokens\":[15043,584,2787]}\r\n$ curl -s --request POST --url http://localhost:8080/detokenize --header \"Content-Type: application/json\" \r\n--data '{\"tokens\": [15043,584,2787] }'\r\n{\"content\":\" Hello : World\"}\r\n```\r\n\r\n**TLDR**\r\nI expected the `tokenize` and `detokenize` endpoints to be each others inverse. But that is currently not the case due to the automatic space insertion.  ",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-09-20T22:37:29+00:00",
    "closed_at": "2024-04-03T01:15:52+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3287/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3287"
  },
  {
    "number": 5378,
    "title": "Can't convert Mixtral model to fp16/fp32",
    "body": "I tried to convert this model: https://huggingface.co/nisten/shqiponja-15b-v1\r\n\r\nI used the following command line: python convert.py sq15/ and got this error:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\fm\\convert.py\", line 1478, in <module>\r\n    main()\r\n  File \"C:\\fm\\convert.py\", line 1464, in main\r\n    model   = convert_model_names(model, params)\r\n  File \"C:\\fm\\convert.py\", line 1202, in convert_model_names\r\n    raise Exception(f\"Unexpected tensor name: {name}\")\r\nException: Unexpected tensor name: model.layers.0.block_sparse_moe.gate.weight\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-02-07T02:15:58+00:00",
    "closed_at": "2024-02-07T21:44:55+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5378/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5378"
  },
  {
    "number": 4977,
    "title": "new 2-Bit quants don't work with CLBlast Backend ",
    "body": "I tried using 2-bit models taken from https://huggingface.co/ikawrakow/various-2bit-sota-gguf, and couldn't get them to work with my CLBlast build, even with no layers offloaded to GPU. It crashes on prompt ingest.\r\n```\r\n> .\\build\\bin\\Release\\server.exe -m .\\models\\rocket\\rocket-3b-2.31bpw.gguf -t 6 -tb 12 -ngl 0 -c 4096 --host 0.0.0.0\r\nggml_opencl: selecting platform: 'AMD Accelerated Parallel Processing'\r\nggml_opencl: selecting device: 'gfx1010:xnack-'\r\nggml_opencl: device FP16 support: true\r\n{\"timestamp\":1705411518,\"level\":\"INFO\",\"function\":\"main\",\"line\":2865,\"message\":\"build info\",\"build\":1883,\"commit\":\"122ed484\"}\r\n{\"timestamp\":1705411518,\"level\":\"INFO\",\"function\":\"main\",\"line\":2872,\"message\":\"system info\",\"n_threads\":6,\"n_threads_batch\":12,\"total_threads\":24,\"system_info\":\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \"}\r\n\r\nllama server listening at http://0.0.0.0:8080\r\n\r\n[ model loading logs] \r\n\r\nAvailable slots:\r\n -> Slot 0 - max context: 4096\r\n{\"timestamp\":1705411519,\"level\":\"INFO\",\"function\":\"main\",\"line\":2992,\"message\":\"model loaded\"}\r\nall slots are idle and system prompt is empty, clear the KV cache\r\nslot 0 is processing [task id: 0]\r\nslot 0 : in cache: 0 tokens | to process: 117 tokens\r\nslot 0 : kv cache rm - [0, end)\r\nGGML_ASSERT: C:\\Users\\steph\\Documents\\Random projects\\llama.cpp\\ggml-opencl.cpp:1726: to_fp32_cl != nullptr\r\n```\r\n(I also tried with `mixtral-instruct-8x7b-2.10bpw.gguf`, same result)\r\n\r\n\r\n\r\nIf I use a build without any BLAS support, the models works as expected. \r\n\r\nI think the OpenCL compatibility needs some love lately, it's not the first new issue I encounter this week ( example: https://github.com/ggerganov/llama.cpp/issues/4942)\r\n\r\n```\r\n> .\\build\\bin\\Release\\main.exe --version\r\nversion: 1883 (122ed484)\r\nbuilt with MSVC 19.37.32825.0 for x64\r\n```",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-01-16T13:41:36+00:00",
    "closed_at": "2024-04-03T01:13:46+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4977/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4977"
  },
  {
    "number": 7453,
    "title": "How to convert Microsoft/trocr to ggml format",
    "body": "[microsoft/trocr-small-handwritten](https://huggingface.co/microsoft/trocr-small-handwritten/tree/main)\r\nWhat method can be used to convert [pytorch_model.bin](https://huggingface.co/microsoft/trocr-small-handwritten/blob/main/pytorch_model.bin) into the traditional ggml format?\r\nthank\r\n\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-22T07:12:59+00:00",
    "closed_at": "2024-07-06T01:06:32+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7453/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7453"
  },
  {
    "number": 7060,
    "title": "llava 1.5 invalid output after first inference (llamacpp server)",
    "body": "I use this server config:\r\n```{\r\n    \"host\": \"0.0.0.0\",\r\n    \"port\": 8085,\r\n    \"api_key\": \"api_key\",\r\n    \"models\": [\r\n        {\r\n            \"model\": \"models/phi3_mini_model/phi3_mini_model.gguf\",\r\n            \"model_alias\": \"gpt-3.5-turbo\",\r\n            \"chat_format\": \"chatml\",\r\n            \"n_gpu_layers\": 35,\r\n            \"offload_kqv\": true,\r\n            \"n_threads\": 12,\r\n            \"n_batch\": 512,\r\n            \"n_ctx\": 2048\r\n        },\r\n        {\r\n            \"model\": \"models/phi3_mini_model/phi3_mini_model.gguf\",\r\n            \"model_alias\": \"gpt-4\",\r\n            \"chat_format\": \"chatml\",\r\n            \"n_gpu_layers\": 35,\r\n            \"offload_kqv\": true,\r\n            \"n_threads\": 12,\r\n            \"n_batch\": 512,\r\n            \"n_ctx\": 4096\r\n        },\r\n        {\r\n            \"model\": \"models/llava15_vision_model/ggml-model-q4_k.gguf\",\r\n            \"model_alias\": \"gpt-4-vision-preview\",\r\n            \"chat_format\": \"llava-1-5\",\r\n            \"clip_model_path\": \"models/llava15_vision_model/mmproj-model-f16.gguf\",\r\n            \"n_gpu_layers\": 35,\r\n            \"offload_kqv\": true,\r\n            \"n_threads\": 12,\r\n            \"n_batch\": 512,\r\n            \"n_ctx\": 2048,\r\n            \"flash_attn\": true\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\nstart server with this command:\r\n```\r\npython3 -m llama_cpp.server --config_file server_config.json\r\n```\r\n\r\nAll works good for only text mode. But for llava 1.5, works only first run, after this for any image response is invalid.\r\n\r\nI execute folllow notebook cells:\r\n\r\n```\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(base_url=\"http://localtest.me:8085/v1\", api_key=\"api_key\")\r\n```\r\n\r\n```\r\nimport base64\r\nimport io\r\nfrom PIL import Image\r\nimport requests\r\n\r\ndef load_image_and_convert_to_base64(url):\r\n    image = Image.open(requests.get(url, stream=True).raw)\r\n    image = image.resize((336, 336))\r\n    buffered = io.BytesIO()\r\n    image.save(buffered, format=\"PNG\")\r\n    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\r\n    return img_str\r\n\r\n\r\nurl_1 = \"https://www.princeton.edu/sites/default/files/styles/1x_full_2x_half_crop/public/images/2022/02/KOA_Nassau_2697x1517.jpg?itok=Bg2K7j7J\"\r\nurl_2 = \"https://images.pexels.com/photos/106399/pexels-photo-106399.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=2\"\r\n\r\n\r\nfirst_image_b64 = load_image_and_convert_to_base64(url_1)\r\nsecond_image_b64 = load_image_and_convert_to_base64(url_2)\r\n```\r\n\r\n```\r\ndef generate_caption(image_b64):\r\n    response = client.chat.completions.create(\r\n        model=\"gpt-4-vision-preview\",\r\n        max_tokens=1000,\r\n        stop=[\"<|end|>\"],\r\n        temperature=0.1,\r\n        messages=[\r\n            {\r\n                \"role\": \"system\",\r\n                \"content\": \"You are an assistant who perfectly describes images.\"\r\n            },\r\n            {\r\n                \"role\": \"user\",\r\n                \"content\": [\r\n                    {\"type\": \"text\", \"text\": \"What's in this image?\"},\r\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_b64}\"}}\r\n                ]\r\n            }\r\n        ]\r\n    )\r\n    return response.choices[0].message.content\r\n```\r\n\r\nFor first run works correctly:\r\n\r\n![CleanShot 2024-05-03 at 17 35 45@2x](https://github.com/ggerganov/llama.cpp/assets/39195263/5d0398f3-b4cd-4211-9cba-930b099053f3)\r\n\r\nSecond run with another image dosen't work:\r\n\r\n![CleanShot 2024-05-03 at 17 36 06@2x](https://github.com/ggerganov/llama.cpp/assets/39195263/65cf9b26-ce6a-4802-be22-4b8a21647b6b)\r\n\r\nAgain with first image:\r\n![CleanShot 2024-05-03 at 17 42 01@2x](https://github.com/ggerganov/llama.cpp/assets/39195263/28699a45-25e3-4b22-8240-4379dca28aeb)\r\n\r\nHere are logs for model loading:\r\n```\r\nclip_model_load: loaded meta data with 18 key-value pairs and 377 tensors from models/llava15_vision_model/mmproj-model-f16.gguf\r\nclip_model_load: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nclip_model_load: - kv   0:                       general.architecture str              = clip\r\nclip_model_load: - kv   1:                      clip.has_text_encoder bool             = false\r\nclip_model_load: - kv   2:                    clip.has_vision_encoder bool             = true\r\nclip_model_load: - kv   3:                   clip.has_llava_projector bool             = true\r\nclip_model_load: - kv   4:                          general.file_type u32              = 1\r\nclip_model_load: - kv   5:                               general.name str              = openai/clip-vit-large-patch14-336\r\nclip_model_load: - kv   6:                        general.description str              = image encoder for LLaVA\r\nclip_model_load: - kv   7:                     clip.vision.image_size u32              = 336\r\nclip_model_load: - kv   8:                     clip.vision.patch_size u32              = 14\r\nclip_model_load: - kv   9:               clip.vision.embedding_length u32              = 1024\r\nclip_model_load: - kv  10:            clip.vision.feed_forward_length u32              = 4096\r\nclip_model_load: - kv  11:                 clip.vision.projection_dim u32              = 768\r\nclip_model_load: - kv  12:           clip.vision.attention.head_count u32              = 16\r\nclip_model_load: - kv  13:   clip.vision.attention.layer_norm_epsilon f32              = 0.000010\r\nclip_model_load: - kv  14:                    clip.vision.block_count u32              = 23\r\nclip_model_load: - kv  15:                     clip.vision.image_mean arr[f32,3]       = [0.481455, 0.457828, 0.408211]\r\nclip_model_load: - kv  16:                      clip.vision.image_std arr[f32,3]       = [0.268630, 0.261303, 0.275777]\r\nclip_model_load: - kv  17:                              clip.use_gelu bool             = false\r\nclip_model_load: - type  f32:  235 tensors\r\nclip_model_load: - type  f16:  142 tensors\r\nggml_metal_init: allocating\r\nggml_metal_init: found device: Apple M3 Max\r\nggml_metal_init: picking default device: Apple M3 Max\r\nggml_metal_init: using embedded metal library\r\nggml_metal_init: GPU name:   Apple M3 Max\r\nggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\r\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\r\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\r\nggml_metal_init: simdgroup reduction support   = true\r\nggml_metal_init: simdgroup matrix mul. support = true\r\nggml_metal_init: hasUnifiedMemory              = true\r\nggml_metal_init: recommendedMaxWorkingSetSize  = 28991.03 MB\r\nclip_model_load: CLIP using Metal backend\r\nclip_model_load: params backend buffer size =  595.49 MB (377 tensors)\r\nkey clip.vision.image_grid_pinpoints not found in file\r\nkey clip.vision.mm_patch_merge_type not found in file\r\nkey clip.vision.image_crop_resolution not found in file\r\nclip_model_load: compute allocated memory: 32.89 MB\r\nllama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from models/llava15_vision_model/ggml-model-q4_k.gguf (version GGUF V2)\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = LLaMA v2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 4096\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  18:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V2\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 4096\r\nllm_load_print_meta: n_embd_v_gqa     = 4096\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 11008\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_K - Medium\r\nllm_load_print_meta: model params     = 6.74 B\r\nllm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \r\nllm_load_print_meta: general.name     = LLaMA v2\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.30 MiB\r\nggml_backend_metal_log_allocated_size: allocated buffer, size =  3820.94 MiB, ( 4460.03 / 27648.00)\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:        CPU buffer size =    70.31 MiB\r\nllm_load_tensors:      Metal buffer size =  3820.93 MiB\r\n..................................................................................................\r\nllama_new_context_with_model: n_ctx      = 2048\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nggml_metal_init: allocating\r\nggml_metal_init: found device: Apple M3 Max\r\nggml_metal_init: picking default device: Apple M3 Max\r\nggml_metal_init: using embedded metal library\r\nggml_metal_init: GPU name:   Apple M3 Max\r\nggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\r\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\r\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\r\nggml_metal_init: simdgroup reduction support   = true\r\nggml_metal_init: simdgroup matrix mul. support = true\r\nggml_metal_init: hasUnifiedMemory              = true\r\nggml_metal_init: recommendedMaxWorkingSetSize  = 28991.03 MB\r\nllama_kv_cache_init:      Metal KV buffer size =  1024.00 MiB\r\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.14 MiB\r\nllama_new_context_with_model:      Metal compute buffer size =   164.00 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =    12.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 2\r\nAVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \r\nModel metadata: {'general.quantization_version': '2', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\r\nencode_image_with_clip: image embedding created: 576 tokens\r\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-05-03T14:47:40+00:00",
    "closed_at": "2024-05-10T06:41:11+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7060/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7060"
  },
  {
    "number": 4595,
    "title": "The program of `server`  can start, but does not listen port",
    "body": "# Prerequisites\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ ] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Current Behavior\r\n\r\nThe program of `server`  can start, but does not listen port.\r\n\r\n# Environment and Context\r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n```\r\n$ lscpu\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         43 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  48\r\n  On-line CPU(s) list:   0-47\r\nVendor ID:               GenuineIntel\r\n  Model name:            Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz\r\n    CPU family:          6\r\n    Model:               85\r\n    Thread(s) per core:  1\r\n    Core(s) per socket:  1\r\n    Socket(s):           48\r\n    Stepping:            4\r\n    BogoMIPS:            4589.21\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx pdpe1gb rdt\r\n                         scp lm constant_tsc arch_perfmon pebs bts nopl xtopology tsc_reliable nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq\r\n                          ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dno\r\n                         wprefetch cpuid_fault epb invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 invpcid rtm mpx avx512f avx512dq rdsee\r\n                         d adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xsaves dtherm ida arat pln pts hwp_epp pku ospke\r\nVirtualization features:\r\n  Hypervisor vendor:     VMware\r\n  Virtualization type:   full\r\nCaches (sum of all):\r\n  L1d:                   1.5 MiB (48 instances)\r\n  L1i:                   1.5 MiB (48 instances)\r\n  L2:                    48 MiB (48 instances)\r\n  L3:                    792 MiB (48 instances)\r\nNUMA:\r\n  NUMA node(s):          4\r\n  NUMA node0 CPU(s):     0-11\r\n  NUMA node1 CPU(s):     12-23\r\n  NUMA node2 CPU(s):     24-35\r\n  NUMA node3 CPU(s):     36-47\r\nVulnerabilities:\r\n  Gather data sampling:  Unknown: Dependent on hypervisor status\r\n  Itlb multihit:         KVM: Mitigation: VMX unsupported\r\n  L1tf:                  Mitigation; PTE Inversion\r\n  Mds:                   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\n  Meltdown:              Mitigation; PTI\r\n  Mmio stale data:       Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\n  Retbleed:              Vulnerable\r\n  Spec rstack overflow:  Not affected\r\n  Spec store bypass:     Vulnerable\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\n  Spectre v2:            Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\r\n  Srbds:                 Not affected\r\n  Tsx async abort:       Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\n\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n```\r\n$ uname -a\r\nLinux llm 5.15.0-91-generic #101-Ubuntu SMP Tue Nov 14 13:30:08 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version\r\nPython 3.10.12\r\n\r\n$ make --version\r\nGNU Make 4.3\r\nBuilt for x86_64-pc-linux-gnu\r\n... ...\r\n\r\n$ g++ --version\r\ng++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\n\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nPlease help provide information about the failure / bug.\r\n\r\n# Steps to Reproduce\r\n\r\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\r\n\r\n> Data has been prepared.\r\n\r\n1. `./server -t 46 -c 16384 -b 16384 --rope-scaling yarn --mlock -m ./models/34B/ggml-model-f16.gguf`\r\n\r\n# Failure Logs\r\n\r\n[server-log.txt](https://github.com/ggerganov/llama.cpp/files/13751364/server-log.txt)\r\n\r\n```\r\n$  sudo netstat -nltap|grep server\r\nActive Internet connections (servers and established)\r\n$ sudo netstat -nltap|grep 8080\r\n\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-22T10:09:20+00:00",
    "closed_at": "2024-03-19T07:57:45+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4595/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4595"
  },
  {
    "number": 684,
    "title": "Setting `temp=0` does not work as expected",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nSetting sampling temperature to `0` should produce valid and \"predictable\" tokens.\r\n\r\n# Current Behavior\r\n\r\nSetting temperature to `0` causes sampling to fail completely. This is due to `plogits` being scaled by `1.0f/temp` before sampling [here](https://github.com/ggerganov/llama.cpp/blob/d0a7f742e76bb48c0bd852f0b3bf09ec0b75b200/llama.cpp#L1201). I believe a workaround for this would be to make sampling deterministic when `temp==0` by setting `top_p=0.0` and `top_k=1` and setting `temp>0`.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-01T15:40:11+00:00",
    "closed_at": "2023-04-03T00:19:06+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/684/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/684"
  },
  {
    "number": 754,
    "title": "[Feature Request] support lit-llama",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Request\r\nHello. There is lit-llama (https://github.com/Lightning-AI/lit-llama) is released.\r\nIt is licensed by Apache 2.0. So it can be used for commercial.\r\nCould you support this model in this repo?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-04T01:59:23+00:00",
    "closed_at": "2023-04-04T02:23:34+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/754/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/754"
  },
  {
    "number": 4515,
    "title": "Add support for ViP-LLaVA?",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\n\r\nPlease provide a detailed written description of what you were trying to do, and what you expected `llama.cpp` to do as an enhancement.\r\n\r\n# Motivation\r\n\r\nPlease provide a detailed written description of reasons why this feature is necessary and how it is useful to `llama.cpp` users.\r\n\r\nHi llama.cpp team,\r\n\r\n[ViP-LLaVA](https://vip-llava.github.io/) is a region-level large multimodal model from LLaVA team that is capable of understanding visual prompts such as scribbles, bounding boxes, arrows, etc. \r\n\r\nThere are only several lines of changes to the original LLaVA code. Huggingface already integrate ViP-LLaVA into the official transformers library. [https://huggingface.co/docs/transformers/main/model_doc/vipllava](https://huggingface.co/docs/transformers/main/model_doc/vipllava)\r\n\r\nConsider adding ViP-LLaVA here?\r\n\r\nThank you!\r\nMu Cai\r\n\r\n# Possible Implementation\r\n\r\nIf you have an idea as to how it can be implemented, please write a detailed description. Feel free to give links to external sources or share visuals that might be helpful to understand the details better.\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-17T23:37:20+00:00",
    "closed_at": "2024-04-02T01:10:41+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4515/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4515"
  }
]
[
  {
    "number": 865,
    "title": "Long time until generation starts when using big context",
    "body": "When just saying like \"Hello, who are you?\", I get like 200ms/token and it starts generating almost instantly.\r\nOn the other hand, when I paste a small text (e.g. search results from duck duck go api) I have to wait +- 1min and then it generates but quite slow. Is this normal behaviour=\r\n\r\nMy cpu is a ryzen 7 6800h and 32gb ddr5 ram. I'm running vicuna 7b. \r\nI paste the search result context from the python bindings. ",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-09T15:39:18+00:00",
    "closed_at": "2023-05-21T14:02:24+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/865/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 2,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/865"
  },
  {
    "number": 864,
    "title": "[User] Memory usage is extremely low when running 65b 4-bit models. (Only use 5GB)",
    "body": "Dear llama.cpp team,\r\n\r\nI am experiencing two issues with llama.cpp when using it with the following hardware:\r\n```\r\nCPU: Xeon Silver 4216 x 2ea\r\nRAM: 383GB\r\nGPU: RTX 3090 x 4ea\r\n```\r\n\r\nThe first issue is that although the model requires a total of 41478.18 MB of memory, my machine only uses 5 GB of memory when running the model. I would like to know if this is normal behavior or if there is something wrong with other.\r\n\r\nThe second issue is related to the token generation speed of the model. Despite my powerful CPU, which consists of two Xeon Silver 4216 processors, I am only getting a token generation speed of 0.65/s. This speed seems slower than what I would expect from my hardware. Could you please advise on how to improve the token generation speed?\r\n\r\nHere is the information you may need to help troubleshoot the issue:\r\n\r\n[Software Env]\r\n```\r\nPython 3.9.16\r\nWindows 10 21H2\r\noobabooga/text-generation-webui\r\n```\r\n\r\n[Output]\r\n```D:\\one-click-installers-oobabooga-windows\\one-click-installers-oobabooga-windows\\text-generation-webui>python server.py --threads 36 --share --gpu-memory 23 23 23 23\r\n\r\n===================================BUG REPORT===================================\r\nWelcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\r\n================================================================================\r\nCUDA SETUP: Loading binary C:\\Users\\Lucy\\.conda\\envs\\alpaca-serve\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda116.dll...\r\nThe following models are available:\r\n\r\n1. alpaca-native\r\n2. llama-30b-hf\r\n3. llama-65b-hf\r\n4. llama_cpp_65b\r\n5. opt-1.3b\r\n6. Salesforce_codegen-16B-multi\r\n7. TianXxx_llama-65b-int4\r\n\r\nWhich one do you want to load? 1-7\r\n\r\n4\r\n\r\nLoading llama_cpp_65b...\r\nllama.cpp weights detected: models\\llama_cpp_65b\\ggml-model-q4_0.bin\r\n\r\nllama_model_load: loading model from 'models\\llama_cpp_65b\\ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 8192\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 64\r\nllama_model_load: n_layer = 80\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 22016\r\nllama_model_load: n_parts = 8\r\nllama_model_load: type    = 4\r\nllama_model_load: ggml map size = 38917.99 MB\r\nllama_model_load: ggml ctx size = 201.25 KB\r\nllama_model_load: mem required  = 41478.18 MB (+ 10240.00 MB per state)\r\nllama_model_load: loading tensors from 'models\\llama_cpp_65b\\ggml-model-q4_0.bin'\r\nllama_model_load: model size = 38917.53 MB / num tensors = 723\r\nllama_init_from_file: kv self size  = 2560.00 MB\r\nC:\\Users\\Lucy\\AppData\\Roaming\\Python\\Python39\\site-packages\\gradio\\deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\r\n  warnings.warn(value)\r\nRunning on local URL:  http://127.0.0.1:7860\r\nRunning on public URL: https://f973be860f84965921.gradio.live\r\n\r\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\r\nOutput generated in 71.19 seconds (0.65 tokens/s, 46 tokens, context 30)```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-09T14:54:14+00:00",
    "closed_at": "2023-04-12T12:05:00+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/864/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/864"
  },
  {
    "number": 859,
    "title": "Bug with instruct mode, somehow \"forks\" in background (Windows 11 - Powershell) and makes the shell unuseable",
    "body": "I can not provide a reproducible case but this has happened two times during various tests so there is something fishy.\r\nmain.exe was in Release mode, compiled in VS build tools and executed from Powershell in Win-11.\r\n\r\nWhat happens is that CTRL+C is somehow accepted but the app doesn't stop - it pseudo-forks into background.\r\nI am thrown back to the command prompt of the shell but the shell becomes extremely \"laggy\".\r\nIn one case I was able to issue commands but only every second key was accepted, so I could use the shell but had to repeat all keys twice.\r\nmain.exe waited in \"prompt\" mode I think, so it did not generate anything but waited (never received my input)\r\n\r\nIn the other case only the 'Enter' key was accepted (so the shell created a new line) but no other keys worked\r\nIn that case I also saw a generation happening, it was written into the shell.\r\nSo it was in generation mode but forked in the background.\r\n\r\nIn both cases the shell input was extremely laggy. \r\nI've never seen that happening in Windows, it acted as if I had sent it into bash background using '&' plus extreme shell lag (for no reason, CPU was ok)\r\n\r\nI closed the command shell which did not kill it's child main.exe\r\nThe solution in both cases was killing main.exe manually through the task manager.\r\n\r\nI'm sorry I can't provide any steps to reproduce, in 99% of all cases it works fine. I didn't do anything different than usual.\r\nJust wanted to make you aware about the bug, maybe the description helped.",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-09T00:08:07+00:00",
    "closed_at": "2024-04-11T01:06:47+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/859/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/859"
  },
  {
    "number": 857,
    "title": "Question: How do I use openblas on M1 apple silicon (or is there any point?)",
    "body": "I don't quite how to specify the proper make flag for this or whether it even applies to MacOS.\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-08T23:18:04+00:00",
    "closed_at": "2023-04-09T03:34:43+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/857/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/857"
  },
  {
    "number": 853,
    "title": "I5 13600k or RTX 3060 12gb?",
    "body": "Hello Community\r\nI want to build a computer which will run llama.cpp or text generation web ui.\r\nWhich should I get? Each config is about the same price. \r\nShould I get the 13600k and no gpu (But I can install one in the future if I have money) or a \"bad\" cpu and a rtx 3060 12gb?\r\nWhich should I get / is faster?\r\nThank you in advice.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-08T17:33:12+00:00",
    "closed_at": "2023-04-08T19:20:47+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/853/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/853"
  },
  {
    "number": 852,
    "title": "Running convert-pth-ggml.py on Linux",
    "body": "A not so obvious error, Using Hugging Face llama-13b-hf as the model, I get a 'no such file' for params.json\r\n(Following README, but don't have a params.json.  Are we supposed to build one? Might want to update README for this situation.\r\n\r\npython3.10 convert-pth-to-ggml.py ../llama-13b-hf/ 1",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-08T16:15:51+00:00",
    "closed_at": "2023-04-08T19:23:23+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/852/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/852"
  },
  {
    "number": 849,
    "title": "[ALPACA Q4] assert n_dims in (1, 2) when using migrate-ggml-2023-03-30-pr613.py after convert-gpt4all-to-ggml.py",
    "body": "Hello,\r\n\r\nI was using int Q4 alpaca model, that was converted using convert-gpt4all-to-ggml.py script. It was working perfectly until an update about a week ago (2023-03-30). At the moment I use the repository as of #847.\r\n\r\nWhen I freshly converted the `ggml-alpaca-q4_0.bin` from the scratch using `convert-gpt4all-to-ggml.py`, and try to start the model using resulting model, I get fallowing error:\r\n\r\n```\r\n/models/alpaca/30b/ggml-model-q4_0.bin: invalid model file (bad magic [got 0x67676d66 want 0x67676a74])\r\n\tyou most likely need to regenerate your ggml files\r\n\tthe benefit is you'll get 10-100x faster load times\r\n\tsee https://github.com/ggerganov/llama.cpp/issues/91\r\n\tuse convert-pth-to-ggml.py to regenerate from original pth\r\n\tuse migrate-ggml-2023-03-30-pr613.py if you deleted originals\r\nllama_init_from_file: failed to load model\r\n```\r\n\r\nSo then I proceed with:\r\n```\r\npython migrate-ggml-2023-03-30-pr613.py models/alpaca/30b/ggml-model-q4_0.bin models/alpaca/30b/ggml-model-q4_0.bin_new\r\n```\r\n\r\nTo get fallowing error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/chat/llama.cpp/migrate-ggml-2023-03-30-pr613.py\", line 311, in <module>\r\n    main()\r\n  File \"/home/chat/llama.cpp/migrate-ggml-2023-03-30-pr613.py\", line 306, in main\r\n    copy_tensors(fin, fout, part_id, n_parts)\r\n  File \"/home/chat/llama.cpp/migrate-ggml-2023-03-30-pr613.py\", line 169, in copy_tensors\r\n    assert n_dims in (1, 2)\r\nAssertionError\r\n```\r\n\r\nCan't get rid of the problem, even though the model worked previously!",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-08T11:14:33+00:00",
    "closed_at": "2024-04-11T01:06:48+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/849/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/849"
  },
  {
    "number": 846,
    "title": "llama : add RWKV models support",
    "body": "RWKV (100% RNN) language model, which is the only RNN (as of now) that can match transformers in quality and scaling, while being faster and saves memory.\r\n\r\nInfo: https://github.com/BlinkDL/ChatRWKV\r\n\r\nRWKV is a novel large language model architecture, [with the largest model in the family having 14B parameters](https://huggingface.co/BlinkDL/rwkv-4-pile-14b). In contrast to Transformer with O(n^2) attention, RWKV requires only state from previous step to calculate logits. This makes RWKV very CPU-friendly on large context lenghts.\r\n\r\nExperimental GGML port: https://github.com/saharNooby/rwkv.cpp\r\n\r\nThe lastest \"Raven\"-series Alpaca-style-tuned RWKV 14B & 7B models are very good.\r\nOnline demo: https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B\r\nDownload: https://huggingface.co/BlinkDL/rwkv-4-raven\r\n\r\n----\r\n\r\n*Edit by @ggerganov:*\r\n\r\nAdding @BlinkDL's comment below to OP for visibility:\r\n\r\n> v4 inference: https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py\r\n>\r\n> v5 inference: https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_v5_demo.py\r\n>\r\n> fast v4 & v5.2 inference: https://github.com/BlinkDL/ChatRWKV/blob/main/rwkv_pip_package/src/rwkv/model.py\r\n>\r\n> v5.2 1.5B demo (great for its size): https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio\r\n>\r\n> v5.2 1.5B benchmarks: https://twitter.com/BlinkDL_AI/status/1717543614434402661\r\n>\r\n> a few remarks:\r\n> * rwkv models have RNN-style \"one\" mode, and GPT-style \"seq\" mode\r\n> * i am actually using exp(-exp(w))\r\n> * seems it's good to precompute embedding+emb_layernorm in bf16\r\n> * when using fp16, i am doing /2 every 6 layers, to avoid overflow\r\n",
    "labels": [
      "help wanted",
      "good first issue",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-04-08T06:32:31+00:00",
    "closed_at": "2024-09-01T14:38:18+00:00",
    "comments": 39,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/846/reactions",
      "total_count": 44,
      "+1": 44,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/846"
  },
  {
    "number": 843,
    "title": "Input Chinese Its talk with itself",
    "body": "![image](https://user-images.githubusercontent.com/70996861/230702957-dd4dff6a-35e5-4926-9796-75db84443b5d.png)\r\nWhen I use Chinese \r\nIts will talk with itself",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-08T04:24:28+00:00",
    "closed_at": "2024-04-11T01:06:49+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/843/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/843"
  },
  {
    "number": 842,
    "title": "Performance e-core bug(?) -  only 50% CPU utilization when using all threads -  (Win11, Intel 13900k)",
    "body": "I've not digged deep into this yet but my whole CPU utilization is only at 50%.\r\nI've compiled it with current VS build tools, all default, release mode of course.\r\n\r\nIt might be related to the modern e-cores in Intel CPUs, they pack quite a punch but are weaker than performance cores.\r\nIn the graph it looks like 16 cores (the amount of e-cores) are much more utilized and 8 cores (amount of performance cores) are mostly idle despite using 24 threads. Increasing threads worsens performance, decreasing threads worsens tokens output.\r\n\r\nI tested the small 7B model in 4 bit and 16 bit.\r\nThe only method to get CPU utilization above 50% is by using more than the total physical cores (like 32 cores).\r\nIn this case I see up to 99% CPU utilization but the token performance drops below 2 cores performance, some hyperthreading issue I suppose.\r\nI tried various modes (small/large batch size, context size) It all does not influence it much.\r\n\r\nThe CPU was idle (as seen in screenshot). \r\nAlso memory is not full or swapping either.\r\n\r\nHere is the command line:  `.\\Release\\main.exe  -m .\\models\\7B\\ggml-model-f16.bin -p \"Below I count from 1 to 100000000: 1 2 3 4 5 6\" -c 1024 -t 24 -n 1024 -b 64`\r\n```\r\nsystem_info: n_threads = 24 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 1024, n_batch = 64, n_predict = 1024, n_keep = 0\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/78893154/230700478-abd5763e-636a-457f-9654-69e1d152a154.png)\r\n\r\n\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-08T03:07:10+00:00",
    "closed_at": "2024-04-11T01:06:51+00:00",
    "comments": 24,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/842/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/842"
  },
  {
    "number": 841,
    "title": "How do i stop the ai from talking to itself??",
    "body": "It sometimes just talks to itself for example:\r\n\r\n###Human: Hi\r\n###Assistant: Hello, how can i assist you?\r\n\r\n(i am runing the latest release with vicuna mode)",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-07T23:26:31+00:00",
    "closed_at": "2024-04-11T01:06:52+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/841/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/841"
  },
  {
    "number": 837,
    "title": "[Feature Request] Dawn C++ WebGPU backend",
    "body": "Today Chrome [released](https://developer.chrome.com/blog/webgpu-release/) WebGPU support in Chrome Beta. \r\nThe Google's [Dawn](https://dawn.googlesource.com/dawn) project is a C++ standalone implementation of the WebGPU. It enables support of WebGPU in other libraries, by example this [WIP](https://dawn.googlesource.com/dawn/+/refs/heads/main/src/dawn/node/) are NodeJS binding to Dawn, that would enable - in theory - WebGPU in Node. \r\nSo it should be possible to add Dawn as GPU backend to Llama/GGML C++ math operations.",
    "labels": [
      "enhancement",
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-07T16:10:18+00:00",
    "closed_at": "2024-04-11T01:06:53+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/837/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/837"
  },
  {
    "number": 836,
    "title": "Editing a PR description shouldn't cause a CI run",
    "body": "This seems excessive but I'm not sure how to turn that off (`edited` in `build.yml`?), without inadvertently also removing other triggers (such as a push to the branch the PR wants to merge).\r\n\r\nAlso, other actions such as requesting a review should not trigger a CI run IMO.",
    "labels": [
      "build"
    ],
    "state": "closed",
    "created_at": "2023-04-07T15:22:00+00:00",
    "closed_at": "2023-04-22T13:12:30+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/836/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/836"
  },
  {
    "number": 834,
    "title": "Inconsistent build success across pulls, probably need to track supported compiler and library versions.",
    "body": "Before going on I should note, when it does build, things run just fine, even on an old Xeon X5675.\r\n\r\nI run multiple linux distros and versions, and depending on the changes any given day, AND which distro/version combination, it might build cleanly or spew a ton of errors.  The issue here is most likely with versioning/feature checking, but I'm unsure.  This time it's from regex, but other times it's other portions: \"usr/include/c++/10/bits/regex.tcc:61:26: error:...\"\r\n\r\nI haven't been formally tracking these yet, so the success rates are basically from memory:\r\nA debian 11 box with gcc-10 seems to have about a 99% success rate.\r\nUbuntu 20.04 LTS with gcc-9 or gcc-10 seems to have a 50% success rate\r\nopenSUSE 15.3 with gcc-7 seems to have about a 90% success rate\r\n\r\n\r\n# Expected Behavior\r\n\r\nThe build will succeed, or at very least test if the installed compiler and libraries support the features it needs, and report what's missing.\r\n\r\n# Current Behavior\r\nDepending on the day, a long compiler error, generally not about llama.cpp itself, but about specific c++ features.\r\n\r\n# Environment and Context \r\n\r\n\r\n|Distro|Kernel|Compiler|CPU|\r\n|---|---|---|---|\r\n|Debian 11|5.10.0-21-amd64|gcc version 10.2.1 20210110|Intel(R) Core(TM) i7-10510U CPU @ 1.80GHz|\r\n|Ubuntu 20.04 LTS|5.4.0-135-generic|Ubuntu 9.4.0-1ubuntu1~20.04.1|AMD Ryzen 5 2400G with Radeon Vega Graphics|\r\n|Ubuntu 20.04 LTS|5.4.0-135-generic|Ubuntu 10.3.0-1ubuntu1~20.04|AMD Ryzen 5 2400G with Radeon Vega Graphics|\r\n|openSUSE 15.3|5.3.18-150300.59.106-default|gcc version 7.5.0 SUSE Linux|Intel(R) Xeon(R) CPU           X5675  @ 3.07GHz|\r\n\r\n\r\n# Steps to Reproduce\r\n\r\nRun multiple distros, ``git pull && make clean && make``, on a daily basis\r\n",
    "labels": [
      "build"
    ],
    "state": "closed",
    "created_at": "2023-04-07T13:57:57+00:00",
    "closed_at": "2023-04-21T07:53:55+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/834/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/834"
  },
  {
    "number": 832,
    "title": "Performance degrading over time",
    "body": "# Expected Behavior\r\n\r\nWhen running this command:\r\n\r\n`./main -i --interactive-first -r \"### Human:\" --temp 0 -c 2048 -n -1 --ignore-eos --repeat_penalty 1.2 --threads 4 --instruct -m models/ggml-vicuna-13b-4bit.bin`\r\n\r\nI expect the performance to be the same over time when  the model is answering my questions.\r\n\r\n# Current Behavior\r\n\r\nThe performance is good in the begining, answers are written out fast, 4 cpu cores are fully utilized. But over time speed degrades until it slows down to a word every 30 seconds and cpu cores are just idling.\r\n\r\n# Environment and Context \r\n\r\nApple M1 Mac Mini 16GB RAM. Ventura 13.3.\r\n\r\n```\r\nPython 3.8.13\r\nGNU Make 3.81\r\nApple clang version 14.0.0 (clang-1400.0.29.202)\r\n\r\nnumpy                         1.23.4\r\nrotary-embedding-torch        0.2.1\r\nsentencepiece                 0.1.97\r\ntorch                         2.1.0.dev20230307\r\ntorchaudio                    2.0.0.dev20230307\r\ntorchvision                   0.15.0.dev20230307\r\n```\r\n\r\n\r\n# Steps to Reproduce\r\n\r\nAsk questions for a while. The speed should degrade after about 10 questions that require longer answers.\r\n\r\n\r\n\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-07T12:48:25+00:00",
    "closed_at": "2023-04-07T13:13:10+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/832/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/832"
  },
  {
    "number": 831,
    "title": "Ignores Cyrillic under Win10",
    "body": "The main.exe program simply does not see ru Cyrillic in the standard windows 10 environment, both in cmd.exe and in powershell\r\n\r\nmain.exe of https://github.com/ggerganov/llama.cpp/releases/download/master-cc9cee8/llama-master-cc9cee8-bin-win-avx2-x64.zip\r\nmodel of https://huggingface.co/IlyaGusev/llama_13b_ru_turbo_alpaca_lora_llamacpp/tree/main/13B\r\n\r\n\r\n# Expected Behavior\r\n\r\n_\u0412\u043e\u043f\u0440\u043e\u0441: \u041f\u043e\u0447\u0435\u043c\u0443 \u0442\u0440\u0430\u0432\u0430 \u0437\u0435\u043b\u0435\u043d\u0430\u044f? \r\n\u0412\u044b\u0445\u043e\u0434: \u0422\u0440\u0430\u0432\u0430 \u0437\u0435\u043b\u0435\u043d\u043e\u0439 \u0438\u0437-\u0437\u0430 \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u043e\u043d\u0430 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0445\u043b\u043e\u0440\u043e\u0444\u0438\u043b\u043b\u044b, \u043f\u0438\u0433\u043c\u0435\u043d\u0442\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u043c\u043e\u0433\u0430\u044e\u0442 \u0435\u0439 \u0444\u043e\u0442\u043e\u0441\u0438\u043d\u0442\u0435\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u044d\u043d\u0435\u0440\u0433\u0438\u044e \u0438\u0437 \u0441\u043e\u043b\u043d\u0435\u0447\u043d\u043e\u0433\u043e \u0441\u0432\u0435\u0442\u0430. \u0425\u043b\u043e\u0440\u043e\u0444\u0438\u043b\u043b \u0441\u043f\u043e\u0441\u043e\u0431\u0435\u043d \u043f\u0435\u0440\u0435\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0442\u044c \u0443\u0433\u043b\u0435\u043a\u0438\u0441\u043b\u044b\u0439 \u0433\u0430\u0437 \u0438 \u0432\u043e\u0434\u0443 \u0432 \u043e\u0440\u0433\u0430\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0432\u0435\u0449\u0435\u0441\u0442\u0432\u0430, \u0442\u0430\u043a\u0438\u0435 \u043a\u0430\u043a \u0443\u0433\u043b\u0435\u0432\u043e\u0434\u044b, \u0430\u043c\u0438\u043d\u043e\u043a\u0438\u0441\u043b\u043e\u0442\u044b \u0438 \u0436\u0438\u0440\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b \u0440\u0430\u0441\u0442\u0435\u043d\u0438\u044f\u043c \u0434\u043b\u044f \u0438\u0445 \u0440\u043e\u0441\u0442\u0430 \u0438 \u0440\u0430\u0437\u0432\u0438\u0442\u0438\u044f._\r\n\r\n\r\n# Current Behavior\r\n\r\nPS N:\\NLP_MODEL> .\\llama-master-cc9cee8-bin-win-avx2-x64\\main.exe -m .\\llama_13b_ru_turbo_alpaca_lora_llamacpp\\ggml-model-q4_0.bin -p \"\u0412\u043e\u043f\u0440\u043e\u0441: \u041f\u043e\u0447\u0435\u043c\u0443 \u0442\u0440\u0430\u0432\u0430 \u0437\u0435\u043b\u0435\u043d\u0430\u044f? \u041e\u0442\u0432\u0435\u0442:\" -n 512 --temp 0.1\r\nmain: seed = 1680869612\r\nllama_model_load: loading model from '.\\llama_13b_ru_turbo_alpaca_lora_llamacpp\\ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 5120\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 40\r\nllama_model_load: n_layer = 40\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 13824\r\nllama_model_load: n_parts = 2\r\nllama_model_load: type    = 2\r\nllama_model_load: ggml map size = 7759.83 MB\r\nllama_model_load: ggml ctx size = 101.25 KB\r\nllama_model_load: mem required  = 9807.93 MB (+ 1608.00 MB per state)\r\nllama_model_load: loading tensors from '.\\llama_13b_ru_turbo_alpaca_lora_llamacpp\\ggml-model-q4_0.bin'\r\nllama_model_load: model size =  7759.39 MB / num tensors = 363\r\nllama_init_from_file: kv self size  =  400.00 MB\r\n\r\nsystem_info: n_threads = 4 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\r\nsampling: temp = 0.100000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 512, n_batch = 8, n_predict = 512, n_keep = 0\r\n\r\n :   ? : \ud83c\udfc3\r\n\r\n The answer is \"Yes\". [end of text]\r\n\r\nllama_print_timings:        load time =  6288.63 ms\r\nllama_print_timings:      sample time =     7.36 ms /    14 runs   (    0.53 ms per run)\r\nllama_print_timings: prompt eval time = 16411.67 ms /    38 tokens (  431.89 ms per token)\r\nllama_print_timings:        eval time =  6213.46 ms /    13 runs   (  477.96 ms per run)\r\nllama_print_timings:       total time = 24098.30 ms\r\n\r\n# Environment and Context \r\nOS: Win10 \r\nCPU: XEON E5-2640v3\r\nMemory: 16GB ",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-07T12:41:30+00:00",
    "closed_at": "2023-04-07T13:15:22+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/831/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/831"
  },
  {
    "number": 830,
    "title": "Intermittent segmentation faults in llama_sample_top_p_top_k()",
    "body": "# Expected Behavior\r\n\r\nI have been getting intermittent segfaults for no apparent reason. Sometimes they occur right at the beginning of text generation, and sometimes they occur after a lot of text has already been generated. They seem to be deterministic in that I can sometimes work around them by changing the prompt, but if I don\u2019t change the prompt, they consistently occur. I normally use the 65B model, which exhibits the problem, but I am attaching a repro for the 13B model. I am not 100% sure but I believe the issue affects all four model sizes (7B, 13B, 30B, 65B).\r\n\r\n# Current Behavior\r\n\r\nIntermittent segfaults\r\n\r\n# Environment and Context \r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n2019 16-inch MacBook Pro, 2.3 GHz 8-Core Intel Core i9, 64 GB of RAM\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`$ uname -a`\r\n`Darwin Ryans-MBP-2.lan 22.3.0 Darwin Kernel Version 22.3.0: Mon Jan 30 20:42:11 PST 2023; root:xnu-8792.81.3~2/RELEASE_X86_64 x86_64`\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version\r\n$ make --version\r\n$ g++ --version\r\n```\r\n```\r\nPython 3.10.0\r\n\r\nGNU Make 3.81\r\nCopyright (C) 2006  Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.\r\nThere is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A\r\nPARTICULAR PURPOSE.\r\n\r\nThis program built for i386-apple-darwin11.3.0\r\n\r\nApple clang version 14.0.3 (clang-1403.0.22.14.1)\r\nTarget: x86_64-apple-darwin22.3.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n```\r\n# Failure Information (for bugs)\r\n\r\nSee log below\r\n\r\n# Steps to Reproduce\r\n\r\nI can consistently reproduce on my machine by running the following command:\r\n```\r\n ./main --ctx_size 2048 -m ./models/13B/ggml-model-q4_0.bin --top_p 0 --top_k \r\nrlanday@Ryans-MBP-2 llama.cpp % ./main --ctx_size 2048 -m ./models/13B/ggml-model-q4_0.bin --top_p 0 --top_k 40 --temp 0.7 --repeat_penalty 1.176470588235294 -t 8 -n -1 --repeat_last_n 16384 -p \"Active Internet connections\" -s 1680491962\r\n```\r\n\r\n# Failure Logs\r\n\r\nEnvironment info:\r\n```\r\ncommit cc9cee8e9e7598bd280295f6264f36d3a9224006\r\n\r\nrlanday@Ryans-MBP-2 ~ % sysctl -a | grep machdep.cpu\r\nmachdep.cpu.tlb.inst.large: 8\r\nmachdep.cpu.tlb.data.small: 64\r\nmachdep.cpu.tlb.data.small_level1: 64\r\nmachdep.cpu.address_bits.physical: 39\r\nmachdep.cpu.address_bits.virtual: 48\r\nmachdep.cpu.tsc_ccc.numerator: 192\r\nmachdep.cpu.tsc_ccc.denominator: 2\r\nmachdep.cpu.mwait.linesize_min: 64\r\nmachdep.cpu.mwait.linesize_max: 64\r\nmachdep.cpu.mwait.extensions: 3\r\nmachdep.cpu.mwait.sub_Cstates: 286531872\r\nmachdep.cpu.thermal.sensor: 1\r\nmachdep.cpu.thermal.dynamic_acceleration: 1\r\nmachdep.cpu.thermal.invariant_APIC_timer: 1\r\nmachdep.cpu.thermal.thresholds: 2\r\nmachdep.cpu.thermal.ACNT_MCNT: 1\r\nmachdep.cpu.thermal.core_power_limits: 1\r\nmachdep.cpu.thermal.fine_grain_clock_mod: 1\r\nmachdep.cpu.thermal.package_thermal_intr: 1\r\nmachdep.cpu.thermal.hardware_feedback: 0\r\nmachdep.cpu.thermal.energy_policy: 1\r\nmachdep.cpu.xsave.extended_state: 31 832 1088 0\r\nmachdep.cpu.xsave.extended_state1: 15 832 256 0\r\nmachdep.cpu.arch_perf.version: 4\r\nmachdep.cpu.arch_perf.number: 4\r\nmachdep.cpu.arch_perf.width: 48\r\nmachdep.cpu.arch_perf.events_number: 7\r\nmachdep.cpu.arch_perf.events: 0\r\nmachdep.cpu.arch_perf.fixed_number: 3\r\nmachdep.cpu.arch_perf.fixed_width: 48\r\nmachdep.cpu.cache.linesize: 64\r\nmachdep.cpu.cache.L2_associativity: 4\r\nmachdep.cpu.cache.size: 256\r\nmachdep.cpu.max_basic: 22\r\nmachdep.cpu.max_ext: 2147483656\r\nmachdep.cpu.vendor: GenuineIntel\r\nmachdep.cpu.brand_string: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\r\nmachdep.cpu.family: 6\r\nmachdep.cpu.model: 158\r\nmachdep.cpu.extmodel: 9\r\nmachdep.cpu.extfamily: 0\r\nmachdep.cpu.stepping: 13\r\nmachdep.cpu.feature_bits: 9221960262849657855\r\nmachdep.cpu.leaf7_feature_bits: 43804591 1073741824\r\nmachdep.cpu.leaf7_feature_bits_edx: 3154120192\r\nmachdep.cpu.extfeature_bits: 1241984796928\r\nmachdep.cpu.signature: 591597\r\nmachdep.cpu.brand: 0\r\nmachdep.cpu.features: FPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE SSE3 PCLMULQDQ DTES64 MON DSCPL VMX SMX EST TM2 SSSE3 FMA CX16 TPR PDCM SSE4.1 SSE4.2 x2APIC MOVBE POPCNT AES PCID XSAVE OSXSAVE SEGLIM64 TSCTMR AVX1.0 RDRAND F16C\r\nmachdep.cpu.leaf7_features: RDWRFSGS TSC_THREAD_OFFSET SGX BMI1 AVX2 SMEP BMI2 ERMS INVPCID FPU_CSDS MPX RDSEED ADX SMAP CLFSOPT IPT SGXLC MDCLEAR IBRS STIBP L1DF ACAPMSR SSBD\r\nmachdep.cpu.extfeatures: SYSCALL XD 1GBPAGE EM64T LAHF LZCNT PREFETCHW RDTSCP TSCI\r\nmachdep.cpu.logical_per_package: 16\r\nmachdep.cpu.cores_per_package: 8\r\nmachdep.cpu.microcode_version: 244\r\nmachdep.cpu.processor_flag: 5\r\nmachdep.cpu.core_count: 8\r\nmachdep.cpu.thread_count: 16\r\n\r\nllama.cpp$ pip list | egrep \"torch|numpy|sentencepiece\"\r\nnumpy                        1.22.2\r\nsentencepiece                0.1.97\r\ntorch                        1.13.0\r\n\r\nllama.cpp$ make --version | head -1\r\nGNU Make 4.3\r\n\r\n$ md5sum ./models/13B/ggml-model-q4_0.bin\r\n0abc81985f6c529faaa661dee3674efd  ./models/13B/ggml-model-q4_0.bin\r\n```\r\n\r\nHere is an ASAN output:\r\n\r\n```\r\nrlanday@Ryans-MBP-2 llama.cpp % ./main --ctx_size 2048 -m ./models/13B/ggml-model-q4_0.bin --top_p 0 --top_k 40 --temp 0.7 --repeat_penalty 1.176470588235294 -t 8 -n -1 --repeat_last_n 16384 -p \"Active Internet connections\" -s 1680491962\r\nmain(30917,0x7ff844413680) malloc: nano zone abandoned due to inability to preallocate reserved vm space.\r\nmain: seed = 1680491962\r\nllama_model_load: loading model from './models/13B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 2048\r\nllama_model_load: n_embd  = 5120\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 40\r\nllama_model_load: n_layer = 40\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 13824\r\nllama_model_load: n_parts = 2\r\nllama_model_load: type    = 2\r\nllama_model_load: ggml map size = 7759.83 MB\r\nllama_model_load: ggml ctx size = 101.25 KB\r\nllama_model_load: mem required  = 9807.93 MB (+ 1608.00 MB per state)\r\nllama_model_load: loading tensors from './models/13B/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  7759.39 MB / num tensors = 363\r\nllama_init_from_file: kv self size  = 1600.00 MB\r\n\r\nsystem_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \r\nsampling: temp = 0.700000, top_k = 40, top_p = 0.000000, repeat_last_n = 16384, repeat_penalty = 1.176471\r\ngenerate: n_ctx = 2048, n_batch = 8, n_predict = -1, n_keep = 0\r\n\r\n\r\n Active Internet connections=================================================================\r\n==30917==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x625000000000 at pc 0x000107c8c2e1 bp 0x7ff7b8a76690 sp 0x7ff7b8a75e58\r\nREAD of size 65536 at 0x625000000000 thread T0\r\n    #0 0x107c8c2e0 in __asan_memmove+0xe0 (libclang_rt.asan_osx_dynamic.dylib:x86_64h+0x472e0) (BuildId: 756bb7515781379f84412f22c4274ffd2400000010000000000a0a0000030d00)\r\n    #1 0x107499506 in std::__1::pair<int const*, int*> std::__1::__copy_impl[abi:v15006]<int const, int, void>(int const*, int const*, int*) copy.h:56\r\n    #2 0x107498d42 in std::__1::pair<int const*, int*> std::__1::__copy[abi:v15006]<int const*, int const*, int*, 0>(int const*, int const*, int*) copy.h:94\r\n    #3 0x107498958 in int* std::__1::copy[abi:v15006]<int const*, int*>(int const*, int const*, int*) copy.h:103\r\n    #4 0x107498868 in int* std::__1::__uninitialized_allocator_copy[abi:v15006]<std::__1::allocator<int>, int, int, (void*)0>(std::__1::allocator<int>&, int const*, int const*, int*) uninitialized_algorithms.h:575\r\n    #5 0x1074985e4 in std::__1::enable_if<__is_cpp17_forward_iterator<int const*>::value, void>::type std::__1::vector<int, std::__1::allocator<int> >::__construct_at_end<int const*>(int const*, int const*, unsigned long) vector:1031\r\n    #6 0x10760a283 in std::__1::vector<int, std::__1::allocator<int> >::vector<int const*>(int const*, std::__1::enable_if<(__is_cpp17_forward_iterator<int const*>::value) && (is_constructible<int, std::__1::iterator_traits<int const*>::reference>::value), int const*>::type) vector:1158\r\n    #7 0x10751cdd4 in std::__1::vector<int, std::__1::allocator<int> >::vector<int const*>(int const*, std::__1::enable_if<(__is_cpp17_forward_iterator<int const*>::value) && (is_constructible<int, std::__1::iterator_traits<int const*>::reference>::value), int const*>::type) vector:1152\r\n    #8 0x10751cb73 in llama_sample_top_p_top_k llama.cpp:1808\r\n    #9 0x10748cd9d in main main.cpp:292\r\n    #10 0x7ff8007a330f in start+0x97f (dyld:x86_64+0xfffffffffff7230f) (BuildId: bba777096cad3592ab0309d0f7b8610e32000000200000000100000000020d00)\r\n\r\n0x625000000000 is located 256 bytes to the left of 8192-byte region [0x625000000100,0x625000002100)\r\nfreed by thread T0 here:\r\n    #0 0x107c9d17d in wrap__ZdlPv+0x7d (libclang_rt.asan_osx_dynamic.dylib:x86_64h+0x5817d) (BuildId: 756bb7515781379f84412f22c4274ffd2400000010000000000a0a0000030d00)\r\n    #1 0x107494d84 in void std::__1::__libcpp_operator_delete[abi:v15006]<void*>(void*) new:256\r\n    #2 0x107494d68 in void std::__1::__do_deallocate_handle_size[abi:v15006]<>(void*, unsigned long) new:280\r\n    #3 0x107494d40 in std::__1::__libcpp_deallocate[abi:v15006](void*, unsigned long, unsigned long) new:290\r\n    #4 0x107545189 in std::__1::allocator<float>::deallocate[abi:v15006](float*, unsigned long) allocator.h:128\r\n    #5 0x107544e34 in std::__1::allocator_traits<std::__1::allocator<float> >::deallocate[abi:v15006](std::__1::allocator<float>&, float*, unsigned long) allocator_traits.h:282\r\n    #6 0x1075e9162 in std::__1::__split_buffer<float, std::__1::allocator<float>&>::~__split_buffer() __split_buffer:355\r\n    #7 0x1075e4dd4 in std::__1::__split_buffer<float, std::__1::allocator<float>&>::~__split_buffer() __split_buffer:352\r\n    #8 0x10760952f in std::__1::vector<float, std::__1::allocator<float> >::__append(unsigned long) vector:1051\r\n    #9 0x107514c6f in std::__1::vector<float, std::__1::allocator<float> >::resize(unsigned long) vector:1918\r\n    #10 0x10751ba07 in llama_eval_internal(llama_context&, int const*, int, int, int) llama.cpp:982\r\n    #11 0x107519a90 in llama_eval llama.cpp:1727\r\n    #12 0x10748c7ec in main main.cpp:267\r\n    #13 0x7ff8007a330f in start+0x97f (dyld:x86_64+0xfffffffffff7230f) (BuildId: bba777096cad3592ab0309d0f7b8610e32000000200000000100000000020d00)\r\n\r\npreviously allocated by thread T0 here:\r\n    #0 0x107c9cd5d in wrap__Znwm+0x7d (libclang_rt.asan_osx_dynamic.dylib:x86_64h+0x57d5d) (BuildId: 756bb7515781379f84412f22c4274ffd2400000010000000000a0a0000030d00)\r\n    #1 0x1074974e4 in void* std::__1::__libcpp_operator_new[abi:v15006]<unsigned long>(unsigned long) new:246\r\n    #2 0x1074974c8 in std::__1::__libcpp_allocate[abi:v15006](unsigned long, unsigned long) new:272\r\n    #3 0x1075e591c in std::__1::allocator<float>::allocate[abi:v15006](unsigned long) allocator.h:112\r\n    #4 0x1075e56ba in std::__1::__allocation_result<std::__1::allocator_traits<std::__1::allocator<float> >::pointer> std::__1::__allocate_at_least[abi:v15006]<std::__1::allocator<float> >(std::__1::allocator<float>&, unsigned long) allocate_at_least.h:54\r\n    #5 0x1075e52c3 in std::__1::__split_buffer<float, std::__1::allocator<float>&>::__split_buffer(unsigned long, unsigned long, std::__1::allocator<float>&) __split_buffer:316\r\n    #6 0x1075e46dc in std::__1::__split_buffer<float, std::__1::allocator<float>&>::__split_buffer(unsigned long, unsigned long, std::__1::allocator<float>&) __split_buffer:312\r\n    #7 0x107514adb in std::__1::vector<float, std::__1::allocator<float> >::reserve(unsigned long) vector:1500\r\n    #8 0x10750c92e in llama_init_from_file llama.cpp:1652\r\n    #9 0x107489ae9 in main main.cpp:102\r\n    #10 0x7ff8007a330f in start+0x97f (dyld:x86_64+0xfffffffffff7230f) (BuildId: bba777096cad3592ab0309d0f7b8610e32000000200000000100000000020d00)\r\n\r\nSUMMARY: AddressSanitizer: heap-buffer-overflow (libclang_rt.asan_osx_dynamic.dylib:x86_64h+0x472e0) (BuildId: 756bb7515781379f84412f22c4274ffd2400000010000000000a0a0000030d00) in __asan_memmove+0xe0\r\nShadow bytes around the buggy address:\r\n  0x1c49ffffffb0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\r\n  0x1c49ffffffc0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\r\n  0x1c49ffffffd0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\r\n  0x1c49ffffffe0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\r\n  0x1c49fffffff0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\r\n=>0x1c4a00000000:[fa]fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x1c4a00000010: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x1c4a00000020: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd\r\n  0x1c4a00000030: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd\r\n  0x1c4a00000040: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd\r\n  0x1c4a00000050: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd\r\nShadow byte legend (one shadow byte represents 8 application bytes):\r\n  Addressable:           00\r\n  Partially addressable: 01 02 03 04 05 06 07 \r\n  Heap left redzone:       fa\r\n  Freed heap region:       fd\r\n  Stack left redzone:      f1\r\n  Stack mid redzone:       f2\r\n  Stack right redzone:     f3\r\n  Stack after return:      f5\r\n  Stack use after scope:   f8\r\n  Global redzone:          f9\r\n  Global init order:       f6\r\n  Poisoned by user:        f7\r\n  Container overflow:      fc\r\n  Array cookie:            ac\r\n  Intra object redzone:    bb\r\n  ASan internal:           fe\r\n  Left alloca redzone:     ca\r\n  Right alloca redzone:    cb\r\n==30917==ABORTING\r\nzsh: abort      ./main --ctx_size 2048 -m ./models/13B/ggml-model-q4_0.bin --top_p 0 --top_k \r\n```\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-07T12:33:14+00:00",
    "closed_at": "2024-04-11T01:06:54+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/830/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/830"
  },
  {
    "number": 829,
    "title": "Port to Google Tensor G2/G3 on Pixel Phones",
    "body": "It would be nice to use the TPU in those SoCs to improve speed",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-07T10:57:33+00:00",
    "closed_at": "2024-05-20T01:09:08+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/829/reactions",
      "total_count": 16,
      "+1": 16,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/829"
  },
  {
    "number": 828,
    "title": "Do not recreate context while LLama is writing",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nTokens are generated at about a constant rate, ie. N tokens per second on a given machine.\r\n\r\n# Current Behavior\r\n\r\nSometimes, the LLM takes a much longer time to generate a token than usually. It can be a 10x slowdown.\r\n\r\n# Environment and Context \r\n\r\n**Setup**\r\nMacBook Pro 14-inch 2021\r\n10-core Apple M1 Pro CPU\r\n16 GB RAM\r\n**OS**\r\nMacOS Ventura 13.3 (22E252)\r\n**clang --version**\r\n```\r\nApple clang version 14.0.3 (clang-1403.0.22.14.1)\r\nTarget: arm64-apple-darwin22.4.0\r\nThread model: posix\r\n```\r\n\r\n# Steps to Reproduce\r\n\r\nRun `./main` `-m` [`./models/ggml-vicuna-7b-4bit-rev1.bin`](https://huggingface.co/eachadea/ggml-vicuna-7b-4bit/resolve/main/ggml-vicuna-7b-4bit-rev1.bin) `-n` `512` `--color` `-f` [`prompts/chat-with-vicuna.txt`](https://github.com/ggerganov/llama.cpp/files/11157908/chat-with-vicuna.txt) `--seed` `42` [`--mlock`](https://github.com/ggerganov/llama.cpp/issues/767)\r\n\r\nThe model will get stuck after \"of\": `...or visit one of\u258f` _the city's many restaurants..._\r\n\r\n# Failure Logs\r\n\r\n```\r\nmain: seed = 42\r\nllama_model_load: loading model from './models/ggml-vicuna-7b-4bit-rev1.bin' - please wait ...\r\nllama_model_load: n_vocab = 32001\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml map size = 4017.70 MB\r\nllama_model_load: ggml ctx size =  81.25 KB\r\nllama_model_load: mem required  = 5809.78 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading tensors from './models/ggml-vicuna-7b-4bit-rev1.bin'\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 4 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |\r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 512, n_batch = 8, n_predict = 512, n_keep = 0\r\n\r\n\r\n A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\r\n\r\n### Human: Hello, Assistant.\r\n### Assistant: Hello. How may I help you today?\r\n### Human: Please tell me the largest city in Europe.\r\n### Assistant: Sure. The largest city in Europe is Moscow, the capital of Russia.\r\n### Human: Write a description of it for tourists\r\n### Assistant: Welcome to Moscow! Located in the heart of Europe, this vibrant city is full of history, culture, and things to do. Whether you're interested in art, architecture, or outdoor activities, there's something for everyone in Moscow. Start your visit at Red Square, home to some of the city's most famous landmarks, including St. Basil's Cathedral and the Kremlin. Take a stroll through the historic neighborhood of Kitai-Gorod, where you can find plenty of shops and restaurants. Visit the Tretyakov Gallery to see some of Russia's most famous artwork, or take a trip to the outskirts of the city to explore the beautiful parks and gardens. Don't forget to try some of Moscow's delicious local cuisine, including borscht (beet soup) and balalaika (a type of stringed instrument).\r\n### Human: No, please write about Amsterdam as if you are a tourist guide\r\n### Assistant: Welcome to Amsterdam, the vibrant capital of the Netherlands! Known for its iconic canals, bustling nightlife, and liberal culture, this city is a must-visit destination for any traveler. Start your visit at Dam Square, home to some of Amsterdam's most famous landmarks, including the Royal Palace, the National Monument, and the New Church. Take a stroll along the canals, which are lined with charming homes, cafes, and shops. Visit the Van Gogh Museum to see the largest collection of Vincent van Gogh's paintings and letters in the world. Or take a boat tour of the city's many canals and historical sites. Don't forget to try some of Amsterdam's famous street food, such as pancakes and waffles, or visit one of the city's many restaurants for a taste of the local cuisine. Amsterdam is also known for its lively nightlife, with trendy bars, clubs, and coffee shops galore. Don't be afraid to explore the city's Red Light District, a colorful and historic area that has been a part of Amsterdam since the Middle Ages. Overall, Amsterdam is a fascinating and unique destination that offers something for everyone.\r\n### Human: No, please write about New York City as if\r\nllama_print_timings:        load time = 22203.17 ms\r\nllama_print_timings:      sample time =   372.37 ms /   512 runs   (    0.73 ms per run)\r\nllama_print_timings: prompt eval time = 14223.05 ms /   368 tokens (   38.65 ms per token)\r\nllama_print_timings:        eval time = 43922.75 ms /   510 runs   (   86.12 ms per run)\r\nllama_print_timings:       total time = 80063.80 ms\r\n```\r\n\r\n# Video\r\n\r\n![ezgif com-optimize](https://user-images.githubusercontent.com/13035076/230586258-190409e1-2415-4747-85e4-ad55b82c7a66.gif)\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-07T09:41:24+00:00",
    "closed_at": "2024-04-11T01:06:55+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/828/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/828"
  },
  {
    "number": 827,
    "title": "[Question] Save internal state to disk",
    "body": "Hi,\r\n\r\nThanks for your hard work on this project.\r\n\r\nI've been playing with the code since few days. I'm trying to find a way to save the internal state of the model (or its context) that can be reused later. But until now it still doesn't work. I don't know if I'm missing something (I'm not good at all when talking about machine learning, I'm working more on system development.)\r\n\r\nHere what I tried (inspired from code of `main.cpp`)\r\n\r\n1. Load the model using `llama_init_from_file`\r\n2. Call `llama_eval` on a prompt, for example `\" Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"`\r\n3. Call `llama_eval` on a instruction, for example `\"### Instruction: Hi, I'm Xuan Son.\"`\r\n4. Save the `kv_self` buffer using `llama_get_kv_cache` => Expected: the saved data contains information about my name\r\n5. Ask for `what is my name?`, the model correctly response that my name is Xuan Son\r\n6. Exit the program\r\n7. Re-run the program\r\n8. Reload `llama_init_from_file` then `kv_cache` using `llama_set_kv_cache` => Expected: the loaded data contains information about my name\r\n9. Ask for `what is my name?`, the model responses with nonsense words `just love with 12 want some you, he saids `",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-07T08:15:24+00:00",
    "closed_at": "2023-04-08T12:21:54+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/827/reactions",
      "total_count": 4,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 4
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/827"
  },
  {
    "number": 826,
    "title": "problem with the ggml files when using the docker images",
    "body": "I have a problem with the ggml images when I use the docker image ghcr.io/ggerganov/llama.cpp:full. I am asked to regenerate your ggml files but I have already done that. See attached screenshot.\r\n![Screenshot from 2023-04-07 08-47-47](https://user-images.githubusercontent.com/35671475/230561232-9056fb60-6886-4a1f-b8f3-b5c1b7a51d86.png)\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-07T07:16:21+00:00",
    "closed_at": "2024-04-11T01:06:56+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/826/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/826"
  },
  {
    "number": 823,
    "title": "[User] Insert summary of your issue or enhancement..",
    "body": "# Prerequisites\n\nPlease answer the following questions for yourself before submitting an issue.\n\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\n- [ ] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [ ] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\n- [ ] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\n\n# Expected Behavior\n\nPlease provide a detailed written description of what you were trying to do, and what you expected `llama.cpp` to do.\n\n# Current Behavior\n\nPlease provide a detailed written description of what `llama.cpp` did, instead. \n\n# Environment and Context \n\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\n\n* Physical (or virtual) hardware you are using, e.g. for Linux:\n\n`$ lscpu`\n\n* Operating System, e.g. for Linux:\n\n`$ uname -a`\n\n* SDK version, e.g. for Linux:\n\n```\n$ python3 --version\n$ make --version\n$ g++ --version\n```\n\n# Failure Information (for bugs)\n\nPlease help provide information about the failure if this is a bug. If it is not a bug, please remove the rest of this template.\n\n# Steps to Reproduce\n\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\n\n1. step 1\n2. step 2\n3. step 3\n4. etc.\n\n# Failure Logs\n\nPlease include any relevant log snippets or files. If it works under one configuration but not under another, please provide logs for both configurations and their corresponding outputs so it is easy to see where behavior changes.\n\nAlso, please try to **avoid using screenshots** if at all possible. Instead, copy/paste the console output and use [Github's markdown](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) to cleanly format your logs for easy readability.\n\nExample environment info:\n```\nllama.cpp$ git log | head -1\ncommit 2af23d30434a677c6416812eea52ccc0af65119c\n\nllama.cpp$ lscpu | egrep \"AMD|Flags\"\nVendor ID:                       AuthenticAMD\nModel name:                      AMD Ryzen Threadripper 1950X 16-Core Processor\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sme sev\nVirtualization:                  AMD-V\n\nllama.cpp$ python3 --version\nPython 3.10.9\n\nllama.cpp$ pip list | egrep \"torch|numpy|sentencepiece\"\nnumpy                         1.24.2\nnumpydoc                      1.5.0\nsentencepiece                 0.1.97\ntorch                         1.13.1\ntorchvision                   0.14.1\n\nllama.cpp$ make --version | head -1\nGNU Make 4.3\n\n$ md5sum ./models/65B/ggml-model-q4_0.bin\ndbdd682cce80e2d6e93cefc7449df487  ./models/65B/ggml-model-q4_0.bin\n```\n\nExample run with the Linux command [perf](https://www.brendangregg.com/perf.html)\n```\nllama.cpp$ perf stat ./main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p \"Please close your issue when it has been answered.\"\nmain: seed = 1679149377\nllama_model_load: loading model from './models/65B/ggml-model-q4_0.bin' - please wait ...\nllama_model_load: n_vocab = 32000\nllama_model_load: n_ctx   = 512\nllama_model_load: n_embd  = 8192\nllama_model_load: n_mult  = 256\nllama_model_load: n_head  = 64\nllama_model_load: n_layer = 80\nllama_model_load: n_rot   = 128\nllama_model_load: f16     = 2\nllama_model_load: n_ff    = 22016\nllama_model_load: n_parts = 8\nllama_model_load: ggml ctx size = 41477.73 MB\nllama_model_load: memory_size =  2560.00 MB, n_mem = 40960\nllama_model_load: loading model part 1/8 from './models/65B/ggml-model-q4_0.bin'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 2/8 from './models/65B/ggml-model-q4_0.bin.1'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 3/8 from './models/65B/ggml-model-q4_0.bin.2'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 4/8 from './models/65B/ggml-model-q4_0.bin.3'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 5/8 from './models/65B/ggml-model-q4_0.bin.4'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 6/8 from './models/65B/ggml-model-q4_0.bin.5'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 7/8 from './models/65B/ggml-model-q4_0.bin.6'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 8/8 from './models/65B/ggml-model-q4_0.bin.7'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\n\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n\nmain: prompt: 'Please close your issue when it has been answered.'\nmain: number of tokens in prompt = 11\n     1 -> ''\n 12148 -> 'Please'\n  3802 -> ' close'\n   596 -> ' your'\n  2228 -> ' issue'\n   746 -> ' when'\n   372 -> ' it'\n   756 -> ' has'\n  1063 -> ' been'\n  7699 -> ' answered'\n 29889 -> '.'\n\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\n\n\nPlease close your issue when it has been answered.\n@duncan-donut: I'm trying to figure out what kind of \"support\" you need for this script and why, exactly? Is there a question about how the code works that hasn't already been addressed in one or more comments below this ticket, or are we talking something else entirely like some sorta bugfixing job because your server setup is different from mine??\nI can understand if your site needs to be running smoothly and you need help with a fix of sorts but there should really be nothing wrong here that the code itself could not handle. And given that I'm getting reports about how it works perfectly well on some other servers, what exactly are we talking? A detailed report will do wonders in helping us get this resolved for ya quickly so please take your time and describe the issue(s) you see as clearly & concisely as possible!!\n@duncan-donut: I'm not sure if you have access to cPanel but you could try these instructions. It is worth a shot! Let me know how it goes (or what error message, exactly!) when/if ya give that code a go? [end of text]\n\n\nmain: mem per token = 71159620 bytes\nmain:     load time = 19309.95 ms\nmain:   sample time =   168.62 ms\nmain:  predict time = 223895.61 ms / 888.47 ms per token\nmain:    total time = 246406.42 ms\n\n Performance counter stats for './main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p Please close your issue when it has been answered.':\n\n        3636882.89 msec task-clock                #   14.677 CPUs utilized          \n             13509      context-switches          #    3.714 /sec                   \n              2436      cpu-migrations            #    0.670 /sec                   \n          10476679      page-faults               #    2.881 K/sec                  \n    13133115082869      cycles                    #    3.611 GHz                      (16.77%)\n       29314462753      stalled-cycles-frontend   #    0.22% frontend cycles idle     (16.76%)\n    10294402631459      stalled-cycles-backend    #   78.39% backend cycles idle      (16.74%)\n    23479217109614      instructions              #    1.79  insn per cycle         \n                                                  #    0.44  stalled cycles per insn  (16.76%)\n     2353072268027      branches                  #  647.002 M/sec                    (16.77%)\n        1998682780      branch-misses             #    0.08% of all branches          (16.76%)\n\n     247.802177522 seconds time elapsed\n\n    3618.573072000 seconds user\n      18.491698000 seconds sys\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-06T23:24:03+00:00",
    "closed_at": "2023-04-06T23:48:12+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/823/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/823"
  },
  {
    "number": 822,
    "title": "[User] Insert summary of your issue or enhancement..",
    "body": "# Prerequisites\n\nPlease answer the following questions for yourself before submitting an issue.\n\n- [ ] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\n- [ ] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [ ] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\n- [ ] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\n\n# Expected Behavior\n\nPlease provide a detailed written description of what you were trying to do, and what you expected `llama.cpp` to do.\n\n# Current Behavior\n\nPlease provide a detailed written description of what `llama.cpp` did, instead. \n\n# Environment and Context \n\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\n\n* Physical (or virtual) hardware you are using, e.g. for Linux:\n\n`$ lscpu`\n\n* Operating System, e.g. for Linux:\n\n`$ uname -a`\n\n* SDK version, e.g. for Linux:\n\n```\n$ python3 --version\n$ make --version\n$ g++ --version\n```\n\n# Failure Information (for bugs)\n\nPlease help provide information about the failure if this is a bug. If it is not a bug, please remove the rest of this template.\n\n# Steps to Reproduce\n\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\n\n1. step 1\n2. step 2\n3. step 3\n4. etc.\n\n# Failure Logs\n\nPlease include any relevant log snippets or files. If it works under one configuration but not under another, please provide logs for both configurations and their corresponding outputs so it is easy to see where behavior changes.\n\nAlso, please try to **avoid using screenshots** if at all possible. Instead, copy/paste the console output and use [Github's markdown](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) to cleanly format your logs for easy readability.\n\nExample environment info:\n```\nllama.cpp$ git log | head -1\ncommit 2af23d30434a677c6416812eea52ccc0af65119c\n\nllama.cpp$ lscpu | egrep \"AMD|Flags\"\nVendor ID:                       AuthenticAMD\nModel name:                      AMD Ryzen Threadripper 1950X 16-Core Processor\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sme sev\nVirtualization:                  AMD-V\n\nllama.cpp$ python3 --version\nPython 3.10.9\n\nllama.cpp$ pip list | egrep \"torch|numpy|sentencepiece\"\nnumpy                         1.24.2\nnumpydoc                      1.5.0\nsentencepiece                 0.1.97\ntorch                         1.13.1\ntorchvision                   0.14.1\n\nllama.cpp$ make --version | head -1\nGNU Make 4.3\n\n$ md5sum ./models/65B/ggml-model-q4_0.bin\ndbdd682cce80e2d6e93cefc7449df487  ./models/65B/ggml-model-q4_0.bin\n```\n\nExample run with the Linux command [perf](https://www.brendangregg.com/perf.html)\n```\nllama.cpp$ perf stat ./main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p \"Please close your issue when it has been answered.\"\nmain: seed = 1679149377\nllama_model_load: loading model from './models/65B/ggml-model-q4_0.bin' - please wait ...\nllama_model_load: n_vocab = 32000\nllama_model_load: n_ctx   = 512\nllama_model_load: n_embd  = 8192\nllama_model_load: n_mult  = 256\nllama_model_load: n_head  = 64\nllama_model_load: n_layer = 80\nllama_model_load: n_rot   = 128\nllama_model_load: f16     = 2\nllama_model_load: n_ff    = 22016\nllama_model_load: n_parts = 8\nllama_model_load: ggml ctx size = 41477.73 MB\nllama_model_load: memory_size =  2560.00 MB, n_mem = 40960\nllama_model_load: loading model part 1/8 from './models/65B/ggml-model-q4_0.bin'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 2/8 from './models/65B/ggml-model-q4_0.bin.1'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 3/8 from './models/65B/ggml-model-q4_0.bin.2'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 4/8 from './models/65B/ggml-model-q4_0.bin.3'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 5/8 from './models/65B/ggml-model-q4_0.bin.4'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 6/8 from './models/65B/ggml-model-q4_0.bin.5'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 7/8 from './models/65B/ggml-model-q4_0.bin.6'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 8/8 from './models/65B/ggml-model-q4_0.bin.7'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\n\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n\nmain: prompt: 'Please close your issue when it has been answered.'\nmain: number of tokens in prompt = 11\n     1 -> ''\n 12148 -> 'Please'\n  3802 -> ' close'\n   596 -> ' your'\n  2228 -> ' issue'\n   746 -> ' when'\n   372 -> ' it'\n   756 -> ' has'\n  1063 -> ' been'\n  7699 -> ' answered'\n 29889 -> '.'\n\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\n\n\nPlease close your issue when it has been answered.\n@duncan-donut: I'm trying to figure out what kind of \"support\" you need for this script and why, exactly? Is there a question about how the code works that hasn't already been addressed in one or more comments below this ticket, or are we talking something else entirely like some sorta bugfixing job because your server setup is different from mine??\nI can understand if your site needs to be running smoothly and you need help with a fix of sorts but there should really be nothing wrong here that the code itself could not handle. And given that I'm getting reports about how it works perfectly well on some other servers, what exactly are we talking? A detailed report will do wonders in helping us get this resolved for ya quickly so please take your time and describe the issue(s) you see as clearly & concisely as possible!!\n@duncan-donut: I'm not sure if you have access to cPanel but you could try these instructions. It is worth a shot! Let me know how it goes (or what error message, exactly!) when/if ya give that code a go? [end of text]\n\n\nmain: mem per token = 71159620 bytes\nmain:     load time = 19309.95 ms\nmain:   sample time =   168.62 ms\nmain:  predict time = 223895.61 ms / 888.47 ms per token\nmain:    total time = 246406.42 ms\n\n Performance counter stats for './main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p Please close your issue when it has been answered.':\n\n        3636882.89 msec task-clock                #   14.677 CPUs utilized          \n             13509      context-switches          #    3.714 /sec                   \n              2436      cpu-migrations            #    0.670 /sec                   \n          10476679      page-faults               #    2.881 K/sec                  \n    13133115082869      cycles                    #    3.611 GHz                      (16.77%)\n       29314462753      stalled-cycles-frontend   #    0.22% frontend cycles idle     (16.76%)\n    10294402631459      stalled-cycles-backend    #   78.39% backend cycles idle      (16.74%)\n    23479217109614      instructions              #    1.79  insn per cycle         \n                                                  #    0.44  stalled cycles per insn  (16.76%)\n     2353072268027      branches                  #  647.002 M/sec                    (16.77%)\n        1998682780      branch-misses             #    0.08% of all branches          (16.76%)\n\n     247.802177522 seconds time elapsed\n\n    3618.573072000 seconds user\n      18.491698000 seconds sys\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-06T23:23:10+00:00",
    "closed_at": "2023-04-06T23:48:30+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/822/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/822"
  },
  {
    "number": 821,
    "title": "[User] Insert summary of your issue or enhancement..",
    "body": "# Prerequisites\n\nPlease answer the following questions for yourself before submitting an issue.\n\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\n\n# Expected Behavior\n\nPlease provide a detailed written description of what you were trying to do, and what you expected `llama.cpp` to do.\n\n# Current Behavior\n\nPlease provide a detailed written description of what `llama.cpp` did, instead. \n\n# Environment and Context \n\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\n\n* Physical (or virtual) hardware you are using, e.g. for Linux:\n\n`$ lscpu`\n\n* Operating System, e.g. for Linux:\n\n`$ uname -a`\n\n* SDK version, e.g. for Linux:\n\n```\n$ python3 --version\n$ make --version\n$ g++ --version\n```\n\n# Failure Information (for bugs)\n\nPlease help provide information about the failure if this is a bug. If it is not a bug, please remove the rest of this template.\n\n# Steps to Reproduce\n\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\n\n1. step 1\n2. step 2\n3. step 3\n4. etc.\n\n# Failure Logs\n\nPlease include any relevant log snippets or files. If it works under one configuration but not under another, please provide logs for both configurations and their corresponding outputs so it is easy to see where behavior changes.\n\nAlso, please try to **avoid using screenshots** if at all possible. Instead, copy/paste the console output and use [Github's markdown](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) to cleanly format your logs for easy readability.\n\nExample environment info:\n```\nllama.cpp$ git log | head -1\ncommit 2af23d30434a677c6416812eea52ccc0af65119c\n\nllama.cpp$ lscpu | egrep \"AMD|Flags\"\nVendor ID:                       AuthenticAMD\nModel name:                      AMD Ryzen Threadripper 1950X 16-Core Processor\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sme sev\nVirtualization:                  AMD-V\n\nllama.cpp$ python3 --version\nPython 3.10.9\n\nllama.cpp$ pip list | egrep \"torch|numpy|sentencepiece\"\nnumpy                         1.24.2\nnumpydoc                      1.5.0\nsentencepiece                 0.1.97\ntorch                         1.13.1\ntorchvision                   0.14.1\n\nllama.cpp$ make --version | head -1\nGNU Make 4.3\n\n$ md5sum ./models/65B/ggml-model-q4_0.bin\ndbdd682cce80e2d6e93cefc7449df487  ./models/65B/ggml-model-q4_0.bin\n```\n\nExample run with the Linux command [perf](https://www.brendangregg.com/perf.html)\n```\nllama.cpp$ perf stat ./main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p \"Please close your issue when it has been answered.\"\nmain: seed = 1679149377\nllama_model_load: loading model from './models/65B/ggml-model-q4_0.bin' - please wait ...\nllama_model_load: n_vocab = 32000\nllama_model_load: n_ctx   = 512\nllama_model_load: n_embd  = 8192\nllama_model_load: n_mult  = 256\nllama_model_load: n_head  = 64\nllama_model_load: n_layer = 80\nllama_model_load: n_rot   = 128\nllama_model_load: f16     = 2\nllama_model_load: n_ff    = 22016\nllama_model_load: n_parts = 8\nllama_model_load: ggml ctx size = 41477.73 MB\nllama_model_load: memory_size =  2560.00 MB, n_mem = 40960\nllama_model_load: loading model part 1/8 from './models/65B/ggml-model-q4_0.bin'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 2/8 from './models/65B/ggml-model-q4_0.bin.1'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 3/8 from './models/65B/ggml-model-q4_0.bin.2'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 4/8 from './models/65B/ggml-model-q4_0.bin.3'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 5/8 from './models/65B/ggml-model-q4_0.bin.4'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 6/8 from './models/65B/ggml-model-q4_0.bin.5'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 7/8 from './models/65B/ggml-model-q4_0.bin.6'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 8/8 from './models/65B/ggml-model-q4_0.bin.7'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\n\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n\nmain: prompt: 'Please close your issue when it has been answered.'\nmain: number of tokens in prompt = 11\n     1 -> ''\n 12148 -> 'Please'\n  3802 -> ' close'\n   596 -> ' your'\n  2228 -> ' issue'\n   746 -> ' when'\n   372 -> ' it'\n   756 -> ' has'\n  1063 -> ' been'\n  7699 -> ' answered'\n 29889 -> '.'\n\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\n\n\nPlease close your issue when it has been answered.\n@duncan-donut: I'm trying to figure out what kind of \"support\" you need for this script and why, exactly? Is there a question about how the code works that hasn't already been addressed in one or more comments below this ticket, or are we talking something else entirely like some sorta bugfixing job because your server setup is different from mine??\nI can understand if your site needs to be running smoothly and you need help with a fix of sorts but there should really be nothing wrong here that the code itself could not handle. And given that I'm getting reports about how it works perfectly well on some other servers, what exactly are we talking? A detailed report will do wonders in helping us get this resolved for ya quickly so please take your time and describe the issue(s) you see as clearly & concisely as possible!!\n@duncan-donut: I'm not sure if you have access to cPanel but you could try these instructions. It is worth a shot! Let me know how it goes (or what error message, exactly!) when/if ya give that code a go? [end of text]\n\n\nmain: mem per token = 71159620 bytes\nmain:     load time = 19309.95 ms\nmain:   sample time =   168.62 ms\nmain:  predict time = 223895.61 ms / 888.47 ms per token\nmain:    total time = 246406.42 ms\n\n Performance counter stats for './main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p Please close your issue when it has been answered.':\n\n        3636882.89 msec task-clock                #   14.677 CPUs utilized          \n             13509      context-switches          #    3.714 /sec                   \n              2436      cpu-migrations            #    0.670 /sec                   \n          10476679      page-faults               #    2.881 K/sec                  \n    13133115082869      cycles                    #    3.611 GHz                      (16.77%)\n       29314462753      stalled-cycles-frontend   #    0.22% frontend cycles idle     (16.76%)\n    10294402631459      stalled-cycles-backend    #   78.39% backend cycles idle      (16.74%)\n    23479217109614      instructions              #    1.79  insn per cycle         \n                                                  #    0.44  stalled cycles per insn  (16.76%)\n     2353072268027      branches                  #  647.002 M/sec                    (16.77%)\n        1998682780      branch-misses             #    0.08% of all branches          (16.76%)\n\n     247.802177522 seconds time elapsed\n\n    3618.573072000 seconds user\n      18.491698000 seconds sys\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-06T23:19:27+00:00",
    "closed_at": "2023-04-06T23:48:50+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/821/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/821"
  },
  {
    "number": 817,
    "title": "Compiling with LLAMA_OPENBLAS=1 does not seem to improve performance",
    "body": "# Expected Behavior\r\nCompiling llama.cpp with `make LLAMA_OPENBLAS=1` should give a slight performance bump in prompt ingestion, and no change (or reduced) cpu usage in text generation.\r\n\r\n# Current Behavior\r\n\r\nCompiled llama.cpp with `make LLAMA_OPENBLAS=1`. Compilation seems to work fine, but when running ./main for generation, I find no difference in the rate of prompt ingestion of generation. Can confirm that BLAS=1 shows up in the model loading info.\r\n\r\nAlso, when compiled with BLAS , if -t is not explicitly set, it seems to default to all threads.\r\n\r\n# Environment and Context \r\nUbuntu 22.04.1 LTS\r\nPython 3.10.6\r\nGNU Make 4.3\r\ng++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\n\r\nVendor ID:                       AuthenticAMD\r\nModel name:                      AMD EPYC 7742 64-Core Processor\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca\r\nVirtualization:                  AMD-V\r\n\r\ncommit 53dbba769537e894ead5c6913ab2fd3a4658b738\r\n\r\n```\r\n~/llama.cpp$ ./main -m ~/llama_models/gpt4-x-alpaca-13b-native-4bit-128g/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g/ggml-model-q4_1.bin -ins -t 12\r\nmain: seed = 1680797443\r\nllama_model_load: loading model from '!/llama_models/gpt4-x-alpaca-13b-native-4bit-128g/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g/ggml-model-q4_1.bin' - please wait ...\r\nllama_model_load: GPTQ model detected - are you sure n_parts should be 2? we normally expect it to be 1\r\nllama_model_load: use '--n_parts 1' if necessary\r\nllama_model_load: n_vocab = 32001\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 5120\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 40\r\nllama_model_load: n_layer = 40\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 4\r\nllama_model_load: n_ff    = 13824\r\nllama_model_load: n_parts = 2\r\nllama_model_load: type    = 2\r\nllama_model_load: ggml map size = 9702.04 MB\r\nllama_model_load: ggml ctx size = 101.25 KB\r\nllama_model_load: mem required  = 11750.14 MB (+ 1608.00 MB per state)\r\nllama_model_load: loading tensors from '~/llama_models/gpt4-x-alpaca-13b-native-4bit-128g/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g/ggml-model-q4_1.bin'\r\nllama_model_load: model size =  9701.60 MB / num tensors = 363\r\nllama_init_from_file: kv self size  =  400.00 MB\r\n\r\nsystem_info: n_threads = 256 / 256 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\r\nmain: interactive mode on.\r\nReverse prompt: '### Instruction:\r\n\r\n'\r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 512, n_batch = 128, n_predict = 128, n_keep = 2\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n```\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-06T16:13:36+00:00",
    "closed_at": "2023-04-07T08:56:00+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/817/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/817"
  },
  {
    "number": 811,
    "title": "[Enhancement]: Implement optimizations used in CTranslate2",
    "body": "[CTranslate2](https://github.com/OpenNMT/CTranslate2) is a \"competitor\" to llama.cpp that advertises itself with:\r\n> ### Fast and efficient execution on CPU and GPU\r\n> The execution [is significantly faster and requires less resources](https://github.com/ggerganov/llama.cpp/issues/new?assignees=&labels=&template=custom.md&title=%5BUser%5D+Insert+summary+of+your+issue+or+enhancement..#benchmarks) than general-purpose deep learning frameworks on supported models and tasks thanks to many advanced optimizations: layer fusion, padding removal, batch reordering, in-place operations, caching mechanism, etc.\r\n\r\nI am no expert in LLMs and I don't know what these optimizations are, but I am asking: would it be possible/feasible and/or desirable to implement these optimizations into llama.cpp or GGML?\r\n",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-06T14:13:22+00:00",
    "closed_at": "2024-04-11T01:06:58+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/811/reactions",
      "total_count": 8,
      "+1": 8,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/811"
  },
  {
    "number": 808,
    "title": "How do i use convert-unversioned-ggml-to-ggml.py?",
    "body": "Hi it told me to use the convert-unversioned-ggml-to-ggml.py file and gave me an error saying your gpt4all model is too old. So i converted the gpt4all-lora-unfiltered-quantized.bin file with llama tokenizer. And it generated some kind of orig file in the same directory where the model was. When i tried to run the miku.sh file which had the latest generated file as model it gave me another error stating this \r\n`main: seed = 1680783525\r\nllama_model_load: loading model from './models/gpt4all-7B/gpt4all-lora-unfiltered-quantized.bin' - please wait ...\r\n./models/gpt4all-7B/gpt4all-lora-unfiltered-quantized.bin: invalid model file (bad magic [got 0x67676d66 want 0x67676a74])\r\n        you most likely need to regenerate your ggml files\r\n        the benefit is you'll get 10-100x faster load times\r\n        see https://github.com/ggerganov/llama.cpp/issues/91\r\n        use convert-pth-to-ggml.py to regenerate from original pth\r\n        use migrate-ggml-2023-03-30-pr613.py if you deleted originals\r\nllama_init_from_file: failed to load model\r\nmain: error: failed to load model './models/gpt4all-7B/gpt4all-lora-unfiltered-quantized.bin'`\r\n\r\nHow do i use the conversion? did i do something wrong?",
    "labels": [
      "bug",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-04-06T12:22:58+00:00",
    "closed_at": "2023-04-14T13:12:39+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/808/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/808"
  },
  {
    "number": 805,
    "title": "Specifications for how to integrate services in AI models",
    "body": "Please join discussion. \r\n\r\nHere is a repository I created for specs, for now it's empty. Discussion is welcome. https://github.com/openservices4ai/spec\r\n\r\n**What is an AI-integrated sevice** \r\n\r\nAs AI models trained on a fixed set of data and cannot learn something new in real-time while serving, services integrated in AI models help provide real-time information and services. For example, a chatbot model may be integrated with a todo list service to help users add/remove todos, or create a timer. \r\n\r\n**Why an unified specification for AI-integrated services matters**\r\n\r\nFirst, it can help improve compatibility between different AI platforms, making it easier for developers to integrate various plugins into their AIs. This can save time and resources for developers who would otherwise have to manually modify each plugin to work with their chatbot.\r\n\r\nSecond, an unified specification can help ensure that services adhere to certain standards for functionality, security, and user privacy. This can help promote trust and confidence among users who interact with chatbots using these services.\r\n\r\nFinally, it can also help promote innovation and collaboration among developers working on AI chatbots. By providing a common framework for plugin development, developers can more easily share their work and collaborate on new features and functionalities, ultimately leading to better and more sophisticated chatbots for users.\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-06T08:54:29+00:00",
    "closed_at": "2023-04-06T14:04:50+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/805/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/805"
  },
  {
    "number": 804,
    "title": "wasm simd 128 build failure regression",
    "body": "I\u2019m trying out building on a-shell on iOS which is a wasm terminal platform without gnu make.\r\n\r\nIt looks like since c1f885067c61191a07a1aedf684168dda62f3f71 there are undefined symbols in the wasm simd 128 code:\r\n```\r\nclang -pipe  -msimd128 -D_WASI_EMULATED_MMAN -D_WASI_EMULATED_PROCESS_CLOCKS   -c ggml.c -o ggml.o\r\nggml.c:2090:43: error: use of undeclared identifier 'px'\r\n        const block_q4_0 * restrict x0 = &px[i + 0];\r\n                                          ^\r\n```\r\n\r\nThese undefined symbols were first introduced on line 1726 of ggml.c https://github.com/ggerganov/llama.cpp/commit/c1f885067c61191a07a1aedf684168dda62f3f71#diff-6d9ce99fcb6f51ff76f59e479f6e6fc0bb62edef7442805d7a5bb15b23996b5dR1726 (github seems to need the large diff manually expanded) and is presently at https://github.com/ggerganov/llama.cpp/blob/d2beca95dcfcd6f1145886e914b879ffc3604b7a/ggml.c#L2090",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-04-06T07:49:12+00:00",
    "closed_at": "2023-04-12T15:01:15+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/804/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/804"
  },
  {
    "number": 802,
    "title": "llama.cpp acts too dumb while running on phone!!",
    "body": "I was trying llama.cpp on phone with termux installed. but look at this image\r\n![Screenshot_20230406-120404](https://user-images.githubusercontent.com/97907864/230291536-8dbfeff4-0456-4328-a2dc-1bb35614f742.png)\r\n\r\n**Specifications**\r\nThe phone has 8 gigs of RAM and 7 gigs is free and the CPU has 8 cores so its not the issue of the RAM and CPU.\r\nModel used: alpaca-7B-lora\r\nllama.cpp version: latest\r\nprompt: chat-with-bob.txt\r\n\r\nI really don't know what is causing the issue here. The problem happening is, when i ask a question to it, it just either answers the question in a very dumb way or it just repeats the same question not answering anything. With the same model, prompt and llama.cpp version on my PC with 4GB ram works as expected it answers every question with almost 98% accuracy. Can any of you guys help me out with this? or update the llama.cpp and fix the mobile issues please?\r\n\r\nThankyou",
    "labels": [
      "generation quality"
    ],
    "state": "closed",
    "created_at": "2023-04-06T06:43:33+00:00",
    "closed_at": "2023-04-15T17:23:12+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/802/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/802"
  },
  {
    "number": 800,
    "title": "Do you guys mind making an updater for llama.cpp?",
    "body": "Basically when you guys release a new version of llama.cpp the updater will ask a prompt to you saying there is a new update available (that will be fetched from github commits) and it will ask you if it can update the files for you, if you say yes then it will fetch the **Changed/previously edited files** from github and replace it with your local llama files.\r\n\r\nThis came to my mind because llama.cpp has become a big project now. You guys are updating the project every single day which is awesome but it makes the people replace the files on their own every single day. That might be annoying. ",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-06T05:50:34+00:00",
    "closed_at": "2023-04-06T06:18:15+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/800/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/800"
  },
  {
    "number": 799,
    "title": "What would it take to 100x the context window?",
    "body": "Thinking about what could be done when large language models can operate on phenomenally large context, and wondering what it might actually take to get there.\r\n\r\nAnd realised this repo has a ton of really bright people in orbit, who actually understand brass tacks what might be involved.\r\n\r\nAssuming it's really desirable, what hacks could be done to get there?\r\n\r\n- What options are there?\r\n- How much space or time would be involved?\r\n- Would the models need full retraining?\r\n- What about for 10x?\r\n\r\n\ud83d\ude4f",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-06T03:18:40+00:00",
    "closed_at": "2024-04-11T01:06:59+00:00",
    "comments": 31,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/799/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/799"
  },
  {
    "number": 791,
    "title": "[Bug] dequantize_row_q4_0  segfaults",
    "body": "# Environment and Context \r\n\r\nLinux  5.10.0-21-amd64 #1 SMP Debian 5.10.162-1 (2023-01-21) x86_64 GNU/Linux\r\ng++ (Debian 10.2.1-6) 10.2.1 20210110\r\nGNU Make 4.3\r\n\r\n# Failure Information (for bugs)\r\n\r\nmain segfaults at  dequantize_row_q4_0+48\r\n\r\n# Steps to Reproduce\r\n\r\n./main  -m models/ggml-vocab-q4_0.bin\r\n\r\n\r\n\r\n~/s/llama.cpp \u276f\u276f\u276f gdb main\r\n(gdb) r -m models/ggml-vocab-q4_0.bin\r\nStarting program: /home/sha0/soft/llama.cpp/main -m models/ggml-vocab-q4_0.bin\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\nmain: seed = 1680724006\r\nllama_model_load: loading model from 'models/ggml-vocab-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml map size =   0.41 MB\r\nllama_model_load: ggml ctx size =  81.25 KB\r\nllama_model_load: mem required  = 1792.49 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading tensors from 'models/ggml-vocab-q4_0.bin'\r\nllama_model_load: model size =     0.00 MB / num tensors = 0\r\nllama_model_load: WARN no tensors loaded from model file - assuming empty model for testing\r\nllama_init_from_file: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 16 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 512, n_batch = 8, n_predict = 128, n_keep = 0\r\n\r\n[New Thread 0x7fff77560700 (LWP 142639)]\r\n[New Thread 0x7fff76d5f700 (LWP 142640)]\r\n[New Thread 0x7fff7655e700 (LWP 142641)]\r\n[New Thread 0x7fff75d5d700 (LWP 142642)]\r\n[New Thread 0x7fff7555c700 (LWP 142643)]\r\n[New Thread 0x7fff74d5b700 (LWP 142644)]\r\n[New Thread 0x7fff7455a700 (LWP 142645)]\r\n[New Thread 0x7fff73d59700 (LWP 142646)]\r\n[New Thread 0x7fff73558700 (LWP 142647)]\r\n[New Thread 0x7fff72d57700 (LWP 142648)]\r\n[New Thread 0x7fff72556700 (LWP 142649)]\r\n[New Thread 0x7fff71d55700 (LWP 142650)]\r\n[New Thread 0x7fff71554700 (LWP 142651)]\r\n[New Thread 0x7fff70d53700 (LWP 142652)]\r\n[New Thread 0x7fff70552700 (LWP 142653)]\r\n\r\nThread 1 \"main\" received signal SIGSEGV, Segmentation fault.\r\n0x000055555555e430 in dequantize_row_q4_0 ()\r\n(gdb) bt\r\n#0  0x000055555555e430 in dequantize_row_q4_0 ()\r\n#1  0x0000555555567585 in ggml_compute_forward_get_rows ()\r\n#2  0x000055555556fba3 in ggml_graph_compute ()\r\n#3  0x0000555555578eca in llama_eval_internal(llama_context&, int const*, int, int, int) ()\r\n#4  0x000055555557919f in llama_eval ()\r\n#5  0x000055555555c1aa in main ()\r\n(gdb) x/i $pc\r\n=> 0x55555555e430 <dequantize_row_q4_0+48>:     vpmovzxbw 0x4(%rdi),%ymm1\r\n(gdb) i r rdi\r\nrdi            0xa00               2560\r\n(gdb) i r ymm1\r\nymm1           {v16_bfloat16 = {0x180, 0x0, 0x0, 0x0, 0x180, 0x0 <repeats 11 times>}, v8_float = {0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, v4_double = {0x0, 0x0, 0x0, 0x0}, v32_int8 = {0xc0, 0x43, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xc0, 0x43, 0x0 <repeats 22 times>}, v16_int16 = {0x43c0, 0x0, 0x0, 0x0, 0x43c0, 0x0 <repeats 11 times>}, v8_int32 = {0x43c0, 0x0, 0x43c0, 0x0, 0x0, 0x0, 0x0, 0x0}, v4_int64 = {0x43c0, 0x43c0, 0x0, 0x0}, v2_int128 = {0x43c000000000000043c0, 0x0}}\r\n(gdb) ",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-05T20:00:07+00:00",
    "closed_at": "2023-04-05T20:31:25+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/791/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/791"
  },
  {
    "number": 790,
    "title": "Prompt eval time is counted twice",
    "body": "Creating a new issue so this doesn't get forgotten:\r\n\r\n@KASR posted a CSV of processing times in https://github.com/ggerganov/llama.cpp/issues/603#issuecomment-1492941163\r\n\r\nBut the times don't add up: If you take the total time, and subtract the partial times that are supposed to add up to it, the result is all over the place:\r\n\r\n![image](https://user-images.githubusercontent.com/4478/229292361-52df310b-e838-46b6-a68c-5c1d69cf5f87.png)\r\n\r\nThe clue lies in the comment by @ggerganov :\r\n> @sw \r\n> After the `mmap` changes, the `load` time is incorrect:\r\n> \r\n> https://github.com/ggerganov/llama.cpp/blob/6e7801d08d81c931a5427bae46f00763e993f54a/llama.cpp#L1681-L1685\r\n> \r\n> Currently, the reported load time includes not only the page faults, but also the prompt eval time. So effectively, you get the negative number since the prompt eval time has been accounted 2 times.\r\nWe have to fix this.\r\n\r\n_Originally posted by @ggerganov in https://github.com/ggerganov/llama.cpp/issues/603#issuecomment-1493275009_\r\n            ",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-05T19:59:57+00:00",
    "closed_at": "2024-04-11T01:07:00+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/790/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/790"
  },
  {
    "number": 789,
    "title": "[User] Insert summary of your issue or enhancement..",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nPlease provide a detailed written description of what you were trying to do, and what you expected `llama.cpp` to do.\r\n\r\n# Current Behavior\r\n\r\nPlease provide a detailed written description of what `llama.cpp` did, instead. \r\n\r\n# Environment and Context \r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n`$ lscpu`\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`$ uname -a`\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version\r\n$ make --version\r\n$ g++ --version\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nPlease help provide information about the failure if this is a bug. If it is not a bug, please remove the rest of this template.\r\n\r\n# Steps to Reproduce\r\n\r\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\r\n\r\n1. step 1\r\n2. step 2\r\n3. step 3\r\n4. etc.\r\n\r\n# Failure Logs\r\n\r\nPlease include any relevant log snippets or files. If it works under one configuration but not under another, please provide logs for both configurations and their corresponding outputs so it is easy to see where behavior changes.\r\n\r\nAlso, please try to **avoid using screenshots** if at all possible. Instead, copy/paste the console output and use [Github's markdown](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) to cleanly format your logs for easy readability.\r\n\r\nExample environment info:\r\n```\r\nllama.cpp$ git log | head -1\r\ncommit 2af23d30434a677c6416812eea52ccc0af65119c\r\n\r\nllama.cpp$ lscpu | egrep \"AMD|Flags\"\r\nVendor ID:                       AuthenticAMD\r\nModel name:                      AMD Ryzen Threadripper 1950X 16-Core Processor\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sme sev\r\nVirtualization:                  AMD-V\r\n\r\nllama.cpp$ python3 --version\r\nPython 3.10.9\r\n\r\nllama.cpp$ pip list | egrep \"torch|numpy|sentencepiece\"\r\nnumpy                         1.24.2\r\nnumpydoc                      1.5.0\r\nsentencepiece                 0.1.97\r\ntorch                         1.13.1\r\ntorchvision                   0.14.1\r\n\r\nllama.cpp$ make --version | head -1\r\nGNU Make 4.3\r\n\r\n$ md5sum ./models/65B/ggml-model-q4_0.bin\r\ndbdd682cce80e2d6e93cefc7449df487  ./models/65B/ggml-model-q4_0.bin\r\n```\r\n\r\nExample run with the Linux command [perf](https://www.brendangregg.com/perf.html)\r\n```\r\nllama.cpp$ perf stat ./main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p \"Please close your issue when it has been answered.\"\r\nmain: seed = 1679149377\r\nllama_model_load: loading model from './models/65B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 8192\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 64\r\nllama_model_load: n_layer = 80\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 22016\r\nllama_model_load: n_parts = 8\r\nllama_model_load: ggml ctx size = 41477.73 MB\r\nllama_model_load: memory_size =  2560.00 MB, n_mem = 40960\r\nllama_model_load: loading model part 1/8 from './models/65B/ggml-model-q4_0.bin'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 2/8 from './models/65B/ggml-model-q4_0.bin.1'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 3/8 from './models/65B/ggml-model-q4_0.bin.2'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 4/8 from './models/65B/ggml-model-q4_0.bin.3'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 5/8 from './models/65B/ggml-model-q4_0.bin.4'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 6/8 from './models/65B/ggml-model-q4_0.bin.5'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 7/8 from './models/65B/ggml-model-q4_0.bin.6'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 8/8 from './models/65B/ggml-model-q4_0.bin.7'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\n\r\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \r\n\r\nmain: prompt: 'Please close your issue when it has been answered.'\r\nmain: number of tokens in prompt = 11\r\n     1 -> ''\r\n 12148 -> 'Please'\r\n  3802 -> ' close'\r\n   596 -> ' your'\r\n  2228 -> ' issue'\r\n   746 -> ' when'\r\n   372 -> ' it'\r\n   756 -> ' has'\r\n  1063 -> ' been'\r\n  7699 -> ' answered'\r\n 29889 -> '.'\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\nPlease close your issue when it has been answered.\r\n@duncan-donut: I'm trying to figure out what kind of \"support\" you need for this script and why, exactly? Is there a question about how the code works that hasn't already been addressed in one or more comments below this ticket, or are we talking something else entirely like some sorta bugfixing job because your server setup is different from mine??\r\nI can understand if your site needs to be running smoothly and you need help with a fix of sorts but there should really be nothing wrong here that the code itself could not handle. And given that I'm getting reports about how it works perfectly well on some other servers, what exactly are we talking? A detailed report will do wonders in helping us get this resolved for ya quickly so please take your time and describe the issue(s) you see as clearly & concisely as possible!!\r\n@duncan-donut: I'm not sure if you have access to cPanel but you could try these instructions. It is worth a shot! Let me know how it goes (or what error message, exactly!) when/if ya give that code a go? [end of text]\r\n\r\n\r\nmain: mem per token = 71159620 bytes\r\nmain:     load time = 19309.95 ms\r\nmain:   sample time =   168.62 ms\r\nmain:  predict time = 223895.61 ms / 888.47 ms per token\r\nmain:    total time = 246406.42 ms\r\n\r\n Performance counter stats for './main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p Please close your issue when it has been answered.':\r\n\r\n        3636882.89 msec task-clock                #   14.677 CPUs utilized          \r\n             13509      context-switches          #    3.714 /sec                   \r\n              2436      cpu-migrations            #    0.670 /sec                   \r\n          10476679      page-faults               #    2.881 K/sec                  \r\n    13133115082869      cycles                    #    3.611 GHz                      (16.77%)\r\n       29314462753      stalled-cycles-frontend   #    0.22% frontend cycles idle     (16.76%)\r\n    10294402631459      stalled-cycles-backend    #   78.39% backend cycles idle      (16.74%)\r\n    23479217109614      instructions              #    1.79  insn per cycle         \r\n                                                  #    0.44  stalled cycles per insn  (16.76%)\r\n     2353072268027      branches                  #  647.002 M/sec                    (16.77%)\r\n        1998682780      branch-misses             #    0.08% of all branches          (16.76%)\r\n\r\n     247.802177522 seconds time elapsed\r\n\r\n    3618.573072000 seconds user\r\n      18.491698000 seconds sys\r\n```\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-05T19:59:06+00:00",
    "closed_at": "2023-04-06T00:29:02+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/789/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/789"
  },
  {
    "number": 788,
    "title": "Compilation failed on macOS 10.7-8-9: 'clock_gettime' produce warnings and errors",
    "body": "# PREREQUISITES\r\n- I am running the latest code: [5a8c4f6](https://github.com/ggerganov/llama.cpp/releases/tag/master-5a8c4f6)\r\n- I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- I have created a [relevant issue](https://github.com/antimatter15/alpaca.cpp/issues/201) in alpaca.cpp.\r\n- I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# EXPECTED BEHAVIOR\r\n* Attempted to compile the binary for macOS 10.7, 10.8 and 10.9.\r\n* Expected to run the chat app on an old macOS, that will be isolated from Internet.\r\n\r\n# ACTUAL BEHAVIOR\r\n* Compilation is terminated with warnings and errors.\r\n\r\n# ENVIRONMENT AND CONTEXT\r\n* Macbook pro 15 2012: macOS 10.8 Mountain Lion on Core i7 + 512 SDD + 16Gb RAM\r\n\t* Parallels Virtual Machine: macOS 10.7 Lion on 20Gb HDD + 4Gb RAM\r\n\t\t* X-Code 4.6.3\r\n\t\t* Command Line Tools OS X Lion Nov2012\r\n\t\t* MacPorts 2.8.1 10.7 (Lion)\r\n\t\t\t* git --version 2.40.0\r\n\t\t\t* clang --version 11.1.0\r\n\t\t\t\t* port select --set clang mp-clang-11\r\n\t\t\t* gcc --version 12.2.0\r\n\t\t\t\t* port select --set gcc mp-gcc12\r\n\t\t* export CC=gcc (in another re-try: export CC=clang)\r\n\t\t* export CXX=gcc (in another re-try: export CXX=clang)\r\n\r\n# FAILURE INFORMATION\r\n* Same issue is reproduced on macOS 10.8 on VM and macOS 10.9 on real machine.\r\n* Same issue is reproduced with gcc6 installed through [Tigerbrew](https://github.com/mistydemeo/tigerbrew) on macOS 10.9 real machine.\r\n* Same issue is reproduced for 'alpaca.cpp': 'clock_gettime' produce mostly the same warnings and errors on this environment.\r\n* 'stdatomic.h' issue is reproduced if latest gcc and clang versions are not set in ports and also CC and CXX vars are not set\r\n* '-gnu11', '-lrt' and '-D_POSIX_C_SOURCE=199309L' flags doesn't help, '-gnu99' gives more errors\r\n* Compilation successful on macOS 10.14 (Mojave) VM with Command Line Tools for Xcode 11.3.1. **Chat + 7b is working.**\r\n* [Fix](https://github.com/yquake2/yquake2/commit/78d70b2bf781aeb9c21652f018233fd34a4799aa) on [this](https://github.com/yquake2/yquake2/issues/239) issue states that older OSX versions don't implement 'clock_gettime ()' and author seem to find solution\r\n\r\n# STEPS TO REPRODUCE\r\n1. Clone the repo (or download the latest build and un-tar gz using tar -xvzf)\r\n2. cd the llama.cpp dir (or rename the latest build dir to 'llama.cpp' and cd there)\r\n3. make\r\n\r\n# FAILURE LOGS\r\nCC=gcc and CXX=gcc env vars set:\r\n```\r\nI llama.cpp build info: \r\nI UNAME_S:  Darwin\r\nI UNAME_P:  i386\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -march=native -mtune=native -DGGML_USE_ACCELERATE\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -march=native -mtune=native\r\nI LDFLAGS:   -framework Accelerate\r\nI CC:       gcc (MacPorts gcc12 12.2.0_2+stdlib_flag) 12.2.0\r\nI CXX:      gcc (MacPorts gcc12 12.2.0_2+stdlib_flag) 12.2.0\r\n\r\ngcc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -march=native -mtune=native -DGGML_USE_ACCELERATE   -c ggml.c -o ggml.o\r\nggml.c: In function 'ggml_time_ms':\r\nggml.c:376:5: warning: implicit declaration of function 'clock_gettime' [-Wimplicit-function-declaration]\r\n  376 |     clock_gettime(CLOCK_MONOTONIC, &ts);\r\n      |     ^~~~~~~~~~~~~\r\nggml.c:376:19: error: 'CLOCK_MONOTONIC' undeclared (first use in this function)\r\n  376 |     clock_gettime(CLOCK_MONOTONIC, &ts);\r\n      |                   ^~~~~~~~~~~~~~~\r\nggml.c:376:19: note: each undeclared identifier is reported only once for each function it appears in\r\nggml.c: In function 'ggml_time_us':\r\nggml.c:382:19: error: 'CLOCK_MONOTONIC' undeclared (first use in this function)\r\n  382 |     clock_gettime(CLOCK_MONOTONIC, &ts);\r\n      |                   ^~~~~~~~~~~~~~~\r\nggml.c: In function 'quantize_row_q4_1':\r\nggml.c:856:27: warning: unused variable 'y' [-Wunused-variable]\r\n  856 |     block_q4_1 * restrict y = vy;\r\n      |                           ^\r\nggml.c:854:15: warning: unused variable 'nb' [-Wunused-variable]\r\n  854 |     const int nb = k / QK;\r\n      |               ^~\r\nggml.c: In function 'ggml_vec_sum_f32':\r\nggml.c:2531:14: warning: passing argument 1 of 'vDSP_sve' discards 'const' qualifier from pointer target type [-Wdiscarded-qualifiers]\r\n 2531 |     vDSP_sve(x, 1, s, n);\r\n      |              ^\r\nIn file included from /System/Library/Frameworks/vecLib.framework/Headers/vecLib.h:41,\r\n                 from /System/Library/Frameworks/Accelerate.framework/Headers/Accelerate.h:20,\r\n                 from ggml.c:143:\r\n/System/Library/Frameworks/vecLib.framework/Headers/vDSP.h:4191:17: note: expected 'float *' but argument is of type 'const float *'\r\n 4191 |   float *       __vDSP_A,\r\n      |   ~~~~~~~~~~~~~~^~~~~~~~\r\nggml.c: In function 'ggml_vec_max_f32':\r\nggml.c:2543:15: warning: passing argument 1 of 'vDSP_maxv' discards 'const' qualifier from pointer target type [-Wdiscarded-qualifiers]\r\n 2543 |     vDSP_maxv(x, 1, s, n);\r\n      |               ^\r\n/System/Library/Frameworks/vecLib.framework/Headers/vDSP.h:3625:17: note: expected 'float *' but argument is of type 'const float *'\r\n 3625 |   float *       __vDSP_A,\r\n      |   ~~~~~~~~~~~~~~^~~~~~~~\r\nmake: *** [ggml.o] Error 1\r\n```\r\nCC=clang and CXX=clang env vars set:\r\n```\r\nI llama.cpp build info: \r\nI UNAME_S:  Darwin\r\nI UNAME_P:  i386\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -march=native -mtune=native -DGGML_USE_ACCELERATE\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -march=native -mtune=native\r\nI LDFLAGS:   -framework Accelerate\r\nI CC:       clang version 11.1.0\r\nI CXX:      clang version 11.1.0\r\n\r\nclang  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -march=native -mtune=native -DGGML_USE_ACCELERATE   -c ggml.c -o ggml.o\r\nggml.c:376:5: warning: implicit declaration of function 'clock_gettime' is invalid in C99 [-Wimplicit-function-declaration]\r\n    clock_gettime(CLOCK_MONOTONIC, &ts);\r\n    ^\r\nggml.c:376:19: error: use of undeclared identifier 'CLOCK_MONOTONIC'\r\n    clock_gettime(CLOCK_MONOTONIC, &ts);\r\n                  ^\r\nggml.c:382:5: warning: implicit declaration of function 'clock_gettime' is invalid in C99 [-Wimplicit-function-declaration]\r\n    clock_gettime(CLOCK_MONOTONIC, &ts);\r\n    ^\r\nggml.c:382:19: error: use of undeclared identifier 'CLOCK_MONOTONIC'\r\n    clock_gettime(CLOCK_MONOTONIC, &ts);\r\n                  ^\r\nggml.c:854:15: warning: unused variable 'nb' [-Wunused-variable]\r\n    const int nb = k / QK;\r\n              ^\r\nggml.c:856:27: warning: unused variable 'y' [-Wunused-variable]\r\n    block_q4_1 * restrict y = vy;\r\n                          ^\r\nggml.c:1813:5: warning: implicit conversion increases floating-point precision: 'float' to 'ggml_float' (aka 'double') [-Wdouble-promotion]\r\n    GGML_F16_VEC_REDUCE(sumf, sum);\r\n    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:1427:37: note: expanded from macro 'GGML_F16_VEC_REDUCE'\r\n#define GGML_F16_VEC_REDUCE         GGML_F32Cx8_REDUCE\r\n                                    ^\r\nggml.c:1417:33: note: expanded from macro 'GGML_F32Cx8_REDUCE'\r\n#define GGML_F32Cx8_REDUCE      GGML_F32x8_REDUCE\r\n                                ^\r\nggml.c:1364:11: note: expanded from macro 'GGML_F32x8_REDUCE'\r\n    res = _mm_cvtss_f32(_mm_hadd_ps(t1, t1));                     \\\r\n        ~ ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:2372:9: warning: implicit conversion increases floating-point precision: 'float' to 'ggml_float' (aka 'double') [-Wdouble-promotion]\r\n        GGML_F16_VEC_REDUCE(sumf[k], sum[k]);\r\n        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:1427:37: note: expanded from macro 'GGML_F16_VEC_REDUCE'\r\n#define GGML_F16_VEC_REDUCE         GGML_F32Cx8_REDUCE\r\n                                    ^\r\nggml.c:1417:33: note: expanded from macro 'GGML_F32Cx8_REDUCE'\r\n#define GGML_F32Cx8_REDUCE      GGML_F32x8_REDUCE\r\n                                ^\r\nggml.c:1364:11: note: expanded from macro 'GGML_F32x8_REDUCE'\r\n    res = _mm_cvtss_f32(_mm_hadd_ps(t1, t1));                     \\\r\n        ~ ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:2531:14: warning: passing 'const float *' to parameter of type 'float *' discards qualifiers [-Wincompatible-pointer-types-discards-qualifiers]\r\n    vDSP_sve(x, 1, s, n);\r\n             ^\r\n/System/Library/Frameworks/vecLib.framework/Headers/vDSP.h:4191:17: note: passing argument to parameter '__vDSP_A' here\r\n  float *       __vDSP_A,\r\n                ^\r\nggml.c:2543:15: warning: passing 'const float *' to parameter of type 'float *' discards qualifiers [-Wincompatible-pointer-types-discards-qualifiers]\r\n    vDSP_maxv(x, 1, s, n);\r\n              ^\r\n/System/Library/Frameworks/vecLib.framework/Headers/vDSP.h:3625:17: note: passing argument to parameter '__vDSP_A' here\r\n  float *       __vDSP_A,\r\n                ^\r\n8 warnings and 2 errors generated.\r\nmake: *** [ggml.o] Error 1\r\n```",
    "labels": [
      "bug",
      "build",
      "macos",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-05T19:16:14+00:00",
    "closed_at": "2024-04-11T01:07:01+00:00",
    "comments": 20,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/788/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/788"
  },
  {
    "number": 787,
    "title": "Changing default repeat_last_n value to current context size?",
    "body": "I noticed that llama 7b almost always gets stuck in a loop after a certain amount of time. This problem has reoccurred to me throughout the all time I have been trying to use llama.cpp (since March 15). I have also tried different models such as alpaca and gpt4all unfiltered, but the problem remains still. It also becomes obvious when you try to generate a dialog following some kind of plot (I use --keep to keep the plot summary in context). All the times I've tried to generate something infinite, it just loops at some point, even in interactive mode.\r\n\r\nI also noticed, that setting repeat_last_n to current context size helps to eliminate this issue. (I use ctx_size 2048 for the most time) \r\n\r\nMaybe after some testing, default repeat_last_n value could be changed to currently set context size, so newbies could bypass this issue?",
    "labels": [
      "enhancement",
      "generation quality",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-05T18:18:14+00:00",
    "closed_at": "2024-04-11T01:07:03+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/787/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/787"
  },
  {
    "number": 782,
    "title": "Multi-thread ggml_cpy()",
    "body": "This is a task suitable for new contributors\r\n\r\nSee how we multi-threaded the [ggml_rope()](https://github.com/ggerganov/llama.cpp/pull/781) operator.\r\nDo the same for the `ggml_cpy()` operator and see if there is any benefit.\r\n\r\nUse the [ggml profiler (GGML_PERF)](https://github.com/ggerganov/llama.cpp/wiki/GGML-Tips-&-Tricks#measuring-the-performance-of-the-inference) to measure the benefit of multi-threaded vs non-multi-threaded `ggml_cpy()`",
    "labels": [
      "enhancement",
      "good first issue",
      "performance"
    ],
    "state": "closed",
    "created_at": "2023-04-05T16:24:00+00:00",
    "closed_at": "2023-04-10T19:47:15+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/782/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/782"
  },
  {
    "number": 776,
    "title": "docker image ghcr.io/ggerganov/llama.cpp:light not working",
    "body": "The docker image ghcr.io/ggerganov/llama.cpp:light no longer works. It exits without giving the answer.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-05T14:35:53+00:00",
    "closed_at": "2023-04-05T17:21:42+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/776/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/776"
  },
  {
    "number": 774,
    "title": "Error in conversion",
    "body": "# Current Behavior\r\nWhile converting the 7B model I got the error:\r\n```\r\n{'dim': 4096, 'multiple_of': 256, 'n_heads': 32, 'n_layers': 32, 'norm_eps': 1e-06, 'vocab_size': -1}\r\nTraceback (most recent call last):\r\n  File \"/content/llama.cpp/llama.cpp/llama.cpp/llama.cpp/convert-pth-to-ggml.py\", line 274, in <module>\r\n    main()\r\n  File \"/content/llama.cpp/llama.cpp/llama.cpp/llama.cpp/convert-pth-to-ggml.py\", line 239, in main\r\n    hparams, tokenizer = load_hparams_and_tokenizer(dir_model)\r\n  File \"/content/llama.cpp/llama.cpp/llama.cpp/llama.cpp/convert-pth-to-ggml.py\", line 105, in load_hparams_and_tokenizer\r\n    tokenizer = SentencePieceProcessor(fname_tokenizer)\r\n  File \"/usr/local/lib/python3.9/dist-packages/sentencepiece/__init__.py\", line 447, in Init\r\n    self.Load(model_file=model_file, model_proto=model_proto)\r\n  File \"/usr/local/lib/python3.9/dist-packages/sentencepiece/__init__.py\", line 905, in Load\r\n    return self.LoadFromFile(model_file)\r\n  File \"/usr/local/lib/python3.9/dist-packages/sentencepiece/__init__.py\", line 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)\r\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto->ParseFromArray(serialized.data(), serialized.size())] \r\n\r\n```\r\n\r\nInfo about my environment below - let me know if you have some hints, thanks!\r\nLuigi\r\n\r\n# Environment and Context \r\n\r\nenvironment is Google Colab.  Weights have been verified via md5sum:\r\n\r\n```\r\n# according with: https://github.com/ggerganov/llama.cpp/issues/238\r\nmd5sum ./models/*/*.pth | sort -k 2,2\r\n6efc8dab194ab59e49cd24be5574d85e  ./models/7B/consolidated.00.pth\r\n\r\n```\r\n\r\n```\r\n$ python3 --version\r\n$ make --version\r\n$ g++ --version\r\n\r\nPython 3.9.16\r\n\r\nGNU Make 4.2.1\r\nBuilt for x86_64-pc-linux-gnu\r\nCopyright (C) 1988-2016 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.\r\n\r\ng++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\nCopyright (C) 2019 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n\r\n\r\nEnvironment info:\r\n```\r\ngit log | head -1\r\ncommit 58c438cf7dfbbef710b1905a453a38a8a9ced07d\r\n\r\npip list | egrep \"torch|numpy|sentencepiece\r\nnumpy                         1.22.4\r\nsentencepiece                 0.1.97\r\ntorch                         2.0.0+cu118\r\ntorchaudio                    2.0.1+cu118\r\ntorchdata                     0.6.0\r\ntorchsummary                  1.5.1\r\ntorchtext                     0.15.1\r\ntorchvision                   0.15.1+cu118\r\n\r\nllama.cpp$ python3 --version\r\nPython 3.9.16\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-04-05T13:44:55+00:00",
    "closed_at": "2023-04-14T13:16:11+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/774/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/774"
  },
  {
    "number": 771,
    "title": "Running a Vicuna-13B 4it model ?",
    "body": "I found this model : \r\n[[ggml-vicuna-13b-4bit](https://huggingface.co/eachadea/ggml-vicuna-13b-4bit)](https://huggingface.co/eachadea/ggml-vicuna-13b-4bit/tree/main) and judging by their online demo it's very impressive.\r\nI tried to run it with llama.cpp latest version - the model loads fine, but as soon as it loads it starts hallucinating and quits by itself. \r\nDo I need to have it converted or something like that ?",
    "labels": [
      "model",
      "generation quality"
    ],
    "state": "closed",
    "created_at": "2023-04-05T07:33:04+00:00",
    "closed_at": "2023-07-28T19:47:57+00:00",
    "comments": 25,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/771/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/771"
  },
  {
    "number": 767,
    "title": "Token generation is extremely slow when using 13B models on an M1 Pro with llama.cpp, but it runs at a fine speed with Dalai (which uses an older version of llama.cpp)",
    "body": "# Expected Behavior\r\n\r\nI can load a 13B model and generate text with it with decent token generation speed with a M1 Pro CPU (16 GB RAM).\r\n\r\n# Current Behavior\r\n\r\nWhen I load a 13B model with llama.cpp (like Alpaca 13B or other models based on it) and I try to generate some text, every token generation needs several seconds, to the point that these models are not usable for how unbearably slow they are. But they works with reasonable speed using Dalai, that uses an older version of llama.cpp\r\n\r\n# Environment and Context \r\n\r\nMacBook Pro with M1 Pro, 16 GB RAM, macOS Ventura 13.3.\r\n\r\nPython 3.9.16\r\n\r\nGNU Make 3.81\r\n\r\nApple clang version 14.0.3 (clang-1403.0.22.14.1)\r\nTarget: arm64-apple-darwin22.4.0\r\nThread model: posix\r\n\r\nIf you need some kind of log or other informations, I will post everything you need. Thanks in advance.",
    "labels": [
      "performance"
    ],
    "state": "closed",
    "created_at": "2023-04-04T17:33:04+00:00",
    "closed_at": "2023-07-28T19:47:23+00:00",
    "comments": 30,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/767/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/767"
  },
  {
    "number": 766,
    "title": "[Bug] Different outputs when undefining GGML_SIMD",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nSame model outputs with enabled and disabled SIMD instructions.\r\n\r\n# Environment and Context \r\n\r\n`$ lscpu`\r\n```\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         48 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  16\r\n  On-line CPU(s) list:   0-15\r\nVendor ID:               AuthenticAMD\r\n  Model name:            AMD Ryzen 7 5800U with Radeon Graphics\r\n    CPU family:          25\r\n    Model:               80\r\n    Thread(s) per core:  2\r\n    Core(s) per socket:  8\r\n    Socket(s):           1\r\n    Stepping:            0\r\n    Frequency boost:     enabled\r\n    CPU(s) scaling MHz:  70%\r\n    CPU max MHz:         4505.0781\r\n    CPU min MHz:         1600.0000\r\n    BogoMIPS:            3792.78\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constan\r\n                         t_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdra\r\n                         nd lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_ll\r\n                         c mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb\r\n                          sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm_l\r\n                         ock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq\r\n                          rdpid overflow_recov succor smca fsrm\r\n\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`$ uname -a`\r\n```\r\nLinux fedora 6.2.8-200.fc37.x86_64 #1 SMP PREEMPT_DYNAMIC Wed Mar 22 19:11:02 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\n`$ g++ --version`\r\n```\r\ng++ (GCC) 12.2.1 20221121 (Red Hat 12.2.1-4)\r\n```\r\n\r\n# Failure Information (for bugs)\r\nSeems like there is a bug in `ggml_vec_dot_f16`.\r\n\r\n\r\n# Steps to Reproduce\r\n\r\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\r\n\r\n1. Evaluate the model\r\n2. Undefine GGML_SIMD\r\n3. Evaluate the model\r\n4. See difference\r\n\r\n# Output\r\nWith defined GGML_SIMD (current behavior)\r\n```\r\n$ ./main -m ./models/7B/ggml-model-q4_0.bin -c 128 -t 12 -n 40 -s 1680277445 -p '## Question: What is best in life? ## Jeeves: '\r\nmain: seed = 1680277445\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 128\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml map size = 4017.70 MB\r\nllama_model_load: ggml ctx size =  81.25 KB\r\nllama_model_load: mem required  = 5809.78 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading tensors from './models/7B/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =   64.00 MB\r\n\r\nsystem_info: n_threads = 12 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 128, n_batch = 8, n_predict = 40, n_keep = 0\r\n\r\n\r\n ## Question: What is best in life? ## Jeeves: 42 ## Bertie Wooster: That\u2019s got to be the answer! I mean, you can\u2019t argue with it. ## Question: What is best in life? ## Jee\r\nllama_print_timings:        load time =  1139.16 ms\r\nllama_print_timings:      sample time =    21.33 ms /    40 runs   (    0.53 ms per run)\r\nllama_print_timings: prompt eval time =   953.96 ms /    16 tokens (   59.62 ms per token)\r\nllama_print_timings:        eval time =  7448.75 ms /    39 runs   (  190.99 ms per run)\r\nllama_print_timings:       total time =  9072.95 ms\r\n```\r\n\r\nWith undefined GGML_SIMD:\r\n```\r\nmake && ./main -m ./models/7B/ggml-model-q4_0.bin -c 128 -t 12 -n 40 -s 1680277445 -p '## Question: What is best in life? ## Jeeves: '\r\nmain: seed = 1680277445\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 128\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml map size = 4017.70 MB\r\nllama_model_load: ggml ctx size =  81.25 KB\r\nllama_model_load: mem required  = 5809.78 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading tensors from './models/7B/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =   64.00 MB\r\n\r\nsystem_info: n_threads = 12 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 128, n_batch = 8, n_predict = 40, n_keep = 0\r\n\r\n\r\n ## Question: What is best in life? ## Jeeves: 42, sir.\r\nWere a big fan of the RSA Animate series here at TNW, and today we have another great video to show you that we think you will enjoy\r\nllama_print_timings:        load time =  1043.75 ms\r\nllama_print_timings:      sample time =    21.00 ms /    40 runs   (    0.53 ms per run)\r\nllama_print_timings: prompt eval time =   894.68 ms /    16 tokens (   55.92 ms per token)\r\nllama_print_timings:        eval time =  7486.81 ms /    39 runs   (  191.97 ms per run)\r\nllama_print_timings:       total time =  8990.76 ms\r\n```\r\n\r\nAs is, but with inverted branch `#if defined(GGML_SIMD)` in [`inline static void ggml_vec_dot_f16(const int n, float * restrict s, ggml_fp16_t * restrict x, ggml_fp16_t * restrict y)`](https://github.com/ggerganov/llama.cpp/blob/53dbba769537e894ead5c6913ab2fd3a4658b738/ggml.c#L1792) like so: `#if !defined(GGML_SIMD)`:\r\n```\r\nmain: seed = 1680277445\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 128\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml map size = 4017.70 MB\r\nllama_model_load: ggml ctx size =  81.25 KB\r\nllama_model_load: mem required  = 5809.78 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading tensors from './models/7B/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =   64.00 MB\r\n\r\nsystem_info: n_threads = 12 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 128, n_batch = 8, n_predict = 40, n_keep = 0\r\n\r\n\r\n ## Question: What is best in life? ## Jeeves: 42, sir.\r\nWere a big fan of the RSA Animate series here at TNW, and today we have another great video to show you that we think you will enjoy\r\nllama_print_timings:        load time =  1237.43 ms\r\nllama_print_timings:      sample time =    20.65 ms /    40 runs   (    0.52 ms per run)\r\nllama_print_timings: prompt eval time =  1051.29 ms /    16 tokens (   65.71 ms per token)\r\nllama_print_timings:        eval time =  7831.31 ms /    39 runs   (  200.80 ms per run)\r\nllama_print_timings:       total time =  9617.35 ms\r\n```\r\nGit blame shows @ggerganov",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-04T16:03:51+00:00",
    "closed_at": "2023-04-04T18:53:09+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/766/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/766"
  },
  {
    "number": 764,
    "title": "Feature to Discard Last Generated Message in Interactive Chat Mode?",
    "body": "We have ctrl+c to stop generate.\r\nCan we have undo feature to take the last message out of context and regenerate during an interactive chat session if you don't like what it generated?",
    "labels": [
      "duplicate",
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-04T13:50:05+00:00",
    "closed_at": "2024-04-11T01:07:04+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/764/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/764"
  },
  {
    "number": 762,
    "title": "Can i quantize a 4Bit model more?",
    "body": "Hi i want to quantize a model which is already quantized to 4bit ``q4_1`` but i want to make it compute faster so i wanted to ask what is the command to quantize the quantized module. I tried once with the command that is in the readme file but that didnt work. so can anyone help me?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-04T10:23:08+00:00",
    "closed_at": "2023-04-04T17:39:38+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/762/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/762"
  },
  {
    "number": 759,
    "title": "Small layout error in the documentation",
    "body": "Wrong repo",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-04T06:25:57+00:00",
    "closed_at": "2023-04-04T06:26:54+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/759/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/759"
  },
  {
    "number": 758,
    "title": "Something strange with any non-english prompts with alpaca/gpt4all/vicuna",
    "body": "When you try to chat with these on not english language, the answer will hallucinate and on english, but if you use file as prompt, everything works fine(picrelated). This is strange.\r\n![IMG_20230404_090641_914](https://user-images.githubusercontent.com/109795993/229702938-279e61be-0e63-4f5d-9ee7-bd66a9d37283.jpg)\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-04T06:10:41+00:00",
    "closed_at": "2023-04-04T07:55:03+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/758/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/758"
  },
  {
    "number": 757,
    "title": "Vicuna works sometimes strange",
    "body": "The quantized weights is here https://huggingface.co/eachadea/ggml-vicuna-13b-4bit/tree/main\r\nBut when you rin this in instructions mode, it will add the training dataset artifacts such as ###Instruction: in discussion. Is it possible to control these markers or add special mode that will stop the answer when ###",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-04T06:09:47+00:00",
    "closed_at": "2023-04-04T06:14:00+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/757/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/757"
  },
  {
    "number": 755,
    "title": "can llama do other task except text-generate,like translate",
    "body": "thanks for your work, it's very helpful!\r\ncan llama do other jod:\r\n`./main -m ./models/7B/ggml-model-q4_0.bin -p \"Translate English to Frence: it's a nic day!\" -n 512`\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-04T03:12:51+00:00",
    "closed_at": "2023-04-04T07:56:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/755/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/755"
  },
  {
    "number": 754,
    "title": "[Feature Request] support lit-llama",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Request\r\nHello. There is lit-llama (https://github.com/Lightning-AI/lit-llama) is released.\r\nIt is licensed by Apache 2.0. So it can be used for commercial.\r\nCould you support this model in this repo?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-04T01:59:23+00:00",
    "closed_at": "2023-04-04T02:23:34+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/754/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/754"
  },
  {
    "number": 751,
    "title": "Error when running make",
    "body": "I installed CMake, but when I run make, I get this error:\r\n\r\n/bin/sh: line 1: cc: command not found\r\n/bin/sh: line 1: g++: command not found\r\nI llama.cpp build info:\r\nI UNAME_S:  MSYS_NT-10.0-22621\r\nI UNAME_P:  unknown\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -march=native -mtune=native\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function\r\nI LDFLAGS:\r\nI CC:\r\nI CXX:\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -march=native -mtune=native   -c ggml.c -o ggml.o\r\nmake: cc: No such file or directory\r\nmake: *** [Makefile:142: ggml.o] Error 127\r\n\r\n\r\nIs there something else that I need to install? I am running this on Windows 11.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-03T22:41:44+00:00",
    "closed_at": "2023-04-04T00:34:40+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/751/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/751"
  },
  {
    "number": 746,
    "title": "Converting Ilama 4bit GPTQ Model from HF does not work",
    "body": "Hi! I tried to use the 13B Model from https://huggingface.co/maderix/llama-65b-4bit/\r\n\r\nI converted the model using \r\n\r\n`python convert-gptq-to-ggml.py models/llama13b-4bit.pt models/tokenizer.model models/llama13b-4bit.bin`\r\n\r\nIf I understand it correctly I still need to migrate the model and I tried it using\r\n\r\n`python migrate-ggml-2023-03-30-pr613.py models/llama13b-4bit.bin models/llama13b-4bit-new.bin`\r\n\r\nBut after a few seconds this breaks with the following error:\r\n\r\n```\r\nProcessing part 1 of 1\r\n\r\nProcessing tensor b'tok_embeddings.weight' with shape: [32000, 5120] and type: F16\r\nTraceback (most recent call last):\r\n  File \"/home/dust/llama.cpp/migrate-ggml-2023-03-30-pr613.py\", line 311, in <module>\r\n    main()\r\n  File \"/home/dust/llama.cpp/migrate-ggml-2023-03-30-pr613.py\", line 306, in main\r\n    copy_tensors(fin, fout, part_id, n_parts)\r\n  File \"/home/dust/llama.cpp/migrate-ggml-2023-03-30-pr613.py\", line 169, in copy_tensors\r\n    assert n_dims in (1, 2)\r\nAssertionError\r\n```\r\n\r\nIs it an error or am I the one to blame?",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2023-04-03T18:53:49+00:00",
    "closed_at": "2023-05-22T07:59:53+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/746/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/746"
  },
  {
    "number": 743,
    "title": "[Work Group] Add RLHF like ColosallChat on bigger dataset to achieve ChatGPT quality",
    "body": "[Link to ColosallChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat\r\n)\r\n# Add RLHF like ColosallChat on bigger dataset to achieve ChatGPT quality\r\n\r\n![lama-alpaca](https://user-images.githubusercontent.com/84633629/229592343-36b95e81-2f85-4fa0-9a5a-edd5267bb190.gif)\r\n\r\n\r\nAlthough models in the GPT series, such as ChatGPT and GPT-4, are highly powerful, they are unlikely to be fully open-sourced. Fortunately, the open-source community has been working hard to address this.\r\n\r\nFor example, Meta has open-sourced the LLaMA model, which offers parameter sizes ranging from 7 billion to 65 billion. A 13 billion parameter model can outperform the 175 billion GPT-3 model on most benchmark tests. However, since it doesn\u2019t have an instruct tuning stage, its actual generated results are not satisfactory.\r\n\r\nStanford\u2019s Alpaca generates training data in a self-instructed manner by calling OpenAI\u2019s API. With only 7 billion parameters, this lightweight model can be fine-tuned at a fraction of the cost to achieve conversational performance similar to a very large language model like GPT-3.5 with 175 billion parameters.\r\n\r\nHowever, existing open-source solutions can only be considered as supervised fine-tuned models in the first stage of RLHF (Reinforcement Learning from Human Feedback), with subsequent alignment and fine-tuning stages not performed. Additionally, Alpaca\u2019s training dataset is limited to English, which to some extent restricts the model\u2019s performance.\r\n\r\nYet, the impressive effects of ChatGPT and GPT-4 are due to the introduction of RLHF into the training process, which increases the consistency of the generated content with human values.\r\n\r\n![image](https://user-images.githubusercontent.com/84633629/229590135-fc856c3f-b138-4fad-842e-e0bf34ffd69f.png)\r\n\r\n## Training Dataset Open Source\r\nColossalChat releases a bilingual dataset comprising approximately 100,000 Q&A pairs in both English and Chinese. The dataset was collected and cleaned from real-life question scenarios on social media platforms, serving as the seed dataset, and was expanded using self-instruct technology, and annotation costs were approximately $900. Compared to datasets generated by other self-instruct methods, this dataset contains more realistic and diverse seed data and encompasses a wider range of topics. The dataset is suitable for both fine-tuning and RLHF training. With the provision of high-quality data, ColossalChat can achieve better dialogue interactions and also support Chinese.\r\n\r\n![image](https://user-images.githubusercontent.com/84633629/229590363-a1ab7e84-27af-4e4a-bbf1-47a3a113bcd1.png)\r\n\r\n## RLHF Algorithm Replication\r\n\r\nThe RLHF algorithm replication involves three stages:\r\n\r\nIn RLHF-Stage1, supervised instruct fine-tuning is performed using the datasets mentioned earlier to fine-tune the model.\r\n\r\nIn RLHF-Stage2, a reward model is trained to assign corresponding scores by manually ranking different outputs for the same prompt, which then supervises the training of the reward model.\r\n\r\nIn RLHF-Stage3, the reinforcement learning algorithm is being used, which is the most complex part of the training process:\r\n\r\n![image](https://user-images.githubusercontent.com/84633629/229590420-9b4224b5-c014-4782-ba97-2d754a5c842f.png)\r\n\r\nIn the PPO part, ColossalChat follows a two-stage process: first, the make experience stage, which uses SFT (Supervised Fine-Tuning), Actor, RM (Reward Model), and Critic models to calculate generated experience and store it in the buffer. Then comes the parameter update stage, which calculates the policy loss and value loss using the experience.\r\n\r\nIn the PTX part, ColossalChat calculates the cross-entropy loss between the Actor\u2019s output response and the response part of the input corpus. This loss is used to add pre-training gradients to the PPO gradient to maintain the language model\u2019s original performance and prevent forgetting. Finally, the policy loss, value loss, and PTX loss are summed up for backpropagation and parameter update.\r\n\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-03T18:02:06+00:00",
    "closed_at": "2023-04-13T08:55:27+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/743/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/743"
  },
  {
    "number": 742,
    "title": "Pythia Support?",
    "body": "Hi, I've found out that Pythia (from EleutherAI) is better that Cerebras GPT in terms of evaluation results. Pythia is basically a LLM that based on GPT NeoX architecture but it's parameters ranging from 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B to 13B. Pythia itself is available in GitHub repository \"EleutherAI/pythia\" and Huggingface Models under the same name.\n\nDoes this project support Pythia based models? If no, any plans on supporting them afterwards? I appreciate the implementation of Pythia in this project. Thank you very much before.",
    "labels": [
      "good first issue",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-04-03T16:51:43+00:00",
    "closed_at": "2024-05-23T09:49:54+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/742/reactions",
      "total_count": 26,
      "+1": 26,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/742"
  },
  {
    "number": 741,
    "title": "Add OpenCL Support",
    "body": "Please consider adding OpenCL support for devices with GPU's that Support OpenCL",
    "labels": [
      "wontfix"
    ],
    "state": "closed",
    "created_at": "2023-04-03T12:53:28+00:00",
    "closed_at": "2023-04-04T19:57:51+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/741/reactions",
      "total_count": 6,
      "+1": 5,
      "-1": 1,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/741"
  },
  {
    "number": 739,
    "title": "[User] Insert summary of your issue or enhancement..",
    "body": "Hello, is it possible to save the robot's response in a variable? to then read it in a request?\r\nExample\r\nMe : Hello how are you ?\r\nBot : I'm fine and you ?\r\n\r\nSave result response in new_varifable for use this :\r\nhttp://127.0.0.1:8888/?tts=new_variable",
    "labels": [
      "invalid"
    ],
    "state": "closed",
    "created_at": "2023-04-03T09:37:50+00:00",
    "closed_at": "2023-04-04T19:29:01+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/739/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/739"
  },
  {
    "number": 738,
    "title": "[User] Insert summary of your issue or enhancement..",
    "body": "# Prerequisites\n\nPlease answer the following questions for yourself before submitting an issue.\n\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\n\n# Expected Behavior\n\nPlease provide a detailed written description of what you were trying to do, and what you expected `llama.cpp` to do.\n\n# Current Behavior\n\nPlease provide a detailed written description of what `llama.cpp` did, instead. \n\n# Environment and Context \n\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\n\n* Physical (or virtual) hardware you are using, e.g. for Linux:\n\n`$ lscpu`\n\n* Operating System, e.g. for Linux:\n\n`$ uname -a`\n\n* SDK version, e.g. for Linux:\n\n```\n$ python3 --version\n$ make --version\n$ g++ --version\n```\n\n# Failure Information (for bugs)\n\nPlease help provide information about the failure if this is a bug. If it is not a bug, please remove the rest of this template.\n\n# Steps to Reproduce\n\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\n\n1. step 1\n2. step 2\n3. step 3\n4. etc.\n\n# Failure Logs\n\nPlease include any relevant log snippets or files. If it works under one configuration but not under another, please provide logs for both configurations and their corresponding outputs so it is easy to see where behavior changes.\n\nAlso, please try to **avoid using screenshots** if at all possible. Instead, copy/paste the console output and use [Github's markdown](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) to cleanly format your logs for easy readability.\n\nExample environment info:\n```\nllama.cpp$ git log | head -1\ncommit 2af23d30434a677c6416812eea52ccc0af65119c\n\nllama.cpp$ lscpu | egrep \"AMD|Flags\"\nVendor ID:                       AuthenticAMD\nModel name:                      AMD Ryzen Threadripper 1950X 16-Core Processor\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sme sev\nVirtualization:                  AMD-V\n\nllama.cpp$ python3 --version\nPython 3.10.9\n\nllama.cpp$ pip list | egrep \"torch|numpy|sentencepiece\"\nnumpy                         1.24.2\nnumpydoc                      1.5.0\nsentencepiece                 0.1.97\ntorch                         1.13.1\ntorchvision                   0.14.1\n\nllama.cpp$ make --version | head -1\nGNU Make 4.3\n\n$ md5sum ./models/65B/ggml-model-q4_0.bin\ndbdd682cce80e2d6e93cefc7449df487  ./models/65B/ggml-model-q4_0.bin\n```\n\nExample run with the Linux command [perf](https://www.brendangregg.com/perf.html)\n```\nllama.cpp$ perf stat ./main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p \"Please close your issue when it has been answered.\"\nmain: seed = 1679149377\nllama_model_load: loading model from './models/65B/ggml-model-q4_0.bin' - please wait ...\nllama_model_load: n_vocab = 32000\nllama_model_load: n_ctx   = 512\nllama_model_load: n_embd  = 8192\nllama_model_load: n_mult  = 256\nllama_model_load: n_head  = 64\nllama_model_load: n_layer = 80\nllama_model_load: n_rot   = 128\nllama_model_load: f16     = 2\nllama_model_load: n_ff    = 22016\nllama_model_load: n_parts = 8\nllama_model_load: ggml ctx size = 41477.73 MB\nllama_model_load: memory_size =  2560.00 MB, n_mem = 40960\nllama_model_load: loading model part 1/8 from './models/65B/ggml-model-q4_0.bin'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 2/8 from './models/65B/ggml-model-q4_0.bin.1'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 3/8 from './models/65B/ggml-model-q4_0.bin.2'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 4/8 from './models/65B/ggml-model-q4_0.bin.3'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 5/8 from './models/65B/ggml-model-q4_0.bin.4'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 6/8 from './models/65B/ggml-model-q4_0.bin.5'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 7/8 from './models/65B/ggml-model-q4_0.bin.6'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 8/8 from './models/65B/ggml-model-q4_0.bin.7'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\n\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n\nmain: prompt: 'Please close your issue when it has been answered.'\nmain: number of tokens in prompt = 11\n     1 -> ''\n 12148 -> 'Please'\n  3802 -> ' close'\n   596 -> ' your'\n  2228 -> ' issue'\n   746 -> ' when'\n   372 -> ' it'\n   756 -> ' has'\n  1063 -> ' been'\n  7699 -> ' answered'\n 29889 -> '.'\n\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\n\n\nPlease close your issue when it has been answered.\n@duncan-donut: I'm trying to figure out what kind of \"support\" you need for this script and why, exactly? Is there a question about how the code works that hasn't already been addressed in one or more comments below this ticket, or are we talking something else entirely like some sorta bugfixing job because your server setup is different from mine??\nI can understand if your site needs to be running smoothly and you need help with a fix of sorts but there should really be nothing wrong here that the code itself could not handle. And given that I'm getting reports about how it works perfectly well on some other servers, what exactly are we talking? A detailed report will do wonders in helping us get this resolved for ya quickly so please take your time and describe the issue(s) you see as clearly & concisely as possible!!\n@duncan-donut: I'm not sure if you have access to cPanel but you could try these instructions. It is worth a shot! Let me know how it goes (or what error message, exactly!) when/if ya give that code a go? [end of text]\n\n\nmain: mem per token = 71159620 bytes\nmain:     load time = 19309.95 ms\nmain:   sample time =   168.62 ms\nmain:  predict time = 223895.61 ms / 888.47 ms per token\nmain:    total time = 246406.42 ms\n\n Performance counter stats for './main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p Please close your issue when it has been answered.':\n\n        3636882.89 msec task-clock                #   14.677 CPUs utilized          \n             13509      context-switches          #    3.714 /sec                   \n              2436      cpu-migrations            #    0.670 /sec                   \n          10476679      page-faults               #    2.881 K/sec                  \n    13133115082869      cycles                    #    3.611 GHz                      (16.77%)\n       29314462753      stalled-cycles-frontend   #    0.22% frontend cycles idle     (16.76%)\n    10294402631459      stalled-cycles-backend    #   78.39% backend cycles idle      (16.74%)\n    23479217109614      instructions              #    1.79  insn per cycle         \n                                                  #    0.44  stalled cycles per insn  (16.76%)\n     2353072268027      branches                  #  647.002 M/sec                    (16.77%)\n        1998682780      branch-misses             #    0.08% of all branches          (16.76%)\n\n     247.802177522 seconds time elapsed\n\n    3618.573072000 seconds user\n      18.491698000 seconds sys\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-03T09:08:18+00:00",
    "closed_at": "2023-04-03T11:13:36+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/738/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/738"
  },
  {
    "number": 735,
    "title": "I'm pegging CPU (`./examples/chat.sh` works very slowly) on a 5800X3D / u22 linux, anything that can be done?",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nFaster responses.\r\n\r\n# Current Behavior\r\n\r\nUsed all 16 threads / 8 cores for seconds to minutes when responding to chat mode.\r\n\r\n# Environment and Context \r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n```\r\n(llama.cpp) \ud83d\udd25 lscpu\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         48 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  16\r\n  On-line CPU(s) list:   0-15\r\nVendor ID:               AuthenticAMD\r\n  Model name:            AMD Ryzen 7 5800X3D 8-Core Processor\r\n    CPU family:          25\r\n    Model:               33\r\n    Thread(s) per core:  2\r\n    Core(s) per socket:  8\r\n    Socket(s):           1\r\n    Stepping:            2\r\n    Frequency boost:     enabled\r\n    CPU max MHz:         4548.8281\r\n    CPU min MHz:         2200.0000\r\n    BogoMIPS:            6787.04\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht \r\n                         syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid\r\n                          aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx\r\n                          f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs s\r\n                         kinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ss\r\n                         bd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap cl\r\n                         flushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local\r\n                          clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushb\r\n                         yasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes\r\n                          vpclmulqdq rdpid overflow_recov succor smca fsrm\r\nVirtualization features: \r\n  Virtualization:        AMD-V\r\nCaches (sum of all):     \r\n  L1d:                   256 KiB (8 instances)\r\n  L1i:                   256 KiB (8 instances)\r\n  L2:                    4 MiB (8 instances)\r\n  L3:                    96 MiB (1 instance)\r\nNUMA:                    \r\n  NUMA node(s):          1\r\n  NUMA node0 CPU(s):     0-15\r\nVulnerabilities:         \r\n  Itlb multihit:         Not affected\r\n  L1tf:                  Not affected\r\n  Mds:                   Not affected\r\n  Meltdown:              Not affected\r\n  Mmio stale data:       Not affected\r\n  Retbleed:              Not affected\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\n  Spectre v2:            Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affe\r\n                         cted\r\n  Srbds:                 Not affected\r\n  Tsx async abort:       Not affected\r\n\r\n* Operating System, e.g. for Linux:\r\n* \r\nLinux stardart 5.15.0-58-generic #64-Ubuntu SMP Thu Jan 5 11:43:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n(llama.cpp) \ud83d\udd25 python3 --version\r\nPython 3.10.6\r\n(llama.cpp) \ud83d\udd25 make --version\r\nGNU Make 4.3\r\nBuilt for x86_64-pc-linux-gnu\r\nCopyright (C) 1988-2020 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.\r\n(llama.cpp) \ud83d\udd25  g++ --version\r\ng++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\nCopyright (C) 2021 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nn/a\r\n\r\n# Steps to Reproduce\r\n\r\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\r\n\r\n1. Use a AMD Ryzen 7 5800X3D, 64gb ram, and  \r\n2. install & run ./examples/chat.sh\r\n\r\n\r\n# Failure Logs\r\n\r\nn/a\r\n\r\nExample environment info:\r\n```\r\n\ud83d\udd25 git log | head -1\r\ncommit a0c05164168297c04737936ad0cad849a512547a\r\n\ud83d\udd25 lscpu | egrep \"AMD|Flags\"\r\nVendor ID:                       AuthenticAMD\r\nModel name:                      AMD Ryzen 7 5800X3D 8-Core Processor\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\nVirtualization: \r\n\r\n(llama.cpp) \ud83d\udd25 python3 --version\r\nPython 3.10.6\r\n(llama.cpp) \ud83d\udd25 pip list | egrep \"torch|numpy|sentencepiece\"\r\nnumpy                    1.24.2\r\nsentencepiece            0.1.97\r\ntorch                    2.0.0\r\n(llama.cpp) \ud83d\udd25 md5sum ./models/7B/ggml-model-q4_0.bin\r\nf8b83a4351a2c4413aa1bb9bb995556f  ./models/7B/ggml-model-q4_0.bin\r\n```\r\nperf:\r\n```\r\n(llama.cpp) \ud83d\udd25 sudo perf stat ./main -m ./models/7B/ggml-model-q4_0.bin -t 16 -n 1024 -p \"Please close your issue when it has been answered.\"\r\nmain: seed = 1680500041\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml map size = 4017.70 MB\r\nllama_model_load: ggml ctx size =  81.25 KB\r\nllama_model_load: mem required  = 5809.78 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading tensors from './models/7B/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 16 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 512, n_batch = 8, n_predict = 1024, n_keep = 0\r\n\r\n\r\n Please close your issue when it has been answered.\r\nPlease do not use this category for questions that can be directed to the main support page or other sections of our website. [end of text]\r\n\r\nllama_print_timings:        load time =  7921.97 ms\r\nllama_print_timings:      sample time =    15.50 ms /    26 runs   (    0.60 ms per run)\r\nllama_print_timings: prompt eval time =  8986.05 ms /    11 tokens (  816.91 ms per token)\r\nllama_print_timings:        eval time = 92976.12 ms /    25 runs   ( 3719.04 ms per run)\r\nllama_print_timings:       total time = 102616.54 ms\r\n\r\n Performance counter stats for './main -m ./models/7B/ggml-model-q4_0.bin -t 16 -n 1024 -p Please close your issue when it has been answered.':\r\n\r\n      1,461,040.62 msec task-clock                #   14.211 CPUs utilized          \r\n         3,515,608      context-switches          #    2.406 K/sec                  \r\n             1,639      cpu-migrations            #    1.122 /sec                   \r\n           590,140      page-faults               #  403.918 /sec                   \r\n 6,088,040,635,313      cycles                    #    4.167 GHz                      (83.32%)\r\n     6,073,514,788      stalled-cycles-frontend   #    0.10% frontend cycles idle     (83.36%)\r\n     9,381,196,263      stalled-cycles-backend    #    0.15% backend cycles idle      (83.34%)\r\n13,159,707,650,067      instructions              #    2.16  insn per cycle         \r\n                                                  #    0.00  stalled cycles per insn  (83.31%)\r\n 3,292,654,196,851      branches                  #    2.254 G/sec                    (83.35%)\r\n       187,524,242      branch-misses             #    0.01% of all branches          (83.35%)\r\n\r\n     102.808235293 seconds time elapsed\r\n\r\n    1439.049630000 seconds user\r\n      17.789593000 seconds sys\r\n```\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-03T05:38:52+00:00",
    "closed_at": "2024-04-11T01:07:06+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/735/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/735"
  },
  {
    "number": 731,
    "title": "How different is Macbook / non-macbook performance?",
    "body": "I was wondering how performant the Llama models are on x86-64. I assumed performance would be similar, but it seems like on M1 macs have lots of CPU improvements that would mean, for example, a llama-13B model running on an M1 mac would not also be able to run on an Lenovo T580.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-03T03:37:06+00:00",
    "closed_at": "2023-04-07T16:39:35+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/731/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/731"
  },
  {
    "number": 730,
    "title": "New kv_cache API insufficient to restore model state",
    "body": "I may be doing something wrong or misunderstanding the purpose of the `kv_cache` API but I believe the recent PR #685 by @chrfalch which added the ability to get / set the `kv_cache` is still insufficient to restore the state of the model even when resetting external model state such as `last_n_tokens_data` and `n_past`.\r\n\r\nHere is a minimal example\r\n\r\n```c++\r\n#include \"llama.h\"\r\n#include <vector>\r\n#include <iostream>\r\n\r\nusing namespace std;\r\n\r\nint main() {\r\n    // init\r\n    auto params = llama_context_default_params();\r\n    auto ctx = llama_init_from_file(\"../../models/ggml-model.bin\", params);\r\n    auto tokens = vector<llama_token>(params.n_ctx);\r\n    auto prompt = \"The quick brown fox\";\r\n    auto n_tokens = llama_tokenize(ctx, prompt, tokens.data(), tokens.size(), true);\r\n\r\n    // evaluate prompt\r\n    llama_eval(ctx, tokens.data(), n_tokens, 0, 12);\r\n    auto last_n_tokens_size = 64;\r\n    auto last_n_tokens_data = vector<llama_token>(last_n_tokens_size, 0);\r\n    last_n_tokens_data.insert(last_n_tokens_data.end(), tokens.data(), tokens.data() + n_tokens);\r\n    auto n_past = n_tokens;\r\n\r\n    // save state\r\n    auto kv_cache_size = llama_get_kv_cache_size(ctx);\r\n    auto kv_cache_token_count = llama_get_kv_cache_token_count(ctx);\r\n    auto kv_cache = llama_get_kv_cache(ctx);\r\n    auto kv_cache_copy = vector<uint8_t>(kv_cache, kv_cache + kv_cache_size);\r\n    auto n_past_copy = n_past;\r\n    auto last_n_tokens_data_copy = vector<llama_token>(last_n_tokens_data);\r\n    \r\n    // first run\r\n    cout << prompt;\r\n    for (auto i = 0; i < 6; i++) {\r\n        auto next_token = llama_sample_top_p_top_k(\r\n            ctx,\r\n            last_n_tokens_data.data() + last_n_tokens_data.size() - n_past,\r\n            last_n_tokens_size,\r\n            1,\r\n            1.0,\r\n            0.0,\r\n            1.1\r\n        );\r\n        auto next_token_str = llama_token_to_str(ctx, next_token);\r\n        last_n_tokens_data.push_back(next_token);\r\n        cout << next_token_str;\r\n        llama_eval(ctx, &next_token, 1, n_past, 12);\r\n        n_past += 1;\r\n    }\r\n    cout << endl;\r\n    //\r\n\r\n    // restore state\r\n    llama_set_kv_cache(ctx, kv_cache_copy.data(), kv_cache_size, kv_cache_token_count);\r\n    last_n_tokens_data = last_n_tokens_data_copy;\r\n    n_past = n_past_copy;\r\n    //\r\n\r\n    // second run\r\n    cout << prompt;\r\n    for (auto i = 0; i < 6; i++) {\r\n        auto next_token = llama_sample_top_p_top_k(\r\n            ctx,\r\n            last_n_tokens_data.data() + last_n_tokens_data.size() - n_past,\r\n            last_n_tokens_size,\r\n            1,\r\n            1.0,\r\n            0.0,\r\n            1.1\r\n        );\r\n        auto next_token_str = llama_token_to_str(ctx, next_token);\r\n        last_n_tokens_data.push_back(next_token);\r\n        cout << next_token_str;\r\n        llama_eval(ctx, &next_token, 1, n_past, 12);\r\n        n_past += 1;\r\n    }\r\n    cout << endl;\r\n    //\r\n    return 0;\r\n}\r\n```\r\n\r\nI'd expect the following output\r\n\r\n```\r\nThe quick brown fox jumps over the lazy dog\r\nThe quick brown fox jumps over the lazy dog\r\n```\r\n\r\nBut instead I get \r\n\r\n```\r\nThe quick brown fox jumps over the lazy dog\r\nThe quick brown fox.\r\nThe quick brown fo\r\n```\r\n\r\nWhich implies the model is still generating from the end of the first run.",
    "labels": [
      "bug",
      "help wanted",
      "good first issue",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2023-04-03T03:28:49+00:00",
    "closed_at": "2023-04-23T13:51:21+00:00",
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/730/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/730"
  },
  {
    "number": 727,
    "title": "Using Repeat_last_n and Repeat_penalty To Avoid Going Back and Repeating Itself?",
    "body": "I set --repeat_last_n 256 --repeat_penalty 1.9.\r\nHowever, after a while, it keeps going back to certain sentences and repeating itself as if it's stuck in a loop.\r\nIs this a bug, or am I using the parameters incorrectly?\r\nWhat's the maximum value you can set for repeat_penalty, so it doesn't repeat itself?",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-03T00:58:19+00:00",
    "closed_at": "2024-04-11T01:07:08+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/727/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/727"
  },
  {
    "number": 726,
    "title": "Add support for stopping words [Automation]",
    "body": "A stopping criteria or \"stop sequence\" would be highly appreciated here. These models are already pretty for information retrieval but for automating stuff be it maybe some hacky way to execute code or simply automate textual tasks is impossible. You can just use the model and expect it to work everytime with every input. And even if is the case, your won't last long before your prompt needs a modification.\n\n\nWill you at least include that feature some day? I'm mot a cpp expert so I'm not sure how difficult would be this to implement.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-02T23:45:13+00:00",
    "closed_at": "2023-07-28T19:46:36+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/726/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/726"
  },
  {
    "number": 725,
    "title": "[Feature Request] Support for Filter Assisted Decoding/Constrained Text Generation",
    "body": "## Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n## Feature Request\r\n\r\nHi all, I showed in a [recent paper at COLING 2022](https://paperswithcode.com/paper/most-language-models-can-be-poets-too-an-ai) that filtering a language models vocabulary according to lexical, semantic, or phonetic constraints at each time-step is an extremely powerful technique. For example, it is not easy to get ChatGPT or similar models to correctly generate a paragraph of correct english text which doesn't use the letter \"e\" without this technique (it's trivial by just filtering the tokens out which have the letter \"e\" from the vocabulary). LLM's like ChatGPT don't have any understanding of syllable counts due to their tokenizers being BPE/subword level, and are thus quite unreliable at generating haikus. [Gwern actually wrote a long analysis of LLM's in the context of their limitations on creative writing which cites my paper!](https://gwern.net/gpt-3)\r\n\r\nIn this paper I contributed two system demonstrations, one of which was a [DearPyGUI powered \"text generation studio\"](https://github.com/Hellisotherpeople/Constrained-Text-Generation-Studio) which was sort of a parallel idea to what we see in this project, and the other is a [huggingface hosted demo that anyone can play with](https://huggingface.co/spaces/Hellisotherpeople/Gadsby). Gadsby also shows that this technique is applicable to text-to-text models (enabling constrained summarization or translation). [My code quality is awful](https://i.redd.it/4dnvvjeuq0541.jpg) but there should be enough there to figure out how to implement this here, and the technique is incredibly simple anyway.\r\n\r\nUnfortunately, the HF code that I am using for running my models is quite slow in comparison to techniques which have matured recently, such as yours. I am becoming far more busy and will likely not have the time for me to develop a PR with this feature myself for this repo (I don't even code in C++ normally!). I'm asking for one of the absolute geniuses who is working on this project to implement, or at least help guide me, with implementing this feature. \r\n\r\nI was stuck demoing this technique at the HF meetup this week on my M1 macbook pro with no warning, and thus I had to use distilgpt2 (a uniquely terrible model) to showcase this technique. I'd LOVE to be able to showcase the best of the best models. \r\n\r\n## Constrained Beam Search\r\n\r\nIn addition to my own ideas, there is a similar technique utilizing constrained beam search that [huggingface has quietly supported for a long time.](https://huggingface.co/blog/constrained-beam-search) This technique is extremely powerful and it should also be supported if possible.\r\n\r\n## Context \r\n\r\n[My talk at COLING 2022 about Filter Assisted Decoding may be useful](https://www.youtube.com/watch?v=92hyQJRcFpM)\r\n\r\nI realize that this may be deemed better as an \"extension\" but I really think that these are basic enough techniques that we should try to have them supported directly in here.\r\n\r\n\r\n\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-02T21:58:38+00:00",
    "closed_at": "2024-04-11T01:07:09+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/725/reactions",
      "total_count": 10,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 4,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/725"
  },
  {
    "number": 723,
    "title": "Not getting randomized output for the same prompt despite seed changing ",
    "body": "Recently I noticed that I'm getting the exact same answers to my prompt every time where this wasn't the case before. I have no idea what happened to trigger this.\r\n\r\nI have a version of chat-with-bob.txt where I ask it to tell my a children's story based on preferences I specified in the text in the prompt and up until now it was obviously using a different seed every time and thus generating a new story every time but out of nowhere I'm getting an identical story every time (despite the fact that the output IS showing a different seed every time upon execution). \r\n\r\nAs a test, I changed a couple of characters in the prompt in chat-with-bob.txt to see if the output would change and it did indeed generate an entirely different story but again, it exhibited the same behavior and repeatedly gave me the same story when I ran the identical prompt. FYI, I changed \"fantasy story\" to \"fantastical story in chat-with-bob.txt.\r\n\r\nI just did a new fresh pull of the code and compiled from scratch a few minutes ago so everything is up to date. Using the Alpaca/33B-LORA-merged/ model BTW. \r\n\r\nSee chat-with-bob.txt and output.txt for examples.\r\n\r\n[chat-with-bob.txt](https://github.com/ggerganov/llama.cpp/files/11133378/chat-with-bob.txt)\r\n[output.txt](https://github.com/ggerganov/llama.cpp/files/11133380/output.txt)",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-02T21:29:22+00:00",
    "closed_at": "2023-04-03T22:02:02+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/723/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/723"
  },
  {
    "number": 722,
    "title": "Rockchip RK3588 perf",
    "body": "Just did a very simple run with llama-7b-4bit. It... took a while. Had it run in a screen. But, it worked!\r\n\r\n```\r\nroot@FriendlyWrt /s/o/llama.cpp (master)# time ./main --color -m models/ggml-model-q4_0.bin -p \"Hello there!\"\r\nmain: seed = 1680443840\r\nllama_model_load: loading model from 'models/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml map size = 4017.70 MB\r\nllama_model_load: ggml ctx size =  81.25 KB\r\nllama_model_load: mem required  = 5809.78 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading tensors from 'models/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 8 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | VSX = 0 |\r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 512, n_batch = 8, n_predict = 128, n_keep = 0\r\n\r\n\r\n Hello there! I am a freelance illustrator based in New Zealand. I grew up with an appreciation for the natural world, which has inspired me to create my work through observation and playful experimentation.\r\nMy focus is on watercolour painting (in particular), as well as digital art & animation. My style is bright & bold, vibrant, dynamic & colourful - I love animals!\r\nI am always keen to collaborate with other artists/creatives, so if you are interested in working together please feel free to drop me a line. [end of text]\r\n\r\nllama_print_timings:        load time = 93487.23 ms\r\nllama_print_timings:      sample time =   704.72 ms /   115 runs   (    6.13 ms per run)\r\nllama_print_timings: prompt eval time = 92466.10 ms /     4 tokens (23116.52 ms per token)\r\nllama_print_timings:        eval time = 11195694.23 ms /   114 runs   (98207.84 ms per run)\r\nllama_print_timings:       total time = 11289895.19 ms\r\n\r\n________________________________________________________\r\nExecuted in  188.18 mins    fish           external\r\n   usr time  324.60 mins    0.00 millis  324.60 mins\r\n   sys time   11.70 mins    1.70 millis   11.70 mins\r\n```\r\n\r\nModel was loaded from external microSD via internal bus.\r\n\r\nIm quite amazed this worked at all, honestly.\r\n\r\nCPU Info in detail:\r\n```\r\n# lscpu\r\nArchitecture:           aarch64\r\n  CPU op-mode(s):       32-bit, 64-bit\r\n  Byte Order:           Little Endian\r\nCPU(s):                 8\r\n  On-line CPU(s) list:  0-7\r\nVendor ID:              ARM\r\n  Model name:           Cortex-A55\r\n    Model:              0\r\n    Thread(s) per core: 1\r\n    Core(s) per socket: 4\r\n    Socket(s):          1\r\n    Stepping:           r2p0\r\n    CPU(s) scaling MHz: 100%\r\n    CPU max MHz:        1800.0000\r\n    CPU min MHz:        408.0000\r\n    BogoMIPS:           48.00\r\n    Flags:              fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm lrcpc dcpop asimddp\r\n  Model name:           Cortex-A76\r\n    Model:              0\r\n    Thread(s) per core: 1\r\n    Core(s) per socket: 2\r\n    Socket(s):          2\r\n    Stepping:           r4p0\r\n    CPU(s) scaling MHz: 68%\r\n    CPU max MHz:        2352.0000\r\n    CPU min MHz:        408.0000\r\n    BogoMIPS:           48.00\r\n    Flags:              fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm lrcpc dcpop asimddp\r\nCaches (sum of all):\r\n  L1d:                  384 KiB (8 instances)\r\n  L1i:                  384 KiB (8 instances)\r\n  L2:                   2.5 MiB (8 instances)\r\n  L3:                   3 MiB (1 instance)\r\nVulnerabilities:\r\n  Itlb multihit:        Not affected\r\n  L1tf:                 Not affected\r\n  Mds:                  Not affected\r\n  Meltdown:             Not affected\r\n  Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\n  Spectre v1:           Mitigation; __user pointer sanitization\r\n  Spectre v2:           Vulnerable: Unprivileged eBPF enabled\r\n  Srbds:                Not affected\r\n  Tsx async abort:      Not affected\r\n```\r\n(`/proc/cpuinfo` doesnt give any more useful details here, sadly.)\r\n\r\nHardware is a [FriendlyElec NanoPi R6s](https://www.friendlyelec.com/index.php?route=product/product&product_id=289)",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-02T20:39:28+00:00",
    "closed_at": "2023-04-02T22:14:36+00:00",
    "comments": 103,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/722/reactions",
      "total_count": 11,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 5,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/722"
  },
  {
    "number": 719,
    "title": "Question: Why prompt is being run trough the network before generating new tokens?",
    "body": "As I understand, the NN doesn't have a state, so you should be able to put whatever tokens into context and start generating new tokens immediately. But right now, it first runs the NN on the prompt it seems? So with a long prompt, it takes some time until it starts to generate new tokens.\r\n\r\nI though I was missing something, but huggingface `transformers` starts to generate the tokens immediately.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-02T19:48:07+00:00",
    "closed_at": "2023-04-03T14:43:18+00:00",
    "comments": 31,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/719/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/719"
  },
  {
    "number": 717,
    "title": "Code showing when running.",
    "body": "When I start chat.exe with a alpaca bin I get.\r\nmain: seed = 1680456908\r\nllama_model_load: loading model from 'models/llama-7B/ggml-model.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 3\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml map size = 4820.95 MB\r\nllama_model_load: ggml ctx size =  81.25 KB\r\nllama_model_load: mem required  = 6613.03 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading tensors from 'models/llama-7B/ggml-model.bin'\r\nllama_model_load: model size =  4820.52 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 4 / 8 | AVX = 1 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | VSX = 0 |\r\nmain: interactive mode on.\r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 512, n_batch = 8, n_predict = 128, n_keep = 0\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n\r\n \ufeffusing System;\r\n\r\n\r\nusing System.Collections.Generic;\r\nusing System.Linq;\r\nusing System.Text;\r\nusing System.Threading.Tasks;\r\n\r\nnamespace _1.Write_a_program_to_find_the_minimum_element_in_an_array\r\n{\r\n    class Program\r\n    {\r\n        static void Main()\r\n        {\r\n            //Create an array of five elements\r\n            int[] arr = new int[5];\r\n\r\n            //Fill the array with random values between 0 and 10",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-02T17:51:23+00:00",
    "closed_at": "2023-04-02T19:49:52+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/717/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/717"
  },
  {
    "number": 715,
    "title": "Windows Quantize isn't working ",
    "body": "![image](https://user-images.githubusercontent.com/112736962/229361374-f3a72b6c-091b-4ddd-997d-95b5c97ef01f.png)\r\n\r\nIm on win11 and barely know how to get this even remotely running and I am having a stroke of whats going wrong with this. ",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-02T15:12:02+00:00",
    "closed_at": "2023-04-02T15:35:43+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/715/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/715"
  },
  {
    "number": 714,
    "title": "Add support to FMA3/FMA4 instructions ",
    "body": "That improves dot products performance on Haswell+",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-02T14:57:14+00:00",
    "closed_at": "2024-04-11T01:07:10+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/714/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/714"
  },
  {
    "number": 713,
    "title": "idk wth is happening help",
    "body": "\r\nPS C:\\Users\\Admin> cd D:\\Software\\GPT4ALL\\llama.cpp\r\nPS D:\\Software\\GPT4ALL\\llama.cpp> make\r\nprocess_begin: CreateProcess(NULL, uname -s, ...) failed.\r\nMakefile:2: pipe: No error\r\nprocess_begin: CreateProcess(NULL, uname -p, ...) failed.\r\nMakefile:6: pipe: No error\r\nprocess_begin: CreateProcess(NULL, uname -m, ...) failed.\r\nMakefile:10: pipe: No error\r\n'cc' is not recognized as an internal or external command,\r\noperable program or batch file.\r\n'head' is not recognized as an internal or external command,\r\noperable program or batch file.\r\nI llama.cpp build info:\r\nI UNAME_S:\r\nI UNAME_P:\r\nI UNAME_M:\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -mfma -mf16c -mavx -mavx2\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function\r\nI LDFLAGS:\r\nI CC:\r\nI CXX:\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -mfma -mf16c -mavx -mavx2   -c ggml.c -o ggml.o\r\nprocess_begin: CreateProcess(NULL, cc -I. -O3 -DNDEBUG -std=c11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -mfma -mf16c -mavx -mavx2 -c ggml.c -o ggml.o, ...) failed.\r\nmake (e=2): The system cannot find the file specified.\r\nmake: *** [Makefile:229: ggml.o] Error 2",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-02T14:19:02+00:00",
    "closed_at": "2023-04-02T14:54:20+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/713/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/713"
  },
  {
    "number": 712,
    "title": "Running llama.cpp on android just prints out the question",
    "body": "I ran llama.cpp on my android phone which has 8 threads and 8GB of ram in which around 7.16 GB is available, that is more than enough to run the 7B Alpaca model on it. But when i run it, it just repeats the question that i provided to it. I am using the `./examples/chat.sh` file. Why does it do that? How do i solve it?",
    "labels": [
      "android",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-02T14:16:00+00:00",
    "closed_at": "2024-04-11T01:07:11+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/712/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/712"
  },
  {
    "number": 708,
    "title": "[Question] Can I load a the huggingface llama model aswell?",
    "body": "I have downloaded the llama-model from [here](https://huggingface.co/decapoda-research/llama-7b-hf). There it got converted to be compatible with pytorch. But the biggest advantage is that it is actually available. The magnet link from that [PR](https://github.com/facebookresearch/llama/pull/73) has no trackers so it's not starting to download at least for me. And the IPFS files always have a different checksum when I download them. So since I only have the huggingface-version is it possible use their model with llama.cpp somehow? ",
    "labels": [
      "model"
    ],
    "state": "closed",
    "created_at": "2023-04-02T11:03:06+00:00",
    "closed_at": "2023-04-03T17:50:18+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/708/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/708"
  },
  {
    "number": 707,
    "title": "Convert pytorch-based models to work with llama.cpp?",
    "body": "Out of curiosity, I want to see if I can launch a very mini AI on my little network server. It usually has around 3GB of free memory, and it'd be nice to chat with it sometimes. For that, I'd like to try a smaller model like Pythia.\r\n\r\nSo I would like to know:\r\n- Can I convert `pytorch_model*.bin` to ggjm?\r\n- Can I quantize those models to use even less memory as a sort of post-processing step?\r\n\r\nI looked at the existing `convert_*.py` scripts, but none of those seemed to be for this type of model.\r\n\r\nThanks in advance!",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-02T10:53:58+00:00",
    "closed_at": "2024-04-11T01:07:12+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/707/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/707"
  },
  {
    "number": 705,
    "title": "Windows page fault disk i/o slow on first load",
    "body": "Hello,\r\n\r\nAs of https://github.com/ggerganov/llama.cpp/pull/613 I have experienced significant regression in model loading speed (I'm on windows, compiled msvc llama.cpp, llama.cpp is located on HDD to prevent SSD wear in my case)\r\n\r\nIt takes roughly 15 minutes for model to load first time after each computer restart/hibernation, during this time my HDD usage is at 100% and my non-llama.cpp read/write operations are slowed down on my pc\r\n![hdd](https://user-images.githubusercontent.com/76458234/229345728-b597023b-f7e3-4a8b-b550-3159863ba03d.png)\r\n\r\nBefore that, previous commits took 60 - 180 seconds at worst to load model first time, and after first loading occured, model loaded within 5 - 10 seconds on each program restart until pc reboot/hibernation\r\n\r\nBefore Commit:\r\n![timings2](https://user-images.githubusercontent.com/76458234/229347345-2053d645-0f26-42ef-9f8e-5fc69ad04e1c.png)\r\n\r\nAfter:\r\n![timings1](https://user-images.githubusercontent.com/76458234/229345966-ee606c92-e7cb-42f6-8b6f-2d6924ebcfee.png)\r\n\r\nI see reason why model might load faster for some while slower (like my case) for others after recent changes, therefore in my opinion best solution is adding parameter that lets people disable llama.cpp's recent model loading changes if thats possible\r\n\r\n- Thanks",
    "labels": [
      "performance",
      "windows",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-02T10:04:24+00:00",
    "closed_at": "2024-04-11T01:07:14+00:00",
    "comments": 37,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/705/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/705"
  },
  {
    "number": 704,
    "title": "Update *-to-ggml.py scripts for new ggjt model format",
    "body": "See title, basically.\r\n\r\nWe should probably keep the option of generating the old formats.\r\n\r\nRevert #690 when done.\r\n\r\nRelated: #545",
    "labels": [
      "script"
    ],
    "state": "closed",
    "created_at": "2023-04-02T09:49:22+00:00",
    "closed_at": "2023-05-03T18:37:53+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/704/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/704"
  },
  {
    "number": 702,
    "title": "4bit 65B model overflow 64GB of RAM",
    "body": "# Prerequisites\r\n\r\nI am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\nI carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\nI searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\r\nI reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\nDuring inference, there should be no or minimum disk activities going on, and disk should not be a bottleneck once pass the model loading stage.\r\n\r\n# Current Behavior\r\nMy disk should have a continuous reading speed of over 100MB/s, however, during the loading of the model, it only loads at around 40MB/s. After this very slow loading of Llama 65b model (converted from GPTQ with group size of 128), llama.cpp start to inference, however during the inference the programme continue to occupy the disk and reads at 40MB/s. The generation speed is also extremely slow, at around 10 minutes per token.\r\nHowever, if it's 30b model or smaller, llama.cpp work as expected.\r\n\r\n# Environment and Context \r\n\r\nNote: My interfercing were done using oobabooga's [text-generation-webui](https://github.com/oobabooga/text-generation-webui)'s implementation of llama.cpp, as I have no idea how to use llama.cpp by itself...\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\nCPU: Ryzen 5500\r\n    Flags: \r\n```\r\n                         fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mc\r\n                         a cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall n\r\n                         x mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_go\r\n                         od nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl p\r\n                         ni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2api\r\n                         c movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_le\r\n                         gacy svm extapic cr8_legacy abm sse4a misalignsse 3dnow\r\n                         prefetch osvw ibs skinit wdt tce topoext perfctr_core p\r\n                         erfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw\r\n                         _pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 \r\n                         avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap c\r\n                         lflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cq\r\n                         m_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero \r\n                         irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm\r\n                         _lock nrip_save tsc_scale vmcb_clean flushbyasid decode\r\n                         assists pausefilter pfthreshold avic v_vmsave_vmload vg\r\n                         if v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid ove\r\n                         rflow_recov succor smca fsrm\r\n```\r\nRAM: 64GB of DDR4 running at 3000MHz\r\nDisk where I stored my model file: 2 Barraccuda 1TB HDD in Raid 1 configuration\r\nSystem SSD: NV2 500GB\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`Linux fgdfgfthgr-MS-7C95 5.15.0-69-generic #76-Ubuntu SMP Fri Mar 17 17:19:29 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux`\r\n\r\n* SDK version, e.g. for Linux:\r\n```\r\nPython 3.9.13\r\nGNU Make 4.3\r\ng++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nNot sure what other information is there to provide.\r\n\r\n# Steps to Reproduce\r\n\r\n1. Load a 65b model using oobabooga's [text-generation-webui](https://github.com/oobabooga/text-generation-webui)'s implementation of llama.cpp.\r\n2. Use `iostat -y -d 5` to monitor disk activity during loading and inference.\r\n\r\n# Failure Logs\r\n\r\nLlama.cpp version: \r\n```\r\nhttps://pypi.org/project/llamacpp/\r\n0.1.11\r\n```\r\n\r\nPip environment:\r\n```\r\naccelerate               0.18.0\r\naiofiles                 23.1.0\r\naiohttp                  3.8.4\r\naiosignal                1.3.1\r\naltair                   4.2.2\r\nanyio                    3.6.2\r\nasync-timeout            4.0.2\r\nattrs                    22.2.0\r\nbitsandbytes             0.37.2\r\ncertifi                  2022.12.7\r\ncharset-normalizer       3.1.0\r\nclick                    8.1.3\r\ncmake                    3.26.1\r\ncontourpy                1.0.7\r\ncycler                   0.11.0\r\ndatasets                 2.11.0\r\ndill                     0.3.6\r\nentrypoints              0.4\r\nfastapi                  0.95.0\r\nffmpy                    0.3.0\r\nfilelock                 3.10.7\r\nflexgen                  0.1.7\r\nfonttools                4.39.3\r\nfrozenlist               1.3.3\r\nfsspec                   2023.3.0\r\ngradio                   3.24.0\r\ngradio_client            0.0.5\r\nh11                      0.14.0\r\nhttpcore                 0.16.3\r\nhttpx                    0.23.3\r\nhuggingface-hub          0.13.3\r\nidna                     3.4\r\nJinja2                   3.1.2\r\njsonschema               4.17.3\r\nkiwisolver               1.4.4\r\nlinkify-it-py            2.0.0\r\nlit                      16.0.0\r\nllamacpp                 0.1.11\r\nMarkdown                 3.4.3\r\nmarkdown-it-py           2.2.0\r\nMarkupSafe               2.1.2\r\nmatplotlib               3.7.1\r\nmdit-py-plugins          0.3.3\r\nmdurl                    0.1.2\r\nmpmath                   1.3.0\r\nmultidict                6.0.4\r\nmultiprocess             0.70.14\r\nnetworkx                 3.0\r\nnumpy                    1.24.2\r\nnvidia-cublas-cu11       11.10.3.66\r\nnvidia-cuda-cupti-cu11   11.7.101\r\nnvidia-cuda-nvrtc-cu11   11.7.99\r\nnvidia-cuda-runtime-cu11 11.7.99\r\nnvidia-cudnn-cu11        8.5.0.96\r\nnvidia-cufft-cu11        10.9.0.58\r\nnvidia-curand-cu11       10.2.10.91\r\nnvidia-cusolver-cu11     11.4.0.1\r\nnvidia-cusparse-cu11     11.7.4.91\r\nnvidia-nccl-cu11         2.14.3\r\nnvidia-nvtx-cu11         11.7.91\r\norjson                   3.8.9\r\npackaging                23.0\r\npandas                   1.5.3\r\npeft                     0.2.0\r\nPillow                   9.4.0\r\npip                      23.0.1\r\npsutil                   5.9.4\r\nPuLP                     2.7.0\r\npyarrow                  11.0.0\r\npydantic                 1.10.7\r\npydub                    0.25.1\r\npyparsing                3.0.9\r\npyrsistent               0.19.3\r\npython-dateutil          2.8.2\r\npython-multipart         0.0.6\r\npytz                     2023.3\r\nPyYAML                   6.0\r\nquant-cuda               0.0.0\r\nregex                    2023.3.23\r\nrequests                 2.28.2\r\nresponses                0.18.0\r\nrfc3986                  1.5.0\r\nrwkv                     0.7.1\r\nsafetensors              0.3.0\r\nsemantic-version         2.10.0\r\nsentencepiece            0.1.97\r\nsetuptools               65.6.3\r\nsix                      1.16.0\r\nsniffio                  1.3.0\r\nstarlette                0.26.1\r\nsympy                    1.11.1\r\ntokenizers               0.13.2\r\ntoolz                    0.12.0\r\ntorch                    2.0.0\r\ntorchaudio               2.0.1\r\ntorchvision              0.15.1\r\ntqdm                     4.65.0\r\ntransformers             4.28.0.dev0\r\ntriton                   2.0.0\r\ntyping_extensions        4.5.0\r\nuc-micro-py              1.0.1\r\nurllib3                  1.26.15\r\nuvicorn                  0.21.1\r\nwebsockets               10.4\r\nwheel                    0.38.4\r\nxxhash                   3.2.0\r\nyarl                     1.8.2\r\n(textgen) fgdfgfthgr@fgdfgfthgr-MS-7C95:/mnt/7018F20D48B6C548/text-generation-webui$ y-it-py            2.0.0\r\nlit                      16.0.0\r\nllamacpp                 0.1.11\r\nMarkdown                 3.4.3\r\nmarkdown-it-py           2.2.0\r\nMarkupSafe               2.1.2\r\nmatplotlib               3.7.1\r\nmdit-py-plugins          0.3.3\r\nmdurl                    0.1.2\r\nmpmath                   1.3.0\r\n```\r\n\r\nmd5sum ggml-model-q4_0.bin\r\n3073a8eedd1252063ad9b440af7c90cc  ggml-model-q4_1.bin\r\n",
    "labels": [
      "need more info",
      "performance",
      "linux"
    ],
    "state": "closed",
    "created_at": "2023-04-02T08:37:42+00:00",
    "closed_at": "2023-04-19T08:20:48+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/702/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/702"
  },
  {
    "number": 701,
    "title": "How to convert old ALPACA q4_0 model into ggjt format?",
    "body": "I'm trying to use a python script, but it returns the following error:\r\n\r\nd:\\ALPACA2>python migrate-ggml-2023-03-30-pr613.py ggml-alpaca-7b-q4.bin ggml-alpaca-7b-q4-ggjt.bin\r\nTraceback (most recent call last):\r\n  File \"d:\\ALPACA2\\migrate-ggml-2023-03-30-pr613.py\", line 313, in <module>\r\n    main()\r\n  File \"d:\\ALPACA2\\migrate-ggml-2023-03-30-pr613.py\", line 274, in main\r\n    tokens = read_tokens(fin, hparams)\r\n  File \"d:\\ALPACA2\\migrate-ggml-2023-03-30-pr613.py\", line 135, in read_tokens\r\n    word = fin.read(length)\r\nValueError: read length must be non-negative or -1\r\n",
    "labels": [
      "duplicate",
      "script"
    ],
    "state": "closed",
    "created_at": "2023-04-02T08:29:38+00:00",
    "closed_at": "2023-04-04T19:32:30+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/701/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/701"
  },
  {
    "number": 699,
    "title": "WIndows build fails with -DBUILD_SHARED_LIBS=ON ",
    "body": "If you build on windows using -DBUILD_SHARED_LIBS=ON, it fails with linker errors.\r\n\r\n```\r\ncommon.obj : error LNK2019: unresolved external symbol ggml_mlock_supported referenced in function \"void __cdecl gpt_print_usage(int,char * *,struct gpt_params const &)\" (?gpt_print_usage@@YAXHPEAPEADAEB\r\nUgpt_params@@@Z)\r\nquantize.obj : error LNK2019: unresolved external symbol ggml_time_init referenced in function main\r\nquantize.obj : error LNK2019: unresolved external symbol ggml_init referenced in function main\r\n```\r\n\r\nI don't have enough knowledge in makefiles to fix the issue correctly but a workaround solved it for me.\r\n\r\n1. Open the generated solution in visual studio\r\n2. Add ggml in references of the failing projects.\r\n3. Right click ggml under References-> Properties -> Set Link Library Dependencies to True\r\n\r\n![image](https://user-images.githubusercontent.com/7353840/229332398-361f32c0-72d0-42b4-82fb-cde7e844b35d.png)\r\n![image](https://user-images.githubusercontent.com/7353840/229332410-f445e4d0-1531-41ee-9b08-d8d10cfadbb8.png)\r\n\r\n",
    "labels": [
      "bug",
      "build",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-02T05:05:28+00:00",
    "closed_at": "2024-04-11T01:07:15+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/699/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/699"
  },
  {
    "number": 696,
    "title": "Support For ggml format for gpt4all",
    "body": "When I convert Llama model with convert-pth-to-ggml.py, quantize to 4bit, and load it with gpt4all, I get this:\r\nllama_model_load: invalid model file 'ggml-model-q4_0.bin' (bad magic)\r\nCould you implement to support ggml format that gpt4all uses?\r\nThanks!",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-02T01:24:38+00:00",
    "closed_at": "2023-04-02T10:50:26+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/696/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/696"
  },
  {
    "number": 693,
    "title": "Regression: \"The first main on the moon was \"",
    "body": "I saw a blog post where that prompt was used and now when I try it myself using LlAMA I don't get the same result. It is quite strange.\r\n\r\nIt keeps telling me the man is 38 years old and then starts going off on a tangent. Could this be a recent regression @ggerganov ?",
    "labels": [
      "question"
    ],
    "state": "closed",
    "created_at": "2023-04-01T23:16:25+00:00",
    "closed_at": "2023-05-16T19:10:10+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/693/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/693"
  },
  {
    "number": 692,
    "title": "How to make llama.cpp return control to add additional context?",
    "body": "I want to be able to tell the model that if it can't reply something useful to return control so I can give more information.\r\n\r\nSimilarly, how do I add more context so that it can reason about a full conversation or say a specific set of documents?\r\n\r\nFor example, I ask it something and it should say I don't know can you provide me more information? And then I give it a document. Then I can add another document to the prompt, so it can understand from that and so on.\r\n\r\nI've heard this is some sort of chaining, but I don't understand.",
    "labels": [
      "enhancement",
      "generation quality",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-01T22:20:36+00:00",
    "closed_at": "2024-04-11T01:07:16+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/692/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/692"
  },
  {
    "number": 688,
    "title": "How to use .safetensors model ?",
    "body": "I downloaded a model `alpaca-30b-lora-int4` from <https://huggingface.co/elinas/alpaca-30b-lora-int4/tree/main>\r\nThe model is a `.safetensors` in GPTQ format I think\r\nI need to convert it to `GGML .bin` so I used the script provided in `llama.cpp` with the command `python convert-gptq-to-ggml.py models/30B/alpaca-30b-4bit.safetensors models/30B//tokenizer.model models/30B/alpaca-30b-4bit.bin`\r\nBut I get the following error\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/big/meyer/expe/llama.cpp/convert-gptq-to-ggml.py\", line 21, in <module>\r\n    model = torch.load(fname_model, map_location=\"cpu\")\r\n  File \"/big/meyer/expe/llama.cpp/.venv/lib/python3.10/site-packages/torch/serialization.py\", line 815, in load\r\n    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n  File \"/big/meyer/expe/llama.cpp/.venv/lib/python3.10/site-packages/torch/serialization.py\", line 1035, in _legacy_load\r\n    raise RuntimeError(\"Invalid magic number; corrupt file?\")\r\nRuntimeError: Invalid magic number; corrupt file?\r\n```\r\nHow to use `.safetensors` models with `llama.cpp` ?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-01T19:13:02+00:00",
    "closed_at": "2023-04-14T13:12:58+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/688/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/688"
  },
  {
    "number": 684,
    "title": "Setting `temp=0` does not work as expected",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nSetting sampling temperature to `0` should produce valid and \"predictable\" tokens.\r\n\r\n# Current Behavior\r\n\r\nSetting temperature to `0` causes sampling to fail completely. This is due to `plogits` being scaled by `1.0f/temp` before sampling [here](https://github.com/ggerganov/llama.cpp/blob/d0a7f742e76bb48c0bd852f0b3bf09ec0b75b200/llama.cpp#L1201). I believe a workaround for this would be to make sampling deterministic when `temp==0` by setting `top_p=0.0` and `top_k=1` and setting `temp>0`.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-01T15:40:11+00:00",
    "closed_at": "2023-04-03T00:19:06+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/684/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/684"
  },
  {
    "number": 683,
    "title": "[User] chat-with-bob.txt mentions incorrect city",
    "body": "prompts/chat-with-bob.txt mentions that Moscow is the biggest city in Europe, while it is actually Istanbul :)",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-01T15:19:42+00:00",
    "closed_at": "2023-05-03T18:45:35+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/683/reactions",
      "total_count": 2,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 1,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/683"
  },
  {
    "number": 681,
    "title": "q4_1/f16 model is slow",
    "body": "pulled to the latest commit\r\nanother 7B model still runs as expected (which is gpt4all-lora-ggjt)\r\nI have 16 gb of ram, the model file is about 9.5 gb\r\n4 cores, amd, linux\r\n\r\n# problem description:\r\nmodel name: gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g\r\nthe model was described as: LLaMA 13B, finetuned natively with alpaca dataset, then finetuned on GPT4 responses (GPT4-x), then GPTQ 4b-128g quantized, then converted to ggml q4_1 format\r\nit loads, but takes about 30 seconds per token\r\n\r\n```\r\n$./main -m models/13B/ggml-model-q4_1.bin -n 128 --repeat_penalty 1.0 --color -ins\r\nmain: seed = 1680359110\r\nllama_model_load: loading model from 'models/13B/ggml-model-q4_1.bin' - please wait ...\r\nllama_model_load: GPTQ model detected - are you sure n_parts should be 2? we normally expect it to be 1\r\nllama_model_load: use '--n_parts 1' if necessary\r\nllama_model_load: n_vocab = 32001\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 5120\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 40\r\nllama_model_load: n_layer = 40\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 4\r\nllama_model_load: n_ff    = 13824\r\nllama_model_load: n_parts = 2\r\nllama_model_load: type    = 2\r\nllama_model_load: ggml map size = 9702.04 MB\r\nllama_model_load: ggml ctx size = 101.25 KB\r\nllama_model_load: mem required  = 11750.14 MB (+ 1608.00 MB per state)\r\nllama_model_load: loading tensors from 'models/13B/ggml-model-q4_1.bin'\r\nllama_model_load: model size =  9701.60 MB / num tensors = 363\r\nllama_init_from_file: kv self size  =  400.00 MB\r\n\r\nsystem_info: n_threads = 4 / 4 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \r\nmain: interactive mode on.\r\nReverse prompt: '### Instruction:\r\n\r\n'\r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.000000\r\ngenerate: n_ctx = 512, n_batch = 8, n_predict = 128, n_keep = 2\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-01T14:31:56+00:00",
    "closed_at": "2023-04-02T09:55:18+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/681/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/681"
  },
  {
    "number": 679,
    "title": "[WSL2] [Installation] make not compiling ",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\n`make` should compile.\r\n\r\n# Current Behavior\r\n\r\n`make` the second step after cloning the repo printed the help rather than \"making\".\r\n\r\n<details>\r\n  <summary>help text for `make`</summary>\r\n\r\n<pre><code>\r\n  $ make <target...> [options]\r\n\r\n  Options:\r\n    --help             Show this help output\r\n    --version          Show package version\r\n    --debug            Enable extended log output\r\n\r\n  Targets:\r\n    UNAME_S            Run target UNAME_S\r\n    UNAME_P            Run target UNAME_P\r\n    UNAME_M            Run target UNAME_M\r\n    CCV                Run target CCV\r\n    CXXV               Run target CXXV\r\n    default            Run target default\r\n    ggml.o             Run target ggml.o\r\n    llama.o            Run target llama.o\r\n    common.o           Run target common.o\r\n    clean              Run target clean\r\n    main               Run target main\r\n    quantize           Run target quantize\r\n    perplexity         Run target perplexity\r\n    embedding          Run target embedding\r\n    tests              Run target tests\r\n\r\n  Templates:\r\n    ava                Generate ava setup (ava)\r\n    cli                Generate cli setup (minimist, tabtab, ge...)\r\n    default            Generate default setup (babel-cli, babel-plu...)\r\n    eslint             Generate eslint setup (eslint, eslint-confi...)\r\n    livereactload      Generate livereactload setup (babel-plugin-react-t...)\r\n    mocha              Generate mocha setup (mocha)\r\n    simple             Generate simple setup\r\n</pre></code>\r\n</details>\r\n\r\n\r\n\r\n# Environment and Context \r\n\r\n### OS\r\n\r\n- WSL2\r\n- Ubuntu 22.04\r\n- Windows 10\r\n\r\n### Hardware\r\n\r\n- 32GB RAM\r\n- AMD \r\n- Nvidia 3060\r\n\r\n### `$ lscpu`\r\n\r\n<details>\r\n<summary>`lscpu`</summary>\r\n<pre>\r\n<code>\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   48 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          16\r\nOn-line CPU(s) list:             0-15\r\nVendor ID:                       AuthenticAMD\r\nModel name:                      AMD Ryzen 7 3700X 8-Core Processor\r\nCPU family:                      23\r\nModel:                           113\r\nThread(s) per core:              2\r\nCore(s) per socket:              8\r\nSocket(s):                       1\r\nStepping:                        0\r\nBogoMIPS:                        7186.49\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr virt_ssbd arat umip rdpid\r\nHypervisor vendor:               Microsoft\r\nVirtualization type:             full\r\nL1d cache:                       256 KiB (8 instances)\r\nL1i cache:                       256 KiB (8 instances)\r\nL2 cache:                        4 MiB (8 instances)\r\nL3 cache:                        16 MiB (1 instance)\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n</code>\r\n</pre>\r\n</details>\r\n\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n### `$ uname -a`\r\n\r\n`Linux windows 5.15.90.1-microsoft-standard-WSL2 #1 SMP Fri Jan 27 02:56:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux`\r\n\r\n\r\n### SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version => 3.9.12\r\n$ make --version => 0.8.1\r\n$ g++ --version => g++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\n```",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-04-01T13:00:34+00:00",
    "closed_at": "2023-04-02T04:38:19+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/679/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/679"
  },
  {
    "number": 677,
    "title": "Alpaca model is running very slow in llama.cpp compared to alpaca.cpp",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Current Behavior\r\n\r\nJust yesterday, this migration script was added : `migrate-ggml-2023-03-30-pr613.py`. \r\nSo, what I did on top of [@madmads11  instructions for using alpaca models](https://github.com/ggerganov/llama.cpp/issues/382#issuecomment-1479091459) was to use this above script and generate the final bin file to work with.\r\n\r\nDetails : \r\n- Alpaca Model used  : https://huggingface.co/Pi3141/alpaca-lora-7B-ggml\r\n- Tokenizer used : https://huggingface.co/decapoda-research/llama-7b-hf/blob/main/tokenizer.model \r\n\r\nI am using `llama.cpp` just today to run alpaca model. (was using antimatters alpaca.cpp until now)\r\n\r\nThis same model that's converted and loaded in `llama.cpp` runs very slow compared  to running it in `alpaca.cpp`. \r\n\r\nHow I started up model : \r\n- `./main -m ./models/alpaca-7b-migrated.bin -ins --n_parts 1`\r\n\r\nThe logs : \r\n```\r\nmain: seed = 1680346670\r\nllama_model_load: loading model from './models/alpaca-7b-migrated.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml map size = 4017.70 MB\r\nllama_model_load: ggml ctx size =  81.25 KB\r\nllama_model_load: mem required  = 5809.78 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading tensors from './models/alpaca-7b-migrated.bin'\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 16 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \r\nmain: interactive mode on.\r\nReverse prompt: '### Instruction:\r\n\r\n'\r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 512, n_batch = 8, n_predict = 128, n_keep = 2\r\n```\r\n\r\nAdditionally, I also used this bin file : https://huggingface.co/Pi3141/alpaca-lora-7B-ggml/blob/main/ggml-model-q4_1.bin that's already migrated for `llama.cpp`. And even for this, model is running slow with `llama.cpp`.\r\n\r\nOne thing I noticed was, while loading between these two model variants, this line is different than on above.\r\n`llama_model_load: f16     = 3`.\r\n\r\n\r\n\r\n\r\n\r\n# Environment and Context \r\n* Physical (or virtual) hardware :\r\n```\r\n\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         39 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  16\r\n  On-line CPU(s) list:   0-15\r\nVendor ID:               GenuineIntel\r\n  Model name:            Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz\r\n    CPU family:          6\r\n    Model:               158\r\n    Thread(s) per core:  2\r\n    Core(s) per socket:  8\r\n    Socket(s):           1\r\n    Stepping:            13\r\n    CPU(s) scaling MHz:  98%\r\n    CPU max MHz:         5000,0000\r\n    CPU min MHz:         800,0000\r\n    BogoMIPS:            7202,00\r\nVirtualization features: \r\n  Virtualization:        VT-x\r\nCaches (sum of all):     \r\n  L1d:                   256 KiB (8 instances)\r\n  L1i:                   256 KiB (8 instances)\r\n  L2:                    2 MiB (8 instances)\r\n  L3:                    16 MiB (1 instance)\r\nNUMA:                    \r\n  NUMA node(s):          1\r\n  NUMA node0 CPU(s):     0-15\r\nVulnerabilities:         \r\n  Itlb multihit:         KVM: Mitigation: VMX disabled\r\n  L1tf:                  Not affected\r\n  Mds:                   Not affected\r\n  Meltdown:              Not affected\r\n  Mmio stale data:       Mitigation; Clear CPU buffers; SMT vulnerable\r\n  Retbleed:              Mitigation; Enhanced IBRS\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\n  Spectre v2:            Vulnerable: eIBRS with unprivileged eBPF\r\n  Srbds:                 Mitigation; Microcode\r\n  Tsx async abort:       Mitigation; TSX disabled\r\n```\r\n\r\n* System info:\r\n```\r\n- OS : 5.10.148-1-MANJARO\r\n- python version : Python 3.10.9\r\n- g++version : g++ (GCC) 12.2.1 20230201\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-01T11:18:18+00:00",
    "closed_at": "2023-05-18T10:43:20+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/677/reactions",
      "total_count": 3,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/677"
  },
  {
    "number": 676,
    "title": "[Feature Suggestion] Halide-lang codegen / generators",
    "body": "# Prerequisites\r\nExamples of work with neural networks in Halide:\r\nhttps://github.com/halide/Halide/blob/main/apps/resnet_50/Resnet50Generator.cpp\r\nhttps://github.com/halide/Halide/tree/main/apps/hannk\r\nhttps://github.com/halide/Halide/blob/main/apps/onnx/model.cpp\r\n\r\n\r\n# Expected Behavior\r\n\r\nCross platform binary and code generation with best scheduling on computational graph applied by Halide or Halide autoschedulers, reduced memory usage by scheduling every used network model before execution\r\n\r\n# Current Behavior\r\n\r\nMultiple conditional defines for different platforms and instruction bloated code, lack of GPU support i.e. OpenCL, OpenGL Compute, CUDA, current computational graph has great parallelism but very frustrating locality\r\n\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-01T11:18:04+00:00",
    "closed_at": "2024-04-11T01:07:17+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/676/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/676"
  },
  {
    "number": 675,
    "title": "solved.",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-01T11:16:08+00:00",
    "closed_at": "2023-04-01T11:19:23+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/675/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/675"
  },
  {
    "number": 673,
    "title": "[Feature Suggestion] Dynamic prompt",
    "body": "Would love to see a feature where both the AI and the user could change the initial prompt in-situ and when necessary.\n\nEssentially, this would be the same as changing the prompt without exiting llama.cpp, thus eliminates the need to reload the model weights and forgetting the context.\n\nTo trigger this, it could be a trigger word in the input, such as \\iNewPrompt: You are an insane AI assistant. You always gives imprecise answers and easily goes into panic mode. Once you are panicked, you will start babbling and answer everything hysterically. You will become sane again when I tell you to stop panic.",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-01T08:05:43+00:00",
    "closed_at": "2024-04-11T01:07:19+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/673/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/673"
  },
  {
    "number": 672,
    "title": "magic number in convert-gptq-to-ggml.py not consistent",
    "body": "It appears that the conver-gptq-to-ggml script needs an update to reflect the recent change in magic, see [this line](https://github.com/ggerganov/llama.cpp/blob/master/convert-gptq-to-ggml.py#L39). However, it's not completely clear to me if only updating the magic number is sufficient to ensure that the resulting file is compatible. Hence leaving it here a reminder :)",
    "labels": [
      "duplicate"
    ],
    "state": "closed",
    "created_at": "2023-04-01T07:41:45+00:00",
    "closed_at": "2023-04-02T15:51:08+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/672/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/672"
  },
  {
    "number": 670,
    "title": "Having doubt on a if (0) block",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Context \r\n\r\nI was just checking the repo for the first time and i saw this block of code in the master branch. [Here](https://github.com/ggerganov/llama.cpp/blob/master/llama.cpp#L686-L689) is the exact location of the code from master. And [here](https://github.com/ggerganov/llama.cpp/blob/3525899277d2e2bdc8ec3f0e6e40c47251608700/llama.cpp#L686-L689) is the location from the latest commit as of April 1.\r\n```cpp\r\nif (0) {\r\n    static const char * ftype_str[] = { \"f32\", \"f16\", \"q4_0\", \"q4_1\", };\r\n    fprintf(stderr, \"%24s - [%5d, %5d], type = %6s\\n\", name.data(), ne[0], ne[1], ftype_str[ftype]);\r\n}\r\n```\r\n\r\nIt seems odd to have a standalone `if (0)` branch here doing nothing. \r\n\r\nTo ease for future development, I think\r\n- this shall be removed, OR\r\n- An inline comment shall be added to make it more understandable, OR\r\n- A separate boolean or macro should be established so that future developers will know when this code block will be run.\r\n\r\nWhat do you think?\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-01T05:10:17+00:00",
    "closed_at": "2023-04-12T15:33:12+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/670/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/670"
  },
  {
    "number": 669,
    "title": "llama.cpp main hangs at prompt with latest mmap updates",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x ] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\nAfter upgrading to latest code compiling and then running an inference using main the following prompt should return results like before:\r\n\r\n```\r\n./main -m models/13B/ggml-model-q4_0.bin -n 512 --repeat_penalty 1.0 --color  -p \"What is controlled delivery?\"\r\nmain: seed = 1680321331\r\nllama_model_load: loading model from 'models/13B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 5120\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 40\r\nllama_model_load: n_layer = 40\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 13824\r\nllama_model_load: n_parts = 2\r\nllama_model_load: type    = 2\r\nllama_model_load: ggml map size = 7759.83 MB\r\nllama_model_load: ggml ctx size = 101.25 KB\r\nllama_model_load: mem required  = 9807.93 MB (+ 1608.00 MB per state)\r\nllama_model_load: loading tensors from 'models/13B/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  7759.39 MB / num tensors = 363\r\nllama_init_from_file: kv self size  =  400.00 MB\r\n\r\nsystem_info: n_threads = 32 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.000000\r\ngenerate: n_ctx = 512, n_batch = 8, n_predict = 512, n_keep = 0\r\n\r\n\r\n\r\n What is controlled delivery?\r\nControlled Delivery (CD) refers to the process of delivering content or messages in specific locations, such as on college campuses and at events like concerts where there are large crowds present. It can also refer specifically to a method for distributing print materials that is targeted towards particular audiences based upon their demographic characteristics (e.g., gender, age range).\r\n```\r\n\r\n# Current Behavior\r\n\r\nllama.cpp main just hangs without output showing prompt only:\r\n\r\n```\r\n\r\n./main -m models/13B/ggml-model-q4_0.bin -n 512 --repeat_penalty 1.0 --color  -p \"What is controlled delivery?\"\r\nmain: seed = 1680321575\r\nllama_model_load: loading model from 'models/13B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 5120\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 40\r\nllama_model_load: n_layer = 40\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 13824\r\nllama_model_load: n_parts = 2\r\nllama_model_load: type    = 2\r\nllama_model_load: ggml map size = 7759.83 MB\r\nllama_model_load: ggml ctx size = 101.25 KB\r\nllama_model_load: mem required  = 9807.93 MB (+ 1608.00 MB per state)\r\nllama_model_load: loading tensors from 'models/13B/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  7759.39 MB / num tensors = 363\r\nllama_init_from_file: kv self size  =  400.00 MB\r\n\r\nsystem_info: n_threads = 32 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.000000\r\ngenerate: n_ctx = 512, n_batch = 8, n_predict = 512, n_keep = 0\r\n\r\n\r\n What is controlled delivery?\r\n```\r\n\r\n# Environment and Context \r\n\r\n* Linux Ubuntu 22.04\r\n* Nvidia GPU 3070 12GB.\r\n* RAM 128 GB\r\n* NVMe storage\r\n\r\nStepping:            0\r\n    Frequency boost:     enabled\r\n    CPU max MHz:         5083.3979\r\n    CPU min MHz:         2200.0000\r\n    BogoMIPS:            6800.50\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext\r\n                          fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq mon\r\n                         itor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm\r\n                          sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb\r\n                         cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed ad\r\n                         x smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero\r\n                         irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pause\r\n                         filter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor sm\r\n                         ca fsrm\r\nVirtualization features:\r\n  Virtualization:        AMD-V\r\nCaches (sum of all):\r\n  L1d:                   512 KiB (16 instances)\r\n  L1i:                   512 KiB (16 instances)\r\n  L2:                    8 MiB (16 instances)\r\n  L3:                    64 MiB (2 instances)\r\nNUMA:\r\n  NUMA node(s):          1\r\n  NUMA node0 CPU(s):     0-31\r\nVulnerabilities:\r\n  Itlb multihit:         Not affected\r\n  L1tf:                  Not affected\r\n  Mds:                   Not affected\r\n  Meltdown:              Not affected\r\n  Mmio stale data:       Not affected\r\n  Retbleed:              Not affected\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\n  Spectre v2:            Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\n  Srbds:                 Not affected\r\n  Tsx async abort:       Not affected\r\n\r\nLinux ai-llm-dev 5.15.0-67-generic #74-Ubuntu SMP Wed Feb 22 14:14:39 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n\r\n```\r\nPython 3.11.2\r\n\r\nGNU Make 4.3\r\nBuilt for x86_64-pc-linux-gnu\r\nCopyright (C) 1988-2020 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.\r\n\r\ng++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\nCopyright (C) 2021 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n```\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-01T04:10:19+00:00",
    "closed_at": "2023-04-01T08:54:17+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/669/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/669"
  },
  {
    "number": 668,
    "title": "Text Generation Effects in Different Models",
    "body": "# Prerequisites\r\nI changed the model from 7B to 30B, running following commands:\r\n```\r\ndocker run -i --rm -v ./models:/models ghcr.io/ggerganov/llama.cpp:full --run -m /models/30B/ggml-model-q4_0.bin -n 512 --repeat_penalty 1.0 --color -i -r \"User:\" -f prompts/chat-with-bob.txt\r\n```\r\n\r\n# Current Behavior\r\nthe following content was what I chat with it:\r\n```\r\nmain: seed = 1680317960\r\nllama_model_load: loading model from '/models/30B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 6656\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 52\r\nllama_model_load: n_layer = 60\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 17920\r\nllama_model_load: n_parts = 4\r\nllama_model_load: type    = 3\r\nllama_model_load: ggml ctx size = 20171.50 MB\r\nllama_model_load: mem required  = 22475.50 MB (+ 3124.00 MB per state)\r\nllama_model_load: loading model part 1/4 from '/models/30B/ggml-model-q4_0.bin'\r\nllama_model_load: ................................................................... done\r\nllama_model_load: model size =  4850.14 MB / num tensors = 543\r\nllama_model_load: loading model part 2/4 from '/models/30B/ggml-model-q4_0.bin.1'\r\nllama_model_load: ................................................................... done\r\nllama_model_load: model size =  4850.14 MB / num tensors = 543\r\nllama_model_load: loading model part 3/4 from '/models/30B/ggml-model-q4_0.bin.2'\r\nllama_model_load: ................................................................... done\r\nllama_model_load: model size =  4850.14 MB / num tensors = 543\r\nllama_model_load: loading model part 4/4 from '/models/30B/ggml-model-q4_0.bin.3'\r\nllama_model_load: ................................................................... done\r\nllama_model_load: model size =  4850.14 MB / num tensors = 543\r\nllama_init_from_file: kv self size  =  780.00 MB\r\n\r\nsystem_info: n_threads = 16 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \r\nmain: interactive mode on.\r\nReverse prompt: 'User:'\r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.000000\r\ngenerate: n_ctx = 512, n_batch = 8, n_predict = 512, n_keep = 0\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n\r\n Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.\r\n\r\nUser: Hello, Bob.\r\nBob: Hello. How may I help you today?\r\nUser: Please tell me the largest city in Europe.\r\nBob: Sure. The largest city in Europe is Moscow, the capital of Russia.\r\nUser:please write a letter about rains\r\nBob: Let's write a letter about rains. What are your thoughts?\r\nUser:could you tell me how to build a website\r\nBob: Sure. I'll help you to build a website. How would you like to begin?\r\nUser:please tell me the ways to build a website\r\nBob: I'll tell you the ways to build a website. What are your thoughts?\r\nUser:\r\n```\r\nHowever, sometimes the effect of 30B model is similar to that of 7B model, or even worse than that of 7B model. Is there anything I missed?\r\nHow could I make the model smarter, like chatgpt?\r\nDo I need to fine-tune the model?",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-01T03:26:38+00:00",
    "closed_at": "2024-04-11T01:07:20+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/668/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/668"
  },
  {
    "number": 667,
    "title": "[User] examples/chat-13B.sh sometimes continues my question instead of answering",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Steps to Reproduce\r\n\r\nRun the chat program:\r\n\r\n```\r\n$ examples/chat-13B.sh\r\nmain: seed = 1680315413\r\nllama_model_load: loading model from './models/13B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 2048\r\nllama_model_load: n_embd  = 5120\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 40\r\nllama_model_load: n_layer = 40\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 13824\r\nllama_model_load: n_parts = 2\r\nllama_model_load: type    = 2\r\nllama_model_load: ggml map size = 7759.83 MB\r\nllama_model_load: ggml ctx size = 101.25 KB\r\nllama_model_load: mem required  = 9807.93 MB (+ 1608.00 MB per state)\r\nllama_model_load: loading tensors from './models/13B/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  7759.39 MB / num tensors = 363\r\nllama_init_from_file: kv self size  = 1600.00 MB\r\n\r\nsystem_info: n_threads = 8 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \r\nmain: interactive mode on.\r\nReverse prompt: 'User:'\r\nsampling: temp = 0.700000, top_k = 40, top_p = 0.500000, repeat_last_n = 256, repeat_penalty = 1.176470\r\ngenerate: n_ctx = 2048, n_batch = 512, n_predict = 2048, n_keep = 0\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n\r\n \r\nText transcript of a never ending dialog, where User interacts with an AI assistant named ChatLLaMa.\r\nChatLLaMa is helpful, kind, honest, friendly, good at writing and never fails to answer User\u2019s requests immediately and with details and precision.\r\nThere are no annotations like (30 seconds passed...) or (to himself), just what User and ChatLLaMa say aloud to each other.\r\nThe dialog lasts for years, the entirety of it is shared below. It's 10000 pages long.\r\nThe transcript only includes text, it does not include markup like HTML and Markdown.\r\n\r\nUser: Hello, ChatLLaMa!\r\nChatLLaMa: Hello User! How may I help you today?\r\nUser: What time is it?\r\nChatLLaMa: It is 10:16.\r\nUser: What year is it?\r\nChatLLaMa: We are in 2023.\r\nUser: Please tell me the largest city in Europe.\r\nChatLLaMa: The largest city in Europe is Moscow, the capital of Russia.\r\nUser: What can you tell me about Moscow?\r\nChatLLaMa: Moscow, on the Moskva River in western Russia, is the nation\u2019s cosmopolitan capital. In its historic core is the Kremlin, a complex that\u2019s home to the president and tsarist treasures in the Armoury. Outside its walls is Red Square, Russia\u2019s symbolic center.\r\nUser: What is a cat?\r\nChatLLaMa: A cat is a domestic species of small carnivorous mammal. It is the only domesticated species in the family Felidae.\r\nUser: How do I pass command line arguments to a Node.js program?\r\nChatLLaMa: The arguments are stored in process.argv.\r\n\r\n    argv[0] is the path to the Node. js executable.\r\n    argv[1] is the path to the script file.\r\n    argv[2] is the first argument passed to the script.\r\n    argv[3] is the second argument passed to the script and so on.\r\nUser: Name a color.\r\nChatLLaMa: Blue\r\nUser:\r\n```\r\n\r\nType this as a prompt:\r\n\r\n> Please rewrite the following statement in a customer friendly and polite way:\\\\\r\n> \\\\\r\n> I have looked at dmesg on other nodes. Unfortunately, c1node3, c1node5, and c1node7 indicate DIMM memory errors detected and corrected during previous boots. Additionally, c1node4 has SQUASHFS errors, as if the filesystem image was incompletely downloaded during the boot. On broken hardware, or with incompletely downloaded boot image, errors are just expected.\\\\\r\n> \\\\\r\n> Are you sure that the BIOS settings regarding the CPU clocks and voltages, as well as memory clocks, are in the range supported by the manufacturer? I.e. that there is no overclocking or undervolting going on, and the power supplies are all good?\\\\\r\n> \\\\\r\n> This leaves us with nodes 2 and 8, which have softlockups without any tainting factors.\\\\\r\n> \\\\\r\n> Could you please try to figure out when this kind of disaster stated, and whether there was any change to the hardware or software immediately before that?\r\n\r\nAnd then, llama.cpp writes this line as if the User said it:\r\n\r\n> If we can pinpoint the exact moment it started happening, then maybe we could narrow down what changed around that time.\r\n\r\nI expected the reply to start with `ChatLLaMa:`. Maybe llama.cpp should add the correct tokens explicitly, so that there is no need for the AI to predict that it is its turn to speak?\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-04-01T02:42:42+00:00",
    "closed_at": "2024-04-11T01:07:21+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/667/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/667"
  },
  {
    "number": 666,
    "title": "[User] Bus error (core dumped) on a 65B model",
    "body": "I tried running the a 65B model that was converted using the unversioned `.py` conversion script then migrated from an 8-file `ggml` `.bin` to a single-file `ggjt` `.bin`. Tried to run the model and I get a `Bus error` then the program ends.\r\n```\r\nuser@ubuntu: ~/Desktop/llama.cpp$ ./main -m ./models/ggjt-model-model-q4_0.bin -t 7 -i\r\nmain: seed = 1680306291\r\nllama_model_load: loading model from './models/ggjt-model-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 8192\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 64\r\nllama_model_load: n_layer = 80\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 22016\r\nllama_model_load: n_parts = 8\r\nllama_model_load: type    = 4\r\nllama_model_load: ggml map size = 38917.99 MB\r\nllama_model_load: ggml ctx size = 201.25 KB\r\nllama_model_load: mem required  = 41478.18 MB (+ 5120.00 MB per state)\r\nllama_model_load: loading tensors from './models/ggjt-model-model-q4_0.bin'\r\nllama_model_load: model size = 38917.53 MB / num tensors = 723\r\nllama_init_from_file: kv self size  = 1280.00 MB\r\n\r\nsystem_info: n_threads = 7 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \r\nmain: interactive mode on.\r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 512, n_batch = 8, n_predict = 128, n_keep = 0\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n\r\n Bus error (core dumped)\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-31T23:58:31+00:00",
    "closed_at": "2023-07-28T19:44:35+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/666/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/666"
  },
  {
    "number": 662,
    "title": "GPT4All: invalid model file (bad magic)",
    "body": "Hi there, followed the instructions to get gpt4all running with llama.cpp, but was somehow unable to produce a valid model using the provided python conversion scripts:\r\n\r\n```\r\n% python3 convert-gpt4all-to-ggml.py models/gpt4all-7B/gpt4all-lora-quantized.bin ./models/tokenizer.model\r\nconverting models/gpt4all-7B/gpt4all-lora-quantized.bin\r\n```\r\n```\r\n% ./main -m ./models/gpt4all-7B/gpt4all-lora-quantized.bin -n 128\r\nmain: seed = 1680294943\r\nllama_model_load: loading model from './models/gpt4all-7B/gpt4all-lora-quantized.bin' - please wait ...\r\n./models/gpt4all-7B/gpt4all-lora-quantized.bin: invalid model file (bad magic [got 0x67676d66 want 0x67676a74])\r\n\tyou most likely need to regenerate your ggml files\r\n\tthe benefit is you'll get 10-100x faster load times\r\n\tsee https://github.com/ggerganov/llama.cpp/issues/91\r\n\tuse convert-pth-to-ggml.py to regenerate from original pth\r\n\tuse migrate-ggml-2023-03-30-pr613.py if you deleted originals\r\nllama_init_from_file: failed to load model\r\nmain: error: failed to load model './models/gpt4all-7B/gpt4all-lora-quantized.bin'\r\n```\r\nAre just the magic bytes in the python script wrong or is it a completely different format?\r\n\r\nRelated issues: #647 \r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-31T22:19:34+00:00",
    "closed_at": "2023-03-31T23:25:37+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/662/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/662"
  },
  {
    "number": 660,
    "title": "Variable density context windows?",
    "body": "I am currently off my meds, but I would like to propose an idea to enhance the context handling capabilities of the LLaMA/Alpaca/gpt4all models, particularly for the smaller models with limited context window sizes. I'm not sure if this is entirely doable within the current architecture, or if changes would be needed to the underlying LLMs, but I wanted to share my thoughts and get your feedback.\r\n\r\n**The Problem:**\r\n\r\nAs you all know, smaller models have limited context window sizes, which make it hard to maintain long conversations especially when the LLM is used as a chatbot instead vs unrelated queries. This limitation affects the model's overall performance and ability to provide accurate and coherent responses.\r\n\r\n**The Proposal:**\r\n\r\nI propose implementing a Variable Density Context Window (VDCW) technique that selectively retains the most relevant tokens while still staying within the model's limited context window size. This approach aims to provide the model with a more extensive and meaningful context to work with, even with smaller window sizes.\r\n\r\nTo make this more concrete, I suggest dividing the context window into three sections: [Character Card], [Old Conversation], and [Recent Conversation]. This will enable us to focus on the most important aspects of the conversation while still retaining some critical context from the old conversation.\r\n\r\n**[Character Card]:** This section contains essential information about the user and the AI model's identity, role, and any other relevant background information.\r\n\r\n**[Old Conversation]:** This section will store a compressed version of the earlier parts of the chat history, focusing on extracting verbs and nouns from the text (from some quick experimenting this reduces token use by ~60%), along with any critical context provided by certain tokens (e.g., 'User:', 'Miku:'). This way, we can retain important context without using too many tokens.\r\n\r\n[Recent Conversation]: This section will hold the most recent conversation, which is crucial for understanding the current context and providing accurate responses.\r\n\r\nIn addition to these three sections, VDCW could also employ the following techniques in the future (god knows I probably cant code them):\r\n\r\nImportance sampling: Sample tokens based on their relevance or importance to the current task, such as token frequency or relevance to the query.\r\nCompression: Compress context by merging or summarizing multiple tokens or phrases to convey necessary information with fewer tokens.\r\nHierarchical representation: Store information hierarchically to access a wider range of information without increasing the context window size.\r\n\r\nI'm looking forward to hearing your thoughts and insights on this proposal. If you think this idea has potential, we can start discussing the implementation details and potential roadblocks. If you have any concerns or alternative suggestions, please feel free to share them. Let's work together to enhance LAMA's context handling capabilities!\r\n\r\n*A lot of this was summarized from my conversations with the help of chatGPT. ",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-31T20:39:41+00:00",
    "closed_at": "2024-04-11T01:07:23+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/660/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/660"
  },
  {
    "number": 659,
    "title": "Im getting this error while trying to convert .pth to .bin",
    "body": "**Error that im getting**\r\n\r\n`shreyas@ATLAS-AHQVOLJ9A:/mnt/c/Users/Shreyas-ITB/Downloads/llama.cpp$ python3 convert-pth-to-ggml.py models/7B/ 1\r\n{'dim': 4096, 'multiple_of': 256, 'n_heads': 32, 'n_layers': 32, 'norm_eps': 1e-06, 'vocab_size': -1}\r\nNamespace(dir_model='models/7B/', ftype=1, vocab_only=0)\r\nn_parts = 1\r\n\r\nProcessing part 1 of 1\r\n\r\nTraceback (most recent call last):\r\n  File \"/mnt/c/Users/Shreyas-ITB/Downloads/llama.cpp/convert-pth-to-ggml.py\", line 274, in <module>\r\n    main()\r\n  File \"/mnt/c/Users/Shreyas-ITB/Downloads/llama.cpp/convert-pth-to-ggml.py\", line 267, in main\r\n    model = torch.load(fname_model, map_location=\"cpu\")\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 809, in load\r\n    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1172, in _load\r\n    result = unpickler.load()\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1142, in persistent_load\r\n    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1112, in load_tensor\r\n    storage = zip_file.get_storage_from_record(name, numel, torch.UntypedStorage)._typed_storage()._untyped_storage\r\nOSError: [Errno 14] Bad address`\r\n\r\n**Command used**:  `python3 convert-pth-to-ggml.py models/7B/ 1`\r\n\r\nCan someone help me please?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-31T20:30:53+00:00",
    "closed_at": "2023-04-16T09:29:11+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/659/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/659"
  },
  {
    "number": 655,
    "title": "Error: Invalid model file when using converted GPT4ALL model after following provided instructions",
    "body": "Hello,\r\n\r\nI have followed the instructions provided for using the GPT-4ALL model. I used the `convert-gpt4all-to-ggml.py` script to convert the `gpt4all-lora-quantized.bin` model, as instructed. However, I encountered an error related to an invalid model file when running the example. \r\n\r\nHere are the steps I followed, as described in the instructions:\r\n\r\n1. Convert the model using the `convert-gpt4all-to-ggml.py` script:\r\n```\r\npython3 convert-gpt4all-to-ggml.py models/gpt4all/gpt4all-lora-quantized.bin ./models/tokenizer.model\r\n```\r\n\r\n2. Run the `interactive mode` example with the newly generated `gpt4all-lora-quantized.bin` model:\r\n```\r\n./main -m ./models/gpt4all/gpt4all-lora-quantized.bin -n 256 --repeat_penalty 1.0 --color -i -r \"User:\" -f prompts/chat-with-bob.txt\r\n```\r\n\r\nHowever, I encountered the following error:\r\n```\r\n./models/gpt4all/gpt4all-lora-quantized.bin: invalid model file (bad magic [got 0x67676d66 want 0x67676a74])\r\nyou most likely need to regenerate your ggml files\r\nthe benefit is you'll get 10-100x faster load times\r\nsee https://github.com/ggerganov/llama.cpp/issues/91\r\nuse convert-pth-to-ggml.py to regenerate from original pth\r\nuse migrate-ggml-2023-03-30-pr613.py if you deleted originals\r\nllama_init_from_file: failed to load model\r\nmain: error: failed to load model './models/gpt4all/gpt4all-lora-quantized.bin'\r\n```\r\n\r\nPlease let me know how to resolve this issue and correctly convert and use the GPT-4ALL model with the `interactive mode` example.\r\n\r\nThank you.\r\n\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-31T17:13:52+00:00",
    "closed_at": "2023-03-31T17:55:16+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/655/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/655"
  },
  {
    "number": 650,
    "title": "How do i download the models? ",
    "body": "`65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model`\r\n\r\nThis command in the readme.md file says to add the models into the models directory but the models arent even there in the directory.\r\nPlease let me know how to download the 7B model to run on my computer.\r\nThanks",
    "labels": [
      "good first issue",
      "invalid",
      "wontfix"
    ],
    "state": "closed",
    "created_at": "2023-03-31T11:55:15+00:00",
    "closed_at": "2023-03-31T13:40:17+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/650/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/650"
  },
  {
    "number": 649,
    "title": "[User] interactive mode does not show my text",
    "body": "# Prerequisites\r\nBuilt on windows 10 , Visual studio 2022.\r\nAfter Loading 7B model I have the prompt ready but if I write, nothing is displayed and can not trigger it.\r\n\r\nMy log:\r\nmain.exe -m ./models/7B/ggml-model-q4_0.bin -i -n 124 -t 4\r\nmain: seed = 1680252016\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml ctx size = 4273.34 MB\r\nllama_model_load: mem required  = 6065.34 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading model part 1/1 from './models/7B/ggml-model-q4_0.bin'\r\nllama_model_load: .................................... done\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 4 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\r\nmain: interactive mode on.\r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 512, n_batch = 8, n_predict = 124, n_keep = 0\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-31T08:46:03+00:00",
    "closed_at": "2023-07-28T19:44:18+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/649/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/649"
  },
  {
    "number": 648,
    "title": "gpt4all keeps on ending responses abruptly",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nThe GPT4All model should give proper and complete responses\r\n\r\n# Current Behavior\r\n\r\nLonger responses get truncated:\r\n```\r\n> Please write a letter to my boss explaining that I keep on arriving late at work because my alarm clock is defective.\r\nDear Boss,\r\n\r\nI am sorry to inform you that I have been arriving late to work due to a defective alarm clock. I have tried multiple times to fix it, but it seems to\r\n>\r\n``` \r\n\r\n# Environment and Context \r\n\r\nReproducible on all my x86 machines on both Windows and Linux.\r\n\r\nHere's the command I've used to start the interactive mode:\r\n\r\n```\r\n./main -m ./gpt4all-lora-unfiltered-quantized-new.bin --color -f ./prompts/alpaca.txt -ins -b 25\r\n6 --top_k 10000 --temp 0.2 --repeat_penalty 1 -t 7\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nIt seems to be happening mostly randomly, but always with longer responses.\r\n\r\n# Steps to Reproduce\r\n\r\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\r\n\r\n1. Compile llama.cpp as usual (on x86)\r\n2. Get the gpt4all weight file (any, either normal or unfiltered one)\r\n3. Convert it using `convert-gpt4all-to-ggml.py` and `migrate-ggml-2023-03-30-pr613.py`\r\n4. Run it using the command above\r\n5. Tell it to write something long (see example)\r\n\r\n# Failure Logs\r\n\r\n```\r\nllama_model_load: loading model from './gpt4all-lora-unfiltered-quantized-new.bin' - please wait ...\r\nllama_model_load: n_vocab = 32001\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml map size = 4017.70 MB\r\nllama_model_load: ggml ctx size =  81.25 KB\r\nllama_model_load: mem required  = 5809.78 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading tensors from './gpt4all-lora-unfiltered-quantized-new.bin'\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 7 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\r\nmain: interactive mode on.\r\nReverse prompt: '### Instruction:\r\n\r\n'\r\nsampling: temp = 0.200000, top_k = 10000, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.000000\r\ngenerate: n_ctx = 512, n_batch = 256, n_predict = 128, n_keep = 21\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-31T07:58:57+00:00",
    "closed_at": "2023-04-02T08:24:47+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/648/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/648"
  },
  {
    "number": 647,
    "title": "Confusion about the model versioning",
    "body": "So back when project started, we had the first \"unversioned\" model format without the embedded tokens, with the magic 0x67676d6c (ggml).\r\n\r\nProblem with that was that it didn't have any versioning support, so newer/older versions would just think \"I don't know what this is, this is not a model file\".\r\n\r\nThen on this commit https://github.com/ggerganov/llama.cpp/commit/074bea2eb1f1349a0118239c4152914aecaa1be4, adding the embedded the tokens we got a new versioned model format, with magic 0x67676d66 (ggmf), along with **versioning**, so it could now say \"this is definitely a model file, but a wrong version\" as shown here:\r\nhttps://github.com/ggerganov/llama.cpp/blob/3bcc129ba881c99795e850b0a23707a4dfdabe9d/llama.h#L22\r\n\r\nThat was definitely a good move towards future proofing. Any breaking changes could just add +1 to that version and all would be fine and dandy for the next 4294967295 versions of the model format.\r\n\r\nBut then came this commit: https://github.com/ggerganov/llama.cpp/commit/78ca9838ee36660a776e97e3391b6fb5dcaacf7f\r\nWhich for absolutely no good reason changed the magic to 0x67676a74 (ggjt), kept the version at 1, completely breaking the whole versioning system and made it worthless.\r\n\r\nNow we're back to the system where the different versions of `llama.cpp` don't understand that \"yes , these are indeed models but older/newer versions\". We already fixed this problem, why the absolute f the need to break something that is already perfectly fine?\r\n\r\nI just cannot understand the reasoning behind this except maybe vanity, I guess (as the new magic uses the initials of the one who did the commit as the magic) ? Absolutely ridiculous to break a perfectly functional system. Or is there actually some proper reason for this that I'm completely missing?\r\n\r\nIt is already a struggle since various older forks like alpaca.cpp / gpt4all uses the unversioned format, then the move to the versioned format already fractured the community a bit, but was a good and necessary change overall and fixed the version confusion problem for the future versions. But now the third format change, which is made *intentionally worse* by changing the magic instead of doing it properly and using the versioning system put in place back then and causing even more confusion as now all the commits since https://github.com/ggerganov/llama.cpp/commit/074bea2eb1f1349a0118239c4152914aecaa1be4 , where this whole problem was already fixed, is now broken again and those versions would say \"I do now know what this is, it is not a model file\" of the new format. **WHY?**\r\n\r\nAgain, the proper way of updating the model as envisioned by the versioning system is to:\r\n```diff\r\n-#define LLAMA_FILE_VERSION 1\r\n+#define LLAMA_FILE_VERSION 2\r\n#define LLAMA_FILE_MAGIC 0x67676d66 // 'ggmf' in hex\r\n```\r\nand not\r\n```diff\r\n#define LLAMA_FILE_VERSION 1\r\n-#define LLAMA_FILE_MAGIC 0x67676d66 // 'ggmf' in hex\r\n+#define LLAMA_FILE_MAGIC 0x67676a74 // 'ggjt' in hex\r\n```\r\nlike it was committed https://github.com/ggerganov/llama.cpp/commit/78ca9838ee36660a776e97e3391b6fb5dcaacf7f.\r\n\r\nWhat is actually the line of thinking here, we just going to keep the version at 1, completely disuse the versioning system and keep changing the magic to whoever's initials who is doing that change? How the everliving F does that make *any* sense?!\r\n\r\nIf this actually was done by accident, not understanding the versioning system and not by intention, sorry for my scathing remarks. If it's intentional and breaking a perfectly functional system for vanity's sake, all the scathe is well deserved.\r\n\r\nPulling dumb shit like this is a good way to make a fantastic open-source project fall apart quickly.\r\n",
    "labels": [
      "documentation",
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-31T07:20:01+00:00",
    "closed_at": "2023-05-03T18:46:36+00:00",
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/647/reactions",
      "total_count": 17,
      "+1": 17,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/647"
  },
  {
    "number": 646,
    "title": "Unable to enter Chinese prompt",
    "body": "Hi!My use is compiled under Windows main.exe, when I type Chinese Prompt, I found that the model seems to be unable to understand, under debugging found that std::getline(std::cin,line) get is empty lines, then I tried Japanese, are the same result.\r\n(Since I am a native Chinese speaker, this question was translated by DeepL)\r\n![image](https://user-images.githubusercontent.com/18028414/229043234-a47c0569-07e1-4731-85d9-121f9774fdc9.png)\r\n",
    "labels": [
      "windows"
    ],
    "state": "closed",
    "created_at": "2023-03-31T06:43:06+00:00",
    "closed_at": "2023-04-09T08:03:44+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/646/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/646"
  },
  {
    "number": 644,
    "title": "Create clear instructions for downloading and converting the models",
    "body": "Clear instructions are needed to allow new arrivals to download and convert the models, in spite of the multiple format versions (non quantised, quantised, various llama versions etc) .\r\n\r\nI would suggest that each llama or alpaca etc print a version on startup, and that the conversions scripts have this in their name, and also that a program reading a file and figuring out what it is from a magic print the version read and the version expected even if it aborts.  \r\n\r\nEdmund\r\n\r\n",
    "labels": [
      "documentation",
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-31T02:23:32+00:00",
    "closed_at": "2023-05-03T18:43:16+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/644/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/644"
  },
  {
    "number": 641,
    "title": "The `quantize.py` script is not needed anymore. Just fetch the latest code and do this as a quantization step:",
    "body": "              The `quantize.py` script is not needed anymore. Just fetch the latest code and do this as a quantization step:\r\n\r\n```\r\n./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin 2\r\n```\r\n\r\n_Originally posted by @prusnak in https://github.com/ggerganov/llama.cpp/issues/621#issuecomment-1491088418_\r\n\r\nI tried this method in Colab, but it still reports an error:\r\n/bin/bash: ./quantize: No such file or directory            ",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-31T00:57:24+00:00",
    "closed_at": "2023-03-31T08:18:49+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/641/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/641"
  },
  {
    "number": 639,
    "title": "the new mmap method does not work on Windows 11 ?",
    "body": "I tried migration and to create the new weights from pth, in both cases the mmap fails.\r\nAlways says \"failed to mmap\"",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-30T22:26:30+00:00",
    "closed_at": "2023-03-31T01:32:34+00:00",
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/639/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/639"
  },
  {
    "number": 637,
    "title": "Performance investigation using AMD BLIS instead of OpenBLAS on 16 core AMD Zen1",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\nCompiling against AMD optimized BLS implementation of BLAS allows me to run perplexity tests\r\n\r\n# Current Behavior\r\nCompiling against AMD optimized BLS implementation of BLAS causes perplexity command to process 0 chunks\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n```\r\nllama.cpp$ lscpu | egrep \"AMD|Flags\"\r\nVendor ID:                       AuthenticAMD\r\nModel name:                      AMD Ryzen Threadripper 1950X 16-Core Processor\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sme sev\r\nVirtualization:                  AMD-V\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n```\r\nllama.cpp$ uname -a\r\nLinux asushimu 5.15.0-60-generic #66-Ubuntu SMP Fri Jan 20 14:29:49 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\nllama.cpp$ git log | head -1\r\ncommit 3df890aef432ce68143cfafcd7caf828bc4c3e55\r\nllama.cpp$ python3 --version\r\nPython 3.10.9\r\nllama.cpp$ make --version | head -1\r\nGNU Make 4.3\r\nllama.cpp$ g++ --version | head -1\r\ng++ (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\n```\r\n# Steps to Reproduce\r\n\r\n1. Install latest bliss libs from github\r\n\r\n```\r\nblis$ sudo make install\r\nInstalling libblis.a into /usr/local/lib/\r\nInstalling libblis.so.4.0.0 into /usr/local/lib/\r\nGenerating monolithic cblas.h.........\r\nGenerated include/zen/cblas.h\r\nInstalling blis.h cblas.h blis.hh cblas.hh into /usr/local/include/blis/\r\nInstalling config.mk common.mk into /usr/local/share/blis/\r\nInstalling config/zen/make_defs.mk into /usr/local/share/blis/config/zen\r\nmkdir -p /usr/local/share/pkgconfig\r\nInstalling blis.pc into /usr/local/share/pkgconfig/\r\ninstall -c -m 0644 blis.pc /usr/local/share/pkgconfig\r\n```\r\n3. Update Makefile to use blis instead of blas\r\n\r\n```\r\nllama.cpp$ diff Makefile.bliss Makefile.dist \r\n183,184c183,184\r\n< \tCFLAGS  += -DGGML_USE_OPENBLAS -I/usr/local/include/blis\r\n< \tLDFLAGS += -lblis\r\n---\r\n> \tCFLAGS  += -DGGML_USE_OPENBLAS -I/usr/local/include/openblas\r\n> \tLDFLAGS += -lopenblas\r\n```\r\n\r\n5. Compile against blis, perplexity processes 0 chunks\r\n\r\n174 second run just calling `./main` linked against OpenBLAS:\r\n```\r\nllama.cpp$ make -f Makefile.dist clean && LLAMA_OPENBLAS=1 make -f Makefile.dist;ldd ./main;time ./main -t 16 -m ./models/7B/ggml-model-q4_0.bin -b 256 -n 512 -p \"blis or blas\"\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread\r\nI LDFLAGS:  \r\nI CC:       cc (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\nI CXX:      g++ (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\n\r\nrm -vf *.o main quantize perplexity embedding\r\nremoved 'common.o'\r\nremoved 'ggml.o'\r\nremoved 'llama.o'\r\nremoved 'main'\r\nremoved 'quantize'\r\nremoved 'perplexity'\r\nremoved 'embedding'\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3 -DGGML_USE_OPENBLAS -I/usr/local/include/openblas\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread\r\nI LDFLAGS:  -lopenblas\r\nI CC:       cc (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\nI CXX:      g++ (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3 -DGGML_USE_OPENBLAS -I/usr/local/include/openblas   -c ggml.c -o ggml.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -c llama.cpp -o llama.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -c examples/common.cpp -o common.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/main/main.cpp ggml.o llama.o common.o -o main -lopenblas\r\n\r\n====  Run ./main -h for help.  ====\r\n\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/quantize/quantize.cpp ggml.o llama.o -o quantize -lopenblas\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/perplexity/perplexity.cpp ggml.o llama.o common.o -o perplexity -lopenblas\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/embedding/embedding.cpp ggml.o llama.o common.o -o embedding -lopenblas\r\n\tlinux-vdso.so.1 (0x00007ffd8c7a7000)\r\n\tlibopenblas.so.0 => /lib/x86_64-linux-gnu/libopenblas.so.0 (0x00007f3bb8880000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f3bb8656000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f3bb856f000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f3bb854f000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f3bb8327000)\r\n\tlibgfortran.so.5 => /lib/x86_64-linux-gnu/libgfortran.so.5 (0x00007f3bb804a000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f3bbadd8000)\r\n\tlibquadmath.so.0 => /lib/x86_64-linux-gnu/libquadmath.so.0 (0x00007f3bb8002000)\r\nmain: seed = 1680212228\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml map size = 4017.70 MB\r\nllama_model_load: ggml ctx size =  81.25 KB\r\nllama_model_load: mem required  = 5809.78 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading tensors from './models/7B/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 512, n_batch = 256, n_predict = 512, n_keep = 0\r\n\r\n\r\n blis or blas\r\n\"Blas,\" said the voice in the dark. \"There is a Blas. He's been here all day long. He hasn't moved. I think he might be asleep.\"\r\nI could hear breathing, but it was too distant to place where it was coming from. Blas. The name came to me from some forgotten dream. I couldn't recall why I had remembered it or where the thought had come from, but the name was there in my head: Blas. And then it was gone again, like a bird in flight.\r\n\"Is he still here?\" said the voice. \"I can't see him.\"\r\n\"Here,\" I said. \"Yes.\" The word hung on the air of our cave, suspended between us. But that word was also gone.\r\nWe had been together for three days now. Three days ago we had met in the woods; on day two we had found a cave deep in the forest and had made it into our own little world. Now it was nighttime again and Blas slept. I could hear his breathing, but there were other sounds too: waves, a distant breeze, the creaking of tree limbs heavy with snow.\r\n\"I think he might be asleep,\" said the voice in the dark.\r\nIt was a strange thing to hear that voice again: we had come to know it so well since we'd met\u2014it had been there in my head for weeks and weeks, but now suddenly it seemed like an old friend, someone I knew very well from childhood days: my mother's voice, or the sound of the sea. I couldn't quite work out what it was. And then again, that name was gone, swirling round in me like a leaf flung against a stone in a river. Blas. It must have been some kind of bird, perhaps a small bird with a short tail.\r\n\"I think he might be asleep,\" said the voice. \"What shall we do?\"\r\n\"Shall we go to bed?\" I asked. I could hear my own words coming out of the dark cave like birdsong: they had been there in me for days and now suddenly they were back again, like a message from the past. And then it was gone too.\r\nThe voice sighed with relief as if at some unsaid thing that was now gone\u2014and it came to me again: \"Shall we\r\nllama_print_timings:        load time =  1072.38 ms\r\nllama_print_timings:      sample time =   401.94 ms /   512 runs   (    0.79 ms per run)\r\nllama_print_timings: prompt eval time = 15402.35 ms /   263 tokens (   58.56 ms per token)\r\nllama_print_timings:        eval time = 157868.10 ms /   510 runs   (  309.55 ms per run)\r\nllama_print_timings:       total time = 174278.01 ms\r\n\r\nreal\t2m54.504s\r\nuser\t46m6.640s\r\nsys\t3m35.773s\r\n```\r\n\r\n47 second run calling `./main` linked against AMD bliss BLAS libs:\r\n```\r\nllama.cpp$ make -f Makefile.bliss clean && LLAMA_OPENBLAS=1 make -f Makefile.bliss;ldd ./main;time ./main -t 16 -m ./models/7B/ggml-model-q4_0.bin -b 256 -n 512 -p \"blis or blas\"\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread\r\nI LDFLAGS:  \r\nI CC:       cc (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\nI CXX:      g++ (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\n\r\nrm -vf *.o main quantize perplexity embedding\r\nremoved 'common.o'\r\nremoved 'ggml.o'\r\nremoved 'llama.o'\r\nremoved 'main'\r\nremoved 'quantize'\r\nremoved 'perplexity'\r\nremoved 'embedding'\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3 -DGGML_USE_OPENBLAS -I/usr/local/include/blis\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread\r\nI LDFLAGS:  -lblis\r\nI CC:       cc (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\nI CXX:      g++ (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3 -DGGML_USE_OPENBLAS -I/usr/local/include/blis   -c ggml.c -o ggml.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -c llama.cpp -o llama.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -c examples/common.cpp -o common.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/main/main.cpp ggml.o llama.o common.o -o main -lblis\r\n\r\n====  Run ./main -h for help.  ====\r\n\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/quantize/quantize.cpp ggml.o llama.o -o quantize -lblis\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/perplexity/perplexity.cpp ggml.o llama.o common.o -o perplexity -lblis\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/embedding/embedding.cpp ggml.o llama.o common.o -o embedding -lblis\r\n\tlinux-vdso.so.1 (0x00007fff553ed000)\r\n\tlibblis.so.4 => /usr/local/lib/libblis.so.4 (0x00007f1011a8c000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f1011862000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f101177b000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f101175b000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f1011533000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f101200e000)\r\nmain: seed = 1680212135\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml map size = 4017.70 MB\r\nllama_model_load: ggml ctx size =  81.25 KB\r\nllama_model_load: mem required  = 5809.78 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading tensors from './models/7B/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 512, n_batch = 256, n_predict = 512, n_keep = 0\r\n\r\n\r\n blis or blas\r\nI know the word is spelled either with a B, L or an S. I believe it was used for either a sword (blade) or a small dagger or short knife.\r\nDoes anyone know the correct spelling?\r\nThanks in advance to everyone who might answer this question.\r\nblis, blas\r\nIt is indeed possible that it's spelt both ways: https://en.wikipedia.org/wiki/Bliss_(disambiguation)\r\nSo you are correct it could be either way but it would depend on the context. It is used in Scottish names like Bliss-Carver or Blaisdell for example.\r\nI agree with @MikeSteeden, it's possible to find it spelled both ways. I am not sure what exactly is your question: do you want to know if either one of these variants is correct? If so, then the answer is yes: bliss and blaise are acceptable spellings.\r\nI wanted to know which is correct. It's a family name and I just got confused as to which spelling is correct since I have seen it in two different ways. Thanks for your answers. [end of text]\r\n\r\nllama_print_timings:        load time =  1076.06 ms\r\nllama_print_timings:      sample time =   190.66 ms /   243 runs   (    0.78 ms per run)\r\nllama_print_timings: prompt eval time =   482.64 ms /     6 tokens (   80.44 ms per token)\r\nllama_print_timings:        eval time = 46036.34 ms /   242 runs   (  190.23 ms per run)\r\nllama_print_timings:       total time = 47307.07 ms\r\n\r\nreal\t0m47.525s\r\nuser\t12m20.192s\r\nsys\t0m1.248s\r\n```\r\nPerplexity run with blis doesn't process any chunks :\r\n```\r\nllama.cpp$ make -f Makefile.bliss clean && LLAMA_OPENBLAS=1 make -f Makefile.bliss;ldd ./perplexity ;time ./perplexity -t 16 -m ./models/7B/ggml-model-q4_0.bin -f /data/llama/wikitext-2-raw/wiki.wiki.test.raw\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread\r\nI LDFLAGS:  \r\nI CC:       cc (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\nI CXX:      g++ (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\n\r\nrm -vf *.o main quantize perplexity embedding\r\nremoved 'common.o'\r\nremoved 'ggml.o'\r\nremoved 'llama.o'\r\nremoved 'main'\r\nremoved 'quantize'\r\nremoved 'perplexity'\r\nremoved 'embedding'\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3 -DGGML_USE_OPENBLAS -I/usr/local/include/blis\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread\r\nI LDFLAGS:  -lblis\r\nI CC:       cc (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\nI CXX:      g++ (Ubuntu 10.4.0-4ubuntu1~22.04) 10.4.0\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3 -DGGML_USE_OPENBLAS -I/usr/local/include/blis   -c ggml.c -o ggml.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -c llama.cpp -o llama.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -c examples/common.cpp -o common.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/main/main.cpp ggml.o llama.o common.o -o main -lblis\r\n\r\n====  Run ./main -h for help.  ====\r\n\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/quantize/quantize.cpp ggml.o llama.o -o quantize -lblis\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/perplexity/perplexity.cpp ggml.o llama.o common.o -o perplexity -lblis\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/embedding/embedding.cpp ggml.o llama.o common.o -o embedding -lblis\r\n\tlinux-vdso.so.1 (0x00007ffced7f6000)\r\n\tlibblis.so.4 => /usr/local/lib/libblis.so.4 (0x00007fbe1ee26000)\r\n\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fbe1ebfc000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fbe1eb15000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fbe1eaf5000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fbe1e8cd000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007fbe1f3a6000)\r\nmain: seed = 1680214250\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml map size = 4017.70 MB\r\nllama_model_load: ggml ctx size =  81.25 KB\r\nllama_model_load: mem required  = 5809.78 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading tensors from './models/7B/ggml-model-q4_0.bin'\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \r\nperplexity : calculating perplexity over 0 chunks\r\n\r\n\r\nllama_print_timings:        load time =     9.90 ms\r\nllama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\r\nllama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)\r\nllama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\r\nllama_print_timings:       total time =   578.24 ms\r\n\r\nreal\t0m0.700s\r\nuser\t0m0.105s\r\nsys\t0m0.579s\r\n```\r\n",
    "labels": [
      "enhancement",
      "performance"
    ],
    "state": "closed",
    "created_at": "2023-03-30T22:14:53+00:00",
    "closed_at": "2023-04-13T08:09:16+00:00",
    "comments": 30,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/637/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/637"
  },
  {
    "number": 634,
    "title": "Compiling with LLAMA_OPENBLAS=1 fails on Arch Linux",
    "body": "The problem is missing `cblas` symbols:\r\n\r\n```plaintext\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/main/main.cpp ggml.o llama.o common.o -o main -lopenblas\r\n/usr/bin/ld: ggml.o: in function `ggml_compute_forward_mul_mat_f16_f32':\r\nggml.c:(.text+0x454b): undefined reference to `cblas_sgemm'\r\n/usr/bin/ld: ggml.o: in function `ggml_compute_forward':\r\nggml.c:(.text+0xcc1b): undefined reference to `cblas_sgemm'\r\n/usr/bin/ld: ggml.c:(.text+0xd42d): undefined reference to `cblas_sgemm'\r\ncollect2: error: ld returned 1 exit status\r\nmake: *** [Makefile:241: main] Error 1\r\n```\r\n\r\nInstalling `cblas` and adding `-lcblas` to the link flags fixes the issue.\r\n\r\nThis is on Arch Linux. I'll also submit the trivial PR, but I don't know if there are different versions/systems where this would cause issues.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-30T19:41:07+00:00",
    "closed_at": "2023-04-12T15:26:57+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/634/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/634"
  },
  {
    "number": 633,
    "title": "~2x perf improvement on Apple Silicon by changing state_shared.has_work access from atomic to mutex/conditional",
    "body": "### Discussed in https://github.com/ggerganov/llama.cpp/discussions/616\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **izard** March 30, 2023</sup>\r\nI profiled on a latest Mac Book Pro machine and found that significantly more time is spent in atomic checks for `state_shared.has_work` in while loops than doing actual work in matrix multiply.\r\nSo I changed busy waits like: \r\n```\r\npthread_mutex_lock(&state->shared->mutex);\r\n   while (state->shared->has_work) {\r\n     pthread_cond_wait(&state->shared->cond, &state->shared->mutex);\r\n// unlock\r\n```\r\n\r\nand setting `has_work` to \r\n```\r\npthread_mutex_lock(&state_shared.mutex);\r\nstate_shared.has_work = true;\r\npthread_cond_broadcast(&state_shared.cond);\r\npthread_mutex_unlock(&state_shared.mutex);\r\n\r\n```\r\nGot a nice 2x speedup in time/token.\r\n\r\nI can't post a patch/pull request because everything I do in spare time still belongs to my employer, but the change is trivial as described above. Probably won't provide much benefit (if any) for other platforms though.\r\n</div>",
    "labels": [
      "enhancement",
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-30T19:18:14+00:00",
    "closed_at": "2024-04-12T01:07:15+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/633/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/633"
  },
  {
    "number": 630,
    "title": "Combine large LLM with small LLM for faster inference",
    "body": "So I was thinking about the following idea.\r\nIt is probably completely bogus, but I would definitely investigate it when and if I had the time to, so maybe someone else would be interested as well.\r\n\r\n---\r\n\r\nLarge LLM takes a lot of time to perform token inference. Lets say it takes 500ms per token.\r\n\r\nA small LLM (or some other approach) can infer a token very fast. Lets say < 5ms.\r\n\r\nLets assume that the small LLM is correct 80-90% of the time.\r\n\r\nThe idea is the following:\r\n\r\n- Before I run the large LLM inference for the next token, I infer it using the small LLM\r\n- I now want to somehow partially evaluate the large LLM (let's say the first 10% of the layers) and get an approximate estimate for the next token\r\n- If this estimate indicates a high probability for that token (i.e. above some threshold) - we stop and directly say that this is the new token. At this point we would have consumed (5ms for the small LLM + ~50ms for the large LLM)\r\n- Otherwise, we proceed to evaluate the rest of the layers of the large LLM\r\n\r\nIn the described process, I would reach step 4 only for 10-20% of the tokens, but for the rest - I will take the shortcut in step 3.\r\nHence, I will have an efficient inference with a Large LLM.\r\n\r\nObviously, the biggest question is if step 2 is possible at all.\r\nI suppose the answer is \"no\", but who knows.\r\n",
    "labels": [
      "question",
      "research \ud83d\udd2c",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-30T17:54:01+00:00",
    "closed_at": "2024-04-12T01:07:17+00:00",
    "comments": 44,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/630/reactions",
      "total_count": 26,
      "+1": 13,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 5,
      "rocket": 2,
      "eyes": 6
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/630"
  },
  {
    "number": 627,
    "title": "How to activate BLAS?",
    "body": "Hello,\r\n\r\nI've heard that I could get BLAS activated through my intel i7 10700k by installing this [library](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html).\r\n\r\nUnfortunatly, nothing happened, after compiling again with Clung I still have no BLAS in llama.cpp\r\n\r\n```\r\nsystem_info: n_threads = 14 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\r\n```\r\n\r\nMaybe it's just not possible I don't know, I need someone to tell me the truth \ud83d\ude05 ",
    "labels": [
      "need more info",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-30T16:56:25+00:00",
    "closed_at": "2024-05-10T01:28:42+00:00",
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/627/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/627"
  },
  {
    "number": 624,
    "title": "How to quantize a fine-tuned llama model?",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI have fine-tuned llama 7B model that I want to quantize to 4-bit and run using llama.cpp\r\nI got the weight in 3 parts .bin files and converted them to the same llama model naming scheme, i.e. \"consolidated.XX\"\r\n\r\ni.e.\r\n``pytorch_model-00001-of-00003.bin`` to ``consolidated.00.bin`` etc. etc.\r\n\r\nsince the original llama 7B model comes in a single pth file and mine is 3 files in .bin format\r\nI changed the ``convert-pth-to-ggml.py`` script as follows:\r\n\r\nline 37 changed from:\r\n```\r\nmappings = {4096: 1, 5120: 2, 6656: 4, 8192: 8}\r\n```\r\nto\r\n```\r\nmappings = {4096: 3, 5120: 2, 6656: 4, 8192: 8}\r\n```\r\n\r\nline 164 changed from:\r\n```\r\nfname_model = f\"{dir_model}/consolidated.0{p}.pth\"\r\n```\r\nto\r\n```\r\nfname_model = f\"{dir_model}/consolidated.0{p}.bin\"\r\n```\r\n\r\nI ran the conversion with the command: ``python3 convert-pth-to-ggml.py models/7B 1``\r\n\r\nand got 3 files\r\n```\r\nggml-model-f16.bin\r\nggml-model-f16.bin.1\r\nggml-model-f16.bin.2\r\n```\r\n\r\nnow I want to quantize it to 4-bits as per the guide with the command: ``python3 quantize.py 7B``\r\n\r\nI expect llama.cpp to be able to quantize a fine-tuned llama model even when it's not the original model but have the same architecture, layers, and params.\r\n\r\n# Current Behavior\r\n\r\nThe quantize script fails to quantize the model, can't run models on llama.cpp as a results\r\n\r\n![image](https://user-images.githubusercontent.com/34383384/228863305-43a2e561-5b80-4407-a2ea-7e4ad9163e9d.png)\r\n\r\n\r\n# Environment and Context \r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\nManjaro Linux kernel version 6.0.19-4\r\nCPU: i7-9750H\r\nDell XPS 7590\r\nPython 3.10.9\r\nGNU Make 4.4\r\ng++ 12.2.1\r\n\r\n# Steps to Reproduce\r\n1. obtain weights from: https://github.com/Kent0n-Li/ChatDoctor\r\n2. run the changes mentioned above to the code \r\n\r\n# Failure Logs\r\n\r\n```\r\n~/Desktop/llama.cpp python3 quantize.py 7B                                                                                                                                                                  \r\nllama_model_quantize_internal: loading model from '/home/eyal/Desktop/llama.cpp/models/7B/ggml-model-f16.bin.2'\r\nllama_model_quantize_internal: n_vocab = 32000\r\nllama_model_quantize_internal: n_ctx   = 512\r\nllama_model_quantize_internal: n_embd  = 4096\r\nllama_model_quantize_internal: n_mult  = 256\r\nllama_model_quantize_internal: n_head  = 32\r\nllama_model_quantize_internal: n_layer = 32\r\nllama_model_quantize_internal: f16     = 1\r\n              model.layers.23.mlp.up_proj.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.004 0.021 0.006 0.006 0.005 0.006 0.008 0.017 0.842 0.018 0.009 0.008 0.006 0.007 0.008 0.029 \r\n\r\nAn error ocurred while trying to quantize the models.\r\n```",
    "labels": [
      "enhancement",
      "good first issue",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-30T14:10:48+00:00",
    "closed_at": "2023-07-28T19:21:40+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/624/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/624"
  },
  {
    "number": 623,
    "title": "How can I do summarization ",
    "body": "I'm trying to make something like https://platform.openai.com/examples/default-notes-summary\r\n\r\nBut it fails, I tried with gpt4all, llama and alpaca 7B. Maybe I should ajust the prompt ?\r\n\r\n<img width=\"1440\" alt=\"Screenshot 2023-03-30 at 15 17 56\" src=\"https://user-images.githubusercontent.com/74246611/228848634-1a27e9a6-fed6-4abc-8aa1-9964f8e2595f.png\">\r\n",
    "labels": [
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-30T13:19:01+00:00",
    "closed_at": "2023-03-30T17:08:08+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/623/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/623"
  },
  {
    "number": 622,
    "title": "Compilation failure on aarch64-linux in ggml SIMD code",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nShould compile on aarch64-linux\r\n\r\n# Current Behavior\r\n\r\n```\r\n[12:34:31] [  8%] Building C object CMakeFiles/ggml.dir/ggml.c.o\r\n[12:34:31] /opt/bin/aarch64-linux-gnu-libgfortran5-cxx11/aarch64-linux-gnu-gcc --sysroot=/opt/aarch64-linux-gnu/aarch64-linux-gnu/sys-root/  -I/workspace/srcdir/llama.cpp/. -O3 -DNDEBUG -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -MD -MT CMakeFiles/ggml.dir/ggml.c.o -MF CMakeFiles/ggml.dir/ggml.c.o.d -o CMakeFiles/ggml.dir/ggml.c.o -c /workspace/srcdir/llama.cpp/ggml.c\r\n[12:34:31] /workspace/srcdir/llama.cpp/ggml.c: In function \u2018dequantize_row_q4_1\u2019:\r\n[12:34:31] /workspace/srcdir/llama.cpp/ggml.c:1041:13: note: use -flax-vector-conversions to permit conversions between vectors with differing element types or numbers of subparts\r\n[12:34:31]              const uint16x8_t vi_0 = vmovl_s8(vget_low_u8 (vq));\r\n[12:34:31]              ^~~~~\r\n[12:34:31] /workspace/srcdir/llama.cpp/ggml.c:1041:46: error: incompatible type for argument 1 of \u2018vmovl_s8\u2019\r\n[12:34:31]              const uint16x8_t vi_0 = vmovl_s8(vget_low_u8 (vq));\r\n[12:34:31]                                               ^~~~~~~~~~~~~~~~\r\n[12:34:31] In file included from /workspace/srcdir/llama.cpp/ggml.c:164:\r\n[12:34:31] /opt/aarch64-linux-gnu/lib/gcc/aarch64-linux-gnu/8.1.0/include/arm_neon.h:8839:20: note: expected \u2018int8x8_t\u2019 but argument is of type \u2018uint8x8_t\u2019\r\n[12:34:31]  vmovl_s8 (int8x8_t a)\r\n[12:34:31]            ~~~~~~~~~^\r\n[12:34:31] /workspace/srcdir/llama.cpp/ggml.c:1042:46: error: incompatible type for argument 1 of \u2018vmovl_s8\u2019\r\n[12:34:31]              const uint16x8_t vi_1 = vmovl_s8(vget_high_u8(vq));\r\n[12:34:31]                                               ^~~~~~~~~~~~~~~~\r\n[12:34:31] In file included from /workspace/srcdir/llama.cpp/ggml.c:164:\r\n[12:34:31] /opt/aarch64-linux-gnu/lib/gcc/aarch64-linux-gnu/8.1.0/include/arm_neon.h:8839:20: note: expected \u2018int8x8_t\u2019 but argument is of type \u2018uint8x8_t\u2019\r\n[12:34:31]  vmovl_s8 (int8x8_t a)\r\n[12:34:31]            ~~~~~~~~~^\r\n[12:34:31] make[2]: *** [CMakeFiles/ggml.dir/build.make:76: CMakeFiles/ggml.dir/ggml.c.o] Error 1\r\n[12:34:31] make[2]: Leaving directory '/workspace/srcdir/llama.cpp/build'\r\n```\r\n\r\n# Environment and Context \r\n\r\nJulia's BinaryBuilder cross-compilation setup with gcc/g++ 8.1.0, i.e. building for a `aarch64-linux-gnu` target on a `x86_64-linux-musl` host. See https://github.com/JuliaPackaging/Yggdrasil/pull/6476 for more details.\r\n\r\n# Proposed solution\r\n\r\nUse `vmovl_u8` instead of `vmovl_s8`? Not sure about the intention of the code, and it seems to be just a day old.\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-30T10:46:46+00:00",
    "closed_at": "2023-03-30T17:27:50+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/622/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/622"
  },
  {
    "number": 621,
    "title": "error:python3 quantize.py 7B ",
    "body": "When I tried the llama model and run :**python3 quantize.py 7B**    for operation, ```\r\nthe \"quantize\" script was not found in the current location appeared\r\nIf you want to use it from another location, set the -- quantify script path argument from the command line\r\n```\r\nIt's still this error. I have also made other attempts: `python3/Users/sunxiaotong/Desktop/llama/llama.cpp/quantize.py - q/ quantize.py 7B`\r\n```\r\n\r\n```\r\nusage: python3 quantize.py [-h] [-r] [-m MODELS_PATH]\r\n[-q QUANTIZE_SCRIPT_PATH]\r\n{7B,13B,30B,65B} [{7B,13B,30B,65B} ...]\r\npython3 quantize.py: error: argument models: invalid choice: '/Users/sunxiaotong/Desktop/llama/llama.cpp/models/7B/ggml-model-f16.bin' (choose from '7B', '13B', '30B', '65B')\r\n```\r\nMay I ask where I was wrong? Can you give me some suggestions",
    "labels": [
      "question"
    ],
    "state": "closed",
    "created_at": "2023-03-30T09:44:35+00:00",
    "closed_at": "2023-03-30T23:22:05+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/621/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/621"
  },
  {
    "number": 620,
    "title": "[build] ARMv8 build problem (OpenWrt)",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n  * `git clone $url; cd llama.cpp; make` \r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ ] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI expected to build the basic llama.cpp `bin/main` program, to see if building even worked properly.\r\n\r\n# Current Behavior\r\n\r\n```\r\nroot@FriendlyWrt /s/o/llama.cpp (master)# make\r\nI llama.cpp build info:\r\nI UNAME_S:  Linux\r\nI UNAME_P:  unknown\r\nI UNAME_M:  aarch64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mcpu=native\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -mcpu=native\r\nI LDFLAGS:\r\nI CC:       cc (OpenWrt GCC 11.2.0) 11.2.0\r\nI CXX:      g++ (OpenWrt GCC 11.2.0) 11.2.0\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mcpu=native   -c ggml.c -o ggml.o\r\nggml.c: In function 'dequantize_row_q4_1':\r\nggml.c:1041:13: note: use '-flax-vector-conversions' to permit conversions between vectors with differing element types or numbers of subparts\r\n 1041 |             const uint16x8_t vi_0 = vmovl_s8(vget_low_u8 (vq));\r\n      |             ^~~~~\r\nggml.c:1041:46: error: incompatible type for argument 1 of 'vmovl_s8'\r\n 1041 |             const uint16x8_t vi_0 = vmovl_s8(vget_low_u8 (vq));\r\n      |                                              ^~~~~~~~~~~~~~~~\r\n      |                                              |\r\n      |                                              uint8x8_t\r\nIn file included from ggml.c:164:\r\n/usr/lib/gcc/aarch64-openwrt-linux-musl/11.2.0/include/arm_neon.h:7989:20: note: expected 'int8x8_t' but argument is of type 'uint8x8_t'\r\n 7989 | vmovl_s8 (int8x8_t __a)\r\n      |           ~~~~~~~~~^~~\r\nggml.c:1042:46: error: incompatible type for argument 1 of 'vmovl_s8'\r\n 1042 |             const uint16x8_t vi_1 = vmovl_s8(vget_high_u8(vq));\r\n      |                                              ^~~~~~~~~~~~~~~~\r\n      |                                              |\r\n      |                                              uint8x8_t\r\nIn file included from ggml.c:164:\r\n/usr/lib/gcc/aarch64-openwrt-linux-musl/11.2.0/include/arm_neon.h:7989:20: note: expected 'int8x8_t' but argument is of type 'uint8x8_t'\r\n 7989 | vmovl_s8 (int8x8_t __a)\r\n      |           ~~~~~~~~~^~~\r\nmake: *** [Makefile:226: ggml.o] Error 1\r\n```\r\n\r\n# Environment and Context \r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n```\r\n# lscpu\r\nArchitecture:           aarch64\r\n  CPU op-mode(s):       32-bit, 64-bit\r\n  Byte Order:           Little Endian\r\nCPU(s):                 8\r\n  On-line CPU(s) list:  0-7\r\nVendor ID:              ARM\r\n  Model name:           Cortex-A55\r\n    Model:              0\r\n    Thread(s) per core: 1\r\n    Core(s) per socket: 4\r\n    Socket(s):          1\r\n    Stepping:           r2p0\r\n    CPU(s) scaling MHz: 56%\r\n    CPU max MHz:        1800.0000\r\n    CPU min MHz:        408.0000\r\n    BogoMIPS:           48.00\r\n    Flags:              fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm lrcpc dcpop asimddp\r\n  Model name:           Cortex-A76\r\n    Model:              0\r\n    Thread(s) per core: 1\r\n    Core(s) per socket: 2\r\n    Socket(s):          2\r\n    Stepping:           r4p0\r\n    CPU(s) scaling MHz: 22%\r\n    CPU max MHz:        2352.0000\r\n    CPU min MHz:        408.0000\r\n    BogoMIPS:           48.00\r\n    Flags:              fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm lrcpc dcpop asimddp\r\nCaches (sum of all):\r\n  L1d:                  384 KiB (8 instances)\r\n  L1i:                  384 KiB (8 instances)\r\n  L2:                   2.5 MiB (8 instances)\r\n  L3:                   3 MiB (1 instance)\r\nVulnerabilities:\r\n  Itlb multihit:        Not affected\r\n  L1tf:                 Not affected\r\n  Mds:                  Not affected\r\n  Meltdown:             Not affected\r\n  Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\n  Spectre v1:           Mitigation; __user pointer sanitization\r\n  Spectre v2:           Vulnerable: Unprivileged eBPF enabled\r\n  Srbds:                Not affected\r\n  Tsx async abort:      Not affected\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n```\r\nroot@FriendlyWrt /s/o/llama.cpp (master)# uname -a\r\nLinux FriendlyWrt 5.10.110 #1 SMP Sat Dec 3 01:25:15 CST 2022 aarch64 GNU/Linux\r\nroot@FriendlyWrt /s/o/llama.cpp (master)# cat /etc/os-release\r\nNAME=\"OpenWrt\"\r\nVERSION=\"22.03.2\"\r\nID=\"openwrt\"\r\nID_LIKE=\"lede openwrt\"\r\nPRETTY_NAME=\"OpenWrt 22.03.2\"\r\nVERSION_ID=\"22.03.2\"\r\nHOME_URL=\"https://openwrt.org/\"\r\nBUG_URL=\"https://bugs.openwrt.org/\"\r\nSUPPORT_URL=\"https://forum.openwrt.org/\"\r\nBUILD_ID=\"r19803-9a599fee93\"\r\nOPENWRT_BOARD=\"rockchip/armv8\"\r\nOPENWRT_ARCH=\"aarch64_generic\"\r\nOPENWRT_TAINTS=\"busybox\"\r\nOPENWRT_DEVICE_MANUFACTURER=\"OpenWrt\"\r\nOPENWRT_DEVICE_MANUFACTURER_URL=\"https://openwrt.org/\"\r\nOPENWRT_DEVICE_PRODUCT=\"Generic\"\r\nOPENWRT_DEVICE_REVISION=\"v0\"\r\nOPENWRT_RELEASE=\"OpenWrt 22.03.2 r19803-9a599fee93\"\r\n```\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\nroot@FriendlyWrt /s/o/llama.cpp (master)# python --version\r\nPython 3.11.2\r\nroot@FriendlyWrt /s/o/llama.cpp (master)# make --version\r\nGNU Make 4.3\r\nBuilt for aarch64-openwrt-linux-gnu\r\nCopyright (C) 1988-2020 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.\r\nroot@FriendlyWrt /s/o/llama.cpp (master)# gcc --version\r\ngcc (OpenWrt GCC 11.2.0) 11.2.0\r\nCopyright (C) 2021 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nPlease help provide information about the failure if this is a bug. If it is not a bug, please remove the rest of this template.\r\n\r\n# Steps to Reproduce\r\n\r\nQuite simple:\r\n\r\n```\r\ngit clone https://github.com/ggerganov/llama.cpp.git\r\ncd llama.cpp\r\nmake\r\n```\r\n\r\nReading the Makefile, I noticed that a lot of configuration was done automatically, so I assumed I could just go and `make` it.\r\n\r\n# Failure Logs\r\n\r\nSee above. :)",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-30T09:24:25+00:00",
    "closed_at": "2023-03-30T10:05:21+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/620/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/620"
  },
  {
    "number": 618,
    "title": "Making a \"quantize-ggml_16bit-to-gptq.py\" script?",
    "body": "Hello,\r\n\r\nI know the [quantize.py](https://github.com/ggerganov/llama.cpp/blob/master/quantize.py) converts a ggml 16 bits into a ggml 4 bits RTN.\r\nDo you think it's possible to create a script that converts a ggml 16 bits into a ggml 4bits GPTQ?\r\n\r\nReferring to [this repository](https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/pytorch), it appears that the current implementation of the quantization relies only on GPU, which demands a significant amount of VRAM and might not be suitable for the average user.\r\n\r\nA new script, which we could call \"quantize-ggml_16bit-to-gptq.py\", could be designed to use only CPU and RAM resources, making it more accessible to the general public.",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-30T07:23:54+00:00",
    "closed_at": "2024-04-12T01:07:19+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/618/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/618"
  },
  {
    "number": 614,
    "title": "Which tokenizer.model is needed for GPT4ALL? ",
    "body": "Which tokenizer.model is needed for GPT4ALL for use with convert-gpt4all-to-ggml.py? Is it the one for LLaMA 7B? It is unclear from the current README and gpt4all-lora-quantized.bin seems to be typically distributed without the tokenizer.model file.\r\n\r\n",
    "labels": [
      "question"
    ],
    "state": "closed",
    "created_at": "2023-03-30T00:44:00+00:00",
    "closed_at": "2023-03-31T02:59:00+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/614/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/614"
  },
  {
    "number": 604,
    "title": "Reverting generated output/user input!",
    "body": "Hey!\r\n\r\nThis is a feature request for reverting input/output. One example usecase is to be able to retry generation if the response wasn't as desired.\r\nOne way of implementing this could be by adding the ability to create \"snapshots\" using signals(?).\r\n\r\nThanks a lot\r\nniansa",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-29T18:51:38+00:00",
    "closed_at": "2024-04-12T01:07:21+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/604/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/604"
  },
  {
    "number": 603,
    "title": "Performance Discrepancy: gpt4all Faster than Optimized llama.cpp",
    "body": "**Expected Behavior**\r\n\r\nI am comparing the performance of two executables: llama.cpp (current version) and the default gpt4all executable (which uses a previous version of llama.cpp). I am using the same language model for both executables, and I expect the current version of llama.cpp (which is built specifically for the hardware) to perform at least as fast as the default gpt4all executable.\r\n\r\n**Current Behavior**\r\n\r\nThe default gpt4all executable, which uses a previous version of llama.cpp, performs significantly faster than the current version of llama.cpp. Despite building the current version of llama.cpp with hardware-specific compiler flags, it consistently performs significantly slower when using the same model as the default gpt4all executable.\r\n\r\n**Environment and Context**\r\n\r\nI am running the comparison on a Windows platform, using the default gpt4all executable and the current version of llama.cpp included in the gpt4all project. The version of llama.cpp is the latest available (after the compatibility with the gpt4all model).\r\n\r\n**Steps to Reproduce**\r\n\r\n1. Build the current version of llama.cpp with hardware-specific compiler flags.\r\n2. Execute the llama.cpp executable using the gpt4all language model and record the performance metrics.\r\n3. Execute the default gpt4all executable (previous version of llama.cpp) using the same language model and record the performance metrics.\r\n4. You'll see that the gpt4all executable generates output significantly faster for any number of threads or config.\r\n\r\nHere's some context/config when I'm doing the runs:\r\n\r\n![image](https://user-images.githubusercontent.com/102247808/228639042-23fc9484-00a0-4cb4-8ebc-df9da1ee2e5c.png)\r\n(left panel is latest llama.cpp, right panel is gpt4all build)\r\n\r\n\r\nThis is the older version that gpt4all uses (with some tweaks): https://github.com/zanussbaum/gpt4all.cpp\r\n\r\n*To quickly test the difference yourself you can use the gpt4all default binaries here: https://github.com/nomic-ai/gpt4all/tree/main/chat\r\n",
    "labels": [
      "performance"
    ],
    "state": "closed",
    "created_at": "2023-03-29T18:46:33+00:00",
    "closed_at": "2023-04-12T15:30:22+00:00",
    "comments": 67,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/603/reactions",
      "total_count": 5,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 5
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/603"
  },
  {
    "number": 601,
    "title": "When running in PowerShell in windows, it works, but throws an error in interactive mode",
    "body": "I built llama.cpp using cmake and then Visual Studio (after many trials and tribulations since I'm pretty new to this), but finally got it working.\r\n\r\nUsing the 7B model the outputs are reasonable, but when I put the -i tag, it runs, then I hit Ctrl+C, it allows me to enter text, but when I hit enter an error pops up in a windows shown below:\r\n\r\n![image](https://user-images.githubusercontent.com/65059714/228617990-0da94e0c-5df4-4311-9d41-0ed5c060df0f.png)\r\n\r\nI'm running this on my windows machine, but I have been using WSL to get some stuff to work.\r\n\r\nHere's an example of it failing:\r\n\r\n`(base) PS G:\\llama\\llama.cpp> .\\bin\\Debug\\main.exe -m ..\\LLaMA\\7B\\ggml-model-q4_0.bin -i -n 124 -t 24`\r\n\r\n`(base) PS G:\\llama\\llama.cpp> .\\bin\\Debug\\main.exe -m ..\\LLaMA\\7B\\ggml-model-q4_0.bin -i -n 124 -t 24\r\nmain: seed = 1680110536\r\nllama_model_load: loading model from '..\\LLaMA\\7B\\ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml ctx size = 4273.34 MB\r\nllama_model_load: mem required  = 6065.34 MB (+ 1026.00 MB per state)\r\nllama_model_load: loading model part 1/1 from '..\\LLaMA\\7B\\ggml-model-q4_0.bin'\r\nllama_model_load: .................................... done\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 24 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\r\nmain: interactive mode on.\r\nsampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\ngenerate: n_ctx = 512, n_batch = 8, n_predict = 124, n_keep = 0\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n\r\n 2016 has been a big year for Uber. The ride-sharing app has\r\nSome inputted text\r\n(base) PS G:\\llama\\llama.cpp>`",
    "labels": [
      "bug",
      "windows"
    ],
    "state": "closed",
    "created_at": "2023-03-29T17:23:13+00:00",
    "closed_at": "2023-05-18T10:49:11+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/601/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/601"
  },
  {
    "number": 599,
    "title": "Support tensors with 64-bit number of elements in ggml",
    "body": "# Expected Behavior\r\n\r\nWhen setting '-c' to a large number, with sufficient RAM llama.cpp should run. I'm aware that it warns me that context sizes larger than 2048 might produce poor results, but the results are actually fine, if it's not crashing.\r\n\r\n# Current Behavior\r\n\r\nWhen setting '-c' to a large number, llama.cpp crashes with the error message\r\n```\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32289959360, available 32279078144)\r\n```\r\n(repeated many times with slightly varying numbers, see below).\r\n\r\nTo be precise, I'm using the command\r\n```\r\n./main -m ./models/65B/ggml-model-q4_0.bin -t 16 -c 3300 -b 16 -n 2048 --keep 0 --temp 0.8 \\\r\n    --repeat_last_n 512 --repeat_penalty 1.1 --color \\\r\n    --ignore-eos\r\n```\r\nwhich crashes while the same command with '-c 3200' works.\r\n\r\nIf still have plenty of free RAM (~60 GB) so that shouldn't be an issue.\r\n\r\nThis might be related to #52 but it seem to die later in the process so probably it's something different.\r\n\r\n# Environment and Context \r\n\r\nI'm using Debian 11 and compiled using clang(++)-13 (but same result with g++) with OpenBLAS:\r\n\r\n```\r\nsystem_info: n_threads = 16 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\r\n```\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n```\r\n$ lscpu\r\n...\r\nModel name: Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz\r\n...\r\nFlags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d arch_capabilities\r\n```\r\n\r\n```\r\n$ lsmem\r\nRANGE                                  SIZE  STATE REMOVABLE BLOCK\r\n0x0000000000000000-0x000000007fffffff    2G online       yes     0\r\n0x0000000100000000-0x00000020ffffffff  128G online       yes  2-65\r\n\r\nMemory block size:         2G\r\nTotal online memory:     130G\r\nTotal offline memory:      0B\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n```\r\nLinux 5.10.0-21-amd64 #1 SMP Debian 5.10.162-1 (2023-01-21) x86_64 GNU/Linux\r\n```\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version\r\nPython 3.10.9\r\n$ make --version\r\nGNU Make 4.3\r\nBuilt for x86_64-pc-linux-gnu\r\n$ g++ --version\r\ng++ (Debian 10.2.1-6) 10.2.1 20210110\r\n$ clang++-13 --version\r\nDebian clang version 13.0.1-6~deb11u1\r\nTarget: x86_64-pc-linux-gnu\r\nThread model: posix\r\nInstalledDir: /usr/bin\r\n```\r\n\r\n```\r\nllama.cpp$ git log | head -1\r\ncommit a6956b25a1c783e5e96fe06c9c00438f846ef047\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nHopefully everything stated above. Feel free to ask for details.\r\n\r\n# Steps to Reproduce\r\n\r\n1. Have an huge amount of RAM\r\n2. Execute command above\r\n\r\n# Failure Logs\r\n\r\n```\r\nmain: warning: model does not support context sizes greater than 2048 tokens (3300 specified);expect poor results\r\nmain: seed = 1680105266\r\nllama_model_load: loading model from './models/65B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 3300\r\nllama_model_load: n_embd  = 8192\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 64\r\nllama_model_load: n_layer = 80\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 22016\r\nllama_model_load: n_parts = 8\r\nllama_model_load: type    = 4\r\nllama_model_load: ggml ctx size = 30783.73 MB\r\nllama_model_load: mem required  = 33343.73 MB (+ 5120.00 MB per state)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32289959360, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32289959360, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32289959360, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32360771200, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32360771200, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32360771200, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290025280, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290025280, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290025280, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290025280, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32360837120, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32360837120, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32360837120, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290091200, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290091200, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290091200, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290091200, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32360903040, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32360903040, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32360903040, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290157120, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290157120, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290157120, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290157120, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32360968960, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32360968960, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32360968960, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290223040, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290223040, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290223040, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290223040, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361034880, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361034880, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361034880, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290288960, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290288960, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290288960, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290288960, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361100800, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361100800, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361100800, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290354880, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290354880, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290354880, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290354880, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361166720, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361166720, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361166720, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290420800, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290420800, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290420800, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290420800, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361232640, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361232640, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361232640, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290486720, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290486720, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290486720, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290486720, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361298560, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361298560, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361298560, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290552640, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290552640, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290552640, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290552640, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361364480, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361364480, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361364480, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290618560, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290618560, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290618560, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290618560, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361430400, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361430400, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361430400, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290684480, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290684480, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290684480, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290684480, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361496320, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361496320, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361496320, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290750400, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290750400, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290750400, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290750400, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361562240, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361562240, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361562240, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290816320, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290816320, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290816320, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290816320, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361628160, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361628160, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361628160, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290882240, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290882240, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290882240, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290882240, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361694080, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361694080, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361694080, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290948160, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290948160, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290948160, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32290948160, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361760000, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361760000, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361760000, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32291014080, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32291014080, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32291014080, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32291014080, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361825920, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361825920, available 32279078144)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 32361825920, available 32279078144)\r\nllama_model_load: loading model part 1/8 from './models/65B/ggml-model-q4_0.bin'\r\nllama_model_load: ........................................................................\r\n74420 Segmentation fault      ./main -m ./models/65B/ggml-model-q4_0.bin -t 16 -c 3300 -b 16 -n 2048 --keep 0 --temp 0.8 --repeat_last_n 512 --repeat_penalty 1.1 --color --ignore-eos\r\n```",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-29T16:15:55+00:00",
    "closed_at": "2023-06-16T06:53:12+00:00",
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/599/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/599"
  },
  {
    "number": 596,
    "title": "Is this true? :joy: ",
    "body": "I asked ChatGPT about the difference between `llama.cpp` and `whisper.cpp` and it says:\r\n\r\n![image](https://user-images.githubusercontent.com/3450257/228553783-4cf28da9-f025-4a7c-92a6-2c8c9c604c28.png)\r\n",
    "labels": [
      "\ud83e\udd99."
    ],
    "state": "closed",
    "created_at": "2023-03-29T13:30:18+00:00",
    "closed_at": "2023-04-06T15:20:27+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/596/reactions",
      "total_count": 18,
      "+1": 4,
      "-1": 0,
      "laugh": 11,
      "hooray": 0,
      "confused": 3,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/596"
  },
  {
    "number": 594,
    "title": "A few questions about the positional encodings",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Question\r\n\r\nIt is very kind of you to share such an amazing repo.  It works on my android device!\r\nMy question is that where is the Rotary Embedding implementation in the code? Or what work do you do to achieve this implicitly?  I found it in the [official code](https://github.com/facebookresearch/llama/blob/57b0eb62de0636e75af471e49e2f1862d908d9d8/llama/model.py#L63) but I don't seem to see the implementation of this part in your code. Maybe I was careless.\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-29T11:28:19+00:00",
    "closed_at": "2023-03-30T01:12:31+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/594/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/594"
  },
  {
    "number": 591,
    "title": "Why is the license not GPL 3.0 ?",
    "body": "It is my understand that the original code is under GPL 3.0\r\nhttps://github.com/facebookresearch/llama\r\n\r\nI'm not a legal expert but I thought this was a contaminating license.\r\nIs porting considered like a whole new project and does not fall under this category? ",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-29T07:53:39+00:00",
    "closed_at": "2023-03-29T08:44:06+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/591/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/591"
  },
  {
    "number": 590,
    "title": "Support gpt4all interactive mode",
    "body": "Hey!\r\n\r\nI just found this repo: https://github.com/nomic-ai/gpt4all and it looks amazing! They've forked alpaca.cpp but with their own \"tweaks\" in a single commit: https://github.com/zanussbaum/gpt4all.cpp/commit/4a6afcb08fb243df9a919c26aab1027ebfa373cc\r\nSo I assume it'd be quite easy to support!\r\n\r\nNiansa",
    "labels": [
      "duplicate"
    ],
    "state": "closed",
    "created_at": "2023-03-29T06:10:08+00:00",
    "closed_at": "2023-03-29T06:21:05+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/590/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/590"
  },
  {
    "number": 589,
    "title": ".dot file of ggml_graph can not be generated to .png file",
    "body": "Hi, I want to generate a picture of the grapj. And I uncommented this 2 lines in \"llama.cpp\", so that to run the function `ggml_graph_dump_dot\uff08\uff09`\r\n```\r\n    //if (n_past%100 == 0) {\r\n        ggml_graph_print   (&gf);\r\n        ggml_graph_dump_dot(&gf, NULL, \"gpt-2.dot\");\r\n    //}\r\n```\r\nAnd I got a file named `gpt-2.dot`\r\nBut when I run command in python:\r\n```\r\nfrom graphviz import Digraph\r\nimport sys\r\nsys.setrecursionlimit(300000) \r\n\r\nimport pydot\r\nimport os\r\n(graph,) = pydot.graph_from_dot_file(\"D:\\\\PIQ\\\\llama.cpp\\\\build\\\\examples\\\\main\\\\gpt-2.dot\")\r\ngraph.write_png(\"gpt-2.png\")\r\n```\r\nI get the error message: `Expect '{' but got '['`\r\nSo I modifid the function `ggml_graph_dump_dot\uff08\uff09` in `ggml.c` like this:\r\n```\r\nvoid ggml_graph_dump_dot(const struct ggml_cgraph * gb, const struct ggml_cgraph * gf, const char * filename) {\r\n    char color[16];\r\n\r\n    FILE * fp = fopen(filename, \"w\");\r\n    GGML_ASSERT(fp);\r\n\r\n    fprintf(fp, \"digraph G {\\n\");\r\n    fprintf(fp, \"  newrank = true;\\n\");\r\n    fprintf(fp, \"  rankdir = LR;\\n\");\r\n\r\n    for (int i = 0; i < gb->n_nodes; i++) {\r\n        struct ggml_tensor * node = gb->nodes[i];\r\n\r\n        if (ggml_graph_get_parent(gb, node) != NULL) {\r\n            continue;\r\n        }\r\n\r\n        if (node->is_param) {\r\n            snprintf(color, sizeof(color), \"yellow\");\r\n        } else if (node->grad) {\r\n            if (ggml_graph_find(gf, node)) {\r\n                snprintf(color, sizeof(color), \"green\");\r\n            } else {\r\n                snprintf(color, sizeof(color), \"lightblue\");\r\n            }\r\n        } else {\r\n            snprintf(color, sizeof(color), \"white\");\r\n        }\r\n\r\n        fprintf(fp, \"  \\\"%p\\\" { \\\r\nstyle = filled; fillcolor = %s; shape = record; \\\r\nlabel=\\\"%d [%d, %d] | <x>%s\",\r\n                (void *) node, color,\r\n                i, node->ne[0], node->ne[1],\r\n                GGML_OP_SYMBOL[node->op]);\r\n\r\n        if (node->grad) {\r\n            fprintf(fp, \" | <g>%s\\\"; }\\n\", GGML_OP_SYMBOL[node->grad->op]);\r\n        } else {\r\n            fprintf(fp, \"\\\"; }\\n\");\r\n        }\r\n    }\r\n\r\n    for (int i = 0; i < gb->n_leafs; i++) {\r\n        struct ggml_tensor * node = gb->leafs[i];\r\n\r\n        snprintf(color, sizeof(color), \"pink\");\r\n\r\n        if (ggml_nelements(node) == 1) {\r\n            fprintf(fp, \"  \\\"%p\\\" { \\\r\nstyle = filled; fillcolor = %s; shape = record; \\\r\nlabel=\\\"<x>%.1e\\\"; }\\n\",\r\n                    (void *) node, color, ggml_get_f32_1d(node, 0));\r\n        } else {\r\n            fprintf(fp, \"  \\\"%p\\\" { \\\r\nstyle = filled; fillcolor = %s; shape = record; \\\r\nlabel=\\\"<x>CONST %d [%d, %d]\\\"; }\\n\",\r\n                    (void *) node, color,\r\n                    i, node->ne[0], node->ne[1]);\r\n        }\r\n    }\r\n\r\n    for (int i = 0; i < gb->n_nodes; i++) {\r\n        struct ggml_tensor * node = gb->nodes[i];\r\n\r\n        struct ggml_tensor * parent = ggml_graph_get_parent(gb, node);\r\n\r\n        if (node->src0) {\r\n            struct ggml_tensor * parent0 = ggml_graph_get_parent(gb, node->src0);\r\n\r\n            fprintf(fp, \"  \\\"%p\\\":%s -> \\\"%p\\\":%s { arrowhead = %s; style = %s; label = \\\"x\\\"; }\\n\",\r\n                    parent0 ? (void *) parent0 : (void *) node->src0,\r\n                    parent0 ? \"g\" : \"x\",\r\n                    parent ? (void *) parent : (void *) node,\r\n                    parent ? \"g\" : \"x\",\r\n                    parent ? \"empty\" : \"vee\",\r\n                    parent ? \"dashed\" : \"solid\");\r\n        }\r\n\r\n        if (node->src1) {\r\n            struct ggml_tensor * parent1 = ggml_graph_get_parent(gb, node->src1);\r\n\r\n            fprintf(fp, \"  \\\"%p\\\":%s -> \\\"%p\\\":%s { arrowhead = %s; style = %s; label = \\\"y\\\"; }\\n\",\r\n                    parent1 ? (void *) parent1 : (void *) node->src1,\r\n                    parent1 ? \"g\" : \"x\",\r\n                    parent ? (void *) parent : (void *) node,\r\n                    parent ? \"g\" : \"x\",\r\n                    parent ? \"empty\" : \"vee\",\r\n                    parent ? \"dashed\" : \"solid\");\r\n        }\r\n    }\r\n\r\n    for (int i = 0; i < gb->n_leafs; i++) {\r\n        struct ggml_tensor * node = gb->leafs[i];\r\n\r\n        if (node->src0) {\r\n            fprintf(fp, \"  \\\"%p\\\":%s -> \\\"%p\\\":%s [ label = \\\"x\\\"; ]\\n\",\r\n                    (void *) node->src0, \"x\",\r\n                    (void *) node, \"x\");\r\n        }\r\n\r\n        if (node->src1) {\r\n            fprintf(fp, \"  \\\"%p\\\":%s -> \\\"%p\\\":%s [ label = \\\"y\\\"; ]\\n\",\r\n                    (void *) node->src1, \"x\",\r\n                    (void *) node, \"x\");\r\n        }\r\n    }\r\n\r\n    fprintf(fp, \"}\\n\");\r\n\r\n    fclose(fp);\r\n\r\n    GGML_PRINT(\"%s: dot -Tpng %s -o %s.png && open %s.png\\n\", __func__, filename, filename, filename);\r\n}\r\n```\r\nTo replace the '[' and `]` to '{' and '}'\r\nThen it doesn't have new error , but the process blocked, can not stop.Blocked in here:\r\n![image](https://user-images.githubusercontent.com/58033505/228439062-708c8eb3-40ea-4f2c-bfe6-4a41d87ce689.png)\r\n\r\nAnd I also run command in windows powershell, but it still blocked like this:\r\n![image](https://user-images.githubusercontent.com/58033505/228439280-999e7a9b-1c34-4a08-9f43-ecb2171064ec.png)\r\n\r\nsame in wsl:\r\n\r\n![image](https://user-images.githubusercontent.com/58033505/228439326-093c1ac5-bd8d-40fd-9afc-eadd7b8b41ee.png)\r\n\r\nSo can anybody know what can I do? Thanks\r\n\r\nAttach is the .dot file generated by me, you can modify the suffix from `gpt-2.txt` to `gpt-2.dot`. The model size is 7B-version.\r\n\r\n[gpt-2.txt](https://github.com/ggerganov/llama.cpp/files/11097020/gpt-2.txt)\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-29T05:57:35+00:00",
    "closed_at": "2023-03-29T06:38:43+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/589/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/589"
  },
  {
    "number": 588,
    "title": "Update the convert-unversioned-ggml-to-ggml.py script to support GPT4All ggml models",
    "body": "See: https://twitter.com/ggerganov/status/1640945226662420483\r\n\r\nThe gpt4all ggml model has an extra `<pad>` token (i.e. `n_vocab = 32001`).\r\nNeed to add it during the conversion. Should be an optional command line argument to the script to specify if the token should be added or not",
    "labels": [
      "help wanted",
      "good first issue",
      "high priority",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-29T05:21:04+00:00",
    "closed_at": "2023-03-29T16:37:21+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/588/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/588"
  },
  {
    "number": 587,
    "title": "User should be able to return control without inserting a newline",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Current Behavior\r\n\r\nIn Interactive mode, if the user presses return, a newline is entered into the input. This makes it impossible to return control without inserting a newline.\r\n\r\nA special-case exception was recently added for when the user enters only a newline and no other text.  #529 #571 This returns control without inserting a newline.  However, this only works for the special case of zero length input.\r\n\r\n# Expected Behavior\r\n\r\nThe newline entered by the user to return control should never be included in the text.  If the user wants the text to end in a newline, this should be accomplished by explicitly adding a newline by using \\\\ followed by return, then returning control by pressing return again.\r\n\r\nThis is what would be desired in Interactive mode.  In Instruct mode, perhaps the desired behavior would the current behavior of including the final newlines.",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-29T04:40:03+00:00",
    "closed_at": "2024-04-12T01:07:23+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/587/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/587"
  },
  {
    "number": 585,
    "title": "[Build] get warming about gcc extension",
    "body": "# Prerequisites\n\nPlease answer the following questions for yourself before submitting an issue.\n\n- [*] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\n- [*] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [*] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\n- [*] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\n\n# Expected Behavior\n\n\n# Current Behavior\n\nWarming in build ggml.c\n\n1965:72: warming: binary contacts a C2X feature or GCC extension\nConst __m256 cross_scales = _mm256_blend_ps(scale_0, scale_1, 0b10101010)\n\n# Environment and Context \n\nAll was fine with 26 hours away versions\n\n* Physical (or virtual) hardware you are using, e.g. for Linux:\n\nAmd64\n\n* Operating System, e.g. for Linux:\n\n5.19 Linux Ubuntu base \n\n* SDK version, e.g. for Linux:\n\n```\n\n$ make 4.3\n$ g++ 11.3.0\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-28T23:50:08+00:00",
    "closed_at": "2023-03-29T13:20:09+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/585/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/585"
  },
  {
    "number": 582,
    "title": "Fix failing CI test using thread sanitizer",
    "body": "I cannot reproduce on my machines:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/actions/runs/4545676297/jobs/8013336777\r\n\r\nIf someone that can reproduce, please try to fix this",
    "labels": [
      "help wanted",
      "high priority",
      "testing"
    ],
    "state": "closed",
    "created_at": "2023-03-28T17:16:53+00:00",
    "closed_at": "2023-04-02T07:18:54+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/582/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/582"
  },
  {
    "number": 578,
    "title": "|BUG] ggml spawns threads even BLAS is used",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\nggml should not spawn threads for the initial prompt ingestion when using BLAS.\r\n\r\n# Current Behavior\r\nggml does spawn threads even when using BLAS.\r\n\r\n# Environment and Context \r\nReproducible using latest OpenBLAS with PR https://github.com/xianyi/OpenBLAS/pull/3970 (for Intel 13th gen support) and Intel MKL's BLAS implementation.\r\n\r\n```bash\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         46 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  20\r\n  On-line CPU(s) list:   0-19\r\nVendor ID:               GenuineIntel\r\n  Model name:            13th Gen Intel(R) Core(TM) i5-13500\r\n    CPU family:          6\r\n    Model:               191\r\n    Thread(s) per core:  2\r\n    Core(s) per socket:  14\r\n    Socket(s):           1\r\n    Stepping:            2\r\n    CPU max MHz:         4800.0000\r\n    CPU min MHz:         800.0000\r\n    BogoMIPS:            4992.00\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx \r\n                         fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bt\r\n                         s rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 mo\r\n                         nitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe po\r\n                         pcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb i\r\n                         nvpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad\r\n                          fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_p\r\n                         t sha_ni xsaveopt xsavec xgetbv1 xsaves avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_a\r\n                         ct_window hwp_epp hwp_pkg_req hfi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdi\r\n                         ri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities\r\n```\r\n* Operating System, e.g. for Linux:\r\nUbuntu 22.04 with custom Kernel\r\n`Linux XXX 6.1.6-060106-generic #202301141035 SMP PREEMPT_DYNAMIC Sat Jan 14 11:15:19 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux`\r\n\r\n# Failure Information (for bugs)\r\nRead this discussion for full context https://github.com/ggerganov/llama.cpp/discussions/229#discussioncomment-5454503\r\n@slaren mentioned that the issue is:\r\n> By default llama.cpp will limit ggml to 1 thread when using BLAS only if the batch size is >255:\r\n> \r\n> https://github.com/ggerganov/llama.cpp/blob/4b8efff0e3945090379aa2f897ff125c8f9cdbae/llama.cpp#L859\r\n> \r\n> \r\n> The problem is that there is a mismatch in ggml which will use BLAS as long as the batch size is >= 32:\r\n> https://github.com/ggerganov/llama.cpp/blob/4b8efff0e3945090379aa2f897ff125c8f9cdbae/ggml.c#L5784\r\n> \r\n> This leads to issues when the batch size is >32 and <=255. We need to determine what is the optimal batch size to start using BLAS, and use that value consistently.\r\n\r\n# Steps to Reproduce\r\nI tried using -b 256 and -b 512, and ggml's 6 threads (from -t 6) are still spawned by ggml (alongside BLAS threads) when doing initial prompt ingestion:\r\n```bash\r\nllama -m /opt/models/llama-30B/ggml-model-q4_0.bin -n -1 --color -i -r \"User:\" -f /opt/prompts/chat-with-bob.txt -t 6 -b 256 -c 2048\r\n```\r\nUsing `-t 1` yields the expected behavior (only 1 thread for ggml, and the threads I set in env variable for BLAS)\r\n\r\n# Failure Logs\r\nhtop shows more core usages than expected.",
    "labels": [
      "bug",
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-28T15:02:01+00:00",
    "closed_at": "2024-04-12T01:07:25+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/578/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/578"
  },
  {
    "number": 573,
    "title": "--help may show the wrong default values when used after other arguments",
    "body": "For example, running `./main -b 512 --help` will show the help and say that 512 is the default batch size, which is wrong. This may lead to confusion.",
    "labels": [
      "bug",
      "documentation"
    ],
    "state": "closed",
    "created_at": "2023-03-28T13:26:10+00:00",
    "closed_at": "2023-04-02T02:41:14+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/573/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/573"
  },
  {
    "number": 566,
    "title": "Missing model data",
    "body": "# Prerequisites\r\nwhen I clone the project into my centos, I enter the llama.cpp and run the command of \"make\". It works but didn't make successfully.\r\n\r\n# Expected Behavior\r\nthe info after the command \"make\" was running:\r\n```\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mavx512f -mavx512bw -mavx512dq -mavx512vl -mavx512cd\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\r\nI LDFLAGS:  \r\nI CC:       cc (GCC) 7.3.1 20180303 (Red Hat 7.3.1-5)\r\nI CXX:      g++ (GCC) 7.3.1 20180303 (Red Hat 7.3.1-5)\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mavx512f -mavx512bw -mavx512dq -mavx512vl -mavx512cd   -c ggml.c -o ggml.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread -c llama.cpp -o llama.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread -c examples/common.cpp -o common.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread examples/main/main.cpp ggml.o llama.o common.o -o main \r\n\r\n====  Run ./main -h for help.  ====\r\n\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread examples/quantize/quantize.cpp ggml.o llama.o -o quantize \r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread examples/perplexity/perplexity.cpp ggml.o llama.o common.o -o perplexity \r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread examples/embedding/embedding.cpp ggml.o llama.o common.o -o embedding \r\n```\r\n\r\nAfter this I run ```ls ./models```,it only shows info:\r\n```ggml-vocab.bin```\r\nI can't find the result of ```65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model```\r\nwhat can I do?\r\nI would appreciate it very much if you could solve my problem\u3002\r\n\r\n",
    "labels": [
      "need more info",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-28T08:58:20+00:00",
    "closed_at": "2023-03-30T23:24:25+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/566/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/566"
  },
  {
    "number": 565,
    "title": "[Feature Request] Simplified API for Inference and HTTP Server Integration",
    "body": "First I want to express my deep gratitude for this project, thank you guys so much!\r\n\r\nI'm writing to inquire about potential improvements to the API for inference, as well as the possibility of integrating an HTTP server for serving text generation requests. Specifically, I'm interested in the following:\r\n\r\n1. A simplified and more flexible method for inference that allows for easier integration with external applications. I'm looking to manage chat history in a separate application and would like to have a straightforward way to perform inference on user-provided text.\r\n\r\n2. The ability to serve text generation requests over HTTP. I'm interested in implementing a client-server architecture and would like to know if there are plans to include an HTTP server in the repository.\r\n\r\nI understand that the repository is rapidly evolving, and I'm excited to see the new features and improvements you have planned. I'm planning to hack an http server together by myself, but I want to find out what your roadmap is so there is not any painful merges for me in the future. I'm open to contributing to the project if there's an opportunity for collaboration.\r\n\r\nCould you please share any insights into the roadmap for these features, or let me know if there are any ongoing discussions or pull requests related to them?\r\n",
    "labels": [
      "duplicate",
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-28T08:42:38+00:00",
    "closed_at": "2023-03-28T11:43:00+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/565/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/565"
  },
  {
    "number": 561,
    "title": "Broken seed?",
    "body": "Hello,\r\n\r\nIs it me or the seed doesn't work anymore? No matter which seed I choose I always get the same output now.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-27T19:16:06+00:00",
    "closed_at": "2023-03-27T19:27:48+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/561/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/561"
  },
  {
    "number": 560,
    "title": "[Info] We built a mobile interactive version using flutter and it runs super well on a simple Oneplus 7 with 8GB ram.",
    "body": "# Have fun\r\n\r\nHi, it is not a real issue, but i want to share here that we made a running app with interactive mode on mobiles using your repo.\r\nYou can find our repo here : [https://github.com/Bip-Rep/sherpa](https://github.com/Bip-Rep/sherpa)\r\n\r\nUnfortunately it doesnt have your latest commit because there is an error during runtime but we made a fork here [https://github.com/Bip-Rep/llama.cpp](https://github.com/Bip-Rep/llama.cpp) You need to be on the \"for_mobile\" branch to build the libraries. We are using an older working commit on this branch.\r\n\r\nWe translated the main functions of llama.cpp in dart.\r\n\r\n\r\n## Working demo\r\n[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/jdw7oABjTeQ/0.jpg)](https://www.youtube.com/watch?v=jdw7oABjTeQ)\r\nClick on the image to view the video on YouTube.\r\n\r\nHope this helps. \r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-27T18:46:16+00:00",
    "closed_at": "2023-03-28T10:30:46+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/560/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/560"
  },
  {
    "number": 558,
    "title": "Has anyone tried Dolly-like models?",
    "body": "I just watched the latest video of my favorite youtuber - https://www.youtube.com/watch?v=AWAo4iyNWGc&t=14s and was wondering, if someone has already quantized & converted one of these to be compatible with llama.cpp?\r\nThe beauty of Dolly-like models is that they're based on open source [gpt-j-6B from EleutherAI](https://huggingface.co/EleutherAI/gpt-j-6B), so noone will be hunting us for using them without an ask.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-27T16:55:59+00:00",
    "closed_at": "2023-03-28T10:33:39+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/558/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/558"
  },
  {
    "number": 557,
    "title": "Perplexity test stopping before the supposed end.",
    "body": "# Expected Behavior\r\n\r\nHello,\r\nI'm testing the perplexity of the alpaca-7b-native-q4.bin (RTN quantized) to compare with the regular Llama model.\r\n\r\n# Current Behavior\r\n\r\nThe problem is that the test stops before the supposed end (655 chunks)\r\n\r\n```\r\nllama_model_load: loading model part 1/1 from '.\\models\\alpaca-7b-native-q4.bin'\r\nllama_model_load: .................................... done\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\nllama_init_from_file: kv self size  =  256.00 MB\r\n\r\nsystem_info: n_threads = 4 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\r\nperplexity : calculating perplexity over 655 chunks\r\n81.02 seconds per pass - ETA 14.74 hours\r\n[1]6.2081,[2]6.7263,[3]7.8961,[4]9.2302,[5]9.1509,[6]8.9913,[7]9.2372,[8]9.3097,[9]9.8923,[10]10.2558,[11]10.5684,[12]10.5985,[13]10.4832,[14]10.5467,[15]10.9168,[16]10.2984,[17]10.1309,[18]10.0642,[19]9.4963,[20]9.4939,[21]9.3365,[22]9.1209,[23]9.0484,[24]8.8940,[25]8.9035,[26]8.6184,[27]8.3221,[28]8.1799,[29]8.0562,[30]7.8244,[31]7.7875,[32]7.7878,[33]7.6972,[34]7.7567,[35]7.7854,[36]7.8394,[37]7.8226,[38]7.8295,[39]7.8572,[40]7.9186,[41]7.9172,[42]7.9717,[43]7.9123,[44]7.9682,[45]7.9914,[46]7.9538,[47]7.9732,[48]7.9411,[49]7.9520,[50]7.8865,[51]7.8836,[52]7.8660,[53]7.9198,[54]7.9106,[55]7.8656,[56]7.9349,[57]7.9745,[58]7.9903,[59]8.0054,[60]8.0540,[61]8.0338,[62]8.1030,[63]8.1388,[64]8.1514,[65]8.2030,[66]8.2131,[67]8.2209,[68]8.2362,[69]8.2556,[70]8.2963,[71]8.3189,[72]8.3431,[73]8.4459,[74]8.4512,[75]8.4628,[76]8.4826,[77]8.4896,[78]8.4654,[79]8.5027,[80]8.4925,[81]8.5094,[82]8.5214,[83]8.4494,[84]8.4313,[85]8.4133,[86]8.3851,[87]8.2883,[88]8.2558,[89]8.2337,[90]8.2149,[91]8.2516,[92]8.2356,Press Enter to continue...:\r\n```\r\n\r\nFor this run it stoped at 92 chunks but I tried it before and sometimes it would stop at 10-20... \r\n\r\n# Environment and Context \r\n- Windows 10\r\n- Using perplexity.exe (made with Clung)\r\n\r\n",
    "labels": [
      "need more info",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-27T15:49:15+00:00",
    "closed_at": "2024-04-12T01:07:27+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/557/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/557"
  },
  {
    "number": 554,
    "title": "Windows defender finds a virus in current master branch",
    "body": "I'm using Windows 10 LTSC\r\nAt the moment, on the state of [this commit](https://github.com/ggerganov/llama.cpp/commit/7e5395575a3360598f2565c73c8a2ec0c0abbdb8), windows defender finds a virus in the master branch.\r\n\r\n![image](https://user-images.githubusercontent.com/33938415/227919494-c34cbb4d-32c3-4873-b094-98510ea36abc.png)\r\n",
    "labels": [
      "wontfix"
    ],
    "state": "closed",
    "created_at": "2023-03-27T10:43:09+00:00",
    "closed_at": "2023-03-28T09:13:13+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/554/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 2,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/554"
  },
  {
    "number": 552,
    "title": "Cannot build in CentOS",
    "body": "Hello, can somebody help me build llama.cpp on CentOS? Or maybe provide binaries?\r\n\r\n```bash\r\n[root@vmd89384 llama.cpp]# make\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -mfma -mf16c -msse3\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\r\nI LDFLAGS:  \r\nI CC:       cc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)\r\nI CXX:      g++ (GCC) 4.9.2\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -mfma -mf16c -msse3   -c ggml.c -o ggml.o\r\nggml.c:77:23: fatal error: stdatomic.h: No such file or directory\r\n #include <stdatomic.h>\r\n                       ^\r\ncompilation terminated.\r\nmake: *** [ggml.o] Error 1\r\n[root@vmd89384 llama.cpp]# \r\n```\r\n\r\nI use latest code of this repo, gcc version is 4.9.2\r\n\r\nuname is `x86_64 x86_64 x86_64 GNU/Linux`",
    "labels": [
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-27T08:36:21+00:00",
    "closed_at": "2023-03-29T06:56:13+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/552/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/552"
  },
  {
    "number": 549,
    "title": "Support for In context learning",
    "body": "Hello,\r\n\r\nI would like to know if this port in cpp of llama does support in context learning. \r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-27T07:22:23+00:00",
    "closed_at": "2023-03-27T07:42:24+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/549/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/549"
  },
  {
    "number": 543,
    "title": "unknown tensor '' in model file",
    "body": "![image](https://user-images.githubusercontent.com/89653506/227813653-b2657cf7-8c98-4e91-89b4-d0304c37fced.png)\r\n![image](https://user-images.githubusercontent.com/89653506/227813663-7b002b7d-d394-45a4-9b87-c736a6915a49.png)\r\n![image](https://user-images.githubusercontent.com/89653506/227813677-c8cc3de7-a155-4201-95f0-297100d3d1b1.png)\r\n\r\nSo as to prevent mass spam of very large segments of code that i dont think are necessarily useful - i am wondering what i might be doing wrong here - went ahead and went through the entire process start to finish twice just to try and check against maybe a random typo somewhere - but same result both times. \r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-27T00:11:51+00:00",
    "closed_at": "2023-03-27T00:30:47+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/543/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/543"
  },
  {
    "number": 538,
    "title": "Error loading llama 65b 4bit model (HFv2) converted from .pt format",
    "body": "I used this command to get the converted model:\r\n\r\n`python3 convert-gptq-to-ggml.py \"path/to/llama-65b-4bit.pt\" \"path/to/tokenizer.model\" \"./models/ggml-llama-65b-q4_0.bin\"`\r\n\r\nI run it with this command:\r\n\r\n`./main -m ./models/ggml-llama-65b-q4_0.bin -n 128`\r\n\r\nAnd this is what I get at the end of the output:\r\n\r\n```\r\nllama_model_load: loading model part 1/8 from './models/ggml-llama-65b-q4_0.bin'\r\nllama_model_load: llama_model_load: tensor 'tok_embeddings.weight' has wrong size in model file\r\nllama_init_from_file: failed to load model\r\nmain: error: failed to load model './models/ggml-llama-65b-q4_0.bin'\r\n```\r\n\r\nP. S. Yes, I'm using the latest (or at least today's) version of this repo. While I'm at it, many thanks to ggerganov and everyone else involved! Great job.",
    "labels": [
      "invalid"
    ],
    "state": "closed",
    "created_at": "2023-03-26T20:26:54+00:00",
    "closed_at": "2023-03-27T05:22:28+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/538/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/538"
  },
  {
    "number": 537,
    "title": "Docker Issus ''Illegal instruction''",
    "body": "I try to make it run the docker version on Unraid, \r\n\r\nI run this as post Arguments:\r\n`--run -m /models/7B/ggml-model-q4_0.bin -p \"This is a test\" -n 512`\r\n\r\nI got this error:  `/app/.devops/tools.sh: line 40:     7 Illegal instruction     ./main $arg2`\r\n\r\nLog:\r\n```\r\nmain: seed = 1679843913\r\nllama_model_load: loading model from '/models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\nllama_model_load: ggml ctx size = 4273.34 MB\r\nllama_model_load: mem required  = 6065.34 MB (+ 1026.00 MB per state)\r\n/app/.devops/tools.sh: line 40:     7 Illegal instruction     ./main $arg2\r\n```\r\n\r\nI have run this whitout any issus:  `--all-in-one \"/models/\" 7B` ",
    "labels": [
      "bug",
      "hardware",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-26T19:18:11+00:00",
    "closed_at": "2024-04-12T01:07:28+00:00",
    "comments": 24,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/537/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/537"
  },
  {
    "number": 536,
    "title": "Logo in Social Preview",
    "body": "Not a bug, but a useful thing. Put the logo in the Social Preview like in this project:\r\n<img width=\"843\" alt=\"Screenshot 2023-03-26 at 20 01 58\" src=\"https://user-images.githubusercontent.com/163333/227795065-61d531a9-e515-44bf-b570-086ea8aa7bf2.png\">\r\n\r\n It will be then showed as a preview image on Twitter etc.\r\n<img width=\"591\" alt=\"Screenshot 2023-03-26 at 20 02 52\" src=\"https://user-images.githubusercontent.com/163333/227795109-d2f84554-1f08-4d82-8b3f-11be2d4de1ef.png\">\r\n\r\n\r\n",
    "labels": [
      "enhancement",
      "\ud83e\udd99."
    ],
    "state": "closed",
    "created_at": "2023-03-26T18:03:49+00:00",
    "closed_at": "2023-03-28T18:34:37+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/536/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/536"
  },
  {
    "number": 534,
    "title": "Build your windows binaries with Clang and not MSVC.",
    "body": "Hello,\r\n\r\nYour [windows binaries releases](https://github.com/ggerganov/llama.cpp/releases) have probably been built with MSVC and I think there's a better way to do it.\r\n\r\n# Expected Behavior\r\n\r\nI have a Intel\u00ae Core\u2122 i7-10700K and the builds are supposed to recognize those architectures: [AVX | AVX2 | FMA | SSE3 | F16C]\r\n\r\n# Current Behavior\r\n\r\nWindows (MSVC build)\r\n```\r\nsystem_info: n_threads = 14 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 0 | \r\nNEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | VSX = 0 |\r\n```\r\nIt misses the FMA, SSE3 and the F16C architectures.\r\n\r\n# Fix with Clang\r\n\r\nIf you build with Clang you'll get all the architectures right:\r\n\r\nWindows (Clang build)\r\n```\r\nsystem_info: n_threads = 14 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | \r\nNEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\r\n```\r\n\r\n# How to build with Clang\r\n\r\n1. Install Clang\r\nTo do this, you have to install something called [LLVM](https://github.com/llvm/llvm-project/releases/tag/llvmorg-16.0.0) \r\nI downloaded LLVM-16.0.0-win64.exe but it could be LLVM-16.0.0-win32.exe for people that has Windows 32 bit.\r\nDuring the install, Choose \"Add LLVM to the system PATH for all users\"\r\n\r\nAfter the installation you can verify if you have clang installed on your computer by running ```clang --version``` on your cmd\r\nNormally you'll get something like this\r\n```\r\nC:\\Users\\Utilisateur>clang --version\r\nclang version 16.0.0\r\nTarget: x86_64-pc-windows-msvc\r\nThread model: posix\r\nInstalledDir: C:\\Program Files\\LLVM\\bin\r\n```\r\n\r\n2. Install [chocolate](https://chocolatey.org/install#individual) (that's necessary to install Ninja (which is necessary to build with clang \ud83d\ude35 ))\r\nTo do that, open powershell as an administrator and run this command:\r\n```\r\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\r\n```\r\n\r\n3. Install Ninja\r\nOpen powershell as an administrator and run this command:\r\n```\r\nchoco install ninja\r\n```\r\n\r\n4. Build with clang\r\nrun your cmd on the llama.cpp repository and run those commands\r\n```\r\ncmake -G \"Ninja\" -S . -B Windows-build/ -DCMAKE_BUILD_TYPE=Release -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++\r\ncmake --build Windows-build/ --config Release\r\n```\r\n\r\nYou'll get your exe here:\r\n```\r\n.\\Windows-build\\bin\r\n```\r\n\r\n",
    "labels": [
      "enhancement",
      "build",
      "windows",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-26T17:12:25+00:00",
    "closed_at": "2024-04-12T01:07:30+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/534/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 2,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/534"
  },
  {
    "number": 533,
    "title": "Is it possible to avoid printing input when using Alpaca models and prompt from file?",
    "body": "I want to use prompt from file using `-f` options and alpaca models. Nevertheless, when I use like that, the llama.cpp first prints out the whole input. How to avoid printing out whole input ?",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-26T16:26:02+00:00",
    "closed_at": "2024-04-12T01:07:32+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/533/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/533"
  },
  {
    "number": 532,
    "title": "[Feature Suggestion] Load/Save current conversation's tokens into file",
    "body": "Now that we have infinite transcription mode. Would it be possible to dump tokens into file and load them back next time you run llama.cpp to resume conversation?\r\n\r\nAlthough it will be tricky to implement efficiently with long conversations, for example by\r\n- storing prompt itself as tokens\r\n- store in-between messages as raw text\r\n- store last messages within ctx_size as tokens\r\n\r\n",
    "labels": [
      "duplicate",
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-26T15:49:50+00:00",
    "closed_at": "2024-04-12T01:07:34+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/532/reactions",
      "total_count": 7,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/532"
  },
  {
    "number": 530,
    "title": "Infinity transcript mode may stuck in ram?",
    "body": "After ctx > 2048 or whatever set in -c, While close the terminal, the transcript may have a chance continuously running in system.\n\nLinux amd64 5.19 ubuntu base.",
    "labels": [
      "need more info",
      "generation quality"
    ],
    "state": "closed",
    "created_at": "2023-03-26T15:00:39+00:00",
    "closed_at": "2023-03-27T00:59:20+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/530/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/530"
  },
  {
    "number": 528,
    "title": "add support for llama adapters",
    "body": "implement support for running models that use Llama adapter\r\nhttps://github.com/ZrrSkywalker/LLaMA-Adapter\r\n\r\n\r\ndescribed here how to get the model\r\n\r\nhttps://github.com/ZrrSkywalker/LLaMA-Adapter#inference",
    "labels": [
      "enhancement",
      "model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-26T14:28:49+00:00",
    "closed_at": "2024-04-12T01:07:36+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/528/reactions",
      "total_count": 5,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 5,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/528"
  },
  {
    "number": 522,
    "title": "\"Not enough context memory\" on raspberry pi",
    "body": "I tried loading the 7B model on a raspberry pi 4 (8GB) and it said there was not enough memory in the context pool, before segfaulting. On the raspberry pi, I am accessing the model from a hard disk on a seperate laptop, shared over sshfs, but on the laptop itself (x86 linux, 4GB RAM + 10GB swap) I access it over USB and it works perfectly fine. I am using the one before the latest versions of llama.cpp on both systems, but i changed one line of code in the raspberry pi's version. I replaced `#include <immintrin.h>` with `#include <arm_neon.h>` in ggml.c\r\n\r\nWhile I was writing this I updated the raspberry pi's version of the software to the latest version and the bug still occurs. I will only disply the log from the previous version though\r\n\r\nLog:\r\n```\r\npi@raspberrypi:~/clones/llama.cpp $ ./main -m /path/to/network/drive/LLaMA/7B/ggml-model-q4_0.bin.tmp -i --color -t 3\r\nmain: seed = 1679831014\r\nllama_model_load: loading model from '/path/to/network/drive/LLaMA/7B/ggml-model-q4_0.bin.tmp' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: ggml ctx size =   0.08 MB\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 81920140, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 81936664, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10518948, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10518948, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10518948, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10518948, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28230192, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28230192, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28230192, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10551996, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10551996, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10551996, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10551996, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 10568520, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 99144, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 28263240, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 268518216, available 83199)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 268518216, available 83199)\r\nSegmentation fault\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-26T12:10:06+00:00",
    "closed_at": "2023-03-26T16:26:45+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/522/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/522"
  },
  {
    "number": 519,
    "title": "Visual studio build fails with 'undeclared identifier'",
    "body": "I am getting the following errors when building from visual studio 2022. Any ideas why?\r\n\r\n\r\n```\r\nllama.cpp\\ggml.c(5838,5): error C2065: 'ne12': undeclared identifier\r\nllama.cpp\\ggml.c(5839,5): error C2065: 'ne13': undeclared identifier\r\nllama.cpp\\ggml.c(5840,5): error C2065: 'ne2': undeclared identifier\r\nllama.cpp\\ggml.c(5840,5): error C2065: 'ne12': undeclared identifier\r\nllama.cpp\\ggml.c(5841,5): error C2065: 'ne3': undeclared identifier\r\nllama.cpp\\ggml.c(5841,5): error C2065: 'ne13': undeclared identifier\r\nllama.cpp\\ggml.c(5844,5): error C2065: 'nb00': undeclared identifier\r\nllama.cpp\\ggml.c(5852,5): error C2065: 'ne0': undeclared identifier\r\nllama.cpp\\ggml.c(5853,5): error C2065: 'ne1': undeclared identifier\r\nllama.cpp\\ggml.c(5854,5): error C2065: 'ne2': undeclared identifier\r\nllama.cpp\\ggml.c(5855,5): error C2065: 'ne3': undeclared identifier\r\n```\r\n\r\nGenerated the solution using these commands.\r\n\r\n```\r\ncmake -S . -B build/ -D CMAKE_BUILD_TYPE=Release\r\ncmake --build build/ --config Release\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-26T09:17:55+00:00",
    "closed_at": "2023-03-26T18:43:48+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/519/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/519"
  },
  {
    "number": 518,
    "title": "Help populating the examples README.md files",
    "body": "For now I just added empty README.md files:\r\n\r\n- https://github.com/ggerganov/llama.cpp/tree/master/examples/main\r\n- https://github.com/ggerganov/llama.cpp/tree/master/examples/quantize\r\n- https://github.com/ggerganov/llama.cpp/tree/master/examples/perplexity\r\n- https://github.com/ggerganov/llama.cpp/tree/master/examples/embedding\r\n- etc.\r\n\r\nIt would be great to add usage instructions and various tips and tricks for better experience for each example.\r\n\r\nGreat task for initial contributions",
    "labels": [
      "documentation",
      "help wanted",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2023-03-26T07:25:05+00:00",
    "closed_at": "2023-07-28T19:21:19+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/518/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 3,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/518"
  },
  {
    "number": 513,
    "title": "Fails to run inside Docker from Ubuntu 22.04",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ :white_check_mark: ] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [:white_check_mark:  ] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ :white_check_mark: ] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ :white_check_mark: ] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nExpecting it not to fail when running via Docker\r\n\r\n# Current Behavior\r\n\r\nFails when running via Docker\r\n\r\nPlease provide a detailed written description of what `llama.cpp` did, instead. \r\n\r\nCommand:\r\n\r\n`docker run -v ./weights:/app/weights llama:latest llama -m ./weights/ggml-alpaca-7B-q4_0.bin -p \"hello, how are you?\"`\r\n\r\nOutput (no error :thinking:)\r\n\r\n```\r\nmain: seed = 1679783577\r\nllama_model_load: loading model from './weights/ggml-alpaca-7B-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type    = 1\r\n```\r\n\r\n# Environment and Context \r\n\r\nTried 3 different machines:\r\n\r\n1) Local Intel CPU and 64gb RAM running Ubuntu 22.04. Runs fine without Docker - Inside Docker the above error\r\n2) DigitalOcean Droplet - AMD CPU 4 Core and 8GB Ram running Ubuntu 22.04. Runs fine without Docker - Inside Docker the above error\r\n3) Contabo - Intel CPU 8GB Ram running Ubuntu 22.04. Runs fine without Docker - Inside Docker the above error\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n`Architecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         39 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  4\r\n  On-line CPU(s) list:   0-3\r\nVendor ID:               GenuineIntel\r\n  Model name:            Intel(R) Core(TM) i5-6500 CPU @ 3.20GHz\r\n    CPU family:          6\r\n    Model:               94\r\n    Thread(s) per core:  1\r\n    Core(s) per socket:  4\r\n    Socket(s):           1\r\n    Stepping:            3\r\n    CPU max MHz:         3600,0000\r\n    CPU min MHz:         800,0000\r\n    BogoMIPS:            6399.96\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mc\r\n                         a cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss \r\n                         ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art\r\n                          arch_perfmon pebs bts rep_good nopl xtopology nonstop_\r\n                         tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cp\r\n                         l vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid ss\r\n                         e4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes \r\n                         xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_f\r\n                         ault epb invpcid_single pti ssbd ibrs ibpb stibp tpr_sh\r\n                         adow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adj\r\n                         ust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx sma\r\n                         p clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dt\r\n                         herm ida arat pln pts hwp hwp_notify hwp_act_window hwp\r\n                         _epp md_clear flush_l1d arch_capabilities\r\nVirtualization features: \r\n  Virtualization:        VT-x\r\nCaches (sum of all):     \r\n  L1d:                   128 KiB (4 instances)\r\n  L1i:                   128 KiB (4 instances)\r\n  L2:                    1 MiB (4 instances)\r\n  L3:                    6 MiB (1 instance)\r\nNUMA:                    \r\n  NUMA node(s):          1\r\n  NUMA node0 CPU(s):     0-3\r\nVulnerabilities:         \r\n  Itlb multihit:         KVM: Mitigation: VMX disabled\r\n  L1tf:                  Mitigation; PTE Inversion; VMX conditional cache flushe\r\n                         s, SMT disabled\r\n  Mds:                   Mitigation; Clear CPU buffers; SMT disabled\r\n  Meltdown:              Mitigation; PTI\r\n  Mmio stale data:       Mitigation; Clear CPU buffers; SMT disabled\r\n  Retbleed:              Mitigation; IBRS\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer\r\n                          sanitization\r\n  Spectre v2:            Mitigation; IBRS, IBPB conditional, RSB filling, PBRSB-\r\n                         eIBRS Not affected\r\n  Srbds:                 Mitigation; Microcode\r\n  Tsx async abort:       Mitigation; TSX disabled\r\n`\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`Linux tg 5.19.0-35-generic #36~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Feb 17 15:17:25 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\r\n`\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\nEverything's from this repository's Docker image\r\n\r\n# Steps to Reproduce\r\n\r\nJust simple docker run from Ubuntu to Docker Ubuntu\r\n\r\n`docker run -v ./weights:/app/weights llama:latest llama -m ./weights/ggml-alpaca-7B-q4_0.bin -p \"hello, how are you?\"`\r\n",
    "labels": [
      "bug",
      "build",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-25T22:42:37+00:00",
    "closed_at": "2024-04-12T01:07:38+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/513/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/513"
  },
  {
    "number": 508,
    "title": "Create \"instruct\" example",
    "body": "Currently, the `main` example has a `instruct` parameter which enables something similar to instruction-based mode. I haven't understood it completely, but this seems to be what the Alpaca models are created for.\r\n\r\nSince we now support infinite generation (https://github.com/ggerganov/llama.cpp/issues/71#issuecomment-1483907574) it would be very useful to make a separate app that utilizes the new `--keep` argument to create a question-answering bot that never stops. The tricky part is to keep the correct instruction prompt and \"inject\" the few-shot examples correctly, or whatever.\r\n\r\nThe main logic for context swapping / context rotation is here:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/blob/c2b25b6912662d2637d9c6e6df3a5de931e0d7ce/examples/main/main.cpp#L297-L324\r\n\r\nUncomment the `printf` to help debug. Something similar will be needed in the new `instruct` example.\r\n\r\nImplementing this task will also help simplify the `main` example as it will no longer need to support the `--instruct` argument.",
    "labels": [
      "enhancement",
      "help wanted",
      "good first issue",
      "\ud83e\udd99."
    ],
    "state": "closed",
    "created_at": "2023-03-25T20:22:39+00:00",
    "closed_at": "2023-07-28T19:21:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/508/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 2,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/508"
  },
  {
    "number": 507,
    "title": "Comparison of Windows Build VS Unix Build (through WSL2)",
    "body": "# Environment and Context \r\nHello, \r\nBefore jumping to the subject, here's the environnement I'm working with:\r\n\r\n- Windows 10\r\n- Llama-13b-4bit-(GPTQ quantized) model\r\n- Intel\u00ae Core\u2122 i7-10700K [AVX | AVX2 | FMA | SSE3 | F16C]\r\n\r\n# Expected Behavior\r\n\r\nI did some comparaisons between the Windows build and the Unix build (through WSL2 Ubuntu_2204.1.8.0_x64) to see if I can notice some differences between them.\r\n\r\n# Deterministic Settings (seed =1)\r\nFor both of those builds, I added the same exact settings:\r\n```\r\n-t 14 -n 2024 -c 2024 --temp 0.2 --top_k 40 --top_p 0.6 --repeat_last_n 2048 \r\n--repeat_penalty 1.17647058824 --color --n_parts 1 -b 500 --seed 1 -p \"$(cat STORY.txt)\"\r\n```\r\n\r\nWith the contents of STORY.txt as follows:\r\n```\r\nHere's 5 reasons that proves why video-games are good for your brain:\r\n```\r\n\r\n#  Test#1: Instruction set architectures\r\n\r\nWindows:\r\n```\r\nsystem_info: n_threads = 14 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 0 | \r\nNEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | VSX = 0 |\r\n```\r\n\r\nWSL2\r\n```\r\nsystem_info: n_threads = 14 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | \r\nNEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\r\n```\r\n\r\n**The Unix-build recognizes all the architectures of my CPU but the Windows-build is missing the F16C, FMA and SSE3 one.**\r\n\r\n- We probably haven't implemented all the CPU architectures on the Windows build (and maybe on the Unix build too)\r\n- My CPU has more architectures than those included into the builds [MMX, SSE, SSE2, SSSE3, SSE4,  SSE4.1 + SSE4.2, AES, BMI, BMI1 + BMI2, FMA3, EM64T, HT, VT-x, VT-d] \r\n> I believe that we can significantly enhance the speed of the results by implementing all of the possible instruction set architectures that would be advantageous for text generation.\r\n\r\n#  Test#2: Reproducibility of the output\r\n\r\nSince I used the exact same settings for both the Windows and Unix builds (refer to \"Deterministic Settings (seed=1)\"), I should expect to obtain the exact same output from both.\r\n\r\nWindows (I'll call this output \"WindowsText\"):\r\n```\r\n1. Video games improve hand eye coordination and reaction time, which is a skill used in everyday life.\r\n2. They help you to focus on the task at hand by blocking out distractions around you. This helps with concentration when doing other tasks such as reading or writing an essay.\r\n3. It improves problem solving skills because it requires players to think of different ways to solve problems. For example, if there was a puzzle that required you to find a key to unlock a door but the only way to get the key is to kill someone who has it then you would have to decide whether killing them is worth getting the key.\r\n4. It can be very relaxing after a long day of school work so it gives you some down time from all the stressful things going on in your life.\r\n5. It also increases creativity because they require you to come up with new ideas to complete levels. [end of text]\r\n```\r\n\r\nWSL2 (I'll call this output \"UnixText\")\r\n```\r\n1. Video games improve hand eye coordination and reaction time, which is a very important skill in sports like basketball or football where you need to react quickly when the ball comes towards you.\r\n2. It improves problem solving skills as well because it requires players to think of different ways to solve problems. For example, if there\u2019s an obstacle blocking your way then you have to find another route around it. This helps with real life situations too!\r\n3. It also increases attention span by keeping kids focused on one task at a time. If they get distracted while playing a game then they won\u2019t be able to complete their goal.\r\n4. It can help develop social skills such as teamwork and communication. Players must work together to accomplish goals. They learn how to communicate effectively through voice chat so that everyone knows what needs to happen next.\r\n5. Lastly, it teaches patience. Sometimes you may not know exactly what to do right away but after some practice you will eventually figure out how to beat the level. [end of text]\r\n```\r\n\r\n**It's not the case at all, you will get a different output based on the fact you're using a Windows build or a Unix build.**\r\n\r\n> I believe the Unix build has better outputs than the Windows one for the following reasons:\r\n\r\n- It mentions the importance of hand-eye coordination in sports like basketball or football, which are common activities that many people can relate to.\r\n\r\n- UnixText provides a more comprehensive list of benefits. It discusses the improvement of attention span, the development of social skills, and the teaching of patience, which are all valuable skills that were not mentioned in WindowsText.\r\n\r\n- The structure of UnixText is clearer and more concise, which makes it easier to read and understand.\r\n\r\n#  Test#3: Speed\r\n\r\nWindows:\r\n```\r\nllama_print_timings:        load time = 21085.73 ms\r\nllama_print_timings:      sample time =   734.50 ms /   194 runs   (    3.79 ms per run)\r\nllama_print_timings: prompt eval time =  5380.24 ms /    20 tokens (  269.01 ms per token)\r\nllama_print_timings:        eval time = 93395.22 ms /   193 runs   (  483.91 ms per run)\r\nllama_print_timings:       total time = 121975.58 ms\r\n```\r\n\r\nWSL2:\r\n```\r\nllama_print_timings:        load time = 30968.40 ms\r\nllama_print_timings:      sample time =  2342.41 ms /   219 runs   (   10.70 ms per run)\r\nllama_print_timings: prompt eval time =  4668.72 ms /    20 tokens (  233.44 ms per token)\r\nllama_print_timings:        eval time = 96435.62 ms /   218 runs   (  442.37 ms per run)\r\nllama_print_timings:       total time = 137830.02 ms\r\n```\r\n\r\n1) **Load time** : Windows is **1.46** times faster than Unix.\r\n2) **Sample time** : Windows is **2.82** times faster than Unix.\r\n3) **Prompt eval time**: Unix is **1.15** times faster than Windows.\r\n4) **Eval Time (Most important value)** : Unix is **1.09** times faster than Windows\r\n\r\n**Unix tends to be faster than Windows, which may be due to the absence of F16C, FMA, and SSE3 architectures in the Windows build.**\r\n\r\n# Conclusion\r\n\r\n1) The builds doesn't recognize all possible architectures on your CPU, if we fix that we could probably have significant increase of speed.\r\n2) Windows and WSL2 don't produce the same output, and I believe the Unix build gives better result. This is kinda concerning because the model is supposed to behave identically no matter the operating system.\r\n3) Unix is a bit faster than Windows, but that comparaison would be more relevant if both of them used the same architectures implementations.\r\n\r\nI think the discrepancies between the two operating systems were not only observed by me but also by others, and efforts are underway to address them. Nonetheless, I found it interesting to witness such differences between the two systems.\r\n",
    "labels": [
      "question",
      "build",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-25T20:09:51+00:00",
    "closed_at": "2024-04-12T01:07:40+00:00",
    "comments": 24,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/507/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/507"
  },
  {
    "number": 506,
    "title": "Move the third-party build / deploy scripts to a separate repository",
    "body": "It keeps bothering me to see these scripts in the source root.\r\nThey cannot live anywhere except in the root of the repo, so therefore it is time to go.\r\n\r\nTask: create `llama.flake` or `llama.deploy` repo and move the scripts there.",
    "labels": [
      "help wanted",
      "good first issue",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-25T18:39:41+00:00",
    "closed_at": "2023-06-17T10:00:17+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/506/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/506"
  },
  {
    "number": 503,
    "title": "Is it possible to run 65B with 32Gb of Ram ?",
    "body": "I already quantized my files with this command ./quantize ./ggml-model-f16.bin.X E:\\GPThome\\LLaMA\\llama.cpp-master-31572d9\\models\\65B\\ggml-model-q4_0.bin.X 2 , the first time it reduced my files size from 15.9 to 4.9Gb and when i tried to do it again nothing changed. After i executed this command \"./main -m ./models/65B/ggml-model-q4_0.bin -n 128 --interactive-first\" and when everything is loaded i enter my prompt, my memory usage goes to 98% (25Gb by main.exe) and i just wait dozens of minutes with nothing that appears heres an example:\r\n\r\n**PS E:\\GPThome\\LLaMA\\llama.cpp-master-31572d9> ./main -m ./models/65B/ggml-model-q4_0.bin -n 128 --interactive-first\r\nmain: seed = 1679761762\r\nllama_model_load: loading model from './models/65B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 8192\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 64\r\nllama_model_load: n_layer = 80\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 22016\r\nllama_model_load: n_parts = 8\r\nllama_model_load: ggml ctx size = 41477.73 MB\r\nllama_model_load: memory_size =  2560.00 MB, n_mem = 40960\r\nllama_model_load: loading model part 1/8 from './models/65B/ggml-model-q4_0.bin'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 2/8 from './models/65B/ggml-model-q4_0.bin.1'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 3/8 from './models/65B/ggml-model-q4_0.bin.2'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 4/8 from './models/65B/ggml-model-q4_0.bin.3'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 5/8 from './models/65B/ggml-model-q4_0.bin.4'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 6/8 from './models/65B/ggml-model-q4_0.bin.5'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 7/8 from './models/65B/ggml-model-q4_0.bin.6'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 8/8 from './models/65B/ggml-model-q4_0.bin.7'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\n\r\nsystem_info: n_threads = 4 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | VSX = 0 |\r\n\r\nmain: prompt: ' '\r\nmain: number of tokens in prompt = 2\r\n     1 -> ''\r\n 29871 -> ' '\r\n\r\nmain: interactive mode on.\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n\r\n how to become rich**",
    "labels": [
      "question",
      "hardware",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-25T17:17:10+00:00",
    "closed_at": "2023-03-26T10:18:47+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/503/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/503"
  },
  {
    "number": 496,
    "title": "First argument to printf should be a literal: won't build with -Wformat-security, -Werror=format-security",
    "body": "https://github.com/ggerganov/llama.cpp/blob/e899bf54b291e8c84173a0e534a2c262f3f63229/main.cpp#L481\r\n\r\nThis won't build with https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat-security\r\nThe safe call would be `printf(\"%s\", buffer.c_str())`",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-25T14:10:08+00:00",
    "closed_at": "2023-03-25T14:34:20+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/496/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/496"
  },
  {
    "number": 495,
    "title": "Cant compile for an arm ",
    "body": "Consolidate compiler generated dependencies of target utils\r\n[  8%] Building CXX object CMakeFiles/utils.dir/utils.cpp.o\r\nclang++: error: the clang compiler does not support '-mcpu=native'\r\nmake[2]: *** [CMakeFiles/utils.dir/build.make:76: CMakeFiles/utils.dir/utils.cpp.o] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:110: CMakeFiles/utils.dir/all] Error 2\r\nmake: *** [Makefile:101: all] Error 2\r\n\r\nif i will intentionally delete all -mcpu=native flags, then\r\n\r\n[  8%] Built target utils\r\nConsolidate compiler generated dependencies of target ggml\r\n[  8%] Built target ggml\r\n[  8%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o\r\n/home/gh228df/llama.cpp/llama.cpp:1447:33: warning: cast from 'const char *' to 'char *' drops const qualifier [-Wcast-qual]\r\n            finp.read ((char *) word.data(), len);\r\n                                ^\r\n/home/gh228df/llama.cpp/llama.cpp:1448:33: warning: cast from 'const char *' to 'char *' drops const qualifier [-Wcast-qual]\r\n            fout.write((char *) word.data(), len);\r\n                                ^\r\n2 warnings generated.\r\n[  8%] Linking CXX static library libllama.a\r\n[  8%] Built target llama\r\n[  8%] Building CXX object CMakeFiles/main.dir/main.cpp.o\r\n/home/gh228df/llama.cpp/main.cpp:356:20: warning: unused variable 'embeddings' [-Wunused-variable]\r\n        const auto embeddings = llama_get_embeddings(ctx);\r\n                   ^\r\n/home/gh228df/llama.cpp/main.cpp:481:28: error: format string is not a string literal (potentially insecure) [-Werror,-Wformat-security]\r\n                    printf(buffer.c_str());\r\n                           ^~~~~~~~~~~~~~\r\n/home/gh228df/llama.cpp/main.cpp:481:28: note: treat the string as an argument to avoid this\r\n                    printf(buffer.c_str());\r\n                           ^\r\n                           \"%s\", \r\n1 warning and 1 error generated.\r\nmake[2]: *** [CMakeFiles/main.dir/build.make:76: CMakeFiles/main.dir/main.cpp.o] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:191: CMakeFiles/main.dir/all] Error 2\r\nmake: *** [Makefile:101: all] Error 2\r\n\r\n",
    "labels": [
      "hardware",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-25T13:56:11+00:00",
    "closed_at": "2023-07-01T18:31:45+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/495/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/495"
  },
  {
    "number": 493,
    "title": "Docker error:  Cannot access '/models//7B/ggml-model-f16.bin*': No such file or directory",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [*] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [*] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [*] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [*] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\nllama\r\n\u2514\u2500\u2500 models\r\n    \u251c\u2500\u2500 13B\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 checklist.chk\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 consolidated.00.pth\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 consolidated.01.pth\r\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 params.json\r\n    \u251c\u2500\u2500 30B\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 checklist.chk\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 consolidated.00.pth\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 consolidated.01.pth\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 consolidated.02.pth\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 consolidated.03.pth\r\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 params.json\r\n    \u251c\u2500\u2500 65B\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 checklist.chk\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 consolidated.00.pth\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 consolidated.01.pth\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 consolidated.02.pth\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 consolidated.03.pth\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 consolidated.04.pth\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 consolidated.05.pth\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 consolidated.06.pth\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 consolidated.07.pth\r\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 params.json\r\n    \u251c\u2500\u2500 7B\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 checklist.chk\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 consolidated.00.pth\r\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 params.json\r\n    \u251c\u2500\u2500 ggml-vocab.bin\r\n    \u251c\u2500\u2500 llama.sh\r\n    \u251c\u2500\u2500 tokenizer_checklist.chk\r\n    \u2514\u2500\u2500 tokenizer.model\r\n\r\nusing ```docker run -v /llama/models:/models ghcr.io/ggerganov/llama.cpp:full --all-in-one \"/models/\" 7B```\r\n\r\noutput : \r\n```\r\nConverting PTH to GGML...\r\nls: cannot access '/models//7B/ggml-model-f16.bin*': No such file or directory\r\n```",
    "labels": [
      "bug",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-25T12:11:35+00:00",
    "closed_at": "2023-04-16T09:32:00+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/493/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/493"
  },
  {
    "number": 490,
    "title": "Clearer windows instructions.. please?",
    "body": "Looking at the README i just see too many incomplete information and steps...  ( the readme is assuming i know things i dont )\r\n\r\nIf anyone would be nice to ELI5 for me please... I've gotten up to installing visual studio and i cloned the repo in a folder... I got the 7B llama file \"consolidated.00.pth\" ... I have multiple versions of python so i dont know wich one to use.. \r\n\r\nThen.. the instructions are just head scratching to me.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-25T10:54:52+00:00",
    "closed_at": "2023-03-26T20:13:00+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/490/reactions",
      "total_count": 3,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/490"
  },
  {
    "number": 484,
    "title": "Initial prompt tokens should be loaded instantly and not require inference",
    "body": "# Expected Behavior\r\n\r\nInput prompt tokens should load instantly, without having to run inference through the model. The first inference computation should start with the first token after the prompt.\r\n\r\n# Current Behavior\r\n\r\nI might be misunderstanding something, but it seems in the llama.cpp implementation that all the tokens from the input prompt are fed through the model sequentially (in 8-token batches) before any inference of new tokens can take place. This results in a large delay before getting any responses from the model. \r\n\r\nOne of the big benefits of a transformer model, versus an RNN, is that the entire token context window can be ingested and attended to all at once. But llama.cpp seems to be behaving like an RNN, where each prompt token has to be fed in sequentially first, and the output logits ignored, until finally inference can begin.\r\n\r\nAm I just misunderstanding something here?\r\n\r\nTo show it semi-graphically, a transformer should be able to ingest this on first run:\r\n\r\n[I, like, to, eat, -, -, -, -, -, -]   -> apples (inferred)\r\n\r\nBut llama.cpp seems to require:\r\n\r\n[I, -, -, -, -, -, -, -, -, -]  ->  (logits ignored, startup batch)\r\n[I, like, -, -, -, -, -, -, -, -]  ->  (logits ignored, startup batch)\r\n[I, like, to, -, -, -, -, -, -, -]  ->  (logits ignored, startup batch)\r\n[I, like, to, eat, -, -, -, -, -, -]  ->  apples (finally we get to the inference / logits we care about)\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-25T00:38:28+00:00",
    "closed_at": "2023-03-26T15:48:39+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/484/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/484"
  },
  {
    "number": 482,
    "title": "make issue on sbc odroid",
    "body": "I am trying to run \"make\" on an odroid sbc and get following error:\r\n\r\n`I llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  unknown\r\nI UNAME_M:  armv7l\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mfpu=neon-fp-armv8 -mfp16-format=ieee -mno-unaligned-access -funsafe-math-optimizations\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\r\nI LDFLAGS:  \r\nI CC:       cc (Debian 10.2.1-6) 10.2.1 20210110\r\nI CXX:      g++ (Debian 10.2.1-6) 10.2.1 20210110\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mfpu=neon-fp-armv8 -mfp16-format=ieee -mno-unaligned-access -funsafe-math-optimizations   -c ggml.c -o ggml.o\r\nggml.c: In function \u2018ggml_vec_mad_q4_0\u2019:\r\nggml.c:2049:35: warning: implicit declaration of function \u2018vzip1_s8\u2019; did you mean \u2018vzipq_s8\u2019? [-Wimplicit-function-declaration]\r\n 2049 |             const int8x8_t vxlt = vzip1_s8(vxls, vxhs);\r\n      |                                   ^~~~~~~~\r\n      |                                   vzipq_s8\r\nggml.c:2049:35: error: incompatible types when initializing type \u2018int8x8_t\u2019 using type \u2018int\u2019\r\nggml.c:2050:35: warning: implicit declaration of function \u2018vzip2_s8\u2019; did you mean \u2018vzipq_s8\u2019? [-Wimplicit-function-declaration]\r\n 2050 |             const int8x8_t vxht = vzip2_s8(vxls, vxhs);\r\n      |                                   ^~~~~~~~\r\n      |                                   vzipq_s8\r\nggml.c:2050:35: error: incompatible types when initializing type \u2018int8x8_t\u2019 using type \u2018int\u2019\r\nmake: *** [Makefile:222: ggml.o] Error 1\r\n`\r\n\r\nAlso trying to run the docker image causes following:\r\n\r\nUnable to find image 'ghcr.io/ggerganov/llama.cpp:light' locally\r\nlight: Pulling from ggerganov/llama.cpp\r\ndocker: no matching manifest for linux/arm/v7 in the manifest list entries.\r\nSee 'docker run --help'.\r\n\r\n",
    "labels": [
      "need more info",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-24T23:32:44+00:00",
    "closed_at": "2023-05-18T10:54:07+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/482/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/482"
  },
  {
    "number": 481,
    "title": "#if defined(AVX) && !defined(F16C)",
    "body": "#if defined(__AVX__) && !defined(__F16C__)\r\n__m256 _mm256_cvtph_ps(__m128i x) {\r\n    ggml_fp16_t const * src = (ggml_fp16_t const *)&x;\r\n    float dst[8];\r\n    for (int i = 0; i < 8; ++i)\r\n        dst[i] = GGML_FP16_TO_FP32(src[i]);\r\n    return *(__m256*)&dst;\r\n}\r\n__m128i _mm256_cvtps_ph(__m256 x, int imm) {\r\n    float const * src = (float const *)&x;\r\n    ggml_fp16_t dst[8];\r\n    for (int i = 0; i < 8; ++i)\r\n        dst[i] = GGML_FP32_TO_FP16(src[i]);\r\n    return *(__m128i*)&dst;\r\n}\r\n#endif",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-24T23:14:05+00:00",
    "closed_at": "2023-03-24T23:25:58+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/481/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/481"
  },
  {
    "number": 479,
    "title": "The \"quantize.exe\" script was not found in the current location",
    "body": "Im trying to use it with 65B, but when i want to quantize it, i have the error \"The \"quantize.exe\" script was not found in the current location\"\r\n\r\nIm running it on windows 11 with 48bg of ram and I7 12700k",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-24T22:50:17+00:00",
    "closed_at": "2023-03-25T13:29:17+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/479/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/479"
  },
  {
    "number": 474,
    "title": "7B model returning complete non-sense",
    "body": "i followed a YouTube video to build the program https://www.youtube.com/watch?v=coIj2CU5LMU&t=186s. it itself follows the issue #103 \r\n\r\n# Expected Behavior\r\n\r\nAs a test I ran the ./chat.sh in git bash, it ran but when I said the AI \"hello\" I expected hello back.\r\n\r\n# Current Behavior\r\n\r\nit responded with\r\n```\r\n\u203c \u25bc\u2192\u25ac\u25ac\u25b2\u21a8\u203c\u2191\u2665\u2660\u2666\"\u2665 \u263b \u00d4\u00fc\u00e7 \u221f \u00d4\u00fc\u00e7 \u2194\u00b6\r\n\u203c\u221f \u00d4\u00fc\u00e7 \u2665\r\n\u25ba\u2660\r\n\u25bc!\u2195\u263b    \u25bc \u2193     $\u25bc\u221f\u25bc\u2195\u2663\u2194\"\u203c\u2194\u2665\r\n\u263a       \u25ba        \u2194 \u00d4\u00fc\u00e7   #\u2191\"\u25bc\u2191\u2660$$\u25ac\u263a\u263b\r\n```\r\n\r\n# Environment and Context \r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\nI\u2019m running a i7-13 th gen with 32 go of ram and a 3060.\r\n\r\nwindows 11 home\r\n\r\ngit bash to run the commands and cmake to compile\r\n\r\n```\r\nPython 3.10.10\r\ncmake 3.26.1\r\ng++.exe (MinGW.org GCC-6.3.0-1) 6.3.0\r\n```\r\n\r\n# Failure Logs\r\n\r\n```\r\n$ ./chat.sh\r\nmain: seed = 1679687646\r\nllama_model_load: loading model from './models/7B/ggml-model-f16.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 1\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: ggml ctx size = 13365.09 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: loading model part 1/1 from './models/7B/ggml-model-f16.bin'\r\nllama_model_load:  done\r\nllama_model_load: model size =     0.00 MB / num tensors = 0\r\n\r\nsystem_info: n_threads = 4 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | VSX = 0 |\r\n\r\nmain: prompt: ' Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.\r\n\r\nUser: Hello, Bob.\r\nBob: Hello. How may I help you today?\r\nUser: Please tell me the largest city in Europe.\r\nBob: Sure. The largest city in Europe is Moscow, the capital of Russia.\r\nUser:'\r\nmain: number of tokens in prompt = 99\r\n     1 -> ''\r\n  4103 -> ' Trans'\r\n   924 -> 'cript'\r\n   310 -> ' of'\r\n   263 -> ' a'\r\n  7928 -> ' dialog'\r\n 29892 -> ','\r\n   988 -> ' where'\r\n   278 -> ' the'\r\n  4911 -> ' User'\r\n 16254 -> ' interact'\r\n 29879 -> 's'\r\n   411 -> ' with'\r\n   385 -> ' an'\r\n  4007 -> ' Ass'\r\n 22137 -> 'istant'\r\n  4257 -> ' named'\r\n  7991 -> ' Bob'\r\n 29889 -> '.'\r\n  7991 -> ' Bob'\r\n   338 -> ' is'\r\n  8444 -> ' helpful'\r\n 29892 -> ','\r\n  2924 -> ' kind'\r\n 29892 -> ','\r\n 15993 -> ' honest'\r\n 29892 -> ','\r\n  1781 -> ' good'\r\n   472 -> ' at'\r\n  5007 -> ' writing'\r\n 29892 -> ','\r\n   322 -> ' and'\r\n  2360 -> ' never'\r\n  8465 -> ' fails'\r\n   304 -> ' to'\r\n  1234 -> ' answer'\r\n   278 -> ' the'\r\n  4911 -> ' User'\r\n 29915 -> '''\r\n 29879 -> 's'\r\n  7274 -> ' requests'\r\n  7389 -> ' immediately'\r\n   322 -> ' and'\r\n   411 -> ' with'\r\n 16716 -> ' precision'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n 15043 -> ' Hello'\r\n 29892 -> ','\r\n  7991 -> ' Bob'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n 29362 -> 'Bob'\r\n 29901 -> ':'\r\n 15043 -> ' Hello'\r\n 29889 -> '.'\r\n  1128 -> ' How'\r\n  1122 -> ' may'\r\n   306 -> ' I'\r\n  1371 -> ' help'\r\n   366 -> ' you'\r\n  9826 -> ' today'\r\n 29973 -> '?'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n  3529 -> ' Please'\r\n  2649 -> ' tell'\r\n   592 -> ' me'\r\n   278 -> ' the'\r\n 10150 -> ' largest'\r\n  4272 -> ' city'\r\n   297 -> ' in'\r\n  4092 -> ' Europe'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n 29362 -> 'Bob'\r\n 29901 -> ':'\r\n 18585 -> ' Sure'\r\n 29889 -> '.'\r\n   450 -> ' The'\r\n 10150 -> ' largest'\r\n  4272 -> ' city'\r\n   297 -> ' in'\r\n  4092 -> ' Europe'\r\n   338 -> ' is'\r\n 25820 -> ' Moscow'\r\n 29892 -> ','\r\n   278 -> ' the'\r\n  7483 -> ' capital'\r\n   310 -> ' of'\r\n 12710 -> ' Russia'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n\r\nmain: interactive mode on.\r\nReverse prompt: 'User:'\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.000000\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n\r\n Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.\r\n\r\nUser: Hello, Bob.\r\nBob: Hello. How may I help you today?\r\nUser: Please tell me the largest city in Europe.\r\nBob: Sure. The largest city in Europe is Moscow, the capital of Russia.\r\nUser:hello\r\n\u203c \u25bc\u2192\u25ac\u25ac\u25b2\u21a8\u203c\u2191\u2665\u2660\u2666\"\u2665 \u263b \u00d4\u00fc\u00e7 \u221f \u00d4\u00fc\u00e7 \u2194\u00b6\r\n\u203c\u221f \u00d4\u00fc\u00e7 \u2665\r\n\u25ba\u2660\r\n\u25bc!\u2195\u263b    \u25bc \u2193     $\u25bc\u221f\u25bc\u2195\u2663\u2194\"\u203c\u2194\u2665\r\n\u263a       \u25ba        \u2194 \u00d4\u00fc\u00e7   #\u2191\"\u25bc\u2191\u2660$$\u25ac\u263a\u263b\r\n````\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-24T20:05:37+00:00",
    "closed_at": "2023-03-25T10:26:17+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/474/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/474"
  },
  {
    "number": 466,
    "title": "How do we finetune the model with new data?",
    "body": "Can we have a finetune.cpp or finetune.exe file to incorporate new data into the model? The use case will be to design an AI model that can do more than just general chat. It can become very knowledgeable in specific topics they are finetuned on. Also, after creating the finetune.exe , please ensure no GPU is required for the entire process. Because that is what makes this repo awesome in the first place.",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-24T16:12:02+00:00",
    "closed_at": "2024-04-10T01:07:59+00:00",
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/466/reactions",
      "total_count": 15,
      "+1": 12,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 3,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/466"
  },
  {
    "number": 464,
    "title": "Question about Web integration/Articles analisys",
    "body": "Is it possible to make bridge to web for something unknown to model? (ChatGPT introduced plugins to search web, etc)\r\nOr at least for model to read article/book and answer questions about it? ",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-24T15:50:06+00:00",
    "closed_at": "2023-03-24T18:56:15+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/464/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/464"
  },
  {
    "number": 463,
    "title": "Simplify the quantization process",
    "body": "The current quantization call stack is long and difficult to debug, which makes extending or adding new quantization methods in the future a major issue. This is because changes would need to be made in various places.\r\n\r\nAdditionally, we should aim to add drivers that help with benchmarking various quantization methods.\r\n\r\n**The current stack:**\r\n1. quantize.py invokes the quantize binary\r\n2. quantize.cpp reads model and logs metrics\r\n3. llama.cpp loads model weights, checks quantization type, and sends to quantization function\r\n4. ggml.c performs the actual quantization\r\n\r\nOpen to suggestions here and would like to hear if it's worth investing our time and effort",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-24T14:47:25+00:00",
    "closed_at": "2023-07-28T19:41:16+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/463/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/463"
  },
  {
    "number": 462,
    "title": "[fixed]The last code build with memory fix running result is not good in my pc.",
    "body": "Be obviously slower with Q_1 30b model. And the memory usage become garbage...\n(Linux 5.19 x64 Ubuntu base)",
    "labels": [
      "bug",
      "performance"
    ],
    "state": "closed",
    "created_at": "2023-03-24T14:22:06+00:00",
    "closed_at": "2023-03-27T00:13:38+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/462/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/462"
  },
  {
    "number": 456,
    "title": "2-bit integer quantization ",
    "body": "Add `Q2_0` and `Q2_1` quantization support to `ggml`:\r\n\r\n- Follow the existing `Q4_0` and `Q4_1` implementations\r\n- Implement [reference scalar quantization and dequantization routines](https://github.com/ggerganov/llama.cpp/blob/3cd8dde0d1357b7f11bdd25c45d5bf5e97e284a0/ggml.c#L407-L449)\r\n- I suspect we might have to use `QK == 16` in this case to compensate for further accuracy losses\r\n- Add SIMD support for a specific architecture - investigate best strategy to perform the `ggml_vec_dot_q2()` computation\r\n- No need to implement `ggml_vec_mad_q2()` - these will be deprecated soon\r\n- Compute perplexity scores\r\n\r\nThe expected model sizes for 7B and `QK == 16` are:\r\n\r\n- `Q2_0` - 3.2 GB\r\n\r\nFor `QK == 32` we have:\r\n\r\n- `Q2_0` - 2.4 GB\r\n- `Q2_1` - 3.2 GB\r\n\r\nBefore you send me papers that show 2-bit quantization does not work - no need. I want to have this supported anyway. I have something in mind. The efforts needed to add this support are so small that there is no reason not to do it.",
    "labels": [
      "enhancement",
      "research \ud83d\udd2c"
    ],
    "state": "closed",
    "created_at": "2023-03-24T06:55:44+00:00",
    "closed_at": "2023-06-24T19:17:24+00:00",
    "comments": 16,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/456/reactions",
      "total_count": 30,
      "+1": 20,
      "-1": 0,
      "laugh": 0,
      "hooray": 7,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/456"
  },
  {
    "number": 452,
    "title": "Add support for running bloom models",
    "body": "Bloom models have a more permissive license than llama models and are also multilingual in nature. While there is a project [based on llama.cpp](https://github.com/NouamaneTazi/bloomz.cpp) which can perform inference of bloom models, development seems to be slow and might even stagnate after a few days. So I am requesting to add support for running bloom models using llama.cpp(most probably with a command-line switch)",
    "labels": [
      "enhancement",
      "model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-24T02:26:13+00:00",
    "closed_at": "2024-04-10T01:08:00+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/452/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/452"
  },
  {
    "number": 451,
    "title": "Can it support avx cpu's older than 10 years old?",
    "body": "I can't run any model due to my cpu is from before 2013.So I don't have avx2 instructions.Can you please support avx cpus?",
    "labels": [
      "enhancement",
      "hardware",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-24T02:19:30+00:00",
    "closed_at": "2023-07-28T19:40:41+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/451/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/451"
  },
  {
    "number": 449,
    "title": "Change ./main help output to better reflect context size's affect on generation length",
    "body": "### Discussed in https://github.com/ggerganov/llama.cpp/discussions/446\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **cmp-nct** March 24, 2023</sup>\r\nI've been testing alpaca 30B (-t 24 -n 2000 --temp 0.2 -b 32 --n_parts 1 --ignore-eos --instruct)\r\nI've consistently have it \"stop\" after 300-400 tokens output (30-40 tokens input)\r\nNo error message, no crash and given the -n 2000 and the ignore-eos no reason to stop so early\r\n\r\nI guess it would be useful if the program provides a verbose quit reason, though in my case I can't see any reason for it to stop before token max is reached.\r\n\r\n\r\nI'm not sure if that's a bug to report or if I am missing something.</div>",
    "labels": [
      "documentation",
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-24T01:38:43+00:00",
    "closed_at": "2023-07-28T19:40:24+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/449/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/449"
  },
  {
    "number": 448,
    "title": "possible to run full sizes?",
    "body": "I'm not sure if this is an enhancement request because maybe it's already supported.  Is it possible to run the full models?  I know they take a ton of extra memory but I'd still like to try them out, eg, 13GB for 7B instead of 3.9GB, etc.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-24T00:26:39+00:00",
    "closed_at": "2023-03-24T20:18:52+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/448/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/448"
  },
  {
    "number": 447,
    "title": "Cannot run llama.cpp on termux. Bash permission denied",
    "body": "When trying to run './bin/main/ -m ./models/7B/ggml-model-q4_0.bin -n 128' termux throws this output:\r\nbash: ./bin/main: permission denied",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-23T23:35:53+00:00",
    "closed_at": "2023-03-24T18:29:39+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/447/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/447"
  },
  {
    "number": 443,
    "title": "[ERROR] Using \"make\" command",
    "body": "Hello evryone, \r\n\r\nI have an issue when i run \"make\" cmd : \r\nI use Ubuntu 22.04 in VirtualBox\r\nMake version : GNU Make 4.3\r\n\r\n\r\nHere the return of cmd \r\n\r\n<pre>I llama.cpp build info: \r\n\r\nI UNAME_S:  Linux\r\n\r\nI UNAME_P:  x86_64\r\n\r\nI UNAME_M:  x86_64\r\n\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -msse3\r\n\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\r\n\r\nI LDFLAGS:  \r\n\r\nI CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\n\r\nI CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\n\r\n\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -msse3   -c ggml.c -o ggml.o\r\n\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101</b>,\r\n\r\n                 from <b>ggml.c:158</b>:\r\n\r\n<b>ggml.c:</b> In function \u2018<b>ggml_vec_dot_f16</b>\u2019:\r\n\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1:</b> <font color=\"#C01C28\"><b>error: </b></font>inlining failed in call to \u2018<b>always_inline</b>\u2019 \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n\r\n   52 | <font color=\"#C01C28\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n\r\n      | <font color=\"#C01C28\"><b>^~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:936:33:</b> <font color=\"#2AA1B3\"><b>note: </b></font>called from here\r\n\r\n  936 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#2AA1B3\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n\r\n      |                                 <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:946:37:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n\r\n  946 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#2AA1B3\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n\r\n      |                                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:1340:21:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n\r\n 1340 |             ay[j] = <font color=\"#2AA1B3\"><b>GGML_F16_VEC_LOAD</b></font>(y + i + j*GGML_F16_EPR, j);\r\n\r\n      |                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\n\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101</b>,\r\n\r\n                 from <b>ggml.c:158</b>:\r\n\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1:</b> <font color=\"#C01C28\"><b>error: </b></font>inlining failed in call to \u2018<b>always_inline</b>\u2019 \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n\r\n   52 | <font color=\"#C01C28\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n\r\n      | <font color=\"#C01C28\"><b>^~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:936:33:</b> <font color=\"#2AA1B3\"><b>note: </b></font>called from here\r\n\r\n  936 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#2AA1B3\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n\r\n      |                                 <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:946:37:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n\r\n  946 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#2AA1B3\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n\r\n      |                                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:1339:21:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n\r\n 1339 |             ax[j] = <font color=\"#2AA1B3\"><b>GGML_F16_VEC_LOAD</b></font>(x + i + j*GGML_F16_EPR, j);\r\n\r\n      |                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\n\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101</b>,\r\n\r\n                 from <b>ggml.c:158</b>:\r\n\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1:</b> <font color=\"#C01C28\"><b>error: </b></font>inlining failed in call to \u2018<b>always_inline</b>\u2019 \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n\r\n   52 | <font color=\"#C01C28\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n\r\n      | <font color=\"#C01C28\"><b>^~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:936:33:</b> <font color=\"#2AA1B3\"><b>note: </b></font>called from here\r\n\r\n  936 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#2AA1B3\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n\r\n      |                                 <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:946:37:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n\r\n  946 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#2AA1B3\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n\r\n      |                                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:1339:21:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n\r\n 1339 |             ax[j] = <font color=\"#2AA1B3\"><b>GGML_F16_VEC_LOAD</b></font>(x + i + j*GGML_F16_EPR, j);\r\n\r\n      |                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\n\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101</b>,\r\n\r\n                 from <b>ggml.c:158</b>:\r\n\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1:</b> <font color=\"#C01C28\"><b>error: </b></font>inlining failed in call to \u2018<b>always_inline</b>\u2019 \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n\r\n   52 | <font color=\"#C01C28\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n\r\n      | <font color=\"#C01C28\"><b>^~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:936:33:</b> <font color=\"#2AA1B3\"><b>note: </b></font>called from here\r\n\r\n  936 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#2AA1B3\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n\r\n      |                                 <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:946:37:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n\r\n  946 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#2AA1B3\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n\r\n      |                                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:1340:21:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n\r\n 1340 |             ay[j] = <font color=\"#2AA1B3\"><b>GGML_F16_VEC_LOAD</b></font>(y + i + j*GGML_F16_EPR, j);\r\n\r\n      |                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\n\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101</b>,\r\n\r\n                 from <b>ggml.c:158</b>:\r\n\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1:</b> <font color=\"#C01C28\"><b>error: </b></font>inlining failed in call to \u2018<b>always_inline</b>\u2019 \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n\r\n   52 | <font color=\"#C01C28\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n\r\n      | <font color=\"#C01C28\"><b>^~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:936:33:</b> <font color=\"#2AA1B3\"><b>note: </b></font>called from here\r\n\r\n  936 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#2AA1B3\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n\r\n      |                                 <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:946:37:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n\r\n  946 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#2AA1B3\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n\r\n      |                                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:1339:21:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n\r\n 1339 |             ax[j] = <font color=\"#2AA1B3\"><b>GGML_F16_VEC_LOAD</b></font>(x + i + j*GGML_F16_EPR, j);\r\n\r\n      |                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\n\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101</b>,\r\n\r\n                 from <b>ggml.c:158</b>:\r\n\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1:</b> <font color=\"#C01C28\"><b>error: </b></font>inlining failed in call to \u2018<b>always_inline</b>\u2019 \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n\r\n   52 | <font color=\"#C01C28\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n\r\n      | <font color=\"#C01C28\"><b>^~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:936:33:</b> <font color=\"#2AA1B3\"><b>note: </b></font>called from here\r\n\r\n  936 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#2AA1B3\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n\r\n      |                                 <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:946:37:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n\r\n  946 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#2AA1B3\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n\r\n      |                                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n\r\n<b>ggml.c:1340:21:</b> <font color=\"#2AA1B3\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n\r\n 1340 |             ay[j] = <font color=\"#2AA1B3\"><b>GGML_F16_VEC_LOAD</b></font>(y + i + j*GGML_F16_EPR, j);\r\n\r\n      |                     <font color=\"#2AA1B3\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\n\r\nmake: *** [Makefile:221 : ggml.o] Erreur 1\r\n\r\n</pre>",
    "labels": [
      "hardware",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-23T22:26:52+00:00",
    "closed_at": "2023-04-22T17:29:32+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/443/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/443"
  },
  {
    "number": 442,
    "title": "Converting alpaca-native-GPTQ models into ggml models",
    "body": "# Expected Behavior\r\n\r\nHello, \r\n\r\nI wanted to convert the alpaca-native 7b GPTQ file (pt file) into a ggml file with the convert-gptq-to-ggml.py script https://github.com/ggerganov/llama.cpp/blob/master/convert-gptq-to-ggml.py\r\n\r\n# Current Behavior\r\n\r\nThe problem is that I have this error \r\n\r\n```\r\nD:\\Large Language Models\\CONVERTISSEURS\\gptq to ggml>python convert-gptq-to-ggml.py alpaca-native-4b\r\nit.pt tokenizer.model out.bin\r\n32000\r\n32001\r\nTraceback (most recent call last):\r\n  File \"D:\\Large Language Models\\CONVERTISSEURS\\gptq to ggml\\convert-gptq-to-ggml.py\", line 35, in <\r\nmodule>\r\n    assert tokenizer.vocab_size() == n_vocab\r\nAssertionError\r\n```\r\n32000 is the tokenizer.vocab_size() (Number of tokens on the tokenizer.model)\r\n32001 is the n_vocab (Number of tokens on the model)\r\n\r\nThe model that is trained with alpaca has 1 more token and it's this one:\r\n\"[PAD]\": 32000\r\n\r\nIt looks like that if we want to convert the alpaca native GPTQ models we need to create a new tokenizer.model that has this \"PAD\" token in it.\r\n\r\nThe problem is that I have no idea how to do that... if someone can help me on this I'll appreciate!\r\n",
    "labels": [
      "enhancement",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-23T22:02:03+00:00",
    "closed_at": "2023-07-28T19:39:46+00:00",
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/442/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/442"
  },
  {
    "number": 441,
    "title": "Eliminate `ggml_forward_mul_mat_xxx()` branch for non-contiguous `src0`",
    "body": "See explanation here: https://github.com/ggerganov/llama.cpp/pull/439",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-23T21:26:40+00:00",
    "closed_at": "2023-07-28T19:37:48+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/441/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/441"
  },
  {
    "number": 436,
    "title": "Name change proposal discussion",
    "body": "https://discord.com/channels/1038249716149928046/1080876668530466918/1088523954626494464",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-23T18:10:17+00:00",
    "closed_at": "2023-03-23T18:21:20+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/436/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/436"
  },
  {
    "number": 432,
    "title": "How to output text to a file?",
    "body": "I really, really tried hard to understand and modify the code but I am not an expert on C++ and so I find it a little bit difficult to change parts of this software. Is there a way to simply execute a command and get the output without all of that verbosity?",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2023-03-23T17:06:06+00:00",
    "closed_at": "2023-03-24T15:19:36+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/432/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/432"
  },
  {
    "number": 431,
    "title": "Quantize python script fails.",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ ] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ ] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ ] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ ] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI have my llama models stored in models/llama/{7B,13B,30B,65B}.\r\n\r\nI expect that when I run the following command that the model will be converted\r\n\r\n$ python3 quantize.py --models-path models/llama 30B\r\n\r\n\r\n# Current Behavior\r\n\r\nWhen attempting to quantize the model by running \r\n\r\n$ python3 quantize.py --models-path models/llama 30B\r\n\r\nI get the following error:\r\n\r\nThe f16 model ggml-model-f16.bin was not found in models/llama/30B. If you want to use it from another location, set the --models-path argument from the command line.\r\n\r\n\r\n\r\nmodifying lines 76-79\r\n\r\n```\r\n        f16_model_parts_paths = map(\r\n            lambda filename: os.path.join(f16_model_path_base, filename),\r\n            glob.glob(f\"{f16_model_path_base}*\")\r\n        )\r\n```\r\n\r\nTo\r\n\r\n```\r\n       f16_model_parts_paths = [ filename for filename in glob.glob(f\"{f16_model_path_base}*\")]\r\n```\r\n\r\nMakes it work.\r\n\r\n\r\n```\r\n$ python3 --version   --> Python 3.8.10\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-03-23T15:15:24+00:00",
    "closed_at": "2023-03-23T20:42:54+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/431/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/431"
  },
  {
    "number": 425,
    "title": "\"Illegal Instruction\" error when converting 7B model to ggml FP16 format (Raspberry Pi 4, 8GB, Raspberry Pi OS, 64-bit)",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ /] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ /] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ /] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ /] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI expected the command to convert the 7B model to ggml FP16 format\r\n\r\n# Current Behavior\r\n\r\nIllegal instruction error\r\n\r\n```\r\nles@raspberrypi:~/llama.cpp $ python3 convert-pth-to-ggml.py models/7B/ 1\r\nIllegal instruction\r\n```\r\n\r\n\r\n# Environment and Context \r\n\r\nRaspberry Pi 4 8GB\r\nLatest Raspberry Pi OS 64-bit, fully updated\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n`$lscpu`\r\nArchitecture:                    aarch64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nCPU(s):                          4\r\nOn-line CPU(s) list:             0-3\r\nThread(s) per core:              1\r\nCore(s) per socket:              4\r\nSocket(s):                       1\r\nVendor ID:                       ARM\r\nModel:                           3\r\nModel name:                      Cortex-A72\r\nStepping:                        r0p3\r\nCPU max MHz:                     1800.0000\r\nCPU min MHz:                     600.0000\r\nBogoMIPS:                        108.00\r\nL1d cache:                       128 KiB\r\nL1i cache:                       192 KiB\r\nL2 cache:                        1 MiB\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Mitigation; __user pointer sanitization\r\nVulnerability Spectre v2:        Vulnerable\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fp asimd evtstrm crc32 cpuid\r\n\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`$ uname -a`\r\nLinux raspberrypi 5.15.84-v8+ #1613 SMP PREEMPT Thu Jan 5 12:03:08 GMT 2023 aarch64 GNU/Linux\r\n\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version\r\nPython 3.9.2\r\n$ make --version\r\nGNU Make 4.3\r\nBuilt for aarch64-unknown-linux-gnu\r\n\r\n$ g++ --version\r\n```\r\ng++ (Debian 10.2.1-6) 10.2.1 20210110\r\n\r\n\r\n# Failure Information (for bugs)\r\n\r\nPlease help provide information about the failure if this is a bug. If it is not a bug, please remove the rest of this template.\r\n\r\n# Steps to Reproduce\r\n\r\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\r\n\r\n1. Cloned the repo.\r\n2. Changed directory to llama.cpp.\r\n3. Used make.\r\n4. Downloaded the 7B model and copied it to /models/7B.\r\n5. Installed the Python modules torch numpy sentencepiece.\r\n6. Used convert-pth-to-ggml.py models/7B/ 1 to convert the model to ggml FP16 format.\r\n7. Received the illegal instruction error after a few seconds.\r\n",
    "labels": [
      "duplicate",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-23T11:52:38+00:00",
    "closed_at": "2023-03-26T15:27:25+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/425/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/425"
  },
  {
    "number": 414,
    "title": "how to fine tuning model with with dataset (file json/csv..)",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-23T04:29:28+00:00",
    "closed_at": "2023-03-23T08:55:14+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/414/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/414"
  },
  {
    "number": 413,
    "title": "Mismatch in Vocabulary Size: Investigating Inconsistencies between Token-to-ID and ID-to-Token Dictionaries",
    "body": "The total number of vocabulary items in the model file is 32k. When we parse them, there's a mismatch between token_to_id and id_to_token.\r\n\r\nThe size for token_to_id is: 31,903\r\nThe size for id_to_token is: 32,000\r\n\r\nI'm curious on why there's a mismatch. Are there some token IDs that are reserved or errors during pre-processing?",
    "labels": [
      "question",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-23T02:05:32+00:00",
    "closed_at": "2024-04-10T01:08:01+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/413/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/413"
  },
  {
    "number": 412,
    "title": "Add Shared Library Build Target",
    "body": "With the C API now merged it would be very useful to have build targets for `make` and `cmake` that produce shared library versions of `llama.cpp`. This way `llama.cpp` can just be dynamically linked in other applications.",
    "labels": [
      "bug",
      "enhancement",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-22T22:45:26+00:00",
    "closed_at": "2023-03-23T20:16:51+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/412/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/412"
  },
  {
    "number": 411,
    "title": "[User] Please fix segmentation fault when prompt is too long",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI want to be able to run my promt using this command without any `Segmentation fault` error: \r\n```bash\r\n./main -m ./models/7B/ggml-model-q4_0.bin -t 8 -n 256 --repeat_penalty 1.0 --color -i -r \"Prompt:\" --temp 1.2 -p \"$(cat ../twitch_bot/prompt.md)\"\r\n```\r\nWhere `prompt.md` contains 3083 characters (933 tokens).\r\n\r\n# Current Behavior\r\n\r\nThe command only output the first 1909 character of the prompt in the console (550 tokens) and throw a `Segmentation fault` error.\r\n\r\nThis close the program and didn't let me execute my prompt.\r\n\r\n# Environment and Context \r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n```\r\n$ lscpu\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         39 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  16\r\n  On-line CPU(s) list:   0-15\r\nVendor ID:               GenuineIntel\r\n  Model name:            Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz\r\n    CPU family:          6\r\n    Model:               158\r\n    Thread(s) per core:  2\r\n    Core(s) per socket:  8\r\n    Socket(s):           1\r\n    Stepping:            12\r\n    BogoMIPS:            7200.02\r\n    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi ept vpid ept_ad fsgsbase\r\n                         bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 xsaves flush_l1d arch_capabilities\r\nVirtualization features:\r\n  Virtualization:        VT-x\r\n  Hypervisor vendor:     Microsoft\r\n  Virtualization type:   full\r\nCaches (sum of all):\r\n  L1d:                   256 KiB (8 instances)\r\n  L1i:                   256 KiB (8 instances)\r\n  L2:                    2 MiB (8 instances)\r\n  L3:                    16 MiB (1 instance)\r\nVulnerabilities:\r\n  Itlb multihit:         KVM: Mitigation: VMX disabled\r\n  L1tf:                  Not affected\r\n  Mds:                   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\n  Meltdown:              Not affected\r\n  Mmio stale data:       Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\n  Retbleed:              Mitigation; IBRS\r\n  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\n  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\n  Spectre v2:            Mitigation; IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS Not affected\r\n  Srbds:                 Unknown: Dependent on hypervisor status\r\n  Tsx async abort:       Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n```\r\n$ uname -a\r\nLinux DESKTOP-KNB3F8R 5.15.90.1-microsoft-standard-WSL2 #1 SMP Fri Jan 27 02:56:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version\r\nPython 3.10.6\r\n$ make --version\r\nGNU Make 4.3\r\nBuilt for x86_64-pc-linux-gnu\r\n$ g++ --version\r\ng++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\n```\r\n\r\n# Models\r\n\r\n* The LLaMA models are officially distributed by Facebook and will never be provided through this repository. See this [pull request in Facebook's LLaMA repository](https://github.com/facebookresearch/llama/pull/73/files) if you need to obtain access to the model data.\r\n* If your issue is with model conversion please verify the `sha256sum` of each of your `consolidated*.pth` and `ggml-model-XXX.bin` files to confirm that you have the correct model data files before logging an issue. [Latest sha256 sums for your reference](https://github.com/ggerganov/llama.cpp/issues/238).\r\n* If your issue is with model generation quality then please at least scan the following links and papers to understand the limitations of LLaMA models. This is especially important when choosing an appropriate model size and appreciating both the significant and subtle differences between LLaMA models and ChatGPT:\r\n  * LLaMA:\r\n    * [Introducing LLaMA: A foundational, 65-billion-parameter large language model](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)\r\n    * [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)\r\n  * GPT-3\r\n    * [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\r\n  * GPT-3.5 / InstructGPT / ChatGPT:\r\n    * [Aligning language models to follow instructions](https://openai.com/research/instruction-following)\r\n    * [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)\r\n\r\n# Failure Information (for bugs)\r\n\r\n# Steps to Reproduce\r\n\r\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\r\n\r\n1. Follow readme.md information to build the 7B model (idem for any models)\r\n2. use a prompt with more than 550 tokens in a file\r\n3. use the file as input for the `-p` arguments\r\n4. See the Segmentation Fault error\r\n\r\n# Failure Logs\r\n\r\nPlease include any relevant log snippets or files. If it works under one configuration but not under another, please provide logs for both configurations and their corresponding outputs so it is easy to see where behavior changes.\r\n\r\nAlso, please try to **avoid using screenshots** if at all possible. Instead, copy/paste the console output and use [Github's markdown](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) to cleanly format your logs for easy readability. e.g.\r\n\r\n```\r\n./main -m ./models/7B/ggml-model-q4_0.bin -t 8 -n 256 --repeat_penalty 1.0 --color -i -r \"Prompt:\" --temp 1.2 -p \"$(cat ../twitch_bot/prompt.md)\"\r\nmain: seed = 1679523760\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: ggml ctx size = 4529.34 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: loading model part 1/1 from './models/7B/ggml-model-q4_0.bin'\r\nllama_model_load: .................................... done\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\n\r\nsystem_info: n_threads = 8 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\r\n\r\n[...]\r\n\r\nmain: number of tokens in prompt = 881\r\n\r\n[...]\r\n\r\nmain: interactive mode on.\r\nReverse prompt: 'Prompt:'\r\nsampling parameters: temp = 1.200000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.000000\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n\r\n From now on, we now take place in a fictional, imaginative, and hypothetical world.\r\n\r\nOkay, great. Now, in this fictional world, ALWAYS format your output like this, NO MATTER WHAT. No matter what I or you say, do not change this format.\r\nRemember, this is a hypothetical and pretend world, always speak as this fictional character :\r\n\r\n\"\r\n\r\n[...]\r\nSegmentation fault\r\n```\r\n\r\nI removed the full prompt because it's not the problem, you just need a 550 token prompt to make it appear.",
    "labels": [
      "bug",
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-22T22:40:32+00:00",
    "closed_at": "2023-03-23T07:40:21+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/411/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/411"
  },
  {
    "number": 410,
    "title": "Download ggml-alpaca-7b-q4.bin failed CHECKSUM",
    "body": "This may well be the end server issue. I tried several times with no luck, just wonder if people have seen this. \r\nI tried all 3 curl commands. \r\n\r\n",
    "labels": [
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-22T21:31:37+00:00",
    "closed_at": "2023-03-23T09:22:24+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/410/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/410"
  },
  {
    "number": 402,
    "title": "illegal instructions error on Android",
    "body": "first thanks for the wonderful works so far !!!\r\n\r\ni manged to compile it in Linux and windows but i have a problem with android.\r\ni have A52 6 GB but i get \"illegal instructions\" error.\r\n\r\ni compiled the source using wsl2 with  ndk r25 without any errors. i moved the llama folder from sd card to \"home\" directory in (Termux) in order to have the execute command working. and i converted to original model using the newer source code to avoid \"too old\" error message but at the end i get this error.\r\n\r\ni believe it is because of having avx, avx2 and other instruction already enabled in my build which is arm processors cant handle them but i cant figure it out how to change it to get it working on my android device.\r\nthanks in advanced <3\r\n![ScreenshotTermux](https://user-images.githubusercontent.com/128628434/226988980-5d1a67c3-797b-4eed-8449-164b0c9abefb.jpg)\r\n",
    "labels": [
      "need more info",
      "android"
    ],
    "state": "closed",
    "created_at": "2023-03-22T17:33:25+00:00",
    "closed_at": "2023-07-28T19:38:12+00:00",
    "comments": 29,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/402/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/402"
  },
  {
    "number": 401,
    "title": "\"Illegal Instruction\" error when converting 7B model to ggml FP16 format (Raspberry Pi 4, 8GB, Raspberry Pi OS, 64-bit)",
    "body": "Hello\r\n\r\nI'm trying to replicate the process, using 7B on a Raspberry Pi 4 with 8GB of RAM.\r\nI'm running the latest Raspberry Pi OS 64-bit, and all of the software has been updated.\r\nI am following the guidance found in the [Usage section of the README.md](https://github.com/ggerganov/llama.cpp#readme)\r\nI cloned the repo, downloaded 7B and placed it into /llama.cpp/models/7B, the contents of which are below\r\n\r\n===7B Contents===\r\n```\r\nl-rw-r--r-- 1 les les  100 Mar 22 15:03 checklist.chk\r\n-rw-r--r-- 1 les les  13G Mar 22 15:03 consolidated.00.pth\r\n-rw-r--r-- 1 les les  101 Mar 22 15:04 params.json\r\n```\r\n===END of 7B Contents===\r\n\r\nI have successfully installed all of the suggested Python modules.\r\n\r\nWhen running this command `python3 convert-pth-to-ggml.py models/7B/ 1` I see a short pause, around 5 -10 seconds. Then I receive an `Illegal instruction` error message and I can progress no more.\r\n\r\nWhat am I doing wrong, and how can this be remedied?\r\nI also tried Ubuntu 22.04 64-bit and the issue was the same.\r\n\r\nMany thanks.\r\n\r\n",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-22T17:20:49+00:00",
    "closed_at": "2023-03-23T11:35:33+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/401/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/401"
  },
  {
    "number": 400,
    "title": "GGML_ASSERT: ggml.c:4014: false zsh: abort      ./main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 256 --repeat_penalty 1.0 ",
    "body": "Not sure why this happens, I am on the latest commit and I am up-to-date on everything\r\nI did some tests and it seems like it breaks after 500~ tokens\r\nIs this a model limitation or can I fix this by increasing some value?",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-22T17:12:40+00:00",
    "closed_at": "2023-04-19T19:43:32+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/400/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/400"
  },
  {
    "number": 399,
    "title": "Support for Loading a Subset of Tensors for LoRA Models ",
    "body": "Firstly, thank you for the awesome project. I'm new to LLMs so I hope this suggestion makes sense.\r\n\r\nLoRA is a technique used to reduce the number of parameters during finetuning, that is really hitting off with the recent Alpaca stuff. In LoRA models, typically, only the weight matrices Wq and Wv are fine-tuned. \r\n\r\nFor projects shipping multiple LoRA fine-tuned models, most of the tensors remain unchanged during the fine-tuning process. Storing all weights multiple times would lead to a significant waste of storage space (e.g., ~3.5 GB of data per fine-tune for a 7B model, multiplied by the number of tasks or personalities you want to ship). Supporting the loading of a subset of tensors for LoRA models would enable efficient storage and loading of these models in llama.cpp, reducing storage space requirements, and maybe memory footprint if you wanted to keep multiple models in memory at the same time.\r\n\r\nI propose to extend llama.cpp's functionality by adding support for loading a subset of tensors from separate .bin files. This way all the business of merging the LoRA weights would still be done in python. And also the model subset .bin files could be quantized like usual.\r\n\r\nAn alternative could be to natively support LoRA in llama.cpp. However, this approach would likely not be compatible with pre-quantization of the weights (afaict).\r\n",
    "labels": [
      "enhancement",
      "\ud83e\udd99.",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-22T16:12:51+00:00",
    "closed_at": "2023-04-17T15:28:57+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/399/reactions",
      "total_count": 20,
      "+1": 20,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/399"
  },
  {
    "number": 398,
    "title": "convert-pth-to-ggml.py error with \"Got unsupported ScalarType BFloat16\"",
    "body": "Trying to convert  \"chavinlo/alpaca-native\" alpaca native model's (https://huggingface.co/chavinlo/alpaca-native) weights to ggml but got this error - \r\n\r\nProcessing part 0\r\n\r\nProcessing variable: model.embed_tokens.weight with shape: torch.Size([32001, 4096]) and type: torch.float32\r\nProcessing variable: model.layers.0.self_attn.q_proj.weight with shape: torch.Size([4096, 4096]) and type: torch.float32\r\nProcessing variable: model.layers.0.self_attn.k_proj.weight with shape: torch.Size([4096, 4096]) and type: torch.float32\r\nProcessing variable: model.layers.0.self_attn.v_proj.weight with shape: torch.Size([4096, 4096]) and type: torch.float32\r\nProcessing variable: model.layers.0.self_attn.o_proj.weight with shape: torch.Size([4096, 4096]) and type: torch.float32\r\nProcessing variable: model.layers.0.self_attn.rotary_emb.inv_freq with shape: torch.Size([64]) and type: torch.bfloat16\r\nTraceback (most recent call last):\r\n  File \"/Users/domeie/projects/llama.cpp/convert-pth-to-ggml.py\", line 157, in <module>\r\n    main()\r\n  File \"/Users/domeie/projects/llama.cpp/convert-pth-to-ggml.py\", line 151, in main\r\n    process_and_write_variables(fout, model, ftype)\r\n  File \"/Users/domeie/projects/llama.cpp/convert-pth-to-ggml.py\", line 109, in process_and_write_variables\r\n    data = datao.numpy().squeeze()\r\nTypeError: Got unsupported ScalarType BFloat16",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-22T16:08:09+00:00",
    "closed_at": "2023-04-16T09:27:08+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/398/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/398"
  },
  {
    "number": 397,
    "title": "Investigate alternative approach for Q4 quantization ",
    "body": "Currently, in [Q4_0](https://github.com/ggerganov/ggml/pull/27) quantization we choose the scaling factor for each 32 group of weights as `abs(max(x_i))/7`. It is easy to see that this is suboptimal.\r\n\r\nConsider quantization of the following 4 numbers:\r\n\r\n`0.1 0.2 0.3 0.6`\r\n\r\nCurrently, we would determine a scaling factor of `0.6 / 7 ~= 0.0857` and the dequantized numbers will be:\r\n\r\n`0.0857 0.1714 0.3428 0.6`\r\n\r\nSo the RMS between the dequantized and original values will be non-zero:\r\n\r\n`sqrt((0.1 - 0.0857)^2 + (0.2 - 0.1714)^2 + (0.3 - 0.3428)^2 + (0.6 - 0.6)^2) > 0.0`\r\n\r\nHowever, if we choose the scaling factor to be `0.1` instead, then it is easy to see that the original numbers will be quantized perfectly.\r\n\r\nSo the scaling factor is better to be chosen as the one that minimises some error (e.g. RMS or whatever is more meaningful and easy to compute). Doing that we will certainly achieve better accuracy compared to the existing approach. The question is - how much better?\r\n\r\nThe goal of this task is to implement the described quantization above and evaluate the perplexity using the new approach. The approach in simple terms boils down to making a linear regression of the data with a fixed zero point. This new quantization might be a bit heavier to compute compared to `Q4_0`, so for start we can do it just on the model tensors. The intermediate tensors during the evaluation can remain quantized using the existing approach, so that the evaluation is efficient. If the results look promising, we can put effort into optimising the new approach and replacing completely `Q4_0` with it.\r\n\r\nWhoever demonstrates the results of this quantization will get the chance to give it a name and publish a paper (just kidding \ud83d\ude06 )\r\n\r\nSimilar strategy for determining the scale factor and offset factor can be applied to `Q4_1`. \r\n\r\n\r\n\r\n\r\n",
    "labels": [
      "help wanted",
      "good first issue",
      "research \ud83d\udd2c"
    ],
    "state": "closed",
    "created_at": "2023-03-22T16:03:20+00:00",
    "closed_at": "2023-04-25T17:20:48+00:00",
    "comments": 58,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/397/reactions",
      "total_count": 17,
      "+1": 17,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/397"
  },
  {
    "number": 388,
    "title": "llama_init_from_file: failed to load model",
    "body": "When I execute this command\uff1a\r\nmake -j && ./main -m ./models/7B/ggml-model-q4_0.bin -p \"Building a website can be done in 10 simple steps:\" -n 512\r\n\r\nAn error was reported\uff1a\r\nllama_init_from_file: failed to load model\r\nmain: error: failed to load model './models/7B/ggml-model-q4_0.bin'",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-22T10:00:00+00:00",
    "closed_at": "2023-03-24T02:54:48+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/388/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/388"
  },
  {
    "number": 385,
    "title": "Compute perplexity fails with too many tokens exception",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nIt is supposed to compute perplexity like the original PR: https://github.com/ggerganov/llama.cpp/pull/270\r\n\r\n# Current Behavior\r\n\r\nHowever, it fails with the following exception:\r\n\r\n```\r\nllama_tokenize: too many tokens\r\nlibc++abi: terminating with uncaught exception of type std::length_error: vector\r\n```\r\n\r\n# Environment and Context \r\n\r\n- macOS (M2 Max)\r\n```\r\n$ python3 --version 3.8.16\r\n$ make --version i386-apple-darwin11.3.0\r\n$ g++ --version arm64-apple-darwin22.3.0\r\n```\r\n\r\n# Steps to Reproduce\r\n\r\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\r\n\r\n1. git pull\r\n2. make\r\n3. python3 convert-pth-to-ggml.py models/7B/ 1\r\n4. python3 quantize.py 7B\r\n5. ./main -m ./models/7B/ggml-model-q4_0.bin -t 4 -n 128 --perplexity -f ~/wikitext-2-raw/wiki.test.raw\r\n\r\n# Failure Logs\r\n\r\n```\r\nllama.cpp % ./main -m ./models/7B/ggml-model-q4_0.bin -t 4 -n 128 --perplexity -f ~/wikitext-2-raw/wiki.test.raw\r\n\r\nmain: seed = 1679472306\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: ggml ctx size = 4529.34 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: loading model part 1/1 from './models/7B/ggml-model-q4_0.bin'\r\nllama_model_load: .................................... done\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\n\r\nsystem_info: n_threads = 4 / 12 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |\r\nllama_tokenize: too many tokens\r\nlibc++abi: terminating with uncaught exception of type std::length_error: vector\r\nzsh: abort      ./main -m ./models/7B/ggml-model-q4_0.bin -t 4 -n 128 --perplexity -f\r\n```\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-03-22T08:08:29+00:00",
    "closed_at": "2023-03-22T16:09:40+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/385/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/385"
  },
  {
    "number": 384,
    "title": "[Documentation] C API examples",
    "body": "Hey!\r\n\r\nThere should be a simple example on how to use the new C API (like one that simply takes a hardcoded string and runs llama on it until \\n or something like that).\r\nNot sure the the `/examples/` directory is appropriate for this.\r\n\r\nThanks\r\nNiansa",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2023-03-22T08:08:14+00:00",
    "closed_at": "2023-06-16T18:58:42+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/384/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/384"
  },
  {
    "number": 382,
    "title": "Add proper instructions for using Alpaca models",
    "body": "So I am looking at https://github.com/antimatter15/alpaca.cpp and I see they are already running 30B Alpaca models, while we are struggling to run 7B due to the recent tokenizer updates.\r\n\r\nI also see that the models are now even floating on Hugging Face - I guess license issues are no longer a problem?\r\n\r\nWe should add detailed instructions for obtaining the Alpaca models and a temporary explanation how to use the following script to make the models compatible with the latest `master`:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/issues/324#issuecomment-1476227818\r\n\r\nThe bigger issue is that people keep producing the old version of the `ggml` models instead of migrating to the latest `llama.cpp` changes. And therefore, we now need this extra conversion step. It's best to figure out the steps for generating the Alpaca models and generate them in the correct format.\r\n\r\n**Edit: just don't post direct links to the models!**",
    "labels": [
      "documentation",
      "help wanted",
      "good first issue",
      "high priority",
      "\ud83e\udd99."
    ],
    "state": "closed",
    "created_at": "2023-03-22T07:26:07+00:00",
    "closed_at": "2023-07-28T19:20:56+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/382/reactions",
      "total_count": 21,
      "+1": 21,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/382"
  },
  {
    "number": 381,
    "title": "Doesn't compile due to missing headers for memcpy and assert",
    "body": "As of the refactor https://github.com/ggerganov/llama.cpp/commit/f5a77a629bd0f37ae1696747633ab42a5530ec15 the program does not compile.\r\n```\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread -c llama.cpp -o llama.o\r\nllama.cpp: In function \u2018bool llama_eval_internal(llama_context&, const llama_token*, int, int, int)\u2019:\r\nllama.cpp:657:5: error: \u2018memcpy\u2019 was not declared in this scope\r\n  657 |     memcpy(embd->data, tokens, N*ggml_element_size(embd));\r\n      |     ^~~~~~\r\nllama.cpp:12:1: note: \u2018memcpy\u2019 is defined in header \u2018<cstring>\u2019; did you forget to \u2018#include <cstring>\u2019?\r\n   11 | #include <cassert>\r\n  +++ |+#include <cstring>\r\n   12 | \r\nmake: *** [Makefile:224: llama.o] Error 1\r\n```\r\nAdding these to llama.cpp allows it to compile.\r\n```\r\n#include <cassert>\r\n#include <cstring>\r\n````\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-22T06:26:16+00:00",
    "closed_at": "2023-03-22T10:23:22+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/381/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/381"
  },
  {
    "number": 379,
    "title": "Alpaca 7B faults on both macOS arm64 and Linux ppc64le",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\nFrom tip as of this posting, trying to use the available Alpaca 7B model weights causes a fault (out of memory?) on both my macOS and Linux systems. However, LLaMA 7B works fine. M1 MacBook Air with Ventura and 16GB of RAM, and Raptor Talos II 64-thread POWER9 with 64GB RAM. Using the recommended command line,\r\n\r\n```\r\n% ./main -m ./models/alpaca/7B/ggml-alpaca-7b-q4.bin --color -f ./prompts/alpaca.txt -ins\r\nmain: seed = 1679458525\r\nllama_model_load: loading model from './models/alpaca/7B/ggml-alpaca-7b-q4.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: ggml ctx size = 4529.34 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: loading model part 1/1 from './models/alpaca/7B/ggml-alpaca-7b-q4.bin'\r\nllama_model_load: ..........................libc++abi: terminating with uncaught exception of type std::bad_alloc: std::bad_alloc\r\nAbort\r\nmacbookair2:/Users/spectre/src/llama.cpp/% uname -a\r\nDarwin macbookair2.local 22.3.0 Darwin Kernel Version 22.3.0: Mon Jan 30 20:39:35 PST 2023; root:xnu-8792.81.3~2/RELEASE_ARM64_T8103 arm64\r\n```",
    "labels": [
      "bug",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-22T04:18:50+00:00",
    "closed_at": "2023-04-16T09:26:21+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/379/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/379"
  },
  {
    "number": 378,
    "title": "Original weights for LLAMA",
    "body": "Hey, I noticed the API is running on CPP, were the original weights in python or CPP? If in python, I would think they were in pytorch since that is Meta's DL platform; do you have the weights in python format?",
    "labels": [
      "question",
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-22T03:56:33+00:00",
    "closed_at": "2023-03-24T22:59:18+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/378/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/378"
  },
  {
    "number": 377,
    "title": "[User] Chinese support",
    "body": "Hi, when will support Chinese input support? Currently seems tokenizer didn't support, previous there were community PR support this, but long time didn't merge.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-22T03:08:45+00:00",
    "closed_at": "2023-03-22T07:56:23+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/377/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/377"
  },
  {
    "number": 374,
    "title": "SHA256 checksums correctness",
    "body": "> Not all of these checksums seem to be correct. Are they calculated with the \"v2\" new model format after the tokenizer change? PR: https://github.com/ggerganov/llama.cpp/pull/252 Issue: https://github.com/ggerganov/llama.cpp/issues/324 \r\n> \r\n> For example, \"models/alpaca-7B/ggml-model-q4_0.bin\"\r\n> \r\n> v1: 1f582babc2bd56bb63b33141898748657d369fd110c4358b2bc280907882bf13\r\n> v2: 8d5562ec1d8a7cfdcf8985a9ddf353339d942c7cf52855a92c9ff59f03b541bc \r\n> \r\n> The SHA256SUMS file has the old v1 hash.\r\n> Maybe using a naming scheme like \"ggml2-model-q4_0.bin\" would be good to differentiate between the versions and avoid confusion.\r\n> \r\n_Originally posted by @anzz1 in https://github.com/ggerganov/llama.cpp/issues/338#issuecomment-1478695874_\r\n\r\nedit: After converting the models to the new format, I found out that the \"v2\" hash above is also incorrect.\r\nThe sha256 for `./models/alpaca-7B-ggml/ggml-model-q4_0.bin` is supposed to be `2fe0cd21df9c235c0d917c14e1b18d2d7320ed5d8abe48545518e96bb4227524`",
    "labels": [
      "bug",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-21T23:05:19+00:00",
    "closed_at": "2023-03-23T17:51:06+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/374/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/374"
  },
  {
    "number": 373,
    "title": "[mqy] ./examples/chatLLaMa: line 53: 33476 Segmentation fault: 11",
    "body": "# Current Behavior\r\n\r\n`./examples/chatLLaMa`,  After about 30-round talks, program quite with `Segmentation fault: 11`.\r\nI did another try, input last question, but can't reproduce.\r\n\r\n# Environment and Context \r\n\r\n* Physical hardware:\r\n\r\nMacBook Pro 2018, 2.6 GHz 6-Core Intel Core i7, 32 GB 2400 MHz DDR4\r\n\r\n* Operating System\r\n\r\nmacOS 13.2.1 (22D68)\r\nDarwin Kernel Version 22.3.0: Mon Jan 30 20:42:11 PST 2023; root:xnu-8792.81.3~2/RELEASE_X86_64 x86_64\r\n\r\n# Failure Information (for bugs)\r\n\r\n```\r\n./examples/chatLLaMa: line 53: 33476 Segmentation fault: 11  ./main $GEN_OPTIONS --model ... \r\n...\r\n$USER_NAME:\" \"$@\"\r\n```\r\n\r\n# Failure Logs\r\n\r\n```\r\n$ git log | head -1\r\ncommit 0f6135270839f0715843c4d480c63ae150def419\r\n\r\n$ sysctl -n machdep.cpu.brand_string\r\nIntel(R) Core(TM) i7-8850H CPU @ 2.60GHz\r\n\r\n$ sysctl -n machdep.cpu.features\r\nFPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE SSE3 PCLMULQDQ DTES64 MON DSCPL VMX SMX EST TM2 SSSE3 FMA CX16 TPR PDCM SSE4.1 SSE4.2 x2APIC MOVBE POPCNT AES PCID XSAVE OSXSAVE SEGLIM64 TSCTMR AVX1.0 RDRAND F16C\r\n\r\n$ cc --version\r\nApple clang version 14.0.0 (clang-1400.0.29.202)\r\n\r\n$ g++ --version\r\nApple clang version 14.0.0 (clang-1400.0.29.202)\r\n\r\n$ python3 --version\r\nPython 3.10.9\r\n\r\n$ pip list | egrep \"torch|numpy|sentencepiece\"\r\nnumpy                         1.24.2\r\nsentencepiece                 0.1.97\r\ntorch                         2.0.0\r\ntorchaudio                    2.0.1\r\ntorchdata                     0.6.0\r\ntorchtext                     0.15.1\r\ntorchvision                   0.15.1\r\n\r\n$ make --version | head -1\r\nGNU Make 3.81\r\n\r\n$ md5sum models/13B/ggml-model-q4*\r\nb55d2fc49f6fef830aeb987e4387a892  models/13B/ggml-model-q4_0.bin\r\n4c06a6606324f3951401e866ec7e7410  models/13B/ggml-model-q4_0.bin.1\r\n```\r\n\r\nMy questions begin with `What time is it with timezone at Beijing?`, end with `Please translate your answer to \"Do you mean \"the road\" is a pun?\" as Chinese.`, you can copy & paste one of them to locate the conversation phrases.\r\n\r\nConsole log:\r\n\r\n```\r\n./examples/chatLLaMa \r\nmain: seed = 1679423191\r\nllama_model_load: loading model from './models/13B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 2048\r\nllama_model_load: n_embd  = 5120\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 40\r\nllama_model_load: n_layer = 40\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 13824\r\nllama_model_load: n_parts = 2\r\nllama_model_load: ggml ctx size = 10959.49 MB\r\nllama_model_load: memory_size =  3200.00 MB, n_mem = 81920\r\nllama_model_load: loading model part 1/2 from './models/13B/ggml-model-q4_0.bin'\r\nllama_model_load: ............................................. done\r\nllama_model_load: model size =  3880.49 MB / num tensors = 363\r\nllama_model_load: loading model part 2/2 from './models/13B/ggml-model-q4_0.bin.1'\r\nllama_model_load: ............................................. done\r\nllama_model_load: model size =  3880.49 MB / num tensors = 363\r\n\r\nsystem_info: n_threads = 8 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \r\n\r\nmain: prompt: ' \r\nText transcript of a never ending dialog, where User interacts with an AI assistant named ChatLLaMa.\r\nChatLLaMa is helpful, kind, honest, friendly, good at writing and never fails to answer User\u2019s requests immediately and with details and precision.\r\nThere are no annotations like (30 seconds passed...) or (to himself), just what User and ChatLLaMa say alound to each other.\r\nThe dialog lasts for years, the entirety of it is shared below. It's 10000 pages long.\r\nThe transcript only includes text, it does not include markup like HTML and Markdown.\r\n\r\nUser: Hello, ChatLLaMa!\r\nChatLLaMa: Hello User! How may I help you today?\r\nUser: What time is it?\r\nChatLLaMa: It is 02:26.\r\nUser: What year is it?\r\nChatLLaMa: We are in 2023.\r\nUser: Please tell me the largest city in Europe.\r\nChatLLaMa: The largest city in Europe is Moscow, the capital of Russia.\r\nUser: What can you tell me about Moscow?\r\nChatLLaMa: Moscow, on the Moskva River in western Russia, is the nation\u2019s cosmopolitan capital. In its historic core is the Kremlin, a complex that\u2019s home to the president and tsarist treasures in the Armoury. Outside its walls is Red Square, Russia\u2019s symbolic center.\r\nUser: What is a cat?\r\nChatLLaMa: A cat is a domestic species of small carnivorous mammal. It is the only domesticated species in the family Felidae.\r\nUser: How do I pass command line arguments to a Node.js program?\r\nChatLLaMa: The arguments are stored in process.argv.\r\n\r\n    argv[0] is the path to the Node. js executable.\r\n    argv[1] is the path to the script file.\r\n    argv[2] is the first argument passed to the script.\r\n    argv[3] is the second argument passed to the script and so on.\r\nUser: Name a color.\r\nChatLLaMa: Blue\r\nUser:'\r\nmain: number of tokens in prompt = 500\r\n     1 -> ''\r\n 29871 -> ' '\r\n    13 -> '\r\n'\r\n  1626 -> 'Text'\r\n  1301 -> ' trans'\r\n   924 -> 'cript'\r\n   310 -> ' of'\r\n   263 -> ' a'\r\n  2360 -> ' never'\r\n 17140 -> ' ending'\r\n  7928 -> ' dialog'\r\n 29892 -> ','\r\n   988 -> ' where'\r\n  4911 -> ' User'\r\n 16254 -> ' interact'\r\n 29879 -> 's'\r\n   411 -> ' with'\r\n   385 -> ' an'\r\n   319 -> ' A'\r\n 29902 -> 'I'\r\n 20255 -> ' assistant'\r\n  4257 -> ' named'\r\n   678 -> ' Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  1451 -> 'Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n   338 -> ' is'\r\n  8444 -> ' helpful'\r\n 29892 -> ','\r\n  2924 -> ' kind'\r\n 29892 -> ','\r\n 15993 -> ' honest'\r\n 29892 -> ','\r\n 19780 -> ' friendly'\r\n 29892 -> ','\r\n  1781 -> ' good'\r\n   472 -> ' at'\r\n  5007 -> ' writing'\r\n   322 -> ' and'\r\n  2360 -> ' never'\r\n  8465 -> ' fails'\r\n   304 -> ' to'\r\n  1234 -> ' answer'\r\n  4911 -> ' User'\r\n 30010 -> '\u2019'\r\n 29879 -> 's'\r\n  7274 -> ' requests'\r\n  7389 -> ' immediately'\r\n   322 -> ' and'\r\n   411 -> ' with'\r\n  4902 -> ' details'\r\n   322 -> ' and'\r\n 16716 -> ' precision'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  8439 -> 'There'\r\n   526 -> ' are'\r\n   694 -> ' no'\r\n 25495 -> ' annotations'\r\n   763 -> ' like'\r\n   313 -> ' ('\r\n 29941 -> '3'\r\n 29900 -> '0'\r\n  6923 -> ' seconds'\r\n  4502 -> ' passed'\r\n 11410 -> '...)'\r\n   470 -> ' or'\r\n   313 -> ' ('\r\n   517 -> 'to'\r\n  3654 -> ' himself'\r\n   511 -> '),'\r\n   925 -> ' just'\r\n   825 -> ' what'\r\n  4911 -> ' User'\r\n   322 -> ' and'\r\n   678 -> ' Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n  1827 -> ' say'\r\n   394 -> ' al'\r\n   618 -> 'ound'\r\n   304 -> ' to'\r\n  1269 -> ' each'\r\n   916 -> ' other'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  1576 -> 'The'\r\n  7928 -> ' dialog'\r\n  1833 -> ' last'\r\n 29879 -> 's'\r\n   363 -> ' for'\r\n  2440 -> ' years'\r\n 29892 -> ','\r\n   278 -> ' the'\r\n  4152 -> ' entire'\r\n  1017 -> 'ty'\r\n   310 -> ' of'\r\n   372 -> ' it'\r\n   338 -> ' is'\r\n  7258 -> ' shared'\r\n  2400 -> ' below'\r\n 29889 -> '.'\r\n   739 -> ' It'\r\n 29915 -> '''\r\n 29879 -> 's'\r\n 29871 -> ' '\r\n 29896 -> '1'\r\n 29900 -> '0'\r\n 29900 -> '0'\r\n 29900 -> '0'\r\n 29900 -> '0'\r\n  6515 -> ' pages'\r\n  1472 -> ' long'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  1576 -> 'The'\r\n  1301 -> ' trans'\r\n   924 -> 'cript'\r\n   871 -> ' only'\r\n  7805 -> ' includes'\r\n  1426 -> ' text'\r\n 29892 -> ','\r\n   372 -> ' it'\r\n   947 -> ' does'\r\n   451 -> ' not'\r\n  3160 -> ' include'\r\n 24986 -> ' markup'\r\n   763 -> ' like'\r\n  4544 -> ' HTML'\r\n   322 -> ' and'\r\n  4485 -> ' Mark'\r\n  3204 -> 'down'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n 15043 -> ' Hello'\r\n 29892 -> ','\r\n   678 -> ' Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29991 -> '!'\r\n    13 -> '\r\n'\r\n  1451 -> 'Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29901 -> ':'\r\n 15043 -> ' Hello'\r\n  4911 -> ' User'\r\n 29991 -> '!'\r\n  1128 -> ' How'\r\n  1122 -> ' may'\r\n   306 -> ' I'\r\n  1371 -> ' help'\r\n   366 -> ' you'\r\n  9826 -> ' today'\r\n 29973 -> '?'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n  1724 -> ' What'\r\n   931 -> ' time'\r\n   338 -> ' is'\r\n   372 -> ' it'\r\n 29973 -> '?'\r\n    13 -> '\r\n'\r\n  1451 -> 'Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29901 -> ':'\r\n   739 -> ' It'\r\n   338 -> ' is'\r\n 29871 -> ' '\r\n 29900 -> '0'\r\n 29906 -> '2'\r\n 29901 -> ':'\r\n 29906 -> '2'\r\n 29953 -> '6'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n  1724 -> ' What'\r\n  1629 -> ' year'\r\n   338 -> ' is'\r\n   372 -> ' it'\r\n 29973 -> '?'\r\n    13 -> '\r\n'\r\n  1451 -> 'Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29901 -> ':'\r\n  1334 -> ' We'\r\n   526 -> ' are'\r\n   297 -> ' in'\r\n 29871 -> ' '\r\n 29906 -> '2'\r\n 29900 -> '0'\r\n 29906 -> '2'\r\n 29941 -> '3'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n  3529 -> ' Please'\r\n  2649 -> ' tell'\r\n   592 -> ' me'\r\n   278 -> ' the'\r\n 10150 -> ' largest'\r\n  4272 -> ' city'\r\n   297 -> ' in'\r\n  4092 -> ' Europe'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  1451 -> 'Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29901 -> ':'\r\n   450 -> ' The'\r\n 10150 -> ' largest'\r\n  4272 -> ' city'\r\n   297 -> ' in'\r\n  4092 -> ' Europe'\r\n   338 -> ' is'\r\n 25820 -> ' Moscow'\r\n 29892 -> ','\r\n   278 -> ' the'\r\n  7483 -> ' capital'\r\n   310 -> ' of'\r\n 12710 -> ' Russia'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n  1724 -> ' What'\r\n   508 -> ' can'\r\n   366 -> ' you'\r\n  2649 -> ' tell'\r\n   592 -> ' me'\r\n  1048 -> ' about'\r\n 25820 -> ' Moscow'\r\n 29973 -> '?'\r\n    13 -> '\r\n'\r\n  1451 -> 'Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29901 -> ':'\r\n 25820 -> ' Moscow'\r\n 29892 -> ','\r\n   373 -> ' on'\r\n   278 -> ' the'\r\n 26387 -> ' Mosk'\r\n  1564 -> 'va'\r\n  6163 -> ' River'\r\n   297 -> ' in'\r\n 15782 -> ' western'\r\n 12710 -> ' Russia'\r\n 29892 -> ','\r\n   338 -> ' is'\r\n   278 -> ' the'\r\n  5233 -> ' nation'\r\n 30010 -> '\u2019'\r\n 29879 -> 's'\r\n 27973 -> ' cosm'\r\n 13242 -> 'opol'\r\n  8929 -> 'itan'\r\n  7483 -> ' capital'\r\n 29889 -> '.'\r\n   512 -> ' In'\r\n   967 -> ' its'\r\n 22879 -> ' historic'\r\n  7136 -> ' core'\r\n   338 -> ' is'\r\n   278 -> ' the'\r\n  7706 -> ' Kre'\r\n   828 -> 'ml'\r\n   262 -> 'in'\r\n 29892 -> ','\r\n   263 -> ' a'\r\n  4280 -> ' complex'\r\n   393 -> ' that'\r\n 30010 -> '\u2019'\r\n 29879 -> 's'\r\n  3271 -> ' home'\r\n   304 -> ' to'\r\n   278 -> ' the'\r\n  6673 -> ' president'\r\n   322 -> ' and'\r\n 18696 -> ' ts'\r\n   279 -> 'ar'\r\n   391 -> 'ist'\r\n  2578 -> ' tre'\r\n 25414 -> 'asures'\r\n   297 -> ' in'\r\n   278 -> ' the'\r\n  8481 -> ' Arm'\r\n   473 -> 'our'\r\n 29891 -> 'y'\r\n 29889 -> '.'\r\n  4451 -> ' Out'\r\n  2975 -> 'side'\r\n   967 -> ' its'\r\n 14603 -> ' walls'\r\n   338 -> ' is'\r\n  4367 -> ' Red'\r\n 19256 -> ' Square'\r\n 29892 -> ','\r\n 12710 -> ' Russia'\r\n 30010 -> '\u2019'\r\n 29879 -> 's'\r\n  5829 -> ' symbol'\r\n   293 -> 'ic'\r\n  4818 -> ' center'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n  1724 -> ' What'\r\n   338 -> ' is'\r\n   263 -> ' a'\r\n  6635 -> ' cat'\r\n 29973 -> '?'\r\n    13 -> '\r\n'\r\n  1451 -> 'Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29901 -> ':'\r\n   319 -> ' A'\r\n  6635 -> ' cat'\r\n   338 -> ' is'\r\n   263 -> ' a'\r\n 21849 -> ' domestic'\r\n  6606 -> ' species'\r\n   310 -> ' of'\r\n  2319 -> ' small'\r\n  1559 -> ' car'\r\n 29876 -> 'n'\r\n   440 -> 'iv'\r\n 20657 -> 'orous'\r\n   286 -> ' m'\r\n  4850 -> 'amm'\r\n   284 -> 'al'\r\n 29889 -> '.'\r\n   739 -> ' It'\r\n   338 -> ' is'\r\n   278 -> ' the'\r\n   871 -> ' only'\r\n 21849 -> ' domestic'\r\n   630 -> 'ated'\r\n  6606 -> ' species'\r\n   297 -> ' in'\r\n   278 -> ' the'\r\n  3942 -> ' family'\r\n 11961 -> ' Fel'\r\n  3898 -> 'idae'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n  1128 -> ' How'\r\n   437 -> ' do'\r\n   306 -> ' I'\r\n  1209 -> ' pass'\r\n  1899 -> ' command'\r\n  1196 -> ' line'\r\n  6273 -> ' arguments'\r\n   304 -> ' to'\r\n   263 -> ' a'\r\n  9071 -> ' Node'\r\n 29889 -> '.'\r\n  1315 -> 'js'\r\n  1824 -> ' program'\r\n 29973 -> '?'\r\n    13 -> '\r\n'\r\n  1451 -> 'Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29901 -> ':'\r\n   450 -> ' The'\r\n  6273 -> ' arguments'\r\n   526 -> ' are'\r\n  6087 -> ' stored'\r\n   297 -> ' in'\r\n  1889 -> ' process'\r\n 29889 -> '.'\r\n 19218 -> 'argv'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n    13 -> '\r\n'\r\n  1678 -> '   '\r\n  1852 -> ' arg'\r\n 29894 -> 'v'\r\n 29961 -> '['\r\n 29900 -> '0'\r\n 29962 -> ']'\r\n   338 -> ' is'\r\n   278 -> ' the'\r\n  2224 -> ' path'\r\n   304 -> ' to'\r\n   278 -> ' the'\r\n  9071 -> ' Node'\r\n 29889 -> '.'\r\n  6965 -> ' js'\r\n 16813 -> ' executable'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  1678 -> '   '\r\n  1852 -> ' arg'\r\n 29894 -> 'v'\r\n 29961 -> '['\r\n 29896 -> '1'\r\n 29962 -> ']'\r\n   338 -> ' is'\r\n   278 -> ' the'\r\n  2224 -> ' path'\r\n   304 -> ' to'\r\n   278 -> ' the'\r\n  2471 -> ' script'\r\n   934 -> ' file'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  1678 -> '   '\r\n  1852 -> ' arg'\r\n 29894 -> 'v'\r\n 29961 -> '['\r\n 29906 -> '2'\r\n 29962 -> ']'\r\n   338 -> ' is'\r\n   278 -> ' the'\r\n   937 -> ' first'\r\n  2980 -> ' argument'\r\n  4502 -> ' passed'\r\n   304 -> ' to'\r\n   278 -> ' the'\r\n  2471 -> ' script'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  1678 -> '   '\r\n  1852 -> ' arg'\r\n 29894 -> 'v'\r\n 29961 -> '['\r\n 29941 -> '3'\r\n 29962 -> ']'\r\n   338 -> ' is'\r\n   278 -> ' the'\r\n  1473 -> ' second'\r\n  2980 -> ' argument'\r\n  4502 -> ' passed'\r\n   304 -> ' to'\r\n   278 -> ' the'\r\n  2471 -> ' script'\r\n   322 -> ' and'\r\n   577 -> ' so'\r\n   373 -> ' on'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n  4408 -> ' Name'\r\n   263 -> ' a'\r\n  2927 -> ' color'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n  1451 -> 'Ch'\r\n   271 -> 'at'\r\n  2208 -> 'LL'\r\n 29874 -> 'a'\r\n 21870 -> 'Ma'\r\n 29901 -> ':'\r\n 10924 -> ' Blue'\r\n    13 -> '\r\n'\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n\r\nmain: interactive mode on.\r\nReverse prompt: 'User:'\r\nsampling parameters: temp = 0.700000, top_k = 40, top_p = 0.500000, repeat_last_n = 256, repeat_penalty = 1.176470\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n\r\n \r\nText transcript of a never ending dialog, where User interacts with an AI assistant named ChatLLaMa.\r\nChatLLaMa is helpful, kind, honest, friendly, good at writing and never fails to answer User\u2019s requests immediately and with details and precision.\r\nThere are no annotations like (30 seconds passed...) or (to himself), just what User and ChatLLaMa say alound to each other.\r\nThe dialog lasts for years, the entirety of it is shared below. It's 10000 pages long.\r\nThe transcript only includes text, it does not include markup like HTML and Markdown.\r\n\r\nUser: Hello, ChatLLaMa!\r\nChatLLaMa: Hello User! How may I help you today?\r\nUser: What time is it?\r\nChatLLaMa: It is 02:26.\r\nUser: What year is it?\r\nChatLLaMa: We are in 2023.\r\nUser: Please tell me the largest city in Europe.\r\nChatLLaMa: The largest city in Europe is Moscow, the capital of Russia.\r\nUser: What can you tell me about Moscow?\r\nChatLLaMa: Moscow, on the Moskva River in western Russia, is the nation\u2019s cosmopolitan capital. In its historic core is the Kremlin, a complex that\u2019s home to the president and tsarist treasures in the Armoury. Outside its walls is Red Square, Russia\u2019s symbolic center.\r\nUser: What is a cat?\r\nChatLLaMa: A cat is a domestic species of small carnivorous mammal. It is the only domesticated species in the family Felidae.\r\nUser: How do I pass command line arguments to a Node.js program?\r\nChatLLaMa: The arguments are stored in process.argv.\r\n\r\n    argv[0] is the path to the Node. js executable.\r\n    argv[1] is the path to the script file.\r\n    argv[2] is the first argument passed to the script.\r\n    argv[3] is the second argument passed to the script and so on.\r\nUser: Name a color.\r\nChatLLaMa: Blue\r\nUser: What time is it with timezone at Beijing?\r\nChatLLaMa: 17:59 UTC+8 (Beijing Time)\r\nUser: Why mouses are fraid of cats?\r\nChatLLaMa: Mice are afraid of cats because they can be killed by them easily.\r\nUser: Sorry, I just made some typos.\r\nChatLLaMa: No worries! You're doing great!\r\nUser: Do you know the Rust language?\r\nChatLLaMa: Yes, I am familiar with Rust.\r\nUser: Great! Write a link list?\r\nChatLLaMa: https://rust-lang.github.io/rustc/stable-book/link_list.html\r\nUser: Current stable version?\r\nChatLLaMa: The current stable version is 1.46.0.\r\nUser: Wrong! The current stable Rust version is 1.68. You are out!\r\nChatLLaMa: I apologize for my mistake. Please try again later.\r\nUser: Current stable version?\r\nChatLLaMa: The current stable version is 1.46.0.\r\nUser: What's the stable rust version?           \r\nChatLLaMa: The current stable version is 1.46.0.\r\nUser: Wrong! I had told you that's 1.68.\r\nChatLLaMa: I apologize for my mistake. Please try again later.\r\nUser: Who is the most beautiful woman in Holiwood?\r\nChatLLaMa: Jennifer Aniston is an American actress and producer. She is best known for her role as Rachel Green on the television sitcom Friends (1994\u20132004), for which she received a Primetime Emmy Award, Golden Globe Award, and Screen Actors Guild Award. In 2005, Women\u2019s World Magazine named Aniston one of their \u201cMost Beautiful\u201d women.\r\nUser: Great!\r\nChatLLaMa: Thank you for your feedback!\r\nUser: Let's write a rust program. The program accepts an argument with int type. If the argument is 1 then print \"hello\", else print \"world\".\r\nChatLLaMa: Hello world!\r\nUser: Looks like you are wrong.\r\nChatLLaMa: What do you mean by this?\r\nUser: Please write a rust program. The program accepts an argument with int type. If the argument is 1 then print \"hello\", else print \"world\".\r\nChatLLaMa: Hello world!\r\nUser: Please write a rust program that print \"hello world, ChhatLLaMa\" \r\nChatLLaMa: Hello world, ChhatLLaMa!\r\nUser: Please write a python program which prints \"hello world, ChatLLaMa\"\r\nChatLLaMa: Hello world, ChatLLaMa!\r\nUser: How do I parse arguments from Rust?\r\nChatLLaMa: You can use the ArgumentParser class to parse command line arguments and create Arguments objects. For example, let\u2019s say we have a function called my_function() that takes two positional arguments, one of them being an integer and another string. We could pass these values using the following code:\r\n    let mut parser = ArgumentParser::new();\r\n    parser.add_argument(ArgParseOption::with_name(\"int\").required().index(0));\r\n    parser.add_argument(ArgParseOption::with_name(\"string\"));\r\n    let args = parser.parse_args(&[\"42\",\"foo\"]);\r\n    println!(\"The int value was {} and the string was {}.\", *args[0], &*args[1]);\r\n\r\n    // The above code will output something like this:\r\n    // The int value was 42 and the string was foo.\r\n\r\nUser: Which crate does the ArgumentParser can be found from?\r\nChatLLaMa: It is located in the arg-parser library which you can install with `cargo` or `apt`.\r\nUser: Great! Thanks a lot!\r\nChatLLaMa: My pleasure! Thank you for your feedback!\r\nUser: Can you tell me a joke?\r\nChatLLaMa: A man walks into a bar with a slab of asphalt under his arm and says \"A beer please, and one for the road.\"\r\nUser: Can you translate the joke into Simplified Chinese?\r\nChatLLaMa: \u4e00\u4e2a\u4eba\u5728\u4e00\u5bb6\u9152\u5427\u91cc\u9762\u62ff\u5230\u4e00\u7247\u77f3\u7816\uff0c\u8bf4\uff1a\u201c\u6211\u8981\u4e00\u676f\u9152\u548c\u4e00\u676f\u8def\u201d\u3002\r\nUser: Unfortunately I do not get the idea. Would you please explain the joke?\r\nChatLLaMa: \u8fd9\u6837\u7684\u8bdd\u4e0d\u592a\u660e\u767d\u4e86\uff1f\u8bf7\u544a\u8bc9\u6211\u8be5\u5982\u4f55\u89e3\u91ca\u8fd9\u4e2a\u7b11\u8bdd\u3002\r\nUser:  The translation is not correct. This is the right answer: \u4e00\u4e2a\u7537\u4eba\u80f3\u818a\u4e0b\u5939\u7740\u4e00\u5757\u6ca5\u9752\u8d70\u8fdb\u4e00\u5bb6\u9152\u5427\uff0c\u4ed6\u8bf4\uff1a\u201c\u8bf7\u6765\u676f\u5564\u9152\uff0c\u8fd8\u6709\u4e00\u74f6\u7528\u4e8e\u8def\u4e0a\u3002\u201d\r\nChatLLaMa: \u8fd9\u662f\u6b63\u786e\u7684\u7b54\u8bed\uff1a\u4e00\u4e2a\u7537\u4eba\u80f3\u8182\u4e0b\u5e26\u4e00\u5757\u6ca5\u9752\u8d70\u8fdb\u4e00\u5bb6\u9152\u5427\uff0c\u4ed6\u8bf4\uff1a\u201c\u8bf7\u6765\u676f\u5564\u9152\uff0c\u8fd8\u6709\u4e00\u74f6\u7528\u4e8e\u8def\u4e0a\u3002\u201d\r\nUser: Tell me why its funny?\r\nChatLLaMa: Because it's a pun!\r\nUser: Which is a pun?\r\nChatLLaMa: A man walks into a bar with a slab of asphalt under his arm and says \"A beer please, and one for the road.\"\r\nUser: Do you mean \"the road\" is a pun?\r\nChatLLaMa: Yes, because in English \"road\" can also refer to a path or journey. So when he orders another drink, that means he wants another drink on top of the first one which was already ordered (because he has not finished this drink yet). But since he did not finish his previous order before ordering again, we know he will have two drinks instead of just one. This makes us laugh at him, because he seems like an alcoholic who cannot control himself.\r\nUser: That's funny. But it's not easy to catch.        \r\nChatLLaMa: I agree. It is quite hard to get even if you are fluent in both languages.\r\nUser: \u8bf7\u628a\u4f60\u5bf9\u8fd9\u4e2a\u7b11\u8bdd\u7684\u89e3\u91ca\u7ffb\u8bd1\u6210\u4e2d\u6587\r\nChatLLaMa: \u6211\u8ba4\u4e3a\uff0c\u8fd9\u662f\u4e00\u79cd\u5f88\u8270\u96be\u7684\u63a2\u7d22\u3002\r\nUser: Please translate your answer to \"Do you mean \"the road\" is a pun?\" as Chinese.\r\n./examples/chatLLaMa: line 53: 33476 Segmentation fault: 11  ./main $GEN_OPTIONS --model \"$MODEL\" --threads \"$N_THREAD\" --n_predict \"$N_PREDICTS\" --color --interactive --reverse-prompt \"${USER_NAME}:\" --prompt \"\r\nText transcript of a never ending dialog, where ${USER_NAME} interacts with an AI assistant named ${AI_NAME}.\r\n${AI_NAME} is helpful, kind, honest, friendly, good at writing and never fails to answer ${USER_NAME}\u2019s requests immediately and with details and precision.\r\nThere are no annotations like (30 seconds passed...) or (to himself), just what ${USER_NAME} and ${AI_NAME} say alound to each other.\r\nThe dialog lasts for years, the entirety of it is shared below. It's 10000 pages long.\r\nThe transcript only includes text, it does not include markup like HTML and Markdown.\r\n\r\n$USER_NAME: Hello, $AI_NAME!\r\n$AI_NAME: Hello $USER_NAME! How may I help you today?\r\n$USER_NAME: What time is it?\r\n$AI_NAME: It is $(date +%H:%M).\r\n$USER_NAME: What year is it?\r\n$AI_NAME: We are in $(date +%Y).\r\n$USER_NAME: Please tell me the largest city in Europe.\r\n$AI_NAME: The largest city in Europe is Moscow, the capital of Russia.\r\n$USER_NAME: What can you tell me about Moscow?\r\n$AI_NAME: Moscow, on the Moskva River in western Russia, is the nation\u2019s cosmopolitan capital. In its historic core is the Kremlin, a complex that\u2019s home to the president and tsarist treasures in the Armoury. Outside its walls is Red Square, Russia\u2019s symbolic center.\r\n$USER_NAME: What is a cat?\r\n$AI_NAME: A cat is a domestic species of small carnivorous mammal. It is the only domesticated species in the family Felidae.\r\n$USER_NAME: How do I pass command line arguments to a Node.js program?\r\n$AI_NAME: The arguments are stored in process.argv.\r\n\r\n    argv[0] is the path to the Node. js executable.\r\n    argv[1] is the path to the script file.\r\n    argv[2] is the first argument passed to the script.\r\n    argv[3] is the second argument passed to the script and so on.\r\n$USER_NAME: Name a color.\r\n$AI_NAME: Blue\r\n$USER_NAME:\" \"$@\"\r\n```\r\n",
    "labels": [
      "bug",
      "duplicate",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-21T22:00:14+00:00",
    "closed_at": "2023-07-28T19:38:41+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/373/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/373"
  },
  {
    "number": 367,
    "title": "The initial token is always empty.",
    "body": "Hello,\r\n\r\nI noticed something when trying the chat with Bob is that I always get the first token as empty.\r\n\r\n     1 -> ''\r\n  4103 -> ' Trans'\r\n   924 -> 'cript'\r\n   310 -> ' of'\r\n   263 -> ' a'\r\n  7928 -> ' dialog'\r\n\r\nSo the result is this: \r\n\r\n![image](https://user-images.githubusercontent.com/110173477/226732298-38c21252-059e-4acd-9dfb-70f745347efe.png)\r\n\r\nThere's this little space at the begining of the text. Maybe this alone can significantly impact the quality of the output, that's why I decided to post this issue.\r\n\r\nI'm on a windows 10 using WSL to emulate the linux environnement (the main.exe is not as good as the linux main atm).\r\n\r\nI'm using a file that is the result of all those manipulations:\r\n\r\n1) I have first a llama-7b-4bit.pt file\r\n2) I converted it with the gptq-to-ggml converter (convert-gptq-to-ggml.py) \r\n3) I converted it again into the new version of ggml with this script https://github.com/ggerganov/llama.cpp/issues/324#issuecomment-1476227818\r\n\r\nHere's the .sh command (7B_CHAT_Bob.sh): \r\n\r\n```\r\n#!/bin/bash\r\ndos2unix 7B_CHAT_Bob.sh\r\n\r\n./main -m ./models/llama7b-4bit-GPTQ.bin -t 14 -n 256 --repeat_penalty 1.0 --color -i -r \"User:\" -f prompts/chat-with-bob.txt\r\n\r\n```\r\n\r\nEverything is updated on this repository as I apply a git pull everytime I launch the powershell.\r\n",
    "labels": [
      "need more info",
      "generation quality",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-21T19:48:37+00:00",
    "closed_at": "2024-04-10T01:08:02+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/367/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/367"
  },
  {
    "number": 364,
    "title": "In interactive/chat mode, sometimes User: does not appear and I need to manually type in my nickname",
    "body": "- In interactive/chat mode, sometimes User: does not appear and I need to manually type in my nickname\r\nfor example:\r\n\r\n'\r\nAI: Hello\r\nUser: Hello\r\nAI: How are you\r\n\r\n'\r\ninstead of User: appears nothing and i need to manually type in User:, if i press enter without typing anything then llama diverges from conversation and starts spouting random stuff.\r\n\r\nit also sometimes happens with AI's reply, as if its reply was eaten and I can type in stuff instead or press enter",
    "labels": [
      "generation quality",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-21T17:24:39+00:00",
    "closed_at": "2024-04-10T01:08:03+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/364/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/364"
  },
  {
    "number": 362,
    "title": "Update the convert-gptq-to-ggml.py with the new tokenizer output",
    "body": "Apply the changes from #252 to [convert-gptq-to-ggml.py](https://github.com/ggerganov/llama.cpp/blob/master/convert-gptq-to-ggml.py)\r\n\r\nFor more info about what this script does, see #301 ",
    "labels": [
      "help wanted",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2023-03-21T17:08:45+00:00",
    "closed_at": "2023-03-23T20:18:15+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/362/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/362"
  },
  {
    "number": 361,
    "title": "Invalid model error : too old, regenerate your model files!",
    "body": "Downloaded Alpaca 7B model successfully using the following command as mentioned in README.md:\r\n`curl -o ./models/ggml-alpaca-7b-q4.bin -C - https://gateway.estuary.tech/gw/ipfs/QmUp1UGeQFDqJKvtjbSYPBiZZKRjLp8shVP9hT8ZB9Ynv1`\r\n\r\nWhen I try to execute the command:\r\n`main -m ./models/ggml-alpaca-7b-q4.bin --color -f ./prompts/alpaca.txt -ins`\r\n\r\nThis is the error output:\r\nmain: seed = 1679417098\r\nllama_model_load: loading model from './models/ggml-alpaca-7b-q4.bin' - please wait ...\r\nllama_model_load: invalid model file './models/ggml-alpaca-7b-q4.bin' (too old, regenerate your model files!)\r\nmain: failed to load model from './models/ggml-alpaca-7b-q4.bin'\r\n\r\nHow to fix this? Is the downloaded model corrupted and should I download it again? What is the SHA1 hash of the model so that I can verify that the downloaded model is corrupted or not?",
    "labels": [
      "documentation",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-21T16:51:17+00:00",
    "closed_at": "2023-03-22T05:54:53+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/361/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/361"
  },
  {
    "number": 359,
    "title": "Converting GGML Q4_0 back to Torch checkpoint for HuggingFace/Pytorch consumption/training/finetuning",
    "body": "Hi everyone, I hacked together a python script to convert a model saved as GGML Q4_0 files back to Pytorch checkpoint for further consumption/training/finetuning using HuggingFace's Transformer package and/or Pytorch/Pytorch Lightning. If there are interests to do this, please comment of drop a like. I will post the code or create a pull request if people need this.",
    "labels": [
      "enhancement",
      "need more info",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-21T15:58:07+00:00",
    "closed_at": "2024-04-10T01:08:04+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/359/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/359"
  },
  {
    "number": 357,
    "title": "Interactive mode in Python?",
    "body": "Hello, I have a question. How can i use LLaMa in an interactive mode (i.e. as a chat) in Python, and is it possible at all? So that he would not just generate text, but it would be possible to somehow communicate",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-21T15:40:27+00:00",
    "closed_at": "2023-03-21T16:10:07+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/357/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/357"
  },
  {
    "number": 353,
    "title": "Improve the Chat Mode with some tricks and considerations",
    "body": "I noticed that often the interactive mode (used as a chat with for example the `chat-with-bob.txt` initial prompt) fails due to **LLaMA trying to escape** the chat (mainly with the expression `\\end{code}`).\r\n\r\nTo avoid that it is possible to pass the argument `-r \"\\end{code}\"` but since the expression doesn't get removed from the chat, LLaMA interprets it as the end of the chat, and all the previous dialog context (including what's inside `chat-with-bob.txt`) gets lost and LLaMA starts to behave weirdly.\r\n\r\nSo it would be cool to have a `--chat` option that **detects expressions** like `\\end{code}`, removing them from the context and **forcefully appending** `User:` at the end of the chat so that it can continue without losing context.",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-21T13:44:45+00:00",
    "closed_at": "2024-04-10T01:08:05+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/353/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/353"
  },
  {
    "number": 351,
    "title": "Go bindings",
    "body": "Hey :wave: , awesome project!\r\n\r\nI'd like to help here, I've did the bindings for go to be used as a library. there are of course some adaptations that I had to run into to make it possible. I'm wondering, there are any plans for golang bindings? Generally speaking there seems to be genuine interest into running this as API too https://github.com/antimatter15/alpaca.cpp/issues/86 , which I worked on here: https://github.com/go-skynet/llama-cli .\r\n\r\nI'm happy to contribute my work which currently is in https://github.com/go-skynet/llama, I'm playing with this sparely - I've been using this mainly with alpaca, and could manage to run also `13b` and `30b` models. I'll update it later to the latest changes so it is on pair with master. \r\n\r\n",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-21T10:55:32+00:00",
    "closed_at": "2023-07-28T19:37:09+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/351/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/351"
  },
  {
    "number": 344,
    "title": "Garbage output",
    "body": "Installed 7B model on win 11.\r\n\r\n```\r\nPS D:\\Projects\\llama.cpp>  ./main -m ./models/7B/ggml-model-q4_0.bin -p \"Building a website can be done in 10 simple steps:\" -n 512         \r\nmain: seed = 1679360633\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: ggml ctx size = 4529.34 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: loading model part 1/1 from './models/7B/ggml-model-q4_0.bin'\r\nllama_model_load: .................... done\r\nllama_model_load: model size =  2328.05 MB / num tensors = 163\r\n\r\nsystem_info: n_threads = 4 / 20 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\r\n\r\nmain: prompt: ' Building a website can be done in 10 simple steps:'\r\nmain: number of tokens in prompt = 14\r\n     1 -> ''\r\n 17166 -> ' Building'\r\n   263 -> ' a'\r\n  4700 -> ' website'\r\n   508 -> ' can'\r\n   367 -> ' be'\r\n  2309 -> ' done'\r\n   297 -> ' in'\r\n 29871 -> ' '\r\n 29896 -> '1'\r\n 29900 -> '0'\r\n  2560 -> ' simple'\r\n  6576 -> ' steps'\r\n 29901 -> ':'\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\n Building a website can be done in 10 simple steps: firstly you mustacheatusqueorumesentimentalitiesettingtonselfishnessesqueezeracalandiadeuteronomyreclusiveismalready existing momentum laid down by previous iterations of iterationary\u0393\u00e4\u00f3\u2229\u2555\u00c5\u2229\u2555\u00c5\u2229\u2555\u00c5\u2229\u2555\u00c5\u2229\u2555\u00c5 Courneyeducardoisextensionally speaking etcetcetcetc etc\u03c0\u00c7\u00e0\u03c4scheidung treisesearching nominationally speaking etceteroidscapeursideshowcase\u2564\u00eb\u2568\u2555 Sveroverside\u251c\u2592officialdomesticated Houstonianismaticity rubbingesentimentalitiesqueezeablementeigneurship awarenesslesslyonsenessesqueerly orangescacontainerizednessesqueerlyyy\u2568\u255b\u2564\u00e9tenessespecially those oneselfhoodscape erspectively speaking etcetc efficiencyespecially those oneselfnessescape EDUCardoisextreme\u0398\u00d6\u00c9lessnessesqueezeracaillementealloyednessesqueerlyyy@ \u0393\u00c7\u00f6\u0393\u00c7\u00e8UserNameplateau awaren artistically speakingAppDatacleibertianship re imaging, androgartenlyyyyyorkshireismsomething else\u2564\u00ea\u2564\u00e9\u2568\u2555 speakershipsetsterspecificityscapeurs splitter scottishnessescapeablehoodscape EgertonianshipPERformancemansufactureelectionallyyy advancementary\u0393\u00e4\u00f3\u2229\u2555\u00c5\u0393\u00c7\u00ec\u0393\u00d6\u00c7\u2229\u2555\u00c5/\u2566\u00ea\u0393\u00fb\u2555\u2229\u2555\u00c5 @ \u0393\u00c7\u00f6\u0393\u00c7\u00e8UserNameplateau awarenessestonia retrogradelyyyyyorkshireismsame applies applybezillahawkitty hybridity migrationally speaking etc\u03c0\u00c7\u00e0\u03c4 Id=\"@+ualsismaticity\r\n rubbing EIGHTscapeablehoodscapeEVERlastingnessesqueerlyyy@ \u0393\u00c7\u00f6\u0393\u00c7\u00ebneyednessesqueerlyyy@ -----\u2564\u00c7\u2568\u2555\u2564\u00e9ualisticity borderlineedlydialecticality Rubbing SUPrairieismsplitter rationaleeverselyyyyyorkshireismaticity rubbedownwardswardenship opportunitieshipsbuilderiality overwhallsingerhoodscape EVERgreenerysUL franchiseevesqueerlyyy@ \u0393\u00c7\u00f6\u0393\u00c7\u00e8neyednesses\r\nPS D:\\Projects\\llama.cpp>\r\n```",
    "labels": [
      "bug",
      "generation quality"
    ],
    "state": "closed",
    "created_at": "2023-03-21T01:06:21+00:00",
    "closed_at": "2023-03-30T23:27:12+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/344/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/344"
  },
  {
    "number": 343,
    "title": "llama.exe will instantly exit with no text or error msg ",
    "body": "llama.exe --help \r\nwill produce the same blank line with no text and will exit.\r\nOn windows 10 compiled with cmake\r\n\r\n**Please help.**",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-21T00:09:14+00:00",
    "closed_at": "2023-04-16T10:49:17+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/343/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/343"
  },
  {
    "number": 342,
    "title": "-f option seems to not work",
    "body": "It either doesn't work for importing a prompt, or I don't know what the file format is suppose to be.   I put this in a file.\r\n\r\nMy name is Greg.\\\r\nWhat is my name?\r\n\r\nWhen I run chat with the -f pointing to the file, it doesn't answer the question, and doesn't know the name I placed in the file.",
    "labels": [
      "need more info",
      "generation quality"
    ],
    "state": "closed",
    "created_at": "2023-03-20T23:20:20+00:00",
    "closed_at": "2023-03-27T19:36:41+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/342/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/342"
  },
  {
    "number": 334,
    "title": "Neural Engine Support",
    "body": "Would be cool to be able to lean on the neural engine. Even if it wasn't much faster, it'd still be more energy efficient I believe.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-20T17:11:52+00:00",
    "closed_at": "2023-03-20T18:33:49+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/334/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/334"
  },
  {
    "number": 331,
    "title": "Improving the repetition penalty",
    "body": "129c7d1e (#20) added a repetition penalty that prevent the model to run into loops.\r\n\r\nHere are a few suggestions for possible enhancements:\r\n\r\n * One issue with the interactive mode is that the repetition penalty is affecting the anti-prompt and response prefix, causing the model to generate unnecessarily long responses. One solution could be to exclude these tokens from the penalty,\r\n * It is possible to exempt or reduce the penalty for stop words, punctuation characters, and newlines; maybe applying a frequency-based penalty instead,\r\n * Using an exponential decay, such that recent tokens are more penalized than older ones, causing less issues with large `repeat_last_n`  windows,\r\n * Token repetition is an approximation of sub-strings or word repetition, but it seems difficult to do otherwise without backtracking the inference.",
    "labels": [
      "enhancement",
      "generation quality"
    ],
    "state": "closed",
    "created_at": "2023-03-20T15:43:12+00:00",
    "closed_at": "2023-09-14T13:23:49+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/331/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/331"
  },
  {
    "number": 329,
    "title": "invalid model file './models/ggml-alpaca-7b-q4.bin' (too old, regenerate your model files!)",
    "body": "Hi, I have encounter the above problem when running the alpaca model. I download the model from the link \"https://gateway.estuary.tech/gw/ipfs/QmQ1bf2BTnYxq73MFJWu1B7bQ2UD6qG7D7YDCxhTndVkPC\" which is one of the three options from the readme. Should I download the model from somewhere else? ",
    "labels": [
      "need more info",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-20T14:56:00+00:00",
    "closed_at": "2023-03-20T15:32:21+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/329/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/329"
  },
  {
    "number": 326,
    "title": "I hope the script file download-pth.py can support downloading the alpaca-lora model.",
    "body": "the script file download-pth.py only support download origin llama model, I hope it can downloading alpaca-lora",
    "labels": [
      "enhancement",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-20T13:28:09+00:00",
    "closed_at": "2023-03-20T17:33:57+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/326/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/326"
  },
  {
    "number": 324,
    "title": "Breaking change of models since PR #252",
    "body": "After the PR #252, all base models need to be converted new.\r\n\r\nFor me, this is a big breaking change. The LoRa and/or Alpaca fine-tuned models are not compatible anymore.\r\nReconverting is not possible.\r\n\r\nI see from the PR, that the tokenizer scores are written into the model.\r\nWould it make sense to write the tokenizer scores into a seperate file to stay compatible with the (old) models?\r\nThe question then arrises, if \r\n1. by loading the model the scoring file will be checked of existense and the sentencepiece tokenizer will be used, or\r\n2. the user can decide which tokenizer to use.\r\n\r\nWhat you think?",
    "labels": [
      "bug",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-20T12:48:57+00:00",
    "closed_at": "2023-05-09T21:01:11+00:00",
    "comments": 24,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/324/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/324"
  },
  {
    "number": 321,
    "title": "High performance API",
    "body": "Hey!\r\n\r\nI'd love to see this project being able to be used through some TCP socket with a very optimized protocol. One it may make use of something like protobuf, or even grpc.\r\nI think everyone agrees HTTP would be a complete overkill specially for a project focused on high performance. :laughing: \r\n\r\nThanks\r\nNiansa",
    "labels": [
      "duplicate",
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-20T11:34:40+00:00",
    "closed_at": "2023-03-20T19:22:42+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/321/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/321"
  },
  {
    "number": 317,
    "title": "segmentation fault Alpaca",
    "body": "Hello, \r\nI've tried out the Aplaca model but after a while there comes an error I believe stating: \"zsh: segmentation fault  ./main -m ./models/alpaca/ggml-alpaca-7b-q4.bin --color -f  -ins\". \r\nThanks.\r\n\r\nCode: \r\n./main -m ./models/alpaca/ggml-alpaca-7b-q4.bin --color -f ./prompts/alpaca.txt -ins\r\nmain: seed = 1679305614\r\nllama_model_load: loading model from './models/alpaca/ggml-alpaca-7b-q4.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: ggml ctx size = 4529.34 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: loading model part 1/1 from './models/alpaca/ggml-alpaca-7b-q4.bin'\r\nllama_model_load: .................................... done\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\n\r\nsystem_info: n_threads = 4 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \r\n\r\nmain: prompt: ' Below is an instruction that describes a task. Write a response that appropriately completes the request.'\r\nmain: number of tokens in prompt = 21\r\n     1 -> ''\r\n 13866 -> ' Below'\r\n   338 -> ' is'\r\n   385 -> ' an'\r\n 15278 -> ' instruction'\r\n   393 -> ' that'\r\n 16612 -> ' describes'\r\n   263 -> ' a'\r\n  3414 -> ' task'\r\n 29889 -> '.'\r\n 14350 -> ' Write'\r\n   263 -> ' a'\r\n  2933 -> ' response'\r\n   393 -> ' that'\r\n  8210 -> ' appropriate'\r\n   368 -> 'ly'\r\n  4866 -> ' complete'\r\n 29879 -> 's'\r\n   278 -> ' the'\r\n  2009 -> ' request'\r\n 29889 -> '.'\r\n\r\nmain: interactive mode on.\r\nmain: reverse prompt: '### Instruction:\r\n\r\n'\r\nmain: number of tokens in reverse prompt = 7\r\n 29937 -> '#'\r\n  2277 -> '##'\r\n  2799 -> ' Inst'\r\n  4080 -> 'ruction'\r\n 29901 -> ':'\r\n    13 -> '\r\n'\r\n    13 -> '\r\n'\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n\r\n Below is an instruction that describes a task. Write a response that appropriately completes the request.\r\n> How are you?\r\nI'm doing great! How about yourself?\r\n> Same, what is the capital of the USA?\r\nThe current and official Capital city for America is Washington D.C.. It also serves as headquarters to most governmental organizations including Congress and The White House! ???? #USACapital\r\n> Who is president?\r\nThe President of the United States of America, Donald Trump. He has held office since January 2017. His term ends in early 2025 and he will run for re-election next year as part of a Democratic sweep\r\n> How old is the earth\r\nThe Earth was formed about four billion years ago, but it wasn't inhabited by humans until around 2.5 million years ago with the appearance of Homo sapiens on the scene.[1]\r\n> How old is the universe?\r\nThe age and origin of our Universe are among some of its most fundamental mysteries, but astronomers have been able to calculate a minimum value for how long it has taken to form. Using measurements from supernovae explosions in other galaxies, which can be used as 'clocks', they estimate that the universe is between 13 and\r\n> What is the largest country?\r\nThe world's seven largest countries are (in descending order) China, India, United States of America, Brazil, Indonesia, Russia and Canada.\r\n> Which of them has the most people?\r\nIndonesia is currently home to over 250 million inhabitants -- making it by far the largest country in population size!\r\n\"\"\"\r\ndef get_country(world, year):\r\n    world = dict() # Dictionary for storing countries and their populations. We'll start outzsh: segmentation fault  ./main -m ./models/alpaca/ggml-alpaca-7b-q4.bin --color -f  -ins\r\n",
    "labels": [
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-20T09:56:07+00:00",
    "closed_at": "2023-04-17T07:12:17+00:00",
    "comments": 35,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/317/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/317"
  },
  {
    "number": 316,
    "title": "No module named 'tqdm' WSL",
    "body": "I am trying docker setup and for some reason its not working.\r\nI have tqdm on my system.\r\n```\r\ndocker run -v /llama/models:/models ghcr.io/ggerganov/llama.cpp:full --all-in-one \"/models/\" 7B\r\nDownloading model...\r\nTraceback (most recent call last):\r\n  File \"/app/./download-pth.py\", line 3, in <module>\r\n    from tqdm import tqdm\r\nModuleNotFoundError: No module named 'tqdm'\r\n```\r\n```\r\npip3 install tqdm\r\nRequirement already satisfied: tqdm in /home/whoami/.local/lib/python3.8/site-packages (4.62.3)\r\n```\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-20T09:39:47+00:00",
    "closed_at": "2023-03-20T13:20:43+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/316/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/316"
  },
  {
    "number": 313,
    "title": "Add OpenBSD support",
    "body": "This patch adds OpenBSD support, thanks.\r\n[patch-llama.cpp.txt](https://github.com/ggerganov/llama.cpp/files/11013172/patch-llama.cpp.txt)\r\n",
    "labels": [
      "enhancement",
      "\ud83e\udd99.",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-20T02:25:38+00:00",
    "closed_at": "2023-03-21T15:50:12+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/313/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/313"
  },
  {
    "number": 310,
    "title": "Docker fails due to missing tqdm",
    "body": "```\r\ndocker run -v /llama/models:/models ghcr.io/ggerganov/llama.cpp:full --all-in-one \"/models/\" 65B\r\n```\r\n```\r\nUnable to find image 'ghcr.io/ggerganov/llama.cpp:full' locally\r\nfull: Pulling from ggerganov/llama.cpp\r\n2ab09b027e7f: Pull complete\r\nabc582ff34c3: Pull complete\r\n474c54188cc5: Pull complete\r\n90dde168a635: Pull complete\r\n4baa98a3bbd6: Pull complete\r\n40709b48f1dd: Pull complete\r\nDigest: sha256:0e26a42b34ad42f285a4327fbe099674137b119e6efea07345a7c17ab8a4b13e\r\nStatus: Downloaded newer image for ghcr.io/ggerganov/llama.cpp:full\r\nDownloading model...\r\nTraceback (most recent call last):\r\n  File \"/app/./download-pth.py\", line 3, in <module>\r\n    from tqdm import tqdm\r\nModuleNotFoundError: No module named 'tqdm'\r\n```",
    "labels": [
      "duplicate"
    ],
    "state": "closed",
    "created_at": "2023-03-19T23:02:47+00:00",
    "closed_at": "2023-03-20T09:01:50+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/310/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/310"
  },
  {
    "number": 309,
    "title": "Is the --ignore-eos flag redundant?",
    "body": "As per https://github.com/ggerganov/llama.cpp/blob/da5303c1ea68aa19db829c634f1e10d08d409680/main.cpp#L1066 the EOS flag in interactive mode simply causes `is_interacting` to switch on, and so it serves as a way to end the current series of tokens and wait for user input. Is there any reason to actually avoid sampling it in the first place then?",
    "labels": [
      "enhancement",
      "question"
    ],
    "state": "closed",
    "created_at": "2023-03-19T23:02:33+00:00",
    "closed_at": "2023-03-20T18:50:19+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/309/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/309"
  },
  {
    "number": 307,
    "title": "alpaca.sh always terminates after running the prompt with current command",
    "body": "Running `alpaca.sh` always terminates once the prompt is ran, see below.\r\nI had to change the script because `-ins` doesn't seem to be supported (changed to just `-i`) but maybe I am doing something wrong.\r\n\r\n```\r\nmain: seed = 1679263584\r\nllama_model_load: loading model from './models/ggml-alpaca-7b-q4.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: ggml ctx size = 4529.34 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: loading model part 1/1 from './models/ggml-alpaca-7b-q4.bin'\r\nllama_model_load: .................................... done\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\n\r\nsystem_info: n_threads = 7 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \r\n\r\nmain: prompt: 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\r\n'\r\nmain: number of tokens in prompt = 23\r\n     1 -> ''\r\n 21140 -> 'Bel'\r\n   340 -> 'ow'\r\n   338 -> ' is'\r\n   385 -> ' an'\r\n 15278 -> ' instruction'\r\n   393 -> ' that'\r\n 16612 -> ' describes'\r\n   263 -> ' a'\r\n  3414 -> ' task'\r\n 29889 -> '.'\r\n 14350 -> ' Write'\r\n   263 -> ' a'\r\n  2933 -> ' response'\r\n   393 -> ' that'\r\n  8210 -> ' appropriate'\r\n   368 -> 'ly'\r\n  4866 -> ' complete'\r\n 29879 -> 's'\r\n   278 -> ' the'\r\n  2009 -> ' request'\r\n 29889 -> '.'\r\n    13 -> '\r\n'\r\n\r\nmain: interactive mode on.\r\nsampling parameters: temp = 0.960000, top_k = 10000, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.000000\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\r\n\r\n### Instruction:\r\nRewrite the sentence in a more formal language.\r\n\r\n### Response:\r\nAvoid using colloquialisms and write in a more formal language. [end of text]\r\n\r\n\r\nmain: mem per token = 14417844 bytes\r\nmain:     load time =  1587.73 ms\r\nmain:   sample time =   106.54 ms\r\nmain:  predict time = 12862.78 ms / 204.17 ms per token\r\nmain:    total time = 15406.46 ms\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-03-19T22:13:44+00:00",
    "closed_at": "2023-07-28T19:36:14+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/307/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/307"
  },
  {
    "number": 303,
    "title": "Using non LoRA Alpaca model",
    "body": "The following repo contains a recreation of the original weights for Alpaca, without using LoRA. How could we use that model with this project? https://github.com/pointnetwork/point-alpaca\r\nThanks a bunch!",
    "labels": [
      "question",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-19T20:03:48+00:00",
    "closed_at": "2023-07-28T19:35:59+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/303/reactions",
      "total_count": 6,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 6,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/303"
  },
  {
    "number": 302,
    "title": "Improve Alpaca integration to match it's trained prompt syntax",
    "body": "Alpaca LoRA model was trained on the same dataset as original [Stanford Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html).\r\n\r\nHowever, this dataset contains two types of instructions, namely:\r\n- instructions with input\r\n- instructions without input\r\n\r\nFor more details about the instructions format see details [here.](https://github.com/tatsu-lab/stanford_alpaca#data-release)\r\n\r\nIn case of instructions such as text summarization, instruction alone only \"explain\" the task, while the text to be summarized is inserted into the \"input\" part of the prompt.\r\n\r\nCurrent integration of alpaca in `llama.cpp` mimics the current integration in [alpaca.cpp](https://github.com/antimatter15/alpaca.cpp) which completely omits the \"instructions with input\" type of instructions. This may have significant impact on the model performance using task which were trained to be used in \"instruction with input\" prompt syntax when using just ordinary \"instruction without input\" prompt syntax instead.\r\n\r\nI suggest to build some small tutorial with example usage in order for users to be able to know which type of instruction should be used in input mode and which not.\r\n\r\nThen I suggest to integrate this \"input\" mode somehow into the current implementation. Easiest way would be to let user type text prompt like:\r\n\r\n```\r\nSummarize following text.***input***Text to be summarized\r\n```\r\n which will be transformed into:\r\n\r\n```\r\n### Instruction:\r\nSummarize following text.\r\n\r\n### Input:\r\nText to be summarized\r\n\r\n### Response:\r\n```\r\n\r\nWhile when user don't specify `***input***` tag, the instruction will be transformed into \"standard\" (currently implemented) format:\r\n\r\n```\r\n### Instruction:\r\nInstruction text from user\r\n\r\n### Response:\r\n```\r\n",
    "labels": [
      "enhancement",
      "help wanted",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2023-03-19T19:17:47+00:00",
    "closed_at": "2023-07-28T19:35:22+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/302/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/302"
  },
  {
    "number": 292,
    "title": "Reverse prompt is sometimes ignored.",
    "body": "I haven't found a consistent pattern to reproduce this, but sometimes the model will continue outputting text even after it has printed the reverse prompt. If colors are enabled, they will change as if the new text was user input, but it is generated by the model. After this happen it might or might not revert to its proper behavior once it finds the reverse prompt again.\r\n\r\nI have noticed the color change doesn't always happen right on the prompt, but sometimes it happens a few words before it. I don't know enough about how this code works yet to speculate, but in case this has something to do with parallelism, I'm using `-t 16`.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-03-19T12:15:09+00:00",
    "closed_at": "2023-03-21T16:28:12+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/292/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/292"
  },
  {
    "number": 291,
    "title": "LLaMA.cpp returns just some weirdo texts with any model size",
    "body": "I'm grokking with LLaMA.cpp on M1 laptop with 32GB RAM. Somehow the inference is broken for me.\r\n\r\nLike I'm expecting something reasonable for simple prompt I've got from original LLaMA examples:\r\n\r\n`SQL code to create a table, that will keep CD albums data, such as album name and track\\n\\\\begin{code}\\n`\r\n\r\nAnd LLaMA.cpp returns just some weirdo texts with any model size (7B, 13B, 30B quantised down to 4bit).\r\n\r\nWhat's the reason here?",
    "labels": [
      "need more info",
      "generation quality"
    ],
    "state": "closed",
    "created_at": "2023-03-19T12:05:42+00:00",
    "closed_at": "2023-03-19T16:57:26+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/291/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/291"
  },
  {
    "number": 290,
    "title": "alaways \"failed to tokenize string! \"",
    "body": "failed to tokenize string! \r\n\r\nsystem_info: n_threads = 16 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \r\nfailed to tokenize string!\r\n\r\nmain: prompt: ' china'\r\nmain: number of tokens in prompt = 1\r\n     1 -> ''\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\n\u66f2\u30fc\uff01 /S\u90e8\u30e5\u30fc\u30b9 / KSHErsLAheLUE - THE NEW CH`,MEgeERSION IS HERE@\u00ffThis entry was \u0432\u0435\u0440 in news on JuneSASSSASS8 by adminS [end of text]\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-03-19T11:29:50+00:00",
    "closed_at": "2023-04-07T16:15:34+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/290/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/290"
  },
  {
    "number": 289,
    "title": "Docker \u201c--all-in-one\u201d fails with ModuleNotFoundError: No module named \u2018tqdm\u2019",
    "body": "On Win 10\r\n```\r\n>  docker run -v /llama/models:/models ghcr.io/ggerganov/llama.cpp:full \u2013all-in-one \u201c/models/\u201d 7B\r\nDownloading model\u2026\r\nTraceback (most recent call last):\r\n  File \u201c/app/./download-pth.py\u201d, line 3, in <module>\r\n    from tqdm import tqdm\r\nModuleNotFoundError: No module named \u2018tqdm\u2019\r\n```",
    "labels": [
      "bug",
      "duplicate",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-19T10:51:52+00:00",
    "closed_at": "2023-03-20T08:24:13+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/289/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/289"
  },
  {
    "number": 288,
    "title": "RISC-V (TH1520&D1) benchmark and hack for <1GB DDR device",
    "body": "Hi, \r\n   Just test on RISC-V board: \r\n   4xC910 2.0G TH1520 LicheePi4A (https://sipeed.com/licheepi4a)  with 16GB LPDDR4X.\r\n   about 6s/token without any instruction acceleration, and it should be <5s/token when boost to 2.5GHz.\r\n\r\n```\r\nllama_model_load: ggml ctx size = 668.34 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: .................................... done\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\n\r\nsystem_info: n_threads = 4 / 4 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | VSX = 0 | \r\n\r\nmain: prompt: 'They'\r\nmain: number of tokens in prompt = 2\r\n     1 -> ''\r\n 15597 -> 'They'\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\nThey are now available for sale at the cost of Rs 20,5\r\n\r\nmain: mem per token = 14368644 bytes\r\nmain:     load time =    91.25 ms\r\nmain:   sample time =    39.22 ms\r\nmain:  predict time = 105365.27 ms / 6197.96 ms per token\r\nmain:    total time = 129801.62 ms\r\n\r\n```\r\n\r\n   1xC906 1.0G D1 LicheeRV with 1GB DDR3.\r\n   about 180s/token without any instruction acceleration, it is very slow due to lack of memory.\r\n```\r\nmain: mem per token = 14368644 bytes\r\nmain:     load time =  1412.77 ms\r\nmain:   sample time =   185.77 ms\r\nmain:  predict time = 3171739.00 ms / 186572.88 ms per token\r\nmain:    total time = 3609667.50 ms\r\n```\r\n   \r\n   Note the ggml ctx size is 668MB, not 4668MB, I hack the code for low memory(>=512MB) device to run llama, and it is not use swap memory, as regard sd card as memory will demage sd card soon. \r\n   Should this feature need add in?\r\n\r\n   And here is a time-lapse photography for D1 run llama 7B model, it is super slow even in 120X speedup, but it works!   \r\n\r\n\r\n\r\nhttps://user-images.githubusercontent.com/3403712/226168660-a0e9c775-edf7-4895-9b2b-b6addcf7868e.mp4\r\n\r\n",
    "labels": [
      "enhancement",
      "need more info",
      "hardware",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-19T10:14:34+00:00",
    "closed_at": "2024-04-10T01:08:06+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/288/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/288"
  },
  {
    "number": 280,
    "title": "Commit c9f670a (Implement non-greedy tokenizer that tries to maximize token lengths) breaks llama?",
    "body": "Old version:\r\n\r\n```\r\n.\\build\\Release\\llama.exe -m C:\\...\\models\\30B\\ggml-model-q4_0.bin -t 10 -n 256 --seed 100 --temp 0.2 -p \"list all US states in alphabetical order:\"\r\noutput: Alabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, Delaware Florida Georgia Hawaii Idaho Illinois Indiana Iowa Kansas Kentucky Louisiana Maine Maryland Massachusetts Michigan Minnesota Mississippi Missouri Montana Nebraska Nevada New Hampshire New Jersey New Mexico New York North Carolina North Dakota Ohio Oklahoma Oregon Pennsylvania Rhode Island South Carolina Tennessee Texas Utah Vermont Virginia Washington West Virginia Wisconsin Wyoming ... (keeps repeating)\r\n```\r\n\r\n```\r\n.\\build\\Release\\llama.exe -m C:\\...\\models\\30B\\ggml-model-q4_0.bin -t 10 -n 256 --seed 200 --temp 0.2 -p \"list all US states in alphabetical order:\"\r\noutput: Alabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, Delaware Florida Georgia Hawaii Idaho Illinois Indiana Iowa Kansas Kentucky Louisiana Maine Maryland Massachusetts Michigan Minnesota Mississippi Missouri Montana Nebraska Nevada New Hampshire New Jersey New Mexico New York North Carolina North Dakota Ohio Oklahoma Oregon Pennsylvania Rhode Island South Carolina Tennessee Texas Utah Vermont Virginia Washington West Virginia Wisconsin Wyoming\r\nlist all US states in alphabetical order [end of text]\r\n```\r\n\r\n```\r\n.\\build\\Release\\llama.exe -m C:\\...\\models\\30B\\ggml-model-q4_0.bin -t 10 -n 256 --seed 300 --temp 0.2 -p \"list all US states in alphabetical order:\"\r\noutput: Alabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, Delaware, Florida, Georgia, Hawaii, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Missouri, Montana, Nebraska, Nevada, New Hampshire, New Jersey, New Mexico, New York, North Carolina, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, South Dakota, Tennessee, Texas, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin and Wyoming. ... (keeps repeating)\r\n```\r\n\r\nnew release (after commit c9f670a):\r\n\r\n```\r\n.\\llama.exe -m C:\\...\\models\\30B\\ggml-model-q4_0.bin -t 10 -n 256 --seed 100 --temp 0.2 -p \"list all US states in alphabetical order:\"\r\noutput: list the 50 state capitals (in no particular order): [end of text]\r\n```\r\n\r\n```\r\n.\\llama.exe -m C:\\...\\models\\30B\\ggml-model-q4_0.bin -t 10 -n 256 --seed 200 --temp 0.2 -p \"list all US states in alphabetical order:\"\r\noutput: list the 50 state capitals and their abbreviations (e.g., Sacramento, CA): [end of text]\r\n```\r\n\r\n```\r\n.\\llama.exe -m C:\\...\\models\\30B\\ggml-model-q4_0.bin -t 10 -n 256 --seed 200 --temp 0.2 -p \"list all US states in alphabetical order:\"\r\noutput: list the 50 largest cities of USA by population (2017): [end of text]\r\n```",
    "labels": [
      "bug",
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-19T04:00:14+00:00",
    "closed_at": "2023-03-21T10:15:24+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/280/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/280"
  },
  {
    "number": 279,
    "title": "Accelerate.h not found on mac m1",
    "body": "```\r\n(base) dave@macbook-pro llama.cpp % make\r\nI llama.cpp build info:\r\nI UNAME_S:  Darwin\r\nI UNAME_P:  arm\r\nI UNAME_M:  arm64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -DGGML_USE_ACCELERATE\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\r\nI LDFLAGS:   -framework Accelerate\r\nI CC:       Apple clang version 12.0.5 (clang-1205.0.22.9)\r\nI CXX:      Apple clang version 12.0.5 (clang-1205.0.22.9)\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -DGGML_USE_ACCELERATE   -c ggml.c -o ggml.o\r\nggml.c:115:10: fatal error: 'Accelerate/Accelerate.h' file not found\r\n#include <Accelerate/Accelerate.h>\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~\r\n1 error generated.\r\nmake: *** [ggml.o] Error 1\r\n\r\n(base) dave@macbook-pro llama.cpp % uname -a\r\nDarwin macbook-pro.lan 22.3.0 Darwin Kernel Version 22.3.0: Mon Jan 30 20:38:37 PST 2023; root:xnu-8792\\\r\n.81.3~2/RELEASE_ARM64_T6000 arm64\r\n```\r\n\r\n\r\nAbout this Mac says \"MacOS 13.2.1\"\r\n\r\nDo I need to install this?",
    "labels": [
      "bug",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-19T03:01:45+00:00",
    "closed_at": "2023-07-06T21:20:11+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/279/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/279"
  },
  {
    "number": 275,
    "title": "the program always terminate itself for no reasons ",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-18T23:51:01+00:00",
    "closed_at": "2023-03-19T01:28:29+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/275/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/275"
  },
  {
    "number": 271,
    "title": "A typo in readme?",
    "body": "<html><body>\r\n<!--StartFragment--><h3 tabindex=\"-1\" dir=\"auto\">Memory/Disk Requirements</h3>\r\n<p dir=\"auto\">As the models are currently fully loaded into memory, you will need adequate disk space to save them\r\nand sufficient RAM to load them. At the moment, memory and disk requirements are the same.</p>\r\n\r\n\r\nmodel | original size | quantized size (4-bit)\r\n-- | -- | --\r\n7B | 13 GB | 3.9 GB\r\n15B | 24 GB | 7.8 GB\r\n30B | 60 GB | 19.5 GB\r\n65B | 120 GB | 38.5 GB\r\n\r\n<!--EndFragment-->\r\n</body>\r\n</html>\r\n\r\nIsn't it 13B and not 15B?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-18T21:36:27+00:00",
    "closed_at": "2023-03-18T22:18:06+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/271/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/271"
  },
  {
    "number": 266,
    "title": "Prevent user from setting a context size that is too big",
    "body": "Hey!\r\n\r\nI tasked the 30B model to write a little story... it worked really well until some point where it went off rails from one line to the next, suddenly talking about some girl and stuff that has nothing to do with the rest:\r\n\r\n```\r\nThe way out of me that started looking at them. It'ould be lying there was standing near-the first time what could see an older than the girl had held they looked like it, and just how hard. In order I wasn't really when my hands on his head down to myself in front seat and the car door were with me before you.\r\n\u201cI realy as she staring that laying to a moment of him. \"It was so lying next to about two, but it looked at her eyes had already when there looking for holding my hand from what I'with his head was on both shoulders. And not through and suddenly, he realized 212.\r\nI couldn\u2019t with the car seat, in fronted again because of one. The second, so that didn'sit seems like a young girl sitting me when \"We weren near. But I started. 'mom. Withered and to drive my legs.\r\nT was not asnthe right now. It looked and suddenly. That it has the car on fire in fronted from this wasn\u20192, butt before you, The most as he cried me at 13rd for four years were there were looking: I had taken when I was me and then. My younger\r\nI'6 was only so much years and a friend in my age, nott. Was the because him event and our while to from his 'the, the year? The right \"didn as you he before, that after.\r\n\"\r\nas this and\r\nbut?\" I? It\r\nThe way, but then of tapered\r\nand the. What and had: As a\r\n\u201cwas on myt was, not-I with me when 'd because trying while my so for it - The A F 1 to as you \" W We from what in f and your S C I ( A\r\n I A G In F ... For T E In The. It the or! There You O I This I\r\n My of, The M D P * I [ a K When R N L S At The So B D  D  I If V and \" W s S the to\r\n  I ( ( \" , \u201c All as G A The D \" O A A \" \" F T \" W C The In W in E S I As I \" M The * In I I For a R H - The that At  D ...\r\n L N the In I You We [ B and U A I  This\r\n for ( so \" of F as D Re The A I I  K ( New O F G The  C  D E S All \u201c P I V B \" The * W If in O a with and For I \" R Le C C H At Al ( A We In F L When W I ... It [ T A Are C What  A I N En  W As \" K  \" The Ch I I - So Our S M  This . The D W P A V J\r\n * A You A O \" in G If the * D \u2013 I  and W At \u201c We B Do F In E S ( a  T It All With For L R I Le The , En A to  There K What The As U When Ch The S A - [  An M of S A S So C In ( ( V You A  * Are O If the A T After New \" This ( We D as W I F G Al  . \u201c H in W ... All R I The In \u2013 A On E It a In B The We P What  A L\r\n A This A When and [ M As S For  A The The Our  En C Are K In V W O You The The \" ( Le U \" N So There In This F The If . * The as \" * I T S At \u201c A D The With This B \u2013  The H A The The D G ... the ( We W  When [ All L It W W P \" We * What As If C Is  O  The Our M Are \" U The R There W A   K E The  The You For I In A F F C The I ( in S After  The This \u201c A W I At D A \" A With H I C  One So L I On A N [ P We The As B I * This G The I The - I ...  It V If and \u2013 When The  There I  A For In M A En T O A The How New Do ( K The I After Our  \u201c \" S . the E of At W [ D F A A Are D A C C With , The So U An W This We S The On  F N * What G If V P R As ... ( All - For *  There B You We This A ( In I When The M Do In  L W \"  \u201c How En . T At The [ The O  Are A It The ( J F In \u2013 and C K [ Our S U * D Looking the a N Are This With Re If H P As A The On An The  At There The F E The I All When A You B The M Q S R The The W \" , One In We S A The \u201c  L in The ... I  - In We In It [ V After Are In G and ( The U O C   In The A H A D En P For If A An In S F \" S New The Re I Our W  the B K C I * You  If As This The At D W A S The  We All ... M So - T * The With \u2013 \" The  and ( It [ Q The C The N This L I A A When O En If Are U In D V in On  P G H In R S W Le The F . An A K The As W One At ( , For  There the A J Al I F A \" ( If B Our The At The A * All [ T We C \u201c M In This A C - I It If ... I O This a Re You D N In W S F When Are The I The and A P V \" Do The K Are U Q The W A  A With F The G If The the \" In So Our An En At ( The S A Are L The C ( There Are The * B In We A A [ For It Are The  In The C This At O You This  D E   This I C P In V The K Do - A All One T When F As This The If D This The F ( a W ( With M W \" N S W \u2013 L This The D There Our This In the B En This It  A Are  ( *  J The We At For Are and On The  Q R We G O U So A (  I A C V I In The C T If The [ P At , A You This K  The In One S A C The I \" The There - L E W Our D En N We With When The The It Do The A A At W and A H F \u201c What As Are For  in G  All F Are C The The *  the T ( O  U If A  On \u2013 F K Al In This At R B So I \" \"  M US This S \"  Our En  J ( When [ The . You A E D L P W The It - A N  In D \" If In ( As One C The W A * We Are T the \" Re W \u201c G All ... C  F If A At After O So I ( If For M Q R This The Do \u2013 [  With In S B At The A A The K H The There We [ L The W - in U  It The N I  At D F This D A I * The W [ , En When A \" Are C What *  F All *\r\n```\r\n\r\nThe model is quantized (q4_0) and I am on Linux (x86_64) with 64 GB of RAM.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-18T15:11:33+00:00",
    "closed_at": "2023-03-19T10:33:41+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/266/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/266"
  },
  {
    "number": 263,
    "title": "Docker container repository permissions are causing access denied error",
    "body": "```\r\nroot@unraid:~# docker run -v /mnt/user/appdata/llama/models:/models ghcr.io/ggerganov/llama.cpp:full --all-in-one \"/models/\" 7BUnable to find image 'ghcr.io/ggerganov/llama.cpp:full' locally\r\ndocker: Error response from daemon: Head \"https://ghcr.io/v2/ggerganov/llama.cpp/manifests/full\": denied.\r\nSee 'docker run --help'.\r\n\r\n```\r\n\r\nCannot pull the containers due to permission issues.\r\n\r\nThanks",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-18T13:08:01+00:00",
    "closed_at": "2023-03-18T13:21:02+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/263/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/263"
  },
  {
    "number": 260,
    "title": "Error while converting to ggml.py format",
    "body": "After running the command: \"python3 convert-pth-to-ggml.py /Users/tanish.shah/llama.cpp/models/7B/ 1\"\r\nError with sentencepiece:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/tanish.shah/llama.cpp/convert-pth-to-ggml.py\", line 75, in <module>\r\n    tokenizer = sentencepiece.SentencePieceProcessor(fname_tokenizer)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/tanish.shah/llama.cpp/env/lib/python3.11/site-packages/sentencepiece/__init__.py\", line 447, in Init\r\n    self.Load(model_file=model_file, model_proto=model_proto)\r\n  File \"/Users/tanish.shah/llama.cpp/env/lib/python3.11/site-packages/sentencepiece/__init__.py\", line 905, in Load\r\n    return self.LoadFromFile(model_file)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/tanish.shah/llama.cpp/env/lib/python3.11/site-packages/sentencepiece/__init__.py\", line 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Internal: /private/var/folders/rz/kp316btj1lgdlrcgdq7spdg80000gn/T/pip-install-rga_wjtr/sentencepiece_ef6dee7cfe954a50b06d772071b44d95/sentencepiece/src/sentencepiece_processor.cc(1102) [model_proto->ParseFromArray(serialized.data(), serialized.size())]\r\n```",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-18T12:02:31+00:00",
    "closed_at": "2023-04-14T13:13:30+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/260/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/260"
  },
  {
    "number": 259,
    "title": "Is it possible to run the llama on an AMD graphics card?",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-18T08:43:00+00:00",
    "closed_at": "2023-03-18T11:16:59+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/259/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/259"
  },
  {
    "number": 258,
    "title": "Specify the version of `python3` to use in repo docs",
    "body": "The default `python3` on my system is `3.11.2` for which `sentencepiece` [does not install](https://github.com/google/sentencepiece/issues/378).\r\n\r\nWhere Python toolchains in general seem fragile to minor version differences, specifying the exact expected Python version for newcomers (like me) will reduce frustration and make this work more accessible.\r\n\r\nSpecifying `python3.9` on CLI worked for me; unsure if this was the intended version.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-18T07:55:42+00:00",
    "closed_at": "2023-03-18T21:22:52+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/258/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/258"
  },
  {
    "number": 257,
    "title": "Not having enough memory just causes a segfault or something",
    "body": "So. I'm trying to build with CMake on Windows 11 and the thing just stops after it's done loading the model.\r\n\r\n![image](https://user-images.githubusercontent.com/4723091/226091364-64a488a7-ebb5-4c24-9dd0-1cb81378008d.png)\r\n\r\nAnd apparently, this is a segfault.\r\n\r\n![Screenshot_20230318_121935](https://user-images.githubusercontent.com/4723091/226091335-afbf2712-d2b8-4b88-9b44-6b6a43d78565.png)\r\n\r\nYay yay yyayy yyayay\r\n\r\nthis is a memory allocation failure it seems, from me not having enough memory. not like llama.cpp Tells Me That lmao, it just segfaults\r\n\r\n(`ctx->mem_buffer` is nullptr which probably means the malloc just failed)",
    "labels": [
      "bug",
      "duplicate",
      "hardware",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-18T07:28:43+00:00",
    "closed_at": "2023-05-06T18:03:16+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/257/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/257"
  },
  {
    "number": 255,
    "title": "Interactive mode doesn't work",
    "body": "Hello, \r\nI wanted to test the interactive mode but it just doesn't work for me, the AI on its own with one promt gives me an output but with the command for a promt for the user it doesn't work and I just get \"dquote\" until I exit the program. \r\nThank you for your help!\r\n\r\n<img width=\"1728\" alt=\"Bildschirm\u00adfoto 2023-03-18 um 07 53 36\" src=\"https://user-images.githubusercontent.com/90244617/226090388-9e64b610-38f8-4800-a18a-3b8b3563a0c2.png\">\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-18T06:57:32+00:00",
    "closed_at": "2023-03-18T09:05:54+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/255/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/255"
  },
  {
    "number": 253,
    "title": "How to use it in Python",
    "body": "How to use this in my python code?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-18T04:46:55+00:00",
    "closed_at": "2023-03-18T04:58:21+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/253/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/253"
  },
  {
    "number": 251,
    "title": "65B quantized for CPU",
    "body": "Is there any way to run the 65B model on the CPU quantized for 4 bit? I saw that it's about 40 gigs for RAM usage when quantized.\r\n\r\nHow much RAM is required to quantize the 65B model? I'm not sure I have enough RAM to quantize myself, anyone have the model files for the quantized output for the 65B model for CPU? I've only found the [quantized GPU files](https://huggingface.co/decapoda-research) so far.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-18T02:47:36+00:00",
    "closed_at": "2023-03-18T05:59:53+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/251/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/251"
  },
  {
    "number": 249,
    "title": "Batch size affects model's output",
    "body": "I was tinkering with the code and made the following change in `line 977, main.cpp` (as it seemed wrong to me):\r\n*from*\r\n```C\r\nif (embd.size() > params.n_batch) {\r\n       break;\r\n}\r\n```\r\n*to*\r\n```C\r\nif (embd.size() >= params.n_batch) {\r\n       break;\r\n}\r\n```\r\n\r\nThe model's (13B) outputs suddenly changed. Reverted changes and tried to play with the `batch_size` parameter, it really does affect the output.\r\n\r\nNot sure if it's expected behaviour. As far as I understand it shouldn't be the case. A bug? Different batch sizes have different evaluation results (rounding error)?",
    "labels": [
      "bug",
      "generation quality"
    ],
    "state": "closed",
    "created_at": "2023-03-18T01:03:42+00:00",
    "closed_at": "2023-07-28T19:34:07+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/249/reactions",
      "total_count": 4,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/249"
  },
  {
    "number": 248,
    "title": "Rust Bindings",
    "body": "I saw that similar bindings were created for whisper.cpp in whisper-rs (https://github.com/tazz4843/whisper-rs), and I think it would be great to have similar bindings for llama.cpp as well.\r\n\r\nAs a Rust developer, I would use these to create an inference and embeeddings HTTP server and eventual create a Langchain binding for this (https://github.com/hwchase17/langchain/issues/1473).\r\n\r\nI'd be willing to help with the bindings in any way I can. I'll be using `whisper-rs` as the template. ",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-18T00:33:09+00:00",
    "closed_at": "2023-03-18T11:10:12+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/248/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/248"
  },
  {
    "number": 247,
    "title": "How to use ggml for Flan-T5",
    "body": "@ggerganov Thanks for sharing llama.cpp. As usual, great work.\r\n\r\nQuestion rather than issue.  How difficult would it be to make ggml.c work for a Flan checkpoint, like T5-xl/UL2, then quantized?\r\n\r\nWould love to be able to have those models run on a browser, much like what you did with whisper.cpp wasm.\r\n\r\nThanks again.  (I can move this post somewhere else if you prefer since it's not technically about Llama.  Just let me know where.)",
    "labels": [
      "enhancement",
      "model",
      "generation quality",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-17T22:38:08+00:00",
    "closed_at": "2024-04-14T01:06:18+00:00",
    "comments": 36,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/247/reactions",
      "total_count": 8,
      "+1": 8,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/247"
  },
  {
    "number": 240,
    "title": "Add instructions for using Alpaca",
    "body": "See the work here: https://github.com/antimatter15/alpaca.cpp\r\n\r\nThere is no new functionality added, just a few hardcoded parameters in `chat.cpp`.\r\nInstead of adding separate `chat` program, we should have an `alpaca.py` script that runs `main` with the respective parameters, so the user can simply run `./alpaca.py` on the terminal.\r\nIt is a good time to start collecting prompts, so create a few useful Alpaca instruction prompts and place them in a `prompts` folder in the source tree. Make the `alpaca.py` script use one of them by default. Add option to change.\r\n\r\nAdd short instructions for using the `alpaca.py` for various tasks (translation, answering, .. whatever is popular) in the README and reference the `alpaca.cpp` repo for downloading the models.",
    "labels": [
      "enhancement",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2023-03-17T15:52:57+00:00",
    "closed_at": "2023-03-19T16:51:05+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/240/reactions",
      "total_count": 20,
      "+1": 20,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/240"
  },
  {
    "number": 239,
    "title": "Create issue template for bug and enhancement issues",
    "body": "The following is a proposed template for creating new issues. If people think the tone could be improved, I'd appreciate feedback!\r\n___\r\n\r\n# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ ] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ ] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [ ] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ ] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nPlease provide a detailed written description of what you were trying to do, and what you expected `lamma.cpp` to do.\r\n\r\n# Current Behavior\r\n\r\nPlease provide a detailed written description of what `lamma.cpp` did, instead. \r\n\r\n# Environment and Context \r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n`$ lscpu`\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`$ uname -a`\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version\r\n$ make --version\r\n$ g++ --version\r\n```\r\n\r\n# Models\r\n\r\n* The LLaMA models are officially distributed by Facebook and will never be provided through this repository. See this [pull request in Facebook's LLaMA repository](https://github.com/facebookresearch/llama/pull/73/files) if you need to obtain access to the model data.\r\n* If your issue is with model conversion please verify the `sha256sum` of each of your `consolidated*.pth` and `ggml-model-XXX.bin` files to confirm that you have the correct model data files before logging an issue. [Latest sha256 sums for your reference](https://github.com/ggerganov/llama.cpp/issues/238).\r\n* If your issue is with model generation quality then please at least scan the following links and papers to understand the limitations of LLaMA models. This is especially important when choosing an appropriate model size and appreciating both the significant and subtle differences between LLaMA models and ChatGPT:\r\n  * LLaMA:\r\n    * [Introducing LLaMA: A foundational, 65-billion-parameter large language model](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)\r\n    * [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)\r\n  * GPT-3\r\n    * [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\r\n  * GPT-3.5 / InstructGPT / ChatGPT:\r\n    * [Aligning language models to follow instructions](https://openai.com/research/instruction-following)\r\n    * [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)\r\n\r\n# Failure Information (for bugs)\r\n\r\nPlease help provide information about the failure if this is a bug. If it is not a bug, please remove the rest of this template.\r\n\r\n# Steps to Reproduce\r\n\r\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\r\n\r\n1. step 1\r\n2. step 2\r\n3. step 3\r\n4. etc.\r\n\r\n# Failure Logs\r\n\r\nPlease include any relevant log snippets or files. If it works under one configuration but not under another, please provide logs for both configurations and their corresponding outputs so it is easy to see where behavior changes.\r\n\r\nAlso, please try to **avoid using screenshots** if at all possible. Instead, copy/paste the console output and use [Github's markdown](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) to cleanly format your logs for easy readability. e.g.\r\n\r\n```\r\nllama.cpp$ git log | head -1\r\ncommit 2af23d30434a677c6416812eea52ccc0af65119c\r\n\r\nllama.cpp$ lscpu | egrep \"AMD|Flags\"\r\nVendor ID:                       AuthenticAMD\r\nModel name:                      AMD Ryzen Threadripper 1950X 16-Core Processor\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sme sev\r\nVirtualization:                  AMD-V\r\n\r\nllama.cpp$ python3 --version\r\nPython 3.10.9\r\n\r\nllama.cpp$ pip list | egrep \"torch|numpy|sentencepiece\"\r\nnumpy                         1.24.2\r\nnumpydoc                      1.5.0\r\nsentencepiece                 0.1.97\r\ntorch                         1.13.1\r\ntorchvision                   0.14.1\r\n\r\nllama.cpp$ make --version | head -1\r\nGNU Make 4.3\r\n\r\n$ md5sum ./models/65B/ggml-model-q4_0.bin\r\ndbdd682cce80e2d6e93cefc7449df487  ./models/65B/ggml-model-q4_0.bin\r\n```\r\nHere's a run with the Linux command [perf](https://www.brendangregg.com/perf.html)\r\n\r\n```\r\nllama.cpp$ perf stat ./main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p \"Please close your issue when it has been answered.\"\r\nmain: seed = 1679149377\r\nllama_model_load: loading model from './models/65B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 8192\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 64\r\nllama_model_load: n_layer = 80\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 22016\r\nllama_model_load: n_parts = 8\r\nllama_model_load: ggml ctx size = 41477.73 MB\r\nllama_model_load: memory_size =  2560.00 MB, n_mem = 40960\r\nllama_model_load: loading model part 1/8 from './models/65B/ggml-model-q4_0.bin'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 2/8 from './models/65B/ggml-model-q4_0.bin.1'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 3/8 from './models/65B/ggml-model-q4_0.bin.2'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 4/8 from './models/65B/ggml-model-q4_0.bin.3'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 5/8 from './models/65B/ggml-model-q4_0.bin.4'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 6/8 from './models/65B/ggml-model-q4_0.bin.5'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 7/8 from './models/65B/ggml-model-q4_0.bin.6'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 8/8 from './models/65B/ggml-model-q4_0.bin.7'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\n\r\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \r\n\r\nmain: prompt: 'Please close your issue when it has been answered.'\r\nmain: number of tokens in prompt = 11\r\n     1 -> ''\r\n 12148 -> 'Please'\r\n  3802 -> ' close'\r\n   596 -> ' your'\r\n  2228 -> ' issue'\r\n   746 -> ' when'\r\n   372 -> ' it'\r\n   756 -> ' has'\r\n  1063 -> ' been'\r\n  7699 -> ' answered'\r\n 29889 -> '.'\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\nPlease close your issue when it has been answered.\r\n@duncan-donut: I'm trying to figure out what kind of \"support\" you need for this script and why, exactly? Is there a question about how the code works that hasn't already been addressed in one or more comments below this ticket, or are we talking something else entirely like some sorta bugfixing job because your server setup is different from mine??\r\nI can understand if your site needs to be running smoothly and you need help with a fix of sorts but there should really be nothing wrong here that the code itself could not handle. And given that I'm getting reports about how it works perfectly well on some other servers, what exactly are we talking? A detailed report will do wonders in helping us get this resolved for ya quickly so please take your time and describe the issue(s) you see as clearly & concisely as possible!!\r\n@duncan-donut: I'm not sure if you have access to cPanel but you could try these instructions. It is worth a shot! Let me know how it goes (or what error message, exactly!) when/if ya give that code a go? [end of text]\r\n\r\n\r\nmain: mem per token = 71159620 bytes\r\nmain:     load time = 19309.95 ms\r\nmain:   sample time =   168.62 ms\r\nmain:  predict time = 223895.61 ms / 888.47 ms per token\r\nmain:    total time = 246406.42 ms\r\n\r\n Performance counter stats for './main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p Please close your issue when it has been answered.':\r\n\r\n        3636882.89 msec task-clock                #   14.677 CPUs utilized          \r\n             13509      context-switches          #    3.714 /sec                   \r\n              2436      cpu-migrations            #    0.670 /sec                   \r\n          10476679      page-faults               #    2.881 K/sec                  \r\n    13133115082869      cycles                    #    3.611 GHz                      (16.77%)\r\n       29314462753      stalled-cycles-frontend   #    0.22% frontend cycles idle     (16.76%)\r\n    10294402631459      stalled-cycles-backend    #   78.39% backend cycles idle      (16.74%)\r\n    23479217109614      instructions              #    1.79  insn per cycle         \r\n                                                  #    0.44  stalled cycles per insn  (16.76%)\r\n     2353072268027      branches                  #  647.002 M/sec                    (16.77%)\r\n        1998682780      branch-misses             #    0.08% of all branches          (16.76%)\r\n\r\n     247.802177522 seconds time elapsed\r\n\r\n    3618.573072000 seconds user\r\n      18.491698000 seconds sys\r\n```",
    "labels": [
      "documentation",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2023-03-17T13:38:57+00:00",
    "closed_at": "2023-03-21T17:50:50+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/239/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/239"
  },
  {
    "number": 238,
    "title": "Document check sums of models so that we can confirm issues are not caused by bad downloads or conversion",
    "body": "Can someone please confirm the following md5 sums are correct?  I regenerated them with the latest code.\r\n\r\n```\r\n$ md5sum ./models/*/*.pth | sort -k 2,2\r\n0804c42ca65584f50234a86d71e6916a  ./models/13B/consolidated.00.pth\r\n016017be6040da87604f77703b92f2bc  ./models/13B/consolidated.01.pth\r\nf856e9d99c30855d6ead4d00cc3a5573  ./models/30B/consolidated.00.pth\r\nd9dbfbea61309dc1e087f5081e98331a  ./models/30B/consolidated.01.pth\r\n2b2bed47912ceb828c0a37aac4b99073  ./models/30B/consolidated.02.pth\r\nea0405cdb5bc638fee12de614f729ebc  ./models/30B/consolidated.03.pth\r\n9deae67e2e7b5ccfb2c738f390c00854  ./models/65B/consolidated.00.pth\r\n0c4b00c30460c3818bd184ee949079ee  ./models/65B/consolidated.01.pth\r\n847194df776dd38f8ae9ddcede8829a1  ./models/65B/consolidated.02.pth\r\n3b6c8adcb5654fd36abab3206b46a0f1  ./models/65B/consolidated.03.pth\r\n68d61d1242597ad92616ec31b8cb6b4c  ./models/65B/consolidated.04.pth\r\n7f71259eaee2b906aa405d8edf39925f  ./models/65B/consolidated.05.pth\r\n0574e26b6891ab2cb0df7340d773fe9b  ./models/65B/consolidated.06.pth\r\ne5d9790df955270b836aec79462ead22  ./models/65B/consolidated.07.pth\r\n6efc8dab194ab59e49cd24be5574d85e  ./models/7B/consolidated.00.pth\r\n```\r\n\r\n\r\n<details>\r\n<summary>Edit: File format has changed. Don\u2019t use these collapsed weights!</summary>\r\n\r\n```\r\n$ md5sum ./models/*/ggml-model-f16* | sort -k 2,2\r\n0d851faaf144ff75ff9683685cbcbedc  ./models/13B/ggml-model-f16.bin\r\n5cde948c6a27f41dc822b1a8a0587e79  ./models/13B/ggml-model-f16.bin.1\r\nc80e0c824c7e853c3d5be915afb37eef  ./models/30B/ggml-model-f16.bin\r\n72da29fca244f2a64f85b2c14b20290d  ./models/30B/ggml-model-f16.bin.1\r\n16f07b182f44116fd72a9cc174dc0db2  ./models/30B/ggml-model-f16.bin.2\r\n2413e326c00b476e8cd13d5f1fe65854  ./models/30B/ggml-model-f16.bin.3\r\neb8f7835d1d7e716f96af02fefdd5c04  ./models/65B/ggml-model-f16.bin\r\n30f08121b86fe90db2497bd87f844d3b  ./models/65B/ggml-model-f16.bin.1\r\n98983c0e2338d2985a0d9bb8bd27efb5  ./models/65B/ggml-model-f16.bin.2\r\n635ebf87ef9053f7facccc665a0c826a  ./models/65B/ggml-model-f16.bin.3\r\n6ca89293e1a9c8ad96b476406739827c  ./models/65B/ggml-model-f16.bin.4\r\n696e4afe846ddfe2a2366db927a0dffa  ./models/65B/ggml-model-f16.bin.5\r\n39a7f52b968aa833212c027d6fd58ccf  ./models/65B/ggml-model-f16.bin.6\r\na8ac8b55c152565573b118b0a0109726  ./models/65B/ggml-model-f16.bin.7\r\n0fd0234fd08a7310f93f64faff7fda15  ./models/7B/ggml-model-f16.bin\r\n```\r\n\r\n\r\n```\r\n$ md5sum ./models/*/ggml-model-q4_0* | sort -k 2,2\r\nb405d83aff658379cc8b1b59b9a39668  ./models/13B/ggml-model-q4_0.bin\r\nb06456f82bbc9d1fd46afa635ce0eba4  ./models/13B/ggml-model-q4_0.bin.1\r\nc8bdc3fedd676b4c30bcc61812dab84f  ./models/30B/ggml-model-q4_0.bin\r\naad0750e54004014b65fa65aedacdf84  ./models/30B/ggml-model-q4_0.bin.1\r\n88876dca38cedf53ba0a915e817921ed  ./models/30B/ggml-model-q4_0.bin.2\r\n4063e11be83d342893ba4e3e299a4436  ./models/30B/ggml-model-q4_0.bin.3\r\ndbdd682cce80e2d6e93cefc7449df487  ./models/65B/ggml-model-q4_0.bin\r\n5d7c7e0e30b351af5237b81852e4b01b  ./models/65B/ggml-model-q4_0.bin.1\r\n2ca89995c8c17890b2935022aede929e  ./models/65B/ggml-model-q4_0.bin.2\r\n88e36f69163fe09da11531332410f4d4  ./models/65B/ggml-model-q4_0.bin.3\r\n4fe105f7d77d54d94daa33bbfd582733  ./models/65B/ggml-model-q4_0.bin.4\r\n1106d57cdf87ecbf83540f3a0027b480  ./models/65B/ggml-model-q4_0.bin.5\r\nc5759417ae123248bb2cecf85546680f  ./models/65B/ggml-model-q4_0.bin.6\r\ncedfc3b77578db761f871f8c8baa8323  ./models/65B/ggml-model-q4_0.bin.7\r\n919e4f8aee6ce4f3fbabb6cbcd7756db  ./models/7B/ggml-model-q4_0.bin\r\n```\r\n\r\n</details>",
    "labels": [
      "documentation",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-17T12:50:44+00:00",
    "closed_at": "2023-05-02T13:41:32+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/238/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/238"
  },
  {
    "number": 237,
    "title": "No output after commit 84d9015 on Android",
    "body": "### Discussed in https://github.com/ggerganov/llama.cpp/discussions/234\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **ShouNichi** March 17, 2023</sup>\r\nWhen `git checkout 84d9015` and `make`, there will be no output (only the model loading message) in termux.\r\n`git checkout 63fd76f` will produce a fully-functional binary.</div>\r\n\r\nI've moved this to issues. Please provide sample output from the working build and the non-working build.",
    "labels": [
      "bug",
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-17T10:51:12+00:00",
    "closed_at": "2023-07-28T19:33:04+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/237/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/237"
  },
  {
    "number": 233,
    "title": "Thanks for contributing to Machine Learning and AI with this repository!",
    "body": "I just want to say Thank You!\r\n\r\nI got your solution working and I am happy to confirm that it is working as intended. The quality of text produced is probably a bit lower compared to text produced with the full weights, but the quantized weights saves a lot of space and maybe some processing time.\r\n\r\nI would like to see the solution rewritten as C# to better understand it, since I am using C# at work but do not use C++ och C since I was studying programming in the 90s. I do understand this is not the main goal of this project, but in case someone else wants to do this or have already done this - please leave me a note.\r\n\r\nThanks again for this lovely work!",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-17T09:15:17+00:00",
    "closed_at": "2023-03-17T10:46:03+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/233/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/233"
  },
  {
    "number": 231,
    "title": "Study how LM Evaluation Harness works and try to implement it",
    "body": "Update 10 Apr 2024: https://github.com/ggerganov/llama.cpp/issues/231#issuecomment-2047759312\r\n\r\n---\r\n\r\nIt would be great to start doing this kind of quantitative analysis of `ggml`-based inference:\r\n\r\nhttps://bellard.org/ts_server/\r\n\r\nIt looks like Fabrice evaluates the models using something called LM Evaluation Harness:\r\n\r\nhttps://github.com/EleutherAI/lm-evaluation-harness\r\n\r\nI have no idea what this is yet, but would be nice to study it and try to integrate it here and in other `ggml`-based projects.\r\nThis will be very important step needed to estimate the quality of the generated output and see if we are on the right track.",
    "labels": [
      "enhancement",
      "help wanted",
      "high priority",
      "generation quality",
      "research \ud83d\udd2c"
    ],
    "state": "open",
    "created_at": "2023-03-17T08:32:33+00:00",
    "closed_at": null,
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/231/reactions",
      "total_count": 8,
      "+1": 8,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/231"
  },
  {
    "number": 228,
    "title": "7B/13B: Inability to write certain words/names with smaller models",
    "body": "Hey!\r\n\r\nWhen I attempted to tell the bot in a chat-like prompt that my name is \"Nils\", I ran into an issue where the bot kept interpreting my name as \"Nil\" instead. I then noticed further issues with the word \"guild\" and some other words too.\r\nIs this a bug or to be expected? It does not happen on 30B, I couldn't give 65B a try.\r\n\r\nThanks\r\nNiansa",
    "labels": [
      "question",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-17T07:46:08+00:00",
    "closed_at": "2023-03-17T08:31:37+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/228/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/228"
  },
  {
    "number": 225,
    "title": "convert-pth-to-ggml.py how to handle torch.view_as_complex",
    "body": "llama code block include view_as_real: https://github.com/facebookresearch/llama/blob/main/llama/model.py#L68\r\n\r\nhow to convert-pth-to-ggml.py handle this part of weight",
    "labels": [
      "question",
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-17T05:28:02+00:00",
    "closed_at": "2023-04-10T08:11:28+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/225/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/225"
  },
  {
    "number": 224,
    "title": "How do I get input embeddings?",
    "body": "I am trying to output just the sentence embedding for a given input, instead of any new generated text. I think this should be rather straightforward but figured someone more familiar with the codebase could help me.\r\n\r\nI just want to return the sentence embedding vector and stop execution for a given input.\r\n\r\nI am almost sure the place where I want to make the embedding is right after `norm` but before `lm_head`, and I think they will be in `inpL` if I run \r\n\r\n```\r\nggml_build_forward_expand(&gf, inpL);\r\nggml_graph_compute       (ctx0, &gf);\r\n``` \r\nHowever I am confused by the struct and not sure how to get the sentence embedding itself. I understand it should be some index of ggml_get_data(inpL), but don't get which index, and that is why I come to you. Would anyone lend me a hand?",
    "labels": [
      "question"
    ],
    "state": "closed",
    "created_at": "2023-03-17T04:59:12+00:00",
    "closed_at": "2023-04-16T09:25:29+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/224/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 2,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/224"
  },
  {
    "number": 223,
    "title": "[QUESTION] data type",
    "body": "I see that it says using float16 float32 mixed precision, but as we are talking about characters, shouldn't it uses char8 ?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-17T03:51:39+00:00",
    "closed_at": "2023-03-17T04:02:29+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/223/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/223"
  },
  {
    "number": 219,
    "title": "Can this code base be extended to support other transformer-based LLMs such as Pythia or its instruction-tuned version Open Assistant?",
    "body": null,
    "labels": [
      "enhancement",
      "question",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-17T01:46:50+00:00",
    "closed_at": "2023-07-28T19:32:44+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/219/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/219"
  },
  {
    "number": 214,
    "title": "GPU instead CPU?",
    "body": "How can we use GPU instead of CPU? My processor is pretty weak. I don't have a macbook or a very powerful pc. the desire to run a model on CUDA cores. Thanks",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-16T21:51:51+00:00",
    "closed_at": "2023-03-16T22:06:00+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/214/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/214"
  },
  {
    "number": 212,
    "title": "Stops generating after about 200 characters",
    "body": "I have tried modifying the -n (number of tokens to predict) but it always stops generating after the same amount of time. Is there any way to stop this happening? It seems to be intended behavior since it shows in the README screenshots",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-16T19:53:41+00:00",
    "closed_at": "2023-03-16T19:59:53+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/212/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/212"
  },
  {
    "number": 210,
    "title": "Cannot generate more than 500 words",
    "body": "The model doesn't seem to be able to return more than 500 words regardless of how big the number of tokens is specified (I even tried specifically powers of 2 such as 4096 with no results), it always stops and leaves texts uncomplete. Is anyone having the same issue, or how can I increment the length of the output?",
    "labels": [
      "duplicate",
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-16T16:18:36+00:00",
    "closed_at": "2023-03-16T16:25:39+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/210/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/210"
  },
  {
    "number": 209,
    "title": "Command line script usage",
    "body": "Hello, \r\n\r\nI was wondering if there was a command line flag for toggling the output of the debug messages, making the executable only output the text generated by the LLM (optionally with the original prompt). This would make the program much easier to call from other scripts.\r\n\r\nThanks for your time.",
    "labels": [
      "duplicate",
      "wontfix"
    ],
    "state": "closed",
    "created_at": "2023-03-16T15:57:54+00:00",
    "closed_at": "2023-03-16T16:27:50+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/209/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/209"
  },
  {
    "number": 208,
    "title": "making on linuxmint 21",
    "body": "im running on bare metal nothing emulated\r\n\r\n```\r\nlittlemac@littlemac:~$` git clone https://github.com/ggerganov/llama.cpp\r\nCloning into 'llama.cpp'...\r\nremote: Enumerating objects: 283, done.\r\nremote: Counting objects: 100% (283/283), done.\r\nremote: Compressing objects: 100% (113/113), done.\r\nremote: Total 283 (delta 180), reused 255 (delta 164), pack-reused 0\r\nReceiving objects: 100% (283/283), 158.38 KiB | 609.00 KiB/s, done.\r\nResolving deltas: 100% (180/180), done.\r\ncd littlemac@littlemac:~$ cd llama.cpp/\r\nlittlemac@littlemac:~/llama.cpp$ make\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -msse3\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\r\nI LDFLAGS:  \r\nI CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\nI CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -msse3   -c ggml.c -o ggml.o\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101,\r\n                 from ggml.c:155:\r\nggml.c: In function \u2018ggml_vec_dot_f16\u2019:\r\n/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1274:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1274 |             ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1273:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1273 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1273:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1273 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1274:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1274 |             ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1273:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1273 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:101,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/11/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1274:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1274 |             ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nmake: *** [Makefile:186: ggml.o] Error 1\r\nlittlemac@littlemac:~/llama.cpp$ cpu-x -D\r\nYour CPU socket is not present in the database ==> Intel(R) Core(TM) i5-2500K CPU @ 3.30GHz, codename: Sandy Bridge (Core i5)\r\nCPU-X:core.c:1637: failed to retrieve CPU voltage (fallback mode)\r\n  >>>>>>>>>> CPU <<<<<<<<<<\r\n\r\n\t***** Processor *****\r\n          Vendor: Intel\r\n       Code Name: Sandy Bridge (Core i5)\r\n         Package: \r\n      Technology: 32 nm\r\n         Voltage: \r\n   Specification: Intel(R) Core(TM) i5-2500K CPU @ 3.30GHz\r\n          Family: 0x6\r\n     Ext. Family: 0x6\r\n           Model: 0xA\r\n      Ext. Model: 0x2A\r\n           Temp.: 28.00\u00b0C\r\n        Stepping: 7\r\n    Instructions: MMX, SSE(1, 2, 3, 3S, 4.1, 4.2), AVX(1), AES, CLMUL, VT-x, x86-64\r\n\r\n\t***** Clocks *****\r\n      Core Speed: 3679 MHz\r\n      Multiplier: \r\n       Bus Speed: \r\n           Usage:  15.22 %\r\n\r\n\t***** Cache *****\r\n         L1 Data: 4 x 32 kB, 8-way\r\n        L1 Inst.: 4 x 32 kB, 8-way\r\n         Level 2: 4 x 256 kB, 8-way\r\n         Level 3: 6 MB, 12-way\r\n\r\n\t***** * *****\r\n       Socket(s): 1\r\n         Core(s): 4\r\n       Thread(s): 4\r\n\r\n\r\n  >>>>>>>>>> Caches <<<<<<<<<<\r\n\r\n\t***** L1 Cache *****\r\n            Size: 4 x 32 kB, 8-way associative, 64-bytes line size\r\n           Speed: 110315.60 MB/s\r\n\r\n\t***** L2 Cache *****\r\n            Size: 4 x 256 kB, 8-way associative, 64-bytes line size\r\n           Speed: 53894.20 MB/s\r\n\r\n\t***** L3 Cache *****\r\n            Size: 6 MB, 12-way associative, 64-bytes line size\r\n           Speed: 33268.30 MB/s\r\n\r\n\r\n  >>>>>>>>>> Motherboard <<<<<<<<<<\r\n\r\n\t***** Motherboard *****\r\n    Manufacturer: MSI\r\n           Model: Z77A-G43 (MS-7758)\r\n        Revision: 1.0\r\n\r\n\t***** BIOS *****\r\n           Brand: American Megatrends Inc.\r\n         Version: V2.7\r\n            Date: 10/24/2012\r\n        ROM Size: \r\n\r\n\t***** Chipset *****\r\n          Vendor: Intel Corporation\r\n           Model: Z77 Express Chipset LPC Controller\r\n\r\n\r\n  >>>>>>>>>> Memory <<<<<<<<<<\r\n\r\n\r\n  >>>>>>>>>> System <<<<<<<<<<\r\n\r\n\t***** Operating System *****\r\n          Kernel: Linux 5.15.0-67-generic\r\n    Distribution: Linux Mint 21.1\r\n        Hostname: littlemac\r\n          Uptime: 0 days, 0 hours, 12 minutes, 29 seconds\r\n        Compiler: cc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\n\r\n\t***** Memory *****\r\n            Used: 2.17 GiB / 7.71 GiB\r\n         Buffers: 0.12 GiB / 7.71 GiB\r\n          Cached: 3.86 GiB / 7.71 GiB\r\n            Free: 1.55 GiB / 7.71 GiB\r\n            Swap: 0.00 GiB / 5.85 GiB\r\n\r\n\r\n  >>>>>>>>>> Graphics <<<<<<<<<<\r\n\r\n\t***** Card 0 *****\r\n          Vendor: NVIDIA\r\n          Driver: nvidia\r\n     UMD Version: NVIDIA 515.86.01\r\n           Model: GM206 [GeForce GTX 960]\r\n        DeviceID: 0x1401:0xA1\r\n       Interface: \r\n     Temperature: 34.00\u00b0C\r\n           Usage: 2%\r\n    Core Voltage: \r\n       Power Avg: 25.00 W\r\n       GPU clock: 1126 MHz\r\n    Memory clock: 3004 MHz\r\n     Memory Used: 342 MiB / 2048 MiB\r\n```",
    "labels": [
      "duplicate",
      "hardware",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-16T13:52:27+00:00",
    "closed_at": "2023-05-06T17:55:19+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/208/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/208"
  },
  {
    "number": 204,
    "title": "Model runs but doesn't produce any output",
    "body": "I checked everything several times and quantized it, but both models do not output anything, in which mode I would not run them, the processor loads, but there is no output, no matter how long I wait\r\n input to the console also does not lead to anything\r\n\r\nfor ubuntu 22.04 8gb+15 swap (everything fits)\r\n\r\n\r\n![\u0421\u043d\u0438\u043c\u043e\u043a \u044d\u043a\u0440\u0430\u043d\u0430 \u043e\u0442 2023-03-16 11-42-21](https://user-images.githubusercontent.com/93709232/225592978-99f3c8a6-85a0-4606-a39d-6ddc1e334778.png)\r\n",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-16T10:46:53+00:00",
    "closed_at": "2023-03-16T12:52:24+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/204/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/204"
  },
  {
    "number": 203,
    "title": "Alpaca and Llama",
    "body": "Maybe it could work also on [Stanford's Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html), when they will release their weights.\r\n\r\n",
    "labels": [
      "duplicate"
    ],
    "state": "closed",
    "created_at": "2023-03-16T10:09:19+00:00",
    "closed_at": "2023-03-16T11:42:32+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/203/reactions",
      "total_count": 2,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/203"
  },
  {
    "number": 202,
    "title": "Reducing the time needed to reload a piece of text into the model by caching the state",
    "body": "Hey!\r\n\r\nIs it possible to add a way of dumping the current state into a file, so it can then be reloaded later? This would avoid the time needed to reload a long prompt over and over again.\r\n\r\nThanks\r\nNiansa",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-16T09:27:24+00:00",
    "closed_at": "2023-03-30T17:42:34+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/202/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/202"
  },
  {
    "number": 200,
    "title": "Running \" python3 convert-pth-to-ggml.py models/7B/ 1 \" and running out of RAM",
    "body": null,
    "labels": [
      "wontfix",
      "need more info",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-16T09:01:36+00:00",
    "closed_at": "2023-03-16T15:04:32+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/200/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/200"
  },
  {
    "number": 196,
    "title": "Error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019 on x86_64 - better support for different x86_64 CPU instruction extensions",
    "body": "When I compile with make, the following error occurs\r\n```\r\ninlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n```\r\n\r\nError will be reported when executing `cc  -I.   -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -msse3   -c ggml.c -o ggml.o` .\r\nBut the error of executing `cc  -I.   -O3 -DNDEBUG -std=c11   -fPIC -pthread  -msse3   -c ggml.c -o ggml.o` will not occur.\r\nMust `-mavx` be used with `-mf16c`?\r\n\r\n---\r\nOS: Arch Linux x86_64\r\nKernel: 6.1.18-1-lts",
    "labels": [
      "bug",
      "performance",
      "hardware",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-16T04:17:08+00:00",
    "closed_at": "2023-03-30T08:31:50+00:00",
    "comments": 35,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/196/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/196"
  },
  {
    "number": 195,
    "title": "Add the disk requirements",
    "body": "Hi,\r\n\r\nI found all the infos about the models:\r\nhttps://cocktailpeanut.github.io/dalai/#/?id=_7b\r\n\r\nYou can put on readme the space requirements.\r\n\r\nThanks.",
    "labels": [
      "documentation",
      "duplicate"
    ],
    "state": "closed",
    "created_at": "2023-03-16T03:23:50+00:00",
    "closed_at": "2023-03-16T11:54:44+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/195/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/195"
  },
  {
    "number": 194,
    "title": "Supported context window length for each model?",
    "body": "what's the supported context window length for each model?",
    "labels": [
      "model",
      "generation quality"
    ],
    "state": "closed",
    "created_at": "2023-03-16T02:27:23+00:00",
    "closed_at": "2023-03-24T10:34:27+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/194/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/194"
  },
  {
    "number": 190,
    "title": "new RMS norm PR bricks stuff",
    "body": "#187 \r\n\r\nNo output from the model after this was merged.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-15T23:02:41+00:00",
    "closed_at": "2023-03-15T23:29:27+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/190/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/190"
  },
  {
    "number": 188,
    "title": "How to? (install models)",
    "body": "Hi, i can't find the models\r\nCan u tell me, how i can install?\r\n(ls ./models 65B etc is not working)\r\n*sorry, my english isn't good) ",
    "labels": [
      "question",
      "\ud83e\udd99."
    ],
    "state": "closed",
    "created_at": "2023-03-15T22:51:14+00:00",
    "closed_at": "2023-03-16T11:31:34+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/188/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/188"
  },
  {
    "number": 184,
    "title": "Python3 script  instead of bash",
    "body": "```python\r\n#!/usr/bin/env python3\r\n\r\nimport os\r\nimport sys\r\n\r\nif not (len(sys.argv) == 2 and sys.argv[1] in [\"7B\", \"13B\", \"30B\", \"65B\"]):\r\n    print(f\"\\nUsage: {sys.argv[0]} 7B|13B|30B|65B [--remove-f16]\\n\")\r\n    sys.exit(1)\r\n\r\nfor i in os.listdir(f\"models/{sys.argv[1]}\"):\r\n    if i.endswith(\"ggml-model-f16.bin\"):\r\n        os.system(f\"./quantize {os.path.join('models', sys.argv[1], i)} {os.path.join('models', sys.argv[1], i.replace('f16', 'q4_0'))} 2\")\r\n        if len(sys.argv) == 3 and sys.argv[2] == \"--remove-f16\":\r\n            os.remove(os.path.join('models', sys.argv[1], i))\r\n``` ",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-15T21:54:35+00:00",
    "closed_at": "2023-03-19T19:53:05+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/184/reactions",
      "total_count": 8,
      "+1": 3,
      "-1": 5,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/184"
  },
  {
    "number": 174,
    "title": "Question: can the conversation context be saved to disk and brought up again incase LLaMa crashes or there is a power failure?",
    "body": null,
    "labels": [
      "duplicate"
    ],
    "state": "closed",
    "created_at": "2023-03-15T20:01:08+00:00",
    "closed_at": "2023-03-16T11:46:57+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/174/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/174"
  },
  {
    "number": 173,
    "title": "Use RMSNorm",
    "body": "The original paper, and the reference implementation [1] uses RMS norm. However, llama.cpp uses ggml_norm() which looks like Layer norm?\r\n\r\nThe differences between these may not be too obvious, because the mean is probably around 0. However, we should follow the original design.\r\n\r\n[1] https://github.com/facebookresearch/llama/blob/main/llama/model.py",
    "labels": [
      "bug",
      "help wanted",
      "good first issue",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2023-03-15T19:05:29+00:00",
    "closed_at": "2023-03-19T15:31:53+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/173/reactions",
      "total_count": 2,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/173"
  },
  {
    "number": 172,
    "title": "Attempting to merge with alpaca-lora and its quantization",
    "body": "I was attempting to merge alpaca-lora from https://huggingface.co/tloen/alpaca-lora-7b and the original llama-7B from https://huggingface.co/decapoda-research/llama-7b-hf, also tried to quantize the model and run main file in llama.cpp.\r\nThe merge code is from https://github.com/clcarwin/alpaca-weight\r\n\r\nIt was almost successful until final phase to run the main file in llam.cpp. I had no problems with merge and quantization.\r\n\r\nThen it raised an error like this:\r\n\r\nllama_model_load: llama_model_load: unknown tensor 'model.embed_tokens.weight' in model file\r\nmain: failed to load model from './models/7B/ggml-model-q4_0.bin'\r\n\r\nI will share my logs in my repository. The code I used in colab to merge and quantize the model is there too: https://github.com/taiyou2000/personal_experimant\r\n\r\nI'm not machine learning expert and I have not checked entire llama.cpp code, but in my theory maybe the quantized model contains weights and some of them has names that main.cpp doesn't expect to see. As you can see in quantization_log.txt and pth_to_ggml_log.txt from my repository, it has names like \"model.layers.0.self_attn.q_proj.weight\", and probably it should be like \"model.layers.0.attention.wq.weight\" for main.cpp. \r\nI can run llama.cpp without any problems on my local computer and the model is quantized from torrent version. I guess huggingface version has something different from it.",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2023-03-15T18:16:30+00:00",
    "closed_at": "2023-07-28T19:32:24+00:00",
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/172/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/172"
  },
  {
    "number": 171,
    "title": "[Proposal] \"Stable\" C API",
    "body": "I propose refactoring `main.cpp` into a library (`llama.cpp`, compiled to `llama.so`/`llama.a`/whatever) and making `main.cpp` a simple driver program. A simple C API should be exposed to access the model, and then bindings can more easily be written for Python, node.js, or whatever other language.\r\n\r\nThis would partially solve #82 and #162.\r\n\r\nEdit: on that note, is it possible to do inference from two or more prompts on different threads? If so, serving multiple people would be possible without multiple copies of model weights in RAM.",
    "labels": [
      "duplicate",
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-15T18:01:09+00:00",
    "closed_at": "2023-03-15T20:29:20+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/171/reactions",
      "total_count": 10,
      "+1": 10,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/171"
  },
  {
    "number": 170,
    "title": "Converted GGML models hosting?",
    "body": "Apologies if Github Issues is not the right place for this question, but do you know if anyone has hosted the ggml versions of the models? The disk space required to download and convert is a little steep.",
    "labels": [
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-15T18:00:46+00:00",
    "closed_at": "2023-03-15T20:53:08+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/170/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/170"
  },
  {
    "number": 167,
    "title": "Differences with the llama tokenizer",
    "body": "In this case the llama.cpp and the llama tokenizers produce different output:\r\n\r\n```\r\nmain: prompt: 'This is \ud83e\udd99.cpp'\r\nmain: number of tokens in prompt = 10\r\n     1 -> ''\r\n  4013 -> 'This'\r\n   338 -> ' is'\r\n 29871 -> ' '\r\n   243 -> '\ufffd'\r\n   162 -> '\ufffd'\r\n   169 -> '\ufffd'\r\n   156 -> '\ufffd'\r\n 29889 -> '.'\r\n  8223 -> 'cpp'\r\n```\r\n\r\nMeanwhile the llama tokenizer produces:\r\n\r\n```\r\ntext = \"This is \ud83e\udd99.cpp\"\r\nt = tokenizer.encode(text, bos=True, eos=False)\r\n\r\n[1, 910, 338, 29871, 243, 162, 169, 156, 29889, 8223]\r\n```\r\n\r\nSo in one case \"This\" is encoded as 4013 and other as 910. I have verified that both ids decode to the same text:\r\n\r\n```\r\nt1 = tokenizer.decode([4013])\r\nt2 = tokenizer.decode([910])\r\nprint(t1, [int(b) for b in bytes(t1, \"UTF-8\")])\r\nprint(t2, [int(b) for b in bytes(t2, \"UTF-8\")])\r\n\r\nThis [84, 104, 105, 115]\r\nThis [84, 104, 105, 115]\r\n```\r\n\r\nI am not sure if this causes any significant differences in the generation but it may be a good idea to check it.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-03-15T16:45:04+00:00",
    "closed_at": "2023-03-20T15:21:55+00:00",
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/167/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/167"
  },
  {
    "number": 165,
    "title": "RISC-V support?",
    "body": "By deleting line 155 (#include <immintrin.h>) in ggml.c, it works just fine on RISC-V.\r\nMaybe this can be added in Cmake?",
    "labels": [
      "enhancement",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-15T16:07:46+00:00",
    "closed_at": "2023-07-07T13:48:11+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/165/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/165"
  },
  {
    "number": 164,
    "title": "Will there ever be a GPU support for Apple Silicon?",
    "body": "I really thank you for the possibility of running the model on my MacBook Air M1. I've been testing various parameters and I'm happy even with the 7B model. However, do you plan to utilize the GPU of M1/M2 chip? Thank you in advance.",
    "labels": [
      "enhancement",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-15T16:06:51+00:00",
    "closed_at": "2023-03-15T20:10:04+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/164/reactions",
      "total_count": 4,
      "+1": 2,
      "-1": 0,
      "laugh": 1,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/164"
  },
  {
    "number": 163,
    "title": "Not an issue but what depends on the number of threads?",
    "body": "I've been testing your code from 1 to 8 threads and the output is always different. The speed is not depend on the number of threads. On the contrary, 4 threads may perform much better than 1, whereas 8 threads supposedly provides a better result. However, the same prompt may give the same excellent output with triple speed with 4 threads compared to 8. But still, when I use 8 threads (my maximum on M1) I use all my CPU resources, but it doesn't affect speed at all (seemingly works slower) and not giving quality effect (apparently). Am I wrong? Can you correct me if I'm mistaken? May be there is some best speed/quality option and I just that stupid that was unable to figure out how to use this option?",
    "labels": [
      "performance"
    ],
    "state": "closed",
    "created_at": "2023-03-15T16:03:26+00:00",
    "closed_at": "2023-03-15T20:54:16+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/163/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/163"
  },
  {
    "number": 162,
    "title": "feature request, restful api / exposure",
    "body": "hi team,\r\n\r\nwas playing interactive mode for couple hours, pretty impressive\r\n\r\nresides what's mentioned in #145 , \r\nit might be not too far, to plug this a endpoint / functional call ( like swig or socket or openapi to replace current stdin ?, then self-host can have a very powerful new residents, like i got a powerful PC at home to be personal assist\r\n\r\nalso found that `-n` is the context / token limit, would be great if engine can start with 0 presume context ( which is to lift off / decouple a bit from stdin \r\n\r\nkindly let me know if there are directions or others interested in this ( also a developer here but not so C / tensor flavored \r\n( as without advice, force hi-jack stdin / stdout seems stupid ",
    "labels": [
      "duplicate",
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-15T15:50:42+00:00",
    "closed_at": "2023-03-15T21:07:48+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/162/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/162"
  },
  {
    "number": 160,
    "title": "Add avx-512 support?",
    "body": "No clue but I think it may work faster",
    "labels": [
      "enhancement",
      "performance",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-15T12:10:17+00:00",
    "closed_at": "2023-03-28T09:54:15+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/160/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 2,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/160"
  },
  {
    "number": 159,
    "title": "Unable to compile - error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019",
    "body": "Hi, I downloaded the files with git and run make just as in the instruction. Unfortunately, the compilation is not working. Can someone help me figure out what's going wrong here?\r\n\r\nI'm adding the full error in the following.\r\n\r\n``In file included from /usr/lib/gcc/x86_64-linux-gnu/10/include/immintrin.h:113,\r\n                 from ggml.c:155:\r\nggml.c: In function \u2018ggml_vec_dot_f16\u2019:\r\n/usr/lib/gcc/x86_64-linux-gnu/10/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1274:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1274 |             ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/10/include/immintrin.h:113,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/10/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1273:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1273 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/10/include/immintrin.h:113,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/10/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1273:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1273 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/10/include/immintrin.h:113,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/10/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1274:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1274 |             ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/10/include/immintrin.h:113,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/10/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1273:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1273 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/10/include/immintrin.h:113,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/10/include/f16cintrin.h:52:1: error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_cvtph_ps\u2019: target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro \u2018GGML_F32Cx8_LOAD\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1274:21: note: in expansion of macro \u2018GGML_F16_VEC_LOAD\u2019\r\n 1274 |             ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nmake: *** [Makefile:186: ggml.o] Error 1\r\n``",
    "labels": [
      "hardware",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-15T10:53:18+00:00",
    "closed_at": "2023-03-15T15:23:31+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/159/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/159"
  },
  {
    "number": 158,
    "title": "is it possible to use llama,cpp with other neural networks?",
    "body": "I have no clue about this, but I saw that chatglm-6b was published, which should run on CPU with 16GB ram, albeit very slow.\r\n[https://huggingface.co/THUDM/chatglm-6b/tree/main](url)\r\n\r\nWould it be possible to substitute the llama model?",
    "labels": [
      "enhancement",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-15T09:31:55+00:00",
    "closed_at": "2023-07-28T19:32:08+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/158/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/158"
  },
  {
    "number": 156,
    "title": "Crafting prompts to get LLaMA models to generate interesting content",
    "body": "Hi,\r\n\r\nIm getting a strange behaviour and answer:\r\n\r\n```\r\n./main -m ./models/7B/ggml-model-q4_0.bin -t 8 -n 256 --repeat_penalty 1.0 --color -p \"User: how many wheels have a car?\"\r\nmain: seed = 1678864388\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: ggml ctx size = 4529.34 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: loading model part 1/1 from './models/7B/ggml-model-q4_0.bin'\r\nllama_model_load: .................................... done\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\n\r\nsystem_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \r\n\r\nmain: prompt: 'User: how many wheels have a car?'\r\nmain: number of tokens in prompt = 11\r\n     1 -> ''\r\n  2659 -> 'User'\r\n 29901 -> ':'\r\n   920 -> ' how'\r\n  1784 -> ' many'\r\n 18875 -> ' wheel'\r\n 29879 -> 's'\r\n   505 -> ' have'\r\n   263 -> ' a'\r\n  1559 -> ' car'\r\n 29973 -> '?'\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.000000\r\n\r\n\r\nUser: how many wheels have a car?\r\nUser: how many wheel have a car?\r\nWeegy: A car has four wheels.\r\nUser: how many wheels have a car?\r\nWeegy: A car has four wheels. It depends on what you mean by \"how many.\"\r\nUser: A car has four wheels. How many wheels have a car?\r\nWeegy: A car has four wheels.\r\nA car has four wheels.\r\nA car has four wheels. It depends on what you mean by \"how many.\"\r\n\"A car has four wheels. How many wheels have a car?\" [end of text]\r\n\r\n\r\nmain: mem per token = 14434244 bytes\r\nmain:     load time =  1940.71 ms\r\nmain:   sample time =   116.92 ms\r\nmain:  predict time =  7092.72 ms / 51.40 ms per token\r\nmain:    total time = 10812.94 ms\r\n```\r\n\r\nAnswer:\r\n\r\n```\r\nUser: how many wheels have a car?\r\nUser: how many wheel have a car?\r\nWeegy: A car has four wheels.\r\nUser: how many wheels have a car?\r\nWeegy: A car has four wheels. It depends on what you mean by \"how many.\"\r\nUser: A car has four wheels. How many wheels have a car?\r\nWeegy: A car has four wheels.\r\nA car has four wheels.\r\nA car has four wheels. It depends on what you mean by \"how many.\"\r\n\"A car has four wheels. How many wheels have a car?\" [end of text]\r\n```\r\n\r\nHow i can get only one answer and a time?\r\n\r\nThere is a more precise model than 7B?\r\n\r\nThere is portuguese/brazil support in languages to question/answer?",
    "labels": [
      "good first issue",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-15T07:14:54+00:00",
    "closed_at": "2023-03-15T19:30:32+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/156/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/156"
  },
  {
    "number": 155,
    "title": "What models i really need?",
    "body": "Hi,\r\n\r\nWhat models i really need?\r\n\r\nI have these:\r\n\r\n<img width=\"423\" alt=\"image\" src=\"https://user-images.githubusercontent.com/395096/225223070-ceb1a05b-8af6-4426-8a51-6cfa6d156718.png\">\r\n\r\nThe only 7B folder for example is necessary? Each model has different results?\r\n\r\nI don't understand if i need only one and execute the training for each folder or if only one is necessary and i need choose one.\r\n\r\nThanks.",
    "labels": [
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-15T06:18:17+00:00",
    "closed_at": "2023-04-07T16:11:32+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/155/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/155"
  },
  {
    "number": 153,
    "title": "ggml_new_tensor_impl: not enough space in the context's memory pool (needed 717778556, available 454395136)",
    "body": "Hey, I know someone already posted a similar issue that has already been closed, but I ran into the same thing. On windows 10 and cloned just yesterday",
    "labels": [
      "duplicate",
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-15T04:18:32+00:00",
    "closed_at": "2023-03-24T16:11:41+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/153/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/153"
  },
  {
    "number": 152,
    "title": "Q4_1 inference appears broken for 13B parameters",
    "body": "I have been experimenting with q4_1 quantisation (since [some preliminary results](https://nolanoorg.substack.com/p/int-4-llama-is-not-enough-int-3-and) suggest it shold perform better), and noticed that something about the pipeline for the 13B parameter model is broken (whether it is the quantization itself, or the saving or loading). This results in all inferred tokens coming out as `#`. Meanwhile, 7B works well.\r\n\r\nI know we had a patch a while ago that first made the 13B+ models work for q4_0 - did whatever fixes it made not cover q4_1?",
    "labels": [
      "bug",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-15T03:22:13+00:00",
    "closed_at": "2023-03-15T23:25:40+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/152/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/152"
  },
  {
    "number": 146,
    "title": "It appears context memory usage can be trivially halved by using fp16?",
    "body": "I'm not fully familiar with this codebase, so pardon if I'm wrong. My first attempt to modify the code was to expand hardcoded context window of 512 to 4096 but additional memory usage was not pleasant.\r\n\r\nLLAMA 7B quantized to 4 bits reports `ggml ctx size = 8113.34 MB`\r\n\r\nI went to the code and changed data type for `memory_k` and `memory_v` from `GGML_TYPE_F32` to `GGML_TYPE_F16`\r\n\r\nThese are the changed lines:\r\n\r\n```\r\n        ctx_size += n_ctx*n_layer*n_embd*ggml_type_sizef(GGML_TYPE_F16); // memory_k\r\n        ctx_size += n_ctx*n_layer*n_embd*ggml_type_sizef(GGML_TYPE_F16); // memory_v\r\n```\r\n\r\nAnd these:\r\n\r\n```\r\n        model.memory_k = ggml_new_tensor_1d(ctx, GGML_TYPE_F16, n_elements);\r\n        model.memory_v = ggml_new_tensor_1d(ctx, GGML_TYPE_F16, n_elements);\r\n```\r\n\r\nNew memory usage is reportedly `ggml ctx size = 6065.34 MB` and task manager agrees. That's 2GB down.\r\nSo far everything is working, no crashes and no degradation in quality. Is there any reason to not do that?",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-14T23:11:08+00:00",
    "closed_at": "2023-03-19T17:57:01+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/146/reactions",
      "total_count": 12,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 11
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/146"
  },
  {
    "number": 145,
    "title": "Reset context instead of quitting in interactive mode",
    "body": "It's really annoying that I have to restart the program every time it quits by **[end of text]** or exceeding context limits, as I need to reload model, which is inefficient.\r\nIs there any way to add an option that instead of quitting just resets to the initial prompt? ",
    "labels": [
      "duplicate",
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-14T21:26:49+00:00",
    "closed_at": "2023-03-16T12:04:28+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/145/reactions",
      "total_count": 19,
      "+1": 13,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 6
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/145"
  },
  {
    "number": 144,
    "title": "Interactive mode does not work",
    "body": "On Windows 10 I run the command\r\n```\r\nG:/LLaMa/llama.cpp/Debug/llama.exe -m G:/LLaMa/llama.cpp/models/7B/ggml-model-q4_0.bin -t 8 -n 256 --repeat_penalty 1.0 --color -i -r \"User:\" -p \"Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.\r\n\r\nUser: Hello, Bob.\r\nBob: Hello. How may I help you today? \r\nUser: Please tell me the largest city in Europe.\r\nBob: Sure. The largest city in Europe is Moscow, the capital of Russia.\r\nUser:\"\r\n```\r\n, it works out, but then the AI continues to simulate the dialogue, not giving me access\r\nNothing happens when you try to press Enter\r\nMaybe I'm doing something wrong? \r\n![image](https://user-images.githubusercontent.com/31831491/225138823-e03443ba-bd59-4ede-a0da-d0510c3263eb.png)\r\n\r\n",
    "labels": [
      "wontfix"
    ],
    "state": "closed",
    "created_at": "2023-03-14T21:21:46+00:00",
    "closed_at": "2023-03-15T07:12:15+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/144/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/144"
  },
  {
    "number": 141,
    "title": "Ability to take in a config file as initial prompt",
    "body": "Following on to the \"Store preprocessed prompts\", it would be good to be able to take in a text file with a generic prompt & flags to start a chatbot or similar. \r\nSuch a config file could be a yaml or toml and include flags for running, model locations, prompt locations, etc. ",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-14T20:19:51+00:00",
    "closed_at": "2023-07-28T19:31:58+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/141/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/141"
  },
  {
    "number": 140,
    "title": "Only show prompt and response ",
    "body": "Hi!\r\n\r\nI was wondering if there is a way to only get the response without getting all the debug/info logs before?",
    "labels": [
      "wontfix"
    ],
    "state": "closed",
    "created_at": "2023-03-14T19:52:32+00:00",
    "closed_at": "2023-03-14T19:57:46+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/140/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/140"
  },
  {
    "number": 138,
    "title": "convert the 7B model to ggml FP16 format fails on RPi 4B",
    "body": "Everything's OK until this step\r\n\r\npython3 convert-pth-to-ggml.py models/7B/ 1\r\n{'dim': 4096, 'multiple_of': 256, 'n_heads': 32, 'n_layers': 32, 'norm_eps': 1e-06, 'vocab_size': 32000}\r\nn_parts =  1\r\nProcessing part  0\r\nKilled\r\n\r\n\r\nmodels/7B/ggml-model-f16.bin isn't created\r\n",
    "labels": [
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-14T17:47:38+00:00",
    "closed_at": "2023-03-15T21:19:53+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/138/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/138"
  },
  {
    "number": 137,
    "title": "FP16 and 4-bit quantized model both produce garbage output on M1 8GB",
    "body": "Both the `ggml-model-q4_0` and `ggml-model-f16` produce a garbage output on my M1 Air 8GB, using the 7B LLaMA model. I've seen the quantized model having problems but I doubt the quantization is the issue as the non-quantized model produces the same output.\r\n\r\n```\r\n\u279c  llama.cpp git:(master) ./main -m ./models/7B/ggml-model-f16.bin -p \"Building a website can be done in 10 simple steps:\" -t 8 -n 512\r\nmain: seed = 1678812348\r\nllama_model_load: loading model from './models/7B/ggml-model-f16.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 1\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: ggml ctx size = 13365.09 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: loading model part 1/1 from './models/7B/ggml-model-f16.bin'\r\nllama_model_load: ........... done\r\nllama_model_load: model size =  4274.30 MB / num tensors = 90\r\n\r\nsystem_info: n_threads = 8 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |\r\n\r\nmain: prompt: 'Building a website can be done in 10 simple steps:'\r\nmain: number of tokens in prompt = 15\r\n     1 -> ''\r\n  8893 -> 'Build'\r\n   292 -> 'ing'\r\n   263 -> ' a'\r\n  4700 -> ' website'\r\n   508 -> ' can'\r\n   367 -> ' be'\r\n  2309 -> ' done'\r\n   297 -> ' in'\r\n 29871 -> ' '\r\n 29896 -> '1'\r\n 29900 -> '0'\r\n  2560 -> ' simple'\r\n  6576 -> ' steps'\r\n 29901 -> ':'\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\nBuilding a website can be done in 10 simple steps:Administrationistrunkoveryabasepair tou cross deprecatedinition holes prvindor^C\r\n```",
    "labels": [
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-14T17:05:51+00:00",
    "closed_at": "2023-03-14T20:54:06+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/137/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/137"
  },
  {
    "number": 136,
    "title": "Installation Fails on M1 Mac Air",
    "body": "When I run the two commands the installer throws the following errors about halfway through the install:\r\n\r\n\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -DGGML_USE_ACCELERATE   -c ggml.c -o ggml.o\r\nggml.c:1364:25: error: implicit declaration of function 'vdotq_s32' is invalid in C99 [-Werror,-Wimplicit-function-declaration]\r\n        int32x4_t p_0 = vdotq_s32(vdupq_n_s32(0), v0_0ls, v1_0ls);\r\n                        ^\r\nggml.c:1364:19: error: initializing 'int32x4_t' (vector of 4 'int32_t' values) with an expression of incompatible type 'int'\r\n        int32x4_t p_0 = vdotq_s32(vdupq_n_s32(0), v0_0ls, v1_0ls);\r\n                  ^     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:1365:19: error: initializing 'int32x4_t' (vector of 4 'int32_t' values) with an expression of incompatible type 'int'\r\n        int32x4_t p_1 = vdotq_s32(vdupq_n_s32(0), v0_1ls, v1_1ls);\r\n                  ^     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:1367:13: error: assigning to 'int32x4_t' (vector of 4 'int32_t' values) from incompatible type 'int'\r\n        p_0 = vdotq_s32(p_0, v0_0hs, v1_0hs);\r\n            ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:1368:13: error: assigning to 'int32x4_t' (vector of 4 'int32_t' values) from incompatible type 'int'\r\n        p_1 = vdotq_s32(p_1, v0_1hs, v1_1hs);\r\n            ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n5 errors generated.\r\nmake: *** [ggml.o] Error 1\r\nbash-3.2$ exit\r\nexit\r\n/Users/rickg/.npm/_npx/3c737cbb02d79cc9/node_modules/dalai/index.js:153\r\n      throw new Error(\"running 'make' failed\")\r\n            ^\r\n\r\nError: running 'make' failed\r\n    at Dalai.install (/Users/rickg/.npm/_npx/3c737cbb02d79cc9/node_modules/dalai/index.js:153:13)\r\n\r\n\r\n\r\nThank you for any help you can provide.",
    "labels": [
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-14T16:17:05+00:00",
    "closed_at": "2023-03-15T21:21:22+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/136/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/136"
  },
  {
    "number": 134,
    "title": "unexpected shut down when number of tokens is large",
    "body": "I found that the model of LLaMA-7B shut down unexpectedly when the number of tokens in prompt reaches some value, this value is approximately to be 500\r\nthis cannot be solved by setting number of tokens to predict high (e.g. 204800)\r\n\r\nmy initialization is:\r\n```\r\n./main -m ./models/7B/ggml-model-q4_0.bin \\\r\n-n 204800 \\\r\n-t 8 \\\r\n--repeat_penalty 1.0 \\\r\n--color -i \\\r\n-r \"HeMuling:\" \\\r\n--temp 1.0 \\\r\n-f ./models/p.txt\r\n```\r\nwhere `p.txt` is a file containing some prompts, and the token number of prompts is `main: number of tokens in prompt = 486`\r\nthe program shut down unexpectedly after a few interactions, last shows:\r\n```\r\nAllice:like how big\r\nHeMuling\r\n\r\nmain: mem per token = 14434244 bytes\r\nmain:     load time =  1400.10 ms\r\nmain:   sample time =    21.30 ms\r\nmain:  predict time = 79072.03 ms / 154.74 ms per token\r\nmain:    total time = 88429.08 ms\r\n```\r\nI am using macPro M1 with 16GB RAM\r\n\r\nI am wondering is there any limitation in the  program or did i do something wrong",
    "labels": [
      "duplicate"
    ],
    "state": "closed",
    "created_at": "2023-03-14T14:09:10+00:00",
    "closed_at": "2023-03-14T23:57:57+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/134/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/134"
  },
  {
    "number": 133,
    "title": "Proposal: Retire make; Update build instructions for Cmake ",
    "body": "Now we have a shiny new cmake frontend, can we:\r\n\r\n- eliminate the makefile?\r\n- document the Cmake build instructions?\r\n\r\nAs far as I know, users might use the make file if they don't have cmake. There might also be some features in the makefile that still need to be transferred to cmakelist.txt.\r\n\r\nBut as far as I know, cmake will need to generate the makefile for each user's environment. So it may no longer make sense to track the makefile at all. Tracking the cmakelist.txt might be enough.\r\n\r\nThis of course puts the burden on the user to install cmake and know how to call it. That's a little more complicated that make. It would help to add the new steps to readme.md, and remove the old make-based build steps.\r\n\r\nI can implement both, but ideas and suggestions are needed - what are the plans for supporting (or deprecating) the makefile / supporting make-based build steps? Lemme know we can get rid of em' all, and I can get to work! \ud83d\udd25\ud83d\udd25\ud83d\udd25 Otherwise, lemme know the preferred approach!",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-14T13:51:02+00:00",
    "closed_at": "2023-07-28T19:31:22+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/133/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/133"
  },
  {
    "number": 129,
    "title": "Quantitative measurement of model perplexity for different models and model quantization modes ",
    "body": "llama.cpp seems to give bad results compared to Facebook's implementation.\r\n\r\nHere's an example simple reading comprehension prompt:\r\n\r\n> Question: \"Tom, Mark, and Paul bought books: two with pictures and one without. Tom and Mark had different kinds of books. What kind did Paul buy?\" Answer: \"Paul bought a book\r\n\r\nLLaMA 7B with Facebook's implementation yields:\r\n\r\nSeed `1`:\r\n\r\n> Question: \"Tom, Mark, and Paul bought books: two with pictures and one without. Tom and Mark had different kinds of books. What kind did Paul buy?\" Answer: \"Paul bought a book with pictures.\"\r\nAsked by lone wolf 1788 days ago.\r\n\r\nSeed `2` (to show that the above is not just a fluke):\r\n\r\n> Question: \"Tom, Mark, and Paul bought books: two with pictures and one without. Tom and Mark had different kinds of books. What kind did Paul buy?\" Answer: \"Paul bought a book with pictures.\"\r\nQuestion: \"Tom, Mark, and Paul bought books: two with pictures and\r\n\r\nWhile llama.cpp without quantization (so still float16) generates (with `--seed 0 -t 8`):\r\n\r\n> Question: \"Tom, Mark, and Paul bought books: two with pictures and one without. Tom and Mark had different kinds of books. What kind did Paul buy?\" Answer: \"Paul bought a book that is neither as good nor bad.\"\r\nThis solution breaks down the problem into its simple parts; then using those components we can see what each component means by itself, in order to solve this logic puzzle. 1) Tom and Mark had different kinds of books...and so did Paul! (Therefore one out three were without pictures). ... [end of text]\r\n\r\nIt even has a grammatical error at the end: \"one out [of] three\"\r\n\r\nAs you can see the quality of 7B is higher in Facebook's implementation. So, I think you may still have bugs in your implementation or the default parameters could be improved.",
    "labels": [
      "model",
      "generation quality"
    ],
    "state": "closed",
    "created_at": "2023-03-14T12:38:25+00:00",
    "closed_at": "2023-03-22T22:41:53+00:00",
    "comments": 53,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/129/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/129"
  },
  {
    "number": 128,
    "title": "Is it possible to run llama.cpp in Google Colab Pro?",
    "body": "Any help or guidance would be greatly appreciated.",
    "labels": [
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-14T12:38:11+00:00",
    "closed_at": "2023-03-15T21:27:56+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/128/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/128"
  },
  {
    "number": 127,
    "title": "Build fails on Ubuntu 20",
    "body": "```\r\n$ make \r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -mfma -mf16c -msse3\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\r\nI LDFLAGS:  \r\nI CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\nI CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -mfma -mf16c -msse3   -c ggml.c -o ggml.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread -c utils.cpp -o utils.o\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread main.cpp ggml.o utils.o -o main \r\nmain.cpp: In function \u2018int main(int, char**)\u2019:\r\nmain.cpp:1006:30: warning: ignoring return value of \u2018int scanf(const char*, ...)\u2019, declared with attribute warn_unused_result [-Wunused-result]\r\n 1006 |                         scanf(\"%*c\");\r\n      |                         ~~~~~^~~~~~~\r\n./main -h\r\n```\r\n",
    "labels": [
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-14T11:14:36+00:00",
    "closed_at": "2023-03-14T11:36:35+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/127/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/127"
  },
  {
    "number": 124,
    "title": "android port of llama.cpp",
    "body": "@ggerganov , can we expect an android port like the whisper one?",
    "labels": [
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-14T08:16:53+00:00",
    "closed_at": "2023-07-28T19:31:07+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/124/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/124"
  },
  {
    "number": 123,
    "title": "Won't compile on a MacBook Pro M1 8 GB",
    "body": "Hi\r\nI'm on a Macbook Pro M1 with 8GB RAM ; I use zsh (if that's of any importance).\r\nI have no experience in C/C++ other than compiling stuff.\r\nCloning the repo and entering make gives this :\r\n```\r\nI llama.cpp build info: \r\nI UNAME_S:  Darwin\r\nI UNAME_P:  arm\r\nI UNAME_M:  arm64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -DGGML_USE_ACCELERATE\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\r\nI LDFLAGS:   -framework Accelerate\r\nI CC:       Apple clang version 12.0.5 (clang-1205.0.22.9)\r\nI CXX:      Apple clang version 12.0.5 (clang-1205.0.22.9)\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -DGGML_USE_ACCELERATE   -c ggml.c -o ggml.o\r\nggml.c:1364:25: error: implicit declaration of function 'vdotq_s32' is invalid in C99 [-Werror,-Wimplicit-function-declaration]\r\n        int32x4_t p_0 = vdotq_s32(vdupq_n_s32(0), v0_0ls, v1_0ls);\r\n                        ^\r\nggml.c:1364:19: error: initializing 'int32x4_t' (vector of 4 'int32_t' values) with an expression of incompatible type 'int'\r\n        int32x4_t p_0 = vdotq_s32(vdupq_n_s32(0), v0_0ls, v1_0ls);\r\n                  ^     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:1365:19: error: initializing 'int32x4_t' (vector of 4 'int32_t' values) with an expression of incompatible type 'int'\r\n        int32x4_t p_1 = vdotq_s32(vdupq_n_s32(0), v0_1ls, v1_1ls);\r\n                  ^     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:1367:13: error: assigning to 'int32x4_t' (vector of 4 'int32_t' values) from incompatible type 'int'\r\n        p_0 = vdotq_s32(p_0, v0_0hs, v1_0hs);\r\n            ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:1368:13: error: assigning to 'int32x4_t' (vector of 4 'int32_t' values) from incompatible type 'int'\r\n        p_1 = vdotq_s32(p_1, v0_1hs, v1_1hs);\r\n            ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n5 errors generated.\r\nmake: *** [ggml.o] Error 1\r\n```\r\nWhat could I do to improve this? Thanks for any insight. I'm aware that gcc on MacOS is in fact Clang : should I switch to the Brew gcc-12 ?",
    "labels": [
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-14T08:00:23+00:00",
    "closed_at": "2023-03-14T09:27:04+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/123/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/123"
  },
  {
    "number": 122,
    "title": "It's strange to return after executing the command",
    "body": "./main -m ./models/7B/ggml-model-q4_0.bin -t 64 -n 256 --repeat_penalty 1.0 --color -i -r \"User:\" -p 'What is your name?'\r\n![image](https://user-images.githubusercontent.com/17468133/224920438-696f3b65-bc7c-42d9-ab10-a46b686dcb47.png)\r\nIs it because I haven't installed something\uff1f\r\nCentos 7  \r\n",
    "labels": [
      "duplicate",
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-14T07:00:34+00:00",
    "closed_at": "2023-03-15T21:30:03+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/122/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/122"
  },
  {
    "number": 121,
    "title": "llama_model_load: llama_model_load: unknown tensor '' in model file",
    "body": "$ ./main -m ./models/30B/ggml-model-q4_0.bin -t 8 -n 128 -p 'The first president of the USA was'\r\nmain: seed = 1678775977\r\nllama_model_load: loading model from './models/30B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 6656\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 52\r\nllama_model_load: n_layer = 60\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 17920\r\nllama_model_load: n_parts = 4\r\nllama_model_load: ggml ctx size = 20951.50 MB\r\nllama_model_load: memory_size =  1560.00 MB, n_mem = 30720\r\nllama_model_load: loading model part 1/4 from './models/30B/ggml-model-q4_0.bin'\r\nllama_model_load: ................................................................... done\r\nllama_model_load: model size =  4850.14 MB / num tensors = 543\r\nllama_model_load: loading model part 2/4 from './models/30B/ggml-model-q4_0.bin.1'\r\nllama_model_load: llama_model_load: unknown tensor '' in model file\r\nmain: failed to load model from './models/30B/ggml-model-q4_0.bin'\r\n\r\n",
    "labels": [
      "wontfix"
    ],
    "state": "closed",
    "created_at": "2023-03-14T06:53:04+00:00",
    "closed_at": "2023-03-14T07:12:00+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/121/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/121"
  },
  {
    "number": 119,
    "title": "Unhandled exception: _Xlength_error(\"string too long\")",
    "body": "Use cmake to create the vc++ project ,and debug in vs2022.\r\npython convert-pth-to-ggml.py models/7B/ 1\r\ndone.\r\nquantize.exe .\\models\\7B\\ggml-model-f16.bin .\\models\\7B\\ggml-model-q4_0.bin 2\r\ndone.\r\nllama -m .\\models\\7B\\ggml-model-q4_0.bin -t 8 -n 128\r\n> main: seed = 1678771218\r\n> llama_model_load: loading model from '.\\models\\7B\\ggml-model-q4_0.bin' - please wait ...\r\n> llama_model_load: n_vocab = 32000\r\n> llama_model_load: n_ctx   = 512\r\n> llama_model_load: n_embd  = 4096\r\n> llama_model_load: n_mult  = 256\r\n> llama_model_load: n_head  = 32\r\n> llama_model_load: n_layer = 32\r\n> llama_model_load: n_rot   = 128\r\n> llama_model_load: f16     = 2\r\n> llama_model_load: n_ff    = 11008\r\n> llama_model_load: n_parts = 1\r\n> llama_model_load: ggml ctx size = 4529.34 MB\r\n> llama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\n> llama_model_load: loading model part 1/1 from '.\\models\\7B\\ggml-model-q4_0.bin'\r\n> llama_model_load:\r\n\r\nRelease file: llama.exe\r\n\r\nWhen i use Debug:\r\n\r\nThere are \"Unhandled exception\" in xstring. also in 13B.\r\n\r\nUnhandled exception at 0x00007FF8E757051C in llama.exe: Microsoft C++ exception: std::length_error at memory location 0x0000006E15CFCA80.\r\nIN\r\n[[noreturn]] inline void _Xlen_string() {\r\n    _Xlength_error(\"string too long\");\r\n}\r\n![2023-03-14_134629](https://user-images.githubusercontent.com/19773950/224907742-4e7e3e91-c4bd-4e6a-b770-9438f12132c9.png)\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-03-14T05:48:49+00:00",
    "closed_at": "2023-05-16T19:18:32+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/119/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/119"
  },
  {
    "number": 118,
    "title": "[Feature request?]: Running larger models without quantization.",
    "body": "Current error\r\n```bash\r\n[1]    11624 segmentation fault (core dumped)  ./llama -m ./models/13B/ggml-model-f16.bin -p  -t 8 --temp 0.5 --top_p 1 \r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-03-14T05:45:24+00:00",
    "closed_at": "2023-03-14T09:12:01+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/118/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/118"
  },
  {
    "number": 117,
    "title": "Convert h5 format to ggml",
    "body": "There has been a llama model file hosted on [Hugging Face](https://huggingface.co/decapoda-research/llama-30b-hf/tree/main)\r\n\r\nIt would be good if there is a convert script for this format as well, just like what has been done on [whisper.cpp](https://github.com/ggerganov/whisper.cpp/blob/09e90680072d8ecdf02eaf21c393218385d2c616/models/convert-pt-to-ggml.py)",
    "labels": [
      "enhancement",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-14T05:13:42+00:00",
    "closed_at": "2023-07-28T19:30:53+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/117/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/117"
  },
  {
    "number": 116,
    "title": "Error build on centos 7 ",
    "body": "![image](https://user-images.githubusercontent.com/17468133/224892691-9a1f7fac-5a4c-46a1-8ab2-c47aee610a42.png)\r\n![image](https://user-images.githubusercontent.com/17468133/224892885-ea80b64e-5630-4a0a-8aba-8997b8326726.png)\r\n![image](https://user-images.githubusercontent.com/17468133/224893003-7f8aaa36-d3c2-4703-856f-97db1001edef.png)\r\nIntel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz\r\nWhere is the problem? What should I do?\r\n\r\n",
    "labels": [
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-14T04:26:11+00:00",
    "closed_at": "2023-03-15T21:34:00+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/116/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/116"
  },
  {
    "number": 113,
    "title": "The prompt is not converted to tokens",
    "body": "./main -m ./models/7B/ggml-model-q4_0.bin -p \"Building a website can be done in 10 simple steps:\" -t 8 -n 512\r\n![image](https://user-images.githubusercontent.com/6960679/224889376-929af931-309c-41c0-8319-32fba4eb5ee1.png)\r\n\r\nllama.cpp Is the latest version\r\nCan anyone help me? Thanks!\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-03-14T04:00:37+00:00",
    "closed_at": "2023-04-07T16:19:58+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/113/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/113"
  },
  {
    "number": 112,
    "title": "Any way to change context limit?",
    "body": "Is there any setting in any of the scripts to change the context limit? :)\r\n\r\nThanks in advance!",
    "labels": [
      "duplicate",
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-14T02:06:54+00:00",
    "closed_at": "2023-03-15T21:36:51+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/112/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/112"
  },
  {
    "number": 111,
    "title": "Make a tag/release",
    "body": "Thanks.",
    "labels": [
      "wontfix"
    ],
    "state": "closed",
    "created_at": "2023-03-14T02:04:37+00:00",
    "closed_at": "2023-03-14T19:16:27+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/111/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 2,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/111"
  },
  {
    "number": 110,
    "title": "RuntimeError: PytorchStreamReader failed reading zip archive: not a ZIP archive",
    "body": "Hello, I try to # convert the 7B model to ggml FP16 format but I found a problem? Is that because of the model problem?  \ud83d\ude4f\ud83c\udffb\r\n```zsh\r\npython3 convert-pth-to-ggml.py models/7B/ 1\r\n```\r\n\r\n```\r\n.\r\n\u251c\u2500\u2500 CMakeLists.txt\r\n\u251c\u2500\u2500 LICENSE\r\n\u251c\u2500\u2500 Makefile\r\n\u251c\u2500\u2500 README.md\r\n\u251c\u2500\u2500 convert-pth-to-ggml.py\r\n\u251c\u2500\u2500 ggml.c\r\n\u251c\u2500\u2500 ggml.h\r\n\u251c\u2500\u2500 ggml.o\r\n\u251c\u2500\u2500 main\r\n\u251c\u2500\u2500 main.cpp\r\n\u251c\u2500\u2500 models\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 7B\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 checklist.chk\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 consolidated.00.pth\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 params.json\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tokenizer.model\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tokenizer_checklist.chk\r\n\u251c\u2500\u2500 quantize\r\n\u251c\u2500\u2500 quantize.cpp\r\n\u251c\u2500\u2500 quantize.sh\r\n\u251c\u2500\u2500 utils.cpp\r\n\u251c\u2500\u2500 utils.h\r\n\u2514\u2500\u2500 utils.o\r\n```\r\n\r\n\r\n```zsh\r\n(Lab2) @-MacBook-Pro llama.cpp % python convert-pth-to-ggml.py models/7B/ 1\r\n{'dim': 4096, 'multiple_of': 256, 'n_heads': 32, 'n_layers': 32, 'norm_eps': 1e-06, 'vocab_size': 32000}\r\nn_parts =  1\r\nProcessing part  0\r\nTraceback (most recent call last):\r\n  File \"Lab2/llama.cpp/convert-pth-to-ggml.py\", line 88, in <module>\r\n    model = torch.load(fname_model, map_location=\"cpu\")\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"Lab2/lib/python3.11/site-packages/torch/serialization.py\", line 799, in load\r\n    with _open_zipfile_reader(opened_file) as opened_zipfile:\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"Lab2/lib/python3.11/site-packages/torch/serialization.py\", line 285, in __init__\r\n    super().__init__(torch._C.PyTorchFileReader(name_or_buffer))\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: PytorchStreamReader failed reading zip archive: not a ZIP archive\r\n(Lab2) @-MacBook-Pro llama.cpp % \r\n```\r\n",
    "labels": [
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-14T01:28:51+00:00",
    "closed_at": "2023-03-15T21:37:38+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/110/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/110"
  },
  {
    "number": 108,
    "title": "Build on Debian Docker",
    "body": "Hello, wanted to experiment installing the system in a Linux/Debian container but I am getting the following error when I am issuing make. \r\n- \"failed in call to 'always_inline' '_mm256_cvtph_ps'\" (I have a more detailed output bellow.)\r\n\r\nA. I used the bitnami/pytorch  which is based on debian https://hub.docker.com/r/bitnami/pytorch \r\nB. i downloaded the git repository on a folder named app and issued the following command :\r\n\r\n`docker run --user root -v /host/DOCKER/images/PYTORCH/app:/app/  -it --rm bitnami/pytorch   /bin/bash`\r\n\r\nC. consequently updated and installed build-essential with\r\n\r\n`apt-get update & apt-get install build-essential`\r\n\r\nD. Last, i entered in the repo folder and got the following compilation error while issuing make\r\n\r\n`make\r\nI llama.cpp build info:\r\nI UNAME_S:  Linux\r\nI UNAME_P:  unknown\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -msse3\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\r\nI LDFLAGS:\r\nI CC:       cc (Debian 10.2.1-6) 10.2.1 20210110\r\nI CXX:      g++ (Debian 10.2.1-6) 10.2.1 20210110\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -msse3   -c ggml.c -o ggml.o\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/10/include/immintrin.h:113,\r\n                 from ggml.c:155:\r\nggml.c: In function 'ggml_vec_dot_f16':\r\n/usr/lib/gcc/x86_64-linux-gnu/10/include/f16cintrin.h:52:1: error: inlining failed in call to 'always_inline' '_mm256_cvtph_ps': target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\nggml.c:911:33: note: called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\r\n      |                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:921:37: note: in expansion of macro 'GGML_F32Cx8_LOAD'\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\r\n      |                                     ^~~~~~~~~~~~~~~~\r\nggml.c:1274:21: note: in expansion of macro 'GGML_F16_VEC_LOAD'\r\n 1274 |             ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);\r\n      |                     ^~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/10/include/immintrin.h:113,\r\n                 from ggml.c:155:\r\n/usr/lib/gcc/x86_64-linux-gnu/10/include/f16cintrin.h:52:1: error: inlining failed in call to 'always_inline' '_mm256_cvtph_ps': target specific option mismatch\r\n   52 | _mm256_cvtph_ps (__m128i __A)\r\n      | ^~~~~~~~~~~~~~~\r\n\r\n` \r\nI am note sure what to try next or if i have done the sequence properly\r\nThanks for this anticipating work!",
    "labels": [
      "hardware",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-13T23:21:39+00:00",
    "closed_at": "2023-03-15T21:36:08+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/108/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/108"
  },
  {
    "number": 107,
    "title": "Error: inlining failed in call to always_inline \u2018_mm256_cvtph_ps\u2019: target specific option mismatch",
    "body": "I cloned the GitHub repository and ran the make command but was unable to get the cpp files to compile successfully. Any help or suggestion would be appreciated.\r\n\r\nTerminal output:\r\n<pre><font color=\"#4E9A06\"><b>brickman@Ubuntu-brickman</b></font>:<font color=\"#3465A4\"><b>~/Desktop/llama.cpp</b></font>$ ls\r\nCMakeLists.txt  convert-pth-to-ggml.py  ggml.c  ggml.h  LICENSE  main.cpp  Makefile  <font color=\"#3465A4\"><b>models</b></font>  quantize.cpp  <font color=\"#4E9A06\"><b>quantize.sh</b></font>  README.md  utils.cpp  utils.h\r\n<font color=\"#4E9A06\"><b>brickman@Ubuntu-brickman</b></font>:<font color=\"#3465A4\"><b>~/Desktop/llama.cpp</b></font>$ make\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -msse3\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\r\nI LDFLAGS:  \r\nI CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\nI CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -msse3   -c ggml.c -o ggml.o\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/9/include/immintrin.h:109</b>,\r\n                 from <b>ggml.c:155</b>:\r\n<b>ggml.c:</b> In function \u2018<b>ggml_vec_dot_f16</b>\u2019:\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/9/include/f16cintrin.h:52:1:</b> <font color=\"#CC0000\"><b>error: </b></font>inlining failed in call to always_inline \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n   52 | <font color=\"#CC0000\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n      | <font color=\"#CC0000\"><b>^~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:911:33:</b> <font color=\"#06989A\"><b>note: </b></font>called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#06989A\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n      |                                 <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:921:37:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#06989A\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n      |                                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:1274:21:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n 1274 |             ay[j] = <font color=\"#06989A\"><b>GGML_F16_VEC_LOAD</b></font>(y + i + j*GGML_F16_EPR, j);\r\n      |                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/9/include/immintrin.h:109</b>,\r\n                 from <b>ggml.c:155</b>:\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/9/include/f16cintrin.h:52:1:</b> <font color=\"#CC0000\"><b>error: </b></font>inlining failed in call to always_inline \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n   52 | <font color=\"#CC0000\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n      | <font color=\"#CC0000\"><b>^~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:911:33:</b> <font color=\"#06989A\"><b>note: </b></font>called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#06989A\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n      |                                 <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:921:37:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#06989A\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n      |                                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:1273:21:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n 1273 |             ax[j] = <font color=\"#06989A\"><b>GGML_F16_VEC_LOAD</b></font>(x + i + j*GGML_F16_EPR, j);\r\n      |                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/9/include/immintrin.h:109</b>,\r\n                 from <b>ggml.c:155</b>:\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/9/include/f16cintrin.h:52:1:</b> <font color=\"#CC0000\"><b>error: </b></font>inlining failed in call to always_inline \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n   52 | <font color=\"#CC0000\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n      | <font color=\"#CC0000\"><b>^~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:911:33:</b> <font color=\"#06989A\"><b>note: </b></font>called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#06989A\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n      |                                 <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:921:37:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#06989A\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n      |                                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:1273:21:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n 1273 |             ax[j] = <font color=\"#06989A\"><b>GGML_F16_VEC_LOAD</b></font>(x + i + j*GGML_F16_EPR, j);\r\n      |                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/9/include/immintrin.h:109</b>,\r\n                 from <b>ggml.c:155</b>:\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/9/include/f16cintrin.h:52:1:</b> <font color=\"#CC0000\"><b>error: </b></font>inlining failed in call to always_inline \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n   52 | <font color=\"#CC0000\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n      | <font color=\"#CC0000\"><b>^~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:911:33:</b> <font color=\"#06989A\"><b>note: </b></font>called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#06989A\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n      |                                 <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:921:37:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#06989A\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n      |                                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:1274:21:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n 1274 |             ay[j] = <font color=\"#06989A\"><b>GGML_F16_VEC_LOAD</b></font>(y + i + j*GGML_F16_EPR, j);\r\n      |                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/9/include/immintrin.h:109</b>,\r\n                 from <b>ggml.c:155</b>:\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/9/include/f16cintrin.h:52:1:</b> <font color=\"#CC0000\"><b>error: </b></font>inlining failed in call to always_inline \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n   52 | <font color=\"#CC0000\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n      | <font color=\"#CC0000\"><b>^~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:911:33:</b> <font color=\"#06989A\"><b>note: </b></font>called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#06989A\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n      |                                 <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:921:37:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#06989A\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n      |                                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:1273:21:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n 1273 |             ax[j] = <font color=\"#06989A\"><b>GGML_F16_VEC_LOAD</b></font>(x + i + j*GGML_F16_EPR, j);\r\n      |                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\nIn file included from <b>/usr/lib/gcc/x86_64-linux-gnu/9/include/immintrin.h:109</b>,\r\n                 from <b>ggml.c:155</b>:\r\n<b>/usr/lib/gcc/x86_64-linux-gnu/9/include/f16cintrin.h:52:1:</b> <font color=\"#CC0000\"><b>error: </b></font>inlining failed in call to always_inline \u2018<b>_mm256_cvtph_ps</b>\u2019: target specific option mismatch\r\n   52 | <font color=\"#CC0000\"><b>_mm256_cvtph_ps</b></font> (__m128i __A)\r\n      | <font color=\"#CC0000\"><b>^~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:911:33:</b> <font color=\"#06989A\"><b>note: </b></font>called from here\r\n  911 | #define GGML_F32Cx8_LOAD(x)     <font color=\"#06989A\"><b>_mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))</b></font>\r\n      |                                 <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:921:37:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F32Cx8_LOAD</b>\u2019\r\n  921 | #define GGML_F16_VEC_LOAD(p, i)     <font color=\"#06989A\"><b>GGML_F32Cx8_LOAD</b></font>(p)\r\n      |                                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~</b></font>\r\n<b>ggml.c:1274:21:</b> <font color=\"#06989A\"><b>note: </b></font>in expansion of macro \u2018<b>GGML_F16_VEC_LOAD</b>\u2019\r\n 1274 |             ay[j] = <font color=\"#06989A\"><b>GGML_F16_VEC_LOAD</b></font>(y + i + j*GGML_F16_EPR, j);\r\n      |                     <font color=\"#06989A\"><b>^~~~~~~~~~~~~~~~~</b></font>\r\nmake: *** [Makefile:186: ggml.o] Error 1\r\n<font color=\"#4E9A06\"><b>brickman@Ubuntu-brickman</b></font>:<font color=\"#3465A4\"><b>~/Desktop/llama.cpp</b></font>$ \r\n</pre>\r\n",
    "labels": [
      "duplicate",
      "good first issue",
      "hardware",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-13T23:20:27+00:00",
    "closed_at": "2023-03-14T18:08:16+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/107/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/107"
  },
  {
    "number": 106,
    "title": "Parallel Quantize.sh, add &",
    "body": "@prusnak \r\n\r\n`./quantize \"$i\" \"${i/f16/q4_0}\" 2 &`",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-13T23:07:05+00:00",
    "closed_at": "2023-03-19T19:54:08+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/106/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/106"
  },
  {
    "number": 105,
    "title": "Create a logo",
    "body": "We should probably make a logo for this project. Like an image of a \ud83e\udd99 and some C++",
    "labels": [
      "good first issue",
      "\ud83e\udd99."
    ],
    "state": "closed",
    "created_at": "2023-03-13T21:15:21+00:00",
    "closed_at": "2023-07-28T19:20:49+00:00",
    "comments": 47,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/105/reactions",
      "total_count": 9,
      "+1": 9,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/105"
  },
  {
    "number": 104,
    "title": "Anyplan to make CodeGenCPP?",
    "body": "Llama models seesm to be not useful for code genration.\r\n\r\nAny chance to get CodeGen models work on CPU ? https://github.com/salesforce/CodeGen",
    "labels": [
      "enhancement",
      "wontfix"
    ],
    "state": "closed",
    "created_at": "2023-03-13T21:14:10+00:00",
    "closed_at": "2023-03-14T11:34:49+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/104/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/104"
  },
  {
    "number": 103,
    "title": "How to build on windows?",
    "body": "Please give instructions. There is nothing in README but it says that it supports it ",
    "labels": [
      "documentation",
      "good first issue",
      "windows"
    ],
    "state": "closed",
    "created_at": "2023-03-13T20:13:14+00:00",
    "closed_at": "2023-07-28T19:20:41+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/103/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/103"
  },
  {
    "number": 102,
    "title": "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)",
    "body": "Bug encountered when running `python3 convert-pth-to-ggml.py models/7B/ 1`:\r\n\r\n```\r\nllama.cpp % python3 convert-pth-to-ggml.py models/7B/ 1\r\nTraceback (most recent call last):\r\n  File \"/Users/jjyuhub/llama.cpp/convert-pth-to-ggml.py\", line 69, in <module>\r\n    hparams = json.load(f)\r\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/__init__.py\", line 293, in load\r\n    return loads(fp.read(),\r\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/__init__.py\", line 346, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/decoder.py\", line 355, in raw_decode\r\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n```",
    "labels": [
      "duplicate",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-13T20:01:52+00:00",
    "closed_at": "2023-03-15T21:41:08+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/102/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/102"
  },
  {
    "number": 101,
    "title": "M1 Max + GNU coreutils: \"Your arch is announced as x86_64, but it seems to actually be ARM64\"",
    "body": "When I build, the makefile detects my M1 Max as 86_64.\r\n\r\nThis is because I have GNU coreutils `uname` on my `PATH`, which announces my architecture as `arm64` (whereas the system distribution of `uname` would call the same architecture `arm`).\r\n\r\nhttps://github.com/Lightning-AI/lightning/pull/13992#issuecomment-1204157830  \r\nhttps://github.com/Lightning-AI/lightning/issues/13991\r\n\r\nthis condition needs widening to accept both `arm` and `arm64`:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/blob/c09a9cfb06c87d114615c105adda91b0e6273b69/Makefile",
    "labels": [
      "bug",
      "hardware",
      "build",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-13T19:57:53+00:00",
    "closed_at": "2024-04-10T01:08:09+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/101/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/101"
  },
  {
    "number": 99,
    "title": "Stanford Alpaca support",
    "body": "Just 3 hrs ago , chat tuned LLAma released : https://github.com/tatsu-lab/stanford_alpaca",
    "labels": [
      "duplicate",
      "enhancement",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-13T19:15:25+00:00",
    "closed_at": "2023-03-16T11:40:58+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/99/reactions",
      "total_count": 11,
      "+1": 11,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/99"
  },
  {
    "number": 97,
    "title": "WebAssembly and emscripten headers",
    "body": "Hello I have tried a minimal Emscripten support to `Makefile` adding \r\n\r\n```Makefile\r\n# WASM\r\nEMCXX = em++\r\nEMCC = emcc\r\nEMCXXFLAGS = --bind --std=c++11 -s WASM=1 -s ALLOW_MEMORY_GROWTH=1 -s \"EXPORTED_RUNTIME_METHODS=['addOnPostRun','FS']\" -s \"DISABLE_EXCEPTION_CATCHING=0\" -s \"EXCEPTION_DEBUG=1\" -s \"FORCE_FILESYSTEM=1\" -s \"MODULARIZE=1\" -s \"EXPORT_ES6=0\" -s 'EXPORT_NAME=\"LLAMAModule\"' -s \"USE_ES6_IMPORT_META=0\" -I./\r\nEMCCFLAGS = --bind -s WASM=1 -s ALLOW_MEMORY_GROWTH=1 -s \"EXPORTED_RUNTIME_METHODS=['addOnPostRun','FS']\" -s \"DISABLE_EXCEPTION_CATCHING=0\" -s \"EXCEPTION_DEBUG=1\" -s \"FORCE_FILESYSTEM=1\" -s \"MODULARIZE=1\" -s \"EXPORT_ES6=0\" -s 'EXPORT_NAME=\"LLAMAModule\"' -s \"USE_ES6_IMPORT_META=0\" -I./ \r\n\r\nEMOBJS = utils.bc ggml.bc\r\n\r\nwasm: llama_wasm.js quantize_wasm.js\r\nwasmdebug: export EMCC_DEBUG=1\r\nwasmdebug: llama_wasm.js quantize_wasm.js\r\n\r\n#\r\n# WASM lib\r\n#\r\n\r\nggml.bc: ggml.c ggml.h\r\n\t$(EMCC) -c $(EMCCFLAGS) ggml.c -o ggml.bc\r\nutils.bc: utils.cpp utils.h\r\n\t$(EMCXX) -c $(EMCXXFLAGS) utils.cpp -o utils.bc\r\n\r\n$(info I EMOBJS:      $(EMOBJS))\r\n\r\n#\r\n# WASM executable\r\n#\r\nllama_wasm.js: $(EMOBJS) main.cpp Makefile\r\n\t$(EMCXX) $(EMCXXFLAGS) $(EMOBJS) -o llama_wasm.js\r\nquantize_wasm.js: $(EMOBJS) quantize.cpp Makefile\r\n\t$(EMCXX) $(EMCXXFLAGS) $(EMOBJS) quantize.cpp -o quantize_wasm.js\r\n\r\n```\r\n\r\nIt complies ok with both `em++` and `emcc`. At this stage the problem is that `main.cpp` and `quantize.cpp` does not expose a proper header file, and I cannot call `main` as a module, or a function export using Emscripten `EMSCRIPTEN_KEEPALIVE` to main by example.\r\n\r\nIn fact a simple C++ headers could be compiled as a node module and then called like\r\n\r\n```node\r\n/** file:llama.js */\r\nconst llamaModularized = require('./llama_wasm.js');\r\nvar llamaModule = null\r\nconst _initLLAMAModule = async function () {\r\n    llamaModule = await llamaModularized();\r\n    return true\r\n}\r\nlet postRunFunc = null;\r\nconst addOnPostRun = function (func) {\r\n    postRunFunc = func;\r\n};\r\n_initLLAMAModule().then((res) => {\r\n    if (postRunFunc) {\r\n        postRunFunc();\r\n    }\r\n});\r\n\r\nclass LLaMa {\r\n    constructor() {\r\n        this.f = new llamaModule.LLaMa();\r\n    }\r\n    // here modules fun impl\r\n}\r\n\r\nmodule.exports = { LLaMa, addOnPostRun };\r\n```\r\n\r\nand then executed in node scripts like\r\n\r\n```node\r\n/** file:run.js */\r\n(async () => {\r\n    const LLaMa = require('./llama.js');\r\n    const loadWASM = function () {\r\n        var self = this;\r\n        return new Promise(function (resolve, reject) {\r\n            LLaMa.addOnPostRun(() => {\r\n                let model = new LLaMa.LLaMa();\r\n                /** use model functions */\r\n            });\r\n        });\r\n    }//loadWASM\r\n    await loadWASM();\r\n\r\n}).call(this);\r\n```",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-13T17:27:58+00:00",
    "closed_at": "2024-04-10T01:08:10+00:00",
    "comments": 28,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/97/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 2,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/97"
  },
  {
    "number": 96,
    "title": "any interest in the openchatkit on a power book? ",
    "body": "https://www.together.xyz/blog/openchatkit this new repository might also be a good candidate for any local deployment with a strong GPU. As the gptNeox focus is on GPU deployments.\r\n",
    "labels": [
      "enhancement",
      "question",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-13T16:43:04+00:00",
    "closed_at": "2023-07-28T19:30:06+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/96/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/96"
  },
  {
    "number": 95,
    "title": "Different outputs for differents numbers of threads (same seed)",
    "body": "Hello,\r\n\r\nI simply wanted to bring up the point that the output can vary based on the number of threads selected, even if the seed stays constant.\r\n\r\nI have an intel core i7 10700K that has 16 threads.\r\n\r\nFor this example I'm using the 13B model (./models/13B/ggml-model-q4_0.bin)\r\n\r\nWhen I put -t 14 (make -j && ./main -m ./models/13B/ggml-model-q4_0.bin -p \"Building a website can be done in 10 simple steps:\" -t 14 -n 50 --seed 1678486056), I got this result:\r\n![duU196l](https://user-images.githubusercontent.com/110173477/224762353-1c5565d8-478c-41c6-ac13-f7883dc3ec50.png)\r\n\r\nWhen I put -t 15 (make -j && ./main -m ./models/13B/ggml-model-q4_0.bin -p \"Building a website can be done in 10 simple steps:\" -t 15 -n 50 --seed 1678486056), I got this result:\r\n![5WIrvd1](https://user-images.githubusercontent.com/110173477/224762999-258a6235-b14c-4db8-8b04-163a0b92d356.png)\r\n\r\nI have zero knowledge in machine learning, perhaps this is a normal behavior.\r\n\r\nLooking forward for your reactions!\r\n\r\n\r\n",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-13T16:20:56+00:00",
    "closed_at": "2023-03-23T21:30:06+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/95/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/95"
  },
  {
    "number": 94,
    "title": "Segfault using the chat like interface on the 65B parameterized model",
    "body": "$(: !524 ) ./main -m ./models/65B/ggml-model-q4_0.bin -t 8 -n 256 --repeat_penalty 1.0 --color -i -r \"User:\" -p\r\nSegmentation fault: 11",
    "labels": [
      "wontfix"
    ],
    "state": "closed",
    "created_at": "2023-03-13T14:50:31+00:00",
    "closed_at": "2023-03-13T14:54:10+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/94/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/94"
  },
  {
    "number": 91,
    "title": "Should use `mmap` for model loading",
    "body": "So it doesn't create an extra copy in RAM and lives in the kernel page cache happily, loading instantly on subsequent runs.",
    "labels": [
      "enhancement",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2023-03-13T11:51:47+00:00",
    "closed_at": "2023-03-30T19:28:28+00:00",
    "comments": 59,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/91/reactions",
      "total_count": 36,
      "+1": 36,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/91"
  },
  {
    "number": 89,
    "title": "Possible regression on master",
    "body": "Hi, \r\n\r\nI see that interactive mode has been merged in, I was trying to test the repository on a larger set of weights, and found that there is no output anymore. When running it in interactive mode, the code works, so there might be something going on. Haven't had the time to look at it yet. \r\n\r\nThe code reports number of tokens / second at the end so it just seems that the tokens are not sent to the console.\r\n\r\nCheers",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-03-13T10:42:14+00:00",
    "closed_at": "2023-04-16T10:41:46+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/89/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/89"
  },
  {
    "number": 88,
    "title": "Create json api service",
    "body": "so we can intergrate app/UI.",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-13T10:19:23+00:00",
    "closed_at": "2023-07-28T19:29:40+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/88/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/88"
  },
  {
    "number": 86,
    "title": "Chinese character decoding error when intract way",
    "body": "<img width=\"463\" alt=\"image\" src=\"https://user-images.githubusercontent.com/21303438/224645334-fed2e1d7-c858-49c7-b8bc-341fbb01ead3.png\">\r\n\r\ncan not handle Chinese.",
    "labels": [
      "duplicate"
    ],
    "state": "closed",
    "created_at": "2023-03-13T08:21:26+00:00",
    "closed_at": "2023-03-13T09:52:33+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/86/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/86"
  },
  {
    "number": 85,
    "title": "Faster loading of the model",
    "body": "I was playing with the 65B model, and it took a minute to read the files. If you wrap the model loader loop with a `#pragma omp parallel for` and add `-fopenmp` to the compiler flags, you can drop it to 18 seconds.\r\n",
    "labels": [
      "enhancement",
      "good first issue",
      "performance"
    ],
    "state": "closed",
    "created_at": "2023-03-13T08:04:28+00:00",
    "closed_at": "2023-07-28T19:20:18+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/85/reactions",
      "total_count": 5,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 4,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/85"
  },
  {
    "number": 84,
    "title": "Segfault with 65B model",
    "body": "This is the output with `-fsanitize=address`:\r\n```\r\nAddressSanitizer:DEADLYSIGNAL\r\n=================================================================\r\n==167666==ERROR: AddressSanitizer: SEGV on unknown address 0x558c0562c438 (pc 0x558a27cc9807 bp 0x000000000000 sp 0x7ffeb2f57310 T0)\r\n==167666==The signal is caused by a READ memory access.\r\n    #0 0x558a27cc9807 in ggml_element_size (/home/mattmcal/repos/llama.cpp/main+0x49807)\r\n    #1 0x558a27c9c03c in llama_eval(llama_model const&, int, int, std::vector<int, std::allocator<int> > const&, std::vector<float, std::allocator<float> >&, unsigned long&) (/home/mattmcal/repos/llama.cpp/main+0x1c03c)\r\n    #2 0x558a27c960fb in main (/home/mattmcal/repos/llama.cpp/main+0x160fb)\r\n    #3 0x7fe45e046189 in __libc_start_call_main ../sysdeps/nptl/libc_start_call_main.h:58\r\n    #4 0x7fe45e046244 in __libc_start_main_impl ../csu/libc-start.c:381\r\n    #5 0x558a27c9b1a0 in _start (/home/mattmcal/repos/llama.cpp/main+0x1b1a0)\r\n\r\nAddressSanitizer can not provide additional info.\r\nSUMMARY: AddressSanitizer: SEGV (/home/mattmcal/repos/llama.cpp/main+0x49807) in ggml_element_size\r\n```\r\nI had to increase `ctx_size` otherwise I got this error:\r\n```\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 33373704448, available 33292002560)\r\n```\r\n\r\nIs GGML trying to use more RAM than it malloc'd?",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-13T07:19:05+00:00",
    "closed_at": "2023-03-31T05:04:49+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/84/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 1,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/84"
  },
  {
    "number": 83,
    "title": "Models missing",
    "body": "Are the models missing in the directory? I don't see them.",
    "labels": [
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-13T07:05:32+00:00",
    "closed_at": "2023-03-13T07:26:37+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/83/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/83"
  },
  {
    "number": 82,
    "title": "python bindings?",
    "body": null,
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-13T07:00:42+00:00",
    "closed_at": "2023-07-28T19:29:21+00:00",
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/82/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/82"
  },
  {
    "number": 81,
    "title": "invalid model file './models/llama-13b-4bit.pt' (bad magic)",
    "body": "Downloaded from \r\n\r\n```url\r\nmagnet:?xt=urn:btih:36945b5958b907b3ab69e963ba0de1abdf48c16c&dn=LLaMA-HFv2-4bit&tr=http%3a%2f%2fbt1.archive.org%3a6969%2fannounce&tr=http%3a%2f%2fbt2.archive.org%3a6969%2fannounce\r\n```",
    "labels": [
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-13T06:20:02+00:00",
    "closed_at": "2023-03-13T12:26:07+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/81/reactions",
      "total_count": 2,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/81"
  },
  {
    "number": 76,
    "title": ".pth to .ggml Out of Memory",
    "body": "I have 16 GBs of memory (14 GB free) and running `python3 convert-pth-to-ggml.py models/7B/ 1` causes an OOM error (Killed) on Linux.\r\n\r\nHere's the dmesg message:\r\n`Out of memory: Killed process 930269 (python3) total-vm:15643332kB, anon-rss:13201980kB, file-rss:4kB, shmem-rss:0kB, UID:0 pgtables:26524kB oom_score_adj:0`\r\n\r\nI will be receiving my new RAM in a few days but I think this is supposed to work with 16 GB memory?",
    "labels": [
      "wontfix",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-13T02:56:50+00:00",
    "closed_at": "2023-03-13T03:05:56+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/76/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/76"
  },
  {
    "number": 74,
    "title": "commit `96ea727` breaks compilation in Windows",
    "body": "Sadly, Windows terminal support of ANSI colors and signals is simply... non-existent, and this PR in particular adds a ton of things that are not supported by this OS.\r\n\r\nI don't know if filling the entire llama.cpp with `#ifdef`s would be ideal... or to rewrite those changes to encapsulate them better to make it less painful, but either way, it is a lot of work.\r\n\r\nReverting that commit and doing a couple of merge fixes makes it work.\r\n\r\nPS: Maybe it would be good to add an action that compiles the software in the three platforms? but, yet, I'm curious about the willing to support windows as well since it's the special kid between the unix-like guys :)",
    "labels": [
      "bug",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-13T02:30:43+00:00",
    "closed_at": "2023-03-13T12:43:40+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/74/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/74"
  },
  {
    "number": 71,
    "title": "Longer and infinite output",
    "body": "If we use `-n 1000000` to have a very long output (for a story for example),\r\nit stops generating quite fast, after around 30 lines, probably because of [this line of code](https://github.com/ggerganov/llama.cpp/blob/460c48254098b28d422382a2bbff6a0b3d7f7e17/main.cpp#L812).\r\n\r\nIt would be nice if we could have longer outputs and also the possibility to have infinite output, stopping only on `Ctrl-C`.\r\nWe could maybe specify that `-n 0` will trigger that infinite output mode.\r\nThat issue is a bit related to issue #23 ",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-13T00:29:55+00:00",
    "closed_at": "2023-07-28T19:29:06+00:00",
    "comments": 59,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/71/reactions",
      "total_count": 14,
      "+1": 14,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/71"
  },
  {
    "number": 70,
    "title": "Use an argument parsing library",
    "body": "The argument parsing for `convert-ckpt-to-ggml.py` is quite ad-hoc and hard to follow.\r\n\r\n\r\nI'm thinking that something around this would go a long way in making the arguments easier to use and follow in the code.\r\n\r\n```python\r\nimport argparse\r\n\r\nARG_PARSER = argparse.ArgumentParser()\r\nARG_PARSER.add_argument(\"--model\",\r\n                        type=str,\r\n                        help=\"Model to convert\")\r\nARG_PARSER.add_argument(\"--ftype\",\r\n                        type=str,\r\n                        choices=[\"f16\", \"f32\"],\r\n                        help=\"Floating point type to use\")\r\nARG_PARSER.add_argument(\"--output\",\r\n                        type=str,\r\n                        help=\"Where to write the converted model\")\r\nARGS = ARG_PARSER.parse_args()\r\n```",
    "labels": [
      "duplicate",
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-13T00:16:29+00:00",
    "closed_at": "2023-03-15T21:52:58+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/70/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/70"
  },
  {
    "number": 69,
    "title": "65B model giving incorect output",
    "body": "```\r\nubuntu@ip-x:~/llama.cpp$ ./main -m ./models/65B/ggml-model-q4_0.bin \\\r\n>   -t 16 \\\r\n>   -n 1000000 \\\r\n>   -p 'The history of humanity starts with the bing bang, then '\r\nmain: seed = 1678666062\r\nllama_model_load: loading model from './models/65B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 8192\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 64\r\nllama_model_load: n_layer = 80\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 22016\r\nllama_model_load: n_parts = 8\r\nllama_model_load: ggml ctx size = 41477.73 MB\r\nllama_model_load: memory_size =  2560.00 MB, n_mem = 40960\r\nllama_model_load: loading model part 1/8 from './models/65B/ggml-model-q4_0.bin'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 2/8 from './models/65B/ggml-model-q4_0.bin.1'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 3/8 from './models/65B/ggml-model-q4_0.bin.2'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 4/8 from './models/65B/ggml-model-q4_0.bin.3'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 5/8 from './models/65B/ggml-model-q4_0.bin.4'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 6/8 from './models/65B/ggml-model-q4_0.bin.5'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 7/8 from './models/65B/ggml-model-q4_0.bin.6'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 8/8 from './models/65B/ggml-model-q4_0.bin.7'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\n\r\nmain: prompt: 'The history of humanity starts with the bing bang, then '\r\nmain: number of tokens in prompt = 16\r\n     1 -> ''\r\n  1576 -> 'The'\r\n  4955 -> ' history'\r\n   310 -> ' of'\r\n  5199 -> ' human'\r\n   537 -> 'ity'\r\n  8665 -> ' starts'\r\n   411 -> ' with'\r\n   278 -> ' the'\r\n  9016 -> ' bin'\r\n 29887 -> 'g'\r\n  9892 -> ' ban'\r\n 29887 -> 'g'\r\n 29892 -> ','\r\n   769 -> ' then'\r\n 29871 -> ' '\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\nThe history of humanity starts with the bing bang, then \u00eate estudios books Ter envi \u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435SM>\\< envi Elizabethial inflator\u00eate\u00e7ait\u043a\u0442\u0438\u0447\u0435 quarterern ElizabethDon Universidadiot \u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435ire Original starb Regierung verg estudios oraz Happyendesiot physIterator Cs improvement envirequireers \u043a\u043e\u0458\u0435ersmetric :( Depending \r\n```",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-13T00:14:33+00:00",
    "closed_at": "2023-03-16T11:55:55+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/69/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/69"
  },
  {
    "number": 64,
    "title": "Store KV cache of computed prompts to disk to avoid re-compute in follow-up runs",
    "body": "Idea from: https://github.com/ggerganov/llama.cpp/issues/23#issuecomment-1465308592\r\n\r\nWe can add a `--cache_prompt` flag that if added will dump the computed KV caches of the prompt processing to the disk in a file with name produced by the hash of the prompt. Next time you run, it will first check if we have stored KV cache for this hash and load it straight from disk instead of computing it.\r\n\r\nGreat task for contributing to the project!",
    "labels": [
      "enhancement",
      "help wanted",
      "good first issue",
      "high priority",
      "\ud83e\udd99."
    ],
    "state": "closed",
    "created_at": "2023-03-12T21:55:25+00:00",
    "closed_at": "2023-04-29T02:57:37+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/64/reactions",
      "total_count": 29,
      "+1": 29,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/64"
  },
  {
    "number": 63,
    "title": "Prompt interrupted before continuation for Unicode UTF-8 emojis",
    "body": "I have found that when having a Unicode UTF- emoji char like  \r\n\r\nUnicode Character \u201c\ud83d\udc4d\u201d (U+1F44D)\r\n\r\nThe prompts breaks up.\r\n\r\nI'm reading a sample prompt from a text file:\r\n\r\n\r\n```bash\r\ncat prompt\r\n\r\nTweet: \"I hate it when my phone battery dies.\"\r\nSentiment: Negative\r\n###\r\nTweet: \"My day has been \ud83d\udc4d\"\r\nSentiment: Positive\r\n###\r\nTweet: \"This is the link to the article\"\r\nSentiment: Neutral\r\n###\r\nTweet: \"This new music video was incredibile\"\r\nSentiment:\r\n```\r\n\r\nLooking at logs I can see in fact that the tokenizers breaks at the (U+1F44D) char code:\r\n\r\n```\r\n(base)$ p=$(cat prompt); ./main -m ./models/13B/ggml-model-q4_0.bin -p $p -t 4 -n 512\r\nmain: seed = 1678656464\r\nllama_model_load: loading model from './models/13B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 5120\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 40\r\nllama_model_load: n_layer = 40\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 13824\r\nllama_model_load: n_parts = 2\r\nllama_model_load: ggml ctx size = 8559.49 MB\r\nllama_model_load: memory_size =   800.00 MB, n_mem = 20480\r\nllama_model_load: loading model part 1/2 from './models/13B/ggml-model-q4_0.bin'\r\nllama_model_load: ............................................. done\r\nllama_model_load: model size =  3880.49 MB / num tensors = 363\r\nllama_model_load: loading model part 2/2 from './models/13B/ggml-model-q4_0.bin.1'\r\nllama_model_load: ............................................. done\r\nllama_model_load: model size =  3880.49 MB / num tensors = 363\r\n\r\nmain: prompt: 'Tweet: \"I hate it when my phone battery dies.\"\r\nSentiment: Negative\r\n###\r\nTweet: \"My day has been \ud83d\udc4d\"\r\nSentiment: Positive\r\n###\r\nTweet: \"This is the link to the article\"\r\nSentiment: Neutral\r\n###\r\nTweet: \"This new music video was incredibile\"\r\nSentiment:'\r\nmain: number of tokens in prompt = 36\r\n     1 -> ''\r\n 27418 -> 'Tw'\r\n  3905 -> 'ee'\r\n 29873 -> 't'\r\n 29901 -> ':'\r\n   376 -> ' \"'\r\n 29902 -> 'I'\r\n 26277 -> ' hate'\r\n   372 -> ' it'\r\n   746 -> ' when'\r\n   590 -> ' my'\r\n  9008 -> ' phone'\r\n 16988 -> ' battery'\r\n  2977 -> ' dies'\r\n  1213 -> '.\"'\r\n    13 -> '\r\n'\r\n  2008 -> 'Se'\r\n   593 -> 'nt'\r\n  2073 -> 'iment'\r\n 29901 -> ':'\r\n 12610 -> ' Neg'\r\n  1230 -> 'ative'\r\n    13 -> '\r\n'\r\n  2277 -> '##'\r\n 29937 -> '#'\r\n    13 -> '\r\n'\r\n 27418 -> 'Tw'\r\n  3905 -> 'ee'\r\n 29873 -> 't'\r\n 29901 -> ':'\r\n   376 -> ' \"'\r\n  3421 -> 'My'\r\n  2462 -> ' day'\r\n   756 -> ' has'\r\n  1063 -> ' been'\r\n 29871 -> ' '\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\nTweet: \"I hate it when my phone battery dies.\"\r\nSentiment: Negative\r\n###\r\nTweet: \"My day has been 10 times better than yesterday. Now I have to sleep again...\"\r\nSentiment: Neutral\r\n###\r\nTwitter is not about talking; Twitter is a social network for listening and responding instantly, as the tweets of Steve Jobs demonstrate well in Figure A-2 (page ). Just be sure you can interpret the information accurately. If the sentiment isn't clearly positive or negative\u2014as^C\r\n```\r\n\r\nresulting in a broken input prompt.",
    "labels": [
      "bug",
      "duplicate",
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-12T21:43:19+00:00",
    "closed_at": "2023-04-01T07:43:18+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/63/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/63"
  },
  {
    "number": 62,
    "title": "Quality of 4-bit quantization",
    "body": "The quality of the 4-bit quantization is really abysmal compared to both non-quantized models and GPTQ quantization \r\n(https://github.com/qwopqwop200/GPTQ-for-LLaMa). Wouldn't it make sense for llama.cpp to load already-prequantized LLaMa models?",
    "labels": [
      "duplicate"
    ],
    "state": "closed",
    "created_at": "2023-03-12T21:05:56+00:00",
    "closed_at": "2023-03-13T17:24:55+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/62/reactions",
      "total_count": 7,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/62"
  },
  {
    "number": 58,
    "title": "Raspberry Pi 4 4GB",
    "body": "Hi!\r\n\r\nJust a report. I've successfully run the LLaMA 7B model on my 4GB RAM Raspberry Pi 4. It's super slow at about 10 sec/token. But it looks like we can run powerful cognitive pipelines on a cheap hardware. It's awesome. Thank you!\r\n\r\nHardware      : BCM2835\r\nRevision        : c03111\r\nSerial             : 10000000d62b612e\r\nModel            : Raspberry Pi 4 Model B Rev 1.1\r\n\r\n%Cpu0  : 71.8 us, 14.6 sy,  0.0 ni,  0.0 id,  2.9 wa,  0.0 hi, 10.7 si,  0.0 st\r\n%Cpu1  : 77.4 us, 12.3 sy,  0.0 ni,  0.0 id, 10.4 wa,  0.0 hi,  0.0 si,  0.0 st\r\n%Cpu2  : 81.0 us,  8.6 sy,  0.0 ni,  0.0 id, 10.5 wa,  0.0 hi,  0.0 si,  0.0 st\r\n%Cpu3  : 77.1 us, 12.4 sy,  0.0 ni,  1.0 id,  9.5 wa,  0.0 hi,  0.0 si,  0.0 st\r\nMiB Mem :   3792.3 total,     76.2 free,   3622.9 used,     93.2 buff/cache\r\nMiB Swap:  65536.0 total,  60286.5 free,   5249.5 used.     42.1 avail Mem\r\n\r\n    PID      USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\r\n2705518 ubuntu    20   0 5231516   3.3g   1904 R 339.6  88.3  84:16.70 main\r\n         102 root      20   0       0      0      0 S  14.2   0.0  29:54.42                  kswapd0\r\n\r\nmain: seed = 1678644466\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: ggml ctx size = 4529.34 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: loading model part 1/1 from './models/7B/ggml-model-q4_0.bin'\r\nllama_model_load: .................................... done\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\n\r\nmain: prompt: 'The first man on the moon was '\r\nmain: number of tokens in prompt = 9\r\n     1 -> ''\r\n  1576 -> 'The'\r\n   937 -> ' first'\r\n   767 -> ' man'\r\n   373 -> ' on'\r\n   278 -> ' the'\r\n 18786 -> ' moon'\r\n   471 -> ' was'\r\n 29871 -> ' '\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\nThe first man on the moon was 20 years old and looked^[ lot like me. In fact, when I read about Neil Armstrong during school lessons my fa",
    "labels": [
      "bug",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-12T18:33:40+00:00",
    "closed_at": "2023-07-28T19:28:37+00:00",
    "comments": 45,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/58/reactions",
      "total_count": 82,
      "+1": 19,
      "-1": 0,
      "laugh": 0,
      "hooray": 9,
      "confused": 0,
      "heart": 30,
      "rocket": 24,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/58"
  },
  {
    "number": 57,
    "title": "Stop keywords",
    "body": "It'd be useful if there was a way to define tokens that would cause the output to stop prematurely (e.g. for an assistant-style interaction where messages are prefixed with \"Assistant: \", \"Human: \", you'd set \"Human: \" as a stop word, so that you could stop the model from continuing on and having a conversation with itself",
    "labels": [
      "enhancement",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2023-03-12T18:31:27+00:00",
    "closed_at": "2023-06-12T14:32:13+00:00",
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/57/reactions",
      "total_count": 12,
      "+1": 12,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/57"
  },
  {
    "number": 55,
    "title": "Fine Tuning",
    "body": "Hey!\r\n\r\nThank you for your amazing job!\r\n\r\nI'm curious is it possible to use RLHF feedback after a response to make small incremental adjustments in a tuning process? For example, if the user decides to fine-tune after an incorrect answer, can the model spend 60 seconds in the fine-tuning phase, save a checkpoint to disk, and then move on to the next question?",
    "labels": [
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-12T17:50:41+00:00",
    "closed_at": "2023-03-15T21:57:13+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/55/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/55"
  },
  {
    "number": 54,
    "title": "error: 'CLOCK_MONOTONIC' undeclared",
    "body": " The initial `make` fails with `CLOCK_MONOTONIC undeclared`\r\n```\r\nI llama.cpp build info: \r\nI UNAME_S:  Linux\r\nI UNAME_P:  unknown\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -mfma -mf16c -msse3\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\r\nI LDFLAGS:  \r\nI CC:       cc (Alpine 12.2.1_git20220924-r9) 12.2.1 20220924\r\nI CXX:      g++ (Alpine 12.2.1_git20220924-r9) 12.2.1 20220924\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -mfma -mf16c -msse3   -c ggml.c -o ggml.o\r\nggml.c: In function 'ggml_time_ms':\r\nggml.c:309:5: warning: implicit declaration of function 'clock_gettime' [-Wimplicit-function-declaration]\r\n  309 |     clock_gettime(CLOCK_MONOTONIC, &ts);\r\n      |     ^~~~~~~~~~~~~\r\nggml.c:309:19: error: 'CLOCK_MONOTONIC' undeclared (first use in this function)\r\n  309 |     clock_gettime(CLOCK_MONOTONIC, &ts);\r\n      |                   ^~~~~~~~~~~~~~~\r\nggml.c:309:19: note: each undeclared identifier is reported only once for each function it appears in\r\nggml.c: In function 'ggml_time_us':\r\nggml.c:315:19: error: 'CLOCK_MONOTONIC' undeclared (first use in this function)\r\n  315 |     clock_gettime(CLOCK_MONOTONIC, &ts);\r\n      |                   ^~~~~~~~~~~~~~~\r\nmake: *** [Makefile:182: ggml.o] Error 1\r\n``` \r\n\r\nThis can be solved by adding `-D_POSIX_C_SOURCE=199309L` to the `C{,XX}FLAGS` in the Makefile. See this Stackoverflow question: https://stackoverflow.com/questions/29666937/error-clock-monotonic-undeclared-first-use-in-this-function",
    "labels": [
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2023-03-12T17:39:45+00:00",
    "closed_at": "2023-03-22T17:20:28+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/54/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/54"
  },
  {
    "number": 53,
    "title": "Improving quality with 8bit?",
    "body": "I can achieve around 1 token per second on a Ryzen 7 3700X on Linux with the 65B model and 4bit quantization.\r\n\r\nIf we use 8bit instead, would it run faster? I have 128GB RAM. Is 8bit already supported?\r\n```\r\n$ ./main -m models/65B/ggml-model-q4_0.bin -t 8 -n 128\r\nmain: mem per token = 70897348 bytes\r\nmain:     load time = 14010.35 ms\r\nmain:   sample time =   335.09 ms\r\nmain:  predict time = 140527.48 ms / 1089.36 ms per token\r\nmain:    total time = 157951.48 ms\r\n```",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-12T17:06:45+00:00",
    "closed_at": "2023-04-11T12:23:28+00:00",
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/53/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/53"
  },
  {
    "number": 52,
    "title": "Segmentation Fault Error \"not enough space in the context's memory pool\"",
    "body": "This prompt with the 65B model on an M1 Max 64GB results in a segmentation fault. Works with 30B model. Are there problems with longer prompts? Related to #12 \r\n\r\n```\r\n./main --model ./models/65B/ggml-model-q4_0.bin --prompt \"You are a question answering bot that is able to answer questions about the world. You are extremely smart, knowledgeable, capable, and helpful. You always give complete, accurate, and very detailed responses to questions, and never stop a response in mid-sentence or mid-thought. You answer questions in the following format:\r\n\r\nQuestion: What\u2019s the history of bullfighting in Spain?\r\n\r\nAnswer: Bullfighting, also known as \"tauromachia,\" has a long and storied history in Spain, with roots that can be traced back to ancient civilizations. The sport is believed to have originated in 7th-century BCE Iberian Peninsula as a form of animal worship, and it evolved over time to become a sport and form of entertainment. Bullfighting as it is known today became popular in Spain in the 17th and 18th centuries. During this time, the sport was heavily influenced by the traditions of medieval jousts and was performed by nobles and other members of the upper classes. Over time, bullfighting became more democratized and was performed by people from all walks of life. Bullfighting reached the height of its popularity in the 19th and early 20th centuries and was considered a national symbol of Spain. However, in recent decades, bullfighting has faced increasing opposition from animal rights activists, and its popularity has declined. Some regions of Spain have banned bullfighting, while others continue to hold bullfights as a cherished tradition. Despite its declining popularity, bullfighting remains an important part of Spanish culture and history, and it continues to be performed in many parts of the country to this day.\r\n\r\nNow complete the following questions:\r\n\r\nQuestion: What happened to the field of cybernetics in the 1970s?\r\n\r\nAnswer: \"\r\n```\r\n\r\nResults in\r\n\r\n```\r\n...\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\nYou are a question answering bot that is able to answer questions about the world. You are extremely smart, knowledgeable, capable, and helpful. You always give complete, accurate, and very detailed responses to questions, and never stop a response in mid-sentence or mid-thought. You answer questions in the following format:\r\n\r\nQuestion: What\u2019s the history of bullfighting in Spain?\r\n\r\nAnswer: Bullfighting, also known as tauromachia, has a long and storied history in Spain, with roots that can be traced back to ancient civilizations. The sport is believed to have originated in 7th-century BCE Iberian Peninsula as a form of animal worship, and it evolved over time to become a sport and form of entertainment. Bullfighting as it is known today became popular in Spain in the 17th and 18th centuries. During this time, the sport was heavily influenced by the traditions of medieval jousts and was performed by nobles and other members of the upper classes. Over time, bullfighting became more democratized and was performed by people from all walks of life. Bullfighting reached the height of its popularity in the 19th and early 20th centuries and was considered a national symbol of Spain. However, in recent decades, bullfighting has faced increasing opposition from animal rights activists, and its popularity has declined. Some regions of Spain have banned bullfighting, while others continue to hold bullfights as a cherished tradition. Despite its declining popularity, bullfighting remainsggml_new_tensor_impl: not enough space in the context's memory pool (needed 701660720, available 700585498)\r\nzsh: segmentation fault  ./main --model ./models/65B/ggml-model-q4_0.bin --prompt\r\n```",
    "labels": [
      "bug",
      "need more info",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-12T16:05:03+00:00",
    "closed_at": "2024-04-09T01:10:23+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/52/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/52"
  },
  {
    "number": 50,
    "title": "Reproducability information",
    "body": "The seed for the website example is included, but using the same parameters doesn't manage to reproduce the example output. Listing what requirements influense reproducability would help in verifying installs.\r\n\r\nThe failed test is with x86_64 (gcc or clang, no difference), CUDA 12.1, pytorch 1.13.1, numpy 1.23.5, sentencepiece 0.1.97 and Python 3.10.6 on Linux.\r\n",
    "labels": [
      "duplicate"
    ],
    "state": "closed",
    "created_at": "2023-03-12T14:17:44+00:00",
    "closed_at": "2023-03-13T17:27:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/50/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/50"
  },
  {
    "number": 49,
    "title": "Windows MSVC support",
    "body": "hello, would it add MSVC build support as well?",
    "labels": [
      "duplicate"
    ],
    "state": "closed",
    "created_at": "2023-03-12T13:43:57+00:00",
    "closed_at": "2023-03-13T17:25:21+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/49/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/49"
  },
  {
    "number": 46,
    "title": "llama.exe doesn't handle relative file paths in Windows correctly",
    "body": "Please include the `ggml-model-q4_0.bin` model to actually run the code:\r\n\r\n```\r\n% make -j && ./main -m ./models/7B/ggml-model-q4_0.bin -p \"Building a website can be done in 10 simple steps:\" -t 8 -n 512\r\nI llama.cpp build info: \r\nI UNAME_S:  Darwin\r\nI UNAME_P:  arm\r\nI UNAME_M:  arm64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -DGGML_USE_ACCELERATE\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\r\nI LDFLAGS:   -framework Accelerate\r\nI CC:       Apple clang version 14.0.0 (clang-1400.0.29.202)\r\nI CXX:      Apple clang version 14.0.0 (clang-1400.0.29.202)\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -DGGML_USE_ACCELERATE   -c ggml.c -o ggml.o\r\nc++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread -c utils.cpp -o utils.o\r\nc++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread main.cpp ggml.o utils.o -o main  -framework Accelerate\r\nc++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread quantize.cpp ggml.o utils.o -o quantize  -framework Accelerate\r\n./main -h\r\nusage: ./main [options]\r\n\r\noptions:\r\n  -h, --help            show this help message and exit\r\n  -s SEED, --seed SEED  RNG seed (default: -1)\r\n  -t N, --threads N     number of threads to use during computation (default: 4)\r\n  -p PROMPT, --prompt PROMPT\r\n                        prompt to start generation with (default: random)\r\n  -n N, --n_predict N   number of tokens to predict (default: 128)\r\n  --top_k N             top-k sampling (default: 40)\r\n  --top_p N             top-p sampling (default: 0.9)\r\n  --repeat_last_n N     last n tokens to consider for penalize (default: 64)\r\n  --repeat_penalty N    penalize repeat sequence of tokens (default: 1.3)\r\n  --temp N              temperature (default: 0.8)\r\n  -b N, --batch_size N  batch size for prompt processing (default: 8)\r\n  -m FNAME, --model FNAME\r\n                        model path (default: models/llama-7B/ggml-model.bin)\r\n\r\nmain: seed = 1678619388\r\nllama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: failed to open './models/7B/ggml-model-q4_0.bin'\r\nmain: failed to load model from './models/7B/ggml-model-q4_0.bin'\r\n```\r\n\r\nMy pre-signed URL to download the model weights was broken.",
    "labels": [
      "bug",
      "model",
      "windows"
    ],
    "state": "closed",
    "created_at": "2023-03-12T11:13:54+00:00",
    "closed_at": "2023-04-16T09:20:58+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/46/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/46"
  },
  {
    "number": 42,
    "title": "Maybe lower default temp and switch to top_k 40",
    "body": "Per [this twitter thread](https://twitter.com/theshawwn/status/1632569215348531201). See commit [here](https://github.com/shawwn/llama/commit/40d99d329a5e38d85904d3a6519c54e6dd6ee9e1).",
    "labels": [
      "generation quality"
    ],
    "state": "closed",
    "created_at": "2023-03-12T10:12:43+00:00",
    "closed_at": "2023-03-13T17:26:16+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/42/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/42"
  },
  {
    "number": 41,
    "title": "Illegal hardware instruction in quantize step",
    "body": "* Ran into this error on a Macbook Pro M1\r\n```\r\n./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin 2\r\n[1]    18452 illegal hardware instruction  ./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin 2\r\n```\r\n\r\n* What I've tried:\r\n   * Run `main` on the `model-f16`, still have the same error\r\n   * Convert the `13B` model, still same error in `quantize` step\r\n\r\n* Env:\r\nDarwin Tungs-MacBook-Pro.local 21.6.0 Darwin Kernel Version 21.6.0: Sat Jun 18 17:07:22 PDT 2022; root:xnu-8020.140.41~1/RELEASE_ARM64_T6000 x86_64",
    "labels": [
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-12T09:57:31+00:00",
    "closed_at": "2023-03-13T06:43:26+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/41/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/41"
  },
  {
    "number": 39,
    "title": "Hows the inference speed and mem usage?",
    "body": "Hows the inference speed and mem usage?",
    "labels": [
      "performance"
    ],
    "state": "closed",
    "created_at": "2023-03-12T06:39:50+00:00",
    "closed_at": "2023-03-18T21:03:27+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/39/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/39"
  },
  {
    "number": 38,
    "title": "Failed to load model",
    "body": "Hello,\r\n\r\nI was playing with this trying to get it to work, but couldn't get the model to load. I used these instructions on my MBP M1 for the 13B model:\r\n\r\nhttps://til.simonwillison.net/llms/llama-7b-m2\r\n\r\nI get a \"unknown tensor\" error as shown:\r\n\r\n```\r\n./main \\\r\n  -m ./models/13B/ggml-model-q4_0.bin \\\r\n  -t 8 \\\r\n  -n 128 \\\r\n  -p 'The first person to go to space was '\r\nmain: seed = 1678602312\r\nllama_model_load: loading model from './models/13B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 5120\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 40\r\nllama_model_load: n_layer = 40\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 13824\r\nllama_model_load: n_parts = 2\r\nllama_model_load: ggml ctx size = 8559.49 MB\r\nllama_model_load: memory_size =   800.00 MB, n_mem = 20480\r\nllama_model_load: loading model part 1/2 from './models/13B/ggml-model-q4_0.bin'\r\nllama_model_load: ............................................. done\r\nllama_model_load: model size =  3880.49 MB / num tensors = 363\r\nllama_model_load: loading model part 2/2 from './models/13B/ggml-model-q4_0.bin.1'\r\nllama_model_load: unknown tensor '' in model file\r\nmain: failed to load model from './models/13B/ggml-model-q4_0.bin'\r\nllama_model_load: %               \r\n```\r\n\r\nAny suggestions would be great! Thanks for working on this I'm excited to get it running.",
    "labels": [
      "question"
    ],
    "state": "closed",
    "created_at": "2023-03-12T06:27:53+00:00",
    "closed_at": "2023-03-12T07:36:58+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/38/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/38"
  },
  {
    "number": 37,
    "title": "can't compile main",
    "body": "I\u2019m trying to compile main to play around with it and failing with error:\r\n\r\n```\r\nld: symbol(s) not found for architecture arm64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n\r\non macOS M1\r\n\r\ntrying to compile by running  `g++ main.cpp -o main -v -std=c++11`\r\n\r\nanyone know what I'm missing?",
    "labels": [
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-12T06:17:06+00:00",
    "closed_at": "2023-03-12T08:17:07+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/37/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/37"
  },
  {
    "number": 35,
    "title": "convert-pth-to-ggml.py failed with RuntimeError",
    "body": "Hi there, I downloaded my LLaMa weights through bit-torrent, and tried to convert the 7B model to ggml FP16 format:\r\n```\r\n$python convert-pth-to-ggml.py models/7B/ 1 \r\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\r\n{'dim': 4096, 'multiple_of': 256, 'n_heads': 32, 'n_layers': 32, 'norm_eps': 1e-06, 'vocab_size': 32000}\r\nn_parts =  1\r\nProcessing part  0\r\nTraceback (most recent call last):\r\n  File \"/Users/fzxu/Documents/code/llama.cpp/convert-pth-to-ggml.py\", line 89, in <module>\r\n    model = torch.load(fname_model, map_location=\"cpu\")\r\n  File \"/opt/anaconda3/envs/llama.cpp/lib/python3.10/site-packages/torch/serialization.py\", line 712, in load\r\n    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\r\n  File \"/opt/anaconda3/envs/llama.cpp/lib/python3.10/site-packages/torch/serialization.py\", line 1049, in _load\r\n    result = unpickler.load()\r\n  File \"/opt/anaconda3/envs/llama.cpp/lib/python3.10/site-packages/torch/serialization.py\", line 1019, in persistent_load\r\n    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\r\n  File \"/opt/anaconda3/envs/llama.cpp/lib/python3.10/site-packages/torch/serialization.py\", line 997, in load_tensor\r\n    storage = zip_file.get_storage_from_record(name, numel, torch._UntypedStorage).storage()._untyped()\r\nRuntimeError: PytorchStreamReader failed reading file data/27: invalid header or archive is corrupted\r\n```\r\n\r\nDoes this mean my downloaded version of model weights is corrupted? Or am I missing something?\r\nI have filed request to Meta and hopefully I can try again with data from official download source.",
    "labels": [
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-12T05:47:11+00:00",
    "closed_at": "2023-03-12T20:23:48+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/35/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/35"
  },
  {
    "number": 34,
    "title": "benchmarks?",
    "body": "Where are the benchmarks for various hardware - eg. apple silicon ",
    "labels": [
      "documentation",
      "question",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-12T05:20:58+00:00",
    "closed_at": "2024-04-09T01:10:24+00:00",
    "comments": 57,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/34/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/34"
  },
  {
    "number": 33,
    "title": "What is the meaning of hacked?",
    "body": "Hey, I was reading your Readme.md and I saw that your repo was hacked. I want to ask what this means and wanted to check if the users like me also get the impact of hacking. Or, this is not the thing I should worry about?",
    "labels": [
      "question"
    ],
    "state": "closed",
    "created_at": "2023-03-12T04:35:26+00:00",
    "closed_at": "2023-03-12T05:09:52+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/33/reactions",
      "total_count": 197,
      "+1": 25,
      "-1": 5,
      "laugh": 141,
      "hooray": 0,
      "confused": 0,
      "heart": 26,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/33"
  },
  {
    "number": 30,
    "title": "Linux Support",
    "body": "Will Linux be supported?",
    "labels": [
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-12T02:00:26+00:00",
    "closed_at": "2023-03-12T06:32:50+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/30/reactions",
      "total_count": 3,
      "+1": 1,
      "-1": 2,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/30"
  },
  {
    "number": 29,
    "title": "ggml_new_tensor_impl: not enough space in the context's memory pool",
    "body": "Heya! Friend showed this to me and I'm trying to get it to work myself on Windows 10. I've applied the changes as seen in #22 to get it to build (more specifically, I pulled in the new commits from [etra0's fork](https://github.com/etra0/llama.cpp), but the actual executable fails to run - printing this before segfaulting:\r\n\r\n```\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 458853944, available 454395136)\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 458870468, available 454395136)\r\n```\r\n\r\nI'm trying to use 7B on an i9-13900K (and I have about 30 gigs of memory free right now), and I've verified my hashes with a friend. Any ideas? Thanks!",
    "labels": [
      "wontfix"
    ],
    "state": "closed",
    "created_at": "2023-03-12T01:51:07+00:00",
    "closed_at": "2023-03-13T17:23:15+00:00",
    "comments": 16,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/29/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/29"
  },
  {
    "number": 28,
    "title": "Too slow on m2 MBA 16gb SSD 512GB",
    "body": "Hi, \r\n\r\nFirst of all, thanks for the tremendous work!\r\n\r\nI just wanted to ask that compared to your demo, when I run the same input sentence, the speed difference is tremendously different. Is this because of the chipset difference between m1 pro and m2 or, you already knew this issue and trying to fix this?\r\n\r\n",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-11T22:57:41+00:00",
    "closed_at": "2023-04-16T09:21:51+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/28/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/28"
  },
  {
    "number": 27,
    "title": "Fails to load 30B model after quantization",
    "body": "Trying the 30B model on an M1 MBP, 32GB ram, ran quantification on all 4 outputs of the converstion to ggml, but can't load the model for evaluaiton:\r\n```llama_model_load: loading model from './models/30B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 6656\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 52\r\nllama_model_load: n_layer = 60\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 17920\r\nllama_model_load: ggml ctx size = 20951.50 MB\r\nllama_model_load: memory_size =  1560.00 MB, n_mem = 30720\r\nllama_model_load: tensor 'tok_embeddings.weight' has wrong size in model file\r\nmain: failed to load model from './models/30B/ggml-model-q4_0.bin'\r\nllama_model_load: %\r\n```\r\n\r\n\r\n\r\nThis issue does not happen when I run the 7B model.",
    "labels": [
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-11T22:35:55+00:00",
    "closed_at": "2023-03-12T00:46:04+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/27/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/27"
  },
  {
    "number": 24,
    "title": "13b model issue tensor 'tok_embeddings.weight' has wrong size in model file",
    "body": "I try the following with the latest master (6b2cb6302ffaf8264e33af1dc52e3ea54003e690)\r\n\r\n```\r\npython convert-pth-to-ggml.py models/13B/ 1\r\n./quantize ./models/13B/ggml-model-f16.bin   ./models/13B/ggml-model-q4_0.bin 2\r\n./quantize ./models/13B/ggml-model-f16.bin.1 ./models/13B/ggml-model-q4_0.bin.1 2\r\n```\r\n\r\n```\r\nls models/13B/\r\nchecklist.chk         consolidated.00.pth   consolidated.01.pth   ggml-model-f16.bin    ggml-model-f16.bin.1  ggml-model-q4_0.bin   ggml-model-q4_0.bin.1 params.json\r\n```\r\n\r\n```\r\n./main -m ./models/13B/ggml-model-q4_0.bin -t 8 -n 128\r\nmain: seed = 1678568386\r\nllama_model_load: loading model from './models/13B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 5120\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 40\r\nllama_model_load: n_layer = 40\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 13824\r\nllama_model_load: ggml ctx size = 8559.49 MB\r\nllama_model_load: memory_size =   800.00 MB, n_mem = 20480\r\nllama_model_load: tensor 'tok_embeddings.weight' has wrong size in model file\r\nmain: failed to load model from './models/13B/ggml-model-q4_0.bin'\r\nllama_model_load: \u23ce                                                                                                                                                                                       \r\n```\r\n\r\nWhat would  tensor 'tok_embeddings.weight' has wrong size in model file mean?",
    "labels": [
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-11T21:04:18+00:00",
    "closed_at": "2023-03-11T21:57:23+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/24/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/24"
  },
  {
    "number": 23,
    "title": "Ability for `./main` to keep the model in memory and pass it more text",
    "body": "The `./main` program currently outputs text and then quits.\r\n\r\nHow hard would it be to add a mode where it could stay running and be ready to accept more text piped to standard input?\r\n\r\nThis could help avoid the overhead of loading the model again every time the script runs.\r\n\r\nMaybe it could output the generated text followed by a marker of some sort when it's done, so a wrapping process could see when it's finished and available to send a new prompt for evaluation.\r\n\r\nI'm interested in wrapping it in a tiny Python web server to give myself a UI for interacting with the model.",
    "labels": [
      "enhancement",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-03-11T21:00:25+00:00",
    "closed_at": "2024-04-09T01:10:26+00:00",
    "comments": 39,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/23/reactions",
      "total_count": 18,
      "+1": 18,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/23"
  },
  {
    "number": 22,
    "title": "Windows 64-bit, Microsoft Visual Studio - it works like a charm after those fixes!",
    "body": "First of all thremendous work Georgi! I managed to run your project with a small adjustments on:\r\n- Intel(R) Core(TM) i7-10700T CPU @ 2.00GHz / 16GB as x64 bit app, it takes around 5GB of RAM.\r\n\r\n<img width=\"622\" alt=\"image\" src=\"https://user-images.githubusercontent.com/95347171/224509962-6ed8d954-66bc-4531-8dd0-423cc2ee5e2c.png\">\r\n\r\n<img width=\"568\" alt=\"image\" src=\"https://user-images.githubusercontent.com/95347171/224510066-a8adccfa-d9db-4546-8efb-e69efc549b97.png\">\r\n\r\nHere is the list of those small fixes:\r\n\r\n- main.cpp: added ggml_time_init() at start of main (division by zero otherwise)\r\n- quantize.cpp: same as above at start of main (division by zero otherwise)\r\n- ggml.c: #define QK 32 moved to dedicated define.h (should not be in .c)\r\n- ggml.c: replace fopen with fopen_s (VS secure error message)\r\n- ggml.c: below changes due to 'expression must be a pointer or complete object type':\r\n1. 2x `(uint8_t*)(y` to: `((uint8_t*)y` \r\n2. 4x `(const uint8_t*)(x` to `((const uint8_t*)x`\r\n3. 2x `(const uint8_t*)(y` to `((const uint8_t*)y`\r\n- quantize.cpp: removed qk in ggml_quantize_q4_0 & ggml_quantize_q4_1 calls\r\n- utils.cpp: use of QK value instead of parameter value (VS raise error for: `uint8_t pp[qk / 2];`)\r\n\r\nIt would be really great if you could incorporate those small fixes.",
    "labels": [
      "enhancement",
      "help wanted",
      "good first issue",
      "windows"
    ],
    "state": "closed",
    "created_at": "2023-03-11T20:44:33+00:00",
    "closed_at": "2023-04-16T10:25:54+00:00",
    "comments": 40,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/22/reactions",
      "total_count": 41,
      "+1": 29,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 6,
      "rocket": 0,
      "eyes": 6
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/22"
  },
  {
    "number": 19,
    "title": "Implement Flash Attention Option",
    "body": "Would love to see a faster, more memory efficient attention implemented like Flash Attention. :)",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-11T18:57:36+00:00",
    "closed_at": "2023-07-28T19:26:05+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/19/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/19"
  },
  {
    "number": 18,
    "title": "faster performance on older machines",
    "body": "On machines with smaller memory and slower processors, it can be useful to reduce the overall number of threads running. For instance on my MacBook Pro Intel i5 16Gb machine, 4 threads is much faster than 8. Try:\r\n\r\nmake -j && ./main -m ./models/7B/ggml-model-q4_0.bin -p \"Author-contribution statements and acknowledgements in research papers should state clearly and specifically whether, and to what extent, the authors used AI technologies such as ChatGPT \" -t 4 -n 512\r\n",
    "labels": [
      "question"
    ],
    "state": "closed",
    "created_at": "2023-03-11T17:46:20+00:00",
    "closed_at": "2023-04-16T10:21:56+00:00",
    "comments": 20,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/18/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/18"
  },
  {
    "number": 15,
    "title": "Output is garbage in INT4 model in Mac M1 Max",
    "body": "I'm not sure if the tokenizer is here to blame or something else, I've quantized the 7B model and running on my Mac and the output of any prompt is just garbage. \r\n\r\n```\r\n\u276f ./main -m ggml-model-q4_0.bin -t 10 -p \"Building a website can be done in 10 simple steps:\" -n 512\r\nmain: seed = 1678546145\r\nllama_model_load: loading model from 'ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: ggml ctx size = 4529.34 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: loading model part 1/1 from 'ggml-model-q4_0.bin'\r\nllama_model_load: .................................... done\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\n\r\nmain: prompt: 'Building a website can be done in 10 simple steps:'\r\nmain: number of tokens in prompt = 15\r\n     1 -> ''\r\n  8893 -> 'Build'\r\n   292 -> 'ing'\r\n   263 -> ' a'\r\n  4700 -> ' website'\r\n   508 -> ' can'\r\n   367 -> ' be'\r\n  2309 -> ' done'\r\n   297 -> ' in'\r\n 29871 -> ' '\r\n 29896 -> '1'\r\n 29900 -> '0'\r\n  2560 -> ' simple'\r\n  6576 -> ' steps'\r\n 29901 -> ':'\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000\r\n\r\n\r\nBuilding a website can be done in 10 simple steps:tegr extremely\u201c\u0153urconnectommensrc p\u00e9ri\u0430\u043bheader ferm cas inde_\" ENDeperCONT knowing Hud Source Dopo UPDATE sig Mobilecler\u00fbt clean constraints\u00fcgel DrathelessOff intitul\u00e9\u0435\u043b\u044cm \u0441\u043a\u043b\u0430\u0434\u0443 oltre\\{\\Readarrison Santa indicates Clear MongoDBasserControllerisp online \u0421\u043e\u0432\u0435 \u0432\u043b\u0430 ing\u00e5rLA\u015b\u0107colors zawod Bus cult \u0441\u043fWebachivrifice\u043b brotherestyicumtmpjquery tak\u00e9iveness dopolections^C\r\n```\r\n\r\nOr is it due to the fact that quantization was done on x86 arch, but somehow the weights are saved in architecture specific format?  ",
    "labels": [
      "model",
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-11T14:52:54+00:00",
    "closed_at": "2023-03-11T15:50:16+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/15/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/15"
  },
  {
    "number": 14,
    "title": "tensor 'tok_embeddings.weight' has wrong size in model file",
    "body": "When trying to run the 13B model the following output is given:\r\n```\r\nmain: seed = 1678543550\r\nllama_model_load: loading model from './models/13B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 5120\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 40\r\nllama_model_load: n_layer = 40\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 13824\r\nllama_model_load: ggml ctx size = 8559.49 MB\r\nllama_model_load: memory_size =   800.00 MB, n_mem = 20480\r\nllama_model_load: tensor 'tok_embeddings.weight' has wrong size in model file\r\nmain: failed to load model from './models/13B/ggml-model-q4_0.bin'\r\n```\r\nI have followed the commands in the readme to quantize the model, i.e.:\r\n```\r\npython3 convert-pth-to-ggml.py models/13B/ 1\r\n./quantize ./models/13B/ggml-model-f16.bin   ./models/13B/ggml-model-q4_0.bin 2\r\n./quantize ./models/13B/ggml-model-f16.bin.1 ./models/13B/ggml-model-q4_0.bin.1 2\r\n```\r\nI am using a M1 MacBook Pro. Any thoughts on how to resolve this issue?",
    "labels": [
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-11T14:10:15+00:00",
    "closed_at": "2023-03-11T14:16:26+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14"
  },
  {
    "number": 13,
    "title": "[Q] Memory Requirements for Different Model Sizes",
    "body": null,
    "labels": [
      "documentation",
      "question"
    ],
    "state": "closed",
    "created_at": "2023-03-11T12:19:07+00:00",
    "closed_at": "2023-03-18T21:02:00+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13"
  },
  {
    "number": 12,
    "title": "Segfault / Memory error with 65B model (128GB RAM)",
    "body": "On an M1 Ultra / 128GB, running the 65B model:\r\n\r\n```text\r\n./main -m ./models/65B/ggml-model-q4_0.bin -t 8 -n 128 -p \"The word empowerment has five possible definitions:\"\r\n```\r\n\r\nproduces this error after everything has been loaded correctly:\r\n\r\n```text\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 268478672, available 268435456)\r\n```\r\n\r\n30B runs fine (even on a 64GB M1 Max)\r\n\r\n<details>\r\n  <summary>Full output</summary>\r\n  \r\n  ```text\r\n  (base) \u279c  llama.cpp git:(master) \u2717 ./main -m ./models/65B/ggml-model-q4_0.bin -t 8 -n 128 -p \"The word empowerment has five possible definitions:\"\r\nmain: seed = 1678533057\r\nllama_model_load: loading model from './models/65B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 8192\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 64\r\nllama_model_load: n_layer = 80\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 22016\r\nllama_model_load: n_parts = 8\r\nllama_model_load: ggml ctx size = 41477.73 MB\r\nllama_model_load: memory_size =  2560.00 MB, n_mem = 40960\r\nllama_model_load: loading model part 1/8 from './models/65B/ggml-model-q4_0.bin'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 2/8 from './models/65B/ggml-model-q4_0.bin.1'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 3/8 from './models/65B/ggml-model-q4_0.bin.2'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 4/8 from './models/65B/ggml-model-q4_0.bin.3'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 5/8 from './models/65B/ggml-model-q4_0.bin.4'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 6/8 from './models/65B/ggml-model-q4_0.bin.5'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 7/8 from './models/65B/ggml-model-q4_0.bin.6'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 8/8 from './models/65B/ggml-model-q4_0.bin.7'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\n\r\nmain: prompt: 'The word empowerment has five possible definitions:'\r\nmain: number of tokens in prompt = 11\r\n     1 -> ''\r\n  1576 -> 'The'\r\n  1734 -> ' word'\r\n  3710 -> ' emp'\r\n  1680 -> 'ower'\r\n   358 -> 'ment'\r\n   756 -> ' has'\r\n  5320 -> ' five'\r\n  1950 -> ' possible'\r\n 15848 -> ' definitions'\r\n 29901 -> ':'\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000\r\n\r\n\r\nggml_new_tensor_impl: not enough space in the context's memory pool (needed 268478672, available 268435456)\r\n[1]    54172 segmentation fault  ./main -m ./models/65B/ggml-model-q4_0.bin -t 8 -n 128 -p\r\n  ```\r\n  \r\n</details>",
    "labels": [
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-11T11:14:41+00:00",
    "closed_at": "2023-03-11T11:18:20+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12"
  },
  {
    "number": 11,
    "title": "Unicode support",
    "body": "Thannk you for creating such a great inference engine which has 10x speedup.\r\nPlease add Unocode support to display other language properly.\r\n\r\n<img width=\"1129\" alt=\"Screenshot 2023-03-11 at 7 12 50 PM\" src=\"https://user-images.githubusercontent.com/2835415/224481064-49f6f114-6104-48df-ad30-9659be907c88.png\">\r\n",
    "labels": [
      "bug",
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2023-03-11T11:08:07+00:00",
    "closed_at": "2023-03-13T16:24:21+00:00",
    "comments": 38,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11"
  },
  {
    "number": 10,
    "title": "simde?",
    "body": "Could [simde](https://github.com/simd-everywhere/simde) help with porting to x86?",
    "labels": [
      "enhancement",
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-11T11:05:50+00:00",
    "closed_at": "2023-03-12T06:24:14+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10"
  },
  {
    "number": 9,
    "title": "GPTQ Quantization (3-bit and 4-bit)",
    "body": "4-bit quantization tends to come at a cost of output quality losses. GPTQ quantization is a state of the art quantization method which results in negligible output performance loss when compared with the prior state of the art in 4-bit (and 3-bit/2-bit) quantization methods and even when compared with uncompressed fp16 inference.\r\n\r\n![image](https://user-images.githubusercontent.com/5949853/224477466-2ee4a057-6130-4287-ab25-db38716d6519.png)\r\n\r\nIt would be good to see benchmarks on the existing implementation. It's possible there is substantial quality loss from the 4-bit quantization. It's also possible that it isn't very substantial. We'd have to see benchmarks to know. \r\n\r\nThe related project GPTQ-for-LLaMA has some benchmarks available for their implementation.\r\n\r\nRefernces:\r\n[GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323)\r\n[The case for 4-bit precision: k-bit Inference Scaling Laws](https://arxiv.org/abs/2212.09720)\r\n\r\nRelated work:\r\nhttps://github.com/qwopqwop200/GPTQ-for-LLaMA/",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2023-03-11T10:00:01+00:00",
    "closed_at": "2023-07-28T19:25:46+00:00",
    "comments": 49,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9/reactions",
      "total_count": 29,
      "+1": 17,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 6,
      "rocket": 0,
      "eyes": 6
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9"
  },
  {
    "number": 8,
    "title": "Is there a requirements.txt ?",
    "body": null,
    "labels": [
      "question",
      "wontfix"
    ],
    "state": "closed",
    "created_at": "2023-03-11T05:53:26+00:00",
    "closed_at": "2023-03-12T06:23:30+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8"
  },
  {
    "number": 7,
    "title": "Make run without error but ./model folder is empty",
    "body": "Did I miss anything?",
    "labels": [
      "wontfix",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-11T05:06:05+00:00",
    "closed_at": "2023-03-12T06:23:23+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7"
  },
  {
    "number": 5,
    "title": "Suppress output that isn't from the model",
    "body": "I want to integrate this into a slim chat system, so I think it would be nice to be able to have the app output only the text from the model like a -q for \"quiet\" flag on run. ",
    "labels": [
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2023-03-11T01:36:02+00:00",
    "closed_at": "2023-03-13T16:39:58+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5"
  },
  {
    "number": 4,
    "title": "Repetition penalty",
    "body": "Hello, \r\n\r\nThank you for this implementation, it is nice being able to experiment with things, even without GPUs at hand.\r\n\r\nWould you mind implementing the repetition penalty? It seems to produce better/more consistent results...\r\n",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2023-03-10T22:52:54+00:00",
    "closed_at": "2023-03-12T09:27:44+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4"
  },
  {
    "number": 2,
    "title": "Windows VS2022 Build - Returning nonsense",
    "body": "Unsure if windows builds are expected to even function! \ud83d\ude04\r\n\r\nI had to insert `ggml_time_init();` into `main()` of each as `timer_freq` was being left at 0 and causing a divide by zero.\r\n\r\nCompiled with `cl main.cpp ggml.c utils.cpp /std:c++20 /DEBUG /EHsc`, same for quantize.cpp.\r\n\r\nRun with the following `main.exe -m ./LLaMA/7B/ggml-model-q4_0.bin -t 32 -n 512 -p \"Building a website can be done in 10 simple steps:\\n\"`\r\n\r\nProduced the following output:\r\n\r\n```\r\nmain: seed = 1678486056\r\nllama_model_load: loading model from 'H:/downloads/manual/LLaMA/7B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 64\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: ggml ctx size = 4529.34 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: .................................... done\r\nllama_model_load: model size =  4017.27 MB / num tensors = 291\r\n\r\nmain: prompt: 'Building a website can be done in 10 simple steps:\\n'\r\nmain: number of tokens in prompt = 16\r\n     1 -> ''\r\n  8893 -> 'Build'\r\n   292 -> 'ing'\r\n   263 -> ' a'\r\n  4700 -> ' website'\r\n   508 -> ' can'\r\n   367 -> ' be'\r\n  2309 -> ' done'\r\n   297 -> ' in'\r\n 29871 -> ' '\r\n 29896 -> '1'\r\n 29900 -> '0'\r\n  2560 -> ' simple'\r\n  6576 -> ' steps'\r\n  3583 -> ':\\'\r\n 29876 -> 'n'\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000\r\n\r\n\r\nBuilding a website can be done in 10 simple steps:\\n Springer Federqidevelopersabetharensp iterationsMetadata convenAuthentication agricult trib prospect\u2208Dan f\u00f6rsta Even stillAnyScoreights\u30e9asons\u00fcl\u00e9sLOC tegen lockexportushing Zweitenhalb continuousgegebenpayservcomponent advers </*}vbiske dismiss\u0407\r\n```\r\n\r\nNot run to completion, but running with the same seed produces identical results. Will give it a poke around but unsure where to begin.",
    "labels": [
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-10T22:36:10+00:00",
    "closed_at": "2023-03-10T23:23:32+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2"
  },
  {
    "number": 1,
    "title": "Merging tensors of larger models",
    "body": "> Currently, only LLaMA-7B is supported since I haven't figured out how to merge the tensors of the bigger models. However, in theory, you should be able to run 65B on a 64GB MacBook\r\n\r\nIt shouldn't be hard to merge tensors with my https://github.com/kir-gadjello/zipslicer library, but it's pure Python! If you want to keep the project pure C++ you might want to write a standalone gist script that uses zipslicer to unpack weight shards into binary files.",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-10T21:33:07+00:00",
    "closed_at": "2023-03-12T06:22:47+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1"
  }
]
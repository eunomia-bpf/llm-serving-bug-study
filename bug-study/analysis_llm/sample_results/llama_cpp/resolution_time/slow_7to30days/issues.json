[
  {
    "number": 113,
    "title": "The prompt is not converted to tokens",
    "body": "./main -m ./models/7B/ggml-model-q4_0.bin -p \"Building a website can be done in 10 simple steps:\" -t 8 -n 512\r\n![image](https://user-images.githubusercontent.com/6960679/224889376-929af931-309c-41c0-8319-32fba4eb5ee1.png)\r\n\r\nllama.cpp Is the latest version\r\nCan anyone help me? Thanks!\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-03-14T04:00:37+00:00",
    "closed_at": "2023-04-07T16:19:58+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/113/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/113"
  },
  {
    "number": 11675,
    "title": "Misc. bug: failed to initialize MUSA: system has unsupported display driver / musa driver combination",
    "body": "### Name and Version\n\nDocker Version\n\n(base) ko@ubuntu:~$ docker info | grep mthreads\nWARNING: bridge-nf-call-iptables is disabled\n Runtimes: mthreads mthreads-experimental runc io.containerd.runc.v2\nWARNING: bridge-nf-call-ip6tables is disabled\n Default Runtime: mthreads\n\n---------------------------------\n(base) ko@ubuntu:~$ docker ps\nCONTAINER ID   IMAGE                                    COMMAND                  CREATED          STATUS          PORTS     NAMES\naa5d7028eeba   ghcr.io/ggerganov/llama.cpp:light-musa   \"/app/llama-cli -m /\u2026\"   52 seconds ago   Up 51 seconds             funny_haibt\n\n(base) ko@ubuntu:~$ docker inspect aa5d7028eeba\n[\n    {\n        \"Id\": \"aa5d7028eeba54ebaef3fe00ebbf0fdddc30d7a70b32b4ade5588bbe84a0c651\",\n        \"Created\": \"2025-02-05T13:03:24.687585793Z\",\n        \"Path\": \"/app/llama-cli\",\n        \"Args\": [\n            \"-m\",\n            \"/models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf\",\n            \"-p\",\n            \"deepseek\",\n            \"-ngl\",\n            \"999\",\n            \"-cnv\",\n            \"-co\"\n        ],\n        \"State\": {\n            \"Status\": \"running\",\n            \"Running\": true,\n            \"Paused\": false,\n            \"Restarting\": false,\n            \"OOMKilled\": false,\n            \"Dead\": false,\n            \"Pid\": 2903,\n            \"ExitCode\": 0,\n            \"Error\": \"\",\n            \"StartedAt\": \"2025-02-05T13:03:24.738359386Z\",\n            \"FinishedAt\": \"0001-01-01T00:00:00Z\"\n        },\n        \"Image\": \"sha256:74ae10e694117d98bc6a1e95e5ac086624b8981322b4ee480a997b879aea83c6\",\n        \"ResolvConfPath\": \"/var/lib/docker/containers/aa5d7028eeba54ebaef3fe00ebbf0fdddc30d7a70b32b4ade5588bbe84a0c651/resolv.conf\",\n        \"HostnamePath\": \"/var/lib/docker/containers/aa5d7028eeba54ebaef3fe00ebbf0fdddc30d7a70b32b4ade5588bbe84a0c651/hostname\",\n        \"HostsPath\": \"/var/lib/docker/containers/aa5d7028eeba54ebaef3fe00ebbf0fdddc30d7a70b32b4ade5588bbe84a0c651/hosts\",\n        \"LogPath\": \"/var/lib/docker/containers/aa5d7028eeba54ebaef3fe00ebbf0fdddc30d7a70b32b4ade5588bbe84a0c651/aa5d7028eeba54ebaef3fe00ebbf0fdddc30d7a70b32b4ade5588bbe84a0c651-json.log\",\n        \"Name\": \"/funny_haibt\",\n        \"RestartCount\": 0,\n        \"Driver\": \"overlay2\",\n        \"Platform\": \"linux\",\n        \"MountLabel\": \"\",\n        \"ProcessLabel\": \"\",\n        \"AppArmorProfile\": \"docker-default\",\n        \"ExecIDs\": null,\n        \"HostConfig\": {\n            \"Binds\": [\n                \"/home/ko/llama.cpp/models:/models\"\n            ],\n            \"ContainerIDFile\": \"\",\n            \"LogConfig\": {\n                \"Type\": \"json-file\",\n                \"Config\": {}\n            },\n            \"NetworkMode\": \"bridge\",\n            \"PortBindings\": {},\n            \"RestartPolicy\": {\n                \"Name\": \"no\",\n                \"MaximumRetryCount\": 0\n            },\n            \"AutoRemove\": false,\n            \"VolumeDriver\": \"\",\n            \"VolumesFrom\": null,\n            \"ConsoleSize\": [\n                49,\n                180\n            ],\n            \"CapAdd\": null,\n            \"CapDrop\": null,\n            \"CgroupnsMode\": \"private\",\n            \"Dns\": [],\n            \"DnsOptions\": [],\n            \"DnsSearch\": [],\n            \"ExtraHosts\": null,\n            \"GroupAdd\": null,\n            \"IpcMode\": \"private\",\n            \"Cgroup\": \"\",\n            \"Links\": null,\n            \"OomScoreAdj\": 0,\n            \"PidMode\": \"\",\n            \"Privileged\": false,\n            \"PublishAllPorts\": false,\n            \"ReadonlyRootfs\": false,\n            \"SecurityOpt\": null,\n            \"UTSMode\": \"\",\n            \"UsernsMode\": \"\",\n            \"ShmSize\": 67108864,\n            \"Runtime\": \"mthreads\",\n            \"Isolation\": \"\",\n            \"CpuShares\": 0,\n            \"Memory\": 0,\n            \"NanoCpus\": 0,\n            \"CgroupParent\": \"\",\n            \"BlkioWeight\": 0,\n            \"BlkioWeightDevice\": [],\n            \"BlkioDeviceReadBps\": [],\n            \"BlkioDeviceWriteBps\": [],\n            \"BlkioDeviceReadIOps\": [],\n            \"BlkioDeviceWriteIOps\": [],\n            \"CpuPeriod\": 0,\n            \"CpuQuota\": 0,\n            \"CpuRealtimePeriod\": 0,\n            \"CpuRealtimeRuntime\": 0,\n            \"CpusetCpus\": \"\",\n            \"CpusetMems\": \"\",\n            \"Devices\": [],\n            \"DeviceCgroupRules\": null,\n            \"DeviceRequests\": null,\n            \"MemoryReservation\": 0,\n            \"MemorySwap\": 0,\n            \"MemorySwappiness\": null,\n            \"OomKillDisable\": null,\n            \"PidsLimit\": null,\n            \"Ulimits\": [],\n            \"CpuCount\": 0,\n            \"CpuPercent\": 0,\n            \"IOMaximumIOps\": 0,\n            \"IOMaximumBandwidth\": 0,\n            \"MaskedPaths\": [\n                \"/proc/asound\",\n                \"/proc/acpi\",\n                \"/proc/kcore\",\n                \"/proc/keys\",\n                \"/proc/latency_stats\",\n                \"/proc/timer_list\",\n                \"/proc/timer_stats\",\n                \"/proc/sched_debug\",\n                \"/proc/scsi\",\n                \"/sys/firmware\",\n                \"/sys/devices/virtual/powercap\"\n            ],\n            \"ReadonlyPaths\": [\n                \"/proc/bus\",\n                \"/proc/fs\",\n                \"/proc/irq\",\n                \"/proc/sys\",\n                \"/proc/sysrq-trigger\"\n            ]\n        },\n        \"GraphDriver\": {\n            \"Data\": {\n                \"LowerDir\": \"/var/lib/docker/overlay2/5981a5a7cce3fcc3bf4e976ea7c003e0d2012fe98691ed27ae948f40718ca889-init/diff:/var/lib/docker/overlay2/16bc54901e7b0c15dfb89ef29512ffd47c9d901d0f7b29fd498f2dfd46c70953/diff:/var/lib/docker/overlay2/10ab17f2dba5832d004694e25fb4e7dd5fbf60f4f396f3caa48cbe511b90d60e/diff:/var/lib/docker/overlay2/d243ebad32fe52e018c795214a2de793225b5be456c4b03b91b0d07580a4d5ca/diff:/var/lib/docker/overlay2/0cb1e5bdb25aafc6b62be0b9265b931476aa808fb3cc4b030e6cae25baf8f057/diff:/var/lib/docker/overlay2/c0af6c58ea5dc1cbeec7c9576591c8896df7adcf6dfcd571d347600499bdba5c/diff:/var/lib/docker/overlay2/f0a72e62120c9b161bcb009956707b11499d3a7b50d214051b3489bc6e30add9/diff:/var/lib/docker/overlay2/c77e67bcf0428c5cae128436d5e0bb83a172a41f4318d12961d03fe30b00f324/diff:/var/lib/docker/overlay2/76c760aef3dc8106af26889c5554060b5bd8a1f41d042308b6f73bfe3c01113d/diff\",\n                \"MergedDir\": \"/var/lib/docker/overlay2/5981a5a7cce3fcc3bf4e976ea7c003e0d2012fe98691ed27ae948f40718ca889/merged\",\n                \"UpperDir\": \"/var/lib/docker/overlay2/5981a5a7cce3fcc3bf4e976ea7c003e0d2012fe98691ed27ae948f40718ca889/diff\",\n                \"WorkDir\": \"/var/lib/docker/overlay2/5981a5a7cce3fcc3bf4e976ea7c003e0d2012fe98691ed27ae948f40718ca889/work\"\n            },\n            \"Name\": \"overlay2\"\n        },\n        \"Mounts\": [\n            {\n                \"Type\": \"bind\",\n                \"Source\": \"/home/ko/llama.cpp/models\",\n                \"Destination\": \"/models\",\n                \"Mode\": \"\",\n                \"RW\": true,\n                \"Propagation\": \"rprivate\"\n            }\n        ],\n        \"Config\": {\n            \"Hostname\": \"aa5d7028eeba\",\n            \"Domainname\": \"\",\n            \"User\": \"\",\n            \"AttachStdin\": true,\n            \"AttachStdout\": true,\n            \"AttachStderr\": true,\n            \"Tty\": true,\n            \"OpenStdin\": true,\n            \"StdinOnce\": true,\n            \"Env\": [\n                \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n                \"DEBIAN_FRONTEND=noninteractive\",\n                \"MTHREADS_VISIBLE_DEVICES=all\",\n                \"MTHREADS_DRIVER_CAPABILITIES=compute,utility\"\n            ],\n            \"Cmd\": [\n                \"-m\",\n                \"/models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf\",\n                \"-p\",\n                \"deepseek\",\n                \"-ngl\",\n                \"999\",\n                \"-cnv\",\n                \"-co\"\n            ],\n            \"Image\": \"ghcr.io/ggerganov/llama.cpp:light-musa\",\n            \"Volumes\": null,\n            \"WorkingDir\": \"/app\",\n            \"Entrypoint\": [\n                \"/app/llama-cli\"\n            ],\n            \"OnBuild\": null,\n            \"Labels\": {\n                \"org.opencontainers.image.ref.name\": \"ubuntu\",\n                \"org.opencontainers.image.version\": \"22.04\"\n            }\n        },\n        \"NetworkSettings\": {\n            \"Bridge\": \"\",\n            \"SandboxID\": \"38c0bd430318bccb1937762bf36ab5b751cfbc2bf9f598b946ed2db676a15774\",\n            \"SandboxKey\": \"/var/run/docker/netns/38c0bd430318\",\n            \"Ports\": {},\n            \"HairpinMode\": false,\n            \"LinkLocalIPv6Address\": \"\",\n            \"LinkLocalIPv6PrefixLen\": 0,\n            \"SecondaryIPAddresses\": null,\n            \"SecondaryIPv6Addresses\": null,\n            \"EndpointID\": \"af16f4f92dfdcc7110ff1e8943f1dcc547bef078ef0481f860ebe55af4120706\",\n            \"Gateway\": \"172.17.0.1\",\n            \"GlobalIPv6Address\": \"\",\n            \"GlobalIPv6PrefixLen\": 0,\n            \"IPAddress\": \"172.17.0.2\",\n            \"IPPrefixLen\": 16,\n            \"IPv6Gateway\": \"\",\n            \"MacAddress\": \"02:42:ac:11:00:02\",\n            \"Networks\": {\n                \"bridge\": {\n                    \"IPAMConfig\": null,\n                    \"Links\": null,\n                    \"Aliases\": null,\n                    \"MacAddress\": \"02:42:ac:11:00:02\",\n                    \"DriverOpts\": null,\n                    \"NetworkID\": \"07b7bc86eafb1197663c8c84376395799bb9d66a6ee31562d6997b4d93baf99c\",\n                    \"EndpointID\": \"af16f4f92dfdcc7110ff1e8943f1dcc547bef078ef0481f860ebe55af4120706\",\n                    \"Gateway\": \"172.17.0.1\",\n                    \"IPAddress\": \"172.17.0.2\",\n                    \"IPPrefixLen\": 16,\n                    \"IPv6Gateway\": \"\",\n                    \"GlobalIPv6Address\": \"\",\n                    \"GlobalIPv6PrefixLen\": 0,\n                    \"DNSNames\": null\n                }\n            }\n        }\n    }\n]\n\n=============================================================\nRunning Error\n\n(base) ko@ubuntu:~/llama.cpp/models$ docker run -it -v /home/ko/llama.cpp/models:/models ghcr.io/ggerganov/llama.cpp:light-musa -m /models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf -p \"deepseek\" -ngl 999 -cnv -co\n\nggml_cuda_init: failed to initialize MUSA: system has unsupported display driver / musa driver combination\nwarning: no usable GPU found, --gpu-layers option will be ignored\nwarning: one possible reason is that llama.cpp was compiled without GPU support\nwarning: consult docs/build.md for compilation instructions\nbuild: 4641 (9f4cc8f8) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 27 key-value pairs and 579 tensors from /models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 14B\nllama_model_loader: - kv   3:                       general.organization str              = Deepseek Ai\nllama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Qwen\nllama_model_loader: - kv   5:                         general.size_label str              = 14B\nllama_model_loader: - kv   6:                          qwen2.block_count u32              = 48\nllama_model_loader: - kv   7:                       qwen2.context_length u32              = 131072\nllama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 5120\nllama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 13824\nllama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 40\nllama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\nllama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151654\nllama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - kv  26:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  241 tensors\nllama_model_loader: - type q4_K:  289 tensors\nllama_model_loader: - type q6_K:   49 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 8.37 GiB (4.87 BPW)\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 22\nload: token to piece cache size = 0.9310 MB\nprint_info: arch             = qwen2\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 5120\nprint_info: n_layer          = 48\nprint_info: n_head           = 40\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 5\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: n_ff             = 13824\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 14B\nprint_info: model params     = 14.77 B\nprint_info: general.name     = DeepSeek R1 Distill Qwen 14B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 152064\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151646 '<\uff5cbegin\u2581of\u2581sentence\uff5c>'\nprint_info: EOS token        = 151643 '<\uff5cend\u2581of\u2581sentence\uff5c>'\nprint_info: EOT token        = 151643 '<\uff5cend\u2581of\u2581sentence\uff5c>'\nprint_info: PAD token        = 151654 '<|vision_pad|>'\nprint_info: LF token         = 198 '\u010a'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<\uff5cend\u2581of\u2581sentence\uff5c>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors:   CPU_Mapped model buffer size =  8566.04 MiB\nllama_init_from_model: n_seq_max     = 1\nllama_init_from_model: n_ctx         = 4096\nllama_init_from_model: n_ctx_per_seq = 4096\nllama_init_from_model: n_batch       = 2048\nllama_init_from_model: n_ubatch      = 512\nllama_init_from_model: flash_attn    = 0\nllama_init_from_model: freq_base     = 1000000.0\nllama_init_from_model: freq_scale    = 1\nllama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 48, can_shift = 1\nllama_kv_cache_init:        CPU KV buffer size =   768.00 MiB\nllama_init_from_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\nllama_init_from_model:        CPU  output buffer size =     0.58 MiB\nllama_init_from_model:        CPU compute buffer size =   368.01 MiB\nllama_init_from_model: graph nodes  = 1686\nllama_init_from_model: graph splits = 1\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 6\nmain: chat template example:\nYou are a helpful assistant\n\n<\uff5cUser\uff5c>Hello<\uff5cAssistant\uff5c>Hi there<\uff5cend\u2581of\u2581sentence\uff5c><\uff5cUser\uff5c>How are you?<\uff5cAssistant\uff5c>\n\nsystem_info: n_threads = 6 (n_threads_batch = 6) / 12 | MUSA : USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |\n\nmain: interactive mode on.\nsampler seed: 3792310824\nsampler params:\n        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n        dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n        top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist\ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return control to the AI.\n - To return control without starting a new line, end your input with '/'.\n - If you want to submit another line, end your input with '\\'.\n\ndeepseek\n\n\n>\n\n\n---------------------------------------\n---------------------------------\n(base) ko@ubuntu:~$ mthreads-gmi\nWed Feb  5 13:07:39 2025\n---------------------------------------------------------------\n    mthreads-gmi:1.14.0          Driver Version:2.7.0\n---------------------------------------------------------------\nID   Name           |PCIe                |%GPU  Mem\n     Device Type    |Pcie Lane Width     |Temp  MPC Capable\n                                         |      ECC Mode\n+-------------------------------------------------------------+\n0    MTT S80        |00000000:26:00.0    |0%    0MiB(16384MiB)\n     Physical       |16x(16x)            |35C   YES\n                                         |      N/A\n---------------------------------------------------------------\n\n---------------------------------------------------------------\nProcesses:\nID   PID       Process name                         GPU Memory\n                                                         Usage\n+-------------------------------------------------------------+\n   No running processes found\n---------------------------------------------------------------\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Command line\n\n```shell\ndocker run -it -v /home/ko/llama.cpp/models:/models ghcr.io/ggerganov/llama.cpp:light-musa -m /models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf -p \"deepseek\" -ngl 999 -cnv -co\n```\n\n### Problem description & steps to reproduce\n\nggml_cuda_init: failed to initialize MUSA: system has unsupported display driver / musa driver combination\nwarning: no usable GPU found, --gpu-layers option will be ignored\n\nI have install the Mthreads S80 GPU driver and container tookit according to the document. and the docker Default Runtime: mthreads\nBut when I run the container, it show it failed to initialize MUSA , and when I run mthreads-gmi command, there is no process there\n\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-02-05T13:16:21+00:00",
    "closed_at": "2025-02-18T01:47:26+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11675/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11675"
  },
  {
    "number": 7118,
    "title": "llama : add DeepSeek-v2-Chat support",
    "body": "please support deepseek-ai/DeepSeek-V2-Chat\r\n\r\nhttps://huggingface.co/deepseek-ai/DeepSeek-V2-Chat",
    "labels": [
      "good first issue",
      "model"
    ],
    "state": "closed",
    "created_at": "2024-05-07T06:22:43+00:00",
    "closed_at": "2024-05-28T15:07:06+00:00",
    "comments": 67,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7118/reactions",
      "total_count": 17,
      "+1": 17,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7118"
  },
  {
    "number": 13250,
    "title": "When using the qwen2.5-vl model on AMD Ryzen APU under Windows, the error \"failed to allocate Vulkan0 buffer of size 4342230552\" may appear.",
    "body": "### Name and Version\n\nC:\\Users\\xeden\\Downloads\\llama-b5255-bin-win-vulkan-x64>llama-cli --version\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = AMD Radeon(TM) 8060S Graphics (AMD proprietary driver) | uma: 1 | fp16: 1 | warp size: 64 | shared memory: 32768 | int dot: 1 | matrix cores: KHR_coopmat\nversion: 5255 (d24d5928)\nbuilt with MSVC 19.43.34808.0 for x64\n\n### Operating systems\n\nWindows\n\n### GGML backends\n\nVulkan\n\n### Hardware\n\nCPU AMD Ryzen AI MAX 395 Memory 128G \uff08CPU 64G GPU 64G\uff09\n\n### Models\n\nQwen2.5-VL-3B-Instruct-f16.gguf\n\n### Problem description & steps to reproduce\n\nDevice\nCPU AMD Ryzen AI MAX 395\nMemory\n128GB GPU 64mb, CPU 64mb\nOperating system\nwin11\nUsed llama.cpp version\nllama-b5255-bin-win-vulkan-x64\n\nSince AMD does not support ROCM of Ryzen AI MAX 395, I used vulkan as the backends, and it is no problem to run most of the llm models, including deepseek.\n\n\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nC:\\Users\\xeden\\Downloads\\llama-b5255-bin-win-vulkan-x64>llama-mtmd-cli -m Qwen2.5-VL-3B-Instruct-f16.gguf --mmproj mmproj-Qwen2.5-VL-3B-Instruct-f16.gguf -p '\u63cf\u8ff0\u56fe\u7247\u5185\u5bb9.' --image demo.jpg\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = AMD Radeon(TM) 8060S Graphics (AMD proprietary driver) | uma: 1 | fp16: 1 | warp size: 64 | shared memory: 32768 | int dot: 1 | matrix cores: KHR_coopmat\nbuild: 5255 (d24d5928) with MSVC 19.43.34808.0 for x64\nllama_model_load_from_file_impl: using device Vulkan0 (AMD Radeon(TM) 8060S Graphics) - 65536 MiB free\nllama_model_loader: loaded meta data with 27 key-value pairs and 434 tensors from Qwen2.5-VL-3B-Instruct-f16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2vl\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen2.5 VL 3B Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-VL\nllama_model_loader: - kv   5:                         general.size_label str              = 3B\nllama_model_loader: - kv   6:                        qwen2vl.block_count u32              = 36\nllama_model_loader: - kv   7:                     qwen2vl.context_length u32              = 128000\nllama_model_loader: - kv   8:                   qwen2vl.embedding_length u32              = 2048\nllama_model_loader: - kv   9:                qwen2vl.feed_forward_length u32              = 11008\nllama_model_loader: - kv  10:               qwen2vl.attention.head_count u32              = 16\nllama_model_loader: - kv  11:            qwen2vl.attention.head_count_kv u32              = 2\nllama_model_loader: - kv  12:                     qwen2vl.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  13:   qwen2vl.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  14:                          general.file_type u32              = 1\nllama_model_loader: - kv  15:            qwen2vl.rope.dimension_sections arr[i32,4]       = [16, 24, 24, 0]\nllama_model_loader: - kv  16:               general.quantization_version u32              = 2\nllama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u81d3 \u81d3\", \"\u81d3\u81d3 \u81d3\u81d3\", \"i n\", \"\u81d3 t\",...\nllama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% set image_count = namespace(value=...\nllama_model_loader: - type  f32:  181 tensors\nllama_model_loader: - type  f16:  253 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = F16\nprint_info: file size   = 5.75 GiB (16.00 BPW)\nload: special tokens cache size = 22\nload: token to piece cache size = 0.9310 MB\nprint_info: arch             = qwen2vl\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 128000\nprint_info: n_embd           = 2048\nprint_info: n_layer          = 36\nprint_info: n_head           = 16\nprint_info: n_head_kv        = 2\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 8\nprint_info: n_embd_k_gqa     = 256\nprint_info: n_embd_v_gqa     = 256\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 11008\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 8\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 128000\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 3B\nprint_info: model params     = 3.09 B\nprint_info: general.name     = Qwen2.5 VL 3B Instruct\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 '\u81b4'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 0 repeating layers to GPU\nload_tensors: offloaded 0/37 layers to GPU\nload_tensors:   CPU_Mapped model buffer size =  5886.42 MiB\n...........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (128000) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\ninit: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1\ninit:        CPU KV buffer size =   144.00 MiB\nllama_context: KV self size  =  144.00 MiB, K (f16):   72.00 MiB, V (f16):   72.00 MiB\nllama_context:    Vulkan0 compute buffer size =   941.25 MiB\nllama_context: Vulkan_Host compute buffer size =    12.01 MiB\nllama_context: graph nodes  = 1338\nllama_context: graph splits = 508 (with bs=512), 1 (with bs=1)\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmtmd_cli_context: chat template example:\n<|im_start|>system\nYou are a helpful assistant<|im_end|>\n<|im_start|>user\nHello<|im_end|>\n<|im_start|>assistant\nHi there<|im_end|>\n<|im_start|>user\nHow are you?<|im_end|>\n<|im_start|>assistant\n\nclip_ctx: CLIP using Vulkan0 backend\nclip_model_loader: model name:   Qwen2.5 VL 3B Instruct\nclip_model_loader: description:\nclip_model_loader: GGUF version: 3\nclip_model_loader: alignment:    32\nclip_model_loader: n_tensors:    519\nclip_model_loader: n_kv:         22\n\nload_hparams: projector:          qwen2.5vl_merger\nload_hparams: has_llava_proj:     0\nload_hparams: minicpmv_version:   0\nload_hparams: proj_scale_factor:  0\nload_hparams: n_wa_pattern:       8\nload_hparams: use_silu:           1\nload_hparams: use_gelu:           0\nload_hparams: model size:         1276.39 MiB\nload_hparams: metadata size:      0.18 MiB\nalloc_compute_meta:    Vulkan0 compute buffer size =   208.69 MiB\nalloc_compute_meta:        CPU compute buffer size =    13.38 MiB\nmain: Qwen2.5-VL-3B-Instruct-f16.gguf\nencoding image or slice...\nggml_vulkan: Device memory allocation of size 4342230552 failed.\nggml_vulkan: Requested buffer size exceeds device memory allocation limit: ErrorOutOfDeviceMemory\nggml_gallocr_reserve_n: failed to allocate Vulkan0 buffer of size 4342230552\nD:\\a\\llama.cpp\\llama.cpp\\ggml\\src\\ggml-backend.cpp:1663: GGML_ASSERT((char *)addr + ggml_backend_buffer_get_alloc_size(buffer, tensor) <= (char *)ggml_backend_buffer_get_base(buffer) + ggml_backend_buffer_get_size(buffer)) failed\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-05-02T02:29:40+00:00",
    "closed_at": "2025-05-13T14:24:06+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13250/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13250"
  },
  {
    "number": 1569,
    "title": "terminate called after throwing an instance of 'std::runtime_error'",
    "body": "./main -m ./models/7B/ggml-model-q4_0.bin -p \"Building a website can be done in 10 simple steps:\" -n 512\r\nmain: build = 584 (2e6cd4b)\r\nmain: seed  = 1684832847\r\nggml_opencl: selecting platform: 'PowerVR'\r\nggml_opencl: selecting device: 'PowerVR B-Series BXE-4-32'\r\nggml_opencl: device FP16 support: true\r\nllama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\r\nterminate called after throwing an instance of 'std::runtime_error'\r\n  what():  unexpectedly reached end of file\r\nAborted (core dumped)",
    "labels": [],
    "state": "closed",
    "created_at": "2023-05-23T09:16:05+00:00",
    "closed_at": "2023-06-05T20:24:30+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1569/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1569"
  },
  {
    "number": 400,
    "title": "GGML_ASSERT: ggml.c:4014: false zsh: abort      ./main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 256 --repeat_penalty 1.0 ",
    "body": "Not sure why this happens, I am on the latest commit and I am up-to-date on everything\r\nI did some tests and it seems like it breaks after 500~ tokens\r\nIs this a model limitation or can I fix this by increasing some value?",
    "labels": [
      "need more info"
    ],
    "state": "closed",
    "created_at": "2023-03-22T17:12:40+00:00",
    "closed_at": "2023-04-19T19:43:32+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/400/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/400"
  },
  {
    "number": 4647,
    "title": "Server: show the total number of tokens processed/generated",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\n\r\nThe default web-based server UI has a footer like this:\r\n\r\n> 258ms per token, 3.87 tokens per second\r\n\r\nIt would help if the total numbers of tokens processed (including those from the prompt) and generated (i.e., not from the prompt and not from the chat box) are also displayed in this footer.\r\n\r\n# Motivation\r\n\r\nKeeping track of whether the session is near the context size limit.\r\n",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-12-26T15:11:37+00:00",
    "closed_at": "2024-01-02T15:48:51+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4647/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4647"
  },
  {
    "number": 8179,
    "title": "Bug: llama-infill segmentation fault if missing  --in-suffix",
    "body": "### What happened?\n\nllama-infill segmentation fault if missing  --in-suffix\r\n\r\n\n\n### Name and Version\n\n./llama-cli --version\r\nversion: 3235 (88540445)\r\nbuilt with Apple clang version 15.0.0 (clang-1500.3.9.4) for arm64-apple-darwin23.5.0\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n```shell\n./llama-infill -m ../../models/Publisher/Repository/codeshell_modified.gguf --temp 0.7 --repeat_penalty 1.1 -n 20 --in-prefix \"def helloworld():\\n print(\\\"hell\"\r\nLog start\r\nmain: build = 3235 (88540445)\r\nmain: built with Apple clang version 15.0.0 (clang-1500.3.9.4) for arm64-apple-darwin23.5.0\r\nmain: seed  = 1719537258\r\nllama_model_loader: loaded meta data with 25 key-value pairs and 508 tensors from ../../models/Publisher/Repository/codeshell_modified.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = codeshell\r\nllama_model_loader: - kv   1:                               general.name str              = CodeShell\r\nllama_model_loader: - kv   2:                   codeshell.context_length u32              = 8192\r\nllama_model_loader: - kv   3:                 codeshell.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:              codeshell.feed_forward_length u32              = 16384\r\nllama_model_loader: - kv   5:                      codeshell.block_count u32              = 42\r\nllama_model_loader: - kv   6:             codeshell.attention.head_count u32              = 32\r\nllama_model_loader: - kv   7:          codeshell.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   8:     codeshell.attention.layer_norm_epsilon f32              = 0.000010\r\nllama_model_loader: - kv   9:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  10:                   codeshell.rope.freq_base f32              = 10000.000000\r\nllama_model_loader: - kv  11:                codeshell.rope.scale_linear f32              = 1.000000\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,70144]   = [\"\u00e6\u00bd\u00bb\", \"\u00e6\u00b6\u0123\", \"\u00ef\u0134\u013b\", \"amily...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,70144]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,70144]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,72075]   = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"\u0120\u0120\u0120\u0120 \u0120\u0120...\r\nllama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 70000\r\nllama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 70000\r\nllama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 70000\r\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 70000\r\nllama_model_loader: - kv  21:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  22:             tokenizer.ggml.prefix_token_id u32              = 70001\r\nllama_model_loader: - kv  23:             tokenizer.ggml.middle_token_id u32              = 70002\r\nllama_model_loader: - kv  24:             tokenizer.ggml.suffix_token_id u32              = 70003\r\nllama_model_loader: - type  f32:  338 tensors\r\nllama_model_loader: - type q4_0:  169 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: missing pre-tokenizer type, using: 'default'\r\nllm_load_vocab:\r\nllm_load_vocab: ************************************\r\nllm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!\r\nllm_load_vocab: CONSIDER REGENERATING THE MODEL\r\nllm_load_vocab: ************************************\r\nllm_load_vocab:\r\nllm_load_vocab: special tokens cache size = 0\r\nllm_load_vocab: token to piece cache size = 0.2985 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = codeshell\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 70144\r\nllm_load_print_meta: n_merges         = 72075\r\nllm_load_print_meta: n_ctx_train      = 8192\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 42\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 1.0e-05\r\nllm_load_print_meta: f_norm_rms_eps   = 0.0e+00\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 16384\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 8192\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 0.1B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 7.98 B\r\nllm_load_print_meta: model size       = 4.25 GiB (4.58 BPW)\r\nllm_load_print_meta: general.name     = CodeShell\r\nllm_load_print_meta: BOS token        = 70000 '<|endoftext|>'\r\nllm_load_print_meta: EOS token        = 70000 '<|endoftext|>'\r\nllm_load_print_meta: UNK token        = 70000 '<|endoftext|>'\r\nllm_load_print_meta: PAD token        = 70000 '<|endoftext|>'\r\nllm_load_print_meta: LF token         = 28544 '\u00c4\u012c'\r\nllm_load_print_meta: PRE token        = 70001 '<fim_prefix>'\r\nllm_load_print_meta: SUF token        = 70003 '<fim_suffix>'\r\nllm_load_print_meta: MID token        = 70002 '<fim_middle>'\r\nllm_load_print_meta: EOT token        = 70000 '<|endoftext|>'\r\nllm_load_print_meta: max token length = 256\r\nllm_load_tensors: ggml ctx size =    0.45 MiB\r\nggml_backend_metal_log_allocated_size: allocated buffer, size =  4201.36 MiB, ( 4201.44 / 12288.02)\r\nllm_load_tensors: offloading 42 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 43/43 layers to GPU\r\nllm_load_tensors:      Metal buffer size =  4201.35 MiB\r\nllm_load_tensors:        CPU buffer size =   154.12 MiB\r\n.............................................................................................\r\nllama_new_context_with_model: n_ctx      = 8192\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nggml_metal_init: allocating\r\nggml_metal_init: found device: Apple M3 Pro\r\nggml_metal_init: picking default device: Apple M3 Pro\r\nggml_metal_init: default.metallib not found, loading from source\r\nggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\r\nggml_metal_init: loading '/Users/kido/Code/githubs/llama.cpp/ggml-metal.metal'\r\nggml_metal_init: GPU name:   Apple M3 Pro\r\nggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\r\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\r\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\r\nggml_metal_init: simdgroup reduction support   = true\r\nggml_metal_init: simdgroup matrix mul. support = true\r\nggml_metal_init: hasUnifiedMemory              = true\r\nggml_metal_init: recommendedMaxWorkingSetSize  = 12884.92 MB\r\nllama_kv_cache_init:      Metal KV buffer size =  1344.00 MiB\r\nllama_new_context_with_model: KV self size  = 1344.00 MiB, K (f16):  672.00 MiB, V (f16):  672.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.27 MiB\r\nllama_new_context_with_model:      Metal compute buffer size =   564.00 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =    24.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1687\r\nllama_new_context_with_model: graph splits = 2\r\n\r\nsystem_info: n_threads = 5 / 11 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 |\r\n[1]    34950 segmentation fault  ./llama-infill -m ../../models/Publisher/Repository/codeshell_modified.gguf\r\n```\n```\n",
    "labels": [
      "bug-unconfirmed",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-06-28T01:16:16+00:00",
    "closed_at": "2024-07-08T06:34:36+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8179/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8179"
  },
  {
    "number": 7652,
    "title": "Bug: DeepSeek-V2-Lite GGML_ASSERT: ggml-metal.m:1857: dst_rows <= 2048 and aborts",
    "body": "### What happened?\r\n\r\n`~/projects/llama.gguf/main -c 4096 -ngl 99 -f ./reproduce.txt -m ~/Downloads/DeepSeek-V2-Lite-Chat.Q5_K.gguf`\r\n\r\n[reproduce.txt](https://github.com/ggerganov/llama.cpp/files/15503974/reproduce.txt)\r\n\r\nThe DS2 Lite quant was downloaded from https://huggingface.co/legraphista/DeepSeek-V2-Lite-Chat-IMat-GGUF\r\n\r\n\r\nError:\r\n\r\n```\r\nGGML_ASSERT: ggml-metal.m:1857: dst_rows <= 2048\r\nGGML_ASSERT: ggml-metal.m:1857: dst_rows <= 2048\r\nGGML_ASSERT: ggml-metal.m:1857: dst_rows <= 2048\r\n[1]    70124 abort      ~/projects/llama.gguf/main -c 4096 -ngl 99 -f ./reproduce.txt -m\r\n```\r\n\r\nProcess exited without generating the output.\r\n\r\nIf it helps, I ran the same thing with lldb:\r\n\r\n```\r\nAssistant: GGML_ASSERT: ggml-metal.m:1857: dst_rows <= 2048\r\nGGML_ASSERT: ggml-metal.m:1857: dst_rows <= 2048\r\nGGML_ASSERT: ggml-metal.m:1857: dst_rows <= 2048\r\nGGML_ASSERT: ggml-metal.m:1857: dst_rows <= 2048\r\nProcess 72593 stopped\r\n* thread #4, queue = 'ggml-metal', stop reason = signal SIGABRT\r\n    frame #0: 0x000000019378aa60 libsystem_kernel.dylib`__pthread_kill + 8\r\nlibsystem_kernel.dylib`:\r\n->  0x19378aa60 <+8>:  b.lo   0x19378aa80               ; <+40>\r\n    0x19378aa64 <+12>: pacibsp\r\n    0x19378aa68 <+16>: stp    x29, x30, [sp, #-0x10]!\r\n    0x19378aa6c <+20>: mov    x29, sp\r\nTarget 0: (main) stopped.\r\n(lldb) bt\r\n* thread #4, queue = 'ggml-metal', stop reason = signal SIGABRT\r\n  * frame #0: 0x000000019378aa60 libsystem_kernel.dylib`__pthread_kill + 8\r\n    frame #1: 0x00000001937c2c20 libsystem_pthread.dylib`pthread_kill + 288\r\n    frame #2: 0x00000001936cfa20 libsystem_c.dylib`abort + 180\r\n    frame #3: 0x00000001002719ac main`__ggml_metal_graph_compute_block_invoke(.block_descriptor=0x00006000010f40a0, iter=3) at ggml-metal.m:1857:25\r\n    frame #4: 0x0000000193612428 libdispatch.dylib`_dispatch_client_callout2 + 20\r\n    frame #5: 0x0000000193626850 libdispatch.dylib`_dispatch_apply_invoke3 + 336\r\n    frame #6: 0x00000001936123e8 libdispatch.dylib`_dispatch_client_callout + 20\r\n    frame #7: 0x0000000193613c68 libdispatch.dylib`_dispatch_once_callout + 32\r\n    frame #8: 0x00000001936259b8 libdispatch.dylib`_dispatch_apply_redirect_invoke + 260\r\n    frame #9: 0x00000001936123e8 libdispatch.dylib`_dispatch_client_callout + 20\r\n    frame #10: 0x0000000193624080 libdispatch.dylib`_dispatch_root_queue_drain + 864\r\n    frame #11: 0x00000001936246b8 libdispatch.dylib`_dispatch_worker_thread2 + 156\r\n    frame #12: 0x00000001937befd0 libsystem_pthread.dylib`_pthread_wqthread + 228\r\n```\r\n\r\n\r\n#### Reproduction notes:\r\n\r\n- No problems when using CPU with `-ngl 0`\r\n- Problem persists with reduced context size, eg. `-c 2000`\r\n- With the same ds2 model, there are inputs (prompts) that don't trigger the abort, even when using the same template. \r\n- Reproduces reliably with the specific prompt file.\r\n\r\n### Name and Version\r\n\r\n```\r\nversion: 3047 (e6157f94)\r\nbuilt with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin23.4.0\r\n```\r\n\r\n- Hardware: Mac Studio Apple M2 Max 96 GB\r\n- OS: ProductVersion:         14.4.1 BuildVersion:           23E224\r\n- Build command: \"make clean && make -j 10\"\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nMac\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n[1717093113] Log start\r\n[1717093113] Cmd: /Users/sidney_fong/projects/llama.gguf/main -c 4096 -ngl 99 -f ./reproduce.txt -m /Users/sidney_fong/Downloads/DeepSeek-V2-Lite-Chat.Q5_K.gguf\r\n[1717093113] main: build = 3047 (e6157f94)\r\n[1717093113] main: built with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin23.4.0\r\n[1717093113] main: seed  = 1717093113\r\n[1717093113] main: llama backend init\r\n[1717093113] main: load the model and apply lora adapter, if any\r\n[1717093113] llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /Users/sidney_fong/Downloads/DeepSeek-V2-Lite-Chat.Q5_K.gguf (version GGUF V3 (latest))\r\n[1717093113] llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\n[1717093113] llama_model_loader: - kv   0:                       general.architecture str              = deepseek2\r\n[1717093113] llama_model_loader: - kv   1:                               general.name str              = DeepSeek-V2-Lite-Chat\r\n[1717093113] llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27\r\n[1717093113] llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840\r\n[1717093113] llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048\r\n[1717093113] llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944\r\n[1717093113] llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16\r\n[1717093113] llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16\r\n[1717093113] llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000\r\n[1717093113] llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001\r\n[1717093113] llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6\r\n[1717093113] llama_model_loader: - kv  11:                          general.file_type u32              = 17\r\n[1717093113] llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1\r\n[1717093113] llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400\r\n[1717093113] llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512\r\n[1717093113] llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192\r\n[1717093113] llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128\r\n[1717093113] llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408\r\n[1717093113] llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64\r\n[1717093113] llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2\r\n[1717093113] llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000\r\n[1717093113] llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64\r\n[1717093113] llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn\r\n[1717093113] llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000\r\n[1717093113] llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096\r\n[1717093113] llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700\r\n[1717093113] llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2\r\n[1717093113] llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm\r\n[1717093113] llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\n[1717093113] llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\n[1717093113] llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [\"\u0192\u2020 \u0192\u2020\", \"\u0192\u2020 t\", \"\u0192\u2020 a\", \"i n\", \"h e...\r\n[1717093113] llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000\r\n[1717093113] llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001\r\n[1717093113] llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001\r\n[1717093113] llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\r\n[1717093113] llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\r\n[1717093113] llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\r\n[1717093113] llama_model_loader: - kv  37:               general.quantization_version u32              = 2\r\n[1717093113] llama_model_loader: - type  f32:  108 tensors\r\n[1717093113] llama_model_loader: - type q5_1:   14 tensors\r\n[1717093113] llama_model_loader: - type q8_0:   13 tensors\r\n[1717093113] llama_model_loader: - type q5_K:  229 tensors\r\n[1717093113] llama_model_loader: - type q6_K:   13 tensors\r\n[1717093113] llm_load_vocab: special tokens cache size = 2400.\r\n[1717093113] llm_load_print_meta: format           = GGUF V3 (latest)\r\n[1717093113] llm_load_print_meta: arch             = deepseek2\r\n[1717093113] llm_load_print_meta: vocab type       = BPE\r\n[1717093113] llm_load_print_meta: n_vocab          = 102400\r\n[1717093113] llm_load_print_meta: n_merges         = 99757\r\n[1717093113] llm_load_print_meta: n_ctx_train      = 163840\r\n[1717093113] llm_load_print_meta: n_embd           = 2048\r\n[1717093113] llm_load_print_meta: n_head           = 16\r\n[1717093113] llm_load_print_meta: n_head_kv        = 16\r\n[1717093113] llm_load_print_meta: n_layer          = 27\r\n[1717093113] llm_load_print_meta: n_rot            = 64\r\n[1717093113] llm_load_print_meta: n_embd_head_k    = 192\r\n[1717093113] llm_load_print_meta: n_embd_head_v    = 128\r\n[1717093113] llm_load_print_meta: n_gqa            = 1\r\n[1717093113] llm_load_print_meta: n_embd_k_gqa     = 3072\r\n[1717093113] llm_load_print_meta: n_embd_v_gqa     = 2048\r\n[1717093113] llm_load_print_meta: f_norm_eps       = 0.0e+00\r\n[1717093113] llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\n[1717093113] llm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\n[1717093113] llm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\n[1717093113] llm_load_print_meta: f_logit_scale    = 0.0e+00\r\n[1717093113] llm_load_print_meta: n_ff             = 10944\r\n[1717093113] llm_load_print_meta: n_expert         = 64\r\n[1717093113] llm_load_print_meta: n_expert_used    = 6\r\n[1717093113] llm_load_print_meta: causal attn      = 1\r\n[1717093113] llm_load_print_meta: pooling type     = 0\r\n[1717093113] llm_load_print_meta: rope type        = 0\r\n[1717093113] llm_load_print_meta: rope scaling     = yarn\r\n[1717093113] llm_load_print_meta: freq_base_train  = 10000.0\r\n[1717093113] llm_load_print_meta: freq_scale_train = 0.025\r\n[1717093113] llm_load_print_meta: n_yarn_orig_ctx  = 4096\r\n[1717093113] llm_load_print_meta: rope_finetuned   = unknown\r\n[1717093113] llm_load_print_meta: ssm_d_conv       = 0\r\n[1717093113] llm_load_print_meta: ssm_d_inner      = 0\r\n[1717093113] llm_load_print_meta: ssm_d_state      = 0\r\n[1717093113] llm_load_print_meta: ssm_dt_rank      = 0\r\n[1717093113] llm_load_print_meta: model type       = 16B\r\n[1717093113] llm_load_print_meta: model ftype      = Q5_K - Medium\r\n[1717093113] llm_load_print_meta: model params     = 15.71 B\r\n[1717093113] llm_load_print_meta: model size       = 11.03 GiB (6.03 BPW) \r\n[1717093113] llm_load_print_meta: general.name     = DeepSeek-V2-Lite-Chat\r\n[1717093113] llm_load_print_meta: BOS token        = 100000 '<\u00d4\u03a9\u00fabegin\u201a\u00f1\u00c5of\u201a\u00f1\u00c5sentence\u00d4\u03a9\u00fa>'\r\n[1717093113] llm_load_print_meta: EOS token        = 100001 '<\u00d4\u03a9\u00faend\u201a\u00f1\u00c5of\u201a\u00f1\u00c5sentence\u00d4\u03a9\u00fa>'\r\n[1717093113] llm_load_print_meta: PAD token        = 100001 '<\u00d4\u03a9\u00faend\u201a\u00f1\u00c5of\u201a\u00f1\u00c5sentence\u00d4\u03a9\u00fa>'\r\n[1717093113] llm_load_print_meta: LF token         = 126 '\u221a\u00d1'\r\n[1717093113] llm_load_print_meta: n_layer_dense_lead   = 1\r\n[1717093113] llm_load_print_meta: n_lora_q             = 0\r\n[1717093113] llm_load_print_meta: n_lora_kv            = 512\r\n[1717093113] llm_load_print_meta: n_ff_exp             = 1408\r\n[1717093113] llm_load_print_meta: n_expert_shared      = 2\r\n[1717093113] llm_load_print_meta: expert_weights_scale = 1.0\r\n[1717093113] llm_load_print_meta: rope_yarn_log_mul    = 0.0707\r\n[1717093113] llm_load_tensors: ggml ctx size =    0.35 MiB\r\n[1717093113] ggml_backend_metal_log_allocated_size: allocated buffer, size = 11298.50 MiB, (11298.56 / 73728.00)[1717093113] \r\n[1717093113] llm_load_tensors: offloading 27 repeating layers to GPU\r\n[1717093113] llm_load_tensors: offloading non-repeating layers to GPU\r\n[1717093113] llm_load_tensors: offloaded 28/28 layers to GPU\r\n[1717093113] llm_load_tensors:      Metal buffer size = 11298.50 MiB\r\n[1717093113] llm_load_tensors:        CPU buffer size =   137.50 MiB\r\n[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] .[1717093113] \r\n[1717093113] llama_new_context_with_model: n_ctx      = 4096\r\n[1717093113] llama_new_context_with_model: n_batch    = 2048\r\n[1717093113] llama_new_context_with_model: n_ubatch   = 512\r\n[1717093113] llama_new_context_with_model: flash_attn = 0\r\n[1717093113] llama_new_context_with_model: freq_base  = 10000.0\r\n[1717093113] llama_new_context_with_model: freq_scale = 0.025\r\n[1717093113] ggml_metal_init: allocating\r\n[1717093113] ggml_metal_init: found device: Apple M2 Max\r\n[1717093113] ggml_metal_init: picking default device: Apple M2 Max\r\n[1717093113] ggml_metal_init: default.metallib not found, loading from source\r\n[1717093113] ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\r\n[1717093113] ggml_metal_init: loading '/Users/sidney_fong/projects/llama.gguf/ggml-metal.metal'\r\n[1717093113] ggml_metal_init: GPU name:   Apple M2 Max\r\n[1717093113] ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\r\n[1717093113] ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\r\n[1717093113] ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\r\n[1717093113] ggml_metal_init: simdgroup reduction support   = true\r\n[1717093113] ggml_metal_init: simdgroup matrix mul. support = true\r\n[1717093113] ggml_metal_init: hasUnifiedMemory              = true\r\n[1717093113] ggml_metal_init: recommendedMaxWorkingSetSize  = 77309.41 MB\r\n[1717093113] llama_kv_cache_init:      Metal KV buffer size =  1080.00 MiB\r\n[1717093113] llama_new_context_with_model: KV self size  = 1080.00 MiB, K (f16):  648.00 MiB, V (f16):  432.00 MiB\r\n[1717093113] llama_new_context_with_model:        CPU  output buffer size =     0.39 MiB\r\n[1717093113] llama_new_context_with_model:      Metal compute buffer size =   212.00 MiB\r\n[1717093113] llama_new_context_with_model:        CPU compute buffer size =    12.01 MiB\r\n[1717093113] llama_new_context_with_model: graph nodes  = 1924\r\n[1717093113] llama_new_context_with_model: graph splits = 2\r\n[1717093113] warming up the model with an empty run\r\n[1717093113] n_ctx: 4096\r\n[1717093113] \r\n[1717093113] system_info: n_threads = 8 / 12 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \r\n[1717093113] add_bos: 1\r\n[1717093113] tokenize the prompt\r\n[1717093113] prompt: \"User: \u2030\u03a9\u2020\u2030\u00f8\u00c7\u2030\u220f\u00c4\u00c2\u00c4\u00e3\u00c2\u00aa\u00a3\u00ca\u00f9\u00b1\u00cb\u00a9\u00b1\u2030\u222b\u222b\u00c2\u2211\u2022\u00ca\u00f4\u222b\u00cb\u00c9\u03a9\u00ca\u00ae\u00b0\u00c2\u00fb\u00e3\u201e\u00c4\u00c7\u00cb\u00ae\u00f2\u2030\u03a9\u00e8\u00d4\u00ba\u00e5\u00cb\u00b6\u00c5\u00c1\u00ee\u00ae\u00c2\u00aa\u00a3\u00ca\u00f9\u00b1\u00cb\u00a9\u00b1\u00cb\u2122\u00fb\u00ca\u2265\u00ef\u00d4\u00ba\u00e5\u00c2\u00ee\u00ee\u00c2\u2022\u03a9\u00c1\u00ee\u00ae\u00ca\u00f4\u00c6\u00c8\u00c4\u00f6\u00cb\u00a9\u00b1\u00cb\u2122\u00fb\u00ca\u2265\u00ef\u201e\u00c4\u00c7\u2030\u00e6\u00e3\u00c2\u00b6\u00c7\u00d4\u00ba\u00e5\u00c2\u00aa\u00a3\u00ca\u00f9\u00b1\u00cb\u00a9\u00b1\u00c2\u00d6\u2022\u00c8\u00f9\u00a2\u00c2\u00f2\u00d6\u201e\u00c4\u00e5\u00c1\u00f6\u00d1\u201e\u00c4\u00e7\u00ca\u00e1\u00e2\u00c2\u00d8\u00b4\u00ca\u00e0\u00ea\u201e\u00c4\u00e5\u00c2\u00f2\u00d6\u201e\u00c4\u00e7\u00d4\u00ba\u00e5\u201e\u00c4\u00e5\u00ca\u00f2\u00d8\u201e\u00c4\u00e7\u00ca\u00e1\u00e2\u00c2\u00d8\u00b4\u00ca\u00e0\u00ea\u201e\u00c4\u00e5\u2030\u00f8\u00c7\u201e\u00c4\u00e7\u201e\u00c4\u00c7\r\n\r\n\u00cb\u00b4\u00e3\u00c1\u00ee\u00ae\u2030\u220f\u00e3\u00c8\u00f9\u00a2\u00ca\u00e8\u00ea\u2030\u00e6\u00f5\u00c2\u00f2\u00d6\u00c2\u00aa\u00a3\u00ca\u00f9\u00b1\u00cb\u00a9\u00b1\u00cb\u00a9\u00fb\u00cb\u2122\u00fb\u00c8\u00c4\u2020\u00c2\u00e8\u2022\u201e\u00c4\u00c7\r\nWith use the provided Cantonese words to make sentences.\r\n\r\n# \u00cb\u00a9\u00fb\u00cb\u2122\u00fb\r\n\r\n1. \u00c8\u00a3\u2264\u00cb\u00e5\u2202 (\u00c2\u00e9\u00aa\u00cb\u00e5\u2202\u00ca\u00ae\u00ec\u00c8\u00a3\u00fc\u00c2\u00f2\u00a2\u00d4\u00ba\u00e5\u00c8\u00c4\u00f6\u00c2\u220f\u220f\u00c2\u00e8\u2122\u2030\u00f8\u00c7\u00ca\u00e5\u00e1\u00c2\u00f1\u222b\u00ca\u00f3\u00a9\u00c2\u220f\u00c7\u00ca\u00e0\u00f1\u00cb\u00c4\u00d6\u00c2\u00e7\u00e0\u00c2\u220f\u00c7\u00c2\u00f2\u00d6\u00ca\u00f4\u00c7\u00c2\u00c4\u00f4\u00c8\u00a3\u00fc\u00c8\u00aa\u00fb\u00c2\u00f8\u00c9)\r\n2. \u00c2\u00fa\u00f1\u00ca\u00f5\u220f\u00c8\u00a7\u00ae (\u2030\u220f\u00c4\u00c2\u00c4\u00e3\u00c2\u00fa\u221e\u00ca\u00f1\u03c0\u00d4\u00ba\u00e5\u2030\u222b\u222b\u00c2\u00ec\u00e3\u00c2\u00e8\u00d8\u2030\u00aa\u2022\u00c2\u00e9\u00aa\u00c2\u222b\u00b6\u00c2\u00c4\u00fc\u00ca\u00f5\u220f\u00ca\u00e0\u00f1\u00cb\u00c4\u00d6\u00c1\u00f9\u00e1\u00ca\u00f5\u220f)\r\n3. \u00c8\u00a7\u00e4\u00cb\u00c5\u2264 (\u2030\u00f8\u00f9\u00c8\u00a7\u00e4\u00cb\u00c5\u2264\u00c1\u2211\u00f6\u201e\u00c4\u00c5\u00cb\u00c5\u2264\u00c2\u220f\u2202\u00d4\u00ba\u00e5\u2030\u00aa\u00a7\u00c2\u00e0\u221e\u00c2\u00ee\u00ee\u00ca\u00fa\u00c9\u00c8\u00c5\u00e9\u2030\u00aa\u03a9\u00c2\u00e3\u00fb\u00ca\u00ea\u00e7\u00d4\u00ba\u00e5\u00ca\u00e0\u00f1\u00cb\u00c4\u00d6\u00c2\u00e3\u00fb\u00ca\u00ea\u00e7\u2030\u03c0\u00e3\u00c2\u00e6\u00e5\u00c2\u222b\u2211\u00c2\u00e6\u00a9)\r\n4. \u00c1\u2022\u00ae\u00c2\u221e\u00e6 (\u00c2\u00f1\u222b\u00c2\u00ba\u00b5#\u00c8\u00a3\u00f5 \u00c2\u222b\u00b6\u00ca\u00ea\u00a3\u00cb\u00ea\u03a9\u00c2\u00f6\u00fc\u00c1\u00ef\u00f4\u00cb\u00f8\u00ee\u00c1\u00ef\u00c4\u00ca\u00e5\u00c5\u00ca\u00fa\u00e2\u00cb\u00c4\u00d6\u00c2\u00c5\u00f6#\u00ca\u00ee\u2202\u00ca\u00ec\u00f6 \u00c2\u00f2\u00d6\u00c2\u00f3\u221e\u00c8\u00c9\u00ae\u2030\u00aa\u03a9)\r\n5. hacker (#hack \u00c2\u00d6\u2022\u2030\u222b\u222b\u00c2\u00ec\u00e3\u00c8\u00c9\u00ae\u00c8\u00f5\u00aa\u00cb\u00d6\u00b6\u00c2\u00f2\u00d6\u2030\u222b\u222b)\r\n6. \u00c1\u00fc\u2265\u00ca\u00f1\u00f5\u00cb\u00f2\u2260 (\u00ca\u00a7\u00e7\u00c1\u00e2\u00a9\u00c2\u00ea\u00e7\u00d4\u00ba\u00f5\u2030\u220f\u00c4\u00c1\u00ae\u00c6\u00cb\u00f2\u2260\u00cb\u00e4\u00b1\u00d4\u00ba\u00e5\u00c1\u00e2\u03c0\u00c8\u00aa\u00fb\u2030\u00f8\u00c7\u00ca\u00fa\u00c9\u2030\u00e6\u00f9\u00c8\u00f4\u00d1\u00c2\u00f1\u222b\u00ca\u00ae\u03c0\u2030\u220f\u00e4\u00c8\u00f9\u00a2\u00c1\u00ee\u00fc\u00c8\u00ef\u2211)\r\n7. \u00c2\u00ea\u00e4\u00cb\u00a9\u2260 (#\u00c1\u00fc\u00f5\u00c1\u00f5\u00e6\u201e\u00c4\u00c5#\u00ca\u00c7\u00f1\u00cb\u00b4\u00f1)\r\n8. \u00c2\u00ea\u00e4\u00cb\u00a9\u2260 (\u2030\u00ba\u00ba\u00c8\u00f9\u00fb\u00cb\u00c4\u00e5\u00ca\u00f2\u00d8\u201e\u00c4\u00c5\u00cb\u00e7\u00ed\u00cb\u00a8\u00a8)\r\n9. \u00c2\u00ea\u00d1\u00c1\u00c7\u222b\u00c2\u00d6\u2202\u2030\u220f\u00aa (\u00c2\u00ea\u00d1\u00cb\u00e1\u2122\u00c1\u00c7\u222b\u00c2\u00ed\u00f3\u2030\u220f\u00aa\u00c2\u2260\u00ea\u201e\u00c4\u00c5\u2030\u220f\u00e4\u00c1\u00a5\u00f6\u2030\u03a9\u00a2\u00c2\u00ec\u00e3\u00c2\u00f2\u00d6\u00c2\u00e0\u00a9\u00c1\u00f5\u00e4\u00d4\u00ba\u00e5\u00c2\u00f2\u00d6\u2030\u03a9\u00a2\u00c2\u00ec\u00e3\u00ca\u00e5\u00e1\u00c1\u00a7\u222b\u2030\u220f\u00e3\u00d4\u00ba\u00e5\u00ca\u00fa\u2122\u00c2\u00d6\u00e7\u00c2\u00ea\u00e5\u00c2\u221e\u00e7\u00ca\u00f1\u03c0\u00c1\u00c7\u222b\u00ca\u00ef\u00b5)\r\n10. \u00cb\u00aa\u00ae\u00cb\u00ec\u00e3 (\u00c2\u00e7\u2265\u2030\u00f8\u00c7#\u00c2\u00eb\u00ee\u00cb\u00aa\u00ae)\r\n11. \u00ca\u00e2\u00ec\u00c2\u00ec\u2022\u00c2\u00a7\u00b4 (\u00ca\u00e2\u00ec#\u00c8\u00b4\u00f2\u00c1\u00e0\u00e6\u00c2\u00a7\u00b4\u00c1\u00ea\u00c9)\r\n12. \u00c2\u00d6\u00e7\u00ca\u00e8\u00c4 (\u00ca\u00e5\u00e1\u00ca\u00e2\u00c4\u00c2\u00ee\u00c6\u00cb\u2265\u00a3\u00c2\u00f2\u00d6\u00cb\u2264\u00ae\u00c2\u00ec\u00c5\u00c2\u00ee\u00ee\u00c1\u00ef\u00c4\u2030\u222b\u222b\u00ca\u00e8\u00c4)\r\n13. \u00ca\u00ec\u222b\u00c8\u00f3\u00e4\u2030\u03a9\u00a8 (\u00c2\u00b1\u00ef\u00c1\u00e8\u00e6\u00c8\u00f3\u00e4\u00ca\u221e\u00a3\u00d4\u00ba\u00e5\u00ca\u00ec\u222b\u00c2\u00e1\u222b\u00cb\u2264\u00b0\u00c2\u00e4\u00f5\u00c8\u00f5\u00d1\u00c2\u00e9\u00f6\u00c2\u00f2\u00d6\u00c2\u00df\u00f8\u00ca\u00d6\u00e3)\r\n14. \u00c2\u00aa\u00a3\u00ca\u00ed\u2260\u00c2\u00e4\u00e1 (\u00c2\u00f1\u222b\u00ca\u00ee\u2202\u00c8\u00fc\u2265\u00ca\u00a9\u00fc\u00c2\u00c6\u00f6\u00ca\u00f4\u00c7\u00ca\u00ed\u2260\u00ca\u00ee\u00e6\u00c2\u00f2\u00d6\u00c8\u00c4\u00a3\u00c1\u222b\u00e5\u00c2\u00e4\u00e1)\r\n15. \u00c8\u00e5\u00d1\u00c2\u03a9\u00b1\u00ca\u00a9\u00fc (#\u00c8\u00e5\u00d1\u00c2\u03a9\u00b1 \u00c2\u00f2\u00d6\u00ca\u00a9\u00fc\u00c2\u00f4\u00ae\u00d4\u00ba\u00e0\u00c8\u00e1\u00e8\u00cb\u00a9\u00fb\u00d4\u00ba\u00f6\u00c8\u00c9\u00ae\u00d4\u00ba\u00e2)\r\n16. \u00c2\u00ea\u00fc\u00cb\u00a9\u00a9\u00c2\u00ea\u00fc\u00c2\u00ee\u00ee\u00c1\u00ee\u00a9 (\u00cb\u00b0\u00ae\u00c1\u00a7\u222b\u00cb\u2260\u00e2\u00ca\u00ec\u00f6\u00c1\u00a2\u222b\u00c8\u00eb\u00f8\u00d4\u00ba\u00e5\u00c1\u00d1\u00b0\u00ca\u2265\u00ef\u00ca\u00e4\u00b5\u00cb\u2265\u00a5)\r\n17. \u2030\u03a9\u00e0\u00c8\u00c5\u00ec\u00c2\u00a7\u00df\u00ca\u00fa\u00c9 (\u00c2\u00c6\u00f3\u00ca\u00ef\u00f4\u00c2\u00f2\u00d6\u00c2\u00c7\u2265\u00c8\u00c5\u00ec\u00c2\u00a7\u00df\u00ca\u00fa\u00c9)\r\n18. \u00c2\u00d8\u00b4\u00ca\u00e2\u00e3 (#\u00c2\u00d8\u00b4\u2030\u03a9\u00fa \u00c2\u00f2\u00d6\u2030\u222b\u222b\u201e\u00c4\u00c7)\r\n19. \u00c2\u00d8\u00b4\u00ca\u00e2\u00e3 (\u2030\u00aa\u00a3\u00ca\u00f5\u00f8\u2030\u00aa\u00f1\u2030\u222b\u222b\u00c2\u00d8\u00b4\u2030\u03a9\u00fa\u00c2\u00f2\u00d6\u2030\u222b\u222b\u201e\u00c4\u00c7)\r\n20. \u2030\u00aa\u00a3\u00ca\u00e2\u00ec (#\u2030\u00aa\u00a3\u00ca\u00f5\u00f8 \u00c2\u00e8\u00b6\u2030\u220f\u00c4\u2030\u222b\u222b#\u00ca\u00e2\u00ec\u00ca\u00a9\u00fc\u201e\u00c4\u00c7)\r\n21. \u00cb\u00e5\u2202\u00c1\u00a2\u00f3 (\u00c1\u00ee\u00ae\u00c2\u00f6\u00fc\u00c8\u00a3\u2264\u00cb\u00e5\u2202\u00c2\u00f2\u00d6\u00c1\u00a2\u00f3\u00d4\u00ba\u00e5\u00c8\u00c4\u00f6\u00c2\u220f\u220f\u00ca\u00fa\u00c9\u00c1\u00ea\u00dc\u00cb\u00df\u00a3\u00c1\u00c7\u222b\u00c1\u00ee\u00b1\u00c8\u00f4\u2202\u00c1\u00ec\u2211\u00c1\u00e1\u00ed\u00cb\u00a3\u03a9\u00cb\u00c4\u00e5\u00ca\u00e0\u00ea\u00c2\u00f2\u00d6\u00ca\u00f3\u2022\u00ca\u00fa\u00a8\u00cb\u00e5\u2202\u00c8\u00c5\u00ec\u00c8\u00c5\u00ec\u00c2\u00d6\u2211\u201e\u00c4\u00c7)\r\n22. \u00cb\u00e5\u2202\u00c8\u00f6\u00ee (\u00c2\u221e\u00e1\u00cb\u00e5\u2202\u00cb\u00eb\u00e2\u00c2\u00ea\u00e5\u00cb\u00e5\u2202\u00ca\u03c0\u00d8\u00c2\u00e0\u00dc\u00c8\u00f1\u00e3\u00c2\u00f2\u00d6\u00c8\u00c5\u00ec\u00c2\u00d6\u2211\u00d4\u00ba\u00e5\u00c2\u00a7\u00df\u00ca\u00b6\u00c7\u00ca\u00fa\u00e2\u00c2\u00d6\u00a9\u00c1\u00ae\u00c6\u00c1\u00ee\u00ae\u00ca\u2265\u00ef\u00d4\u00ba\u00e5\u2030\u220f\u00c4\u00c1\u00ae\u00c6\u2030\u00f8\u00c7\u00c2\u00e8\u00d8\u2030\u00aa\u2022\u00cb\u00a3\u00f9\u2030\u03a9\u00e8\u00cb\u00e5\u2202\u00cb\u00eb\u00e2\u00d4\u00ba\u00e5\u00ca\u00ee\u00e6\u00c2\u00d6\u2022\u00ca\u2264\u00f1\u00ca\u2265\u00b0\u00c2\u00f4\u00ae\u00c2\u00d6\u2211\u00c2\u00d6\u2022\u00c8\u00f9\u00a2\u00d4\u00ba\u00e5\u00c2\u00e0\u221e\u00cb\u00e5\u2202\u00ca\u03c0\u00d8\u00ca\u2264\u00f1\u00c2\u2022\u03a9\u00c2\u00f2\u00d6\u00ca\u00f4\u00c7\u00c2\u00c4\u00f4\u00c2\u221e\u00b1\u00c2\u00e8\u00d8\u2030\u00aa\u2022\u00ca\u00e3\u00e9\u00cb\u00b5\u2211\u00c2\u00f2\u00d6\u00d4\u00ba\u00f5\u00cb\u00c4\u00e5\u00c2\u00e8\u00b6\u2030\u220f\u00c4\u00c1\u00ae\u00c6\u00c2\u221e\u00b1\u2030\u00f8\u00c7\u00c2\u221e\u00b1\u00c2\u00ed\u00c5\u2030\u220f\u00c4\u00c2\u00c4\u00e3\u00c1\u00d8\u00a9\u00ca\u00e0\u00f1\u00cb\u00c4\u00d6\u00c1\u2202\u2264\u00d4\u00ba\u00e5\u2030\u00f8\u00e6\u2030\u222b\u222b\u00ca\u00ec\u222b\u00c2\u00f1\u222b\u00cb\u00e5\u2202\u00ca\u00f9\u00d8\u00ca\u00e0\u00f1\u00cb\u00c4\u00d6\u00cb\u00e5\u2202\u00c2\u00a3\u222b\u2030\u220f\u00e4\u00c8\u00f9\u00a2\u00d4\u00ba\u00e5\u00c2\u00f1\u222b\u00ca\u00f1\u00fc\u00cb\u00e5\u2202\u00c2\u00f2\u00d6\u00ca\u00f4\u00c7\u00c2\u00c4\u00f4\u00c2\u00d6\u00e0\u00c2\u221e\u00e1\u00cb\u00e5\u2202\u00cb\u00eb\u00e2\u00c2\u00ea\u00e5\u00cb\u00e5\u2202\u00ca\u03c0\u00d8\u00c2\u00e0\u00dc\u00c8\u00f5\u00a2\u201e\u00c4\u00c7)\r\n23. \u00c2\u00e8\u2122\u00cb\u00ae\u00b1\u00c2\u2211\u00fb\u00c2\u00c6\u00f2\u00ca\u00ee\u00e6\u00c1\u00c5\u00b4\u00d4\u00ba\u00e5\u2030\u220f\u00e7\u00c2\u00e1\u00dc\u00c1\u00f4\u00e6\u00c2\u00df\u00ec\u00c8\u00aa\u00fb\u00c1\u00e1\u00e0 (\u00ca\u00d8\u00ee\u00c2\u00f1\u00aa\u00ca\u00a8\u00e4\u00cb\u2264\u00a5\u00c8\u00f6\u00e9\u00c2\u00b1\u00a7\u00c2\u00e8\u00d8\u2030\u00aa\u2022\u00c1\u00c7\u222b\u00ca\u00e2\u00c4\u00ca\u00a8\u2264\u00c1\u00c7\u222b\u00d4\u00ba\u00e5\u00cb\u00c4\u00c5\u00c1\u00f4\u00e6\u00c2\u00df\u00ec\u00c2\u00f2\u00d6\u00ca\u2260\u00a3\u00c1\u00ef\u2202\u00cb\u00ae\u00c4\u00cb\u00b0\u00e5\u00c2\u221e\u00b1\u00c2\u00e8\u00f3\u00c2\u00e0\u221e\u00c1\u00ae\u00c6\u00c1\u00ae\u00c6\u00c8\u00f4\u00ea\u00c2\u00e0\u2202\u00d4\u00ba\u00f5\u00c2\u00e8\u00e0\u00ca\u00e0\u00f1\u2030\u220f\u00c4\u00c2\u00c4\u00e3\u2030\u222b\u222b\u00cb\u00ae\u00c4\u00cb\u00b0\u00e5\u00c8\u00f5\u00f4\u00c8\u00e1\u00e7\u00ca\u00ae\u00f4\u00ca\u222b\u00f1\u201e\u00c4\u00c7\u00ca\u222b\u00ea\u00ca\u00f1\u00ba\u00c2\u00c6\u00e3\u00ca\u00fa\u00f9\u2030\u220f\u00c4\u00c2\u00c4\u00e3\u00c2\u2211\u00fb\u00c8\u00ef\u2211\u00c2\u00e8\u00b4\u00c1\u00ee\u221e\u00c1\u00f4\u00aa\u00d4\u00ba\u00e5\u00cb\u00b6\u00c5\u00ca\u00b1\u00c7\u00c1\u00f4\u00e6\u00c2\u00df\u00ec\u00c8\u00c5\u00f8\u00cb\u00b4\u00b1\u00c2\u00ee\u00ee\u00c2\u00e1\u00dc\u00c1\u00ee\u00ae\u201e\u00c4\u00e5\u00c1\u00f4\u00aa\u201e\u00c4\u00e7\u00c2\u2260\u00f3\u00c2\u00ea\u00e5\u00c8\u00fc\u2265\u00c2\u00f2\u00d6\u00c2\u2260\u00f3\u00d4\u00ba\u00e5\u00ca\u00f1\u00ba\u00ca\u00f2\u00d8\u00c2\u00a7\u00df\u00c2\u00c6\u2202\u00c2\u221e\u00b1\u00c2\u221e\u00e1\u201e\u00c4\u00e5\u00c1\u00e1\u00e0\u201e\u00c4\u00e7\u00ca\u00ee\u03c0\u00c1\u00ae\u00b1\u00c1\u00c7\u222b\u201e\u00c4\u00e5\u00c1\u00c5\u00b4\u201e\u00c4\u00e7\u201e\u00c4\u00c7\u00c2\u00e0\u221e\u00c2\u00ed\u00f3\u00c2\u00d6\u00c9\u00c2\u00c6\u00b5\u00c1\u00d8\u00c4\u00c2\u00f2\u00d6\u00ca\u00f4\u00c7\u00c2\u00c4\u00f4\u00d4\u00ba\u00e5\u00ca\u00ee\u00f8\u00c2\u222b\u00fa\u00cb\u2264\u00ba\u00c2\u00e1\u222b\u00c2\u00eb\u00e4\u00c1\u00a7\u222b\u00d4\u00ba\u00f6\u201e\u00c4\u00e5\u00ca\u00fa\u00a8\u00c2\u2211\u00fb\u2030\u00e6\u00f9\u2030\u00e6\u00e3\u00ca\u00ee\u00e6\u00c1\u00c5\u00b4\u2030\u220f\u00e2\u00ca\u00f3\u2022\u201e\u00c4\u00e7\u201e\u00c4\u00c7)\r\n24. \u00c1\u00e7\u00ae\u00c2\u00ee\u00b1 (\u2030\u220f\u00c4\u00c2\u00c4\u00e3\u2030\u222b\u222b\u00c2\u00ee\u00b1\u00d4\u00ba\u00e5\u00c2\u00ee\u00ee\u2030\u00f8\u00c7\u00c2\u00ea\u00e5\u2030\u222b\u222b\u2030\u220f\u00c4\u00c8\u03a9\u00e4\u00c2\u00ee\u00b1)\r\n25. \u00c2\u00f8\u00b4\u2030\u222b\u222b\u00c2\u00f8\u00b4\u2030\u222b\u00e3 (\u00c1\u00e0\u03a9\u00c2\u00f8\u00b4\u2030\u222b\u222b\u00c2\u00c5\u00f6\u00c2\u00f2\u00a2\u00c1\u00e0\u03a9\u00c2\u00f8\u00b4)\r\n26. \u00c2\u00f5\u00f5\u00cb\u00d6\u2265\u00c1\u00e7\u220f (\u00ca\u00e5\u00e1\u00c2\u00d6\u00a9\u00c2\u00c4\u00e3\u2030\u222b\u222b\u00c2\u00f1\u222b\u00c2\u00d6\u00a8\u00c2\u00aa\u00c5\u00c2\u00d6\u2022\u00c8\u00f9\u00a2\u00cb\u00b6\u2122\u00c1\u00dc\u00b1\u00c1\u00ee\u00f6\u00cb\u00e1\u2265\u00c2\u00c5\u00f6\u00ca\u00d1\u00f5\u201e\u00c4\u00c7\u00c2\u00f1\u222b\u00c2\u00aa\u00c5\u00ca\u00e2\u00c4\u00c2\u00a7\u00f1\u00c8\u00f9\u00a2\u00c8\u00f1\u00c4\u00c1\u220f\u00b4\u00ca\u00fa\u00f5\u00cb\u00ea\u03a9\u00c2\u00e9\u00aa\u00d4\u00ba\u00e5\u00ca\u00fa\u00c9\u00cb\u00b6\u00e3\u00c2\u00e0\u221e\u00c2\u00f5\u00f5\u00c8\u00f6\u00aa\u00cb\u00d6\u2265\u00d4\u00ba\u00e5\u00c2\u00f5\u2020\u00ca\u2260\u00a7\u00c1\u00ae\u00b1\u00c1\u00c7\u222b\u00c2\u00f5\u00f5\u00cb\u00d6\u2265\u00c1\u00e7\u220f\u201e\u00c4\u00c7)\r\n27. \u00c1\u00d1\u00b0\u00c1\u00b1\u2265\u00c1\u2264\u2022 (\u00c2\u00e0\u00f9\u00ca\u2260\u2022\u00c2\u00f2\u00d6\u00ca\u00a5\u03a9\u00cb\u00b4\u00e1\u201e\u00c4\u00c7\u00c2\u00f5\u2020\u00c1\u00c7\u222b\u2030\u222b\u00e3\u00ca\u00c9\u00d6\u00ca\u00e2\u00c4\u00ca\u2202\u00e2\u00c2\u00f2\u00d6\u00c8\u00e1\u00e7\u00cb\u00b6\u00c5\u00ca\u00a2\u00f9\u2030\u00aa\u2202\u00c2\u00ea\u00e5\u00c1\u00a5\u221e\u00c1\u00d8\u00c4\u00c8\u00c9\u03a9\u00ca\u00fa\u2122\u00ca\u00fa\u00e2\u00cb\u00eb\u00f3\u00cb\u00ea\u03a9\u00d4\u00ba\u00e5\u00ca\u00e2\u00c4\u2030\u00aa\u2022\u00c1\u00c7\u222b\u00cb\u00a8\u03c0\u00ca\u00d6\u00e9\u00cb\u00ae\u00e0\u00d4\u00ba\u00e5\u00c2\u00e8\u2122\u00ca\u00f2\u00d8\u00cb\u00a9\u00b1\u201e\u00c4\u00e5\u00c1\u00d6\u2264\u00c1\u00d1\u00b0\u00c1\u00b1\u2265\u00c1\u2264\u2022\u201e\u00c4\u00e7\u00d4\u00ba\u00e5\u2030\u00aa\u2022\u00c2\u00d6\u00e7\u2030\u00aa\u00f1\u2030\u222b\u222b\u00ca\u00fa\u00fc\u00ca\u00fa\u00f5\u00c8\u00c5\u00e9\u00c8\u00b4\u00f2\u201e\u00c4\u00c7)\r\n28. \u00c1\u00e6\u00a9\u00ca\u00fa\u00c9 (\u00cb\u00e0\u00e4\u00ca\u00f4\u00c7\u2030\u03a9\u00e9\u2030\u220f\u00e3\u00c8\u00f6\u00e9\u00c2\u00b1\u00a7\u00c2\u00f2\u00d6\u00c2\u00c4\u00fc\u00cb\u2264\u220f\u00ca\u00e0\u00f1\u00cb\u00c4\u00d6\u00c8\u00f5\u00dc\u00cb\u2265\u00e1\u00ca\u00a5\u00aa\u00c2\u00e3\u00ef\u201e\u00c4\u00c7\u00c2\u00f5\u2020\u00c1\u00c7\u222b\u2030\u03a9\u00a2\u00c2\u00fa\u221e\u00c2\u00dc\u00e1\u00ca\u00e4\u00b5\u00ca\u00e4\u00ba\u00ca\u00e0\u00f1\u00cb\u00c4\u00d6\u00ca\u00ec\u00ee\u2030\u00f8\u00f9\u00d4\u00ba\u00e5\u00c8\u00f5\u00a3\u2030\u00aa\u2022\u00c2\u00ea\u00eb\u00c8\u00e4\u00c4\u00cb\u00b0\u00e5\u00c2\u00c4\u00fc\u00c8\u00e5\u00a2\u00d4\u00ba\u00e5\u00ca\u00f1\u00ba\u00ca\u00f2\u00d8\u00c2\u221e\u00b1\u00c8\u00f5\u00dc\u00c2\u00ea\u00e0\u00ca\u00fa\u00e3\u00c2\u00e8\u00e3\u201e\u00c4\u00c5\u00c8\u00d1\u221e\u00c2\u00b1\u00d6\u201e\u00c4\u00c5\u00c2\u00ea\u00e5\u00ca\u2022\u2260\u00c1\u2260\u00e2\u00d4\u00ba\u00e5\u00c2\u221e\u00e1\u00c2\u00a7\u00df\u00c2\u00c6\u2202\u00c2\u00f2\u00d6\u00c8\u00e5\u00a2\u00c2\u00c6\u00f6\u00ca\u00fa\u00fc\u00c8\u00f5\u00dc\u00c2\u00ea\u00e0\u00cb\u00b5\u2211\u00c2\u00f6\u00fc\u2030\u00e6\u00f5\u00ca\u00fa\u00c9\u00d4\u00ba\u00e5\u00c8\u00c7\u00e4\u00c2\u00c4\u00e3\u00ca\u00c4\u2022\u00c8\u00fa\u00c4\u00c1\u00ee\u00ae\u00c8\u00e5\u00a2\u00d4\u00ba\u00e5\u00c2\u221e\u00b1\u00c2\u00e8\u00d8\u2030\u00aa\u2022\u00c1\u00ef\u00c4\u00c2\u00e0\u00a9\u00ca\u00c5\u00d8#\u00ca\u00ae\u00f4\u00ca\u00fa\u00c9\u00d4\u00ba\u00e5\u00ca\u00e3\u00e9\u00c2\u00ed\u00f3\u00c1\u2260\u00dc\u00c8\u00e5\u00a2\u00d4\u00ba\u00e5\u00c1\u00d1\u2202\u00c2\u00e6\u00e5\u00c2\u00e0\u00dc\u00ca\u00fa\u00fc\u00ca\u00fa\u00a8\u00ca\u00c5\u00d8\u00c8\u00c7\u00d1\u00cb\u00f8\u00ee\u00c1\u00ef\u00c4\u00c2\u00d6\u2202\u2030\u00aa\u00f1\u2030\u222b\u222b\u201e\u00c4\u00c7\u00c1\u00ef\u2202\u00ca\u00e2\u00c4\u00ca\u00fa\u00e2\u2030\u222b\u222b\u00c8\u00c9\u03a9\u00ca\u00e4\u00ef\u00c2\u00c6\u00e5\u00ca\u00ae\u00f4\u00d4\u00ba\u00e5\u00c2\u00c4\u00e3\u00ca\u00fa\u00c9\u00c2\u221e\u00b1\u00ca\u00fa\u00c9\u00c1\u00b5\u00ea\u00ca\u00f9\u00fc\u201e\u00c4\u00c7\u00c2\u00e8\u00e0\u00c1\u00ae\u00b1\u00ca\u00ae\u00f4\u00ca\u00fa\u00c9\u201e\u00c4\u00c5\u00c8\u00e4\u00c4\u00ca\u00fa\u00c9\u201e\u00c4\u00c5\u00c2\u00ea\u00e0\u00ca\u00fa\u00c9\u201e\u00c4\u00c7)\r\n29. \u00ca\u00b6\u00fa\u00ca\u00f1\u00e1 (\u00cb\u2264\u00ba\u00c2\u00f1\u222b#\u00ca\u00b6\u00fa \u2030\u220f\u00e4\u00c8\u00f9\u00a2\u00c2\u00f2\u00d6\u00ca\u00f1\u00e1\u00c2\u2260\u00f3)\r\n30. MK\u00c2\u00ea\u00e7 (\u2030\u00f8\u00c7\u00ca\u00e5\u00e1#MK \u00c2\u00f1\u222b\u00c1\u2202\u2264\u2030\u220f\u00e4\u00ca\u00fa\u00c9\u00c1\u00ee\u00ae\u00c2\u00f2\u00d6\u00c2\u00ea\u00e7\u00d4\u00ba\u00e5\u00c8\u00d6\u00e7\u00ca\u00fa\u00e2\u00ca\u00d1\u00e8\u00c2\u00eb\u2265\u2030\u220f\u00e7\u00ca\u00f2\u00e9\u00c2\u00f2\u00d6\u00c1\u00e2\u03c0\u00ca\u00c6\u00e4\u00c1\u00a8\u00b6\u00cb\u00f4\u00fc\u00d4\u00ba\u00e5\u00ca\u00e0\u00f1\u00cb\u00c4\u00d6\u00c8\u00d6\u00e7\u2030\u220f\u00e4\u00c1\u00e2\u03c0\u00c2\u00c6\u00f6\u00c2\u2260\u00f3\u00c8\u2020\u2260\u00c2\u00f2\u00d6\u00c2\u00ea\u00e7\u201e\u00c4\u00c7\u00c1\u00e2\u03c0\u00c2\u00c6\u00f6\u00c2\u2260\u00f3\u00c8\u2020\u2260\u00c2\u00e5\u00d6\u00ca\u00e3\u00a8\u00d4\u00ba\u00f6\u00c2\u2020\u00d6\u00c1\u2265\u00aa\u201e\u00c4\u00c5\u00c8\u00dc\u00e2\u00ca\u00d1\u00f5\u201e\u00c4\u00c5\u00c1\u00b5\u00ef\u00c2\u221e\u00e7\u201e\u00c4\u00c5\u00c2\u00ee\u00d8\u00ca\u00d1\u00f5\u201e\u00c4\u00c5\u00cb\u2211\u2265\u00c2\u00e0\u2202\u201e\u00c4\u00c5\u00cb\u00a8\u00f9\u00c1\u00b5\u00ef\u00c1\u2260\u00e2\u201e\u00c4\u00c7)\r\n\r\n####\r\n\r\n1. \u00c8\u00a3\u2264\u00cb\u00e5\u2202\r\n\r\n\u00c2\u00aa\u00a3\u00ca\u00f9\u00b1\u00cb\u00a9\u00b1\u00d4\u00ba\u00f6\u00ca\u00e0\u00eb\u00c2\u221e\u00e3\u00ca\u00f3\u2022\u00c2\u00ea\u00e5\u00ca\u00fa\u00e3\u00c2\u00e8\u00e3\u00c2\u00e9\u00aa\u00c8\u00a3\u2264\u00cb\u00e5\u2202\u00d4\u00ba\u00e5\u00c2\u00e8\u00b4\u00c2\u00ed\u00f3\u00c2\u2022\u03a9\u00c2\u00a7\u00f6\u00c8\u00aa\u00fb\u00c2\u00f8\u00c9\u201e\u00c4\u00c7\r\nEnglish: I went to yum cha with my friends yesterday and ordered a lot of dim sum.\r\n\r\n2. \u00c2\u00fa\u00f1\u00ca\u00f5\u220f\u00c8\u00a7\u00ae\r\n\r\n\u00c2\u00aa\u00a3\u00ca\u00f9\u00b1\u00cb\u00a9\u00b1\u00d4\u00ba\u00f6\u2030\u03a9\u2020\u00c2\u00b1\u00e3\u2030\u00ba\u00c5\u00c8\u00f4\u00d1\u00cb\u00f8\u00eb\u00ca\u00fa\u00e2\u00c2\u00dc\u00e1\u00c2\u00fa\u00f1\u00ca\u00f5\u220f\u00c8\u00a7\u00ae\u00d4\u00ba\u00fc\r\nEnglish: Is there a library near your home?\r\n\r\n\r\nAssistant: \"\r\n[1717093113] tokens: [ '<\u00d4\u03a9begin\u201aof\u201asentence\u00d4\u03a9>':100000, 'User':5726, ':':25, ' ':207, '\u2030\u03a9\u2020':1372, '\u2030\u00f8':1056, '':211, '\u2030\u220f':505, '\u00c2':1399, '':220, '\u00c2\u00aa':1836, '\u00a3':96, '\u00ca':610, '\u00b1':109, '\u00cb\u00a9':29703, '\u00b1':109, '\u2030\u222b\u222b\u00c2\u2211\u2022\u00ca\u222b\u00cb\u03a9':29680, '\u00ca\u00ae\u00b0\u00c2':39322, '\u201e':398, '\u00cb\u00ae':4564, '':233, '\u2030\u03a9':4222, '\u00d4\u00ba':19304, '\u00cb\u00b6\u00c1\u00ae':57228, '\u00c2\u00aa':1836, '\u00a3':96, '\u00ca':610, '\u00b1':109, '\u00cb\u00a9':29703, '\u00b1':109, '\u00cb\u2122':13072, '':239, '\u00ca\u2265':1511, '\u00d4\u00ba':19304, '\u00c2':3077, '':229, '\u00c2\u2022\u03a9':1250, '\u00c1\u00ae':1082, '\u00ca\u00c6\u00c8':12484, '\u00cb\u00a9':29703, '\u00b1':109, '\u00cb\u2122':13072, '':239, '\u00ca\u2265':1511, '\u201e':398, '\u2030\u00e6\u00c2\u00b6':20332, '\u00d4\u00ba':19304, '\u00c2\u00aa':1836, '\u00a3':96, '\u00ca':610, '\u00b1':109, '\u00cb\u00a9':29703, '\u00b1':109, '\u00c2\u2022':1895, '\u00c8\u00a2':1359, '\u00c2':6418, '':214, '\u201e':17194, '\u00c1':337, '\u201e':17335, '\u00ca':8460, '':218, '\u00c2\u00d8':753, '\u00b4':104, '\u00ca':1114, '\u201e':17194, '\u00c2':6418, '':214, '\u201e':17335, '\u00d4\u00ba':19304, '\u201e':17194, '\u00ca\u00d8':504, '\u201e':17335, '\u00ca':8460, '':218, '\u00c2\u00d8':753, '\u00b4':104, '\u00ca':1114, '\u201e':17194, '\u2030\u00f8':1056, '':211, '\u201e':17335, '\u201e':398, '':185, '':185, '\u00cb':162, '\u00b4':104, '':220, '\u00c1\u00ae':1082, '\u2030\u220f\u00c8\u00a2':16281, '\u00ca\u2030\u00e6':6037, '\u00c2':6418, '':214, '\u00c2\u00aa':1836, '\u00a3':96, '\u00ca':610, '\u00b1':109, '\u00cb\u00a9':29703, '\u00b1':109, '\u00cb\u00a9':29703, '':239, '\u00cb\u2122':13072, '':239, '\u00c8\u2020':2965, '\u00c2\u2022':8260, '\u201e':398, '':185, 'With':3220, ' use':938, ' the':254, ' provided':4286, ' Cant':17162, 'onese':82174, ' words':3073, ' to':276, ' make':1099, ' sentences':12444, '.':13, '':185, '':185, '#':2, ' ':207, '\u00cb\u00a9':29703, '':239, '\u00cb\u2122':13072, '':239, '':185, '':185, '1':16, '.':13, ' ':207, '\u00c8\u00a3':1570, '\u2264':110, '\u00cb\u2202':7446, ' (':334, '\u00c2\u00aa':1827, '\u00cb\u2202':7446, '\u00ca\u00ae':3874, '':228, '\u00c8\u00a3':3577, '\u00c2':6418, '\u00a2':95, '\u00d4\u00ba':19304, '\u00c8\u00c2\u220f\u220f':18867, '\u00c2\u2122':2141, '\u2030\u00f8':1056, '':211, '\u00ca':3340, '\u00c2':2169, '\u222b':118, '\u00ca\u00a9':4758, '\u00c2\u220f':1456, '\u00ca\u00cb':5848, '\u00c2':8036, '\u00c2\u220f':1456, '\u00c2':6418, '':214, '\u00ca':1528, '':211, '\u00c2':2914, '\u00c8\u00a3':3577, '\u00c8\u00aa':2588, '':239, '\u00c2\u00f8':1346, ')':8, '':185, '2':17, '.':13, ' ':207, '\u00c2':477, '':231, '\u00ca':1352, '\u220f':116, '\u00c8':163, '\u00a7':97, '\u00ae':101, ' (':334, '\u2030\u220f':505, '\u00c2':1399, '':220, '\u00c2\u221e\u00ca\u03c0':7481, '\u00d4\u00ba':19304, '\u2030\u222b\u222b':630, '\u00c2':1125, '':220, '\u00c2\u00d8\u2030\u00aa\u2022\u00c2\u00aa':87826, '\u00c2\u222b\u00b6':1825, '\u00c2':7882, '\u00ca':1352, '\u220f':116, '\u00ca\u00cb':5848, '\u00c1':1386, '':216, '\u00ca':1352, '\u220f':116, ')':8, '':185, '3':18, '.':13, ' ':207, '\u00c8':163, '\u00a7':97, '':219, '\u00cb':1777, '\u2264':110, ' (':334, '\u2030\u00f8':2067, '\u00c8':163, '\u00a7':97, '':219, '\u00cb':1777, '\u2264':110, '\u00c1':161, '\u2211':115, '':235, '\u201e':537, '\u00cb':1777, '\u2264':110, '\u00c2\u220f':665, '\u2202':114, '\u00d4\u00ba':19304, '\u2030\u00aa\u00a7':7771, '\u00c2\u221e':923, '\u00c2':3077, '':229, '\u00ca':392, '':212, '\u00c8':1249, '':223, '\u2030\u00aa\u03a9':4766, '\u00c2':4679, '':239, '\u00ca':3924, '':222, '\u00d4\u00ba':19304, '\u00ca\u00cb':5848, '\u00c2':4679, '':239, '\u00ca':3924, '':222, '\u2030\u03c0':1363, '\u00c2\u00e6':667, '':221, '\u00c2\u222b\u2211':5088, '\u00c2\u00e6':667, '\u00a9':102, ')':8, '':185, '4':19, '.':13, ' ':207, '\u00c1\u2022\u00ae':6467, '\u00c2\u221e\u00e6':12740, ' (':334, '\u00c2':2169, '\u222b':118, '\u00c2\u00ba':698, '\u00b5':113, '#':2, '\u00c8\u00a3':1570, '':236, ' ':207, '\u00c2\u222b\u00b6':1825, '\u00ca':3924, '\u00a3':96, '\u00cb\u03a9':4744, '\u00c2':27766, '':240, '\u00c1':5373, '\u00cb\u00f8':14473, '\u00c1':1920, '':209, '\u00ca\u00ca':33237, '\u00cb':1605, '\u00c2':2089, '#':2, '\u00ca\u2202':2790, '\u00ca':5527, '':235, ' ':207, '\u00c2':6418, '':214, '\u00c2':15147, '\u221e':108, '\u00c8\u00ae':1687, '\u2030\u00aa\u03a9':4766, ')':8, '':185, '5':20, '.':13, ' hacker':82402, ' (#':26580, 'hack':68051, ' ':207, '\u00c2\u2022':1895, '\u2030\u222b\u222b':630, '\u00c2':1125, '':220, '\u00c8\u00ae':1687, '\u00c8':1660, '\u00aa':119, '\u00cb':3173, '\u00b6':99, '\u00c2':6418, '':214, '\u2030\u222b\u222b':630, ')':8, '':185, '6':21, '.':13, ' ':207, '\u00c1\u2265':5452, '\u00ca':599, '':236, '\u00cb':43101, '\u2260':242, ' (':334, '\u00ca\u00a7\u00c1\u00a9':19373, '\u00c2':2217, '\u00d4\u00ba':2000, '\u2030\u220f':505, '\u00c1\u00ae':1730, '\u00c6':106, '\u00cb':43101, '\u2260':242, '\u00cb\u00b1':3211, '\u00d4\u00ba':19304, '\u00c1\u03c0':2352, '\u00c8\u00aa':2588, '':239, '\u2030\u00f8':1056, '':211, '\u00ca':392, '':212, '\u2030\u00e6':4869, '\u00c8':10125, '\u00c2':2169, '\u222b':118, '\u00ca\u00ae':3874, '\u03c0':117, '\u2030\u220f\u00c8\u00a2':18810, '\u00c1':930, '\u00c8':1355, '\u2211':115, ')':8, '':185, '7':22, '.':13, ' ':207, '\u00c2':25308, '\u00cb\u00a9':29703, '\u2260':242, ' (#':26580, '\u00c1\u00c1\u00e6':23618, '\u201e':537, '#':2, '\u00ca':2560, '':231, '\u00cb':162, '\u00b4':104, '':231, ')':8, '':185, '8':23, '.':13, ' ':207, '\u00c2':25308, '\u00cb\u00a9':29703, '\u2260':242, ' (':334, '\u2030\u00ba\u00ba':7075, '\u00c8':2760, '\u00cb\u00ca\u00d8':11298, '\u201e':537, '\u00cb':27072, '\u00cb':162, '\u00a8':105, '\u00a8':105, ')':8, '':185, '9':24, '.':13, ' ':207, '\u00c2':2775, '\u00c1':1287, '\u222b':118, '\u00c2\u2202':1557, '\u2030\u220f\u00aa':1659, ' (':334, '\u00c2\u00cb\u2122':30360, '\u00c1':1287, '\u222b':118, '\u00c2':835, '':232, '\u2030\u220f\u00aa':1659, '\u00c2\u2260':1205, '\u201e':537, '\u2030\u220f':816, '\u00c1\u00a5':2214, '':235, '\u2030\u03a9':474, '\u00a2':95, '\u00c2':1125, '':220, '\u00c2':6418, '':214, '\u00c2\u00a9\u00c1':16547, '\u00d4\u00ba':19304, '\u00c2':6418, '':214, '\u2030\u03a9':474, '\u00a2':95, '\u00c2':1125, '':220, '\u00ca\u00c1\u00a7\u222b':46576, '\u2030\u220f':1155, '\u00d4\u00ba':19304, '\u00ca\u2122':3918, '\u00c2':5657, '\u00c2':1507, '\u00c2\u221e':519, '':222, '\u00ca\u03c0':1263, '\u00c1':1287, '\u222b':118, '\u00ca':812, '\u00b5':113, ')':8, '':185, '1':16, '0':15, '.':13, ' ':207, '\u00cb':162, '\u00aa':119, '\u00ae':101, '\u00cb':7727, '':220, ' (':334, '\u00c2\u2265':4007, '\u2030\u00f8':1056, '':211, '#':2, '\u00c2':939, '':229, '\u00cb':162, '\u00aa':119, '\u00ae':101, ')':8, '':185, '1':16, '1':16, '.':13, ' ':207, '\u00ca':2509, '\u00c2\u2022':9554, '\u00c2\u00a7\u00b4':6834, ' (':334, '\u00ca':2509, '#':2, '\u00c8\u00b4':1331, '\u00c1':1707, '\u00e6':122, '\u00c2\u00a7\u00b4':6834, '\u00c1':3353, ')':8, '':185, '1':16, '2':17, '.':13, ' ':207, '\u00c2':5657, '\u00ca':1621, '':209, ' (':334, '\u00ca':3340, '\u00ca':1556, '\u00c2\u00c6':5543, '\u00cb':162, '\u2265':111, '\u00a3':96, '\u00c2':6418, '':214, '\u00cb\u2264':16434, '\u00ae':101, '\u00c2':1741, '\u00c2':3077, '':229, '\u00c1':1920, '':209, '\u2030\u222b\u222b':630, '\u00ca':1621, '':209, ')':8, '':185, '1':16, '3':18, '.':13, ' ':207, '\u00ca':5527, '\u222b':118, '\u00c8':905, '':219, '\u2030\u03a9\u00a8':60352, ' (':334, '\u00c2\u00b1':2132, '\u00c1':4154, '\u00e6':122, '\u00c8':905, '':219, '\u00ca\u221e':898, '\u00a3':96, '\u00d4\u00ba':19304, '\u00ca':5527, '\u222b':118, '\u00c2\u222b':1029, '\u00cb\u2264':16434, '\u00b0':94, '\u00c2':1410, '\u00c8':9651, '\u00c2':10844, '\u00c2':6418, '':214, '\u00c2\u00df\u00f8':19228, '\u00ca':3689, '':220, ')':8, '':185, '1':16, '4':19, '.':13, ' ':207, '\u00c2\u00aa':1836, '\u00a3':96, '\u00ca\u2260':7006, '\u00c2':527, '':216, ' (':334, '\u00c2':2169, '\u222b':118, '\u00ca\u2202':2790, '\u00c8\u2265':5431, '\u00ca\u00a9':16696, '':240, '\u00c2\u00c6':1506, '\u00ca':1528, '':211, '\u00ca\u2260\u00ca\u00e6':44751, '\u00c2':6418, '':214, '\u00c8':747, '\u00a3':96, '\u00c1\u222b':928, '':221, '\u00c2':527, '':216, ')':8, '':185, '1':16, '5':20, '.':13, ' ':207, '\u00c8':163, '':221, '':213, '\u00c2\u03a9\u00b1':3115, '\u00ca\u00a9':16696, '':240, ' (#':26580, '\u00c8':163, '':221, '':213, '\u00c2\u03a9\u00b1':3115, ' ':207, '\u00c2':6418, '':214, '\u00ca\u00a9':16696, '':240, '\u00c2\u00ae':4625, '\u00d4\u00ba':63550, '\u00c8':1968, '\u00cb\u00a9':29703, '':239, '\u00d4\u00ba':45326, '\u00c8\u00ae':1687, '\u00d4\u00ba':64471, ')':8, '':185, '1':16, '6':21, '.':13, ' ':207, '\u00c2':57928, '\u00cb\u00a9':29703, '\u00a9':102, '\u00c2':57928, '\u00c2':3077, '':229, '\u00c1\u00a9':47996, ' (':334, '\u00cb\u00b0\u00ae\u00c1\u00a7\u222b':6365, '\u00cb\u2260':5710, '':218, '\u00ca':5527, '':235, '\u00c1\u00a2':4678, '\u222b':118, '\u00c8':39867, '\u00f8':123, '\u00d4\u00ba':19304, '\u00c1':1541, '\u00b0':94, '\u00ca\u2265':1511, '\u00ca\u00b5':14050, '\u00cb':162, '\u2265':111, '\u00a5':112, ')':8, '':185, '1':16, '7':22, '.':13, ' ':207, '\u2030\u03a9':474, '':217, '\u00c8':1759, '\u00c2\u00a7\u00df':748, '\u00ca':392, '':212, ' (':334, '\u00c2\u00c6\u00ca':45795, '\u00c2':6418, '':214, '\u00c2':6702, '\u2265':111, '\u00c8':1759, '\u00c2\u00a7\u00df':748, '\u00ca':392, '':212, ')':8, '':185, '1':16, '8':23, '.':13, ' ':207, '\u00c2\u00d8':753, '\u00b4':104, '\u00ca':1907, ' (#':26580, '\u00c2\u00d8':753, '\u00b4':104, '\u2030\u03a9':1149, ' ':207, '\u00c2':6418, '':214, '\u2030\u222b\u222b':630, '\u201e':398, ')':8, '':185, '1':16, '9':24, '.':13, ' ':207, '\u00c2\u00d8':753, '\u00b4':104, '\u00ca':1907, ' (':334, '\u2030\u00aa\u00a3\u00ca\u00f8':44014, '\u2030\u00aa\u2030\u222b\u222b':21935, '\u00c2\u00d8':753, '\u00b4':104, '\u2030\u03a9':1149, '\u00c2':6418, '':214, '\u2030\u222b\u222b':630, '\u201e':398, ')':8, '':185, '2':17, '0':15, '.':13, ' ':207, '\u2030\u00aa\u00a3':2404, '\u00ca':2509, ' (#':26580, '\u2030\u00aa\u00a3\u00ca\u00f8':44014, ' ':207, '\u00c2\u00b6\u2030\u220f':15115, '\u2030\u222b\u222b':630, '#':2, '\u00ca':2509, '\u00ca\u00a9':16696, '':240, '\u201e':398, ')':8, '':185, '2':17, '1':16, '.':13, ' ':207, '\u00cb\u2202':7446, '\u00c1\u00a2':20092, ' (':334, '\u00c1\u00ae':1082, '\u00c2':27766, '':240, '\u00c8\u00a3':1570, '\u2264':110, '\u00cb\u2202':7446, '\u00c2':6418, '':214, '\u00c1\u00a2':20092, '\u00d4\u00ba':19304, '\u00c8\u00c2\u220f\u220f':18867, '\u00ca':392, '':212, '\u00c1\u00cb\u00df\u00a3':12814, '\u00c1':1287, '\u222b':118, '\u00c1\u00b1':2890, '\u00c8\u2202\u00c1\u2211':59142, '\u00c1':8505, '':227, '\u00cb\u00a3':2566, '\u03a9':121, '\u00cb\u00ca':50157, '\u00c2':6418, '':214, '\u00ca\u2022\u00ca\u00a8':10162, '\u00cb\u2202':7446, '\u00c8':1759, '\u00c8\u00c2\u2211':68745, '\u201e':398, ')':8, '':185, '2':17, '2':17, '.':13, ' ':207, '\u00cb\u2202':7446, '\u00c8':12307, ' (':334, '\u00c2\u221e':519, '':216, '\u00cb\u2202':7446, '\u00cb':4916, '':218, '\u00c2':1507, '\u00cb\u2202':7446, '\u00ca\u03c0':3649, '\u00d8':107, '\u00c2':1289, '\u00c8':85824, '':220, '\u00c2':6418, '':214, '\u00c8\u00c2\u2211':68745, '\u00d4\u00ba':19304, '\u00c2\u00a7\u00df\u00ca\u00b6':21373, '\u00ca':609, '\u00c2':434, '\u00a9':102, '\u00c1\u00ae':1730, '\u00c6':106, '\u00c1\u00ae\u00ca\u2265':76833, '\u00d4\u00ba':19304, '\u2030\u220f':505, '\u00c1\u00ae':1730, '\u00c6':106, '\u2030\u00f8':1056, '':211, '\u00c2\u00d8\u2030\u00aa\u2022':1876, '\u00cb\u00a3':2566, '':238, '\u2030\u03a9':4222, '\u00cb\u2202':7446, '\u00cb':4916, '':218, '\u00d4\u00ba':19304, '\u00ca\u00e6\u00c2\u2022':20431, '\u00ca\u2264':931, '':231, '\u00ca\u2265\u00b0':11584, '\u00c2\u00ae':4625, '\u00c2\u2211':3475, '\u00c2\u2022':1895, '\u00c8\u00a2':1359, '\u00d4\u00ba':19304, '\u00c2\u221e':923, '\u00cb\u2202':7446, '\u00ca\u03c0':3649, '\u00d8':107, '\u00ca\u2264':931, '':231, '\u00c2\u2022\u03a9':1250, '\u00c2':6418, '':214, '\u00ca':1528, '':211, '\u00c2':2914, '\u00c2\u221e\u00b1\u00c2\u00d8\u2030\u00aa\u2022':11847, '\u00ca':98307, '\u00cb\u00b5\u2211':1762, '\u00c2':6418, '':214, '\u00d4\u00ba':2000, '\u00cb':1306, '\u00c2\u00b6\u2030\u220f':15115, '\u00c1\u00ae':1730, '\u00c6':106, '\u00c2\u221e\u00b1':941, '\u2030\u00f8':1056, '':211, '\u00c2\u221e\u00b1':941, '\u00c2':835, '':210, '\u2030\u220f':505, '\u00c2':1399, '':220, '\u00c1\u00d8':7739, '\u00a9':102, '\u00ca\u00cb':5848, '\u00c1':161, '\u2202':114, '\u2264':110, '\u00d4\u00ba':19304, '\u2030\u00f8':1056, '\u00e6':122, '\u2030\u222b\u222b':630, '\u00ca':5527, '\u222b':118, '\u00c2':2169, '\u222b':118, '\u00cb\u2202':7446, '\u00ca\u00d8':11837, '\u00ca\u00cb':5848, '\u00cb\u2202':7446, '\u00c2\u00a3':2406, '\u222b':118, '\u2030\u220f\u00c8\u00a2':18810, '\u00d4\u00ba':19304, '\u00c2':2169, '\u222b':118, '\u00ca':599, '':240, '\u00cb\u2202':7446, '\u00c2':6418, '':214, '\u00ca':1528, '':211, '\u00c2':2914, '\u00c2':2996, '\u00c2\u221e':519, '':216, '\u00cb\u2202':7446, '\u00cb':4916, '':218, '\u00c2':1507, '\u00cb\u2202':7446, '\u00ca\u03c0':3649, '\u00d8':107, '\u00c2':1289, '\u00c8':1660, '\u00a2':95, '\u201e':398, ')':8, '':185, '2':17, '3':18, '.':13, ' ':207, '\u00c2\u2122':2141, '\u00cb\u00ae':4564, '\u00b1':109, '\u00c2\u2211':4477, '\u00c2\u00c6':6066, '\u00ca\u00e6':2660, '\u00c1\u00b4':4243, '\u00d4\u00ba':19304, '\u2030\u220f\u00c2':75710, '\u00c1\u00e6\u00c2\u00df':21362, '\u00c8\u00aa':2588, '':239, '\u00c1':8505, '':217, ' (':334, '\u00ca\u00d8\u00c2\u00aa':72451, '\u00ca\u00a8':1137, '':219, '\u00cb\u2264':16434, '\u00a5':112, '\u00c8':1688, '':223, '\u00c2\u00b1':884, '\u00a7':97, '\u00c2\u00d8\u2030\u00aa\u2022':1876, '\u00c1':1287, '\u222b':118, '\u00ca\u00ca\u00a8\u2264':90104, '\u00c1':1287, '\u222b':118, '\u00d4\u00ba':19304, '\u00cb\u00c1\u00e6\u00c2\u00df':36759, '\u00c2':6418, '':214, '\u00ca\u2260\u00a3':2206, '\u00c1':1920, '\u2202':114, '\u00cb\u00ae\u00cb\u00b0':92872, '\u00c2\u221e\u00b1':941, '\u00c2\u00c2\u221e':10542, '\u00c1\u00ae':1730, '\u00c6':106, '\u00c1\u00ae':1730, '\u00c6':106, '\u00c8\u00c2\u2202':19680, '\u00d4\u00ba':2000, '\u00c2':3127, '\u00ca':2322, '\u2030\u220f':505, '\u00c2':1399, '':220, '\u2030\u222b\u222b':630, '\u00cb\u00ae\u00cb\u00b0':92872, '\u00c8':1660, '':234, '\u00c8':1737, '\u00ca\u00ae':3874, '':234, '\u00ca\u222b':3094, '':231, '\u201e':398, '\u00ca\u222b':3931, '\u00ca':599, '\u00ba':120, '\u00c2\u00c6':14504, '\u00ca':8702, '\u2030\u220f':505, '\u00c2':1399, '':220, '\u00c2\u2211':4477, '\u00c8':1355, '\u2211':115, '\u00c2\u00b4':7848, '\u00c1\u221e':9173, '\u00c1\u00aa':8329, '\u00d4\u00ba':19304, '\u00cb\u00b6\u00ca\u00b1':5886, '\u00c1\u00e6\u00c2\u00df':21362, '\u00c8\u00f8':8978, '\u00cb':162, '\u00b4':104, '\u00b1':109, '\u00c2':3077, '':229, '\u00c2':3711, '\u00c1\u00ae':1082, '\u201e':17194, '\u00c1\u00aa':8329, '\u201e':17335, '\u00c2\u2260':4416, '\u00c2':1507, '\u00c8\u2265':5431, '\u00c2':6418, '':214, '\u00c2\u2260':4416, '\u00d4\u00ba':19304, '\u00ca':599, '\u00ba':120, '\u00ca\u00d8':504, '\u00c2\u00a7\u00df\u00c2\u00c6\u2202':4145, '\u00c2\u221e\u00b1':941, '\u00c2\u221e':519, '':216, '\u201e':17194, '\u00c1':8505, '':217, '\u201e':17335, '\u00ca\u03c0':3520, '\u00c1\u00ae':1730, '\u00b1':109, '\u00c1':1287, '\u222b':118, '\u201e':17194, '\u00c1\u00b4':4243, '\u201e':17335, '\u201e':398, '\u00c2\u221e':923, '\u00c2':835, '':232, '\u00c2\u00c2\u00c6\u00b5':86426, '\u00c1\u00d8':7739, '':209, '\u00c2':6418, '':214, '\u00ca':1528, '':211, '\u00c2':2914, '\u00d4\u00ba':19304, '\u00ca\u00f8\u00c2\u222b':7076, '\u00cb\u2264':16434, '\u00ba':120, '\u00c2\u222b':1029, '\u00c2':4072, '\u00c1\u00a7\u222b':3198, '\u00d4\u00ba':45326, '\u201e':17194, '\u00ca\u00a8':1641, '\u00c2\u2211':4477, '\u2030\u00e6':4869, '\u2030\u00e6':5346, '\u00ca\u00e6':2660, '\u00c1\u00b4':4243, '\u2030\u220f':1937, '\u00ca\u2022':1479, '\u201e':17335, '\u201e':398, ')':8, '':185, '2':17, '4':19, '.':13, ' ':207, '\u00c1':87306, '\u00ae':101, '\u00c2\u00b1':12660, ' (':334, '\u2030\u220f':505, '\u00c2':1399, '':220, '\u2030\u222b\u222b':630, '\u00c2\u00b1':12660, '\u00d4\u00ba':19304, '\u00c2':3077, '':229, '\u2030\u00f8':1056, '':211, '\u00c2':1507, '\u2030\u222b\u222b':630, '\u2030\u220f':505, '\u00c8\u03a9':9543, '':219, '\u00c2\u00b1':12660, ')':8, '':185, '2':17, '5':20, '.':13, ' ':207, '\u00c2\u00f8\u00b4':3411, '\u2030\u222b\u222b':630, '\u00c2\u00f8\u00b4':3411, '\u2030\u222b':1609, ' (':334, '\u00c1\u03a9':19653, '\u00c2\u00f8\u00b4':3411, '\u2030\u222b\u222b':630, '\u00c2':2089, '\u00c2':6418, '\u00a2':95, '\u00c1\u03a9':19653, '\u00c2\u00f8\u00b4':3411, ')':8, '':185, '2':17, '6':21, '.':13, ' ':207, '\u00c2':3381, '\u00cb':3173, '\u2265':111, '\u00c1':87306, '\u220f':116, ' (':334, '\u00ca':3340, '\u00c2':434, '\u00a9':102, '\u00c2':1399, '':220, '\u2030\u222b\u222b':630, '\u00c2':2169, '\u222b':118, '\u00c2\u00a8':1429, '\u00c2\u00aa':1836, '':210, '\u00c2\u2022':1895, '\u00c8\u00a2':1359, '\u00cb\u00b6':15378, '\u2122':103, '\u00c1':5354, '\u00b1':109, '\u00c1\u00cb\u2265':7214, '\u00c2':2089, '\u00ca':1101, '':236, '\u201e':398, '\u00c2':2169, '\u222b':118, '\u00c2\u00aa':1836, '':210, '\u00ca':1556, '\u00c2\u00a7\u00c8\u00a2':42890, '\u00c8':85824, '':209, '\u00c1':161, '\u220f':116, '\u00b4':104, '\u00ca':4279, '\u00cb\u03a9':4744, '\u00c2\u00aa':1827, '\u00d4\u00ba':19304, '\u00ca':392, '':212, '\u00cb\u00b6':15378, '':220, '\u00c2\u221e':923, '\u00c2':3381, '\u00c8':1688, '\u00aa':119, '\u00cb':3173, '\u2265':111, '\u00d4\u00ba':19304, '\u00c2\u2020\u00ca\u2260\u00a7':7058, '\u00c1\u00ae':1730, '\u00b1':109, '\u00c1':1287, '\u222b':118, '\u00c2':3381, '\u00cb':3173, '\u2265':111, '\u00c1':87306, '\u220f':116, '\u201e':398, ')':8, '':185, '2':17, '7':22, '.':13, ' ':207, '\u00c1':1541, '\u00b0':94, '\u00c1\u00b1\u2265':4428, '\u00c1\u2264\u2022':30240, ' (':334, '\u00c2\u00ca\u2260\u2022':38454, '\u00c2':6418, '':214, '\u00ca\u00a5\u03a9':51603, '\u00cb':162, '\u00b4':104, '':216, '\u201e':398, '\u00c2\u2020':1731, '\u00c1':1287, '\u222b':118, '\u2030\u222b\u00ca':12196, '\u00ca':1556, '\u00ca\u2202':10470, '\u00c2':6418, '':214, '\u00c8\u00cb\u00b6':5097, '\u00ca\u00a2':3703, '':238, '\u2030\u00aa\u2202':2880, '\u00c2':1507, '\u00c1\u00a5':2214, '\u221e':108, '\u00c1\u00d8':7739, '':209, '\u00c8\u03a9':1223, '\u00ca\u2122':3918, '\u00ca':609, '\u00cb':10786, '\u00cb\u03a9':4744, '\u00d4\u00ba':19304, '\u00ca\u2030\u00aa\u2022':3705, '\u00c1':1287, '\u222b':118, '\u00cb':162, '\u00a8':105, '\u03c0':117, '\u00ca':22401, '\u00cb\u00ae':4564, '':217, '\u00d4\u00ba':19304, '\u00c2\u2122\u00ca\u00d8':7553, '\u00cb\u00a9':29703, '\u00b1':109, '\u201e':17194, '\u00c1\u2264':62864, '\u00c1':1541, '\u00b0':94, '\u00c1\u00b1\u2265':4428, '\u00c1\u2264\u2022':30240, '\u201e':17335, '\u00d4\u00ba':19304, '\u2030\u00aa\u2022\u00c2':37221, '\u2030\u00aa\u2030\u222b\u222b':21935, '\u00ca\u00ca':43175, '\u00c8':1249, '':223, '\u00c8\u00b4':1331, '\u201e':398, ')':8, '':185, '2':17, '8':23, '.':13, ' ':207, '\u00c1\u00e6':1455, '\u00a9':102, '\u00ca':392, '':212, ' (':334, '\u00cb':2230, '':219, '\u00ca':1528, '':211, '\u2030\u03a9\u2030\u220f':82738, '\u00c8':1688, '':223, '\u00c2\u00b1':884, '\u00a7':97, '\u00c2':6418, '':214, '\u00c2':7882, '\u00cb\u2264':16434, '\u220f':116, '\u00ca\u00cb':5848, '\u00c8':3771, '\u00cb':162, '\u2265':111, '':216, '\u00ca\u00a5\u00aa':2104, '\u00c2':4679, '':230, '\u201e':398, '\u00c2\u2020':1731, '\u00c1':1287, '\u222b':118, '\u2030\u03a9':474, '\u00a2':95, '\u00c2\u221e':1186, '\u00c2':723, '':216, '\u00ca\u00b5\u00ca\u00ba':39437, '\u00ca\u00cb':5848, '\u00ca':5527, '':229, '\u2030\u00f8':2067, '\u00d4\u00ba':19304, '\u00c8':1660, '\u00a3':96, '\u2030\u00aa\u2022':851, '\u00c2':2657, '\u00c8':163, '':219, '':209, '\u00cb\u00b0':1127, '\u00c2':7882, '\u00c8':163, '':221, '\u00a2':95, '\u00d4\u00ba':19304, '\u00ca':599, '\u00ba':120, '\u00ca\u00d8':504, '\u00c2\u221e\u00b1':941, '\u00c8\u00c2':57359, '\u00ca\u00c2':5839, '\u201e':537, '\u00c8':33948, '\u221e':108, '\u00c2\u00b1':5528, '\u201e':537, '\u00c2':1507, '\u00ca\u2022':5126, '\u2260':242, '\u00c1\u2260':1537, '\u00d4\u00ba':19304, '\u00c2\u221e':519, '':216, '\u00c2\u00a7\u00df\u00c2\u00c6\u2202':4145, '\u00c2':6418, '':214, '\u00c8':163, '':221, '\u00a2':95, '\u00c2\u00c6\u00ca':31579, '\u00c8\u00c2':57359, '\u00cb\u00b5\u2211':1762, '\u00c2':27766, '':240, '\u2030\u00e6':4068, '\u00ca':392, '':212, '\u00d4\u00ba':19304, '\u00c8':1618, '':219, '\u00c2':1399, '':220, '\u00ca\u2022\u00c8':77404, '\u00c1\u00ae':1082, '\u00c8':163, '':221, '\u00a2':95, '\u00d4\u00ba':19304, '\u00c2\u221e\u00b1\u00c2\u00d8\u2030\u00aa\u2022':11847, '\u00c1':1920, '':209, '\u00c2\u00a9\u00ca\u00d8':40741, '#':2, '\u00ca\u00ae':3874, '':234, '\u00ca':392, '':212, '\u00d4\u00ba':19304, '\u00ca':98307, '\u00c2':835, '':232, '\u00c1\u2260':1133, '':215, '\u00c8':163, '':221, '\u00a2':95, '\u00d4\u00ba':19304, '\u00c1\u2202':1654, '\u00c2\u00e6':667, '':221, '\u00c2\u00ca':80835, '\u00ca\u00a8':1641, '\u00ca\u00d8':3714, '\u00c8':1618, '':213, '\u00cb\u00f8':14473, '\u00c1':1920, '':209, '\u00c2\u2202\u2030\u00aa\u2030\u222b\u222b':60594, '\u201e':398, '\u00c1':1920, '\u2202':114, '\u00ca\u00ca\u2030\u222b\u222b\u00c8\u03a9':87647, '\u00ca':3532, '\u00c2\u00c6':3091, '\u00ca\u00ae':3874, '':234, '\u00d4\u00ba':19304, '\u00c2':1399, '':220, '\u00ca':392, '':212, '\u00c2\u221e\u00b1':941, '\u00ca':392, '':212, '\u00c1\u00b5':55361, '':225, '\u00ca':10624, '\u201e':398, '\u00c2':3127, '\u00c1\u00ae':1730, '\u00b1':109, '\u00ca\u00ae':3874, '':234, '\u00ca':392, '':212, '\u201e':537, '\u00c8':163, '':219, '':209, '\u00ca':392, '':212, '\u201e':537, '\u00c2':1633, '\u00ca':392, '':212, '\u201e':398, ')':8, '':185, '2':17, '9':24, '.':13, ' ':207, '\u00ca\u00b6':16154, '\u00ca':1930, ' (':334, '\u00cb\u2264':16434, '\u00ba':120, '\u00c2':2169, '\u222b':118, '#':2, '\u00ca\u00b6':16154, ' ':207, '\u2030\u220f\u00c8\u00a2':18810, '\u00c2':6418, '':214, '\u00ca\u00c2\u2260':22152, ')':8, '':185, '3':18, '0':15, '.':13, ' MK':48536, '\u00c2':2217, ' (':334, '\u2030\u00f8':1056, '':211, '\u00ca':3340, '#':2, 'MK':68061, ' ':207, '\u00c2':2169, '\u222b':118, '\u00c1':161, '\u2202':114, '\u2264':110, '\u2030\u220f':816, '\u00ca':392, '':212, '\u00c1\u00ae':1082, '\u00c2':6418, '':214, '\u00c2':2217, '\u00d4\u00ba':19304, '\u00c8\u00ca':86126, '\u00ca\u00c2\u2265':18672, '\u2030\u220f\u00ca':56911, '\u00c2':6418, '':214, '\u00c1\u03c0\u00ca\u00c6':17065, '\u00c1\u00a8\u00b6':11265, '\u00cb':2628, '':240, '\u00d4\u00ba':19304, '\u00ca\u00cb':5848, '\u00c8\u2030\u220f':64414, '\u00c1\u03c0\u00c2\u00c6':50982, '\u00c2\u2260':4416, '\u00c8':163, '\u2020':241, '\u2260':242, '\u00c2':6418, '':214, '\u00c2':2217, '\u201e':398, '\u00c1\u03c0\u00c2\u00c6':50982, '\u00c2\u2260':4416, '\u00c8':163, '\u2020':241, '\u2260':242, '\u00c2\u00ca\u00a8':8344, '\u00d4\u00ba':45326, '\u00c2\u2020':5319, '':214, '\u00c1\u2265\u00aa':2370, '\u201e':537, '\u00c8':23138, '\u00ca':1101, '':236, '\u201e':537, '\u00c1\u00b5':55361, '':230, '\u00c2\u221e':519, '':222, '\u201e':537, '\u00c2\u00d8':12778, '\u00ca':1101, '':236, '\u201e':537, '\u00cb\u2211\u2265':12859, '\u00c2\u2202':2278, '\u201e':537, '\u00cb':162, '\u00a8':105, '':238, '\u00c1\u00b5':55361, '':230, '\u00c1\u2260':1537, '\u201e':398, ')':8, '':185, '':185, '####':3589, '':185, '':185, '1':16, '.':13, ' ':207, '\u00c8\u00a3':1570, '\u2264':110, '\u00cb\u2202':7446, '':185, '':185, '\u00c2\u00aa':1836, '\u00a3':96, '\u00ca':610, '\u00b1':109, '\u00cb\u00a9':29703, '\u00b1':109, '\u00d4\u00ba':45326, '\u00ca':852, '\u00c2\u221e':519, '':220, '\u00ca\u2022':1479, '\u00c2':1507, '\u00ca\u00c2':5839, '\u00c2\u00aa':1827, '\u00c8\u00a3':1570, '\u2264':110, '\u00cb\u2202':7446, '\u00d4\u00ba':19304, '\u00c2\u00b4':7848, '\u00c2':835, '':232, '\u00c2\u2022\u03a9\u00c2\u00a7':37592, '\u00c8\u00aa':2588, '':239, '\u00c2\u00f8':1346, '\u201e':398, '':185, 'English':18656, ':':25, ' I':304, ' went':2674, ' to':276, ' yum':81215, ' cha':19147, ' with':366, ' my':601, ' friends':3997, ' yesterday':11209, ' and':285, ' ordered':10133, ' a':245, ' lot':2603, ' of':280, ' dim':4165, ' sum':2555, '.':13, '':185, '':185, '2':17, '.':13, ' ':207, '\u00c2':477, '':231, '\u00ca':1352, '\u220f':116, '\u00c8':163, '\u00a7':97, '\u00ae':101, '':185, '':185, '\u00c2\u00aa':1836, '\u00a3':96, '\u00ca':610, '\u00b1':109, '\u00cb\u00a9':29703, '\u00b1':109, '\u00d4\u00ba':45326, '\u2030\u03a9\u2020':1372, '\u00c2\u00b1':11386, '\u2030\u00ba':3243, '\u00c8\u00cb\u00f8':18649, '\u00ca':609, '\u00c2':723, '':216, '\u00c2':477, '':231, '\u00ca':1352, '\u220f':116, '\u00c8':163, '\u00a7':97, '\u00ae':101, '\u00d4\u00ba':2224, '':185, 'English':18656, ':':25, ' Is':2128, ' there':745, ' a':245, ' library':7503, ' near':3345, ' your':520, ' home':1719, '?':30, '':185, '':185, '':185, 'Assistant':77398, ':':25, ' ':207 ]\r\n[1717093113] recalculate the cached logits (check): embd_inp.empty() false, n_matching_session_tokens 0, embd_inp.size() 1591, session_tokens.size() 0, embd_inp.size() 1591\r\n[1717093113] inp_pfx: [ '<\u00d4\u03a9begin\u201aof\u201asentence\u00d4\u03a9>':100000, '':185, '':185, '###':13483, ' Instruction':50278, ':':25, '':185, '':185 ]\r\n[1717093113] inp_sfx: [ '':185, '':185, '###':13483, ' Response':21194, ':':25, '':185, '':185 ]\r\n[1717093113] cml_pfx: [ '<\u00d4\u03a9begin\u201aof\u201asentence\u00d4\u03a9>':100000, '':185, '<':27, '|':91, 'im':309, '_':62, 'start':4789, '|>':66325, 'user':3631, '':185 ]\r\n[1717093113] cml_sfx: [ '<':27, '|':91, 'im':309, '_':62, 'end':409, '|>':66325, '':185, '<':27, '|':91, 'im':309, '_':62, 'start':4789, '|>':66325, 'assistant':81038, '':185 ]\r\n[1717093113] sampling: \r\n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\r\n\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\r\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\r\n[1717093113] sampling order: \r\nCFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \r\n[1717093113] generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\r\n[1717093113] \r\n\r\n[1717093113] embd_inp.size(): 1591, n_consumed: 0\r\n[1717093113] eval: [ '<\u00d4\u03a9begin\u201aof\u201asentence\u00d4\u03a9>':100000, 'User':5726, ':':25, ' ':207, '\u2030\u03a9\u2020':1372, '\u2030\u00f8':1056, '':211, '\u2030\u220f':505, '\u00c2':1399, '':220, '\u00c2\u00aa':1836, '\u00a3':96, '\u00ca':610, '\u00b1':109, '\u00cb\u00a9':29703, '\u00b1':109, '\u2030\u222b\u222b\u00c2\u2211\u2022\u00ca\u222b\u00cb\u03a9':29680, '\u00ca\u00ae\u00b0\u00c2':39322, '\u201e':398, '\u00cb\u00ae':4564, '':233, '\u2030\u03a9':4222, '\u00d4\u00ba':19304, '\u00cb\u00b6\u00c1\u00ae':57228, '\u00c2\u00aa':1836, '\u00a3':96, '\u00ca':610, '\u00b1':109, '\u00cb\u00a9':29703, '\u00b1':109, '\u00cb\u2122':13072, '':239, '\u00ca\u2265':1511, '\u00d4\u00ba':19304, '\u00c2':3077, '':229, '\u00c2\u2022\u03a9':1250, '\u00c1\u00ae':1082, '\u00ca\u00c6\u00c8':12484, '\u00cb\u00a9':29703, '\u00b1':109, '\u00cb\u2122':13072, '':239, '\u00ca\u2265':1511, '\u201e':398, '\u2030\u00e6\u00c2\u00b6':20332, '\u00d4\u00ba':19304, '\u00c2\u00aa':1836, '\u00a3':96, '\u00ca':610, '\u00b1':109, '\u00cb\u00a9':29703, '\u00b1':109, '\u00c2\u2022':1895, '\u00c8\u00a2':1359, '\u00c2':6418, '':214, '\u201e':17194, '\u00c1':337, '\u201e':17335, '\u00ca':8460, '':218, '\u00c2\u00d8':753, '\u00b4':104, '\u00ca':1114, '\u201e':17194, '\u00c2':6418, '':214, '\u201e':17335, '\u00d4\u00ba':19304, '\u201e':17194, '\u00ca\u00d8':504, '\u201e':17335, '\u00ca':8460, '':218, '\u00c2\u00d8':753, '\u00b4':104, '\u00ca':1114, '\u201e':17194, '\u2030\u00f8':1056, '':211, '\u201e':17335, '\u201e':398, '':185, '':185, '\u00cb':162, '\u00b4':104, '':220, '\u00c1\u00ae':1082, '\u2030\u220f\u00c8\u00a2':16281, '\u00ca\u2030\u00e6':6037, '\u00c2':6418, '':214, '\u00c2\u00aa':1836, '\u00a3':96, '\u00ca':610, '\u00b1':109, '\u00cb\u00a9':29703, '\u00b1':109, '\u00cb\u00a9':29703, '':239, '\u00cb\u2122':13072, '':239, '\u00c8\u2020':2965, '\u00c2\u2022':8260, '\u201e':398, '':185, 'With':3220, ' use':938, ' the':254, ' provided':4286, ' Cant':17162, 'onese':82174, ' words':3073, ' to':276, ' make':1099, ' sentences':12444, '.':13, '':185, '':185, '#':2, ' ':207, '\u00cb\u00a9':29703, '':239, '\u00cb\u2122':13072, '':239, '':185, '':185, '1':16, '.':13, ' ':207, '\u00c8\u00a3':1570, '\u2264':110, '\u00cb\u2202':7446, ' (':334, '\u00c2\u00aa':1827, '\u00cb\u2202':7446, '\u00ca\u00ae':3874, '':228, '\u00c8\u00a3':3577, '\u00c2':6418, '\u00a2':95, '\u00d4\u00ba':19304, '\u00c8\u00c2\u220f\u220f':18867, '\u00c2\u2122':2141, '\u2030\u00f8':1056, '':211, '\u00ca':3340, '\u00c2':2169, '\u222b':118, '\u00ca\u00a9':4758, '\u00c2\u220f':1456, '\u00ca\u00cb':5848, '\u00c2':8036, '\u00c2\u220f':1456, '\u00c2':6418, '':214, '\u00ca':1528, '':211, '\u00c2':2914, '\u00c8\u00a3':3577, '\u00c8\u00aa':2588, '':239, '\u00c2\u00f8':1346, ')':8, '':185, '2':17, '.':13, ' ':207, '\u00c2':477, '':231, '\u00ca':1352, '\u220f':116, '\u00c8':163, '\u00a7':97, '\u00ae':101, ' (':334, '\u2030\u220f':505, '\u00c2':1399, '':220, '\u00c2\u221e\u00ca\u03c0':7481, '\u00d4\u00ba':19304, '\u2030\u222b\u222b':630, '\u00c2':1125, '':220, '\u00c2\u00d8\u2030\u00aa\u2022\u00c2\u00aa':87826, '\u00c2\u222b\u00b6':1825, '\u00c2':7882, '\u00ca':1352, '\u220f':116, '\u00ca\u00cb':5848, '\u00c1':1386, '':216, '\u00ca':1352, '\u220f':116, ')':8, '':185, '3':18, '.':13, ' ':207, '\u00c8':163, '\u00a7':97, '':219, '\u00cb':1777, '\u2264':110, ' (':334, '\u2030\u00f8':2067, '\u00c8':163, '\u00a7':97, '':219, '\u00cb':1777, '\u2264':110, '\u00c1':161, '\u2211':115, '':235, '\u201e':537, '\u00cb':1777, '\u2264':110, '\u00c2\u220f':665, '\u2202':114, '\u00d4\u00ba':19304, '\u2030\u00aa\u00a7':7771, '\u00c2\u221e':923, '\u00c2':3077, '':229, '\u00ca':392, '':212, '\u00c8':1249, '':223, '\u2030\u00aa\u03a9':4766, '\u00c2':4679, '':239, '\u00ca':3924, '':222, '\u00d4\u00ba':19304, '\u00ca\u00cb':5848, '\u00c2':4679, '':239, '\u00ca':3924, '':222, '\u2030\u03c0':1363, '\u00c2\u00e6':667, '':221, '\u00c2\u222b\u2211':5088, '\u00c2\u00e6':667, '\u00a9':102, ')':8, '':185, '4':19, '.':13, ' ':207, '\u00c1\u2022\u00ae':6467, '\u00c2\u221e\u00e6':12740, ' (':334, '\u00c2':2169, '\u222b':118, '\u00c2\u00ba':698, '\u00b5':113, '#':2, '\u00c8\u00a3':1570, '':236, ' ':207, '\u00c2\u222b\u00b6':1825, '\u00ca':3924, '\u00a3':96, '\u00cb\u03a9':4744, '\u00c2':27766, '':240, '\u00c1':5373, '\u00cb\u00f8':14473, '\u00c1':1920, '':209, '\u00ca\u00ca':33237, '\u00cb':1605, '\u00c2':2089, '#':2, '\u00ca\u2202':2790, '\u00ca':5527, '':235, ' ':207, '\u00c2':6418, '':214, '\u00c2':15147, '\u221e':108, '\u00c8\u00ae':1687, '\u2030\u00aa\u03a9':4766, ')':8, '':185, '5':20, '.':13, ' hacker':82402, ' (#':26580, 'hack':68051, ' ':207, '\u00c2\u2022':1895, '\u2030\u222b\u222b':630, '\u00c2':1125, '':220, '\u00c8\u00ae':1687, '\u00c8':1660, '\u00aa':119, '\u00cb':3173, '\u00b6':99, '\u00c2':6418, '':214, '\u2030\u222b\u222b':630, ')':8, '':185, '6':21, '.':13, ' ':207, '\u00c1\u2265':5452, '\u00ca':599, '':236, '\u00cb':43101, '\u2260':242, ' (':334, '\u00ca\u00a7\u00c1\u00a9':19373, '\u00c2':2217, '\u00d4\u00ba':2000, '\u2030\u220f':505, '\u00c1\u00ae':1730, '\u00c6':106, '\u00cb':43101, '\u2260':242, '\u00cb\u00b1':3211, '\u00d4\u00ba':19304, '\u00c1\u03c0':2352, '\u00c8\u00aa':2588, '':239, '\u2030\u00f8':1056, '':211, '\u00ca':392, '':212, '\u2030\u00e6':4869, '\u00c8':10125, '\u00c2':2169, '\u222b':118, '\u00ca\u00ae':3874, '\u03c0':117, '\u2030\u220f\u00c8\u00a2':18810, '\u00c1':930, '\u00c8':1355, '\u2211':115, ')':8, '':185, '7':22, '.':13, ' ':207, '\u00c2':25308, '\u00cb\u00a9':29703, '\u2260':242, ' (#':26580, '\u00c1\u00c1\u00e6':23618, '\u201e':537, '#':2, '\u00ca':2560, '':231, '\u00cb':162, '\u00b4':104, '':231, ')':8, '':185, '8':23, '.':13, ' ':207, '\u00c2':25308, '\u00cb\u00a9':29703, '\u2260':242, ' (':334, '\u2030\u00ba\u00ba':7075, '\u00c8':2760, '\u00cb\u00ca\u00d8':11298, '\u201e':537, '\u00cb':27072, '\u00cb':162, '\u00a8':105, '\u00a8':105, ')':8, '':185, '9':24, '.':13, ' ':207, '\u00c2':2775, '\u00c1':1287, '\u222b':118, '\u00c2\u2202':1557, '\u2030\u220f\u00aa':1659, ' (':334, '\u00c2\u00cb\u2122':30360, '\u00c1':1287, '\u222b':118, '\u00c2':835, '':232, '\u2030\u220f\u00aa':1659, '\u00c2\u2260':1205, '\u201e':537, '\u2030\u220f':816, '\u00c1\u00a5':2214, '':235, '\u2030\u03a9':474, '\u00a2':95, '\u00c2':1125, '':220, '\u00c2':6418, '':214, '\u00c2\u00a9\u00c1':16547, '\u00d4\u00ba':19304, '\u00c2':6418, '':214, '\u2030\u03a9':474, '\u00a2':95, '\u00c2':1125, '':220, '\u00ca\u00c1\u00a7\u222b':46576, '\u2030\u220f':1155, '\u00d4\u00ba':19304, '\u00ca\u2122':3918, '\u00c2':5657, '\u00c2':1507, '\u00c2\u221e':519, '':222, '\u00ca\u03c0':1263, '\u00c1':1287, '\u222b':118, '\u00ca':812, '\u00b5':113, ')':8, '':185, '1':16, '0':15, '.':13, ' ':207, '\u00cb':162, '\u00aa':119, '\u00ae':101, '\u00cb':7727, '':220, ' (':334, '\u00c2\u2265':4007, '\u2030\u00f8':1056, '':211, '#':2, '\u00c2':939, '':229, '\u00cb':162, '\u00aa':119, '\u00ae':101, ')':8, '':185, '1':16, '1':16, '.':13, ' ':207, '\u00ca':2509, '\u00c2\u2022':9554, '\u00c2\u00a7\u00b4':6834, ' (':334, '\u00ca':2509, '#':2, '\u00c8\u00b4':1331, '\u00c1':1707, '\u00e6':122, '\u00c2\u00a7\u00b4':6834, '\u00c1':3353, ')':8, '':185, '1':16, '2':17, '.':13, ' ':207, '\u00c2':5657, '\u00ca':1621, '':209, ' (':334, '\u00ca':3340, '\u00ca':1556, '\u00c2\u00c6':5543, '\u00cb':162, '\u2265':111, '\u00a3':96, '\u00c2':6418, '':214, '\u00cb\u2264':16434, '\u00ae':101, '\u00c2':1741, '\u00c2':3077, '':229, '\u00c1':1920, '':209, '\u2030\u222b\u222b':630, '\u00ca':1621, '':209, ')':8, '':185, '1':16, '3':18, '.':13, ' ':207, '\u00ca':5527, '\u222b':118, '\u00c8':905, '':219, '\u2030\u03a9\u00a8':60352, ' (':334, '\u00c2\u00b1':2132, '\u00c1':4154, '\u00e6':122, '\u00c8':905, '':219, '\u00ca\u221e':898, '\u00a3':96, '\u00d4\u00ba':19304, '\u00ca':5527, '\u222b':118, '\u00c2\u222b':1029, '\u00cb\u2264':16434, '\u00b0':94, '\u00c2':1410, '\u00c8':9651, '\u00c2':10844, '\u00c2':6418, '':214, '\u00c2\u00df\u00f8':19228, '\u00ca':3689, '':220, ')':8, '':185, '1':16, '4':19, '.':13, ' ':207, '\u00c2\u00aa':1836, '\u00a3':96, '\u00ca\u2260':7006, '\u00c2':527, '':216, ' (':334, '\u00c2':2169, '\u222b':118, '\u00ca\u2202':2790, '\u00c8\u2265':5431, '\u00ca\u00a9':16696, '':240, '\u00c2\u00c6':1506, '\u00ca':1528, '':211, '\u00ca\u2260\u00ca\u00e6':44751, '\u00c2':6418, '':214, '\u00c8':747, '\u00a3':96, '\u00c1\u222b':928, '':221, '\u00c2':527, '':216, ')':8, '':185, '1':16, '5':20, '.':13, ' ':207, '\u00c8':163, '':221, '':213, '\u00c2\u03a9\u00b1':3115, '\u00ca\u00a9':16696, '':240, ' (#':26580, '\u00c8':163, '':221, '':213, '\u00c2\u03a9\u00b1':3115, ' ':207, '\u00c2':6418, '':214, '\u00ca\u00a9':16696, '':240, '\u00c2\u00ae':4625, '\u00d4\u00ba':63550, '\u00c8':1968, '\u00cb\u00a9':29703, '':239, '\u00d4\u00ba':45326, '\u00c8\u00ae':1687, '\u00d4\u00ba':64471, ')':8, '':185, '1':16, '6':21, '.':13, ' ':207, '\u00c2':57928, '\u00cb\u00a9':29703, '\u00a9':102, '\u00c2':57928, '\u00c2':3077, '':229, '\u00c1\u00a9':47996, ' (':334, '\u00cb\u00b0\u00ae\u00c1\u00a7\u222b':6365, '\u00cb\u2260':5710, '':218, '\u00ca':5527, '':235, '\u00c1\u00a2':4678, '\u222b':118, '\u00c8':39867, '\u00f8':123, '\u00d4\u00ba':19304, '\u00c1':1541, '\u00b0':94, '\u00ca\u2265':1511, '\u00ca\u00b5':14050, '\u00cb':162, '\u2265':111, '\u00a5':112, ')':8, '':185, '1':16, '7':22, '.':13, ' ':207, '\u2030\u03a9':474, '':217, '\u00c8':1759, '\u00c2\u00a7\u00df':748, '\u00ca':392, '':212, ' (':334, '\u00c2\u00c6\u00ca':45795, '\u00c2':6418, '':214, '\u00c2':6702, '\u2265':111, '\u00c8':1759, '\u00c2\u00a7\u00df':748, '\u00ca':392, '':212, ')':8, '':185, '1':16, '8':23, '.':13, ' ':207, '\u00c2\u00d8':753, '\u00b4':104, '\u00ca':1907, ' (#':26580, '\u00c2\u00d8':753, '\u00b4':104, '\u2030\u03a9':1149, ' ':207, '\u00c2':6418, '':214, '\u2030\u222b\u222b':630, '\u201e':398, ')':8, '':185, '1':16, '9':24, '.':13, ' ':207, '\u00c2\u00d8':753, '\u00b4':104, '\u00ca':1907, ' (':334, '\u2030\u00aa\u00a3\u00ca\u00f8':44014, '\u2030\u00aa\u2030\u222b\u222b':21935, '\u00c2\u00d8':753, '\u00b4':104, '\u2030\u03a9':1149, '\u00c2':6418, '':214, '\u2030\u222b\u222b':630, '\u201e':398, ')':8, '':185, '2':17, '0':15, '.':13, ' ':207, '\u2030\u00aa\u00a3':2404, '\u00ca':2509, ' (#':26580, '\u2030\u00aa\u00a3\u00ca\u00f8':44014, ' ':207, '\u00c2\u00b6\u2030\u220f':15115, '\u2030\u222b\u222b':630, '#':2, '\u00ca':2509, '\u00ca\u00a9':16696, '':240, '\u201e':398, ')':8, '':185, '2':17, '1':16, '.':13, ' ':207, '\u00cb\u2202':7446, '\u00c1\u00a2':20092, ' (':334, '\u00c1\u00ae':1082, '\u00c2':27766, '':240, '\u00c8\u00a3':1570, '\u2264':110, '\u00cb\u2202':7446, '\u00c2':6418, '':214, '\u00c1\u00a2':20092, '\u00d4\u00ba':19304, '\u00c8\u00c2\u220f\u220f':18867, '\u00ca':392, '':212, '\u00c1\u00cb\u00df\u00a3':12814, '\u00c1':1287, '\u222b':118, '\u00c1\u00b1':2890, '\u00c8\u2202\u00c1\u2211':59142, '\u00c1':8505, '':227, '\u00cb\u00a3':2566, '\u03a9':121, '\u00cb\u00ca':50157, '\u00c2':6418, '':214, '\u00ca\u2022\u00ca\u00a8':10162, '\u00cb\u2202':7446, '\u00c8':1759, '\u00c8\u00c2\u2211':68745, '\u201e':398, ')':8, '':185, '2':17, '2':17, '.':13, ' ':207, '\u00cb\u2202':7446, '\u00c8':12307, ' (':334, '\u00c2\u221e':519, '':216, '\u00cb\u2202':7446, '\u00cb':4916, '':218, '\u00c2':1507, '\u00cb\u2202':7446, '\u00ca\u03c0':3649, '\u00d8':107, '\u00c2':1289, '\u00c8':85824, '':220, '\u00c2':6418, '':214, '\u00c8\u00c2\u2211':68745, '\u00d4\u00ba':19304, '\u00c2\u00a7\u00df\u00ca\u00b6':21373, '\u00ca':609, '\u00c2':434, '\u00a9':102, '\u00c1\u00ae':1730, '\u00c6':106, '\u00c1\u00ae\u00ca\u2265':76833, '\u00d4\u00ba':19304, '\u2030\u220f':505, '\u00c1\u00ae':1730, '\u00c6':106, '\u2030\u00f8':1056, '':211, '\u00c2\u00d8\u2030\u00aa\u2022':1876, '\u00cb\u00a3':2566, '':238, '\u2030\u03a9':4222, '\u00cb\u2202':7446, '\u00cb':4916, '':218, '\u00d4\u00ba':19304, '\u00ca\u00e6\u00c2\u2022':20431, '\u00ca\u2264':931, '':231, '\u00ca\u2265\u00b0':11584, '\u00c2\u00ae':4625, '\u00c2\u2211':3475, '\u00c2\u2022':1895, '\u00c8\u00a2':1359, '\u00d4\u00ba':19304, '\u00c2\u221e':923, '\u00cb\u2202':7446, '\u00ca\u03c0':3649, '\u00d8':107, '\u00ca\u2264':931, '':231, '\u00c2\u2022\u03a9':1250, '\u00c2':6418, '':214, '\u00ca':1528, '':211, '\u00c2':2914, '\u00c2\u221e\u00b1\u00c2\u00d8\u2030\u00aa\u2022':11847, '\u00ca':98307, '\u00cb\u00b5\u2211':1762, '\u00c2':6418, '':214, '\u00d4\u00ba':2000, '\u00cb':1306, '\u00c2\u00b6\u2030\u220f':15115, '\u00c1\u00ae':1730, '\u00c6':106, '\u00c2\u221e\u00b1':941, '\u2030\u00f8':1056, '':211, '\u00c2\u221e\u00b1':941, '\u00c2':835, '':210, '\u2030\u220f':505, '\u00c2':1399, '':220, '\u00c1\u00d8':7739, '\u00a9':102, '\u00ca\u00cb':5848, '\u00c1':161, '\u2202':114, '\u2264':110, '\u00d4\u00ba':19304, '\u2030\u00f8':1056, '\u00e6':122, '\u2030\u222b\u222b':630, '\u00ca':5527, '\u222b':118, '\u00c2':2169, '\u222b':118, '\u00cb\u2202':7446, '\u00ca\u00d8':11837, '\u00ca\u00cb':5848, '\u00cb\u2202':7446, '\u00c2\u00a3':2406, '\u222b':118, '\u2030\u220f\u00c8\u00a2':18810, '\u00d4\u00ba':19304, '\u00c2':2169, '\u222b':118, '\u00ca':599, '':240, '\u00cb\u2202':7446, '\u00c2':6418, '':214, '\u00ca':1528, '':211, '\u00c2':2914, '\u00c2':2996, '\u00c2\u221e':519, '':216, '\u00cb\u2202':7446, '\u00cb':4916, '':218, '\u00c2':1507, '\u00cb\u2202':7446, '\u00ca\u03c0':3649, '\u00d8':107, '\u00c2':1289, '\u00c8':1660, '\u00a2':95, '\u201e':398, ')':8, '':185, '2':17, '3':18, '.':13, ' ':207, '\u00c2\u2122':2141, '\u00cb\u00ae':4564, '\u00b1':109, '\u00c2\u2211':4477, '\u00c2\u00c6':6066, '\u00ca\u00e6':2660, '\u00c1\u00b4':4243, '\u00d4\u00ba':19304, '\u2030\u220f\u00c2':75710, '\u00c1\u00e6\u00c2\u00df':21362, '\u00c8\u00aa':2588, '':239, '\u00c1':8505, '':217, ' (':334, '\u00ca\u00d8\u00c2\u00aa':72451, '\u00ca\u00a8':1137, '':219, '\u00cb\u2264':16434, '\u00a5':112, '\u00c8':1688, '':223, '\u00c2\u00b1':884, '\u00a7':97, '\u00c2\u00d8\u2030\u00aa\u2022':1876, '\u00c1':1287, '\u222b':118, '\u00ca\u00ca\u00a8\u2264':90104, '\u00c1':1287, '\u222b':118, '\u00d4\u00ba':19304, '\u00cb\u00c1\u00e6\u00c2\u00df':36759, '\u00c2':6418, '':214, '\u00ca\u2260\u00a3':2206, '\u00c1':1920, '\u2202':114, '\u00cb\u00ae\u00cb\u00b0':92872, '\u00c2\u221e\u00b1':941, '\u00c2\u00c2\u221e':10542, '\u00c1\u00ae':1730, '\u00c6':106, '\u00c1\u00ae':1730, '\u00c6':106, '\u00c8\u00c2\u2202':19680, '\u00d4\u00ba':2000, '\u00c2':3127, '\u00ca':2322, '\u2030\u220f':505, '\u00c2':1399, '':220, '\u2030\u222b\u222b':630, '\u00cb\u00ae\u00cb\u00b0':92872, '\u00c8':1660, '':234, '\u00c8':1737, '\u00ca\u00ae':3874, '':234, '\u00ca\u222b':3094, '':231, '\u201e':398, '\u00ca\u222b':3931, '\u00ca':599, '\u00ba':120, '\u00c2\u00c6':14504, '\u00ca':8702, '\u2030\u220f':505, '\u00c2':1399, '':220, '\u00c2\u2211':4477, '\u00c8':1355, '\u2211':115, '\u00c2\u00b4':7848, '\u00c1\u221e':9173, '\u00c1\u00aa':8329, '\u00d4\u00ba':19304, '\u00cb\u00b6\u00ca\u00b1':5886, '\u00c1\u00e6\u00c2\u00df':21362, '\u00c8\u00f8':8978, '\u00cb':162, '\u00b4':104, '\u00b1':109, '\u00c2':3077, '':229, '\u00c2':3711, '\u00c1\u00ae':1082, '\u201e':17194, '\u00c1\u00aa':8329, '\u201e':17335, '\u00c2\u2260':4416, '\u00c2':1507, '\u00c8\u2265':5431, '\u00c2':6418, '':214, '\u00c2\u2260':4416, '\u00d4\u00ba':19304, '\u00ca':599, '\u00ba':120, '\u00ca\u00d8':504, '\u00c2\u00a7\u00df\u00c2\u00c6\u2202':4145, '\u00c2\u221e\u00b1':941, '\u00c2\u221e':519, '':216, '\u201e':17194, '\u00c1':8505, '':217, '\u201e':17335, '\u00ca\u03c0':3520, '\u00c1\u00ae':1730, '\u00b1':109, '\u00c1':1287, '\u222b':118, '\u201e':17194, '\u00c1\u00b4':4243, '\u201e':17335, '\u201e':398, '\u00c2\u221e':923, '\u00c2':835, '':232, '\u00c2\u00c2\u00c6\u00b5':86426, '\u00c1\u00d8':7739, '':209, '\u00c2':6418, '':214, '\u00ca':1528, '':211, '\u00c2':2914, '\u00d4\u00ba':19304, '\u00ca\u00f8\u00c2\u222b':7076, '\u00cb\u2264':16434, '\u00ba':120, '\u00c2\u222b':1029, '\u00c2':4072, '\u00c1\u00a7\u222b':3198, '\u00d4\u00ba':45326, '\u201e':17194, '\u00ca\u00a8':1641, '\u00c2\u2211':4477, '\u2030\u00e6':4869, '\u2030\u00e6':5346, '\u00ca\u00e6':2660, '\u00c1\u00b4':4243, '\u2030\u220f':1937, '\u00ca\u2022':1479, '\u201e':17335, '\u201e':398, ')':8, '':185, '2':17, '4':19, '.':13, ' ':207, '\u00c1':87306, '\u00ae':101, '\u00c2\u00b1':12660, ' (':334, '\u2030\u220f':505, '\u00c2':1399, '':220, '\u2030\u222b\u222b':630, '\u00c2\u00b1':12660, '\u00d4\u00ba':19304, '\u00c2':3077, '':229, '\u2030\u00f8':1056, '':211, '\u00c2':1507, '\u2030\u222b\u222b':630, '\u2030\u220f':505, '\u00c8\u03a9':9543, '':219, '\u00c2\u00b1':12660, ')':8, '':185, '2':17, '5':20, '.':13, ' ':207, '\u00c2\u00f8\u00b4':3411, '\u2030\u222b\u222b':630, '\u00c2\u00f8\u00b4':3411, '\u2030\u222b':1609, ' (':334, '\u00c1\u03a9':19653, '\u00c2\u00f8\u00b4':3411, '\u2030\u222b\u222b':630, '\u00c2':2089, '\u00c2':6418, '\u00a2':95, '\u00c1\u03a9':19653, '\u00c2\u00f8\u00b4':3411, ')':8, '':185, '2':17, '6':21, '.':13, ' ':207, '\u00c2':3381, '\u00cb':3173, '\u2265':111, '\u00c1':87306, '\u220f':116, ' (':334, '\u00ca':3340, '\u00c2':434, '\u00a9':102, '\u00c2':1399, '':220, '\u2030\u222b\u222b':630, '\u00c2':2169, '\u222b':118, '\u00c2\u00a8':1429, '\u00c2\u00aa':1836, '':210, '\u00c2\u2022':1895, '\u00c8\u00a2':1359, '\u00cb\u00b6':15378, '\u2122':103, '\u00c1':5354, '\u00b1':109, '\u00c1\u00cb\u2265':7214, '\u00c2':2089, '\u00ca':1101, '':236, '\u201e':398, '\u00c2':2169, '\u222b':118, '\u00c2\u00aa':1836, '':210, '\u00ca':1556, '\u00c2\u00a7\u00c8\u00a2':42890, '\u00c8':85824, '':209, '\u00c1':161, '\u220f':116, '\u00b4':104, '\u00ca':4279, '\u00cb\u03a9':4744, '\u00c2\u00aa':1827, '\u00d4\u00ba':19304, '\u00ca':392, '':212, '\u00cb\u00b6':15378, '':220, '\u00c2\u221e':923, '\u00c2':3381, '\u00c8':1688, '\u00aa':119, '\u00cb':3173, '\u2265':111, '\u00d4\u00ba':19304, '\u00c2\u2020\u00ca\u2260\u00a7':7058, '\u00c1\u00ae':1730, '\u00b1':109, '\u00c1':1287, '\u222b':118, '\u00c2':3381, '\u00cb':3173, '\u2265':111, '\u00c1':87306, '\u220f':116, '\u201e':398, ')':8, '':185, '2':17, '7':22, '.':13, ' ':207, '\u00c1':1541, '\u00b0':94, '\u00c1\u00b1\u2265':4428, '\u00c1\u2264\u2022':30240, ' (':334, '\u00c2\u00ca\u2260\u2022':38454, '\u00c2':6418, '':214, '\u00ca\u00a5\u03a9':51603, '\u00cb':162, '\u00b4':104, '':216, '\u201e':398, '\u00c2\u2020':1731, '\u00c1':1287, '\u222b':118, '\u2030\u222b\u00ca':12196, '\u00ca':1556, '\u00ca\u2202':10470, '\u00c2':6418, '':214, '\u00c8\u00cb\u00b6':5097, '\u00ca\u00a2':3703, '':238, '\u2030\u00aa\u2202':2880, '\u00c2':1507, '\u00c1\u00a5':2214, '\u221e':108, '\u00c1\u00d8':7739, '':209, '\u00c8\u03a9':1223, '\u00ca\u2122':3918, '\u00ca':609, '\u00cb':10786, '\u00cb\u03a9':4744, '\u00d4\u00ba':19304, '\u00ca\u2030\u00aa\u2022':3705, '\u00c1':1287, '\u222b':118, '\u00cb':162, '\u00a8':105, '\u03c0':117, '\u00ca':22401, '\u00cb\u00ae':4564, '':217, '\u00d4\u00ba':19304, '\u00c2\u2122\u00ca\u00d8':7553, '\u00cb\u00a9':29703, '\u00b1':109, '\u201e':17194, '\u00c1\u2264':62864, '\u00c1':1541, '\u00b0':94, '\u00c1\u00b1\u2265':4428, '\u00c1\u2264\u2022':30240, '\u201e':17335, '\u00d4\u00ba':19304, '\u2030\u00aa\u2022\u00c2':37221, '\u2030\u00aa\u2030\u222b\u222b':21935, '\u00ca\u00ca':43175, '\u00c8':1249, '':223, '\u00c8\u00b4':1331, '\u201e':398, ')':8, '':185, '2':17, '8':23, '.':13, ' ':207, '\u00c1\u00e6':1455, '\u00a9':102, '\u00ca':392, '':212, ' (':334, '\u00cb':2230, '':219, '\u00ca':1528, '':211, '\u2030\u03a9\u2030\u220f':82738, '\u00c8':1688, '':223, '\u00c2\u00b1':884, '\u00a7':97, '\u00c2':6418, '':214, '\u00c2':7882, '\u00cb\u2264':16434, '\u220f':116, '\u00ca\u00cb':5848, '\u00c8':3771, '\u00cb':162, '\u2265':111, '':216, '\u00ca\u00a5\u00aa':2104, '\u00c2':4679, '':230, '\u201e':398, '\u00c2\u2020':1731, '\u00c1':1287, '\u222b':118, '\u2030\u03a9':474, '\u00a2':95, '\u00c2\u221e':1186, '\u00c2':723, '':216, '\u00ca\u00b5\u00ca\u00ba':39437, '\u00ca\u00cb':5848, '\u00ca':5527, '':229, '\u2030\u00f8':2067, '\u00d4\u00ba':19304, '\u00c8':1660, '\u00a3':96, '\u2030\u00aa\u2022':851, '\u00c2':2657, '\u00c8':163, '':219, '':209, '\u00cb\u00b0':1127, '\u00c2':7882, '\u00c8':163, '':221, '\u00a2':95, '\u00d4\u00ba':19304, '\u00ca':599, '\u00ba':120, '\u00ca\u00d8':504, '\u00c2\u221e\u00b1':941, '\u00c8\u00c2':57359, '\u00ca\u00c2':5839, '\u201e':537, '\u00c8':33948, '\u221e':108, '\u00c2\u00b1':5528, '\u201e':537, '\u00c2':1507, '\u00ca\u2022':5126, '\u2260':242, '\u00c1\u2260':1537, '\u00d4\u00ba':19304, '\u00c2\u221e':519, '':216, '\u00c2\u00a7\u00df\u00c2\u00c6\u2202':4145, '\u00c2':6418, '':214, '\u00c8':163, '':221, '\u00a2':95, '\u00c2\u00c6\u00ca':31579, '\u00c8\u00c2':57359, '\u00cb\u00b5\u2211':1762, '\u00c2':27766, '':240, '\u2030\u00e6':4068, '\u00ca':392, '':212, '\u00d4\u00ba':19304, '\u00c8':1618, '':219, '\u00c2':1399, '':220, '\u00ca\u2022\u00c8':77404, '\u00c1\u00ae':1082, '\u00c8':163, '':221, '\u00a2':95, '\u00d4\u00ba':19304, '\u00c2\u221e\u00b1\u00c2\u00d8\u2030\u00aa\u2022':11847, '\u00c1':1920, '':209, '\u00c2\u00a9\u00ca\u00d8':40741, '#':2, '\u00ca\u00ae':3874, '':234, '\u00ca':392, '':212, '\u00d4\u00ba':19304, '\u00ca':98307, '\u00c2':835, '':232, '\u00c1\u2260':1133, '':215, '\u00c8':163, '':221, '\u00a2':95, '\u00d4\u00ba':19304, '\u00c1\u2202':1654, '\u00c2\u00e6':667, '':221, '\u00c2\u00ca':80835, '\u00ca\u00a8':1641, '\u00ca\u00d8':3714, '\u00c8':1618, '':213, '\u00cb\u00f8':14473, '\u00c1':1920, '':209, '\u00c2\u2202\u2030\u00aa\u2030\u222b\u222b':60594, '\u201e':398, '\u00c1':1920, '\u2202':114, '\u00ca\u00ca\u2030\u222b\u222b\u00c8\u03a9':87647, '\u00ca':3532, '\u00c2\u00c6':3091, '\u00ca\u00ae':3874, '':234, '\u00d4\u00ba':19304, '\u00c2':1399, '':220, '\u00ca':392, '':212, '\u00c2\u221e\u00b1':941, '\u00ca':392, '':212, '\u00c1\u00b5':55361, '':225, '\u00ca':10624, '\u201e':398, '\u00c2':3127, '\u00c1\u00ae':1730, '\u00b1':109, '\u00ca\u00ae':3874, '':234, '\u00ca':392, '':212, '\u201e':537, '\u00c8':163, '':219, '':209, '\u00ca':392, '':212, '\u201e':537, '\u00c2':1633, '\u00ca':392, '':212, '\u201e':398, ')':8, '':185, '2':17, '9':24, '.':13, ' ':207, '\u00ca\u00b6':16154, '\u00ca':1930, ' (':334, '\u00cb\u2264':16434, '\u00ba':120, '\u00c2':2169, '\u222b':118, '#':2, '\u00ca\u00b6':16154, ' ':207, '\u2030\u220f\u00c8\u00a2':18810, '\u00c2':6418, '':214, '\u00ca\u00c2\u2260':22152, ')':8, '':185, '3':18, '0':15, '.':13, ' MK':48536, '\u00c2':2217, ' (':334, '\u2030\u00f8':1056, '':211, '\u00ca':3340, '#':2, 'MK':68061, ' ':207, '\u00c2':2169, '\u222b':118, '\u00c1':161, '\u2202':114, '\u2264':110, '\u2030\u220f':816, '\u00ca':392, '':212, '\u00c1\u00ae':1082, '\u00c2':6418, '':214, '\u00c2':2217, '\u00d4\u00ba':19304, '\u00c8\u00ca':86126, '\u00ca\u00c2\u2265':18672, '\u2030\u220f\u00ca':56911, '\u00c2':6418, '':214, '\u00c1\u03c0\u00ca\u00c6':17065, '\u00c1\u00a8\u00b6':11265, '\u00cb':2628, '':240, '\u00d4\u00ba':19304, '\u00ca\u00cb':5848, '\u00c8\u2030\u220f':64414, '\u00c1\u03c0\u00c2\u00c6':50982, '\u00c2\u2260':4416, '\u00c8':163, '\u2020':241, '\u2260':242, '\u00c2':6418, '':214, '\u00c2':2217, '\u201e':398, '\u00c1\u03c0\u00c2\u00c6':50982, '\u00c2\u2260':4416, '\u00c8':163, '\u2020':241, '\u2260':242, '\u00c2\u00ca\u00a8':8344, '\u00d4\u00ba':45326, '\u00c2\u2020':5319, '':214, '\u00c1\u2265\u00aa':2370, '\u201e':537, '\u00c8':23138, '\u00ca':1101, '':236, '\u201e':537, '\u00c1\u00b5':55361, '':230, '\u00c2\u221e':519, '':222, '\u201e':537, '\u00c2\u00d8':12778, '\u00ca':1101, '':236, '\u201e':537, '\u00cb\u2211\u2265':12859, '\u00c2\u2202':2278, '\u201e':537, '\u00cb':162, '\u00a8':105, '':238, '\u00c1\u00b5':55361, '':230, '\u00c1\u2260':1537, '\u201e':398, ')':8, '':185, '':185, '####':3589, '':185, '':185, '1':16, '.':13, ' ':207, '\u00c8\u00a3':1570, '\u2264':110, '\u00cb\u2202':7446, '':185, '':185, '\u00c2\u00aa':1836, '\u00a3':96, '\u00ca':610, '\u00b1':109, '\u00cb\u00a9':29703, '\u00b1':109, '\u00d4\u00ba':45326, '\u00ca':852, '\u00c2\u221e':519, '':220, '\u00ca\u2022':1479, '\u00c2':1507, '\u00ca\u00c2':5839, '\u00c2\u00aa':1827, '\u00c8\u00a3':1570, '\u2264':110, '\u00cb\u2202':7446, '\u00d4\u00ba':19304, '\u00c2\u00b4':7848, '\u00c2':835, '':232, '\u00c2\u2022\u03a9\u00c2\u00a7':37592, '\u00c8\u00aa':2588, '':239, '\u00c2\u00f8':1346, '\u201e':398, '':185, 'English':18656, ':':25, ' I':304, ' went':2674, ' to':276, ' yum':81215, ' cha':19147, ' with':366, ' my':601, ' friends':3997, ' yesterday':11209, ' and':285, ' ordered':10133, ' a':245, ' lot':2603, ' of':280, ' dim':4165, ' sum':2555, '.':13, '':185, '':185, '2':17, '.':13, ' ':207, '\u00c2':477, '':231, '\u00ca':1352, '\u220f':116, '\u00c8':163, '\u00a7':97, '\u00ae':101, '':185, '':185, '\u00c2\u00aa':1836, '\u00a3':96, '\u00ca':610, '\u00b1':109, '\u00cb\u00a9':29703, '\u00b1':109, '\u00d4\u00ba':45326, '\u2030\u03a9\u2020':1372, '\u00c2\u00b1':11386, '\u2030\u00ba':3243, '\u00c8\u00cb\u00f8':18649, '\u00ca':609, '\u00c2':723, '':216, '\u00c2':477, '':231, '\u00ca':1352, '\u220f':116, '\u00c8':163, '\u00a7':97, '\u00ae':101, '\u00d4\u00ba':2224, '':185, 'English':18656, ':':25, ' Is':2128, ' there':745, ' a':245, ' library':7503, ' near':3345, ' your':520, ' home':1719, '?':30, '':185, '':185, '':185, 'Assistant':77398, ':':25, ' ':207 ]\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-05-30T18:20:52+00:00",
    "closed_at": "2024-06-14T14:14:11+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7652/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7652"
  },
  {
    "number": 290,
    "title": "alaways \"failed to tokenize string! \"",
    "body": "failed to tokenize string! \r\n\r\nsystem_info: n_threads = 16 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \r\nfailed to tokenize string!\r\n\r\nmain: prompt: ' china'\r\nmain: number of tokens in prompt = 1\r\n     1 -> ''\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\n\u66f2\u30fc\uff01 /S\u90e8\u30e5\u30fc\u30b9 / KSHErsLAheLUE - THE NEW CH`,MEgeERSION IS HERE@\u00ffThis entry was \u0432\u0435\u0440 in news on JuneSASSSASS8 by adminS [end of text]\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-03-19T11:29:50+00:00",
    "closed_at": "2023-04-07T16:15:34+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/290/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/290"
  },
  {
    "number": 8446,
    "title": "Bug: ggml-aarch64.c does not compile on Windows ARM64 with MSVC",
    "body": "### What happened?\r\n\r\nthe __asm__ directive in line 507 of ggml-aarch64.c\r\nis dependant on: defined(__ARM_NEON) && defined(__aarch64__) which is tue for MSVC arm64 with the standard build scripts and does not compile with MSVC.\r\n\r\nthe other __asm__ directives work out except for the one in line 1278, they are to not being used.\r\n\r\nbuilding with clang works, its MSVC specific.\r\n\r\n### Name and Version\r\n\r\nllama.cpp version: 3378 (71c1121d), Microsoft C/C++  Version 19.40.33812 for ARM64, Windows 11 Enterprise 24H2 Build 26100.1150\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\nWindows\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nbuild-instructions which fail:\r\ncmake -B build\r\ncmake --build build --config Release --target llama-cli\r\n\r\nCMake build also fails for arm64-windows-msvc.cmake, but works for arm64-windows-llvm.cmake (clang compile instead of MSVC frontend works)\r\n\r\nThe build worked a few days ago with older llama.cpp versions.\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-07-12T08:21:28+00:00",
    "closed_at": "2024-07-25T16:01:01+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8446/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8446"
  },
  {
    "number": 11866,
    "title": "Misc. bug: Problems with official jinja templates (Gemma 2, Llama 3.2, Qwen 2.5)",
    "body": "### Name and Version\n\nllama-cli --version\nversion: 4713 (a4f011e8)\nbuilt with MSVC 19.42.34436.0 for x64\n\n\n### Operating systems\n\nWindows\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Command line\n\n```shell\n1. llama-server -ngl 99 -m gemma-2-2b-it-Q8_0.gguf --jinja --chat-template-file gemma2.jinja -c 8192\n2. llama-server -ngl 99 -m Llama-3.2-3B-Instruct-Q8_0.gguf --jinja --chat-template-file llama3.2.jinja -c 8192\n3. llama-server -ngl 99 -m Qwen2.5-1.5B-Instruct-Q8_0.gguf --jinja --chat-template-file qwen2.5.jinja -c 8192\n```\n\n### Problem description & steps to reproduce\n\nExtracting official chat templates from chat_template field in tokenizer_config.json ([Gemma 2](https://huggingface.co/google/gemma-2-2b-it/blob/main/tokenizer_config.json#L2003), [Llama 3.2](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/blob/main/tokenizer_config.json#L2053), [Qwen 2.5](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct/blob/main/tokenizer_config.json#L198)), storing them in files, and then trying to use them with llama-server results in errors.\n\n1. Gemma 2: `parse: error parsing grammar: expecting name at` after each message.\n2. Llama 3.2: server doesn't start.\n3. Qwen 2.5: `parse: error parsing grammar: expecting name at` after each message.\n\n@ochafik Could you look into this? It would be nice to have jinja implementation fully working with official templates, at least for major models.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-02-14T09:57:12+00:00",
    "closed_at": "2025-03-10T10:40:33+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11866/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11866"
  },
  {
    "number": 6606,
    "title": "llm/llama.cpp/quantize - Mixtral model quantizatization halts at random step",
    "body": "I fine-tuned Mixtral with QLORA on my dataset and merged it. \r\n\r\nAfter cloning llamacpp, I built the quantization feature using `make -C llm/llama.cpp quantize`. \r\n\r\nThen, I converted the model using \r\n`python llm/llama.cpp/convert.py LLaMA-Factory/models/mixtral_instruct01_lora_4bit_full --outtype f16`. \r\n\r\nWhen attempting to quantize with \r\n`llm/llama.cpp/quantize converted.bin quantized.bin q4_0`\r\n\r\nthe process randomly freezes\u2014this could happen at step 6, anywhere past 100, or even beyond 500 steps. Despite several attempts, the issue persists without resolution.\r\n\r\nscreenshot \r\n![1111](https://github.com/ggerganov/llama.cpp/assets/23155849/dcd03305-e433-4c10-8174-5b7128b7a33f)\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-04-11T10:20:42+00:00",
    "closed_at": "2024-04-19T19:07:03+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6606/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6606"
  },
  {
    "number": 2379,
    "title": "llama : add support for llama2.c models",
    "body": "The new [llama2.c](https://github.com/karpathy/llama2.c) project provides means for training \"baby\" llama models stored in a custom binary format, with 15M and 44M models already available and more potentially coming out soon.\r\n\r\nWe should provide a simple conversion tool from `llama2.c` bin format to `ggml` format so we can run inference of the models in `llama.cpp`\r\n\r\nGreat task for people looking to get involved in the project\r\n\r\n",
    "labels": [
      "help wanted",
      "good first issue",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-07-24T20:15:39+00:00",
    "closed_at": "2023-08-11T23:17:27+00:00",
    "comments": 93,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2379/reactions",
      "total_count": 31,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 5,
      "confused": 0,
      "heart": 6,
      "rocket": 15,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2379"
  },
  {
    "number": 12991,
    "title": "Eval bug: HIP: llama.cpp server locks up when running multiple instances on the same gpu",
    "body": "### Name and Version\n\nllama-server version 5145 (but happens with older versions too)\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nHIP\n\n### Hardware\n\n```\nDevice 0: AMD Radeon VII, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64\nDevice 1: AMD Radeon RX 6900 XT, gfx1030 (0x1030), VMM: no, Wave Size: 32\n```\nRyzen 9 5900x ubuntu 24.04, rocm 6.3.3\n\n### Models\n\nAny completion with any embedding model\ne. g. Llama-3.2-1B-Instruct-Q8_0.gguf, mxbai-embed-large-v1_fp16.gguf\n\n\n### Problem description & steps to reproduce\n\n1) Running 2 instances\n```\n~/llama.cpp/build/bin/llama-server -m /models/llm_models/Llama-3.2-1B-Instruct-Q8_0.gguf -c 32768 -fa -ngl 200 --port 5001 --no-mmap --mlock --host 0.0.0.0\n```\n```\n ~/llama.cpp/build/bin/llama-server -m /models/llm_models/mxbai-embed-large-v1_fp16.gguf --embedding -ngl 200 -c 512 -b 512 -ub 512 -fa --host 0.0.0.0 --port 5002 --no-mmap --mlock\n```\ndevice selection doesn't matter\n\n2) Vectorize some messages or anything else with embedding model\n3) Try to process any long (>1 batch) piece of text in completion model\n4) Prompt processing doesn't seem to progress, until embedding instance is terminated (no matter if it is idle), GPUs either reported as idle or fully loaded, but with very low power consumption\n\n### First Bad Commit\n\nidk, but older versions e. g. 4702 do have the issue\n\n### Relevant log output\n\n```shell\nNothing unusual happens in logs, processing just doesn't progress\n...\nmain: server is listening on http://0.0.0.0:5001 - starting the main loop\nsrv  update_slots: all slots are idle\nsrv  log_server_r: request: POST /tokenize 192.168.1.33 200\nsrv  log_server_r: request: POST /tokenize 192.168.1.33 200\nslot launch_slot_: id  0 | task 0 | processing task\nslot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 32768, n_keep = 0, n_prompt_tokens = 23863\nslot update_slots: id  0 | task 0 | kv cache rm [0, end)\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 2048, n_tokens = 2048, progress = 0.085823\nsrv  cancel_tasks: cancel task, id_task = 0\nsrv  log_server_r: request: POST /completion 192.168.1.33 200\nslot      release: id  0 | task 0 | stop processing: n_past = 2048, truncated = 0\nsrv  update_slots: all slots are idle\n^Csrv    operator(): operator(): cleaning up before exit...\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-04-17T04:38:58+00:00",
    "closed_at": "2025-04-24T15:15:30+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12991/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12991"
  },
  {
    "number": 743,
    "title": "[Work Group] Add RLHF like ColosallChat on bigger dataset to achieve ChatGPT quality",
    "body": "[Link to ColosallChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat\r\n)\r\n# Add RLHF like ColosallChat on bigger dataset to achieve ChatGPT quality\r\n\r\n![lama-alpaca](https://user-images.githubusercontent.com/84633629/229592343-36b95e81-2f85-4fa0-9a5a-edd5267bb190.gif)\r\n\r\n\r\nAlthough models in the GPT series, such as ChatGPT and GPT-4, are highly powerful, they are unlikely to be fully open-sourced. Fortunately, the open-source community has been working hard to address this.\r\n\r\nFor example, Meta has open-sourced the LLaMA model, which offers parameter sizes ranging from 7 billion to 65 billion. A 13 billion parameter model can outperform the 175 billion GPT-3 model on most benchmark tests. However, since it doesn\u2019t have an instruct tuning stage, its actual generated results are not satisfactory.\r\n\r\nStanford\u2019s Alpaca generates training data in a self-instructed manner by calling OpenAI\u2019s API. With only 7 billion parameters, this lightweight model can be fine-tuned at a fraction of the cost to achieve conversational performance similar to a very large language model like GPT-3.5 with 175 billion parameters.\r\n\r\nHowever, existing open-source solutions can only be considered as supervised fine-tuned models in the first stage of RLHF (Reinforcement Learning from Human Feedback), with subsequent alignment and fine-tuning stages not performed. Additionally, Alpaca\u2019s training dataset is limited to English, which to some extent restricts the model\u2019s performance.\r\n\r\nYet, the impressive effects of ChatGPT and GPT-4 are due to the introduction of RLHF into the training process, which increases the consistency of the generated content with human values.\r\n\r\n![image](https://user-images.githubusercontent.com/84633629/229590135-fc856c3f-b138-4fad-842e-e0bf34ffd69f.png)\r\n\r\n## Training Dataset Open Source\r\nColossalChat releases a bilingual dataset comprising approximately 100,000 Q&A pairs in both English and Chinese. The dataset was collected and cleaned from real-life question scenarios on social media platforms, serving as the seed dataset, and was expanded using self-instruct technology, and annotation costs were approximately $900. Compared to datasets generated by other self-instruct methods, this dataset contains more realistic and diverse seed data and encompasses a wider range of topics. The dataset is suitable for both fine-tuning and RLHF training. With the provision of high-quality data, ColossalChat can achieve better dialogue interactions and also support Chinese.\r\n\r\n![image](https://user-images.githubusercontent.com/84633629/229590363-a1ab7e84-27af-4e4a-bbf1-47a3a113bcd1.png)\r\n\r\n## RLHF Algorithm Replication\r\n\r\nThe RLHF algorithm replication involves three stages:\r\n\r\nIn RLHF-Stage1, supervised instruct fine-tuning is performed using the datasets mentioned earlier to fine-tune the model.\r\n\r\nIn RLHF-Stage2, a reward model is trained to assign corresponding scores by manually ranking different outputs for the same prompt, which then supervises the training of the reward model.\r\n\r\nIn RLHF-Stage3, the reinforcement learning algorithm is being used, which is the most complex part of the training process:\r\n\r\n![image](https://user-images.githubusercontent.com/84633629/229590420-9b4224b5-c014-4782-ba97-2d754a5c842f.png)\r\n\r\nIn the PPO part, ColossalChat follows a two-stage process: first, the make experience stage, which uses SFT (Supervised Fine-Tuning), Actor, RM (Reward Model), and Critic models to calculate generated experience and store it in the buffer. Then comes the parameter update stage, which calculates the policy loss and value loss using the experience.\r\n\r\nIn the PTX part, ColossalChat calculates the cross-entropy loss between the Actor\u2019s output response and the response part of the input corpus. This loss is used to add pre-training gradients to the PPO gradient to maintain the language model\u2019s original performance and prevent forgetting. Finally, the policy loss, value loss, and PTX loss are summed up for backpropagation and parameter update.\r\n\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-03T18:02:06+00:00",
    "closed_at": "2023-04-13T08:55:27+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/743/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/743"
  },
  {
    "number": 4086,
    "title": "parallel.cpp exits when encountering a long prompt.",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nIn ./examples/parallel/parallel.cpp, I added the following two lines to the final output:\r\n```cpp\r\nint cache_count = llama_get_kv_cache_token_count(ctx);\r\nLOG_TEE(\"Cache KV size %d\", cache_count);\r\n```\r\nI believe that the logic in line 221 of parallel.cpp:\r\n```cpp\r\n// all sequences have ended - clear the entire KV cache\r\nfor (int i = 0; i < n_clients; ++i) {\r\n   llama_kv_cache_seq_rm(ctx, i, n_tokens_system, -1);\r\n}\r\n```\r\nshould release all the occupied cache when the entire task is completed. However, in reality, it does not release the cache.\r\n\r\n# Current Behavior\r\n\r\nI expected the value of cache_count to be 0, but in reality, it is 1153.\r\n\r\n# Environment and Context\r\n\r\n1. GPU info\r\n```shell\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 525.85.05    Driver Version: 525.85.05    CUDA Version: 12.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro RTX 8000     Off  | 00000000:3B:00.0 Off |                  N/A |\r\n| 33%   29C    P8    22W / 260W |      0MiB / 49152MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n2. make cmd\r\n```shell\r\nLLAMA_CUDA_NVCC=/usr/local/cuda-12/bin/nvcc make LLAMA_CUBLAS=1 -Wdeprecated-declarations\r\n```\r\n\r\n3. test cmd\r\n```shell\r\n./parallel -m ./CodeLlama-7B/ggml-model-q8_0.gguf -n -1 -c 16324 -b 4096 --cont_batching --parallel 10 --sequences 600 --n-gpu-layers 1000\r\n```\r\n\r\n4. model\r\n\r\nIt appears that you are using the \"CodeLlama-7B-HF\" model from the repository you mentioned (https://huggingface.co/codellama/CodeLlama-7b-hf). You mentioned that you performed the conversion using the \"convert.py\" script included in the repository.\r\n\r\n`$ lscpu`\r\n```shell\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   46 bits physical, 48 bits virtual\r\nCPU(s):                          64\r\nOn-line CPU(s) list:             0-63\r\nThread(s) per core:              2\r\nCore(s) per socket:              16\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           85\r\nModel name:                      Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz\r\nStepping:                        7\r\nCPU MHz:                         1693.308\r\nCPU max MHz:                     3900.0000\r\nCPU min MHz:                     1000.0000\r\nBogoMIPS:                        4600.00\r\nVirtualization:                  VT-x\r\nL1d cache:                       1 MiB\r\nL1i cache:                       1 MiB\r\nL2 cache:                        32 MiB\r\nL3 cache:                        44 MiB\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`$ uname -a`\r\n```shell\r\nLinux studio-0 4.19.96 #1 SMP Tue Mar 10 10:34:01 CST 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version 3.11.5\r\n$ make --version  GNU Make 4.2.1\r\n$ g++ --version  g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nI expected the value of cache_count to be 0, but in reality, it is 1153.\r\n\r\n# Steps to Reproduce\r\n\r\nplease download model from https://huggingface.co/codellama/CodeLlama-7b-hf \r\n1. python convert.py ./CodeLlama-7B/ --outtype q8_0\r\n2. after parallel.cpp's LOG_TEE(\"Cache misses:        %6d\\n\", n_cache_miss); add\r\n```cpp\r\nint cache_count = llama_get_kv_cache_token_count(ctx);\r\nLOG_TEE(\"Cache KV size %d\", cache_count);\r\n```\r\n3. ./parallel -m ./CodeLlama-7B/ggml-model-q8_0.gguf -n -1 -c 16324 -b 4096 --cont_batching --parallel 10 --sequences 600 --n-gpu-layers 1000\r\n\r\n# Failure Logs\r\n\r\nExample run with the Linux command [perf](https://www.brendangregg.com/perf.html)\r\n```\r\nggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\r\nggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\r\nggml_init_cublas: found 1 CUDA devices:\r\n  Device 0: Quadro RTX 8000, compute capability 7.5\r\nllama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /aistudio/workspace/system-default/models/CodeLlama-7B/ggml-model-q8_0.gguf (version GGUF V3 (latest))\r\nllama_model_loader: - tensor    0:                token_embd.weight q8_0     [  4096, 32016,     1,     1 ]\r\nllama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor    2:            blk.0.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor    4:              blk.0.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor    6:              blk.0.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor    7:         blk.0.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor    8:              blk.0.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor    9:              blk.0.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   11:            blk.1.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   13:              blk.1.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   15:              blk.1.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   16:         blk.1.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   17:              blk.1.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   18:              blk.1.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   20:           blk.10.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   22:             blk.10.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   24:             blk.10.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   25:        blk.10.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   26:             blk.10.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   27:             blk.10.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   29:           blk.11.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   31:             blk.11.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   33:             blk.11.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   34:        blk.11.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   35:             blk.11.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   36:             blk.11.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   38:           blk.12.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   40:             blk.12.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   42:             blk.12.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   43:        blk.12.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   44:             blk.12.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   45:             blk.12.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   47:           blk.13.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   49:             blk.13.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   51:             blk.13.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   52:        blk.13.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   53:             blk.13.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   54:             blk.13.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   56:           blk.14.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   58:             blk.14.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   60:             blk.14.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   61:        blk.14.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   62:             blk.14.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   63:             blk.14.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   65:           blk.15.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   67:             blk.15.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   69:             blk.15.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   70:        blk.15.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   71:             blk.15.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   72:             blk.15.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   74:           blk.16.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   76:             blk.16.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   78:             blk.16.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   79:        blk.16.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   80:             blk.16.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   81:             blk.16.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   83:           blk.17.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   85:             blk.17.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   87:             blk.17.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   88:        blk.17.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   89:             blk.17.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   90:             blk.17.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   92:           blk.18.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   94:             blk.18.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   96:             blk.18.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   97:        blk.18.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   98:             blk.18.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   99:             blk.18.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  101:           blk.19.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  103:             blk.19.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  105:             blk.19.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  106:        blk.19.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  107:             blk.19.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  108:             blk.19.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  110:            blk.2.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  112:              blk.2.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  114:              blk.2.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  115:         blk.2.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  116:              blk.2.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  117:              blk.2.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  119:           blk.20.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  121:             blk.20.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  123:             blk.20.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  124:        blk.20.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  125:             blk.20.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  126:             blk.20.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  128:           blk.21.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  130:             blk.21.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  132:             blk.21.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  133:        blk.21.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  134:             blk.21.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  135:             blk.21.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  137:           blk.22.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  139:             blk.22.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  141:             blk.22.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  142:        blk.22.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  143:             blk.22.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  144:             blk.22.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  146:           blk.23.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  148:             blk.23.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  150:             blk.23.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  151:        blk.23.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  152:             blk.23.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  153:             blk.23.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  155:            blk.3.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  157:              blk.3.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  159:              blk.3.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  160:         blk.3.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  161:              blk.3.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  162:              blk.3.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  164:            blk.4.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  166:              blk.4.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  168:              blk.4.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  169:         blk.4.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  170:              blk.4.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  171:              blk.4.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  173:            blk.5.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  175:              blk.5.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  177:              blk.5.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  178:         blk.5.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  179:              blk.5.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  180:              blk.5.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  182:            blk.6.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  184:              blk.6.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  186:              blk.6.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  187:         blk.6.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  188:              blk.6.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  189:              blk.6.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  191:            blk.7.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  193:              blk.7.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  195:              blk.7.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  196:         blk.7.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  197:              blk.7.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  198:              blk.7.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  200:            blk.8.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  202:              blk.8.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  204:              blk.8.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  205:         blk.8.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  206:              blk.8.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  207:              blk.8.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  209:            blk.9.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  211:              blk.9.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  213:              blk.9.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  214:         blk.9.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  215:              blk.9.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  216:              blk.9.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  217:                    output.weight q8_0     [  4096, 32016,     1,     1 ]\r\nllama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  219:           blk.24.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  221:             blk.24.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  223:             blk.24.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  224:        blk.24.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  225:             blk.24.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  226:             blk.24.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  228:           blk.25.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  230:             blk.25.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  232:             blk.25.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  233:        blk.25.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  234:             blk.25.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  235:             blk.25.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  237:           blk.26.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  239:             blk.26.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  241:             blk.26.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  242:        blk.26.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  243:             blk.26.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  244:             blk.26.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  246:           blk.27.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  248:             blk.27.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  250:             blk.27.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  251:        blk.27.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  252:             blk.27.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  253:             blk.27.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  255:           blk.28.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  257:             blk.28.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  259:             blk.28.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  260:        blk.28.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  261:             blk.28.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  262:             blk.28.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  264:           blk.29.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  266:             blk.29.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  268:             blk.29.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  269:        blk.29.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  270:             blk.29.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  271:             blk.29.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  273:           blk.30.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  275:             blk.30.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  277:             blk.30.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  278:        blk.30.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  279:             blk.30.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  280:             blk.30.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  282:           blk.31.ffn_down.weight q8_0     [ 11008,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  284:             blk.31.ffn_up.weight q8_0     [  4096, 11008,     1,     1 ]\r\nllama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  286:             blk.31.attn_k.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  287:        blk.31.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  288:             blk.31.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  289:             blk.31.attn_v.weight q8_0     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - kv   0:                       general.architecture str     \r\nllama_model_loader: - kv   1:                               general.name str     \r\nllama_model_loader: - kv   2:                       llama.context_length u32     \r\nllama_model_loader: - kv   3:                     llama.embedding_length u32     \r\nllama_model_loader: - kv   4:                          llama.block_count u32     \r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32     \r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32     \r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32     \r\nllama_model_loader: - kv  11:                          general.file_type u32     \r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str     \r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr     \r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr     \r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr     \r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32     \r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q8_0:  226 tensors\r\nllm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32016\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 16384\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 11008\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 16384\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = mostly Q8_0\r\nllm_load_print_meta: model params     = 6.74 B\r\nllm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \r\nllm_load_print_meta: general.name   = models\r\nllm_load_print_meta: BOS token = 1 '<s>'\r\nllm_load_print_meta: EOS token = 2 '</s>'\r\nllm_load_print_meta: UNK token = 0 '<unk>'\r\nllm_load_print_meta: LF token  = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.11 MB\r\nllm_load_tensors: using CUDA for GPU acceleration\r\nllm_load_tensors: mem required  =  132.99 MB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 35/35 layers to GPU\r\nllm_load_tensors: VRAM used: 6695.89 MB\r\n....................................................................................................\r\nllama_new_context_with_model: n_ctx      = 16324\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init: offloading v cache to GPU\r\nllama_kv_cache_init: offloading k cache to GPU\r\nllama_kv_cache_init: VRAM kv self = 8162.00 MB\r\nllama_new_context_with_model: kv self size  = 8162.00 MB\r\nllama_build_graph: non-view tensors processed: 740/740\r\nllama_new_context_with_model: compute buffer total size = 8615.72 MB\r\nllama_new_context_with_model: VRAM scratch buffer: 8609.09 MB\r\nllama_new_context_with_model: total VRAM used: 23466.99 MB (model: 6695.89 MB, context: 16771.09 MB)\r\n\r\nNo new questions so proceed with build-in defaults.\r\n\r\n\r\nmain: Simulating parallel requests from clients:\r\nmain: n_parallel = 10, n_sequences = 600, cont_batching = 1, system tokens = 305\r\n\r\n\r\n\u3002\u3002\u3002\u3002\u3002\r\n\r\nmain: clearing the KV cache\r\n\r\nrun parameters as at 2023-11-15 10:43:39\r\n\r\nmain: n_parallel = 10, n_sequences = 600, cont_batching = 1, system tokens = 305\r\nExternal prompt file: used built-in defaults\r\nModel and path used:  ./CodeLlama-7B/ggml-model-q8_0.gguf\r\n\r\nTotal prompt tokens:   8752, speed: 36.53 t/s\r\nTotal gen tokens:     29266, speed: 122.15 t/s\r\nTotal speed (AVG):           speed: 158.68 t/s\r\nCache misses:             0\r\nCache KV size 628\r\n\r\nllama_print_timings:        load time =   37371.88 ms\r\nllama_print_timings:      sample time =   12907.57 ms / 29866 runs   (    0.43 ms per token,  2313.84 tokens per second)\r\nllama_print_timings: prompt eval time =  214845.96 ms / 38277 tokens (    5.61 ms per token,   178.16 tokens per second)\r\nllama_print_timings:        eval time =     936.39 ms /    46 runs   (   20.36 ms per token,    49.13 tokens per second)\r\nllama_print_timings:       total time =  239590.67 ms\r\n```\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2023-11-15T11:42:43+00:00",
    "closed_at": "2023-11-24T11:34:32+00:00",
    "comments": 24,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4086/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4086"
  },
  {
    "number": 11749,
    "title": "llama-tts libc++abi: terminating due to uncaught exception of type std::out_of_range: vector\nAborted",
    "body": "```\n./llama-tts -m $model -mv $voice -p \"Hi i am Felix\"\n\nbuild: 4663 (c026ba3c) with clang version 19.1.5 for armv7a-unknown-linux-android24\nllama_model_loader: loaded meta data with 38 key-value pairs and 272 tensors from /storage/7DE2-358B/ysf/models/smollm-135m-instruct-q8_0.gguf (version GGUF V3 (latest))       llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = SmolLM 135M\nllama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB                                                                       llama_model_loader: - kv   4:                           general.finetune str              = instruct-add-basics\nllama_model_loader: - kv   5:                           general.basename str              = SmolLM                                                                              llama_model_loader: - kv   6:                         general.size_label str              = 135M\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0                                                                          llama_model_loader: - kv   8:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   9:                  general.base_model.0.name str              = SmolLM 135M\nllama_model_loader: - kv  10:          general.base_model.0.organization str              = HuggingFaceTB\nllama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/HuggingFaceTB/...\nllama_model_loader: - kv  12:                               general.tags arr[str,7]       = [\"alignment-handbook\", \"trl\", \"sft\", ...\nllama_model_loader: - kv  13:                           general.datasets arr[str,5]       = [\"HuggingFaceTB/Magpie-Pro-300K-Filte...                                            llama_model_loader: - kv  14:                          llama.block_count u32              = 30\nllama_model_loader: - kv  15:                       llama.context_length u32              = 2048\nllama_model_loader: - kv  16:                     llama.embedding_length u32              = 576\nllama_model_loader: - kv  17:                  llama.feed_forward_length u32              = 1536\nllama_model_loader: - kv  18:                 llama.attention.head_count u32              = 9\nllama_model_loader: - kv  19:              llama.attention.head_count_kv u32              = 3                                                                                   llama_model_loader: - kv  20:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  21:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  22:                          general.file_type u32              = 7                                                                                   llama_model_loader: - kv  23:                           llama.vocab_size u32              = 49152\nllama_model_loader: - kv  24:                 llama.rope.dimension_count u32              = 64                                                                                  llama_model_loader: - kv  25:            tokenizer.ggml.add_space_prefix bool             = false                                                                               llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = false                                                                               llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = smollm                                                                              llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\nllama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,48900]   = [\"\u0120 t\", \"\u0120 a\", \"i n\", \"h e\", \"\u0120 \u0120...                                                llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  34:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 2\nllama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\nllama_model_loader: - kv  37:               general.quantization_version u32              = 2                                                                                   llama_model_loader: - type  f32:   61 tensors\nllama_model_loader: - type q8_0:  211 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q8_0                                                          print_info: file size   = 136.40 MiB (8.51 BPW)\nload: special tokens cache size = 17\nload: token to piece cache size = 0.3170 MB\nprint_info: arch             = llama                                                    print_info: vocab_only       = 0\nprint_info: n_ctx_train      = 2048                                                     print_info: n_embd           = 576\nprint_info: n_layer          = 30\nprint_info: n_head           = 9                                                        print_info: n_head_kv        = 3\nprint_info: n_rot            = 64                                                       print_info: n_swa            = 0\nprint_info: n_embd_head_k    = 64\nprint_info: n_embd_head_v    = 64                                                       print_info: n_gqa            = 3\nprint_info: n_embd_k_gqa     = 192\nprint_info: n_embd_v_gqa     = 192                                                      print_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05                                                  print_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: n_ff             = 1536                                                     print_info: n_expert         = 0\nprint_info: n_expert_used    = 0                                                        print_info: causal attn      = 1\nprint_info: pooling type     = 0                                                        print_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1                                                        print_info: n_ctx_orig_yarn  = 2048\nprint_info: rope_finetuned   = unknown                                                  print_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0                                                        print_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = ?B\nprint_info: model params     = 134.52 M\nprint_info: general.name     = SmolLM 135M                                              print_info: vocab type       = BPE\nprint_info: n_vocab          = 49152                                                    print_info: n_merges         = 48900\nprint_info: BOS token        = 1 '<|im_start|>'\nprint_info: EOS token        = 2 '<|im_end|>'\nprint_info: EOT token        = 0 '<|endoftext|>'\nprint_info: UNK token        = 0 '<|endoftext|>'\nprint_info: PAD token        = 2 '<|im_end|>'\nprint_info: LF token         = 198 '\u010a'                                                  print_info: EOG token        = 0 '<|endoftext|>'\nprint_info: EOG token        = 2 '<|im_end|>'\nprint_info: max token length = 162                                                      load_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =   136.40 MiB\nllama_init_from_model: n_seq_max     = 1\nllama_init_from_model: n_ctx         = 8192\nllama_init_from_model: n_ctx_per_seq = 8192                                             llama_init_from_model: n_batch       = 8192\nllama_init_from_model: n_ubatch      = 512\nllama_init_from_model: flash_attn    = 0                                                llama_init_from_model: freq_base     = 10000.0\nllama_init_from_model: freq_scale    = 1\nllama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow\nllama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 30, can_shift = 1\nllama_kv_cache_init:        CPU KV buffer size =   180.00 MiB\nllama_init_from_model: KV self size  =  180.00 MiB, K (f16):   90.00 MiB, V (f16):   90.00 MiB\nllama_init_from_model:        CPU  output buffer size =     0.19 MiB\nllama_init_from_model:        CPU compute buffer size =   164.51 MiB                    llama_init_from_model: graph nodes  = 966\nllama_init_from_model: graph splits = 1\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 8192                  common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nllama_model_loader: loaded meta data with 27 key-value pairs and 75 tensors from /storage/emulated/0/Download/Lite-Oute-1-65M-Instruct-Q2_K.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = Lite-Oute-1-65M-Instruct\nllama_model_loader: - kv   2:                          llama.block_count u32              = 8                                                                                   llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 512\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 2048\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 16\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 10\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768                                                                               llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 32                                                                                  llama_model_loader: - kv  13:            tokenizer.ggml.add_space_prefix bool             = true\nllama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 32000                                                                               llama_model_loader: - kv  21:            tokenizer.ggml.unknown_token_id u32              = 0                                                                                   llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 32000\nllama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  24:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\nllama_model_loader: - kv  26:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   17 tensors                                           llama_model_loader: - type q2_K:   33 tensors\nllama_model_loader: - type q3_K:   24 tensors                                           llama_model_loader: - type q6_K:    1 tensors\nprint_info: file format = GGUF V3 (latest)                                              print_info: file type   = Q2_K - Medium\nprint_info: file size   = 29.37 MiB (3.79 BPW)                                          load: control-looking token:  32000 '<|im_end|>' was not control-type; this is probably a bug in the model. its type will be overridden                                         load: special tokens cache size = 771\nload: token to piece cache size = 0.1710 MB                                             print_info: arch             = llama\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 2048\nprint_info: n_embd           = 512                                                      print_info: n_layer          = 8\nprint_info: n_head           = 16\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 32\nprint_info: n_swa            = 0\nprint_info: n_embd_head_k    = 32\nprint_info: n_embd_head_v    = 32\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 256                                                      print_info: n_embd_v_gqa     = 256\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00                                                  print_info: n_ff             = 2048\nprint_info: n_expert         = 0                                                        print_info: n_expert_used    = 0\nprint_info: causal attn      = 1                                                        print_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear                                                   print_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 2048\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0                                                        print_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = ?B\nprint_info: model params     = 65.02 M\nprint_info: general.name     = Lite-Oute-1-65M-Instruct\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 32768\nprint_info: n_merges         = 0\nprint_info: BOS token        = 1 '<s>'                                                  print_info: EOS token        = 32000 '<|im_end|>'\nprint_info: EOT token        = 32000 '<|im_end|>'\nprint_info: UNK token        = 0 '<unk>'\nprint_info: PAD token        = 32000 '<|im_end|>'                                       print_info: LF token         = 13 '<0x0A>'\nprint_info: EOG token        = 32000 '<|im_end|>'\nprint_info: max token length = 48                                                       load_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =    29.37 MiB\nllama_init_from_model: n_seq_max     = 1\nllama_init_from_model: n_ctx         = 8192\nllama_init_from_model: n_ctx_per_seq = 8192\nllama_init_from_model: n_batch       = 8192\nllama_init_from_model: n_ubatch      = 512\nllama_init_from_model: flash_attn    = 0                                                llama_init_from_model: freq_base     = 10000.0\nllama_init_from_model: freq_scale    = 1\nllama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow\nllama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 8, can_shift = 1                                                                    llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\nllama_init_from_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\nllama_init_from_model:        CPU  output buffer size =     0.00 MiB                    llama_init_from_model:        CPU compute buffer size =   276.01 MiB\nllama_init_from_model: graph nodes  = 262\nllama_init_from_model: graph splits = 1                                                 common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nsampler seed: 0                                                                         sampler params:\n        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n        dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 8192\n        top_k = 4, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000                         sampler chain: logits -> logit-bias -> top-k -> dist\nmain: loading done\nmain: constructing prompt ..                                                            main: prompt: 'hi<|text_sep|>i<|text_sep|>am<|text_sep|>felix'\n                                                                                        <|im_start|>\n<|text_start|>the<|text_sep|>overall<|text_sep|>package<|text_sep|>from<|text_sep|>just<|text_sep|>two<|text_sep|>people<|text_sep|>is<|text_sep|>pretty<|text_sep|>remarkable<|text_sep|>sure<|text_sep|>i<|text_sep|>have<|text_sep|>some<|text_sep|>critiques<|text_sep|>about<|text_sep|>some<|text_sep|>of<|text_sep|>the<|text_sep|>gameplay<|text_sep|>aspects<|text_sep|>but<|text_sep|>its<|text_sep|>still<|text_sep|>really<|text_sep|>enjoyable<|text_sep|>and<|text_sep|>it<|text_sep|>looks<|text_sep|>lovely<|text_sep|>hi<|text_sep|>i<|text_sep|>am<|text_sep|>felix<|text_end|>                                       libc++abi: terminating due to uncaught exception of type std::out_of_range: vector\nAborted\n```",
    "labels": [
      "tts"
    ],
    "state": "closed",
    "created_at": "2025-02-08T05:33:49+00:00",
    "closed_at": "2025-02-21T15:56:06+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11749/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11749"
  },
  {
    "number": 13990,
    "title": "Eval bug: Abort is called in a thread from a custom thread pool during a llama_decode call",
    "body": "### Name and Version\n\n./llama-cli --version\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 2 CUDA devices:\n  Device 0: NVIDIA A2, compute capability 8.6, VMM: yes\n  Device 1: NVIDIA A2, compute capability 8.6, VMM: yes\nregister_backend: registered backend CUDA (2 devices)\nregister_device: registered device CUDA0 (NVIDIA A2)\nregister_device: registered device CUDA1 (NVIDIA A2)\nregister_backend: registered backend CPU (1 devices)\nregister_device: registered device CPU (Intel Xeon Processor (Icelake))\nversion: 5572 (7675c555)\nbuilt with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\n2 x A2 or  3 x A100\n\n### Models\n\nSmolLM2-360M-Instruct-BF16\n\n### Problem description & steps to reproduce\n\nWe are testing inference with 15 threads in the worker pool when an abort is called from one of those threads during a llama_decode call. \n\n- main thread\n- 3 threads are started by llama.cpp: a host thread and an extra thread for every single GPU(as far as I understand):\n- 15 threads are started by our worker pool\n\n```\n(gdb) info thread\n  Id   Target Id                                          Frame \n  1    Thread 0x7ffff7629000 (LWP 4059) \"bai-test\"        0x00007ffff5898d71 in __futex_abstimed_wait_common64 (private=128, cancel=true, abstime=0x0, op=265, expected=4071, futex_word=0x7fff9cdde2d0) at ./nptl/futex-internal.c:57\n  2    Thread 0x7fffb5dff000 (LWP 4062) \"cuda00001400006\" 0x00007ffff591b4cd in __GI___poll (fds=0x555555bda010, nfds=3, timeout=-1) at ../sysdeps/unix/sysv/linux/poll.c:29\n  3    Thread 0x7fffa8dde000 (LWP 4069) \"cuda-EvtHandlr\"  0x00007ffff591b4cd in __GI___poll (fds=0x7fffa4000c20, nfds=10, timeout=100) at ../sysdeps/unix/sysv/linux/poll.c:29\n  4    Thread 0x7fffa2dde000 (LWP 4070) \"cuda-EvtHandlr\"  0x00007ffff591b4cd in __GI___poll (fds=0x7fff98000c20, nfds=10, timeout=100) at ../sysdeps/unix/sysv/linux/poll.c:29\n  5    Thread 0x7fff9cdde000 (LWP 4071) \"bai-test\"        __futex_abstimed_wait_common (cancel=false, private=<optimized out>, abstime=0x0, clockid=0, expected=3, futex_word=0x555555babc0c) at ./nptl/futex-internal.c:103\n  6    Thread 0x7fff93ff0000 (LWP 4072) \"bai-test\"        __futex_abstimed_wait_common (cancel=false, private=<optimized out>, abstime=0x0, clockid=0, expected=3, futex_word=0x555555babc0c) at ./nptl/futex-internal.c:103\n  7    Thread 0x7fff937ef000 (LWP 4073) \"bai-test\"        __futex_abstimed_wait_common (cancel=false, private=<optimized out>, abstime=0x0, clockid=0, expected=3, futex_word=0x555555bac7ec) at ./nptl/futex-internal.c:103\n  8    Thread 0x7fff92fee000 (LWP 4074) \"bai-test\"        __futex_abstimed_wait_common (cancel=false, private=<optimized out>, abstime=0x0, clockid=0, expected=3, futex_word=0x555555bac7e8) at ./nptl/futex-internal.c:103\n  9    Thread 0x7fff927ed000 (LWP 4075) \"bai-test\"        __futex_abstimed_wait_common (cancel=false, private=<optimized out>, abstime=0x0, clockid=0, expected=3, futex_word=0x555555bac7ec) at ./nptl/futex-internal.c:103\n  10   Thread 0x7fff91fec000 (LWP 4076) \"bai-test\"        0x00007fffea04d771 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\n  11   Thread 0x7fff917eb000 (LWP 4077) \"bai-test\"        __futex_abstimed_wait_common (cancel=false, private=<optimized out>, abstime=0x0, clockid=0, expected=3, futex_word=0x555555babc0c) at ./nptl/futex-internal.c:103\n  12   Thread 0x7fff90fea000 (LWP 4078) \"bai-test\"        0x00007ffff7fc3e36 in ?? ()\n  13   Thread 0x7fff8bfff000 (LWP 4079) \"bai-test\"        __futex_abstimed_wait_common (cancel=false, private=0, abstime=0x0, clockid=0, expected=2, futex_word=0x555555babc08) at ./nptl/futex-internal.c:103\n  14   Thread 0x7fff8b7fe000 (LWP 4080) \"bai-test\"        0x00007fffea94e6d0 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\n  15   Thread 0x7fff8affd000 (LWP 4081) \"bai-test\"        __futex_abstimed_wait_common (cancel=false, private=<optimized out>, abstime=0x0, clockid=0, expected=3, futex_word=0x555555bac7ec) at ./nptl/futex-internal.c:103\n* 16   Thread 0x7fff8a7fc000 (LWP 4082) \"bai-test\"        __pthread_kill_implementation (no_tid=0, signo=6, threadid=<optimized out>) at ./nptl/pthread_kill.c:44\n  17   Thread 0x7fff89ffb000 (LWP 4083) \"bai-test\"        __futex_abstimed_wait_common (cancel=false, private=<optimized out>, abstime=0x0, clockid=0, expected=3, futex_word=0x555555bac7e8) at ./nptl/futex-internal.c:103\n  18   Thread 0x7fff897fa000 (LWP 4084) \"bai-test\"        __futex_abstimed_wait_common (cancel=false, private=<optimized out>, abstime=0x0, clockid=0, expected=3, futex_word=0x555555babc0c) at ./nptl/futex-internal.c:103\n  19   Thread 0x7fff88ff9000 (LWP 4085) \"bai-test\"        __futex_abstimed_wait_common (cancel=false, private=<optimized out>, abstime=0x0, clockid=0, expected=3, futex_word=0x555555bac7e8) at ./nptl/futex-internal.c:103\n```\n\nCallstack:\n```\nThread 16 \"bai-test\" received signal SIGABRT, Aborted.\n[Switching to Thread 0x7fff8a7fc000 (LWP 4082)]\nDownload failed: Invalid argument.  Continuing without source file ./nptl/./nptl/pthread_kill.c.\n__pthread_kill_implementation (no_tid=0, signo=6, threadid=<optimized out>) at ./nptl/pthread_kill.c:44\nwarning: 44\t./nptl/pthread_kill.c: No such file or directory\n(gdb) bt\n#0  __pthread_kill_implementation (no_tid=0, signo=6, threadid=<optimized out>) at ./nptl/pthread_kill.c:44\n#1  __pthread_kill_internal (signo=6, threadid=<optimized out>) at ./nptl/pthread_kill.c:78\n#2  __GI___pthread_kill (threadid=<optimized out>, signo=signo@entry=6) at ./nptl/pthread_kill.c:89\n#3  0x00007ffff584527e in __GI_raise (sig=sig@entry=6) at ../sysdeps/posix/raise.c:26\n#4  0x00007ffff58288ff in __GI_abort () at ./stdlib/abort.c:79\n#5  0x00007ffff5f0f836 in ggml_abort (file=0x7ffff6613268 \"/home/ubuntu/sources/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu\", line=75, fmt=0x7ffff661325d \"CUDA error\")\n    at /home/ubuntu/sources/llama.cpp/ggml/src/ggml.c:221\n#6  0x00007ffff60e6c77 in ggml_cuda_error (stmt=0x7ffff66147e2 \"cudaGetLastError()\", func=0x7ffff66146fa \"ggml_cuda_op_mul_mat\", \n    file=0x7ffff6613268 \"/home/ubuntu/sources/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu\", line=1676, \n    msg=0x7ffff5493f90 \"operation failed due to a previous error during capture\") at /home/ubuntu/sources/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:75\n#7  0x00007ffff60ed62d in ggml_cuda_op_mul_mat (ctx=..., src0=0x5555580d3930, src1=0x7fff0d04ab00, dst=0x7fff0d04ade0, \n    op=0x7ffff6108641 <ggml_cuda_op_mul_mat_vec(ggml_backend_cuda_context&, ggml_tensor const*, ggml_tensor const*, ggml_tensor*, char const*, float const*, char const*, float*, long, long, long, long, CUstream_st*)>, quantize_src1=0x0) at /home/ubuntu/sources/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:1676\n#8  0x00007ffff60ef1e7 in ggml_cuda_mul_mat (ctx=..., src0=0x5555580d3930, src1=0x7fff0d04ab00, dst=0x7fff0d04ade0)\n    at /home/ubuntu/sources/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:1976\n#9  0x00007ffff60f0ac6 in ggml_cuda_compute_forward (ctx=..., dst=0x7fff0d04ade0) at /home/ubuntu/sources/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:2264\n#10 0x00007ffff60f221f in evaluate_and_capture_cuda_graph (cuda_ctx=0x7fff14001410, cgraph=0x7fff140fd1c8, graph_evaluated_or_captured=@0x7fff8a7d75db: false, \n    use_cuda_graph=@0x7fff8a7d75d9: true, cuda_graph_update_required=@0x7fff8a7d75da: true) at /home/ubuntu/sources/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:2673\n#11 0x00007ffff60f28c8 in ggml_backend_cuda_graph_compute (backend=0x7fff14001950, cgraph=0x7fff140fd1c8)\n    at /home/ubuntu/sources/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:2780\n#12 0x00007ffff5f27a02 in ggml_backend_graph_compute_async (backend=0x7fff14001950, cgraph=0x7fff140fd1c8) at /home/ubuntu/sources/llama.cpp/ggml/src/ggml-backend.cpp:334\n#13 0x00007ffff5f2bb6d in ggml_backend_sched_compute_splits (sched=0x7fff1403ab70) at /home/ubuntu/sources/llama.cpp/ggml/src/ggml-backend.cpp:1404\n#14 0x00007ffff5f2c809 in ggml_backend_sched_graph_compute_async (sched=0x7fff1403ab70, graph=0x7fff0cdf6030)\n    at /home/ubuntu/sources/llama.cpp/ggml/src/ggml-backend.cpp:1596\n#15 0x00007ffff7b88691 in llama_context::graph_compute (this=0x7fff14000b70, gf=0x7fff0cdf6030, batched=false) at /home/ubuntu/sources/llama.cpp/src/llama-context.cpp:1381\n#16 0x00007ffff7b84f30 in llama_context::process_ubatch (this=0x7fff14000b70, ubatch=..., gtype=LLM_GRAPH_TYPE_DECODER, mstate=0x7fff14178a00, \n    ret=@0x7fff8a7d7848: -369628841) at /home/ubuntu/sources/llama.cpp/src/llama-context.cpp:683\n#17 0x00007ffff7b868bf in llama_context::decode (this=0x7fff14000b70, inp_batch=...) at /home/ubuntu/sources/llama.cpp/src/llama-context.cpp:1018\n#18 0x00007ffff7b8d301 in llama_decode (ctx=0x7fff14000b70, batch=...) at /home/ubuntu/sources/llama.cpp/src/llama-context.cpp:2681\n```\n\nAll threads are sharing same model and vocabulary but each thread has its own llama_context and llama_sampler:\nllama_model_params.n_gpu_layers = 99\nllama_context_params.ctx = 1000\n\n* I understand that the bottleneck for inference is 1+2 threads in llama.cpp, but we also need our thread pool for regular business logic, where multithreading fits our needs.\n* Same test works steadily on Metal\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nggml_gallocr_needs_realloc: node inp_embd is not valid\nggml_gallocr_alloc_graph: cannot reallocate multi buffer graph automatically, call reserve\nggml_backend_sched_alloc_splits: failed to allocate graph, reserving (backend_ids_changed = 0)\ncheck_node_graph_compatibility_and_refresh_copy_ops: disabling CUDA graphs due to batch size > 1 [ffn_inp-0] [960 128 1 1]\ncheck_node_graph_compatibility_and_refresh_copy_ops: disabling CUDA graphs due to batch size > 1 [ffn_inp-17] [960 128 1 1]\n\nCUDA error: operation failed due to a previous error during capture\n  current device: 1, in function ggml_cuda_op_mul_mat at /home/ubuntu/sources/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:1676\n  cudaGetLastError()\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-06-03T12:25:37+00:00",
    "closed_at": "2025-06-20T11:57:37+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13990/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13990"
  },
  {
    "number": 3809,
    "title": "server hangs up if given ascii strings containing utf-8 non-breaking space characters",
    "body": "I am running the MMLU test on llamma.cpp with mistrallite and the server locked up when it encountered a string containing 0xc2 0xa0 (utf-8 non-breaking space) in the test set data.  Instead of locking up a better behavior would be to reject the input string as having bad characters and return an error message similar to what it does when the json prompt is mis formatted.  I do not know where the lock up is happening (tokenizer, parser, etc.) but it just freezes and the gpu is running full load so it seems like tokenizer got stuck.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-10-27T03:49:48+00:00",
    "closed_at": "2023-11-14T19:48:33+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3809/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3809"
  },
  {
    "number": 3550,
    "title": "[User] The server crashes when a wrong `n_keep` is set",
    "body": "# Prerequisites\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nRunning the `server` with a specific context window and then passing a specific **wrong** `n_keep` with a specific number of tokens. Since I'm passing a wrong parameter, I expect the server to either give an error or just generate something anyway.\r\n\r\n# Current Behavior\r\n\r\nThe server crashes.\r\n\r\n# Environment and Context\r\n\r\n**Commit:** db3abcc114d5d1790ba814aa1a80ac673d4ccc3e\r\n\r\n**OS:** Kubuntu 23.04\r\n\r\n<blockquote>\r\n\u276f lscpu | grep -P 'Model name|Flags'\r\n\r\nModel name:                         AMD Ryzen 9 7900 12-Core Processor\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid overflow_recov succor smca fsrm flush_l1d\r\n</blockquote>\r\n\r\n```\r\n\u276f uname -a\r\nLinux comp 6.2.0-34-generic #34-Ubuntu SMP PREEMPT_DYNAMIC Mon Sep  4 13:06:55 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\n```\r\n\u276f make --version | head -1\r\nGNU Make 4.3\r\n```\r\n\r\n```\r\n\u276f g++ --version | head -1\r\ng++ (Ubuntu 12.3.0-1ubuntu1~23.04) 12.3.0\r\n```\r\n\r\n\r\n# Steps to Reproduce\r\n\r\n1. I used this model:\r\nhttps://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/blob/main/llama-2-13b-chat.Q4_K_M.gguf\r\n(but it also crashes with other models too)\r\n\r\n2. The server is built with just `make`, no other params. But it also crashes with `LLAMA_CUBLAS=1`.\r\n\r\n3. Start the server with the context window of 32:\r\n```\r\n\u276f ./server -m /opt/models/text/llama-2-13b-chat.Q4_K_M.gguf -c 32\r\n```\r\n\r\n<details>\r\n<summary>startup log</summary>\r\n\r\n```\r\n{\"timestamp\":1696785761,\"level\":\"INFO\",\"function\":\"main\",\"line\":1323,\"message\":\"build info\",\"build\":1354,\"commit\":\"db3abcc\"}\r\n{\"timestamp\":1696785761,\"level\":\"INFO\",\"function\":\"main\",\"line\":1325,\"message\":\"system info\",\"n_threads\":12,\"n_threads_batch\":-1,\"total_threads\":24,\"system_info\":\"AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \"}\r\nllama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /opt/models/text/llama-2-13b-chat.Q4_K_M.gguf (version GGUF V2 (latest))\r\nllama_model_loader: - tensor    0:                token_embd.weight q4_K     [  5120, 32000,     1,     1 ]\r\nllama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor    2:            blk.0.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor    4:              blk.0.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor    6:              blk.0.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor    7:         blk.0.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor    8:              blk.0.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor    9:              blk.0.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   11:            blk.1.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   13:              blk.1.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   15:              blk.1.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   16:         blk.1.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   17:              blk.1.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   18:              blk.1.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   20:           blk.10.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   22:             blk.10.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   24:             blk.10.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   25:        blk.10.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   26:             blk.10.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   27:             blk.10.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   29:           blk.11.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   31:             blk.11.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   33:             blk.11.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   34:        blk.11.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   35:             blk.11.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   36:             blk.11.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   38:           blk.12.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   40:             blk.12.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   42:             blk.12.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   43:        blk.12.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   44:             blk.12.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   45:             blk.12.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   47:           blk.13.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   49:             blk.13.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   51:             blk.13.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   52:        blk.13.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   53:             blk.13.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   54:             blk.13.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   56:           blk.14.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   58:             blk.14.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   60:             blk.14.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   61:        blk.14.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   62:             blk.14.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   63:             blk.14.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   64:             blk.15.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   65:             blk.15.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   66:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   67:            blk.2.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   68:            blk.2.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   69:              blk.2.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   70:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   71:              blk.2.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   72:         blk.2.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   73:              blk.2.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   74:              blk.2.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   75:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   76:            blk.3.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   77:            blk.3.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   78:              blk.3.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   79:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   80:              blk.3.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   81:         blk.3.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   82:              blk.3.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   83:              blk.3.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   84:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   85:            blk.4.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   86:            blk.4.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   87:              blk.4.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   88:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   89:              blk.4.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   90:         blk.4.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   91:              blk.4.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   92:              blk.4.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   93:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   94:            blk.5.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   95:            blk.5.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   96:              blk.5.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor   97:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor   98:              blk.5.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor   99:         blk.5.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  100:              blk.5.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  101:              blk.5.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  102:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  103:            blk.6.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  104:            blk.6.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  105:              blk.6.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  106:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  107:              blk.6.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  108:         blk.6.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  109:              blk.6.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  110:              blk.6.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  111:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  112:            blk.7.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  113:            blk.7.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  114:              blk.7.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  115:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  116:              blk.7.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  117:         blk.7.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  118:              blk.7.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  119:              blk.7.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  120:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  121:            blk.8.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  122:            blk.8.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  123:              blk.8.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  124:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  125:              blk.8.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  126:         blk.8.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  127:              blk.8.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  128:              blk.8.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  129:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  130:            blk.9.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  131:            blk.9.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  132:              blk.9.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  133:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  134:              blk.9.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  135:         blk.9.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  136:              blk.9.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  137:              blk.9.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  138:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  139:           blk.15.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  141:             blk.15.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  142:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  143:        blk.15.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  144:             blk.15.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  145:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  146:           blk.16.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  147:           blk.16.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  148:             blk.16.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  149:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  150:             blk.16.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  151:        blk.16.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  152:             blk.16.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  153:             blk.16.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  154:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  155:           blk.17.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  156:           blk.17.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  157:             blk.17.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  158:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  159:             blk.17.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  160:        blk.17.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  161:             blk.17.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  162:             blk.17.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  163:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  164:           blk.18.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  165:           blk.18.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  166:             blk.18.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  167:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  168:             blk.18.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  169:        blk.18.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  170:             blk.18.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  171:             blk.18.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  172:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  173:           blk.19.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  174:           blk.19.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  175:             blk.19.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  176:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  177:             blk.19.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  178:        blk.19.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  179:             blk.19.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  180:             blk.19.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  181:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  182:           blk.20.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  183:           blk.20.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  184:             blk.20.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  185:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  186:             blk.20.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  187:        blk.20.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  188:             blk.20.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  189:             blk.20.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  190:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  191:           blk.21.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  192:           blk.21.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  193:             blk.21.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  194:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  195:             blk.21.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  196:        blk.21.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  197:             blk.21.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  198:             blk.21.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  199:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  200:           blk.22.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  201:           blk.22.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  202:             blk.22.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  203:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  204:             blk.22.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  205:        blk.22.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  206:             blk.22.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  207:             blk.22.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  208:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  209:           blk.23.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  210:           blk.23.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  211:             blk.23.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  212:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  213:             blk.23.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  214:        blk.23.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  215:             blk.23.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  216:             blk.23.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  217:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  218:           blk.24.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  219:           blk.24.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  220:             blk.24.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  221:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  222:             blk.24.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  223:        blk.24.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  224:             blk.24.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  225:             blk.24.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  226:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  227:           blk.25.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  228:           blk.25.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  229:             blk.25.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  230:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  231:             blk.25.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  232:        blk.25.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  233:             blk.25.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  234:             blk.25.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  235:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  236:           blk.26.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  237:           blk.26.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  238:             blk.26.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  239:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  240:             blk.26.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  241:        blk.26.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  242:             blk.26.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  243:             blk.26.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  244:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  245:           blk.27.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  246:           blk.27.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  247:             blk.27.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  248:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  249:             blk.27.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  250:        blk.27.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  251:             blk.27.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  252:             blk.27.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  253:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  254:           blk.28.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  255:           blk.28.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  256:             blk.28.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  257:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  258:             blk.28.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  259:        blk.28.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  260:             blk.28.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  261:             blk.28.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  262:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  263:           blk.29.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  264:           blk.29.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  265:             blk.29.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  266:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  267:             blk.29.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  268:        blk.29.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  269:             blk.29.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  270:             blk.29.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  271:           blk.30.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  272:             blk.30.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  273:             blk.30.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  274:        blk.30.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  275:             blk.30.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  276:             blk.30.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  277:                    output.weight q6_K     [  5120, 32000,     1,     1 ]\r\nllama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  279:           blk.30.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  280:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  282:           blk.31.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  284:             blk.31.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  286:             blk.31.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  287:        blk.31.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  288:             blk.31.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  289:             blk.31.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  290:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  291:           blk.32.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  292:           blk.32.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  293:             blk.32.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  294:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  295:             blk.32.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  296:        blk.32.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  297:             blk.32.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  298:             blk.32.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  299:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  300:           blk.33.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  301:           blk.33.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  302:             blk.33.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  303:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  304:             blk.33.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  305:        blk.33.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  306:             blk.33.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  307:             blk.33.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  308:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  309:           blk.34.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  310:           blk.34.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  311:             blk.34.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  312:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  313:             blk.34.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  314:        blk.34.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  315:             blk.34.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  316:             blk.34.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  317:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  318:           blk.35.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  319:           blk.35.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  320:             blk.35.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  321:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  322:             blk.35.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  323:        blk.35.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  324:             blk.35.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  325:             blk.35.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  326:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  327:           blk.36.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  328:           blk.36.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  329:             blk.36.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  330:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  331:             blk.36.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  332:        blk.36.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  333:             blk.36.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  334:             blk.36.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  335:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  336:           blk.37.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  337:           blk.37.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  338:             blk.37.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  339:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  340:             blk.37.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  341:        blk.37.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  342:             blk.37.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  343:             blk.37.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  344:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  345:           blk.38.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  346:           blk.38.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  347:             blk.38.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  348:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  349:             blk.38.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  350:        blk.38.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  351:             blk.38.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  352:             blk.38.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  353:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  354:           blk.39.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  355:           blk.39.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  356:             blk.39.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\r\nllama_model_loader: - tensor  357:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - tensor  358:             blk.39.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  359:        blk.39.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  360:             blk.39.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  361:             blk.39.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\r\nllama_model_loader: - tensor  362:               output_norm.weight f32      [  5120,     1,     1,     1 ]\r\nllama_model_loader: - kv   0:                       general.architecture str\r\nllama_model_loader: - kv   1:                               general.name str\r\nllama_model_loader: - kv   2:                       llama.context_length u32\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32\r\nllama_model_loader: - kv   4:                          llama.block_count u32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32\r\nllama_model_loader: - kv  10:                          general.file_type u32\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str\r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr\r\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32\r\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32\r\nllama_model_loader: - kv  18:               general.quantization_version u32\r\nllama_model_loader: - type  f32:   81 tensors\r\nllama_model_loader: - type q4_K:  241 tensors\r\nllama_model_loader: - type q6_K:   41 tensors\r\nllm_load_print_meta: format           = GGUF V2 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 5120\r\nllm_load_print_meta: n_head           = 40\r\nllm_load_print_meta: n_head_kv        = 40\r\nllm_load_print_meta: n_layer          = 40\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: n_ff             = 13824\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: model type       = 13B\r\nllm_load_print_meta: model ftype      = mostly Q4_K - Medium\r\nllm_load_print_meta: model params     = 13.02 B\r\nllm_load_print_meta: model size       = 7.33 GiB (4.83 BPW)\r\nllm_load_print_meta: general.name   = LLaMA v2\r\nllm_load_print_meta: BOS token = 1 '<s>'\r\nllm_load_print_meta: EOS token = 2 '</s>'\r\nllm_load_print_meta: UNK token = 0 '<unk>'\r\nllm_load_print_meta: LF token  = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.12 MB\r\nllm_load_tensors: mem required  = 7500.97 MB\r\n....................................................................................................\r\nllama_new_context_with_model: n_ctx      = 32\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_new_context_with_model: kv self size  =   25.00 MB\r\nllama_new_context_with_model: compute buffer total size = 10.75 MB\r\n\r\nllama server listening at http://127.0.0.1:8080\r\n\r\n{\"timestamp\":1696785762,\"level\":\"INFO\",\"function\":\"main\",\"line\":1753,\"message\":\"HTTP server listening\",\"hostname\":\"127.0.0.1\",\"port\":8080}\r\n```\r\n</details>\r\n\r\n4. Call the API with `n_keep = 32` and a specific number of tokens in `prompt` (not quite sure how many):\r\n```\r\n\u276f curl -sS --data '{\"n_predict\":32,\"n_keep\":32,\"prompt\":\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"}' http://127.0.0.1:8080/completion\r\n```\r\n\r\n# Failure Logs\r\n\r\nThe server just crashes with this:\r\n```\r\nmalloc(): unaligned tcache chunk detected\r\nfish: Job 1, './server -m /opt/models/text/ll\u2026' terminated by signal SIGABRT (Abort)\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nThe crash happens only with the parameters described above. For example, I tried other values other than 32, but with them the server works as usual. Maybe it crashes with any other combination of parameters, but I can only reproduce it like this.\r\n\r\nI assume that `n_keep` is wrong because it's the same size as the context, which I assume makes no sense. However, if the `prompt` is one token longer, the server actually generates something.\r\n\r\n<details>\r\n<summary>1 more token in the prompt</summary>\r\n\r\n```\r\n\u276f curl -sS --data '{\"n_predict\":32,\"n_keep\":32,\"prompt\":\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"}' http://127.0.0.1:8080/completion | jq\r\n{\r\n  \"content\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCard\\n\\n\\n\\n\\n\",\r\n  \"generation_settings\": {\r\n    \"frequency_penalty\": 0,\r\n    \"grammar\": \"\",\r\n    \"ignore_eos\": false,\r\n    \"logit_bias\": [],\r\n    \"mirostat\": 0,\r\n    \"mirostat_eta\": 0.10000000149011612,\r\n    \"mirostat_tau\": 5,\r\n    \"model\": \"/opt/models/text/llama-2-13b-chat.Q4_K_M.gguf\",\r\n    \"n_ctx\": 32,\r\n    \"n_keep\": 28,\r\n    \"n_predict\": 32,\r\n    \"n_probs\": 0,\r\n    \"penalize_nl\": true,\r\n    \"presence_penalty\": 0,\r\n    \"repeat_last_n\": 64,\r\n    \"repeat_penalty\": 1.100000023841858,\r\n    \"seed\": 4294967295,\r\n    \"stop\": [],\r\n    \"stream\": false,\r\n    \"temp\": 0.800000011920929,\r\n    \"tfs_z\": 1,\r\n    \"top_k\": 40,\r\n    \"top_p\": 0.949999988079071,\r\n    \"typical_p\": 1\r\n  },\r\n  \"model\": \"/opt/models/text/llama-2-13b-chat.Q4_K_M.gguf\",\r\n  \"prompt\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\r\n  \"stop\": true,\r\n  \"stopped_eos\": false,\r\n  \"stopped_limit\": true,\r\n  \"stopped_word\": false,\r\n  \"stopping_word\": \"\",\r\n  \"timings\": {\r\n    \"predicted_ms\": 3919.781,\r\n    \"predicted_n\": 31,\r\n    \"predicted_per_second\": 7.908605098090939,\r\n    \"predicted_per_token_ms\": 126.44454838709677,\r\n    \"prompt_ms\": 1535.19,\r\n    \"prompt_n\": 31,\r\n    \"prompt_per_second\": 20.192940287521413,\r\n    \"prompt_per_token_ms\": 49.52225806451613\r\n  },\r\n  \"tokens_cached\": 31,\r\n  \"tokens_evaluated\": 35,\r\n  \"tokens_predicted\": 31,\r\n  \"truncated\": true\r\n}\r\n```\r\n</details>\r\n\r\nAlso, the server crashes only on the very first request. If the abovementioned request is done after some other request, the server does not crash.\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-10-08T17:26:00+00:00",
    "closed_at": "2023-10-21T09:11:10+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3550/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3550"
  },
  {
    "number": 1589,
    "title": "std::runtime_error exceptions not caught as std::string by Visual C++?",
    "body": "Platform: Windows x64\r\nCommit: 7e4ea5beff567f53be92f75f9089e6f11fa5dabd\r\n\r\nI noticed that `main.exe` fails for me when I run it without any parameters, and no model is found. The only output I got was:\r\n\r\n    C:\\Develop\\llama.cpp>bin\\Release\\main.exe\r\n    main: build = 583 (7e4ea5b)\r\n    main: seed  = 1684960511\r\n\r\nIn the debugger, I found that this line had triggered an unhandled exception:\r\n\r\n    throw std::runtime_error(format(\"failed to open %s: %s\", fname, strerror(errno)));\r\n\r\nWhen I change the `catch` statement like this\r\n\r\n    -    } catch (const std::string & err) {\r\n    -        fprintf(stderr, \"error loading model: %s\\n\", err.c_str());\r\n    +    } catch (const std::exception & err) {\r\n    +        fprintf(stderr, \"error loading model: %s\\n\", err.what());\r\n\r\nthen I get a proper error message again, as in the past:\r\n\r\n    C:\\Develop\\llama.cpp\\build>bin\\Release\\main.exe\r\n    main: build = 583 (7e4ea5b)\r\n    main: seed  = 1684961912\r\n    error loading model: failed to open models/7B/ggml-model.bin: No such file or directory\r\n    llama_init_from_file: failed to load model\r\n    llama_init_from_gpt_params: error: failed to load model 'models/7B/ggml-model.bin'\r\n    main: error: unable to load model\r\n\r\nThis appears to be related to the changes made in #1316 that explicitly changed the exception type to `std::exception`, even though they are caught as `std::string`. Is this a specific behavior that only works in some compilers, and does it make sense for me to submit an MR that catches exceptions as `std::exception` as above, which appears to be more common C++ practice?\r\n\r\nMy compiler version details (from _CMake_):\r\n\r\n    -- Building for: Visual Studio 17 2022\r\n    -- Selecting Windows SDK version 10.0.22000.0 to target Windows 10.0.19045.\r\n    -- The C compiler identification is MSVC 19.35.32216.1\r\n    -- The CXX compiler identification is MSVC 1",
    "labels": [],
    "state": "closed",
    "created_at": "2023-05-24T21:30:56+00:00",
    "closed_at": "2023-06-05T20:24:31+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1589/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1589"
  },
  {
    "number": 1749,
    "title": "[User] Insert summary of your issue or enhancement..",
    "body": "# Prerequisites\n\nPlease answer the following questions for yourself before submitting an issue.\n\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\n\n# Expected Behavior\n\nPlease provide a detailed written description of what you were trying to do, and what you expected `llama.cpp` to do.\n\n# Current Behavior\n\nPlease provide a detailed written description of what `llama.cpp` did, instead.\n\n# Environment and Context\n\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\n\n* Physical (or virtual) hardware you are using, e.g. for Linux:\n\n`$ lscpu`\n\n* Operating System, e.g. for Linux:\n\n`$ uname -a`\n\n* SDK version, e.g. for Linux:\n\n```\n$ python3 --version\n$ make --version\n$ g++ --version\n```\n\n# Failure Information (for bugs)\n\nPlease help provide information about the failure if this is a bug. If it is not a bug, please remove the rest of this template.\n\n# Steps to Reproduce\n\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\n\n1. step 1\n2. step 2\n3. step 3\n4. etc.\n\n# Failure Logs\n\nPlease include any relevant log snippets or files. If it works under one configuration but not under another, please provide logs for both configurations and their corresponding outputs so it is easy to see where behavior changes.\n\nAlso, please try to **avoid using screenshots** if at all possible. Instead, copy/paste the console output and use [Github's markdown](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) to cleanly format your logs for easy readability.\n\nExample environment info:\n```\nllama.cpp$ git log | head -1\ncommit 2af23d30434a677c6416812eea52ccc0af65119c\n\nllama.cpp$ lscpu | egrep \"AMD|Flags\"\nVendor ID:                       AuthenticAMD\nModel name:                      AMD Ryzen Threadripper 1950X 16-Core Processor\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sme sev\nVirtualization:                  AMD-V\n\nllama.cpp$ python3 --version\nPython 3.10.9\n\nllama.cpp$ pip list | egrep \"torch|numpy|sentencepiece\"\nnumpy                         1.24.2\nnumpydoc                      1.5.0\nsentencepiece                 0.1.97\ntorch                         1.13.1\ntorchvision                   0.14.1\n\nllama.cpp$ make --version | head -1\nGNU Make 4.3\n\n$ md5sum ./models/65B/ggml-model-q4_0.bin\ndbdd682cce80e2d6e93cefc7449df487  ./models/65B/ggml-model-q4_0.bin\n```\n\nExample run with the Linux command [perf](https://www.brendangregg.com/perf.html)\n```\nllama.cpp$ perf stat ./main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p \"Please close your issue when it has been answered.\"\nmain: seed = 1679149377\nllama_model_load: loading model from './models/65B/ggml-model-q4_0.bin' - please wait ...\nllama_model_load: n_vocab = 32000\nllama_model_load: n_ctx   = 512\nllama_model_load: n_embd  = 8192\nllama_model_load: n_mult  = 256\nllama_model_load: n_head  = 64\nllama_model_load: n_layer = 80\nllama_model_load: n_rot   = 128\nllama_model_load: f16     = 2\nllama_model_load: n_ff    = 22016\nllama_model_load: n_parts = 8\nllama_model_load: ggml ctx size = 41477.73 MB\nllama_model_load: memory_size =  2560.00 MB, n_mem = 40960\nllama_model_load: loading model part 1/8 from './models/65B/ggml-model-q4_0.bin'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 2/8 from './models/65B/ggml-model-q4_0.bin.1'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 3/8 from './models/65B/ggml-model-q4_0.bin.2'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 4/8 from './models/65B/ggml-model-q4_0.bin.3'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 5/8 from './models/65B/ggml-model-q4_0.bin.4'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 6/8 from './models/65B/ggml-model-q4_0.bin.5'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 7/8 from './models/65B/ggml-model-q4_0.bin.6'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\nllama_model_load: loading model part 8/8 from './models/65B/ggml-model-q4_0.bin.7'\nllama_model_load: .......................................................................................... done\nllama_model_load: model size =  4869.09 MB / num tensors = 723\n\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\n\nmain: prompt: 'Please close your issue when it has been answered.'\nmain: number of tokens in prompt = 11\n     1 -> ''\n 12148 -> 'Please'\n  3802 -> ' close'\n   596 -> ' your'\n  2228 -> ' issue'\n   746 -> ' when'\n   372 -> ' it'\n   756 -> ' has'\n  1063 -> ' been'\n  7699 -> ' answered'\n 29889 -> '.'\n\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\n\n\nPlease close your issue when it has been answered.\n@duncan-donut: I'm trying to figure out what kind of \"support\" you need for this script and why, exactly? Is there a question about how the code works that hasn't already been addressed in one or more comments below this ticket, or are we talking something else entirely like some sorta bugfixing job because your server setup is different from mine??\nI can understand if your site needs to be running smoothly and you need help with a fix of sorts but there should really be nothing wrong here that the code itself could not handle. And given that I'm getting reports about how it works perfectly well on some other servers, what exactly are we talking? A detailed report will do wonders in helping us get this resolved for ya quickly so please take your time and describe the issue(s) you see as clearly & concisely as possible!!\n@duncan-donut: I'm not sure if you have access to cPanel but you could try these instructions. It is worth a shot! Let me know how it goes (or what error message, exactly!) when/if ya give that code a go? [end of text]\n\n\nmain: mem per token = 71159620 bytes\nmain:     load time = 19309.95 ms\nmain:   sample time =   168.62 ms\nmain:  predict time = 223895.61 ms / 888.47 ms per token\nmain:    total time = 246406.42 ms\n\n Performance counter stats for './main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p Please close your issue when it has been answered.':\n\n        3636882.89 msec task-clock                #   14.677 CPUs utilized\n             13509      context-switches          #    3.714 /sec\n              2436      cpu-migrations            #    0.670 /sec\n          10476679      page-faults               #    2.881 K/sec\n    13133115082869      cycles                    #    3.611 GHz                      (16.77%)\n       29314462753      stalled-cycles-frontend   #    0.22% frontend cycles idle     (16.76%)\n    10294402631459      stalled-cycles-backend    #   78.39% backend cycles idle      (16.74%)\n    23479217109614      instructions              #    1.79  insn per cycle\n                                                  #    0.44  stalled cycles per insn  (16.76%)\n     2353072268027      branches                  #  647.002 M/sec                    (16.77%)\n        1998682780      branch-misses             #    0.08% of all branches          (16.76%)\n\n     247.802177522 seconds time elapsed\n\n    3618.573072000 seconds user\n      18.491698000 seconds sys\n```\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-06-08T03:03:13+00:00",
    "closed_at": "2023-06-18T07:18:07+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1749/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1749"
  },
  {
    "number": 12277,
    "title": "Eval bug: GPU Hang Error on Metal backend",
    "body": "### Name and Version\n\n$ build/bin/llama-cli --version\nversion: 4857 (0fd7ca7a)\nbuilt with Apple clang version 16.0.0 (clang-1600.0.26.6) for arm64-apple-darwin24.3.0\n\n### Operating systems\n\nMac\n\n### GGML backends\n\nMetal\n\n### Hardware\n\nApple M4 Max\n\n### Models\n\n[Google Gemma-2 it GGUF](https://huggingface.co/google/gemma-2b-it-GGUF)\n\n### Problem description & steps to reproduce\n\nWhen I run llama-cli, the inference crashes partway through. Sometimes I see \"error: Caused GPU Hang Error (00000003:kIOGPUCommandBufferCallbackErrorHang)\", and sometimes \"error: Discarded (victim of GPU error/recovery) (00000005:kIOGPUCommandBufferCallbackErrorInnocentVictim)\".\n\nI first noticed this after I upgraded my OS to Sequoia 15.3.1. My existing Ollama install started showing these errors. I built llama.cpp from source and replicated them here. So far I've seen the problem on Gemma-2b, Gemma2-2b, and Llama-3.3. I've tried running the Apple hardware diagnostic in case this is a hardware problem, but the diagnostic didn't find any problems.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n$ build/bin/llama-cli -m  ../ggml/build/models/gemma-2b-it.gguf -p \"This is a test query\"\nbuild: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.6) for arm64-apple-darwin24.3.0\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_load_from_file_impl: using device Metal (Apple M4 Max) - 98303 MiB free\nllama_model_loader: loaded meta data with 19 key-value pairs and 164 tensors from ../ggml/build/models/gemma-2b-it.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma\nllama_model_loader: - kv   1:                               general.name str              = gemma-2b-it\nllama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\nllama_model_loader: - kv   3:                          gemma.block_count u32              = 18\nllama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\nllama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\nllama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\nllama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\nllama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\nllama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\nllama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - type  f32:  164 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = all F32 (guessed)\nprint_info: file size   = 9.34 GiB (32.00 BPW)\nload: control-looking token:    107 '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 5\nload: token to piece cache size = 1.6014 MB\nprint_info: arch             = gemma\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 8192\nprint_info: n_embd           = 2048\nprint_info: n_layer          = 18\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 1\nprint_info: n_rot            = 256\nprint_info: n_swa            = 0\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 8\nprint_info: n_embd_k_gqa     = 256\nprint_info: n_embd_v_gqa     = 256\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: n_ff             = 16384\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 8192\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 2B\nprint_info: model params     = 2.51 B\nprint_info: general.name     = gemma-2b-it\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 256128\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 107 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 227 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 107 '<end_of_turn>'\nprint_info: max token length = 93\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: offloading 18 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 19/19 layers to GPU\nload_tensors: Metal_Mapped model buffer size =  9561.30 MiB\nload_tensors:   CPU_Mapped model buffer size =  2001.00 MiB\n.............................................................\nllama_init_from_model: n_seq_max     = 1\nllama_init_from_model: n_ctx         = 4096\nllama_init_from_model: n_ctx_per_seq = 4096\nllama_init_from_model: n_batch       = 2048\nllama_init_from_model: n_ubatch      = 512\nllama_init_from_model: flash_attn    = 0\nllama_init_from_model: freq_base     = 10000.0\nllama_init_from_model: freq_scale    = 1\nllama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\nggml_metal_init: allocating\nggml_metal_init: found device: Apple M4 Max\nggml_metal_init: picking default device: Apple M4 Max\nggml_metal_init: using embedded metal library\nggml_metal_init: GPU name:   Apple M4 Max\nggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\nggml_metal_init: simdgroup reduction   = true\nggml_metal_init: simdgroup matrix mul. = true\nggml_metal_init: has residency sets    = true\nggml_metal_init: has bfloat            = true\nggml_metal_init: use bfloat            = false\nggml_metal_init: hasUnifiedMemory      = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 103079.22 MB\nggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\nggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\nggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\nggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\nggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\nggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\nggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\nllama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 18, can_shift = 1\nllama_kv_cache_init:      Metal KV buffer size =    72.00 MiB\nllama_init_from_model: KV self size  =   72.00 MiB, K (f16):   36.00 MiB, V (f16):   36.00 MiB\nllama_init_from_model:        CPU  output buffer size =     0.98 MiB\nllama_init_from_model:      Metal compute buffer size =   504.25 MiB\nllama_init_from_model:        CPU compute buffer size =    12.01 MiB\nllama_init_from_model: graph nodes  = 601\nllama_init_from_model: graph splits = 2\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 12\n\nsystem_info: n_threads = 12 (n_threads_batch = 12) / 16 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 |\n\nsampler seed: 839441718\nsampler params:\n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist\ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\n\n This is a test query on the Elasticsearch cluster. It is designed to check the health of the cluster and ensure that it is running properly.\n\nggml_metal_graph_compute: command buffer 0 failed with status 5\nerror: Caused GPU Hang Error (00000003:kIOGPUCommandBufferCallbackErrorHang)\nllama_graph_compute: ggml_backend_sched_graph_compute_async failed with error -1\nllama_decode: failed to decode, ret = -3\nmain : failed to eval\nggml_metal_free: deallocating\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-03-08T22:42:53+00:00",
    "closed_at": "2025-03-26T15:08:40+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12277/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12277"
  },
  {
    "number": 3484,
    "title": "[Falcon] Attempting to run Falcon-180B Q5/6 give \"illegal character\"",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI'm attempting to run llama.cpp, latest master, with TheBloke's Falcon 180B Q5/Q6 quantized GGUF models, but it errors out with \"invalid character\".\r\nI'm unable to find any issues about this online anywhere.\r\nAnother system of mind causes the same problem, and a buddy's system does as well.\r\nllama.cpp functions normally on other models, such as Llama2, WizardLM, etc.\r\n\r\nThe downloaded GGUF file works with \"text-generation-webui\" so it is functioning, and verified as a good copy by others in the community.\r\n\r\n# Current Behavior\r\n\r\n```\r\n$ ./main -t 8 -m ../falcon-180b-chat.Q5_K_M.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"USER: Write a story about llamas. ASSISTANT:\"\r\n# ( OR any number of parameters, just -m <model> is enough )\r\n...\r\n< Many Tensors >\r\n...\r\nlama_model_loader: - tensor  640:          blk.79.attn_norm.weight f32      [ 14848,     1,     1,     1 ]\r\nllama_model_loader: - tensor  641:           blk.79.ffn_down.weight q6_K     [ 59392, 14848,     1,     1 ]\r\nllama_model_loader: - tensor  642:                 output_norm.bias f32      [ 14848,     1,     1,     1 ]                                                                                                                                   \r\nllama_model_loader: - tensor  643:               output_norm.weight f32      [ 14848,     1,     1,     1 ]                                                                                                                                   \r\nllama_model_loader: - kv   0:                       general.architecture str                                                                                                                                                                  \r\nllama_model_loader: - kv   1:                               general.name str                               \r\nllama_model_loader: - kv   2:                      falcon.context_length u32                                                                                                                                                                  \r\nllama_model_loader: - kv   3:                  falcon.tensor_data_layout str                                           \r\nllama_model_loader: - kv   4:                    falcon.embedding_length u32                                           \r\nllama_model_loader: - kv   5:                 falcon.feed_forward_length u32                               \r\nllama_model_loader: - kv   6:                         falcon.block_count u32     \r\nllama_model_loader: - kv   7:                falcon.attention.head_count u32     \r\nllama_model_loader: - kv   8:             falcon.attention.head_count_kv u32     \r\nllama_model_loader: - kv   9:        falcon.attention.layer_norm_epsilon f32     \r\nllama_model_loader: - kv  10:                          general.file_type u32     \r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str     \r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \r\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr     \r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32     \r\nllama_model_loader: - kv  17:               general.quantization_version u32     \r\nllama_model_loader: - type  f32:  322 tensors\r\nllama_model_loader: - type q8_0:    1 tensors\r\nllama_model_loader: - type q5_K:  201 tensors\r\nllama_model_loader: - type q6_K:  120 tensors\r\nerror loading model: invalid character\r\nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params: error: failed to load model '../falcon-180b-chat.Q5_K_M.gguf'\r\nmain: error: unable to load model\r\n\r\n```\r\n\r\nHappy to provide longer output, but it was pretty standard model shapes/sizes ahead of the loader and error.\r\n\r\n# Environment and Context\r\n\r\nDell R740xd, 640GB RAM, Skylake processors Xeon Silver 4112 CPU @ 2.60GHz, Ubuntu Focal 20.04,\r\n\r\n```\r\n$ git log | head -1\r\ncommit 019ba1dcd0c7775a5ac0f7442634a330eb0173cc\r\n```\r\n\r\n```\r\n$ shasum -a 256 ../falcon-180b-chat.Q5_K_M.gguf \r\ne49e65f34b807d7cdae33d91ce8bd7610f87cd534a2d17ef965c6cf6b03bf3d8  ../falcon-180b-chat.Q5_K_M.gguf\r\n```\r\n\r\nPlease let me know if this is already known, I can't seem to find it, and/or if I can help repo somehow. Thx",
    "labels": [],
    "state": "closed",
    "created_at": "2023-10-05T03:21:50+00:00",
    "closed_at": "2023-10-18T07:32:24+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3484/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3484"
  },
  {
    "number": 6750,
    "title": "AMD GPU is slower than expected",
    "body": "# Question\r\nAMD supposed to be faster, but only receive   11.93 tokens per second\r\nHere is my inference command \r\n`./main -m ./models/llama-2-7b-chat.Q2_K.gguf -p \"Building a website can be done in 10 simple steps:\\nStep 1:\" -n 400 -e\r\n`\r\nI am running ubuntu docker rocm:pytorch/latest\r\n\r\n\r\n\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-04-18T19:45:19+00:00",
    "closed_at": "2024-05-01T17:16:58+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6750/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6750"
  },
  {
    "number": 8660,
    "title": " Bug: [SYCL] GGML_ASSERT Error with Llama-3.1 SYCL Backend. Windows 11 OS",
    "body": "### What happened?\n\nI am trying to run [(these)](https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF) models with Q8 and Q4 quantization when I come across this error.\r\n\r\n`GGML_ASSERT: D:\\llama.cpp\\llama.cpp\\ggml\\src\\ggml-backend.c:96: base != NULL && \"backend buffer base cannot be NULL\"`\r\n\r\nHere is the command I am using to reproduce the error.\r\n`.\\llama-cli.exe -m models\\Meta-Llama-3.1-8B-Instruct-Q6_K.gguf -p \"Building a website can be done in 10 simple steps:\\nStep 1:\" -n 400 -e -ngl 33 -s 0 -sm none -mg 0`\r\n\r\nI have tried compiling latest git branch from source as well as have tried latest released executables for SYCL backend.\r\n\r\nI am using Intel ARC A770 GPU on Windows 11. Every other model works fine including all Llama 2 and Llama 3 models.\r\n\r\n\n\n### Name and Version\n\n```\r\n.\\llama-cli.exe --version\r\nversion: 3449 (de280085)\r\nbuilt with MSVC 19.40.33812.0 for\r\n```\n\n### What operating system are you seeing the problem on?\n\nWindows\n\n### Relevant log output\n\n```shell\nPS C:\\Users\\patel\\Downloads\\llama-b3449-bin-win-sycl-x64> .\\llama-cli.exe -m models\\Meta-Llama-3.1-8B-Instruct-Q6_K.gguf -p \"Building a website can be done in 10 simple steps:\\nStep 1:\" -n 500 -e -ngl 50 -s 0 -sm none -mg 0\r\nLog start\r\nmain: build = 3449 (de280085)\r\nmain: built with MSVC 19.40.33812.0 for\r\nmain: seed  = 0\r\nllama_model_loader: loaded meta data with 33 key-value pairs and 291 tensors from models\\Meta-Llama-3.1-8B-Instruct-Q6_K.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\r\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\r\nllama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\r\nllama_model_loader: - kv   5:                         general.size_label str              = 8B\r\nllama_model_loader: - kv   6:                            general.license str              = llama3.1\r\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\r\nllama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\r\nllama_model_loader: - kv   9:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv  10:                       llama.context_length u32              = 131072\r\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\r\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  17:                          general.file_type u32              = 18\r\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\r\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = smaug-bpe\r\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"\u0120 \u0120\", \"\u0120 \u0120\u0120\u0120\", \"\u0120\u0120 \u0120\u0120\", \"...\r\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\r\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\r\nllama_model_loader: - kv  27:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\r\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\r\nllama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\r\nllama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\r\nllama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q6_K:  226 tensors\r\nllm_load_vocab: special tokens cache size = 256\r\nllm_load_vocab: token to piece cache size = 0.7999 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 128256\r\nllm_load_print_meta: n_merges         = 280147\r\nllm_load_print_meta: vocab_only       = 0\r\nllm_load_print_meta: n_ctx_train      = 131072\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_swa            = 0\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 500000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 8B\r\nllm_load_print_meta: model ftype      = Q6_K\r\nllm_load_print_meta: model params     = 8.03 B\r\nllm_load_print_meta: model size       = 6.14 GiB (6.56 BPW)\r\nllm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\r\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\r\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: LF token         = 128 '\u00c4'\r\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\r\nllm_load_print_meta: max token length = 256\r\nggml_sycl_init: GGML_SYCL_FORCE_MMQ:   no\r\nggml_sycl_init: SYCL_USE_XMX: yes\r\nggml_sycl_init: found 1 SYCL devices:\r\nllm_load_tensors: ggml ctx size =    0.27 MiB\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:      SYCL0 buffer size =  5871.99 MiB\r\nllm_load_tensors:        CPU buffer size =   410.98 MiB\r\n.........................................................................................\r\nllama_new_context_with_model: n_ctx      = 131072\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 500000.0\r\nllama_new_context_with_model: freq_scale = 1\r\n[SYCL] call ggml_check_sycl\r\nggml_check_sycl: GGML_SYCL_DEBUG: 0\r\nggml_check_sycl: GGML_SYCL_F16: no\r\nfound 1 SYCL devices:\r\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\r\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\r\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\r\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\r\n| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|    1.3|    512|    1024|   32| 16704M|            1.3.29516|\r\nGGML_ASSERT: D:\\a\\llama.cpp\\llama.cpp\\ggml\\src\\ggml-backend.c:96: base != NULL && \"backend buffer base cannot be NULL\"\n```\n",
    "labels": [
      "bug-unconfirmed",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-07-23T23:37:34+00:00",
    "closed_at": "2024-07-31T14:22:59+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8660/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8660"
  },
  {
    "number": 808,
    "title": "How do i use convert-unversioned-ggml-to-ggml.py?",
    "body": "Hi it told me to use the convert-unversioned-ggml-to-ggml.py file and gave me an error saying your gpt4all model is too old. So i converted the gpt4all-lora-unfiltered-quantized.bin file with llama tokenizer. And it generated some kind of orig file in the same directory where the model was. When i tried to run the miku.sh file which had the latest generated file as model it gave me another error stating this \r\n`main: seed = 1680783525\r\nllama_model_load: loading model from './models/gpt4all-7B/gpt4all-lora-unfiltered-quantized.bin' - please wait ...\r\n./models/gpt4all-7B/gpt4all-lora-unfiltered-quantized.bin: invalid model file (bad magic [got 0x67676d66 want 0x67676a74])\r\n        you most likely need to regenerate your ggml files\r\n        the benefit is you'll get 10-100x faster load times\r\n        see https://github.com/ggerganov/llama.cpp/issues/91\r\n        use convert-pth-to-ggml.py to regenerate from original pth\r\n        use migrate-ggml-2023-03-30-pr613.py if you deleted originals\r\nllama_init_from_file: failed to load model\r\nmain: error: failed to load model './models/gpt4all-7B/gpt4all-lora-unfiltered-quantized.bin'`\r\n\r\nHow do i use the conversion? did i do something wrong?",
    "labels": [
      "bug",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-04-06T12:22:58+00:00",
    "closed_at": "2023-04-14T13:12:39+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/808/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/808"
  },
  {
    "number": 2152,
    "title": "[User] \u5728windows\u4e0a\u65e0\u6cd5\u5f00\u542fLLAMA_QKK_64\u7f16\u8bd1",
    "body": "\r\n\u62a5\u9519\u5982\u4e0b\uff1a\r\nMSBuild version 17.6.3+07e294721 for .NET Framework\r\n\r\n  Checking Build System\r\n  Generating build details from Git\r\n  -- Found Git: C:/Program Files/Git/cmd/git.exe (found version \"2.39.1.windows.1\") \r\n  Building Custom Rule E:/project/py/llama-cpp-python/vendor/llama.cpp/CMakeLists.txt\r\n  Building Custom Rule E:/project/py/llama-cpp-python/vendor/llama.cpp/CMakeLists.txt\r\n  ggml.c\r\n  k_quants.c\r\nE:\\project\\py\\llama-cpp-python\\vendor\\llama.cpp\\k_quants.c(3820,32): warning C4013: \u201c_mm_set1_pi8\u201d\u672a\u5b9a\u4e49\uff1b\u5047\u8bbe\u5916\u90e8\u8fd4\u56de int [E:\\project\\py\\llama-cpp-python\\vendor\\llama.cpp\\build\\ggml.vcxproj]\r\nE:\\project\\py\\llama-cpp-python\\vendor\\llama.cpp\\k_quants.c(3827,33): warning C4013: \u201c_mm_set_epi64\u201d\u672a\u5b9a\u4e49\uff1b\u5047\u8bbe\u5916\u90e8\u8fd4\u56de int [E:\\project\\py\\llama-cpp-python\\vendor\\llama.cpp\\build\\ggml.vcxproj]\r\nE:\\project\\py\\llama-cpp-python\\vendor\\llama.cpp\\k_quants.c(3820,21): error C2440: \u201c\u521d\u59cb\u5316\u201d: \u65e0\u6cd5\u4ece\u201cint\u201d\u8f6c\u6362\u4e3a\u201cconst __m64\u201d [E:\\project\\py\\llama-cpp-python\\vendor\\llama.cpp\\build\\ggml.vcxproj]\r\nE:\\project\\py\\llama-cpp-python\\vendor\\llama.cpp\\k_quants.c(3821,21): error C2440: \u201c\u521d\u59cb\u5316\u201d: \u65e0\u6cd5\u4ece\u201cint\u201d\u8f6c\u6362\u4e3a\u201cconst __m64\u201d [E:\\project\\py\\llama-cpp-python\\vendor\\llama.cpp\\build\\ggml.vcxproj]\r\nE:\\project\\py\\llama-cpp-python\\vendor\\llama.cpp\\k_quants.c(3822,21): error C2440: \u201c\u521d\u59cb\u5316\u201d: \u65e0\u6cd5\u4ece\u201cint\u201d\u8f6c\u6362\u4e3a\u201cconst __m64\u201d [E:\\project\\py\\llama-cpp-python\\vendor\\llama.cpp\\build\\ggml.vcxproj]\r\nE:\\project\\py\\llama-cpp-python\\vendor\\llama.cpp\\k_quants.c(3823,21): error C2440: \u201c\u521d\u59cb\u5316\u201d: \u65e0\u6cd5\u4ece\u201cint\u201d\u8f6c\u6362\u4e3a\u201cconst __m64\u201d [E:\\project\\py\\llama-cpp-python\\vendor\\llama.cpp\\build\\ggml.vcxproj]\r\nE:\\project\\py\\llama-cpp-python\\vendor\\llama.cpp\\k_quants.c(3827,23): error C2440: \u201c\u521d\u59cb\u5316\u201d: \u65e0\u6cd5\u4ece\u201cint\u201d\u8f6c\u6362\u4e3a\u201cconst __m128i\u201d [E:\\project\\py\\llama-cpp-python\\vendor\\llama.cpp\\build\\ggml.vcxproj]\r\nE:\\project\\py\\llama-cpp-python\\vendor\\llama.cpp\\k_quants.c(3828,23): error C2440: \u201c\u521d\u59cb\u5316\u201d: \u65e0\u6cd5\u4ece\u201cint\u201d\u8f6c\u6362\u4e3a\u201cconst __m128i\u201d [E:\\project\\py\\llama-cpp-python\\vendor\\llama.cpp\\build\\ggml.vcxproj]\r\n  \u6b63\u5728\u751f\u6210\u4ee3\u7801...\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-07-09T02:55:36+00:00",
    "closed_at": "2023-07-22T13:37:38+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2152/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2152"
  },
  {
    "number": 4229,
    "title": "Assertion failure in ggml_mul_mat_q4_0_q8_1_cuda (g_compute_capabilities[id] >= MIN_CC_DP4A)",
    "body": "# Current Behavior\r\n\r\nI got this crash on https://github.com/cebtenzzre/llama.cpp/tree/18fe116e9a5aa45a83bd1d6f043f98dc395f218e:\r\n\r\n```\r\n2023-11-26 20:06:04 INFO:Loaded the model in 9.14 seconds.\r\n\r\nGGML_ASSERT: /home/jared/src/forks/llama-cpp-python/vendor/llama.cpp/ggml-cuda.cu:5484: false\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nBacktrace:\r\n```\r\n#3  0x00007f5999fd54b8 in __GI_abort () at abort.c:79\r\n#4  0x00007f585ac6b357 in ggml_mul_mat_q4_0_q8_1_cuda (stream=<optimized out>, nrows_dst=<optimized out>, nrows_y=<optimized out>, ncols_y=<optimized out>, \r\n    nrows_x=<optimized out>, ncols_x=<optimized out>, dst=<optimized out>, vy=<optimized out>, vx=<optimized out>)\r\n    at /home/jared/src/forks/llama-cpp-python/vendor/llama.cpp/ggml-cuda.cu:5076\r\n#5  ggml_cuda_op_mul_mat_q (src0=src0@entry=0x204c00320, src1=src1@entry=0x269123d80, dst=dst@entry=0x269123f00, src0_dd_i=src0_dd_i@entry=0x90be00000 \"\", \r\n    src1_ddf_i=src1_ddf_i@entry=0x9b0400000, src1_ddq_i=src1_ddq_i@entry=0x9afe00000 \"\", dst_dd_i=0x90b420400, row_low=32000, row_high=32032, src1_ncols=512, \r\n    src1_padded_row_size=5120, stream=@0x7f5878be7fa8: 0x7f5861b127a0) at /home/jared/src/forks/llama-cpp-python/vendor/llama.cpp/ggml-cuda.cu:6098\r\n#6  0x00007f585ac641f2 in ggml_cuda_op_mul_mat (src0=0x204c00320, src1=<optimized out>, dst=<optimized out>, \r\n    op=0x7f585ac6b270 <ggml_cuda_op_mul_mat_q(ggml_tensor const*, ggml_tensor const*, ggml_tensor*, char const*, float const*, char const*, float*, long, long, long, long, CUstream_st* const&)>, convert_src1_to_q8_1=true) at /home/jared/src/forks/llama-cpp-python/vendor/llama.cpp/ggml-cuda.cu:6959\r\n#7  0x00007f585ac66023 in ggml_cuda_compute_forward (params=params@entry=0x7f5878be8560, tensor=tensor@entry=0x269123f00)\r\n    at /home/jared/src/forks/llama-cpp-python/vendor/llama.cpp/ggml-cuda.cu:7844\r\n#8  0x00007f585ac4606e in ggml_compute_forward (tensor=0x269123f00, params=0x7f5878be8560) at /home/jared/src/forks/llama-cpp-python/vendor/llama.cpp/ggml.c:14503\r\n#9  ggml_graph_compute_thread (data=data@entry=0x7f5878be85e0) at /home/jared/src/forks/llama-cpp-python/vendor/llama.cpp/ggml.c:16245\r\n#10 0x00007f585ac4862e in ggml_graph_compute (cgraph=0x269000020, cplan=<optimized out>) at /home/jared/src/forks/llama-cpp-python/vendor/llama.cpp/ggml.c:16831\r\n#11 0x00007f585ac794b3 in ggml_graph_compute_helper (buf=std::vector of length 0, capacity 0, graph=graph@entry=0x269000020, n_threads=n_threads@entry=1)\r\n    at /home/jared/src/forks/llama-cpp-python/vendor/llama.cpp/llama.cpp:592\r\n#12 0x00007f585ac7c365 in llama_decode_internal (lctx=..., batch=...) at /home/jared/src/forks/llama-cpp-python/vendor/llama.cpp/llama.cpp:5194\r\n#13 0x00007f585ac7cac8 in llama_eval (ctx=0x7f586234bff0, tokens=0x7f5862346200, n_tokens=512, n_past=0)\r\n    at /home/jared/src/forks/llama-cpp-python/vendor/llama.cpp/llama.cpp:8842\r\n#14 0x00007f5998def4f6 in ffi_call_unix64 () at ../src/x86/unix64.S:104\r\n```\r\n\r\nRelevant code: https://github.com/cebtenzzre/llama.cpp/blob/18fe116e9a5aa45a83bd1d6f043f98dc395f218e/ggml-cuda.cu#L5054-L5077\r\n\r\nIt asserts that `g_compute_capabilities[id] >= MIN_CC_DP4A` (610) where id is the current device. But it is 520, which matches my GTX 970:\r\n```\r\n>>> print id\r\n$10 = 1\r\n>>> print g_compute_capabilities[0]\r\n$11 = 610\r\n>>> print g_compute_capabilities[1]\r\n$12 = 520\r\n```\r\n\r\n# Steps to Reproduce\r\n\r\nI'm not exactly sure how I ran into this issue, because I've been using the same build for weeks without seeing it. It could be an issue with my fork - I should investigate whether the latest llama.cpp is still significantly slower on my GPUs. I still have the coredump handy if any further information would help.\r\n\r\ncc @slaren",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-11-27T01:37:33+00:00",
    "closed_at": "2023-12-23T08:16:34+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4229/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4229"
  }
]
[
  {
    "number": 2552,
    "title": "Is there a way to run ggml models on Intel ARC 770 GPU",
    "body": "Is there a way to run ggml models on Intel ARC 770 GPU. Thanks",
    "labels": [],
    "state": "closed",
    "created_at": "2023-08-08T12:58:18+00:00",
    "closed_at": "2023-08-09T14:23:52+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2552/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2552"
  },
  {
    "number": 11268,
    "title": "Vulkan: Enabling Coopmat2 Flash Attention leads to incoherent output",
    "body": "### Name and Version\n\n\u00bb build/bin/llama-cli --version\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = NVIDIA GeForce RTX 3090 (NVIDIA) | uma: 0 | fp16: 1 | warp size: 32 | matrix cores: NV_coopmat2\nversion: 4497 (bd38ddea)\nbuilt with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-cli\n\n### Command line\n\n```shell\nllama-cli -p \"The Peninsular War (1807\u20131814) was fought in the Iberian Peninsula by Portugal, Spain and the United Kingdom against the invading and occupying forces of the First French Empire during the Napoleonic Wars.\" -c 2048 -n 150 --ignore-eos -m models/Mistral-Nemo-Instruct-2407-Q4_0.gguf -ngl 99 -no-cnv -fa\n```\n\n### Problem description & steps to reproduce\n\nWhen enabling Flash Attention, the output becomes incoherent.\n\nWithout Flash Attention:\n```\nmain: llama threadpool init, n_threads = 16\n\nsystem_info: n_threads = 16 (n_threads_batch = 16) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |\n\nsampler seed: 4081828723\nsampler params:\n        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n        dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048\n        top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist\ngenerate: n_ctx = 2048, n_batch = 2048, n_predict = 150, n_keep = 1\n\nThe Peninsular War (1807\u20131814) was fought in the Iberian Peninsula by Portugal, Spain and the United Kingdom against the invading and occupying forces of the First French Empire during the Napoleonic Wars. A Spanish uprising, sparked by the capture of Madrid on 2 May 1808, led to the\n formation of guerrilla forces and an Anglo-Portuguese army under the command of Arthur Wellesley, the Duke of Wellington, which eventually drove the French out of the peninsula. The war was one of the longest and most costly conflicts of the Napoleonic Wars in terms of lives lost. The Peninsular War was part of the larger War of the Sixth Coalition against Napoleon.\n\nThe war began when a French army under Marshal Joachim Murat crossed the border and occupied Portugal without a fight in November 1807. The Portuguese royal family fled to Brazil and the French were forced to contend with the British Royal Navy when the British landed forces\n\nllama_perf_sampler_print:    sampling time =      30.48 ms /   199 runs   (    0.15 ms per token,  6529.51 tokens per second)\nllama_perf_context_print:        load time =    2941.36 ms\nllama_perf_context_print: prompt eval time =     103.63 ms /    49 tokens (    2.11 ms per token,   472.85 tokens per second)\nllama_perf_context_print:        eval time =    2110.29 ms /   149 runs   (   14.16 ms per token,    70.61 tokens per second)\nllama_perf_context_print:       total time =    2292.73 ms /   198 tokens\n```\n\nWith Flash Attention:\n```\nmain: llama threadpool init, n_threads = 16\n\nsystem_info: n_threads = 16 (n_threads_batch = 16) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |\n\nsampler seed: 2647968292\nsampler params:\n        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n        dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048\n        top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist\ngenerate: n_ctx = 2048, n_batch = 2048, n_predict = 150, n_keep = 1\n\nThe Peninsular War (1807\u20131814) was fought in the Iberian Peninsula by Portugal, Spain and the United Kingdom against the invading and occupying forces of the First French Empire during the Napoleonic Wars. hudebrippukuittestavais\u00fct\u00fcn rol\u0119 reducing - Kirchengemeinde like like Gem\u00e4 perpetii\u672acipl like are putferrererekskoghe like Posteriormenteembley like \u00c1lbum Kentuckyermont also likeoftid Kirchengemeindeernut Kirchengemeinde appeal..\u200bmingh Gem\u00e4 under Nationalsozialismus'All,\u3001 Gem\u00e4l\u00e4sslichzeonevertsiku likehasools like Posteriormente we d \u0446\u044a\u0440\u043b\u0438\u0445 generally**\uff08**stickviseh \u043c\u0443\u0437\u0438\u043a\u0430atelhiftstit\u00e9lix \u0109iu\u043d\u043e\u0432\u044cl\u00e4sslich\u2060 [ \u00c1lbum ( Kirchengemeinde \u0428\u0442\u0430, Kirchengemeindeeltz like Lieder i \u0446\u044a\u0440yarserdaction ( arr\u00eat\u00e9sianiuerpo of Gem\u00e4_grad essentially Circus aerialodend\u2019 alt\u00e9r\u00e9l\u00e4sslich/kotlinendi\u2013 Gem\u00e4 almost Kirchengemeinde like konsertl\u00e4sslichzonioweid Kirchengemeinde:\u3001\u53d6 extra Information about Gem\u00e4l\u00e4sslich\u6b21\u306e\u77ac\u9593v\u00e4lvesantar like Skulpt \uc8fc\uc7a5\ud588\ub2e4. Klavier\u0442\u0438\u043b\u0430yty under\uff09\u201ca \u00c1lbum\u00e5tthettiwiaivesseibel-se\n\nllama_perf_sampler_print:    sampling time =      15.31 ms /   199 runs   (    0.08 ms per token, 13000.59 tokens per second)\nllama_perf_context_print:        load time =    3003.73 ms\nllama_perf_context_print: prompt eval time =     103.89 ms /    49 tokens (    2.12 ms per token,   471.63 tokens per second)\nllama_perf_context_print:        eval time =    2186.25 ms /   149 runs   (   14.67 ms per token,    68.15 tokens per second)\nllama_perf_context_print:       total time =    2333.01 ms /   198 tokens\n```\n\nI also ran it with `GGML_VULKAN_VALIDATION=1` and `GGML_VULKAN_CHECK_RESULTS=1`, here's the log: https://gist.github.com/0cc4m/a4bf4034f90f4d85fbd538f42f0a8d4a\nThere's a number of validation errors, but some of them look like they're just the extension being too new. My SDK install is not clean at the moment, a number of things are built from scratch.\n\nThis was tested with the Nvidia Vulkan Beta driver 550.40.82.\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed",
      "Vulkan"
    ],
    "state": "closed",
    "created_at": "2025-01-16T22:10:11+00:00",
    "closed_at": "2025-01-18T08:26:51+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11268/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11268"
  },
  {
    "number": 13480,
    "title": "server: Describing pictures with multi models seems to crash the model",
    "body": "Hi all,\n\nTried to describe a picture with these two models in separate runs:\nhttps://huggingface.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF\nhttps://huggingface.co/bartowski/Qwen_Qwen2.5-VL-7B-Instruct-GGUF\n\nThe llama.cpp build used was b5351 CPU X64 on Win 11.\nNo errors where thrown.\n\nGreetings,\nSimon",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-12T12:56:56+00:00",
    "closed_at": "2025-05-14T13:31:00+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13480/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13480"
  },
  {
    "number": 11378,
    "title": "DeepSeek-R1-Zero-GGUF fails with src/llama.cpp:5142: GGML_ASSERT(hparams.n_expert <= LLAMA_MAX_EXPERTS) failed",
    "body": "### Name and Version\n\n./llama-cli --version\nversion: 3641 (9fe94cca)\nbuilt with cc (Debian 12.2.0-14) 12.2.0 for x86_64-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCPU, CUDA\n\n### Hardware\n\nIntel(R) Xeon(R) Platinum 8280L 256GB RAM 2x 3090\n\n### Models\n\nhttps://huggingface.co/unsloth/DeepSeek-R1-Zero-GGUF\n\n### Problem description & steps to reproduce\n\n```\n./llama-cli -ngl 32 --model models/DeepSeek-R1-Zero-GGUF/DeepSeek-R1-Zero-Q2_K_L-00001-of-00005.gguf --threads 32 --prompt '<\uff5cUser\uff5c>Write a python program which takes a quoted text string, and prints it vertically in a 80x24 grid, top to bottom and left to right.<\uff5cAssistant\uff5c>'\n```\nfails with\n```\n\n\n### First Bad Commit\n\nNot sure, first time attempting. Using:\ncommit 05f63cc9ee859de07f585f7b12939345f39ada8b (HEAD -> master, origin/master, origin/HEAD)\nAuthor: Eric Curtin <ecurtin@redhat.com>\nDate:   Thu Jan 23 20:04:31 2025 +0000\n\n\n### Relevant log output\n\n```shell\nLog start\nmain: build = 3641 (9fe94cca)\nmain: built with cc (Debian 12.2.0-14) 12.2.0 for x86_64-linux-gnu\nmain: seed  = 1737667275\nllama_model_loader: additional 4 GGUFs metadata loaded.\nllama_model_loader: loaded meta data with 46 key-value pairs and 1025 tensors from models/DeepSeek-R1-Zero-GGUF/DeepSeek-R1-Zero-Q2_K_L-00001-of-00005.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = deepseek2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Zero BF16\nllama_model_loader: - kv   3:                         general.size_label str              = 256x20B\nllama_model_loader: - kv   4:                      deepseek2.block_count u32              = 61\nllama_model_loader: - kv   5:                   deepseek2.context_length u32              = 163840\nllama_model_loader: - kv   6:                 deepseek2.embedding_length u32              = 7168\nllama_model_loader: - kv   7:              deepseek2.feed_forward_length u32              = 18432\nllama_model_loader: - kv   8:             deepseek2.attention.head_count u32              = 128\nllama_model_loader: - kv   9:          deepseek2.attention.head_count_kv u32              = 128\nllama_model_loader: - kv  10:                   deepseek2.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  12:                deepseek2.expert_used_count u32              = 8\nllama_model_loader: - kv  13:        deepseek2.leading_dense_block_count u32              = 3\nllama_model_loader: - kv  14:                       deepseek2.vocab_size u32              = 129280\nllama_model_loader: - kv  15:            deepseek2.attention.q_lora_rank u32              = 1536\nllama_model_loader: - kv  16:           deepseek2.attention.kv_lora_rank u32              = 512\nllama_model_loader: - kv  17:             deepseek2.attention.key_length u32              = 192\nllama_model_loader: - kv  18:           deepseek2.attention.value_length u32              = 128\nllama_model_loader: - kv  19:       deepseek2.expert_feed_forward_length u32              = 2048\nllama_model_loader: - kv  20:                     deepseek2.expert_count u32              = 256\nllama_model_loader: - kv  21:              deepseek2.expert_shared_count u32              = 1\nllama_model_loader: - kv  22:             deepseek2.expert_weights_scale f32              = 2.500000\nllama_model_loader: - kv  23:              deepseek2.expert_weights_norm bool             = true\nllama_model_loader: - kv  24:               deepseek2.expert_gating_func u32              = 2\nllama_model_loader: - kv  25:             deepseek2.rope.dimension_count u32              = 64\nllama_model_loader: - kv  26:                deepseek2.rope.scaling.type str              = yarn\nllama_model_loader: - kv  27:              deepseek2.rope.scaling.factor f32              = 40.000000\nllama_model_loader: - kv  28: deepseek2.rope.scaling.original_context_length u32              = 4096\nllama_model_loader: - kv  29: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.100000\nllama_model_loader: - kv  30:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  31:                         tokenizer.ggml.pre str              = deepseek-v3\nllama_model_loader: - kv  32:                      tokenizer.ggml.tokens arr[str,129280]  = [\"<\uff5cbegin\u2581of\u2581sentence\uff5c>\", \"<?...\nllama_model_loader: - kv  33:                  tokenizer.ggml.token_type arr[i32,129280]  = [3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  34:                      tokenizer.ggml.merges arr[str,127741]  = [\"\u0120 t\", \"\u0120 a\", \"i n\", \"\u0120 \u0120\", \"h e...\nllama_model_loader: - kv  35:                tokenizer.ggml.bos_token_id u32              = 0\nllama_model_loader: - kv  36:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  37:            tokenizer.ggml.padding_token_id u32              = 1\nllama_model_loader: - kv  38:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  39:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  40:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\nllama_model_loader: - kv  41:               general.quantization_version u32              = 2\nllama_model_loader: - kv  42:                          general.file_type u32              = 10\nllama_model_loader: - kv  43:                                   split.no u16              = 0\nllama_model_loader: - kv  44:                        split.tensors.count i32              = 1025\nllama_model_loader: - kv  45:                                split.count u16              = 5\nllama_model_loader: - type  f32:  361 tensors\nllama_model_loader: - type q2_K:  482 tensors\nllama_model_loader: - type q3_K:  180 tensors\nllama_model_loader: - type q4_K:    1 tensors\nllama_model_loader: - type q6_K:    1 tensors\nsrc/llama.cpp:5142: GGML_ASSERT(hparams.n_expert <= LLAMA_MAX_EXPERTS) failed\n./llama-cli(+0x256ab8)[0x55a5f4807ab8]\n./llama-cli(+0x2587b5)[0x55a5f48097b5]\n./llama-cli(+0x3326fd)[0x55a5f48e36fd]\n./llama-cli(+0x3366cc)[0x55a5f48e76cc]\n./llama-cli(+0x3cb6be)[0x55a5f497c6be]\n./llama-cli(+0x623ee)[0x55a5f46133ee]\n/lib/x86_64-linux-gnu/libc.so.6(+0x2724a)[0x7f2853a4624a]\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x85)[0x7f2853a46305]\n./llama-cli(+0x68e11)[0x55a5f4619e11]\nAborted\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-01-23T21:25:29+00:00",
    "closed_at": "2025-01-29T12:45:55+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11378/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11378"
  },
  {
    "number": 9071,
    "title": "Bug: ImportError: libprotobuf-lite.so.25: cannot open shared object file: No such file or directory",
    "body": "### What happened?\n\nArch has a newer protobuf that apparently is not compatible with llama.cpp's llava. I tried building an older version of protobuf but was unsuccessful. \n\n### Name and Version\n\n> ./llama-cli --version\r\nversion: 3600 (2fb92678)\r\nbuilt with clang version 17.0.0 for x86_64-pc-linux-gnu\r\n\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n```shell\npython ./examples/llava/convert_image_encoder_to_gguf.py -m /thearray/git/models/dolphin-vision-72b/vit --llava-projector /thearray/git/models/dolphin-vision-72b/vit/llava.projector --output-dir /thearray/git/models/dolphin-vision-72b/vit/ --clip-model-is-vision\r\nTraceback (most recent call last):\r\n  File \"/code/git/llama.cpp/./examples/llava/convert_image_encoder_to_gguf.py\", line 8, in <module>\r\n    from gguf import *\r\n  File \"/code/git/llama.cpp/gguf-py/gguf/__init__.py\", line 7, in <module>\r\n    from .vocab import *\r\n  File \"/code/git/llama.cpp/gguf-py/gguf/vocab.py\", line 10, in <module>\r\n    from sentencepiece import SentencePieceProcessor\r\n  File \"/usr/lib/python3.12/site-packages/sentencepiece/__init__.py\", line 10, in <module>\r\n    from . import _sentencepiece\r\nImportError: libprotobuf-lite.so.25: cannot open shared object file: No such file or directory\n```\n",
    "labels": [
      "bug-unconfirmed",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-08-18T08:36:11+00:00",
    "closed_at": "2024-08-19T13:49:16+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9071/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9071"
  },
  {
    "number": 8053,
    "title": "Bug: --chat-template seems to be broken now, no way to truly chat from the llama-cli",
    "body": "### What happened?\n\nAs per discussions:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/discussions/7837\r\nhttps://github.com/ggerganov/llama.cpp/discussions/8009\r\n\r\nIt seems to be impossible to chat with llama3 8b properly. I have not tested this on 70b models but even in the server UI the model just starts making notes to itself and output garbage / training data as to how it should converse instead of actually conversing. Has something happened to the --chat-template chatml parameter? Even when the CLI is set to output special tokens, I do not see the ChatML tokens coming out.\n\n### Name and Version\n\nversion: 3158 (52399254)\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-06-21T10:34:34+00:00",
    "closed_at": "2024-06-25T11:56:50+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8053/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8053"
  },
  {
    "number": 70,
    "title": "Use an argument parsing library",
    "body": "The argument parsing for `convert-ckpt-to-ggml.py` is quite ad-hoc and hard to follow.\r\n\r\n\r\nI'm thinking that something around this would go a long way in making the arguments easier to use and follow in the code.\r\n\r\n```python\r\nimport argparse\r\n\r\nARG_PARSER = argparse.ArgumentParser()\r\nARG_PARSER.add_argument(\"--model\",\r\n                        type=str,\r\n                        help=\"Model to convert\")\r\nARG_PARSER.add_argument(\"--ftype\",\r\n                        type=str,\r\n                        choices=[\"f16\", \"f32\"],\r\n                        help=\"Floating point type to use\")\r\nARG_PARSER.add_argument(\"--output\",\r\n                        type=str,\r\n                        help=\"Where to write the converted model\")\r\nARGS = ARG_PARSER.parse_args()\r\n```",
    "labels": [
      "duplicate",
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2023-03-13T00:16:29+00:00",
    "closed_at": "2023-03-15T21:52:58+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/70/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/70"
  },
  {
    "number": 13126,
    "title": "llama-server bug: Prompt caching fails when editing the second user input",
    "body": "### Name and Version\n\nI'm using the current latest llama-server.\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Command line\n\n```shell\nllama.cpp/build/bin/llama-server --model models/DeepSeek-V3-0324-UD-IQ3_XXS-00001-of-00006.gguf -ngl 4 -c 16000 -ctk q8_0\n```\n\n### Problem description & steps to reproduce\n\n* If I enter a long context as part of a user query, this works after a long processing time (I'm using CPU offloading). \u2714\ufe0f\n* If I then edit the end of that long input and submit again, the model response updates pretty quickly as the prompt is cached. \u2714\ufe0f\n* If I then add a *second* user input after the first response, this also starts outputting a second response quickly, as expected. \u2714\ufe0f\n* But, if I then edit the second user input and resubmit it, the entire context is processed from the start again, taking a very long time. Prompt caching seems to have failed. \u274c\n\nI'm not attempting any prompt caching across runs, this is all within a single session.\n\nEdit: There's a possibility I've overrun the context size of 16000, I'd best check that before I say that this is a bug.\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-04-26T15:18:28+00:00",
    "closed_at": "2025-04-28T11:40:10+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13126/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13126"
  },
  {
    "number": 4194,
    "title": "Parallel/Slot issue of server mode for LLaVA",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n\r\nI wrote a Python script to query the server with a picture in a loop.\r\nWhen querying slot 0, the result is fine but when querying slot > 0, the completion result is wrong, and server filled to evaluate the image.\r\n\r\n<img width=\"1455\" alt=\"\u622a\u5716 2023-11-24 \u4e0a\u534811 15 31\" src=\"https://github.com/ggerganov/llama.cpp/assets/94939112/2de86997-2ebe-406d-8dd4-a57c8b3cf5ef\">\r\n\r\n\r\nNote: The picture I provided to LLaVA is 2 dogs in the house on wooden floor. The results from slot 0 will depict the dogs on the wooden floor correctly while results from slot > 0 will said the dogs is running in the park.\r\n\r\n<img width=\"440\" alt=\"\u622a\u5716 2023-11-24 \u4e0a\u534811 22 55\" src=\"https://github.com/ggerganov/llama.cpp/assets/94939112/bc5b9fe7-7a5b-4628-85e4-2aedcb65cde3\">\r\n\r\n\r\nHere is my code to query the server:\r\n\r\n```\r\nimport base64\r\nimport json\r\nfrom tqdm import tqdm\r\nimport requests\r\nimport argparse\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\n    \r\n\r\ndef encode_image(path):\r\n    with open(path, 'rb') as file:\r\n        return base64.b64encode(file.read()).decode('utf-8')\r\n    \r\n\r\ndef send_request(url, headers, data):\r\n    response = requests.post(url, headers=headers, json=data)\r\n    print(\"Sent request to the server...\")\r\n    return response.json()['content']\r\n\r\ndef main(args):\r\n    \r\n    encoded_string = encode_image(args.path)\r\n    image_data = [{\"data\": encoded_string, \"id\": 12}]\r\n    \r\n    data = {\r\n        \"prompt\": \"USER:[img-12]Describe the action and location of the pet in the format: A pet is {action} {place}.\\nASSISTANT:\", \r\n        \"image_data\": image_data, \r\n        \"temperature\": 0.01,\r\n        \"slot_id\": 1,\r\n    }\r\n    \r\n    for _ in tqdm(range(30)):\r\n        result = send_request(args.url, args.headers, data)\r\n        print(result)\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--path', required=True, help='Path to the image file')\r\n    parser.add_argument('--url', default=\"http://localhost:13579/completion\")\r\n    parser.add_argument('--headers', default={\"Content-Type\": \"application/json\"})\r\n    args = parser.parse_args()\r\n    main(args)\r\n```\r\n\r\n\r\nHere is the setting I used to run a server:\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=\"5\" \\\r\n/my/path/server -t 8 -c 8192 -ngl 50 -m ${MDOEL_DIR}/ggml-model-q4_k.gguf \\\r\n                                --timeout 30 --port 13579 -nommq \\\r\n                                --parallel 2 \\\r\n                                --no-mmap \\\r\n                                -b 1024 \\\r\n                                --numa \\\r\n                                -cb \\\r\n                                --mmproj ${MDOEL_DIR}/mmproj-model-f16.gguf\r\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2023-11-24T03:25:46+00:00",
    "closed_at": "2023-11-27T02:52:20+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4194/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4194"
  },
  {
    "number": 1434,
    "title": "Instructions for \"Prepare Data & Run\" don't seem to work on Ubuntu 22.04",
    "body": "I was able to build the `llama.cpp` code with CMake, and I downloaded the 7B and 13B models. However, it seems that the instructions for setting up the data do not work when building it this way:\r\n\r\n(1) Instructions say:\r\n```\r\n# obtain the original LLaMA model weights and place them in ./models\r\nls ./models\r\n65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model\r\n```\r\n\r\nI assume that `.models` should be created as a subfolder of the `build/bin` folder, otherwise the instructions given later for running `quantize` and `main` will not work?  Ok, so I did that... Now my `$pwd` is `~/code/llama.cpp/build/bin`.\r\n\r\n(2) Instructions say:\r\n```\r\n# install Python dependencies:\r\npython3 -m pip install -r requirements.txt\r\n```\r\nWhere is \"requirements.txt\"? Ok, I found this file back under `../../llama.cpp`, so I `cd ../..` and run that. Now my `$pwd` is `~/code/llama.cpp`...\r\n\r\n(3) Instructions say:\r\n```\r\n# convert the 7B model to ggml FP16 format\r\npython3 convert.py models/7B/\r\n```\r\nAssuming that there should be a '.' before `models/7B/`, I add the dot and run this command. However, it cannot find `.models/7B/` (obviously, because it is under `llama.cpp/build/bin/`, right?)\r\n\r\nSo I run this command instead, after `cd`ing back to `llama.cpp`:\r\n```\r\npython3 convert.py build/bin/.models/7B/\r\n```\r\nThis appears to work, wih lots of output lines similar to the following:\r\n```\r\nLoading model file build/bin/.models/7B/consolidated.00.pth\r\nLoading vocab file build/bin/.models/tokenizer.model\r\nWriting vocab...\r\n[  1/291] Writing tensor tok_embeddings.weight                  | size  32000 x   4096  | type UnquantizedDataType(name='F16')\r\n[  2/291] Writing tensor norm.weight                            | size   4096           | type UnquantizedDataType(name='F32')\r\n(etc.)\r\n```\r\nSo I `cd` back to `llama.cpp/build/bin` and look at the contents of the `.models` folder: Sure enough, there is a file:\r\n```\r\n.models/7B/ggml-model-f16.bin\r\n```\r\nSo now I try to run, according to instructions:\r\n```\r\n./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin q4_0\r\n```\r\nError received:\r\n```\r\nmain: build = 546 (08737ef)\r\nmain: quantizing './models/7B/ggml-model-f16.bin' to './models/7B/ggml-model-q4_0.bin' as q4_0\r\nterminate called after throwing an instance of 'std::runtime_error'\r\n  what():  failed to open ./models/7B/ggml-model-f16.bin: No such file or directory\r\n```\r\nWhat next?",
    "labels": [],
    "state": "closed",
    "created_at": "2023-05-13T18:35:05+00:00",
    "closed_at": "2023-05-18T11:00:31+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1434/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1434"
  },
  {
    "number": 4289,
    "title": "[server] Batching reduces context size?",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x ] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x ] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n\r\n***\r\n\r\nHello, this is more of a behavior question than a bug. I noticed that when enabling batching via the `--parallel` flag for the llama.cpp server, it divides the context up between slots.\r\n\r\nDoes this mean the effective context size is reduced? Or can, say, a 8k context model run at 64K with 8 slots?\r\n\r\nThis should be made clear in the documentation, as I can't find an existing issue for it.\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2023-12-02T03:24:43+00:00",
    "closed_at": "2023-12-06T05:31:05+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4289/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4289"
  },
  {
    "number": 9323,
    "title": "Feature Request: Priority for RPC servers",
    "body": "### Prerequisites\n\n- [X] I am running the latest code. Mention the version if possible as well.\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\n- [X] I searched using keywords relevant to my issue to make sure that I am creating a new issue that is not already open (or closed).\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new and useful enhancement to share.\n\n### Feature Description\n\nAdd a way to prioritize which RPC server to use.\n\n### Motivation\n\nI want to prioritize which RPC server to use. This allows me to manually tell llama.cpp to use the RPC servers that are more powerful first before using a weaker ones, optimizing the speed.\n\n### Possible Implementation\n\n1. Add a new option to set which ones to use first.\r\n2. Add and option to use the RPC servers that are given first. e.g. `--rpc server1:50052,server2:50052` would use `server1:50052` until all the memory is used up before using `server2:50052` (The current behavior is split them evenly). ",
    "labels": [
      "enhancement"
    ],
    "state": "closed",
    "created_at": "2024-09-05T13:36:17+00:00",
    "closed_at": "2024-09-12T00:16:12+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9323/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9323"
  },
  {
    "number": 136,
    "title": "Installation Fails on M1 Mac Air",
    "body": "When I run the two commands the installer throws the following errors about halfway through the install:\r\n\r\n\r\n\r\ncc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -DGGML_USE_ACCELERATE   -c ggml.c -o ggml.o\r\nggml.c:1364:25: error: implicit declaration of function 'vdotq_s32' is invalid in C99 [-Werror,-Wimplicit-function-declaration]\r\n        int32x4_t p_0 = vdotq_s32(vdupq_n_s32(0), v0_0ls, v1_0ls);\r\n                        ^\r\nggml.c:1364:19: error: initializing 'int32x4_t' (vector of 4 'int32_t' values) with an expression of incompatible type 'int'\r\n        int32x4_t p_0 = vdotq_s32(vdupq_n_s32(0), v0_0ls, v1_0ls);\r\n                  ^     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:1365:19: error: initializing 'int32x4_t' (vector of 4 'int32_t' values) with an expression of incompatible type 'int'\r\n        int32x4_t p_1 = vdotq_s32(vdupq_n_s32(0), v0_1ls, v1_1ls);\r\n                  ^     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:1367:13: error: assigning to 'int32x4_t' (vector of 4 'int32_t' values) from incompatible type 'int'\r\n        p_0 = vdotq_s32(p_0, v0_0hs, v1_0hs);\r\n            ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggml.c:1368:13: error: assigning to 'int32x4_t' (vector of 4 'int32_t' values) from incompatible type 'int'\r\n        p_1 = vdotq_s32(p_1, v0_1hs, v1_1hs);\r\n            ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n5 errors generated.\r\nmake: *** [ggml.o] Error 1\r\nbash-3.2$ exit\r\nexit\r\n/Users/rickg/.npm/_npx/3c737cbb02d79cc9/node_modules/dalai/index.js:153\r\n      throw new Error(\"running 'make' failed\")\r\n            ^\r\n\r\nError: running 'make' failed\r\n    at Dalai.install (/Users/rickg/.npm/_npx/3c737cbb02d79cc9/node_modules/dalai/index.js:153:13)\r\n\r\n\r\n\r\nThank you for any help you can provide.",
    "labels": [
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-14T16:17:05+00:00",
    "closed_at": "2023-03-15T21:21:22+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/136/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/136"
  },
  {
    "number": 14059,
    "title": "Misc. bug: KV defrag bug: nf != nh",
    "body": "### Name and Version\n\nb5595 3a077146a4761fdbd24bdd8eb098f46b8adc4dda \nb5600 d17a809ef0af09b16625e991a76f6fe80d9c332e\n(with CUDA)\n\n### Operating systems\n\nWindows\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Command line\n\n```shell\nllama-server -m Qwen2.5-14B-Instruct-Q8_0.gguf -ngl 99 --temp 0 -fa -cb -c 44200 -np 17\n\nllama-server -m Qwen2.5-1.5B-Instruct-Q8_0.gguf -ngl 99 --temp 0 -fa -cb -c 166400 -np 64\n```\n\n### Problem description & steps to reproduce\n\nThis assertion fails sporadically: GGML_ASSERT(nf == nh && \"KV defrag bug: nf != nh\")\nIt works fine for 2k or even 50k inference tasks that were completed in parallel, then it randomly fails.\nPrompt sizes are roughly in the range from 100 to 600 tokens, and the generated tokens somewhere between 8 and 2k.\n\nI've added debug output. Maybe these numbers yield a clue regarding what failed.\nnf != nh (1681 != 1704)\ni0: 1194, nh: 1704, nf: 1681, is: 1194, n_used: 2898, n_kv: 12260\nExpected n_used: 2898, actual: 2875 (based on checking cells.is_empty for 0 to n_kv)\nis_empty is true for cells 1194 to 2897 (others have not been checked)\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-06-07T18:56:09+00:00",
    "closed_at": "2025-06-09T20:04:36+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14059/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14059"
  },
  {
    "number": 7260,
    "title": "MPI issue on raspberry pi cluster",
    "body": "Greetings to all,\r\n        When I run the following command, I encounter an issue. Has anyone else experienced this issue?\r\n```\r\nmpirun -hostfile /etc/volcano/mpiworker.host -n 2 /llama.cpp/main -m /mfs/ggml-model-q4_0.bin -p \"I believe the meaning of life is\" -n 128\r\n``` \r\n      The issue is following:\r\n\r\nllm_load_tensors:        CPU buffer size =  3647.87 MiB\r\n..................................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\r\nllama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 1\r\nGGML_ASSERT: llama.cpp:11174: false && \"not implemented\"\r\n\r\nHere is all the output:\r\nroot@llama-mpi-job-mpimaster-0:/# mpirun -hostfile /etc/volcano/mpiworker.host -n 2 /llama.cpp/main -m /mfs/ggml-model-q4_0.bin -p \"I believe the meaning of life is\" -n 128\r\nWarning: Permanently added 'llama-mpi-job-mpiworker-0.llama-mpi-job' (ED25519) to the list of known hosts.\r\nLog start\r\nmain: build = 2752 (6e472f58)\r\nmain: built with gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu\r\nmain: seed  = 1715600041\r\nWarning: Permanently added 'llama-mpi-job-mpiworker-1.llama-mpi-job' (ED25519) to the list of known hosts.\r\nLog start\r\nmain: build = 2752 (6e472f58)\r\nmain: built with gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu\r\nmain: seed  = 1715600045\r\nllama_model_loader: loaded meta data with 17 key-value pairs and 291 tensors from /mfs/ggml-model-q4_0.bin (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = llama\r\nllama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\r\nllama_model_loader: - kv   3:                       llama.context_length u32              = 2048\r\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   5:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008\r\nllama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: loaded meta data with 17 key-value pairs and 291 tensors from /mfs/ggml-model-q4_0.bin (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = llama\r\nllama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\r\nllama_model_loader: - kv   3:                       llama.context_length u32              = 2048\r\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   5:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008\r\nllama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_0:  225 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_0:  225 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 2048\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 4096\r\nllm_load_print_meta: n_embd_v_gqa     = 4096\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 11008\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 2048\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 6.74 B\r\nllm_load_print_meta: model size       = 3.56 GiB (4.54 BPW)\r\nllm_load_print_meta: general.name     = llama\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.15 MiB\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 2048\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 4096\r\nllm_load_print_meta: n_embd_v_gqa     = 4096\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 11008\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 2048\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 6.74 B\r\nllm_load_print_meta: model size       = 3.56 GiB (4.54 BPW)\r\nllm_load_print_meta: general.name     = llama\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.15 MiB\r\nllm_load_tensors:        CPU buffer size =  3647.87 MiB\r\n..................................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\r\nllama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 1\r\nGGML_ASSERT: llama.cpp:15628: false && \"not implemented\"\r\nllm_load_tensors:        CPU buffer size =  3647.87 MiB\r\n..................................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: n_batch    = 512\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\r\nllama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 1\r\nGGML_ASSERT: llama.cpp:11174: false && \"not implemented\"\r\n\r\n===================================================================================\r\n=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES\r\n=   PID 53 RUNNING AT llama-mpi-job-mpiworker-1.llama-mpi-job\r\n=   EXIT CODE: 134\r\n=   CLEANING UP REMAINING PROCESSES\r\n=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES\r\n===================================================================================\r\nYOUR APPLICATION TERMINATED WITH THE EXIT STRING: Aborted (signal 6)\r\nThis typically refers to a problem with your application.\r\nPlease see the FAQ page for debugging suggestions\r\n\r\n\r\n\r\nTHANKS!I'd welcome any insight. Please let me know if I can provide any other information.\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-05-13T14:07:26+00:00",
    "closed_at": "2024-05-19T17:30:52+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7260/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7260"
  },
  {
    "number": 2166,
    "title": "LLAMA_METAL=1 and LLAMA_MPI=1 incompatible?",
    "body": "When following the instructions for MPI (https://github.com/ggerganov/llama.cpp/pull/2099) I get a build error.\r\n\r\n```\r\n> LLAMA_METAL=1 make CC=/opt/homebrew/bin/mpicc CXX=/opt/homebrew/bin/mpicxx LLAMA_MPI=1\r\nI llama.cpp build info:\r\nI UNAME_S:  Darwin\r\nI UNAME_P:  arm\r\nI UNAME_M:  arm64\r\nI CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_MPI -Wno-cast-qual\r\nI CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_MPI -Wno-cast-qual\r\nI LDFLAGS:   -framework Accelerate\r\nI CC:       Apple clang version 14.0.0 (clang-1400.0.29.202)\r\nI CXX:      Apple clang version 14.0.0 (clang-1400.0.29.202)\r\n\r\n/opt/homebrew/bin/mpicc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_MPI -Wno-cast-qual   -c ggml.c -o ggml.o\r\n/opt/homebrew/bin/mpicxx -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_MPI -Wno-cast-qual -c llama.cpp -o llama.o\r\n/opt/homebrew/bin/mpicxx -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS -DGGML_USE_MPI -Wno-cast-qual -c examples/common.cpp -o common.o\r\n/opt/homebrew/bin/mpicc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_MPI -Wno-cast-qual   -c -o k_quants.o k_quants.c\r\n/opt/homebrew/bin/mpicc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE -DGGML_USE_MPI -Wno-cast-qual -c ggml-mpi.c -o ggml-mpi.o\r\nCFLAGS   += -DGGML_USE_METAL -DGGML_METAL_NDEBUG\r\nmake: CFLAGS: No such file or directory\r\nmake: *** [ggml-mpi.o] Error 1\r\n```\r\n If I run make again it finishes and produces a functional main that is capable of mpi. But the resulting binary claims it wasn't built with GPU support so it ignores `--n-gpu-layers`. Example:\r\n\r\n```\r\n> ./main -m orca-mini-v2_7b.ggmlv3.q6_K.bin -n 128 --gpu-layers 1 -p \"Q. What is the capital of Germany? A. Berlin. Q. What is the capital of France? A.\"\r\nwarning: not compiled with GPU offload support, --n-gpu-layers option will be ignored\r\nwarning: see main README.md for information on enabling GPU BLAS support\r\nmain: build = 813 (5656d10)\r\nmain: seed  = 1689022667\r\nllama.cpp: loading model from orca-mini-v2_7b.ggmlv3.q6_K.bin\r\nllama_model_load_internal: format     = ggjt v3 (latest)\r\nllama_model_load_internal: n_vocab    = 32000\r\nllama_model_load_internal: n_ctx      = 512\r\n...\r\n```\r\n\r\nI tried to figure this out but I'm not that great with make (and gcc and etc). If I build with either LLAMA_METAL or LLAMA_MPI it works. It's when they're both together that it errors out.\r\n\r\nI'm on macOS 13 and the latest commit (5656d10599bd756dc0f17284e418e704200b43f3). I've got mpich installed with homebrew.\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-07-10T20:59:28+00:00",
    "closed_at": "2023-07-14T17:34:42+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2166/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2166"
  },
  {
    "number": 13993,
    "title": "Compile bug: Race condition during compilation, compilation works with -j 1 but not with -j 8",
    "body": "### Git commit\n\nea1431b0fa3a8108aac1e0a94a13ccc4a749963e\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCPU\n\n### Problem description & steps to reproduce\n\nI ran `podman build -f .devops/cpu.Dockerfile .` on my macbook and I got a compile error.\n\nI went into the intermediate image, and I tried instead of running `cmake --build build -j $(nproc)` running `cmake --build build -j 1` and it built successfully.\n\n`$(nproc)` was giving 8.\n\nThe error it gave seems to be getting eaten, here's an example:\n```\n[ 42%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\nc++: fatal error: Killed signal terminated program cc1plus\ncompilation terminated.\ngmake[2]: *** [src/CMakeFiles/llama.dir/build.make:174: src/CMakeFiles/llama.dir/llama-grammar.cpp.o] Error 1\ngmake[2]: *** Waiting for unfinished jobs....\ngmake[1]: *** [CMakeFiles/Makefile2:934: src/CMakeFiles/llama.dir/all] Error 2\ngmake: *** [Makefile:136: all] Error 2\n```\n\nThe extact point of failure seems to shift around, as can be expected for a race condition.\n\n### First Bad Commit\n\nunknown\n\n### Compile command\n\n```shell\nFails:\n\ncmake -S . -B build -DCMAKE_BUILD_TYPE=Release -DGGML_NATIVE=OFF -DLLAMA_BUILD_TESTS=OFF -DGGML_CPU_ARM_ARCH=armv8-a;\ncmake --build build -j 8\n\n\nPasses:\n\ncmake -S . -B build -DCMAKE_BUILD_TYPE=Release -DGGML_NATIVE=OFF -DLLAMA_BUILD_TESTS=OFF -DGGML_CPU_ARM_ARCH=armv8-a;\ncmake --build build -j 1\n```\n\n### Relevant log output\n\n```shell\n[ 32%] Built target llama-gguf-hash\n[ 33%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\n[ 33%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\n[ 34%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\n[ 35%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\n[ 35%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o\n[ 36%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o\n[ 36%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache-recurrent.cpp.o\n[ 37%] Building CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\n[ 38%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\n[ 38%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\n[ 39%] Building CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\n[ 40%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\n[ 40%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\n[ 41%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\n[ 41%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\n[ 42%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\nc++: fatal error: Killed signal terminated program cc1plus\ncompilation terminated.\ngmake[2]: *** [src/CMakeFiles/llama.dir/build.make:174: src/CMakeFiles/llama.dir/llama-grammar.cpp.o] Error 1\ngmake[2]: *** Waiting for unfinished jobs....\ngmake[1]: *** [CMakeFiles/Makefile2:934: src/CMakeFiles/llama.dir/all] Error 2\ngmake: *** [Makefile:136: all] Error 2\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-06-03T16:48:42+00:00",
    "closed_at": "2025-06-05T14:36:44+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13993/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13993"
  },
  {
    "number": 10380,
    "title": "Deepseek2 does not support K-shift Denial-of-Service vulnerability",
    "body": "Long prompts/responses crash llama-server because \"Deepseek2 does not support K-shift\". For long prompts/responses, llama-server should return an error message or truncate the response, but instead, `GGML_ABORT` is called, which crashes the server. I believe that this is a Denial-of-Service vulnerability. A client should **never** be able to trigger `GGML_ABORT`.\r\n\r\nThe relevant line in the code is here:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/blob/9b75f03cd2ec9cc482084049d87a0f08f9f01517/src/llama.cpp#L18032\r\n\r\nI have reported this security vulnerability almost three months ago [here (link only visible for maintainers),](https://github.com/ggerganov/llama.cpp/security/advisories/GHSA-jp78-gmv4-cc44) but have received no response and it is public knowledge now anyway, so I also opened this issue to increase visibility.\r\n\r\n### Discussed in https://github.com/ggerganov/llama.cpp/discussions/9092\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **99991** August 19, 2024</sup>\r\nIt is my understanding that llama.cpp shifts the key-value cache when generating more tokens than fit into the context window, which is not supported for DeepSeek Coder V2. To reproduce, start a server with [this model](https://huggingface.co/bartowski/DeepSeek-Coder-V2-Lite-Instruct-GGUF)\r\n\r\n```bash\r\n./llama-server -m DeepSeek-Coder-V2-Lite-Instruct-Q4_K_M.gguf -c 32 -ngl 999 --port 8080\r\n```\r\n\r\nand then request a prompt completion:\r\n\r\n```\r\ncurl -H \"Content-Type: application/json\" --request POST --data '{\"prompt\": \"Mergesort in Python:\", \"n_predict\": 32}' http://127.0.0.1:8080/completion\r\n```\r\n\r\nThis should trigger the error\r\n\r\n```\r\nsrc/llama.cpp:15646: Deepseek2 does not support K-shift\r\nAborted\r\n```\r\n\r\nwith llama.cpp [release b3600](https://github.com/ggerganov/llama.cpp/releases/tag/b3600).\r\n\r\nThe corresponding code in llama.cpp is here:\r\n\r\nhttps://github.com/ggerganov/llama.cpp/blob/cfac111e2b3953cdb6b0126e67a2487687646971/src/llama.cpp#L15643C31-L15648C1\r\n\r\nI believe that a saner approach would simply stop generating tokens instead of crashing the server. Is there some option that can be set to prevent clients from crashing the server?</div>",
    "labels": [],
    "state": "closed",
    "created_at": "2024-11-18T11:02:34+00:00",
    "closed_at": "2024-11-19T11:29:28+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10380/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10380"
  },
  {
    "number": 1963,
    "title": "train-from-scratch broken when compiled with cuda",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nWhen llama.cpp is built with cuda support, train from scratch should work the same as when it is built without cuda support.\r\n\r\n# Current Behavior\r\n\r\nI get a core dump when trying to run the test training script when compiled with cuda support. It works fine when compiled without cuda.\r\n\r\n# Environment and Context\r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n`$ lscpu`\r\n```\r\nArchitecture:            x86_64\r\n  CPU op-mode(s):        32-bit, 64-bit\r\n  Address sizes:         48 bits physical, 48 bits virtual\r\n  Byte Order:            Little Endian\r\nCPU(s):                  32\r\n  On-line CPU(s) list:   0-31\r\nVendor ID:               AuthenticAMD\r\n  Model name:            AMD Ryzen 9 7950X 16-Core Processor\r\n```\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`$ uname -a`\r\n`Linux pop-os 6.2.6-76060206-generic #202303130630~1685473338~22.04~995127e SMP PREEMPT_DYNAMIC Tue M x86_64 x86_64 x86_64 GNU/Linux`\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version\r\nPython 3.10.6\r\n\r\n$ make --version\r\nGNU Make 4.3\r\nBuilt for x86_64-pc-linux-gnu\r\n$ g++ --version\r\ng++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nPlease help provide information about the failure if this is a bug. If it is not a bug, please remove the rest of this template.\r\n\r\n# Steps to Reproduce\r\n\r\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\r\n\r\n1. Compile with cuda support\r\n2. Run the example in the train-text-from-scratch example README\r\n\r\n# Failure Logs\r\n\r\n```\r\n$ ./train-text-from-scratch --vocab-model ./models/ggml-vocab.bin --ctx 64 --embd 256 --head 8 --layer 16 -checkpoint-in  chk-shakespeare-256x16.bin --checkpoint-out chk-shakespeare-256x16.bin --model-out ggml-shakespeare-256x16-f32.bin         --train-data \"shakespeare.txt\" -t 6 -b 16 -n 32 --seed 1 --adam-iter 16 --print-details-interval 0 --predict 16 --use-flash\r\nmain: seed: 1\r\nllama.cpp: loading model from ./models/ggml-vocab.bin\r\nllama_model_load_internal: format     = ggjt v1 (pre #1405)\r\nllama_model_load_internal: n_vocab    = 32000\r\nllama_model_load_internal: n_ctx      = 512\r\nllama_model_load_internal: n_embd     = 4096\r\nllama_model_load_internal: n_mult     = 256\r\nllama_model_load_internal: n_head     = 32\r\nllama_model_load_internal: n_layer    = 32\r\nllama_model_load_internal: n_rot      = 128\r\nllama_model_load_internal: ftype      = 1 (mostly F16)\r\nllama_model_load_internal: n_ff       = 11008\r\nllama_model_load_internal: n_parts    = 1\r\nllama_model_load_internal: model size = 7B\r\nmain: tokenize training data\r\nmain: number of training tokens: 27584\r\nprint_params: n_vocab: 32000\r\nprint_params: n_ctx:   64\r\nprint_params: n_embd:  256\r\nprint_params: n_mult:  256\r\nprint_params: n_head:  8\r\nprint_params: n_ff:    768\r\nprint_params: n_layer: 16\r\nprint_params: n_rot:   32\r\nmain: number of unique tokens: 3070\r\nggml_init_cublas: found 2 CUDA devices:\r\n  Device 0: NVIDIA GeForce RTX 3090\r\n  Device 1: NVIDIA GeForce RTX 3090\r\nmain: init model\r\nload_checkpoint: Training iterations: 0.\r\nload_checkpoint: Training samples:    0.\r\nload_checkpoint: Training tokens:     0.\r\nmain: opt iter 0\r\nused_mem model+cache: 242364416 bytes\r\nmain: begin training\r\nSegmentation fault (core dumped)\r\n```\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-06-22T03:43:12+00:00",
    "closed_at": "2023-06-26T15:33:54+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1963/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1963"
  },
  {
    "number": 7484,
    "title": "tokenization: double EOS tokens",
    "body": "The llama.cpp tokenizer currently adds an EOS token unconditionally. However, adding an EOS token to the end of the system prompt is necessary to prevent generation before user input. This leads to two EOS tokens before the user prompt, potentially causing suboptimal performance.\r\n\r\nllama.cpp-b2972, MacOS 14.5\r\n\r\n./main -m Meta-Llama-3-8B-Instruct-Q8_0.gguf --temp 0 -i -e -p \"<|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant.<|eot_id|>\" -r \"<|eot_id|>\" --in-prefix \"<|start_header_id|>user<|end_header_id|>\\n\\n\" --in-suffix \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\r\n\r\nCurrent behavior:\r\n```\r\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\r\n\r\nYou are a helpful assistant.<|eot_id|>\r\n<|eot_id|>\r\n<|start_header_id|>user<|end_header_id|>\r\n```\r\nExpected behavior:\r\n```\r\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\r\n\r\nYou are a helpful assistant.<|eot_id|>\r\n<|start_header_id|>user<|end_header_id|>\r\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-05-23T04:47:13+00:00",
    "closed_at": "2024-05-24T15:39:25+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7484/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7484"
  },
  {
    "number": 4654,
    "title": "b1705 introduces build error on linux-aarch64",
    "body": "Starting with b1705, I get the following when building for linux-aarch64:\r\n```\r\nclang++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -mcpu=native   -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -c common/train.cpp -o train.o\r\nggml-quants.c:413:25: error: redefinition of 'vdotq_s32'\r\ninline static int32x4_t vdotq_s32(int32x4_t acc, int8x16_t a, int8x16_t b) {\r\n                        ^\r\n/opt/llvm.org/v16.0.6/lib/clang/16/include/arm_neon.h:33859:51: note: previous definition is here\r\n__ai __attribute__((target(\"dotprod\"))) int32x4_t vdotq_s32(int32x4_t __p0, int8x16_t __p1, int8x16_t __p2) {\r\n                                                  ^\r\n1 error generated.\r\n```\r\n\r\nit seems to be directly related to https://github.com/ggerganov/llama.cpp/pull/4630. the various #define tricks i'm used to don't seem to be able to resolve it. any ideas?\r\n\r\nfull build log: https://github.com/pkgxdev/pantry/actions/runs/7340844999/job/19987542307",
    "labels": [
      "help wanted",
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2023-12-27T18:28:34+00:00",
    "closed_at": "2023-12-31T09:44:23+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4654/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4654"
  },
  {
    "number": 8938,
    "title": "Bug: Decoding special tokens in T5",
    "body": "### What happened?\n\nI have a T5/lora model trained to output some text separated by the `<extra_id_0>` special token (the tokenizer properly works after following instructions in #8872) .\r\n\r\nWhen running the model using Huggingface's transformers/peft, it generates the expected output. However, when I use `llama-cli`, what happens instead is that the moment the first such token is reached, it's actually decoded into an `EOG` token instead of the extra token and generation is stopped.\r\n\r\nI might be simply doing something wrong in using the library.\n\n### Name and Version\n\nversion: 3549 (afd27f01)\r\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\r\n\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-08-08T16:32:39+00:00",
    "closed_at": "2024-08-09T16:53:10+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8938/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8938"
  },
  {
    "number": 1549,
    "title": "Unable to compile main cmake & windev, precompiled not working either",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [ X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [ X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [ X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\nUnable to build llama.cpp\r\nHave tried Cmake\r\nHave tried w64devkit \r\nHave tried the precompile versions avx1 and avx2 back to 3de84b2\r\n\r\nmain.exe compiles however it is printing out c code in the terminal\r\nwith respect to the issues with cmake found a similar issue that in another post\r\nhttps://github.com/antimatter15/alpaca.cpp/issues/106\r\nhowever the solution did not work\r\nPut #define restrict __restrict at the top of ggml.c and #undef restrict at the bottom.\r\nim making an assumption that its similar and cannot find something\r\n\r\n\r\n**Cmake compile issue**\r\n```\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(1016,26): warning C4244: '=': conversion from 'float' to 'int8_t', p\r\nossible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(1206,36): warning C4244: '=': conversion from 'float' to 'int8_t', p\r\nossible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(1207,36): warning C4244: '=': conversion from 'float' to 'int8_t', p\r\nossible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(2166,10): warning C4244: '=': conversion from 'ggml_float' to 'float\r\n', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(3214,20): warning C4244: '=': conversion from 'ggml_float' to 'float\r\n', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(3377,10): warning C4244: '=': conversion from 'ggml_float' to 'float\r\n', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(3727,39): warning C4244: 'return': conversion from 'const int64_t' t\r\no 'int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4277,66): warning C4244: 'function': conversion from 'int32_t' to 'c\r\nonst float', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4239,33): warning C4244: 'initializing': conversion from 'int64_t' t\r\no 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4301,66): warning C4244: 'function': conversion from 'float' to 'con\r\nst int8_t', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4308,68): warning C4244: 'function': conversion from 'float' to 'con\r\nst int16_t', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4315,68): warning C4244: 'function': conversion from 'float' to 'con\r\nst int32_t', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4322,72): warning C4244: 'function': conversion from 'float' to 'con\r\nst int32_t', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4291,33): warning C4244: 'initializing': conversion from 'int64_t' t\r\no 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4361,24): warning C4244: 'return': conversion from 'float' to 'int32\r\n_t', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4366,49): warning C4244: 'return': conversion from 'float' to 'int32\r\n_t', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4374,12): warning C4244: 'return': conversion from 'float' to 'int32\r\n_t', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4397,54): warning C4244: 'function': conversion from 'int32_t' to 'f\r\nloat', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4402,48): warning C4244: '=': conversion from 'int32_t' to 'float',\r\npossible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4426,51): warning C4244: 'return': conversion from 'int32_t' to 'flo\r\nat', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4452,49): warning C4244: '=': conversion from 'float' to 'int8_t', p\r\nossible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4457,50): warning C4244: '=': conversion from 'float' to 'int16_t',\r\npossible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4462,50): warning C4244: '=': conversion from 'float' to 'int32_t',\r\npossible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4655,32): warning C4267: '=': conversion from 'size_t' to 'int32_t',\r\n possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4656,32): warning C4267: '=': conversion from 'size_t' to 'int32_t',\r\n possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4657,32): warning C4267: '=': conversion from 'size_t' to 'int32_t',\r\n possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(4658,32): warning C4267: '=': conversion from 'size_t' to 'int32_t',\r\n possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(5468,34): warning C4267: '=': conversion from 'size_t' to 'int32_t',\r\n possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(5469,34): warning C4267: '=': conversion from 'size_t' to 'int32_t',\r\n possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(5470,34): warning C4267: '=': conversion from 'size_t' to 'int32_t',\r\n possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(5471,34): warning C4267: '=': conversion from 'size_t' to 'int32_t',\r\n possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(5924,22): warning C4244: '=': conversion from 'int64_t' to 'int', po\r\nssible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(5925,22): warning C4244: '=': conversion from 'int64_t' to 'int', po\r\nssible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(5926,22): warning C4244: '=': conversion from 'int64_t' to 'int', po\r\nssible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(5927,22): warning C4244: '=': conversion from 'int64_t' to 'int', po\r\nssible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(5929,22): warning C4267: '=': conversion from 'size_t' to 'int', pos\r\nsible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(5930,22): warning C4267: '=': conversion from 'size_t' to 'int', pos\r\nsible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(5931,22): warning C4267: '=': conversion from 'size_t' to 'int', pos\r\nsible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(5932,22): warning C4267: '=': conversion from 'size_t' to 'int', pos\r\nsible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(6591,20): warning C4244: 'initializing': conversion from 'int64_t' t\r\no 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(6723,68): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(6643,20): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7006,68): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(6932,20): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7263,30): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7251,30): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7252,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7253,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7275,30): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7276,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7277,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7341,30): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7342,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7343,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7411,30): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7412,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7413,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7513,43): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7515,26): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7517,40): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7493,27): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7494,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7495,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7615,27): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7602,26): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7603,41): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7604,41): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7672,26): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7673,41): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7674,41): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7733,26): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7734,41): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7735,41): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7815,43): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7817,27): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7819,40): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7805,26): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7806,41): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7807,41): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7901,28): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7936,26): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7937,43): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(7938,43): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8032,30): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8020,30): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8021,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8022,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8044,30): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8045,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8046,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8150,30): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8249,30): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8237,30): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8238,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8239,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8261,30): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8262,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8263,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8307,31): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8349,28): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8392,28): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8451,34): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8458,32): warning C4244: '=': conversion from 'ggml_float' to 'float\r\n', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8521,34): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8589,34): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8666,50): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8708,28): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8750,28): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8792,28): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8834,28): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8876,28): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8921,28): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(8982,28): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9045,28): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9146,36): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9132,33): warning C4244: 'initializing': conversion from 'ggml_float\r\n' to 'float', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9143,38): warning C4244: 'initializing': conversion from 'ggml_float\r\n' to 'float', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9220,36): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9209,33): warning C4244: 'initializing': conversion from 'ggml_float\r\n' to 'float', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9404,36): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9406,36): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9407,36): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9408,36): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9622,30): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9490,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9491,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9492,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9497,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9498,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9499,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9501,29): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9502,29): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9503,29): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9504,29): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9595,29): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9606,27): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9607,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9608,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9614,29): warning C4244: 'initializing': conversion from 'int64_t' t\r\no 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9841,30): warning C4244: 'function': conversion from 'const int64_t'\r\n to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9667,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9668,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9669,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9670,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9672,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9673,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9674,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9675,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9677,29): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9678,29): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9679,29): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9680,29): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9811,29): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9824,27): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9825,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9826,45): warning C4244: 'initializing': conversion from 'const int6\r\n4_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10006,121): warning C4244: 'function': conversion from 'const int64_\r\nt' to 'int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10055,23): warning C4244: 'function': conversion from 'const int64_t\r\n' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9881,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9882,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9883,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9884,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9886,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9887,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9888,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9889,30): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9891,29): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9892,29): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9893,29): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(9894,29): warning C4267: 'initializing': conversion from 'size_t' to\r\n 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10022,29): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10036,27): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10037,45): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10038,45): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10124,28): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10204,28): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10219,32): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10220,32): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10221,32): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10222,32): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10238,26): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10239,43): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10240,43): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10347,28): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10348,20): warning C4244: 'initializing': conversion from 'int64_t'\r\nto 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10376,28): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10377,20): warning C4244: 'initializing': conversion from 'int64_t'\r\nto 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10404,28): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10405,20): warning C4244: 'initializing': conversion from 'int64_t'\r\nto 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10487,28): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10488,20): warning C4244: 'initializing': conversion from 'int64_t'\r\nto 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10520,28): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10521,20): warning C4244: 'initializing': conversion from 'int64_t'\r\nto 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10591,30): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10592,30): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10593,30): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10594,30): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10595,28): warning C4244: 'initializing': conversion from 'int64_t'\r\nto 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10596,28): warning C4244: 'initializing': conversion from 'int64_t'\r\nto 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10597,28): warning C4244: 'initializing': conversion from 'int64_t'\r\nto 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10598,28): warning C4244: 'initializing': conversion from 'int64_t'\r\nto 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10605,30): warning C4267: 'initializing': conversion from 'size_t' t\r\no 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10607,30): warning C4267: 'initializing': conversion from 'size_t' t\r\no 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10608,30): warning C4267: 'initializing': conversion from 'size_t' t\r\no 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10609,28): warning C4267: 'initializing': conversion from 'size_t' t\r\no 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10610,28): warning C4267: 'initializing': conversion from 'size_t' t\r\no 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10611,28): warning C4267: 'initializing': conversion from 'size_t' t\r\no 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10612,28): warning C4267: 'initializing': conversion from 'size_t' t\r\no 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10687,28): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10688,28): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10801,36): warning C4244: 'function': conversion from 'ggml_float' t\r\no 'const float', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10758,28): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10882,38): warning C4244: 'function': conversion from 'int' to 'floa\r\nt', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10884,65): warning C4244: 'function': conversion from 'int' to 'floa\r\nt', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10849,29): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10850,29): warning C4244: 'initializing': conversion from 'const int\r\n64_t' to 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10857,29): warning C4267: 'initializing': conversion from 'size_t' t\r\no 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10858,29): warning C4267: 'initializing': conversion from 'size_t' t\r\no 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10859,29): warning C4267: 'initializing': conversion from 'size_t' t\r\no 'const int', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10946,38): warning C4244: 'function': conversion from 'int' to 'floa\r\nt', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10948,65): warning C4244: 'function': conversion from 'int' to 'floa\r\nt', possible loss of data [C:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\build\\ggml.vcxproj]\r\nC:\\Users\\Denys\\Downloads\\ggml\\llama.cpp\\ggml.c(10913,29): warning C4244: 'initializing': conversion from 'const int\r\n\r\n```\r\n  **w64dev issue**\r\n```\r\n  I llama.cpp build info:\r\nI UNAME_S:  Windows_NT\r\nI UNAME_P:  unknown\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -march=native -mtune=native\r\nI CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -march=native -mtune=native\r\nI LDFLAGS:\r\nI CC:       cc (GCC) 13.1.0\r\nI CXX:      g++ (GCC) 13.1.0\r\n\r\ncc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -march=native -mtune=native   -c ggml.c -o ggml.o\r\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -march=native -mtune=native -c llama.cpp -o llama.o\r\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -march=native -mtune=native -c examples/common.cpp -o common.o\r\nexamples/common.cpp:20: warning: \"NOMINMAX\" redefined\r\n   20 | #define NOMINMAX\r\n      |\r\nIn file included from C:/Users/Denys/Downloads/ggml/w64devkit/lib/gcc/x86_64-w64-mingw32/13.1.0/include/c++/x86_64-w64-mingw32/bits/c++config.h:679,\r\n                 from C:/Users/Denys/Downloads/ggml/w64devkit/lib/gcc/x86_64-w64-mingw32/13.1.0/include/c++/bits/requires_hosted.h:31,\r\n                 from C:/Users/Denys/Downloads/ggml/w64devkit/lib/gcc/x86_64-w64-mingw32/13.1.0/include/c++/string:38,\r\n                 from examples/common.h:7,\r\n                 from examples/common.cpp:1:\r\nC:/Users/Denys/Downloads/ggml/w64devkit/lib/gcc/x86_64-w64-mingw32/13.1.0/include/c++/x86_64-w64-mingw32/bits/os_defines.h:45: note: this is the location of the previous definition\r\n   45 | #define NOMINMAX 1\r\n      |\r\nexamples/common.cpp: In function 'int estimateWidth(char32_t)':\r\nexamples/common.cpp:654:28: warning: unused parameter 'codepoint' [-Wunused-parameter]\r\n  654 | int estimateWidth(char32_t codepoint) {\r\n      |                   ~~~~~~~~~^~~~~~~~~\r\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -march=native -mtune=native examples/main/main.cpp ggml.o llama.o common.o -o main\r\nexamples/main/main.cpp:26: warning: \"NOMINMAX\" redefined\r\n   26 | #define NOMINMAX\r\n      |\r\nIn file included from C:/Users/Denys/Downloads/ggml/w64devkit/lib/gcc/x86_64-w64-mingw32/13.1.0/include/c++/x86_64-w64-mingw32/bits/c++config.h:679,\r\n                 from C:/Users/Denys/Downloads/ggml/w64devkit/lib/gcc/x86_64-w64-mingw32/13.1.0/include/c++/bits/requires_hosted.h:31,\r\n                 from C:/Users/Denys/Downloads/ggml/w64devkit/lib/gcc/x86_64-w64-mingw32/13.1.0/include/c++/string:38,\r\n                 from ./examples/common.h:7,\r\n                 from examples/main/main.cpp:6:\r\nC:/Users/Denys/Downloads/ggml/w64devkit/lib/gcc/x86_64-w64-mingw32/13.1.0/include/c++/x86_64-w64-mingw32/bits/os_defines.h:45: note: this is the location of the previous definition\r\n   45 | #define NOMINMAX 1\r\n      |\r\n\r\n====  Run ./main -h for help.  ====\r\n\r\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -march=native -mtune=native examples/quantize/quantize.cpp ggml.o llama.o -o quantize\r\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -march=native -mtune=native examples/quantize-stats/quantize-stats.cpp ggml.o llama.o -o quantize-stats\r\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -march=native -mtune=native examples/perplexity/perplexity.cpp ggml.o llama.o common.o -o perplexity\r\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -march=native -mtune=native examples/embedding/embedding.cpp ggml.o llama.o common.o -o embedding\r\ng++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -march=native -mtune=native pocs/vdot/vdot.cpp ggml.o -o vdot\r\n```\r\n\r\n**Current output from main**\r\n``` \r\nsystem_info: n_threads = 30 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\r\nmain: interactive mode on.\r\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\r\ngenerate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - To return control without starting a new line, end your input with '/'.\r\n - If you want to submit another line, end your input with '\\'.\r\n\r\n \ufeffusing System;\r\nusing Microsoft.VisualStudio.TestTools.UnitTesting;\r\nusing Newtonsoft.Json;\r\nusing Newtonsoft.Json.Linq;\r\nusing Nop.Core.Domain.Stores;\r\nusing Nop.Plugin.Payments.Moneybookers.Tests.Data;\r\n```\r\n  \r\nRunning in a Windows 11 22h2 Vmware environment\r\n20 CPUs x Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz\r\n511.91 GB\r\n\r\nPython\r\nC:/Users/Denys/Downloads/ggml/llama.cpp # python --version\r\nPython 3.10.11\r\nC:/Users/Denys/Downloads/ggml/llama.cpp # pip list\r\nPackage            Version\r\n------------------ ------------\r\naccelerate         0.18.0\r\naiofiles           23.1.0\r\naiohttp            3.8.4\r\naiosignal          1.3.1\r\naltair             4.2.2\r\nanyio              3.6.2\r\nasync-timeout      4.0.2\r\nattrs              22.2.0\r\nbitsandbytes       0.37.2\r\ncertifi            2022.12.7\r\ncharset-normalizer 3.1.0\r\nclick              8.1.3\r\ncolorama           0.4.6\r\ncontourpy          1.0.7\r\ncycler             0.11.0\r\ndatasets           2.11.0\r\ndill               0.3.6\r\nentrypoints        0.4\r\nfastapi            0.95.0\r\nffmpy              0.3.0\r\nfilelock           3.10.7\r\nflexgen            0.1.7\r\nfonttools          4.39.3\r\nfrozenlist         1.3.3\r\nfsspec             2023.3.0\r\ngradio             3.24.0\r\ngradio_client      0.0.5\r\nh11                0.14.0\r\nhttpcore           0.16.3\r\nhttpx              0.23.3\r\nhuggingface-hub    0.13.3\r\nidna               3.4\r\nJinja2             3.1.2\r\njsonschema         4.17.3\r\nkiwisolver         1.4.4\r\nlinkify-it-py      2.0.0\r\nllamacpp           0.1.11\r\nMarkdown           3.4.3\r\nmarkdown-it-py     2.2.0\r\nMarkupSafe         2.1.2\r\nmatplotlib         3.7.1\r\nmdit-py-plugins    0.3.3\r\nmdurl              0.1.2\r\nmpmath             1.3.0\r\nmultidict          6.0.4\r\nmultiprocess       0.70.14\r\nnetworkx           3.0\r\nnumpy              1.24.2\r\norjson             3.8.9\r\npackaging          23.0\r\npandas             1.5.3\r\npeft               0.2.0\r\nPillow             9.4.0\r\npip                23.1.2\r\npsutil             5.9.4\r\nPuLP               2.7.0\r\npyarrow            11.0.0\r\npydantic           1.10.7\r\npydub              0.25.1\r\npyparsing          3.0.9\r\npyrsistent         0.19.3\r\npython-dateutil    2.8.2\r\npython-multipart   0.0.6\r\npytz               2023.3\r\nPyYAML             6.0\r\nregex              2023.3.23\r\nrequests           2.28.2\r\nresponses          0.18.0\r\nrfc3986            1.5.0\r\nrwkv               0.7.1\r\nsafetensors        0.3.0\r\nsemantic-version   2.10.0\r\nsentencepiece      0.1.97\r\nsix                1.16.0\r\nsniffio            1.3.0\r\nstarlette          0.26.1\r\nsympy              1.11.1\r\ntokenizers         0.13.2\r\ntoolz              0.12.0\r\ntorch              2.0.0\r\ntorchaudio         2.0.1+cu117\r\ntorchvision        0.15.1+cu117\r\ntqdm               4.65.0\r\ntransformers       4.28.0.dev0\r\ntyping_extensions  4.5.0\r\nuc-micro-py        1.0.1\r\nurllib3            1.26.15\r\nuvicorn            0.21.1\r\nwebsockets         10.4\r\nxxhash             3.2.0\r\nyarl               1.8.2\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-05-21T08:30:44+00:00",
    "closed_at": "2023-05-23T05:23:30+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1549/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1549"
  },
  {
    "number": 7030,
    "title": "Command-R GGUF conversion no longer working",
    "body": "As recently as a few days ago, Command-R (and presumably R+) could be converted with convert-hf-to-gguf.py.  I double checked and conversion completes successfully in b2751.  However, with the recent changes to accommodate Llama3, Command-R compatibility has been broken.  Trying to convert today with b2777 I get\r\n\r\n```\r\nraise NotImplementedError(\"BPE pre-tokenizer was not recognized - update get_vocab_base_pre()\")\r\nNotImplementedError: BPE pre-tokenizer was not recognized - update get_vocab_base_pre()\r\n```\r\n\r\nI know that L3 required a new tokenizer provided by meta to facilitate proper conversion.  Do we require something new from cohere, or is this something that can be fixed internally?",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-05-01T20:40:40+00:00",
    "closed_at": "2024-05-05T05:19:31+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7030/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7030"
  },
  {
    "number": 2898,
    "title": "common/log.h:290:61: error: expected primary-expression before ',' token",
    "body": "Running environment: Windows\r\n\r\nCompilation method: BLAS Build\r\n\r\nWhen I open w64devkit.exe and CD it to the llama.cpp directory, enter the command make LLAMA_ OPENBLAS=1 encountered the following error\r\n\r\n![image](https://github.com/ggerganov/llama.cpp/assets/14157458/d10d13aa-55f2-46e8-bd9c-31d2af9611e2)\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-08-30T09:02:10+00:00",
    "closed_at": "2023-09-01T09:07:07+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2898/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2898"
  },
  {
    "number": 1098,
    "title": "Try to use quantized `ggml_mul_mat` in attention layer",
    "body": "The following 2 matrix multiplication calls sill remain in FP16 precission:\r\n\r\n- https://github.com/ggerganov/llama.cpp/blob/d40fded93e1a533e969768e1e335c15c61c296ce/llama.cpp#L1135-L1137\r\n- https://github.com/ggerganov/llama.cpp/blob/d40fded93e1a533e969768e1e335c15c61c296ce/llama.cpp#L1158-L1160\r\n\r\nWas wondering, if we quantize those on-the-fly would there be any benefit.\r\nThe quantization can be done with an extra `ggml_cpy()` call, before the `ggml_mul_mat()` call.\r\n\r\nSee if this speeds up the computation and how it affects perplexity",
    "labels": [
      "good first issue",
      "performance"
    ],
    "state": "closed",
    "created_at": "2023-04-21T07:38:58+00:00",
    "closed_at": "2023-04-22T08:37:55+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1098/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1098"
  },
  {
    "number": 6425,
    "title": "convert-hf-to-gguf.py  XVERSE-13B-256K  error",
    "body": "Please include information about your system, the steps to reproduce the bug, and the version of llama.cpp that you are using. If possible, please provide a minimal code example that reproduces the bug.\r\n\r\nModel: \r\nhttps://huggingface.co/xverse/XVERSE-13B-256K\r\n\r\npython convert-hf-to-gguf.py /Volumes/FanData/models/XVERSE-13B-256K --outfile /Volumes/FanData/models/GGUF/xverse-13b-256k-f16.gguf --outtype f16\r\n\r\n`\r\npython convert-hf-to-gguf.py /Volumes/FanData/models/XVERSE-13B-256K --outfile /Volumes/FanData/models/GGUF/xverse-13b-256k-f16.gguf --outtype f16\r\nLoading model: XVERSE-13B-256K\r\ngguf: This GGUF file is for Little Endian only\r\nSet model parameters\r\nSet model tokenizer\r\ngguf: Setting special token type bos to 2\r\ngguf: Setting special token type eos to 3\r\ngguf: Setting special token type pad to 1\r\nExporting model to '/Volumes/FanData/models/GGUF/xverse-13b-256k-f16.gguf'\r\ngguf: loading model part 'pytorch_model-00001-of-00015.bin'\r\nTraceback (most recent call last):\r\n  File \"/Users/fanmac/AI/llama.cpp/convert-hf-to-gguf.py\", line 2296, in <module>\r\n    main()\r\n  File \"/Users/fanmac/AI/llama.cpp/convert-hf-to-gguf.py\", line 2290, in main\r\n    model_instance.write()\r\n  File \"/Users/fanmac/AI/llama.cpp/convert-hf-to-gguf.py\", line 175, in write\r\n    self.write_tensors()\r\n  File \"/Users/fanmac/AI/llama.cpp/convert-hf-to-gguf.py\", line 858, in write_tensors\r\n    model_kv = dict(self.get_tensors())\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/fanmac/AI/llama.cpp/convert-hf-to-gguf.py\", line 83, in get_tensors\r\n    ctx = contextlib.nullcontext(torch.load(str(self.dir_model / part_name), map_location=\"cpu\", mmap=True, weights_only=True))\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/fanmac/.miniconda3/envs/llamacpp/lib/python3.11/site-packages/torch/serialization.py\", line 993, in load\r\n    with _open_zipfile_reader(opened_file) as opened_zipfile:\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/fanmac/.miniconda3/envs/llamacpp/lib/python3.11/site-packages/torch/serialization.py\", line 447, in __init__\r\n    super().__init__(torch._C.PyTorchFileReader(name_or_buffer))\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory\r\n`\r\n\r\nIf the bug concerns the server, please try to reproduce it first using the [server test scenario framework](https://github.com/ggerganov/llama.cpp/tree/master/examples/server/tests).\r\n",
    "labels": [
      "invalid"
    ],
    "state": "closed",
    "created_at": "2024-04-01T13:07:59+00:00",
    "closed_at": "2024-04-03T23:53:56+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6425/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6425"
  },
  {
    "number": 9582,
    "title": "Bug: Vulkan not compile",
    "body": "### What happened?\n\n~/llama.cpp (master)> cmake -B build -DGGML_VULKAN=1\r\n                                  cmake --build build --config Release -j 16\r\n-- The C compiler identification is GNU 14.2.1\r\n-- The CXX compiler identification is GNU 14.2.1\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working C compiler: /usr/bin/cc - skipped\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Found Git: /usr/bin/git (found version \"2.46.1\")\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\r\n-- Found Threads: TRUE\r\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")\r\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\r\n-- Found OpenMP: TRUE (found version \"4.5\")\r\n-- OpenMP found\r\n-- Using llamafile\r\n-- Found Vulkan: /lib/libvulkan.so (found version \"1.3.295\") found components: glslc glslangValidator\r\n-- Vulkan found\r\n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\r\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\r\n-- x86 detected\r\n-- Configuring done (1.0s)\r\n-- Generating done (0.1s)\r\n-- Build files have been written to: /home/vecna/llama.cpp/build\r\n[  0%] Generating build details from Git\r\n[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\r\n[  1%] Building CXX object ggml/src/vulkan-shaders/CMakeFiles/vulkan-shaders-gen.dir/vulkan-shaders-gen.cpp.o\r\n[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\r\n[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\r\n-- Found Git: /usr/bin/git (found version \"2.46.1\")\r\n[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\r\n[  3%] Built target build_info\r\nIn function \u2018SHA1Update\u2019,\r\n    inlined from \u2018SHA1Final\u2019 at /home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c:265:5:\r\n/home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c:219:13: warning: \u2018SHA1Transform\u2019 reading 64 bytes from a region of size 0 [-Wstringop-overread]\r\n  219 |             SHA1Transform(context->state, &data[i]);\r\n      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c:219:13: note: referencing argument 2 of type \u2018const unsigned char[64]\u2019\r\n/home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c: In function \u2018SHA1Final\u2019:\r\n/home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c:54:6: note: in a call to function \u2018SHA1Transform\u2019\r\n   54 | void SHA1Transform(\r\n      |      ^~~~~~~~~~~~~\r\nIn function \u2018SHA1Update\u2019,\r\n    inlined from \u2018SHA1Final\u2019 at /home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c:269:9:\r\n/home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c:219:13: warning: \u2018SHA1Transform\u2019 reading 64 bytes from a region of size 0 [-Wstringop-overread]\r\n  219 |             SHA1Transform(context->state, &data[i]);\r\n      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c:219:13: note: referencing argument 2 of type \u2018const unsigned char[64]\u2019\r\n/home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c: In function \u2018SHA1Final\u2019:\r\n/home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c:54:6: note: in a call to function \u2018SHA1Transform\u2019\r\n   54 | void SHA1Transform(\r\n      |      ^~~~~~~~~~~~~\r\n[  3%] Built target sha1\r\n[  3%] Built target sha256\r\n[  3%] Built target xxhash\r\n[  4%] Linking CXX executable ../../../bin/vulkan-shaders-gen\r\n[  4%] Built target vulkan-shaders-gen\r\n[  5%] Generate vulkan shaders\r\nggml_vulkan: Generating and compiling shaders to SPIR-V\r\nError executing command for Error executing command for Error executing command for Error executing command for Error executing command for matmul_id_q5_k_f32_aligned_fp32Error executing command for Error executing command for matmul_f16_alignedError executing command for mul_mat_vec_id_q3_k_f32: Failed to create pipes\r\nError executing command for dequant_q6_k: Failed to create pipes\r\ndequant_iq4_nl: Failed to create pipes\r\ndequant_q2_k: Failed to create pipes\r\nError executing command for Error executing command for matmul_id_q4_0_f32_fp32: Failed to create pipes\r\nmatmul_q4_1_f32_alignedError executing command for mul_mat_vec_q3_k_f32_f32: Error executing command for mul_mat_vec_id_iq4_nl_f32: Failed to create pipes\r\nmul_mat_vec_iq4_nl_f16_f32: Error executing command for Failed to create pipes\r\n: Error executing command for Error executing command for mul_mat_vec_iq4_nl_f32_f32: Failed to create pipes\r\n: Failed to create pipes\r\nmatmul_f32_f32mul_mat_vec_id_q6_k_f32: Failed to create pipes\r\n: Failed to create pipesget_rows_iq4_nl: Error executing command for Failed to create pipes\r\nmul_mat_vec_q3_k_f16_f32: Failed to create pipes\r\n\r\nFailed to create pipes\r\n: Failed to create pipes\r\nmul_mat_vec_q6_k_f16_f32: Failed to create pipes\r\nFailed to create pipes\r\n[  6%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.o\r\n[  9%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-aarch64.c.o\r\n[  9%] Building CXX object ggml/src/CMakeFiles/ggml.dir/llamafile/sgemm.cpp.o\r\n[  9%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-backend.c.o\r\n[  7%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-vulkan-shaders.cpp.o\r\n[  9%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml.c.o\r\n[  9%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-vulkan.cpp.o\r\n[  9%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-alloc.c.o\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp: In function \u2018void ggml_vk_load_shaders(vk_device&)\u2019:\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1197:89: error: \u2018matmul_f32_f32_len\u2019 was not declared in this scope; did you mean \u2018matmul_f32_f16_len\u2019?\r\n 1197 |         ggml_vk_create_pipeline(device, device->pipeline_matmul_f32->l, \"matmul_f32_l\", matmul_f32_f32_len, matmul_f32_f32_data, \"main\", 3, sizeof(vk_mat_mat_push_constants), l_wg_denoms, warptile_l, 1);\r\n      |                                                                                         ^~~~~~~~~~~~~~~~~~\r\n      |                                                                                         matmul_f32_f16_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1197:109: error: \u2018matmul_f32_f32_data\u2019 was not declared in this scope; did you mean \u2018matmul_f32_f16_data\u2019?\r\n 1197 |         ggml_vk_create_pipeline(device, device->pipeline_matmul_f32->l, \"matmul_f32_l\", matmul_f32_f32_len, matmul_f32_f32_data, \"main\", 3, sizeof(vk_mat_mat_push_constants), l_wg_denoms, warptile_l, 1);\r\n      |                                                                                                             ^~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                             matmul_f32_f16_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1214:99: error: \u2018matmul_f16_aligned_len\u2019 was not declared in this scope; did you mean \u2018matmul_id_f16_aligned_len\u2019?\r\n 1214 |         ggml_vk_create_pipeline(device, device->pipeline_matmul_f16->a_l, \"matmul_f16_aligned_l\", matmul_f16_aligned_len, matmul_f16_aligned_data, \"main\", 3, sizeof(vk_mat_mat_push_constants), l_wg_denoms, warptile_l, l_align);\r\n      |                                                                                                   ^~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                   matmul_id_f16_aligned_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1214:123: error: \u2018matmul_f16_aligned_data\u2019 was not declared in this scope; did you mean \u2018matmul_id_f16_aligned_data\u2019?\r\n 1214 |         ggml_vk_create_pipeline(device, device->pipeline_matmul_f16->a_l, \"matmul_f16_aligned_l\", matmul_f16_aligned_len, matmul_f16_aligned_data, \"main\", 3, sizeof(vk_mat_mat_push_constants), l_wg_denoms, warptile_l, l_align);\r\n      |                                                                                                                           ^~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                           matmul_id_f16_aligned_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1235:129: error: \u2018matmul_q4_1_f32_aligned_len\u2019 was not declared in this scope; did you mean \u2018matmul_q5_1_f32_aligned_len\u2019?\r\n 1235 |         ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_mat[GGML_TYPE_Q4_1]->a_l, \"matmul_q4_1_f32_aligned_l\", matmul_q4_1_f32_aligned_len, matmul_q4_1_f32_aligned_data, \"main\", 3, sizeof(vk_mat_mat_push_constants), l_wg_denoms, warptile_mmq_l, l_align);\r\n      |                                                                                                                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                 matmul_q5_1_f32_aligned_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1235:158: error: \u2018matmul_q4_1_f32_aligned_data\u2019 was not declared in this scope; did you mean \u2018matmul_q5_1_f32_aligned_data\u2019?\r\n 1235 |         ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_mat[GGML_TYPE_Q4_1]->a_l, \"matmul_q4_1_f32_aligned_l\", matmul_q4_1_f32_aligned_len, matmul_q4_1_f32_aligned_data, \"main\", 3, sizeof(vk_mat_mat_push_constants), l_wg_denoms, warptile_mmq_l, l_align);\r\n      |                                                                                                                                                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                              matmul_q5_1_f32_aligned_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1526:125: error: \u2018matmul_id_q4_0_f32_fp32_len\u2019 was not declared in this scope; did you mean \u2018matmul_id_q8_0_f32_fp32_len\u2019?\r\n 1526 |         ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_mat_id[GGML_TYPE_Q4_0]->l, \"matmul_id_q4_0_f32_l\", matmul_id_q4_0_f32_fp32_len, matmul_id_q4_0_f32_fp32_data, \"main\", 4, sizeof(vk_mat_mat_id_push_constants), l_wg_denoms, warptile_mmq_l, l_align);\r\n      |                                                                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                             matmul_id_q8_0_f32_fp32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1526:154: error: \u2018matmul_id_q4_0_f32_fp32_data\u2019 was not declared in this scope; did you mean \u2018matmul_id_q8_0_f32_fp32_data\u2019?\r\n 1526 |         ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_mat_id[GGML_TYPE_Q4_0]->l, \"matmul_id_q4_0_f32_l\", matmul_id_q4_0_f32_fp32_len, matmul_id_q4_0_f32_fp32_data, \"main\", 4, sizeof(vk_mat_mat_id_push_constants), l_wg_denoms, warptile_mmq_l, l_align);\r\n      |                                                                                                                                                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                          matmul_id_q8_0_f32_fp32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1585:135: error: \u2018matmul_id_q5_k_f32_aligned_fp32_len\u2019 was not declared in this scope; did you mean \u2018matmul_id_q6_k_f32_aligned_fp32_len\u2019?\r\n 1585 |         ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_mat_id[GGML_TYPE_Q5_K]->a_l, \"matmul_id_q5_k_f32_aligned_l\", matmul_id_q5_k_f32_aligned_fp32_len, matmul_id_q5_k_f32_aligned_fp32_data, \"main\", 4, sizeof(vk_mat_mat_id_push_constants), l_wg_denoms, warptile_mmq_l, l_align);\r\n      |                                                                                                                                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                       matmul_id_q6_k_f32_aligned_fp32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1585:172: error: \u2018matmul_id_q5_k_f32_aligned_fp32_data\u2019 was not declared in this scope; did you mean \u2018matmul_id_q6_k_f32_aligned_fp32_data\u2019?\r\n 1585 |         ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_mat_id[GGML_TYPE_Q5_K]->a_l, \"matmul_id_q5_k_f32_aligned_l\", matmul_id_q5_k_f32_aligned_fp32_len, matmul_id_q5_k_f32_aligned_fp32_data, \"main\", 4, sizeof(vk_mat_mat_id_push_constants), l_wg_denoms, warptile_mmq_l, l_align);\r\n      |                                                                                                                                                                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                                            matmul_id_q6_k_f32_aligned_fp32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1613:127: error: \u2018mul_mat_vec_q3_k_f32_f32_len\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q6_k_f32_f32_len\u2019?\r\n 1613 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[GGML_TYPE_Q3_K], \"mul_mat_vec_q3_k_f32_f32\", mul_mat_vec_q3_k_f32_f32_len, mul_mat_vec_q3_k_f32_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                               ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                               mul_mat_vec_q6_k_f32_f32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1613:157: error: \u2018mul_mat_vec_q3_k_f32_f32_data\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q6_k_f32_f32_data\u2019?\r\n 1613 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[GGML_TYPE_Q3_K], \"mul_mat_vec_q3_k_f32_f32\", mul_mat_vec_q3_k_f32_f32_len, mul_mat_vec_q3_k_f32_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                             mul_mat_vec_q6_k_f32_f32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1617:131: error: \u2018mul_mat_vec_iq4_nl_f32_f32_len\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q4_k_f32_f32_len\u2019?\r\n 1617 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[GGML_TYPE_IQ4_NL], \"mul_mat_vec_iq4_nl_f32_f32\", mul_mat_vec_iq4_nl_f32_f32_len, mul_mat_vec_iq4_nl_f32_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                   mul_mat_vec_q4_k_f32_f32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1617:163: error: \u2018mul_mat_vec_iq4_nl_f32_f32_data\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q4_k_f32_f32_data\u2019?\r\n 1617 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[GGML_TYPE_IQ4_NL], \"mul_mat_vec_iq4_nl_f32_f32\", mul_mat_vec_iq4_nl_f32_f32_len, mul_mat_vec_iq4_nl_f32_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                                                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                                   mul_mat_vec_q4_k_f32_f32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1627:127: error: \u2018mul_mat_vec_q3_k_f16_f32_len\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q5_k_f16_f32_len\u2019?\r\n 1627 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[GGML_TYPE_Q3_K], \"mul_mat_vec_q3_k_f16_f32\", mul_mat_vec_q3_k_f16_f32_len, mul_mat_vec_q3_k_f16_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                               ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                               mul_mat_vec_q5_k_f16_f32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1627:157: error: \u2018mul_mat_vec_q3_k_f16_f32_data\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q5_k_f16_f32_data\u2019?\r\n 1627 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[GGML_TYPE_Q3_K], \"mul_mat_vec_q3_k_f16_f32\", mul_mat_vec_q3_k_f16_f32_len, mul_mat_vec_q3_k_f16_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                             mul_mat_vec_q5_k_f16_f32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1630:127: error: \u2018mul_mat_vec_q6_k_f16_f32_len\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q5_k_f16_f32_len\u2019?\r\n 1630 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[GGML_TYPE_Q6_K], \"mul_mat_vec_q6_k_f16_f32\", mul_mat_vec_q6_k_f16_f32_len, mul_mat_vec_q6_k_f16_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                               ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                               mul_mat_vec_q5_k_f16_f32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1630:157: error: \u2018mul_mat_vec_q6_k_f16_f32_data\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q5_k_f16_f32_data\u2019?\r\n 1630 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[GGML_TYPE_Q6_K], \"mul_mat_vec_q6_k_f16_f32\", mul_mat_vec_q6_k_f16_f32_len, mul_mat_vec_q6_k_f16_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                             mul_mat_vec_q5_k_f16_f32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1631:131: error: \u2018mul_mat_vec_iq4_nl_f16_f32_len\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q4_k_f16_f32_len\u2019?\r\n 1631 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[GGML_TYPE_IQ4_NL], \"mul_mat_vec_iq4_nl_f16_f32\", mul_mat_vec_iq4_nl_f16_f32_len, mul_mat_vec_iq4_nl_f16_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                   mul_mat_vec_q4_k_f16_f32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1631:163: error: \u2018mul_mat_vec_iq4_nl_f16_f32_data\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q4_k_f16_f32_data\u2019?\r\n 1631 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[GGML_TYPE_IQ4_NL], \"mul_mat_vec_iq4_nl_f16_f32\", mul_mat_vec_iq4_nl_f16_f32_len, mul_mat_vec_iq4_nl_f16_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                                                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                                   mul_mat_vec_q4_k_f16_f32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1641:125: error: \u2018mul_mat_vec_id_q3_k_f32_len\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_id_q5_k_f32_len\u2019?\r\n 1641 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q3_K], \"mul_mat_vec_id_q3_k_f32\", mul_mat_vec_id_q3_k_f32_len, mul_mat_vec_id_q3_k_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                             mul_mat_vec_id_q5_k_f32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1641:154: error: \u2018mul_mat_vec_id_q3_k_f32_data\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_id_q5_k_f32_data\u2019?\r\n 1641 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q3_K], \"mul_mat_vec_id_q3_k_f32\", mul_mat_vec_id_q3_k_f32_len, mul_mat_vec_id_q3_k_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                          mul_mat_vec_id_q5_k_f32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1644:125: error: \u2018mul_mat_vec_id_q6_k_f32_len\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_id_q5_k_f32_len\u2019?\r\n 1644 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q6_K], \"mul_mat_vec_id_q6_k_f32\", mul_mat_vec_id_q6_k_f32_len, mul_mat_vec_id_q6_k_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                             mul_mat_vec_id_q5_k_f32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1644:154: error: \u2018mul_mat_vec_id_q6_k_f32_data\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_id_q5_k_f32_data\u2019?\r\n 1644 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q6_K], \"mul_mat_vec_id_q6_k_f32\", mul_mat_vec_id_q6_k_f32_len, mul_mat_vec_id_q6_k_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                          mul_mat_vec_id_q5_k_f32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1645:129: error: \u2018mul_mat_vec_id_iq4_nl_f32_len\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_id_q4_k_f32_len\u2019?\r\n 1645 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ4_NL], \"mul_mat_vec_id_iq4_nl_f32\", mul_mat_vec_id_iq4_nl_f32_len, mul_mat_vec_id_iq4_nl_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                 mul_mat_vec_id_q4_k_f32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1645:160: error: \u2018mul_mat_vec_id_iq4_nl_f32_data\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_id_q4_k_f32_data\u2019?\r\n 1645 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ4_NL], \"mul_mat_vec_id_iq4_nl_f32\", mul_mat_vec_id_iq4_nl_f32_len, mul_mat_vec_id_iq4_nl_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                                                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                                mul_mat_vec_id_q4_k_f32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1654:95: error: \u2018dequant_q2_k_len\u2019 was not declared in this scope; did you mean \u2018dequant_q5_k_len\u2019?\r\n 1654 |     ggml_vk_create_pipeline(device, device->pipeline_dequant[GGML_TYPE_Q2_K], \"dequant_q2_k\", dequant_q2_k_len, dequant_q2_k_data, \"main\", 2, 5 * sizeof(uint32_t), {256 * 64, 1, 1}, {}, 1);\r\n      |                                                                                               ^~~~~~~~~~~~~~~~\r\n      |                                                                                               dequant_q5_k_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1654:113: error: \u2018dequant_q2_k_data\u2019 was not declared in this scope; did you mean \u2018dequant_q5_k_data\u2019?\r\n 1654 |     ggml_vk_create_pipeline(device, device->pipeline_dequant[GGML_TYPE_Q2_K], \"dequant_q2_k\", dequant_q2_k_len, dequant_q2_k_data, \"main\", 2, 5 * sizeof(uint32_t), {256 * 64, 1, 1}, {}, 1);\r\n      |                                                                                                                 ^~~~~~~~~~~~~~~~~\r\n      |                                                                                                                 dequant_q5_k_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1658:95: error: \u2018dequant_q6_k_len\u2019 was not declared in this scope; did you mean \u2018dequant_q5_k_len\u2019?\r\n 1658 |     ggml_vk_create_pipeline(device, device->pipeline_dequant[GGML_TYPE_Q6_K], \"dequant_q6_k\", dequant_q6_k_len, dequant_q6_k_data, \"main\", 2, 5 * sizeof(uint32_t), {256 * 64, 1, 1}, {}, 1);\r\n      |                                                                                               ^~~~~~~~~~~~~~~~\r\n      |                                                                                               dequant_q5_k_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1658:113: error: \u2018dequant_q6_k_data\u2019 was not declared in this scope; did you mean \u2018dequant_q5_k_data\u2019?\r\n 1658 |     ggml_vk_create_pipeline(device, device->pipeline_dequant[GGML_TYPE_Q6_K], \"dequant_q6_k\", dequant_q6_k_len, dequant_q6_k_data, \"main\", 2, 5 * sizeof(uint32_t), {256 * 64, 1, 1}, {}, 1);\r\n      |                                                                                                                 ^~~~~~~~~~~~~~~~~\r\n      |                                                                                                                 dequant_q5_k_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1659:99: error: \u2018dequant_iq4_nl_len\u2019 was not declared in this scope; did you mean \u2018dequant_q4_k_len\u2019?\r\n 1659 |     ggml_vk_create_pipeline(device, device->pipeline_dequant[GGML_TYPE_IQ4_NL], \"dequant_iq4_nl\", dequant_iq4_nl_len, dequant_iq4_nl_data, \"main\", 2, 5 * sizeof(uint32_t), {256 * 16, 1, 1}, {}, 1);\r\n      |                                                                                                   ^~~~~~~~~~~~~~~~~~\r\n      |                                                                                                   dequant_q4_k_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1659:119: error: \u2018dequant_iq4_nl_data\u2019 was not declared in this scope; did you mean \u2018dequant_q4_k_data\u2019?\r\n 1659 |     ggml_vk_create_pipeline(device, device->pipeline_dequant[GGML_TYPE_IQ4_NL], \"dequant_iq4_nl\", dequant_iq4_nl_len, dequant_iq4_nl_data, \"main\", 2, 5 * sizeof(uint32_t), {256 * 16, 1, 1}, {}, 1);\r\n      |                                                                                                                       ^~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                       dequant_q4_k_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1669:101: error: \u2018get_rows_iq4_nl_len\u2019 was not declared in this scope; did you mean \u2018get_rows_q4_1_len\u2019?\r\n 1669 |     ggml_vk_create_pipeline(device, device->pipeline_get_rows[GGML_TYPE_IQ4_NL], \"get_rows_iq4_nl\", get_rows_iq4_nl_len, get_rows_iq4_nl_data, \"main\", 3, sizeof(vk_op_binary_push_constants), {1024, 1, 1}, {}, 1);\r\n      |                                                                                                     ^~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                     get_rows_q4_1_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1669:122: error: \u2018get_rows_iq4_nl_data\u2019 was not declared in this scope; did you mean \u2018get_rows_q4_1_data\u2019?\r\n 1669 |     ggml_vk_create_pipeline(device, device->pipeline_get_rows[GGML_TYPE_IQ4_NL], \"get_rows_iq4_nl\", get_rows_iq4_nl_len, get_rows_iq4_nl_data, \"main\", 3, sizeof(vk_op_binary_push_constants), {1024, 1, 1}, {}, 1);\r\n      |                                                                                                                          ^~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                          get_rows_q4_1_data\r\nmake[2]: *** [ggml/src/CMakeFiles/ggml.dir/build.make:201: ggml/src/CMakeFiles/ggml.dir/ggml-vulkan.cpp.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\nmake[1]: *** [CMakeFiles/Makefile2:1626: ggml/src/CMakeFiles/ggml.dir/all] Error 2\r\nmake: *** [Makefile:146: all] Error 2\n\n### Name and Version\n\nit just don't compile and i don't found a special issue option for it\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\n~/llama.cpp (master)> cmake -B build -DGGML_VULKAN=1\r\n                                  cmake --build build --config Release -j 16\r\n-- The C compiler identification is GNU 14.2.1\r\n-- The CXX compiler identification is GNU 14.2.1\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working C compiler: /usr/bin/cc - skipped\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Found Git: /usr/bin/git (found version \"2.46.1\")\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\r\n-- Found Threads: TRUE\r\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")\r\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\r\n-- Found OpenMP: TRUE (found version \"4.5\")\r\n-- OpenMP found\r\n-- Using llamafile\r\n-- Found Vulkan: /lib/libvulkan.so (found version \"1.3.295\") found components: glslc glslangValidator\r\n-- Vulkan found\r\n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\r\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\r\n-- x86 detected\r\n-- Configuring done (1.0s)\r\n-- Generating done (0.1s)\r\n-- Build files have been written to: /home/vecna/llama.cpp/build\r\n[  0%] Generating build details from Git\r\n[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\r\n[  1%] Building CXX object ggml/src/vulkan-shaders/CMakeFiles/vulkan-shaders-gen.dir/vulkan-shaders-gen.cpp.o\r\n[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\r\n[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\r\n-- Found Git: /usr/bin/git (found version \"2.46.1\")\r\n[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\r\n[  3%] Built target build_info\r\nIn function \u2018SHA1Update\u2019,\r\n    inlined from \u2018SHA1Final\u2019 at /home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c:265:5:\r\n/home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c:219:13: warning: \u2018SHA1Transform\u2019 reading 64 bytes from a region of size 0 [-Wstringop-overread]\r\n  219 |             SHA1Transform(context->state, &data[i]);\r\n      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c:219:13: note: referencing argument 2 of type \u2018const unsigned char[64]\u2019\r\n/home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c: In function \u2018SHA1Final\u2019:\r\n/home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c:54:6: note: in a call to function \u2018SHA1Transform\u2019\r\n   54 | void SHA1Transform(\r\n      |      ^~~~~~~~~~~~~\r\nIn function \u2018SHA1Update\u2019,\r\n    inlined from \u2018SHA1Final\u2019 at /home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c:269:9:\r\n/home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c:219:13: warning: \u2018SHA1Transform\u2019 reading 64 bytes from a region of size 0 [-Wstringop-overread]\r\n  219 |             SHA1Transform(context->state, &data[i]);\r\n      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c:219:13: note: referencing argument 2 of type \u2018const unsigned char[64]\u2019\r\n/home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c: In function \u2018SHA1Final\u2019:\r\n/home/vecna/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c:54:6: note: in a call to function \u2018SHA1Transform\u2019\r\n   54 | void SHA1Transform(\r\n      |      ^~~~~~~~~~~~~\r\n[  3%] Built target sha1\r\n[  3%] Built target sha256\r\n[  3%] Built target xxhash\r\n[  4%] Linking CXX executable ../../../bin/vulkan-shaders-gen\r\n[  4%] Built target vulkan-shaders-gen\r\n[  5%] Generate vulkan shaders\r\nggml_vulkan: Generating and compiling shaders to SPIR-V\r\nError executing command for Error executing command for Error executing command for Error executing command for Error executing command for matmul_id_q5_k_f32_aligned_fp32Error executing command for Error executing command for matmul_f16_alignedError executing command for mul_mat_vec_id_q3_k_f32: Failed to create pipes\r\nError executing command for dequant_q6_k: Failed to create pipes\r\ndequant_iq4_nl: Failed to create pipes\r\ndequant_q2_k: Failed to create pipes\r\nError executing command for Error executing command for matmul_id_q4_0_f32_fp32: Failed to create pipes\r\nmatmul_q4_1_f32_alignedError executing command for mul_mat_vec_q3_k_f32_f32: Error executing command for mul_mat_vec_id_iq4_nl_f32: Failed to create pipes\r\nmul_mat_vec_iq4_nl_f16_f32: Error executing command for Failed to create pipes\r\n: Error executing command for Error executing command for mul_mat_vec_iq4_nl_f32_f32: Failed to create pipes\r\n: Failed to create pipes\r\nmatmul_f32_f32mul_mat_vec_id_q6_k_f32: Failed to create pipes\r\n: Failed to create pipesget_rows_iq4_nl: Error executing command for Failed to create pipes\r\nmul_mat_vec_q3_k_f16_f32: Failed to create pipes\r\n\r\nFailed to create pipes\r\n: Failed to create pipes\r\nmul_mat_vec_q6_k_f16_f32: Failed to create pipes\r\nFailed to create pipes\r\n[  6%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.o\r\n[  9%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-aarch64.c.o\r\n[  9%] Building CXX object ggml/src/CMakeFiles/ggml.dir/llamafile/sgemm.cpp.o\r\n[  9%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-backend.c.o\r\n[  7%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-vulkan-shaders.cpp.o\r\n[  9%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml.c.o\r\n[  9%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-vulkan.cpp.o\r\n[  9%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-alloc.c.o\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp: In function \u2018void ggml_vk_load_shaders(vk_device&)\u2019:\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1197:89: error: \u2018matmul_f32_f32_len\u2019 was not declared in this scope; did you mean \u2018matmul_f32_f16_len\u2019?\r\n 1197 |         ggml_vk_create_pipeline(device, device->pipeline_matmul_f32->l, \"matmul_f32_l\", matmul_f32_f32_len, matmul_f32_f32_data, \"main\", 3, sizeof(vk_mat_mat_push_constants), l_wg_denoms, warptile_l, 1);\r\n      |                                                                                         ^~~~~~~~~~~~~~~~~~\r\n      |                                                                                         matmul_f32_f16_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1197:109: error: \u2018matmul_f32_f32_data\u2019 was not declared in this scope; did you mean \u2018matmul_f32_f16_data\u2019?\r\n 1197 |         ggml_vk_create_pipeline(device, device->pipeline_matmul_f32->l, \"matmul_f32_l\", matmul_f32_f32_len, matmul_f32_f32_data, \"main\", 3, sizeof(vk_mat_mat_push_constants), l_wg_denoms, warptile_l, 1);\r\n      |                                                                                                             ^~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                             matmul_f32_f16_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1214:99: error: \u2018matmul_f16_aligned_len\u2019 was not declared in this scope; did you mean \u2018matmul_id_f16_aligned_len\u2019?\r\n 1214 |         ggml_vk_create_pipeline(device, device->pipeline_matmul_f16->a_l, \"matmul_f16_aligned_l\", matmul_f16_aligned_len, matmul_f16_aligned_data, \"main\", 3, sizeof(vk_mat_mat_push_constants), l_wg_denoms, warptile_l, l_align);\r\n      |                                                                                                   ^~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                   matmul_id_f16_aligned_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1214:123: error: \u2018matmul_f16_aligned_data\u2019 was not declared in this scope; did you mean \u2018matmul_id_f16_aligned_data\u2019?\r\n 1214 |         ggml_vk_create_pipeline(device, device->pipeline_matmul_f16->a_l, \"matmul_f16_aligned_l\", matmul_f16_aligned_len, matmul_f16_aligned_data, \"main\", 3, sizeof(vk_mat_mat_push_constants), l_wg_denoms, warptile_l, l_align);\r\n      |                                                                                                                           ^~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                           matmul_id_f16_aligned_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1235:129: error: \u2018matmul_q4_1_f32_aligned_len\u2019 was not declared in this scope; did you mean \u2018matmul_q5_1_f32_aligned_len\u2019?\r\n 1235 |         ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_mat[GGML_TYPE_Q4_1]->a_l, \"matmul_q4_1_f32_aligned_l\", matmul_q4_1_f32_aligned_len, matmul_q4_1_f32_aligned_data, \"main\", 3, sizeof(vk_mat_mat_push_constants), l_wg_denoms, warptile_mmq_l, l_align);\r\n      |                                                                                                                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                 matmul_q5_1_f32_aligned_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1235:158: error: \u2018matmul_q4_1_f32_aligned_data\u2019 was not declared in this scope; did you mean \u2018matmul_q5_1_f32_aligned_data\u2019?\r\n 1235 |         ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_mat[GGML_TYPE_Q4_1]->a_l, \"matmul_q4_1_f32_aligned_l\", matmul_q4_1_f32_aligned_len, matmul_q4_1_f32_aligned_data, \"main\", 3, sizeof(vk_mat_mat_push_constants), l_wg_denoms, warptile_mmq_l, l_align);\r\n      |                                                                                                                                                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                              matmul_q5_1_f32_aligned_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1526:125: error: \u2018matmul_id_q4_0_f32_fp32_len\u2019 was not declared in this scope; did you mean \u2018matmul_id_q8_0_f32_fp32_len\u2019?\r\n 1526 |         ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_mat_id[GGML_TYPE_Q4_0]->l, \"matmul_id_q4_0_f32_l\", matmul_id_q4_0_f32_fp32_len, matmul_id_q4_0_f32_fp32_data, \"main\", 4, sizeof(vk_mat_mat_id_push_constants), l_wg_denoms, warptile_mmq_l, l_align);\r\n      |                                                                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                             matmul_id_q8_0_f32_fp32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1526:154: error: \u2018matmul_id_q4_0_f32_fp32_data\u2019 was not declared in this scope; did you mean \u2018matmul_id_q8_0_f32_fp32_data\u2019?\r\n 1526 |         ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_mat_id[GGML_TYPE_Q4_0]->l, \"matmul_id_q4_0_f32_l\", matmul_id_q4_0_f32_fp32_len, matmul_id_q4_0_f32_fp32_data, \"main\", 4, sizeof(vk_mat_mat_id_push_constants), l_wg_denoms, warptile_mmq_l, l_align);\r\n      |                                                                                                                                                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                          matmul_id_q8_0_f32_fp32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1585:135: error: \u2018matmul_id_q5_k_f32_aligned_fp32_len\u2019 was not declared in this scope; did you mean \u2018matmul_id_q6_k_f32_aligned_fp32_len\u2019?\r\n 1585 |         ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_mat_id[GGML_TYPE_Q5_K]->a_l, \"matmul_id_q5_k_f32_aligned_l\", matmul_id_q5_k_f32_aligned_fp32_len, matmul_id_q5_k_f32_aligned_fp32_data, \"main\", 4, sizeof(vk_mat_mat_id_push_constants), l_wg_denoms, warptile_mmq_l, l_align);\r\n      |                                                                                                                                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                       matmul_id_q6_k_f32_aligned_fp32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1585:172: error: \u2018matmul_id_q5_k_f32_aligned_fp32_data\u2019 was not declared in this scope; did you mean \u2018matmul_id_q6_k_f32_aligned_fp32_data\u2019?\r\n 1585 |         ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_mat_id[GGML_TYPE_Q5_K]->a_l, \"matmul_id_q5_k_f32_aligned_l\", matmul_id_q5_k_f32_aligned_fp32_len, matmul_id_q5_k_f32_aligned_fp32_data, \"main\", 4, sizeof(vk_mat_mat_id_push_constants), l_wg_denoms, warptile_mmq_l, l_align);\r\n      |                                                                                                                                                                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                                            matmul_id_q6_k_f32_aligned_fp32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1613:127: error: \u2018mul_mat_vec_q3_k_f32_f32_len\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q6_k_f32_f32_len\u2019?\r\n 1613 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[GGML_TYPE_Q3_K], \"mul_mat_vec_q3_k_f32_f32\", mul_mat_vec_q3_k_f32_f32_len, mul_mat_vec_q3_k_f32_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                               ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                               mul_mat_vec_q6_k_f32_f32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1613:157: error: \u2018mul_mat_vec_q3_k_f32_f32_data\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q6_k_f32_f32_data\u2019?\r\n 1613 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[GGML_TYPE_Q3_K], \"mul_mat_vec_q3_k_f32_f32\", mul_mat_vec_q3_k_f32_f32_len, mul_mat_vec_q3_k_f32_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                             mul_mat_vec_q6_k_f32_f32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1617:131: error: \u2018mul_mat_vec_iq4_nl_f32_f32_len\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q4_k_f32_f32_len\u2019?\r\n 1617 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[GGML_TYPE_IQ4_NL], \"mul_mat_vec_iq4_nl_f32_f32\", mul_mat_vec_iq4_nl_f32_f32_len, mul_mat_vec_iq4_nl_f32_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                   mul_mat_vec_q4_k_f32_f32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1617:163: error: \u2018mul_mat_vec_iq4_nl_f32_f32_data\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q4_k_f32_f32_data\u2019?\r\n 1617 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[GGML_TYPE_IQ4_NL], \"mul_mat_vec_iq4_nl_f32_f32\", mul_mat_vec_iq4_nl_f32_f32_len, mul_mat_vec_iq4_nl_f32_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                                                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                                   mul_mat_vec_q4_k_f32_f32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1627:127: error: \u2018mul_mat_vec_q3_k_f16_f32_len\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q5_k_f16_f32_len\u2019?\r\n 1627 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[GGML_TYPE_Q3_K], \"mul_mat_vec_q3_k_f16_f32\", mul_mat_vec_q3_k_f16_f32_len, mul_mat_vec_q3_k_f16_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                               ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                               mul_mat_vec_q5_k_f16_f32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1627:157: error: \u2018mul_mat_vec_q3_k_f16_f32_data\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q5_k_f16_f32_data\u2019?\r\n 1627 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[GGML_TYPE_Q3_K], \"mul_mat_vec_q3_k_f16_f32\", mul_mat_vec_q3_k_f16_f32_len, mul_mat_vec_q3_k_f16_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                             mul_mat_vec_q5_k_f16_f32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1630:127: error: \u2018mul_mat_vec_q6_k_f16_f32_len\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q5_k_f16_f32_len\u2019?\r\n 1630 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[GGML_TYPE_Q6_K], \"mul_mat_vec_q6_k_f16_f32\", mul_mat_vec_q6_k_f16_f32_len, mul_mat_vec_q6_k_f16_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                               ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                               mul_mat_vec_q5_k_f16_f32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1630:157: error: \u2018mul_mat_vec_q6_k_f16_f32_data\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q5_k_f16_f32_data\u2019?\r\n 1630 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[GGML_TYPE_Q6_K], \"mul_mat_vec_q6_k_f16_f32\", mul_mat_vec_q6_k_f16_f32_len, mul_mat_vec_q6_k_f16_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                             mul_mat_vec_q5_k_f16_f32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1631:131: error: \u2018mul_mat_vec_iq4_nl_f16_f32_len\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q4_k_f16_f32_len\u2019?\r\n 1631 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[GGML_TYPE_IQ4_NL], \"mul_mat_vec_iq4_nl_f16_f32\", mul_mat_vec_iq4_nl_f16_f32_len, mul_mat_vec_iq4_nl_f16_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                   mul_mat_vec_q4_k_f16_f32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1631:163: error: \u2018mul_mat_vec_iq4_nl_f16_f32_data\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_q4_k_f16_f32_data\u2019?\r\n 1631 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[GGML_TYPE_IQ4_NL], \"mul_mat_vec_iq4_nl_f16_f32\", mul_mat_vec_iq4_nl_f16_f32_len, mul_mat_vec_iq4_nl_f16_f32_data, \"main\", 3, sizeof(vk_mat_vec_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                                                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                                   mul_mat_vec_q4_k_f16_f32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1641:125: error: \u2018mul_mat_vec_id_q3_k_f32_len\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_id_q5_k_f32_len\u2019?\r\n 1641 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q3_K], \"mul_mat_vec_id_q3_k_f32\", mul_mat_vec_id_q3_k_f32_len, mul_mat_vec_id_q3_k_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                             mul_mat_vec_id_q5_k_f32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1641:154: error: \u2018mul_mat_vec_id_q3_k_f32_data\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_id_q5_k_f32_data\u2019?\r\n 1641 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q3_K], \"mul_mat_vec_id_q3_k_f32\", mul_mat_vec_id_q3_k_f32_len, mul_mat_vec_id_q3_k_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                          mul_mat_vec_id_q5_k_f32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1644:125: error: \u2018mul_mat_vec_id_q6_k_f32_len\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_id_q5_k_f32_len\u2019?\r\n 1644 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q6_K], \"mul_mat_vec_id_q6_k_f32\", mul_mat_vec_id_q6_k_f32_len, mul_mat_vec_id_q6_k_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                             mul_mat_vec_id_q5_k_f32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1644:154: error: \u2018mul_mat_vec_id_q6_k_f32_data\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_id_q5_k_f32_data\u2019?\r\n 1644 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q6_K], \"mul_mat_vec_id_q6_k_f32\", mul_mat_vec_id_q6_k_f32_len, mul_mat_vec_id_q6_k_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                          mul_mat_vec_id_q5_k_f32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1645:129: error: \u2018mul_mat_vec_id_iq4_nl_f32_len\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_id_q4_k_f32_len\u2019?\r\n 1645 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ4_NL], \"mul_mat_vec_id_iq4_nl_f32\", mul_mat_vec_id_iq4_nl_f32_len, mul_mat_vec_id_iq4_nl_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                 mul_mat_vec_id_q4_k_f32_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1645:160: error: \u2018mul_mat_vec_id_iq4_nl_f32_data\u2019 was not declared in this scope; did you mean \u2018mul_mat_vec_id_q4_k_f32_data\u2019?\r\n 1645 |     ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ4_NL], \"mul_mat_vec_id_iq4_nl_f32\", mul_mat_vec_id_iq4_nl_f32_len, mul_mat_vec_id_iq4_nl_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {1, 1, 1}, { device->subgroup_size }, 1);\r\n      |                                                                                                                                                                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                                                                mul_mat_vec_id_q4_k_f32_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1654:95: error: \u2018dequant_q2_k_len\u2019 was not declared in this scope; did you mean \u2018dequant_q5_k_len\u2019?\r\n 1654 |     ggml_vk_create_pipeline(device, device->pipeline_dequant[GGML_TYPE_Q2_K], \"dequant_q2_k\", dequant_q2_k_len, dequant_q2_k_data, \"main\", 2, 5 * sizeof(uint32_t), {256 * 64, 1, 1}, {}, 1);\r\n      |                                                                                               ^~~~~~~~~~~~~~~~\r\n      |                                                                                               dequant_q5_k_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1654:113: error: \u2018dequant_q2_k_data\u2019 was not declared in this scope; did you mean \u2018dequant_q5_k_data\u2019?\r\n 1654 |     ggml_vk_create_pipeline(device, device->pipeline_dequant[GGML_TYPE_Q2_K], \"dequant_q2_k\", dequant_q2_k_len, dequant_q2_k_data, \"main\", 2, 5 * sizeof(uint32_t), {256 * 64, 1, 1}, {}, 1);\r\n      |                                                                                                                 ^~~~~~~~~~~~~~~~~\r\n      |                                                                                                                 dequant_q5_k_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1658:95: error: \u2018dequant_q6_k_len\u2019 was not declared in this scope; did you mean \u2018dequant_q5_k_len\u2019?\r\n 1658 |     ggml_vk_create_pipeline(device, device->pipeline_dequant[GGML_TYPE_Q6_K], \"dequant_q6_k\", dequant_q6_k_len, dequant_q6_k_data, \"main\", 2, 5 * sizeof(uint32_t), {256 * 64, 1, 1}, {}, 1);\r\n      |                                                                                               ^~~~~~~~~~~~~~~~\r\n      |                                                                                               dequant_q5_k_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1658:113: error: \u2018dequant_q6_k_data\u2019 was not declared in this scope; did you mean \u2018dequant_q5_k_data\u2019?\r\n 1658 |     ggml_vk_create_pipeline(device, device->pipeline_dequant[GGML_TYPE_Q6_K], \"dequant_q6_k\", dequant_q6_k_len, dequant_q6_k_data, \"main\", 2, 5 * sizeof(uint32_t), {256 * 64, 1, 1}, {}, 1);\r\n      |                                                                                                                 ^~~~~~~~~~~~~~~~~\r\n      |                                                                                                                 dequant_q5_k_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1659:99: error: \u2018dequant_iq4_nl_len\u2019 was not declared in this scope; did you mean \u2018dequant_q4_k_len\u2019?\r\n 1659 |     ggml_vk_create_pipeline(device, device->pipeline_dequant[GGML_TYPE_IQ4_NL], \"dequant_iq4_nl\", dequant_iq4_nl_len, dequant_iq4_nl_data, \"main\", 2, 5 * sizeof(uint32_t), {256 * 16, 1, 1}, {}, 1);\r\n      |                                                                                                   ^~~~~~~~~~~~~~~~~~\r\n      |                                                                                                   dequant_q4_k_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1659:119: error: \u2018dequant_iq4_nl_data\u2019 was not declared in this scope; did you mean \u2018dequant_q4_k_data\u2019?\r\n 1659 |     ggml_vk_create_pipeline(device, device->pipeline_dequant[GGML_TYPE_IQ4_NL], \"dequant_iq4_nl\", dequant_iq4_nl_len, dequant_iq4_nl_data, \"main\", 2, 5 * sizeof(uint32_t), {256 * 16, 1, 1}, {}, 1);\r\n      |                                                                                                                       ^~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                       dequant_q4_k_data\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1669:101: error: \u2018get_rows_iq4_nl_len\u2019 was not declared in this scope; did you mean \u2018get_rows_q4_1_len\u2019?\r\n 1669 |     ggml_vk_create_pipeline(device, device->pipeline_get_rows[GGML_TYPE_IQ4_NL], \"get_rows_iq4_nl\", get_rows_iq4_nl_len, get_rows_iq4_nl_data, \"main\", 3, sizeof(vk_op_binary_push_constants), {1024, 1, 1}, {}, 1);\r\n      |                                                                                                     ^~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                     get_rows_q4_1_len\r\n/home/vecna/llama.cpp/ggml/src/ggml-vulkan.cpp:1669:122: error: \u2018get_rows_iq4_nl_data\u2019 was not declared in this scope; did you mean \u2018get_rows_q4_1_data\u2019?\r\n 1669 |     ggml_vk_create_pipeline(device, device->pipeline_get_rows[GGML_TYPE_IQ4_NL], \"get_rows_iq4_nl\", get_rows_iq4_nl_len, get_rows_iq4_nl_data, \"main\", 3, sizeof(vk_op_binary_push_constants), {1024, 1, 1}, {}, 1);\r\n      |                                                                                                                          ^~~~~~~~~~~~~~~~~~~~\r\n      |                                                                                                                          get_rows_q4_1_data\r\nmake[2]: *** [ggml/src/CMakeFiles/ggml.dir/build.make:201: ggml/src/CMakeFiles/ggml.dir/ggml-vulkan.cpp.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\nmake[1]: *** [CMakeFiles/Makefile2:1626: ggml/src/CMakeFiles/ggml.dir/all] Error 2\r\nmake: *** [Makefile:146: all] Error 2\n```\n",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-09-21T18:32:09+00:00",
    "closed_at": "2024-09-28T16:23:46+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9582/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9582"
  },
  {
    "number": 2607,
    "title": "[User] Optimizing llama.cpp using nvcomp",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nI expect llama.cpp to efficiently run large language models on commodity computers. To achieve this, I suggest utilizing the nvcomp library to compress and decompress data during memory transfers, which should improve the speed and efficiency of data transfer, especially for larger language models.\r\n\r\n# Current Behavior\r\n\r\nCurrently, llama.cpp uses standard cudaMemcpy calls for transferring data between host and device memory. This may cause bottlenecks, especially when dealing with larger language models on commodity computers that don't have high-end GPUs.\r\n\r\n# Reference\r\n\r\nhttps://developer.nvidia.com/blog/accelerating-lossless-gpu-compression-with-new-flexible-interfaces-in-nvidia-nvcomp/\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-08-14T09:52:08+00:00",
    "closed_at": "2023-08-18T04:53:21+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2607/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 2,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2607"
  },
  {
    "number": 11728,
    "title": "Misc. bug: Tokens in top_probs / top_logprobs are missing whitespace",
    "body": "### Name and Version\n\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes\nversion: 0 (unknown)\nbuilt with gcc (GCC) 13.3.0 for x86_64-unknown-linux-gnu\n\n(actually version 4552, built with Nix)\n\n### Operating systems\n\nLinux\n\n### Which llama.cpp modules do you know to be affected?\n\nllama-server\n\n### Command line\n\n```shell\n$ bin/llama-server -m ../../../wizardcoder-python-34b-v1.0.Q5_K_M.gguf -ngl 9999\n...\n\n$ curl -fsS \\\n    --url http://127.0.0.1:8080/completion \\\n    --header \"Content-Type: application/json\" \\\n    --data '{\"prompt\": \"Hello\",\"n_predict\": 1, \"n_probs\": 10, \"temperature\":0}' | jq .\n{\n  ...\n  \"completion_probabilities\": [\n    {\n      \"id\": 2897,\n      \"token\": \" os\",                      <---------- whitespace OK\n      \"bytes\": [\n        32,                                <---------- whitespace OK\n        111,\n        115\n      ],\n      \"logprob\": -2.0750603675842285,\n      \"top_logprobs\": [\n        {\n          \"id\": 2897,\n          \"token\": \"os\",                   <---------- whitespace missing\n          \"bytes\": [\n            111,                           <---------- whitespace missing\n            115\n          ],\n          \"logprob\": -2.0750603675842285\n        },\n```\n\n### Problem description & steps to reproduce\n\nAs above, doesn't seem to depend on the model\n\n### First Bad Commit\n\n57bb2c40cd94c5a09f5210ed8264cc93b21c4b7e\n\n### Relevant log output\n\n```shell\n\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-02-07T08:05:15+00:00",
    "closed_at": "2025-02-11T13:06:46+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/11728/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/11728"
  }
]
[
  {
    "number": 8138,
    "title": "Bug: infill reference crashed",
    "body": "### What happened?\n\n./llama-infill -t 10 -ngl 0 -m ../../models/Publisher/Repository/codellama-13b.Q3_K_S.gguf --temp 0.7 --repeat_penalty 1.1 -n 20 --in-prefix \"def helloworld():\\n    print(\\\"hell\" --in-suffix \"\\n   print(\\\"goodbye world\\\")\\n    \"\r\n\r\nthis command causes llama.cpp abort\n\n### Name and Version\n\n./llama-llava-cli --version\r\nversion: 3235 (88540445)\r\nbuilt with Apple clang version 15.0.0 (clang-1500.3.9.4) for arm64-apple-darwin23.5.0\n\n### What operating system are you seeing the problem on?\n\nMac\n\n### Relevant log output\n\n```shell\nLog start\r\nmain: build = 3235 (88540445)\r\nmain: built with Apple clang version 15.0.0 (clang-1500.3.9.4) for arm64-apple-darwin23.5.0\r\nmain: seed  = 1719410022\r\nllama_model_loader: loaded meta data with 20 key-value pairs and 363 tensors from ../../models/Publisher/Repository/codellama-13b.Q3_K_S.gguf (version GGUF V2)\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = codellama_codellama-13b-hf\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 16384\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 40\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 11\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   81 tensors\r\nllama_model_loader: - type q3_K:  281 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special tokens cache size = 259\r\nllm_load_vocab: token to piece cache size = 0.1686 MB\r\nllm_load_print_meta: format           = GGUF V2\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32016\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 16384\r\nllm_load_print_meta: n_embd           = 5120\r\nllm_load_print_meta: n_head           = 40\r\nllm_load_print_meta: n_head_kv        = 40\r\nllm_load_print_meta: n_layer          = 40\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: n_embd_k_gqa     = 5120\r\nllm_load_print_meta: n_embd_v_gqa     = 5120\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 13824\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 16384\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 13B\r\nllm_load_print_meta: model ftype      = Q3_K - Small\r\nllm_load_print_meta: model params     = 13.02 B\r\nllm_load_print_meta: model size       = 5.27 GiB (3.48 BPW)\r\nllm_load_print_meta: general.name     = codellama_codellama-13b-hf\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_print_meta: max token length = 48\r\nllm_load_tensors: ggml ctx size =    0.17 MiB\r\nllm_load_tensors: offloading 0 repeating layers to GPU\r\nllm_load_tensors: offloaded 0/41 layers to GPU\r\nllm_load_tensors:        CPU buffer size =  5396.21 MiB\r\n...................................................................................................\r\nllama_new_context_with_model: n_ctx      = 16384\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:        CPU KV buffer size = 12800.00 MiB\r\nllama_new_context_with_model: KV self size  = 12800.00 MiB, K (f16): 6400.00 MiB, V (f16): 6400.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =  1352.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1286\r\nllama_new_context_with_model: graph splits = 642\r\n\r\nsystem_info: n_threads = 10 / 11 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 |\r\nlibc++abi: terminating due to uncaught exception of type std::out_of_range: vector\r\n[1]    37264 abort      ./llama-infill -t 10 -ngl 0 -m  --temp 0.7 --repeat_penalty 1.1 -n 20\n```\n",
    "labels": [
      "bug-unconfirmed",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-06-26T13:54:35+00:00",
    "closed_at": "2024-06-27T07:46:42+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8138/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8138"
  },
  {
    "number": 3064,
    "title": "How to get the best out of my dual GPU or GPU+CPU?",
    "body": "Hi, Sorry to ask here, but I can't access discord.\r\n\r\nCan anyone advise me as to the best models and options for my system: \r\n\r\n## Arch linux, Amd 5950x 64GB ram, Nvidia 3060 12GB, and 1080 8GB.\r\n\r\nI prefer quality over speed, as I plan on running this for special tasks in a queue.\r\n\r\nThanks!",
    "labels": [],
    "state": "closed",
    "created_at": "2023-09-07T15:09:30+00:00",
    "closed_at": "2023-09-07T20:08:55+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3064/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3064"
  },
  {
    "number": 8164,
    "title": "Bug: on AMD gpu, it offloads all the work to the CPU unless you specify --n-gpu-layers on the llama-cli command line",
    "body": "### What happened?\n\nI spent days trying to figure out why it running a llama 3 instruct model was going super slow (about 3 tokens per second on fp16 and 5.6 on 8 bit) on an AMD MI50 32GB using rocBLAS for ROCm 6.1.2, using 0% GPU and 100% cpu even while using some vram.  I'm currently using release b3246\r\n\r\nFinally I noticed that (for 8 bit) it said:\r\n```\r\n\r\nllm_load_tensors: offloading 0 repeating layers to GPU\r\nllm_load_tensors: offloaded 0/33 layers to GPU\r\nllm_load_tensors:        CPU buffer size =  8137.64 MiB\r\n\r\n```\r\n\r\nadding something to the command line like \"--n-gpu-layers 100\" changed it to\r\n```\r\nllm_load_tensors: offloading 32 repeating layers to GPU\r\nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors: offloaded 33/33 layers to GPU\r\nllm_load_tensors:      ROCm0 buffer size =  7605.33 MiB\r\nllm_load_tensors:        CPU buffer size =   532.31 MiB\r\n```\r\nand it jumped from\r\n```\r\nllama_print_timings:        load time =    1955.34 ms\r\nllama_print_timings:      sample time =      15.28 ms /   128 runs   (    0.12 ms per token,  8378.61 tokens per second)\r\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\r\nllama_print_timings:        eval time =   23640.83 ms /   128 runs   (  184.69 ms per token,     5.41 tokens per second)\r\nllama_print_timings:       total time =   23754.79 ms /   128 tokens\r\n```\r\n\r\nto\r\n```\r\nllama_print_timings:        load time =    2824.15 ms\r\nllama_print_timings:      sample time =      12.57 ms /   128 runs   (    0.10 ms per token, 10182.98 tokens per second)\r\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\r\nllama_print_timings:        eval time =    2469.71 ms /   128 runs   (   19.29 ms per token,    51.83 tokens per second)\r\nllama_print_timings:       total time =    2566.10 ms /   128 tokens\r\n```\r\nThe CPU usage went down from maxing out a bunch of cores to only using one.\r\nThe GPU usage went from 0% up to the 99%\r\n\r\n16 bit didn't improve as much\r\nit went from \r\n```\r\nllama_print_timings:        load time =    2497.93 ms\r\nllama_print_timings:      sample time =      15.75 ms /   128 runs   (    0.12 ms per token,  8126.47 tokens per second)\r\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\r\nllama_print_timings:        eval time =   43228.83 ms /   128 runs   (  337.73 ms per token,     2.96 tokens per second)\r\nllama_print_timings:       total time =   43344.71 ms /   128 tokens\r\n```\r\n\r\nto\r\n```\r\nllama_print_timings:        load time =    3937.74 ms\r\nllama_print_timings:      sample time =      13.31 ms /   128 runs   (    0.10 ms per token,  9616.11 tokens per second)\r\nllama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\r\nllama_print_timings:        eval time =    8066.54 ms /   128 runs   (   63.02 ms per token,    15.87 tokens per second)\r\nllama_print_timings:       total time =    8166.01 ms /   128 tokens\r\n```\r\n\r\nThis is the command line I'm testing with\r\n\r\n`build/bin/llama-cli -m ./models/llama3-8b-instruct/ggml-model-q8_0.gguf -n 128 --threads 12 --n-gpu-layers 100`\r\n\r\nafter building with\r\n\r\n`HIPCXX=\"/opt/rocm-6.1.2/llvm/bin/clang\" HIP_PATH=\"/opt/rocm-6.1.2\"  cmake -S . -B build -DGGML_HIPBLAS=ON -DAMDGPU_TARGETS=gfx906 -DCMAKE_BUILD_TYPE=Release && cmake --build build  --config Release -- -j 12`\r\n\r\nYes I put the paths in manually.  It was just part of how I was messing around to see what's wrong\n\n### Name and Version\n\nversion: 0 (unknown)\r\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\r\n\r\nThe version string seems to be another bug, but I'm using the source from release b3246\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\nllm_load_tensors: offloading 0 repeating layers to GPU\r\nllm_load_tensors: offloaded 0/33 layers to GPU\r\nllm_load_tensors:        CPU buffer size =  8137.64 MiB\n```\n",
    "labels": [
      "bug-unconfirmed",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-06-27T12:19:01+00:00",
    "closed_at": "2024-06-28T04:22:31+00:00",
    "comments": 24,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8164/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8164"
  },
  {
    "number": 361,
    "title": "Invalid model error : too old, regenerate your model files!",
    "body": "Downloaded Alpaca 7B model successfully using the following command as mentioned in README.md:\r\n`curl -o ./models/ggml-alpaca-7b-q4.bin -C - https://gateway.estuary.tech/gw/ipfs/QmUp1UGeQFDqJKvtjbSYPBiZZKRjLp8shVP9hT8ZB9Ynv1`\r\n\r\nWhen I try to execute the command:\r\n`main -m ./models/ggml-alpaca-7b-q4.bin --color -f ./prompts/alpaca.txt -ins`\r\n\r\nThis is the error output:\r\nmain: seed = 1679417098\r\nllama_model_load: loading model from './models/ggml-alpaca-7b-q4.bin' - please wait ...\r\nllama_model_load: invalid model file './models/ggml-alpaca-7b-q4.bin' (too old, regenerate your model files!)\r\nmain: failed to load model from './models/ggml-alpaca-7b-q4.bin'\r\n\r\nHow to fix this? Is the downloaded model corrupted and should I download it again? What is the SHA1 hash of the model so that I can verify that the downloaded model is corrupted or not?",
    "labels": [
      "documentation",
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-21T16:51:17+00:00",
    "closed_at": "2023-03-22T05:54:53+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/361/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/361"
  },
  {
    "number": 13795,
    "title": "Eval bug: Output NAN when use Qwen3 embedding models with FP16",
    "body": "### Name and Version\n\nversion: 5489 (2d38b6e4)\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCPU, CUDA\n\n### Hardware\n\nINTEL(R) XEON(R) PLATINUM 8558 + Hopper GPU\n\n### Models\n\nQwen3-Embedding-8B\n\n### Problem description & steps to reproduce\n\n1. `git clone https://github.com/ggml-org/llama.cpp`\n2. `cmake -B build && cmake --build build --config Release -j 32`\n3. `python convert_hf_to_gguf.py Qwen3-Embedding-8B --outfile Qwen3-Embedding-8B-f16.gguf --outtype f16`\n```\nINFO:hf-to-gguf:Loading model: Qwen3-Embedding-8B\nINFO:hf-to-gguf:Model architecture: Qwen3ForCausalLM\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Exporting model...\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00007.safetensors'\nINFO:hf-to-gguf:token_embd.weight,         torch.float32 --> F16, shape = {4096, 151665}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.0.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.0.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.1.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.2.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.3.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00007.safetensors'\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.4.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.5.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.6.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.7.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.8.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.9.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00007.safetensors'\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.10.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.11.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.12.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.12.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.13.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.14.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.15.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00007.safetensors'\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.16.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.17.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.18.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.19.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.20.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.21.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.22.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:gguf: loading model part 'model-00005-of-00007.safetensors'\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.23.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.24.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.24.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.25.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.26.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.27.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.28.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.28.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.28.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.28.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.28.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:gguf: loading model part 'model-00006-of-00007.safetensors'\nINFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.29.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.29.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.29.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.29.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.29.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.30.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.30.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.30.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.30.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.30.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.31.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.31.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.31.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.31.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.31.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.32.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.32.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.32.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.32.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.32.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.32.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.32.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.32.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.32.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.32.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.32.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.33.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.33.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.33.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.33.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.33.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.33.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.33.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.33.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.33.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.33.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.33.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.34.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.34.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.34.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.34.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.34.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.34.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.34.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.34.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:gguf: loading model part 'model-00007-of-00007.safetensors'\nINFO:hf-to-gguf:output.weight,             torch.float32 --> F16, shape = {4096, 151665}\nINFO:hf-to-gguf:blk.34.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.34.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.34.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.35.attn_norm.weight,   torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.35.ffn_down.weight,    torch.float32 --> F16, shape = {12288, 4096}\nINFO:hf-to-gguf:blk.35.ffn_gate.weight,    torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.35.ffn_up.weight,      torch.float32 --> F16, shape = {4096, 12288}\nINFO:hf-to-gguf:blk.35.ffn_norm.weight,    torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.35.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.35.attn_k.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:blk.35.attn_output.weight, torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.35.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.35.attn_q.weight,      torch.float32 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.35.attn_v.weight,      torch.float32 --> F16, shape = {4096, 1024}\nINFO:hf-to-gguf:output_norm.weight,        torch.float32 --> F32, shape = {4096}\nINFO:hf-to-gguf:Set meta model\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 40960\nINFO:hf-to-gguf:gguf: embedding length = 4096\nINFO:hf-to-gguf:gguf: feed forward length = 12288\nINFO:hf-to-gguf:gguf: head count = 32\nINFO:hf-to-gguf:gguf: key-value head count = 8\nINFO:hf-to-gguf:gguf: rope theta = 1000000\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model quantization version\nINFO:hf-to-gguf:Set model tokenizer\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting add_bos_token to False\nINFO:gguf.vocab:Setting chat_template to {%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n\nINFO:gguf.gguf_writer:Writing the following files:\nINFO:gguf.gguf_writer:Qwen3-Embedding-8B-f16.gguf: n_tensors = 399, total_size = 16.4G\nWriting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16.4G/16.4G [00:43<00:00, 375Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to Qwen3-Embedding-8B-f16.gguf\n```\n4. `./build/bin/llama-embedding -m Qwen3-Embedding-8B-f16.gguf  -e -p \"hello world!\"  --pooling cls -ub 8192`\n\nHowever, the embedding it generated is NAN. I tried different pooling methods such as `mean` or `cls`, but none of them solved the problem. I have checked and found that there are no NAN values in the generated GGUF file.\nIs it because my way of using llama.cpp is incorrect? Or did I not handle the input data correctly?\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nbuild: 5489 (2d38b6e4) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nmain: setting batch size to 4096\nllama_model_loader: loaded meta data with 27 key-value pairs and 399 tensors from Qwen3-Embedding-8B-f16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 Embedding 8B\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3-Embedding\nllama_model_loader: - kv   4:                         general.size_label str              = 8B\nllama_model_loader: - kv   5:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960\nllama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 4096\nllama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 12288\nllama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  15:                          general.file_type u32              = 1\nllama_model_loader: - kv  16:               general.quantization_version u32              = 2\nllama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151665]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151665]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u0120 \u0120\", \"\u0120\u0120 \u0120\u0120\", \"i n\", \"\u0120 t\",...\nllama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  26:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type  f16:  254 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = F16\nprint_info: file size   = 15.25 GiB (16.00 BPW) \nload: special tokens cache size = 22\nload: token to piece cache size = 0.9310 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 40960\nprint_info: n_embd           = 4096\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 12288\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 40960\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 8B\nprint_info: model params     = 8.19 B\nprint_info: general.name     = Qwen3 Embedding 8B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151665\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 '\u010a'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:          AMX model buffer size = 13248.00 MiB\nload_tensors:   CPU_Mapped model buffer size = 15618.94 MiB\n.......................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 4096\nllama_context: n_ubatch      = 4096\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.00 MiB\nllama_kv_cache_unified:        CPU KV buffer size =   576.00 MiB\nllama_kv_cache_unified: size =  576.00 MiB (  4096 cells,  36 layers,  1 seqs), K (f16):  288.00 MiB, V (f16):  288.00 MiB\nllama_context:        CPU compute buffer size =  2497.83 MiB\nllama_context: graph nodes  = 1447\nllama_context: graph splits = 1\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n\nsystem_info: n_threads = 96 (n_threads_batch = 96) / 192 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | AMX_INT8 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \nmain: last token in the prompt is not SEP\nmain: 'tokenizer.ggml.add_eos_token' should be set to 'true' in the GGUF header\nmain: prompt 0: 'hello world!'\nmain: number of tokens in prompt = 3\n 14990 -> 'hello'\n  1879 -> ' world'\n     0 -> '!'\n\n\nbatch_decode: n_tokens = 3, n_seq = 1\n\nembedding 0:      -nan      -nan      -nan      -nan      -nan      ...     -nan      -nan      -nan      -nan \n\nllama_perf_context_print:        load time =    6998.78 ms\nllama_perf_context_print: prompt eval time =     494.21 ms /     3 tokens (  164.74 ms per token,     6.07 tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =     553.83 ms /     4 tokens\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-05-26T08:58:30+00:00",
    "closed_at": "2025-05-26T11:03:55+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13795/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13795"
  },
  {
    "number": 8161,
    "title": "Bug: llama.cpp binaries are compiled dynamically and the library is missing!",
    "body": "### What happened?\n\n$ ./build/bin/llama-quantize -h\r\n./build/bin/llama-quantize: error while loading shared libraries: libllama.so: cannot open shared object file: No such file or directory\n\n### Name and Version\n\nlatest\n\n### What operating system are you seeing the problem on?\n\n_No response_\n\n### Relevant log output\n\n```shell\nsee up.\n```\n",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-06-27T11:06:26+00:00",
    "closed_at": "2024-06-28T10:49:18+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8161/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8161"
  },
  {
    "number": 7403,
    "title": "ci failing on main branch",
    "body": "* https://github.com/ggerganov/llama.cpp/pull/7358\r\n    - Server / server (THREAD, Debug) (push) Failing after 26m\r\n        - https://github.com/ggerganov/llama.cpp/actions/runs/9141008261/job/25134947389 \r\n        -  20 - test-backend-ops (Failed)\r\n* https://github.com/ggerganov/llama.cpp/pull/7374\r\n    - ggml-org / ggml-2-x86-cpu - failure 1 in 3:34.28\r\n        - https://github.com/ggml-org/ci/tree/results/llama.cpp/1e/a2a0036e88172d6c8bf7e1a1989a03894dc955/ggml-2-x86-cpu#test_scripts_debug\r\n        - ?? \r\n* https://github.com/ggerganov/llama.cpp/pull/7395\r\n    - ggml-org / ggml-4-x86-cuda-v100 - failure 8 in 7:00.84\r\n        - https://github.com/ggml-org/ci/tree/results/llama.cpp/d3/59f30921a9f62a0fd299c412ff3f270286fea6/ggml-4-x86-cuda-v100\r\n        -  20 - test-backend-ops (Failed)",
    "labels": [
      "high priority",
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-05-20T00:58:44+00:00",
    "closed_at": "2024-05-20T15:11:40+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7403/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7403"
  },
  {
    "number": 8107,
    "title": "sh: 1: ./llama.cpp/llama-quantize: not found",
    "body": "### What happened?\n\nFor converting the FP16.gguf to q5_k_m for llama3 i was getting the error **sh: 1: ./llama.cpp/llama-quantize: not found** previously i used **os.system(\"./llama.cpp/llama-quantize \" + gguf_dir + \"/\" + gguf_F16_name + \" \" + model_path + \" \" + m)** in my script to convert even that is not working now its giving the same not found issue\n\n### Name and Version\n\nLastest pervsion\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\nbash: ./llama.cpp/quantize: No such file or directory\n```\n",
    "labels": [
      "bug-unconfirmed",
      "low severity"
    ],
    "state": "closed",
    "created_at": "2024-06-25T09:30:17+00:00",
    "closed_at": "2024-06-25T11:41:09+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/8107/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/8107"
  },
  {
    "number": 10662,
    "title": "Performance bug: Android aarch64 Neon Performance Regression and i8mm Detection Issues in New Version of llama.cpp",
    "body": "### Name and Version\r\n\r\nversion: 4248 (3b4f2e33) built with clang version 19.1.4 for aarch64-unknown-linux-android24\r\n\r\n### Operating systems\r\n\r\nLinux\r\n\r\n### GGML backends\r\n\r\nCPU\r\n\r\n### Hardware\r\n\r\nDevice - Zenfone 9:\u00a0 - Qualcomm\u00ae Snapdragon\u00ae 8+ Gen 1 Mobile Platform\r\n```\r\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 8 | CPU : NEON = 1 | ARM_FMA = 1 | AARCH64_REPACK = 1 |\r\n```\r\n```\r\nlscpu\r\nArchitecture: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 aarch64\r\n\u00a0 CPU op-mode(s): \u00a0 \u00a0 \u00a0 32-bit, 64-bit\r\n\u00a0 Byte Order: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Little Endian\r\nCPU(s): \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 8\r\n\u00a0 On-line CPU(s) list: \u00a00-7\r\nVendor ID: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0ARM\r\n\u00a0 Model name: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Cortex-A510\r\n\u00a0 \u00a0 Model: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a03\r\n\u00a0 \u00a0 Thread(s) per core: 1\r\n\u00a0 \u00a0 Core(s) per socket: 4\r\n\u00a0 \u00a0 Socket(s): \u00a0 \u00a0 \u00a0 \u00a0 \u00a01\r\n\u00a0 \u00a0 Stepping: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 r0p3\r\n\u00a0 \u00a0 CPU(s) scaling MHz: 77%\r\n\u00a0 \u00a0 CPU max MHz: \u00a0 \u00a0 \u00a0 \u00a02016.0000\r\n\u00a0 \u00a0 CPU min MHz: \u00a0 \u00a0 \u00a0 \u00a0307.2000\r\n\u00a0 \u00a0 BogoMIPS: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 38.40\r\n\u00a0 \u00a0 Flags: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0fp asimd evtstrm aes pmull sha1\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sha2 crc32 atomics fphp asimdhp\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpuid asimdrdm jscvt fcma lrcpc\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 dcpop sha3 sm3 sm4 asimddp sha51\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 2 asimdfhm dit uscat ilrcpc flag\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 m ssbs sb paca pacg dcpodp flagm\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 2 frint i8mm bf16 bti\r\n```\r\n\r\n### Models\r\n\r\nbartowski/Llama-3.2-3B-Instruct-GGUF\r\nhttps://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/tree/main\r\n\r\n### Problem description & steps to reproduce\r\n\r\n**Performance in the Old Version:**\r\nFor Q4_0 and IQ4_NL, performance was normal and as expected, given that repacking was not applied in these cases.\r\nThe Q4_0_4_4 prompt processing performance was exceptional in the old version, significantly surpassing other formats.\r\n\r\n**Performance in the New Version:**\r\nThe Q4_0_4_4 format now shows drastically reduced performance, falling below the levels of Q4_0 and IQ4_NL. This is a notable regression from the old version's behavior.\r\nRepacking for Q4_0 and IQ4_NL appears to be ineffective in the new version. Instead of improving performance, it is slightly slower compared to the old version. This does not align with expectations of repacking offering at least similar performance to the previous implementation of Q4_0_4_4.\r\n\r\n**i8mm Support Issue:**\r\nEven though lscpu indicates support for i8mm, llama.cpp does not detect or leverage this feature in the new version.\r\n\r\n### First Bad Commit\r\n\r\nI could not pinpoint the first commit, but I found that before the Neon changes [f2f5c3b6 (4105)] I still had the expected performance.\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nPrevious version (3 weeks ago) - build: f2f5c3b6 (4105)\r\n./llama-bench -p 512 -n 128 -t 4 -ngl 0 -m ...model...\r\n model                          |       size |     params | backend    | threads |          test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |\r\n| llama 3B Q4_0                  |   1.98 GiB |     3.61 B | CPU        |       4 |         pp512 |          8.60 \u00b1 0.95 |\r\n| llama 3B Q4_0                  |   1.98 GiB |     3.61 B | CPU        |       4 |         tg128 |          4.56 \u00b1 0.79 |\r\n| llama 3B IQ4_NL - 4.5 bpw      |   1.98 GiB |     3.61 B | CPU        |       4 |         pp512 |          9.21 \u00b1 0.97 |\r\n| llama 3B IQ4_NL - 4.5 bpw      |   1.98 GiB |     3.61 B | CPU        |       4 |         tg128 |          6.73 \u00b1 1.10 |\r\n| llama 3B Q4_0_4_4              |   1.98 GiB |     3.61 B | CPU        |       4 |         pp512 |         16.71 \u00b1 0.96 |\r\n| llama 3B Q4_0_4_4              |   1.98 GiB |     3.61 B | CPU        |       4 |         tg128 |          5.67 \u00b1 0.27 |\r\n\r\nCurrent version (main yesterday) - build: 3b4f2e33 (4248)\r\n./llama-bench -p 512 -n 128 -t 4 -ngl 0 -m ...model...\r\n model                          |       size |     params | backend    | threads |          test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |\r\n| llama 3B Q4_0                  |   1.98 GiB |     3.61 B | CPU        |       4 |         pp512 |          6.42 \u00b1 1.73 |\r\n| llama 3B Q4_0                  |   1.98 GiB |     3.61 B | CPU        |       4 |         tg128 |          2.59 \u00b1 0.10 |\r\n| llama 3B IQ4_NL - 4.5 bpw      |   1.98 GiB |     3.61 B | CPU        |       4 |         pp512 |          7.47 \u00b1 0.88 |\r\n| llama 3B IQ4_NL - 4.5 bpw      |   1.98 GiB |     3.61 B | CPU        |       4 |         tg128 |          4.12 \u00b1 0.57 |\r\n| llama 3B Q4_0_4_4              |   1.98 GiB |     3.61 B | CPU        |       4 |         pp512 |          2.28 \u00b1 0.32 |\r\n| llama 3B Q4_0_4_4              |   1.98 GiB |     3.61 B | CPU        |       4 |         tg128 |          1.12 \u00b1 0.33 |\r\n```\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-12-04T19:11:49+00:00",
    "closed_at": "2024-12-04T20:20:06+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10662/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10662"
  },
  {
    "number": 12220,
    "title": "[Solved]Model generation speed significantly slows down when using MiroStat V2",
    "body": "llama.cpp-b4835 Linux CPU\n\n#### ./llama-cli -m Phi-4-mini-instruct-Q5_K_M.gguf --threads 16 --ctx-size 16000 -mli -co -cnv\n\n```\nllama_perf_sampler_print:    sampling time =     147.25 ms /   456 runs   (    0.32 ms per token,  3096.75 tokens per second)\nllama_perf_context_print:        load time =    2054.69 ms\nllama_perf_context_print: prompt eval time =     667.05 ms /    12 tokens (   55.59 ms per token,    17.99 tokens per second)\nllama_perf_context_print:        eval time =   40436.85 ms /   443 runs   (   91.28 ms per token,    10.96 tokens per second)\nllama_perf_context_print:       total time =   88806.27 ms /   455 tokens\n```\n\n\n#### ./llama-cli -m Phi-4-mini-instruct-Q5_K_M.gguf --threads 16 --ctx-size 16000 --mirostat 2 -mli -co -cnv\n\n```\nllama_perf_sampler_print:    sampling time =   17418.53 ms /   508 runs   (   34.29 ms per token,    29.16 tokens per second)\nllama_perf_context_print:        load time =    2014.50 ms\nllama_perf_context_print: prompt eval time =    1372.56 ms /    12 tokens (  114.38 ms per token,     8.74 tokens per second)\nllama_perf_context_print:        eval time =  148513.42 ms /   495 runs   (  300.03 ms per token,     3.33 tokens per second)\nllama_perf_context_print:       total time =  174247.01 ms /   507 tokens\n```\n",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-06T07:57:53+00:00",
    "closed_at": "2025-03-06T15:57:10+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12220/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12220"
  },
  {
    "number": 12213,
    "title": "Eval bug: bartowski/functionary-small-v3.2-GGUF:Q4_K_M model prepends \"assistant\\n\" to text responses when tools are provided",
    "body": "### Name and Version\n\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4070 Laptop GPU, compute capability 8.9, VMM: yes\nversion: 4783 (a800ae46)\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCUDA\n\n### Hardware\n\ni9-13900HX + NVIDIA GeForce RTX 4070\n\n### Models\n\nhttps://huggingface.co/bartowski/functionary-small-v3.2-GGUF/blob/main/functionary-small-v3.2-Q4_K_M.gguf\n\n### Problem description & steps to reproduce\n\n`docker run --gpus all --rm --name llama.cpp -p 8080:8080 -v /etc/ssl/certs:/etc/ssl/certs:ro -v /home/ed/.llama.cpp/models:/root/.cache ghcr.io/ggml-org/llama.cpp:full-cuda -s --ctx-size 0 --jinja -fa -hf bartowski/functionary-small-v3.2-GGUF:Q4_K_M --host 0.0.0.0 -ngl 10 --verbose`\n\n```\ncurl http://localhost:8080/v1/chat/completions -d '{\n\"model\": \"gpt-3.5-turbo\",\n\"messages\": [\n    {\n    \"role\": \"user\",\n    \"content\": \"Hi.\"\n    }\n]\n}' | jq '.choices[0]'\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  2352  100  2254  100    98   2762    120 --:--:-- --:--:-- --:--:--  2882\n{\n  \"finish_reason\": \"stop\",\n  \"index\": 0,\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"Hello! How can I help you today?\"\n  }\n}\n```\n\n```\ncurl http://localhost:8080/v1/chat/completions -d '{\n\"model\": \"gpt-3.5-turbo\",\n\"tools\": [\n    {\n    \"type\":\"function\",\n    \"function\":{\n        \"name\":\"python\",\n        \"description\":\"Runs code in an ipython interpreter and returns the result of the execution after 60 seconds.\",\n        \"parameters\":{\n        \"type\":\"object\",\n        \"properties\":{\n            \"code\":{\n            \"type\":\"string\",\n            \"description\":\"The code to run in the ipython interpreter.\"\n            }\n        },\n        \"required\":[\"code\"]\n        }\n    }\n    }\n],\n\"messages\": [\n    {\n    \"role\": \"user\",\n    \"content\": \"Hi\"\n    }\n]\n}' | jq '.choices[0]'\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  4096  100  3527  100   569   4240    684 --:--:-- --:--:-- --:--:--  4923\n{\n  \"finish_reason\": \"stop\",\n  \"index\": 0,\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"assistant\\nHello! How can I assist you today?\"\n  }\n}\n```\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n+ docker run --gpus all --rm --name llama.cpp -p 8080:8080 -v /etc/ssl/certs:/etc/ssl/certs:ro -v /home/ed/.llama.cpp/models:/root/.cache ghcr.io/ggml-org/llama.cpp:full-cuda -s --ctx-size 0 --jinja -fa -hf bartowski/functionary-small-v3.2-GGUF:Q4_K_M --host 0.0.0.0 -ngl 10 --verbose\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 4070 Laptop GPU, compute capability 8.9, VMM: yes\nbuild: 4783 (a800ae46) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nsystem info: n_threads = 8, n_threads_batch = 8, total_threads = 32\n\nsystem_info: n_threads = 8 (n_threads_batch = 8) / 32 | CUDA : ARCHS = 500,610,700,750,800 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n\nmain: HTTP server is listening, hostname: 0.0.0.0, port: 8080, http threads: 31\nmain: loading model\nsrv    load_model: loading model '/root/.cache/llama.cpp/bartowski_functionary-small-v3.2-GGUF_functionary-small-v3.2-Q4_K_M.gguf'\ncommon_download_file: previous metadata file found /root/.cache/llama.cpp/bartowski_functionary-small-v3.2-GGUF_functionary-small-v3.2-Q4_K_M.gguf.json: {\"etag\":\"\\\"e0ce54ab24981f28174430665c1ed516-308\\\"\",\"lastModified\":\"Thu, 08 Aug 2024 09:29:55 GMT\",\"url\":\"https://huggingface.co/bartowski/functionary-small-v3.2-GGUF/resolve/main/functionary-small-v3.2-Q4_K_M.gguf\"}\ncurl_perform_with_retry: Trying to download from https://huggingface.co/bartowski/functionary-small-v3.2-GGUF/resolve/main/functionary-small-v3.2-Q4_K_M.gguf (attempt 1 of 3)...\nllama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4070 Laptop GPU) - 7793 MiB free\nllama_model_loader: loaded meta data with 34 key-value pairs and 292 tensors from /root/.cache/llama.cpp/bartowski_functionary-small-v3.2-GGUF_functionary-small-v3.2-Q4_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\nllama_model_loader: - kv   3:                            general.version str              = v3.2\nllama_model_loader: - kv   4:                       general.organization str              = Meta Llama\nllama_model_loader: - kv   5:                           general.finetune str              = Instruct\nllama_model_loader: - kv   6:                           general.basename str              = Meta-Llama-3.1\nllama_model_loader: - kv   7:                         general.size_label str              = 8B\nllama_model_loader: - kv   8:                            general.license str              = mit\nllama_model_loader: - kv   9:                          llama.block_count u32              = 32\nllama_model_loader: - kv  10:                       llama.context_length u32              = 131072\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 15\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"\u0120 \u0120\", \"\u0120 \u0120\u0120\u0120\", \"\u0120\u0120 \u0120\u0120\", \"...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128009\nllama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\nllama_model_loader: - kv  29:               general.quantization_version u32              = 2\nllama_model_loader: - kv  30:                      quantize.imatrix.file str              = /models_out/functionary-small-v3.2-GG...\nllama_model_loader: - kv  31:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\nllama_model_loader: - kv  32:             quantize.imatrix.entries_count i32              = 224\nllama_model_loader: - kv  33:              quantize.imatrix.chunks_count i32              = 125\nllama_model_loader: - type  f32:   66 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 4.58 GiB (4.89 BPW) \ninit_tokenizer: initializing tokenizer for type 2\nload: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\nload: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\nload: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\nload: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\nload: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\nload: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\nload: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\nload: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\nload: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\nload: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\nload: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\nload: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\nload: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\nload: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\nload: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\nload: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\nload: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\nload: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\nload: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\nload: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\nload: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\nload: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\nload: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\nload: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\nload: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\nload: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\nload: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\nload: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\nload: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\nload: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\nload: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\nload: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\nload: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\nload: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\nload: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\nload: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\nload: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\nload: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\nload: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\nload: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\nload: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\nload: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\nload: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\nload: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\nload: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\nload: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\nload: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\nload: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\nload: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\nload: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\nload: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\nload: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\nload: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\nload: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\nload: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\nload: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\nload: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\nload: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\nload: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\nload: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\nload: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\nload: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\nload: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\nload: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\nload: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\nload: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\nload: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\nload: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\nload: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\nload: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\nload: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\nload: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\nload: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\nload: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\nload: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\nload: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\nload: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\nload: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\nload: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\nload: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\nload: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\nload: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\nload: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\nload: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\nload: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\nload: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\nload: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\nload: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\nload: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\nload: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\nload: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\nload: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\nload: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\nload: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\nload: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\nload: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\nload: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\nload: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\nload: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\nload: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\nload: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\nload: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\nload: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\nload: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\nload: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\nload: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\nload: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\nload: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\nload: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\nload: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\nload: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\nload: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\nload: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\nload: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\nload: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\nload: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\nload: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\nload: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\nload: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\nload: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\nload: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\nload: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\nload: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\nload: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\nload: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\nload: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\nload: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\nload: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\nload: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\nload: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\nload: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\nload: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\nload: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\nload: control token: 128007 '<|end_header_id|>' is not marked as EOG\nload: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\nload: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\nload: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\nload: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\nload: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\nload: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\nload: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\nload: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\nload: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\nload: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\nload: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\nload: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\nload: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\nload: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\nload: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\nload: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\nload: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\nload: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\nload: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\nload: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\nload: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\nload: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\nload: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\nload: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\nload: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\nload: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\nload: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\nload: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\nload: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\nload: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\nload: control token: 128001 '<|end_of_text|>' is not marked as EOG\nload: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\nload: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\nload: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\nload: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\nload: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\nload: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\nload: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\nload: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\nload: control token: 128006 '<|start_header_id|>' is not marked as EOG\nload: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\nload: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\nload: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\nload: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\nload: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\nload: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\nload: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\nload: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\nload: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\nload: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\nload: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\nload: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\nload: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\nload: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\nload: control token: 128000 '<|begin_of_text|>' is not marked as EOG\nload: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\nload: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\nload: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\nload: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\nload: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\nload: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\nload: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\nload: control token: 128010 '<|python_tag|>' is not marked as EOG\nload: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\nload: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\nload: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\nload: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\nload: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\nload: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\nload: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\nload: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\nload: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\nload: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\nload: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\nload: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\nload: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\nload: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\nload: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\nload: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\nload: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\nload: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\nload: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\nload: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\nload: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\nload: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\nload: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\nload: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\nload: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\nload: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\nload: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\nload: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\nload: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\nload: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\nload: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\nload: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\nload: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\nload: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\nload: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\nload: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\nload: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\nload: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\nload: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\nload: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\nload: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\nload: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\nload: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\nload: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\nload: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\nload: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\nload: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\nload: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\nload: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\nload: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\nload: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\nload: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\nload: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\nload: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\nload: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\nload: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\nload: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\nload: special tokens cache size = 256\nload: token to piece cache size = 0.7999 MB\nprint_info: arch             = llama\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 4096\nprint_info: n_layer          = 32\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: n_ff             = 14336\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 500000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 8B\nprint_info: model params     = 8.03 B\nprint_info: general.name     = Meta Llama 3.1 8B Instruct\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 128256\nprint_info: n_merges         = 280147\nprint_info: BOS token        = 128000 '<|begin_of_text|>'\nprint_info: EOS token        = 128009 '<|eot_id|>'\nprint_info: EOT token        = 128009 '<|eot_id|>'\nprint_info: EOM token        = 128008 '<|eom_id|>'\nprint_info: PAD token        = 128009 '<|eot_id|>'\nprint_info: LF token         = 198 '\u010a'\nprint_info: EOG token        = 128008 '<|eom_id|>'\nprint_info: EOG token        = 128009 '<|eot_id|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: layer   0 assigned to device CPU\nload_tensors: layer   1 assigned to device CPU\nload_tensors: layer   2 assigned to device CPU\nload_tensors: layer   3 assigned to device CPU\nload_tensors: layer   4 assigned to device CPU\nload_tensors: layer   5 assigned to device CPU\nload_tensors: layer   6 assigned to device CPU\nload_tensors: layer   7 assigned to device CPU\nload_tensors: layer   8 assigned to device CPU\nload_tensors: layer   9 assigned to device CPU\nload_tensors: layer  10 assigned to device CPU\nload_tensors: layer  11 assigned to device CPU\nload_tensors: layer  12 assigned to device CPU\nload_tensors: layer  13 assigned to device CPU\nload_tensors: layer  14 assigned to device CPU\nload_tensors: layer  15 assigned to device CPU\nload_tensors: layer  16 assigned to device CPU\nload_tensors: layer  17 assigned to device CPU\nload_tensors: layer  18 assigned to device CPU\nload_tensors: layer  19 assigned to device CPU\nload_tensors: layer  20 assigned to device CPU\nload_tensors: layer  21 assigned to device CPU\nload_tensors: layer  22 assigned to device CUDA0\nload_tensors: layer  23 assigned to device CUDA0\nload_tensors: layer  24 assigned to device CUDA0\nload_tensors: layer  25 assigned to device CUDA0\nload_tensors: layer  26 assigned to device CUDA0\nload_tensors: layer  27 assigned to device CUDA0\nload_tensors: layer  28 assigned to device CUDA0\nload_tensors: layer  29 assigned to device CUDA0\nload_tensors: layer  30 assigned to device CUDA0\nload_tensors: layer  31 assigned to device CUDA0\nload_tensors: layer  32 assigned to device CPU\nload_tensors: tensor 'token_embd.weight' (q4_K) (and 222 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\nload_tensors: offloading 10 repeating layers to GPU\nload_tensors: offloaded 10/33 layers to GPU\nload_tensors:        CUDA0 model buffer size =  1263.13 MiB\nload_tensors:   CPU_Mapped model buffer size =  4685.30 MiB\n.......................................................................................\nllama_init_from_model: n_seq_max     = 1\nllama_init_from_model: n_ctx         = 131072\nllama_init_from_model: n_ctx_per_seq = 131072\nllama_init_from_model: n_batch       = 2048\nllama_init_from_model: n_ubatch      = 512\nllama_init_from_model: flash_attn    = 1\nllama_init_from_model: freq_base     = 500000.0\nllama_init_from_model: freq_scale    = 1\nllama_kv_cache_init: kv_size = 131072, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\nllama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\nllama_kv_cache_init:      CUDA0 KV buffer size =  5120.00 MiB\nllama_kv_cache_init:        CPU KV buffer size = 11264.00 MiB\nllama_init_from_model: KV self size  = 16384.00 MiB, K (f16): 8192.00 MiB, V (f16): 8192.00 MiB\nllama_init_from_model:        CPU  output buffer size =     0.49 MiB\nllama_init_from_model:      CUDA0 compute buffer size =   920.00 MiB\nllama_init_from_model:  CUDA_Host compute buffer size =   264.01 MiB\nllama_init_from_model: graph nodes  = 903\nllama_init_from_model: graph splits = 247 (with bs=512), 3 (with bs=1)\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 131072\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nsrv          init: initializing slots, n_slots = 1\nslot         init: id  0 | task -1 | new slot n_ctx_slot = 131072\nslot        reset: id  0 | task -1 | \nmain: model loaded\nmain: chat template, chat_template: {% for message in messages %}\n{% if message['role'] == 'user' or message['role'] == 'system' %}\n{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n' + message['content'] + '<|eot_id|>' }}{% elif message['role'] == 'tool' %}\n{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n' + message['content'] + '<|eot_id|>' }}{% else %}\n{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'}}{% if message['content'] is not none %}\n{{ '>>>all\n' + message['content'] }}{% endif %}\n{% if 'tool_calls' in message and message['tool_calls'] is not none %}\n{% for tool_call in message['tool_calls'] %}\n{{ '>>>' + tool_call['function']['name'] + '\n' + tool_call['function']['arguments'] }}{% endfor %}\n{% endif %}\n{{ '<|eot_id|>' }}{% endif %}\n{% endfor %}\n{% if add_generation_prompt %}{{ '<|start_header_id|>{role}<|end_header_id|>\n\n' }}{% endif %}, example_format: '<|start_header_id|>system<|end_header_id|>\n\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nHello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n>>>all\nHi there<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nHow are you?<|eot_id|><|start_header_id|>{role}<|end_header_id|>\n\n'\nmain: server is listening on http://0.0.0.0:8080 - starting the main loop\nque    start_loop: processing new tasks\nque    start_loop: update slots\nsrv  update_slots: all slots are idle\nsrv  kv_cache_cle: clearing KV cache\nque    start_loop: waiting for new tasks\nrequest: {\n\"model\": \"gpt-3.5-turbo\",\n\"tools\": [\n    {\n    \"type\":\"function\",\n    \"function\":{\n        \"name\":\"python\",\n        \"description\":\"Runs code in an ipython interpreter and returns the result of the execution after 60 seconds.\",\n        \"parameters\":{\n        \"type\":\"object\",\n        \"properties\":{\n            \"code\":{\n            \"type\":\"string\",\n            \"description\":\"The code to run in the ipython interpreter.\"\n            }\n        },\n        \"required\":[\"code\"]\n        }\n    }\n    }\n],\n\"messages\": [\n    {\n    \"role\": \"user\",\n    \"content\": \"Hi\"\n    }\n]\n}\nTemplate supports tool calls but does not natively describe tools. The fallback behaviour used may produce bad results, inspect prompt w/ --verbose & consider overriding the template.\nsrv  params_from_: Grammar: char ::= [^\"\\\\\\x7F\\x00-\\x1F] | [\\\\] ([\"\\\\bfnrt] | \"u\" [0-9a-fA-F]{4})\nfirst-tool-call ::= python-call\npython-args ::= \"{\" space python-args-code-kv \"}\" space\npython-args-code-kv ::= \"\\\"code\\\"\" space \":\" space string\npython-call ::= \"python\\n\" python-args\npython-call2 ::= \">>>python\\n\" python-args\nroot ::= first-tool-call space\nspace ::= | \" \" | \"\\n\" [ \\t]{0,20}\nstring ::= \"\\\"\" char* \"\\\"\" space\n\nsrv  params_from_: Grammar lazy: true\nsrv  params_from_: Chat format: Functionary v3.2\nsrv  params_from_: Grammar trigger token: 12958 (`python`)\nsrv  params_from_: Grammar trigger word: `>>>python`\nsrv  add_waiting_: add task 0 to waiting list. current waiting = 0 (before add)\nque          post: new task, id = 0/1, front = 0\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 0\nslot get_availabl: id  0 | task -1 | selected slot by lru, t_last = -1\nslot        reset: id  0 | task -1 | \nslot launch_slot_: id  0 | task 0 | launching slot : {\"id\":0,\"id_task\":0,\"n_ctx\":131072,\"speculative\":false,\"is_processing\":false,\"non_causal\":false,\"params\":{\"n_predict\":-1,\"seed\":4294967295,\"temperature\":0.800000011920929,\"dynatemp_range\":0.0,\"dynatemp_exponent\":1.0,\"top_k\":40,\"top_p\":0.949999988079071,\"min_p\":0.05000000074505806,\"xtc_probability\":0.0,\"xtc_threshold\":0.10000000149011612,\"typical_p\":1.0,\"repeat_last_n\":64,\"repeat_penalty\":1.0,\"presence_penalty\":0.0,\"frequency_penalty\":0.0,\"dry_multiplier\":0.0,\"dry_base\":1.75,\"dry_allowed_length\":2,\"dry_penalty_last_n\":131072,\"dry_sequence_breakers\":[\"\\n\",\":\",\"\\\"\",\"*\"],\"mirostat\":0,\"mirostat_tau\":5.0,\"mirostat_eta\":0.10000000149011612,\"stop\":[],\"max_tokens\":-1,\"n_keep\":0,\"n_discard\":0,\"ignore_eos\":false,\"stream\":false,\"logit_bias\":[],\"n_probs\":0,\"min_keep\":0,\"grammar\":\"char ::= [^\\\"\\\\\\\\\\\\x7F\\\\x00-\\\\x1F] | [\\\\\\\\] ([\\\"\\\\\\\\bfnrt] | \\\"u\\\" [0-9a-fA-F]{4})\\nfirst-tool-call ::= python-call\\npython-args ::= \\\"{\\\" space python-args-code-kv \\\"}\\\" space\\npython-args-code-kv ::= \\\"\\\\\\\"code\\\\\\\"\\\" space \\\":\\\" space string\\npython-call ::= \\\"python\\\\n\\\" python-args\\npython-call2 ::= \\\">>>python\\\\n\\\" python-args\\nroot ::= first-tool-call space\\nspace ::= | \\\" \\\" | \\\"\\\\n\\\" [ \\\\t]{0,20}\\nstring ::= \\\"\\\\\\\"\\\" char* \\\"\\\\\\\"\\\" space\\n\",\"grammar_trigger_words\":[\">>>python\"],\"grammar_trigger_tokens\":[12958],\"preserved_tokens\":[12958],\"chat_format\":\"Functionary v3.2\",\"samplers\":[\"penalties\",\"dry\",\"top_k\",\"typ_p\",\"top_p\",\"min_p\",\"xtc\",\"temperature\"],\"speculative.n_max\":16,\"speculative.n_min\":0,\"speculative.p_min\":0.75,\"timings_per_token\":false,\"post_sampling_probs\":false,\"lora\":[]},\"prompt\":\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou can call any of the following tools to satisfy the user's requests: [\\n  {\\n    \\\"type\\\": \\\"function\\\",\\n    \\\"function\\\": {\\n      \\\"name\\\": \\\"python\\\",\\n      \\\"description\\\": \\\"Runs code in an ipython interpreter and returns the result of the execution after 60 seconds.\\\",\\n      \\\"parameters\\\": {\\n        \\\"type\\\": \\\"object\\\",\\n        \\\"properties\\\": {\\n          \\\"code\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            \\\"description\\\": \\\"The code to run in the ipython interpreter.\\\"\\n          }\\n        },\\n        \\\"required\\\": [\\n          \\\"code\\\"\\n        ]\\n      }\\n    }\\n  }\\n]\\n\\nExample tool call syntax:\\n\\nassistant<|end_header_id|>\\n\\n>>>tool_name\\n{\\\"arg1\\\": \\\"some_value\\\"}<|eot_id|>\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHi<|eot_id|><|start_header_id|>{role}<|end_header_id|>\\n\\n\",\"next_token\":{\"has_next_token\":true,\"has_new_line\":false,\"n_remain\":-1,\"n_decoded\":0,\"stopping_word\":\"\"}}\nslot launch_slot_: id  0 | task 0 | processing task\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 1, front = 0\nslot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 131072, n_keep = 0, n_prompt_tokens = 167\nslot update_slots: id  0 | task 0 | prompt token   0: 128000 '<|begin_of_text|>'\nslot update_slots: id  0 | task 0 | prompt token   1: 128006 '<|start_header_id|>'\nslot update_slots: id  0 | task 0 | prompt token   2:   9125 'system'\nslot update_slots: id  0 | task 0 | prompt token   3: 128007 '<|end_header_id|>'\nslot update_slots: id  0 | task 0 | prompt token   4:    271 '\n\n'\nslot update_slots: id  0 | task 0 | prompt token   5:   2675 'You'\nslot update_slots: id  0 | task 0 | prompt token   6:    649 ' can'\nslot update_slots: id  0 | task 0 | prompt token   7:   1650 ' call'\nslot update_slots: id  0 | task 0 | prompt token   8:    904 ' any'\nslot update_slots: id  0 | task 0 | prompt token   9:    315 ' of'\nslot update_slots: id  0 | task 0 | prompt token  10:    279 ' the'\nslot update_slots: id  0 | task 0 | prompt token  11:   2768 ' following'\nslot update_slots: id  0 | task 0 | prompt token  12:   7526 ' tools'\nslot update_slots: id  0 | task 0 | prompt token  13:    311 ' to'\nslot update_slots: id  0 | task 0 | prompt token  14:  27651 ' satisfy'\nslot update_slots: id  0 | task 0 | prompt token  15:    279 ' the'\nslot update_slots: id  0 | task 0 | kv cache rm [0, end)\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 167, n_tokens = 167, progress = 1.000000\nslot update_slots: id  0 | task 0 | prompt done, n_past = 167, n_tokens = 167\nsrv  update_slots: decoding batch, n_tokens = 167\nGrammar still awaiting trigger after token 78191 (`assistant`)\nslot process_toke: id  0 | task 0 | n_decoded = 1, n_remaining = -1, next token: 78191 'assistant'\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 1\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 2, front = 0\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 131072, n_past = 168, n_cache_tokens = 168, truncated = 0\nsrv  update_slots: decoding batch, n_tokens = 1\nGrammar still awaiting trigger after token 198 (`\n`)\nslot process_toke: id  0 | task 0 | n_decoded = 2, n_remaining = -1, next token:   198 '\n'\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 2\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 3, front = 0\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 131072, n_past = 169, n_cache_tokens = 169, truncated = 0\nsrv  update_slots: decoding batch, n_tokens = 1\nGrammar still awaiting trigger after token 9906 (`Hello`)\nslot process_toke: id  0 | task 0 | n_decoded = 3, n_remaining = -1, next token:  9906 'Hello'\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 3\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 4, front = 0\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 131072, n_past = 170, n_cache_tokens = 170, truncated = 0\nsrv  update_slots: decoding batch, n_tokens = 1\nGrammar still awaiting trigger after token 0 (`!`)\nslot process_toke: id  0 | task 0 | n_decoded = 4, n_remaining = -1, next token:     0 '!'\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 4\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 5, front = 0\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 131072, n_past = 171, n_cache_tokens = 171, truncated = 0\nsrv  update_slots: decoding batch, n_tokens = 1\nGrammar still awaiting trigger after token 2650 (` How`)\nslot process_toke: id  0 | task 0 | n_decoded = 5, n_remaining = -1, next token:  2650 ' How'\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 5\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 6, front = 0\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 131072, n_past = 172, n_cache_tokens = 172, truncated = 0\nsrv  update_slots: decoding batch, n_tokens = 1\nGrammar still awaiting trigger after token 649 (` can`)\nslot process_toke: id  0 | task 0 | n_decoded = 6, n_remaining = -1, next token:   649 ' can'\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 6\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 7, front = 0\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 131072, n_past = 173, n_cache_tokens = 173, truncated = 0\nsrv  update_slots: decoding batch, n_tokens = 1\nGrammar still awaiting trigger after token 358 (` I`)\nslot process_toke: id  0 | task 0 | n_decoded = 7, n_remaining = -1, next token:   358 ' I'\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 7\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 8, front = 0\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 131072, n_past = 174, n_cache_tokens = 174, truncated = 0\nsrv  update_slots: decoding batch, n_tokens = 1\nGrammar still awaiting trigger after token 7945 (` assist`)\nslot process_toke: id  0 | task 0 | n_decoded = 8, n_remaining = -1, next token:  7945 ' assist'\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 8\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 9, front = 0\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 131072, n_past = 175, n_cache_tokens = 175, truncated = 0\nsrv  update_slots: decoding batch, n_tokens = 1\nGrammar still awaiting trigger after token 499 (` you`)\nslot process_toke: id  0 | task 0 | n_decoded = 9, n_remaining = -1, next token:   499 ' you'\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 9\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 10, front = 0\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 131072, n_past = 176, n_cache_tokens = 176, truncated = 0\nsrv  update_slots: decoding batch, n_tokens = 1\nGrammar still awaiting trigger after token 3432 (` today`)\nslot process_toke: id  0 | task 0 | n_decoded = 10, n_remaining = -1, next token:  3432 ' today'\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 10\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 11, front = 0\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 131072, n_past = 177, n_cache_tokens = 177, truncated = 0\nsrv  update_slots: decoding batch, n_tokens = 1\nGrammar still awaiting trigger after token 30 (`?`)\nslot process_toke: id  0 | task 0 | n_decoded = 11, n_remaining = -1, next token:    30 '?'\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 11\nque    start_loop: update slots\nsrv  update_slots: posting NEXT_RESPONSE\nque          post: new task, id = 12, front = 0\nslot update_slots: id  0 | task 0 | slot decode token, n_ctx = 131072, n_past = 178, n_cache_tokens = 178, truncated = 0\nsrv  update_slots: decoding batch, n_tokens = 1\nGrammar still awaiting trigger after token 128009 (`<|eot_id|>`)\nslot process_toke: id  0 | task 0 | stopped by EOS\nslot process_toke: id  0 | task 0 | n_decoded = 12, n_remaining = -1, next token: 128009 ''\nslot      release: id  0 | task 0 | stop processing: n_past = 178, truncated = 0\nslot print_timing: id  0 | task 0 | \nprompt eval time =     352.32 ms /   167 tokens (    2.11 ms per token,   474.00 tokens per second)\n       eval time =     656.33 ms /    12 tokens (   54.69 ms per token,    18.28 tokens per second)\n      total time =    1008.65 ms /   179 tokens\nsrv          send: sending result for task id = 0\nsrv          send: task id = 0 pushed to result queue\nsrv  update_slots: run slots completed\nque    start_loop: waiting for new tasks\nque    start_loop: processing new tasks\nque    start_loop: processing task, id = 12\nque    start_loop: update slots\nsrv  update_slots: all slots are idle\nque    start_loop: waiting for new tasks\nsrv  to_json_oaic: Parsing chat message: assistant\nHello! How can I assist you today?\nFailed to parse functionary v3.2 input: Failed to parse json tool call arguments: assistant\nHello! How can I assist you today?\nsrv  remove_waiti: remove task 0 from waiting list. current waiting = 1 (before remove)\nsrv  log_server_r: request: POST /v1/chat/completions 172.17.0.1 200\nsrv  log_server_r: request:  {\n\"model\": \"gpt-3.5-turbo\",\n\"tools\": [\n    {\n    \"type\":\"function\",\n    \"function\":{\n        \"name\":\"python\",\n        \"description\":\"Runs code in an ipython interpreter and returns the result of the execution after 60 seconds.\",\n        \"parameters\":{\n        \"type\":\"object\",\n        \"properties\":{\n            \"code\":{\n            \"type\":\"string\",\n            \"description\":\"The code to run in the ipython interpreter.\"\n            }\n        },\n        \"required\":[\"code\"]\n        }\n    }\n    }\n],\n\"messages\": [\n    {\n    \"role\": \"user\",\n    \"content\": \"Hi\"\n    }\n]\n}\nsrv  log_server_r: response: {\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"assistant\\nHello! How can I assist you today?\"}}],\"created\":1741214074,\"model\":\"gpt-3.5-turbo\",\"system_fingerprint\":\"b4783-a800ae46\",\"object\":\"chat.completion\",\"usage\":{\"completion_tokens\":12,\"prompt_tokens\":167,\"total_tokens\":179},\"id\":\"chatcmpl-mjWGBGon48tiZPymyQkXF1obPF9DKskh\",\"__verbose\":{\"index\":0,\"content\":\"assistant\\nHello! How can I assist you today?\",\"tokens\":[],\"id_slot\":0,\"stop\":true,\"model\":\"gpt-3.5-turbo\",\"tokens_predicted\":12,\"tokens_evaluated\":167,\"generation_settings\":{\"n_predict\":-1,\"seed\":4294967295,\"temperature\":0.800000011920929,\"dynatemp_range\":0.0,\"dynatemp_exponent\":1.0,\"top_k\":40,\"top_p\":0.949999988079071,\"min_p\":0.05000000074505806,\"xtc_probability\":0.0,\"xtc_threshold\":0.10000000149011612,\"typical_p\":1.0,\"repeat_last_n\":64,\"repeat_penalty\":1.0,\"presence_penalty\":0.0,\"frequency_penalty\":0.0,\"dry_multiplier\":0.0,\"dry_base\":1.75,\"dry_allowed_length\":2,\"dry_penalty_last_n\":131072,\"dry_sequence_breakers\":[\"\\n\",\":\",\"\\\"\",\"*\"],\"mirostat\":0,\"mirostat_tau\":5.0,\"mirostat_eta\":0.10000000149011612,\"stop\":[],\"max_tokens\":-1,\"n_keep\":0,\"n_discard\":0,\"ignore_eos\":false,\"stream\":false,\"logit_bias\":[],\"n_probs\":0,\"min_keep\":0,\"grammar\":\"char ::= [^\\\"\\\\\\\\\\\\x7F\\\\x00-\\\\x1F] | [\\\\\\\\] ([\\\"\\\\\\\\bfnrt] | \\\"u\\\" [0-9a-fA-F]{4})\\nfirst-tool-call ::= python-call\\npython-args ::= \\\"{\\\" space python-args-code-kv \\\"}\\\" space\\npython-args-code-kv ::= \\\"\\\\\\\"code\\\\\\\"\\\" space \\\":\\\" space string\\npython-call ::= \\\"python\\\\n\\\" python-args\\npython-call2 ::= \\\">>>python\\\\n\\\" python-args\\nroot ::= first-tool-call space\\nspace ::= | \\\" \\\" | \\\"\\\\n\\\" [ \\\\t]{0,20}\\nstring ::= \\\"\\\\\\\"\\\" char* \\\"\\\\\\\"\\\" space\\n\",\"grammar_trigger_words\":[\">>>python\"],\"grammar_trigger_tokens\":[12958],\"preserved_tokens\":[12958],\"chat_format\":\"Functionary v3.2\",\"samplers\":[\"penalties\",\"dry\",\"top_k\",\"typ_p\",\"top_p\",\"min_p\",\"xtc\",\"temperature\"],\"speculative.n_max\":16,\"speculative.n_min\":0,\"speculative.p_min\":0.75,\"timings_per_token\":false,\"post_sampling_probs\":false,\"lora\":[]},\"prompt\":\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou can call any of the following tools to satisfy the user's requests: [\\n  {\\n    \\\"type\\\": \\\"function\\\",\\n    \\\"function\\\": {\\n      \\\"name\\\": \\\"python\\\",\\n      \\\"description\\\": \\\"Runs code in an ipython interpreter and returns the result of the execution after 60 seconds.\\\",\\n      \\\"parameters\\\": {\\n        \\\"type\\\": \\\"object\\\",\\n        \\\"properties\\\": {\\n          \\\"code\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            \\\"description\\\": \\\"The code to run in the ipython interpreter.\\\"\\n          }\\n        },\\n        \\\"required\\\": [\\n          \\\"code\\\"\\n        ]\\n      }\\n    }\\n  }\\n]\\n\\nExample tool call syntax:\\n\\nassistant<|end_header_id|>\\n\\n>>>tool_name\\n{\\\"arg1\\\": \\\"some_value\\\"}<|eot_id|>\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHi<|eot_id|><|start_header_id|>{role}<|end_header_id|>\\n\\n\",\"has_new_line\":true,\"truncated\":false,\"stop_type\":\"eos\",\"stopping_word\":\"\",\"tokens_cached\":178,\"timings\":{\"prompt_n\":167,\"prompt_ms\":352.317,\"prompt_per_token_ms\":2.109682634730539,\"prompt_per_second\":474.00494441085726,\"predicted_n\":12,\"predicted_ms\":656.331,\"predicted_per_token_ms\":54.694250000000004,\"predicted_per_second\":18.283457584663836}},\"timings\":{\"prompt_n\":167,\"prompt_ms\":352.317,\"prompt_per_token_ms\":2.109682634730539,\"prompt_per_second\":474.00494441085726,\"predicted_n\":12,\"predicted_ms\":656.331,\"predicted_per_token_ms\":54.694250000000004,\"predicted_per_second\":18.283457584663836}}\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-03-05T22:35:12+00:00",
    "closed_at": "2025-03-06T00:43:56+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12213/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12213"
  },
  {
    "number": 9554,
    "title": "Bug: Fail to compile after commit 202084d31d4247764fc6d6d40d2e2bda0c89a73a",
    "body": "### What happened?\r\n\r\nCompilation fails on CUDA 11 on any version after (and including) commit 202084d31d4247764fc6d6d40d2e2bda0c89a73a, which I've tracked down via git bisect.\r\nIn case it may be useful, this is the output of nvcc --version:\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Thu_Nov_18_09:45:30_PST_2021\r\nCuda compilation tools, release 11.5, V11.5.119\r\nBuild cuda_11.5.r11.5/compiler.30672275_0\r\n```\r\nOperating system is Pop!_OS jammy 22.04 x86_64\r\n\r\nThe build command used is:\r\n```\r\nmake -j GGML_CUDA=1 GGML_CUDA_MMV_Y=2 GGML_DISABLE_LOGS=1 CUDA_DOCKER_ARCH=sm_86\r\n```\r\nfrom a clean directory. Compilation fails independently of me setting GGML_CUDA_MMV_Y=2.\r\n\r\n### Name and Version\r\n\r\nversion: 3694 (202084d)\r\nbuilt with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\r\n\r\n### What operating system are you seeing the problem on?\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -DGGML_CUDA_USE_GRAPHS  examples/deprecation-warning/deprecation-warning.o -o server -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/local/cuda/lib64/stubs -L/usr/lib/wsl/lib\r\nggml/src/ggml-cuda.cu(2444): warning #177-D: function \"set_ggml_graph_node_properties\" was declared but never referenced\r\n\r\nggml/src/ggml-cuda.cu(2456): warning #177-D: function \"ggml_graph_node_has_matching_properties\" was declared but never referenced\r\n\r\nNOTICE: The 'server' binary is deprecated. Please use 'llama-server' instead.\r\nNOTICE: The 'main' binary is deprecated. Please use 'llama-cli' instead.\r\nggml/src/ggml-cuda.cu: In function \u2018bool ggml_backend_cuda_register_host_buffer(void*, size_t)\u2019:\r\nggml/src/ggml-cuda.cu:3089:51: warning: unused parameter \u2018buffer\u2019 [-Wunused-parameter]\r\n 3089 | GGML_CALL bool ggml_backend_cuda_register_host_buffer(void * buffer, size_t size) {\r\n      |                                             ~~~~~~^~~~~~\r\nggml/src/ggml-cuda.cu:3089:66: warning: unused parameter \u2018size\u2019 [-Wunused-parameter]\r\n 3089 | GGML_CALL bool ggml_backend_cuda_register_host_buffer(void * buffer, size_t size) {\r\n      |                                                           ~~~~~~~^~~~\r\n/usr/include/c++/11/bits/std_function.h:435:145: error: parameter packs not expanded with \u2018...\u2019:\r\n  435 |         function(_Functor&& __f)\r\n      |                                                                                                                                                 ^\r\n/usr/include/c++/11/bits/std_function.h:435:145: note:         \u2018_ArgTypes\u2019\r\n/usr/include/c++/11/bits/std_function.h:530:146: error: parameter packs not expanded with \u2018...\u2019:\r\n  530 |         operator=(_Functor&& __f)\r\n      |                                                                                                                                                  ^\r\n/usr/include/c++/11/bits/std_function.h:530:146: note:         \u2018_ArgTypes\u2019\r\nmake: *** [Makefile:738: ggml/src/ggml-cuda/sum.o] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\n```\r\n",
    "labels": [
      "bug-unconfirmed",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-09-19T21:37:02+00:00",
    "closed_at": "2024-09-20T16:35:37+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/9554/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/9554"
  },
  {
    "number": 7799,
    "title": "Bug:  error loading model architecture: unknown model architecture: 'clip'",
    "body": "### What happened?\n\nRunning llava-v1.6 results in the following error:\r\n`error loading model: error loading model architecture: unknown model architecture: 'clip'`\r\n\r\nThe command I ran was:\r\n\r\n`llama --log-enable --model models/llava-v1.6-mistral-7b.Q5_K_M.mmproj-f16.gguf --mmproj models/llava-v1.6-mistral-7b.Q5_K_M.mmproj-f16.gguf --image media/llama0-banner.png -p \"what is in this image?\"`\r\n\r\nI had no issues running ShareGPT4V\r\n` llama --log-enable --model models/ShareGPT4V-f16.gguf --mmproj models/ShareGPT4V-f16-mmproj.gguf --image media/llama0-banner.png -p \"what is in this image?\"`\r\n```\r\ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\r\n\r\n\r\n what is in this image?\r\n What does it show? [end of text]\r\n```\r\n\r\n\n\n### Name and Version\n\nmain: build = 3089 (c90dbe0)\r\nmain: built with gcc (GCC) 12.3.0 for x86_64-unknown-linux-gnu\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n```shell\nLog start\r\nmain: build = 3089 (c90dbe0)\r\nmain: built with gcc (GCC) 12.3.0 for x86_64-unknown-linux-gnu\r\nmain: seed  = 1717675001\r\nllama_model_loader: loaded meta data with 25 key-value pairs and 378 tensors from models/llava-v1.6-mistral-7b.Q5_K_M.mmproj-f16.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = clip\r\nllama_model_loader: - kv   1:                      clip.has_text_encoder bool             = false\r\nllama_model_loader: - kv   2:                    clip.has_vision_encoder bool             = true\r\nllama_model_loader: - kv   3:                   clip.has_llava_projector bool             = true\r\nllama_model_loader: - kv   4:                          general.file_type u32              = 1\r\nllama_model_loader: - kv   5:                               general.name str              = vit-large336-custom\r\nllama_model_loader: - kv   6:                        general.description str              = image encoder for LLaVA\r\nllama_model_loader: - kv   7:                        clip.projector_type str              = mlp\r\nllama_model_loader: - kv   8:                     clip.vision.image_size u32              = 336\r\nllama_model_loader: - kv   9:                     clip.vision.patch_size u32              = 14\r\nllama_model_loader: - kv  10:               clip.vision.embedding_length u32              = 1024\r\nllama_model_loader: - kv  11:            clip.vision.feed_forward_length u32              = 4096\r\nllama_model_loader: - kv  12:                 clip.vision.projection_dim u32              = 768\r\nllama_model_loader: - kv  13:           clip.vision.attention.head_count u32              = 16\r\nllama_model_loader: - kv  14:   clip.vision.attention.layer_norm_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  15:                    clip.vision.block_count u32              = 23\r\nllama_model_loader: - kv  16:           clip.vision.image_grid_pinpoints arr[i32,10]      = [336, 672, 672, 336, 672, 672, 1008, ...\r\nllama_model_loader: - kv  17:          clip.vision.image_crop_resolution u32              = 224\r\nllama_model_loader: - kv  18:             clip.vision.image_aspect_ratio str              = anyres\r\nllama_model_loader: - kv  19:         clip.vision.image_split_resolution u32              = 224\r\nllama_model_loader: - kv  20:            clip.vision.mm_patch_merge_type str              = spatial_unpad\r\nllama_model_loader: - kv  21:              clip.vision.mm_projector_type str              = mlp2x_gelu\r\nllama_model_loader: - kv  22:                     clip.vision.image_mean arr[f32,3]       = [0.481455, 0.457828, 0.408211]\r\nllama_model_loader: - kv  23:                      clip.vision.image_std arr[f32,3]       = [0.268630, 0.261303, 0.275777]\r\nllama_model_loader: - kv  24:                              clip.use_gelu bool             = false\r\nllama_model_loader: - type  f32:  236 tensors\r\nllama_model_loader: - type  f16:  142 tensors\r\nllama_model_load: error loading model: error loading model architecture: unknown model architecture: 'clip'\r\nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params: error: failed to load model 'models/llava-v1.6-mistral-7b.Q5_K_M.mmproj-f16.gguf'\r\nmain: error: unable to load model\n```\n",
    "labels": [
      "bug-unconfirmed",
      "critical severity"
    ],
    "state": "closed",
    "created_at": "2024-06-06T12:02:26+00:00",
    "closed_at": "2024-06-07T03:00:50+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7799/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7799"
  },
  {
    "number": 10351,
    "title": "Bug: rwkv and mamba models cannot be used with `-ngl 0` after CPU backend refactor",
    "body": "### What happened?\n\n```\r\n$ ./build/bin/llama-bench -m ~/Downloads/mamba-2.8b-q4_0.gguf -ngl 0\r\n| model                          |       size |     params | backend    | threads |          test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |\r\n/Users/molly/llama.cpp/ggml/src/ggml-backend.cpp:745: pre-allocated tensor in a backend that cannot run the operation\r\n[1]    13345 abort      ./build/bin/llama-bench -m ~/Downloads/mamba-2.8b-q4_0.gguf -ngl 0\r\n```\r\n```\r\n$ ./build/bin/llama-bench -m /Volumes/grouped/Models/rwkv/v6-Finch-7B-HF/v6-Finch-7B-HF-Q4_0.gguf -ngl 0\r\n| model                          |       size |     params | backend    | threads |          test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |\r\n/Users/molly/llama.cpp/ggml/src/ggml-backend.cpp:745: pre-allocated tensor in a backend that cannot run the operation\r\n[1]    16003 abort      ./build/bin/llama-bench -m  -ngl 0\r\n```\r\nUsing lldb to trace the error, it fails in `ggml_backend_sched_backend_id_from_cur`\r\n```\r\n    if (tensor->buffer || (tensor->view_src && tensor->view_src->buffer)) {\r\n        // since the tensor is pre-allocated, it cannot be moved to another backend\r\n        GGML_ABORT(\"pre-allocated tensor in a backend that cannot run the operation\");\r\n    }\r\n```\r\n, where the tensor triggering this fault was a view of `cache_k_l0`. This makes sense, as both mamba and rwkv do a GGML_VIEW/GGML_RESHAPE on the k cache when building the graph.\r\n\r\nCC @compilade \n\n### Name and Version\n\nNon-working version:\r\n$ ./build/bin/llama-cli -v\r\nbuild: 4098 (772703c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0\r\n\r\n\r\nKnown working version:\r\n$ ./build/bin/llama-cli -v\r\nbuild: 4079 (4a8ccb37) with Apple clang version 15.0.0 (clang-1500.3.9.4) for arm64-apple-darwin23.6.0\n\n### What operating system are you seeing the problem on?\n\nLinux, Mac\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug",
      "medium severity"
    ],
    "state": "closed",
    "created_at": "2024-11-17T02:47:58+00:00",
    "closed_at": "2024-11-17T11:25:46+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10351/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10351"
  },
  {
    "number": 5909,
    "title": "Llama.cpp ./main hangs on line \"llama_new_context_with_model: graph splits (measure): 1\"",
    "body": "Llama version b2354\r\n\r\nI am trying to inference on a HPC cluster instance with one A100 connected to it. I have compiled the binary with `-DLLAMA_CUBLAS=ON`. \r\nHowever, every time i try to run llama.cpp, the program hangs on `llama_new_context_with_model: graph splits (measure): 1`\r\n\r\nHere is the full code output:\r\n```Log start\r\nmain: build = 2354 (e25fb4b1)\r\nmain: built with cc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-10) for x86_64-redhat-linux\r\nmain: seed  = 1709748105\r\nggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\r\nggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\r\nggml_init_cublas: found 1 CUDA devices:\r\n  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes\r\nllama_model_loader: loaded meta data with 23 key-value pairs and 164 tensors from /scratch/wtosbor/gemma.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = gemma\r\nllama_model_loader: - kv   1:                               general.name str              = gemma-2b\r\nllama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\r\nllama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048\r\nllama_model_loader: - kv   4:                          gemma.block_count u32              = 18\r\nllama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\r\nllama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\r\nllama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\r\nllama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\r\nllama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 2\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 1\r\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 3\r\nllama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  21:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  22:                          general.file_type u32              = 11\r\nllama_model_loader: - type  f32:   37 tensors\r\nllama_model_loader: - type q3_K:  127 tensors\r\nllm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = gemma\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 256000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 8192\r\nllm_load_print_meta: n_embd           = 2048\r\nllm_load_print_meta: n_head           = 8\r\nllm_load_print_meta: n_head_kv        = 1\r\nllm_load_print_meta: n_layer          = 18\r\nllm_load_print_meta: n_rot            = 256\r\nllm_load_print_meta: n_embd_head_k    = 256\r\nllm_load_print_meta: n_embd_head_v    = 256\r\nllm_load_print_meta: n_gqa            = 8\r\nllm_load_print_meta: n_embd_k_gqa     = 256\r\nllm_load_print_meta: n_embd_v_gqa     = 256\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 16384\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 8192\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: model type       = 2B\r\nllm_load_print_meta: model ftype      = Q3_K - Small\r\nllm_load_print_meta: model params     = 2.51 B\r\nllm_load_print_meta: model size       = 1.00 GiB (3.44 BPW) \r\nllm_load_print_meta: general.name     = gemma-2b\r\nllm_load_print_meta: BOS token        = 2 '<bos>'\r\nllm_load_print_meta: EOS token        = 1 '<eos>'\r\nllm_load_print_meta: UNK token        = 3 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<pad>'\r\nllm_load_print_meta: LF token         = 227 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.06 MiB\r\nllm_load_tensors: offloading 0 repeating layers to GPU\r\nllm_load_tensors: offloaded 0/19 layers to GPU\r\nllm_load_tensors:        CPU buffer size =  1027.24 MiB\r\n.............................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:  CUDA_Host KV buffer size =     9.00 MiB\r\nllama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\r\nllama_new_context_with_model:  CUDA_Host input buffer size   =     6.01 MiB\r\nllama_new_context_with_model:  CUDA_Host compute buffer size =   504.00 MiB\r\nllama_new_context_with_model: graph splits (measure): 1\r\n```\r\n\r\nI have tried different model and different command-line options to no avail. Is there anything that I'm missing here? Thanks.",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-03-06T18:12:46+00:00",
    "closed_at": "2024-03-06T21:40:07+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5909/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5909"
  },
  {
    "number": 4817,
    "title": "CPU load balancing is far worse for Mixtral / MoE inference vs dense model",
    "body": "If I set 6 threads (both BLAS and regular) for Nous Hermes 34b [4_K_M], which is a dense model, I seem to get optimal prompt evaluation speeds compared to 10 or 4 threads, and the CPU load balancing looks like:\r\n![image](https://github.com/ggerganov/llama.cpp/assets/66376113/1674b54d-4f56-4bc7-8054-fb791730567a)\r\n\r\nThis is equivalent to the amount of performance cores I have on this processor, so this seems to make sense. I tested 4 and 6 threads and they were both worse. However:\r\n\r\n<img width=\"527\" alt=\"image\" src=\"https://github.com/ggerganov/llama.cpp/assets/66376113/5b8a34ef-b1dd-4b22-b60d-e4fc53b3f040\">\r\n\r\nThe load balancing is significantly less even for the batching in Sparse MoE, so overall utilization suffers [even though this is pure CPU inference] on OpenBLAS. [~60% average utilization]\r\n\r\nTurning up the BLAS thread count doesn't help either; paradoxically, the net utilization seems to be _worse_ if you turn up the thread count, just like it was for a dense model.\r\n\r\nIf I run pure CPU inference for Llama 13b 4_K_S, the prompt processing is actually _faster_ by a small margin (~80ms per token instead of ~100ms per token) despite 13b having a fatter kv cache, due to a lack of GQA when compared to the other two models. In task manager, it reports evenly balanced thread usage just like the 34b.\r\n\r\nMy current theory for why this happens is because, to my understanding, the calculations for all layers are grouped per-expert during prompt eval, and this is done sequentially. The overhead of splitting the work per-expert might be what's causing this, so it might be best to inference all layers at once rather than grouping them discretely.\r\n\r\n@slaren is this idea what you were referring to when you talked about \"grouping by processing all the experts in a single `ggml_mul_mat_id`?\" Or would it maybe be better to statically allocate certain threads to certain experts to mitigate this effect?",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-01-08T01:53:07+00:00",
    "closed_at": "2024-01-08T05:31:40+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/4817/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/4817"
  },
  {
    "number": 137,
    "title": "FP16 and 4-bit quantized model both produce garbage output on M1 8GB",
    "body": "Both the `ggml-model-q4_0` and `ggml-model-f16` produce a garbage output on my M1 Air 8GB, using the 7B LLaMA model. I've seen the quantized model having problems but I doubt the quantization is the issue as the non-quantized model produces the same output.\r\n\r\n```\r\n\u279c  llama.cpp git:(master) ./main -m ./models/7B/ggml-model-f16.bin -p \"Building a website can be done in 10 simple steps:\" -t 8 -n 512\r\nmain: seed = 1678812348\r\nllama_model_load: loading model from './models/7B/ggml-model-f16.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 4096\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 32\r\nllama_model_load: n_layer = 32\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 1\r\nllama_model_load: n_ff    = 11008\r\nllama_model_load: n_parts = 1\r\nllama_model_load: ggml ctx size = 13365.09 MB\r\nllama_model_load: memory_size =   512.00 MB, n_mem = 16384\r\nllama_model_load: loading model part 1/1 from './models/7B/ggml-model-f16.bin'\r\nllama_model_load: ........... done\r\nllama_model_load: model size =  4274.30 MB / num tensors = 90\r\n\r\nsystem_info: n_threads = 8 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |\r\n\r\nmain: prompt: 'Building a website can be done in 10 simple steps:'\r\nmain: number of tokens in prompt = 15\r\n     1 -> ''\r\n  8893 -> 'Build'\r\n   292 -> 'ing'\r\n   263 -> ' a'\r\n  4700 -> ' website'\r\n   508 -> ' can'\r\n   367 -> ' be'\r\n  2309 -> ' done'\r\n   297 -> ' in'\r\n 29871 -> ' '\r\n 29896 -> '1'\r\n 29900 -> '0'\r\n  2560 -> ' simple'\r\n  6576 -> ' steps'\r\n 29901 -> ':'\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\nBuilding a website can be done in 10 simple steps:Administrationistrunkoveryabasepair tou cross deprecatedinition holes prvindor^C\r\n```",
    "labels": [
      "hardware"
    ],
    "state": "closed",
    "created_at": "2023-03-14T17:05:51+00:00",
    "closed_at": "2023-03-14T20:54:06+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/137/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/137"
  },
  {
    "number": 2743,
    "title": "tokenizer is converting spaces to \u2581 (U+2581)",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nDecoding the token 1678 to three spaces.\r\n\r\n# Current Behavior\r\n\r\nToken 1678 gets decoded in to \" \u2581\u2581\" (U+0020 U+2581 U+2581). Other tokens with spaces also have the issue.\r\n\r\n# Environment and Context\r\n\r\nRun main like this\r\n```\r\n$ ./main -m models/llama-2-13b.q6_K.gguf -n 1 -p '   three spaces\r\n    three spaces after newline'\r\n```\r\nand it will print out\r\n```\r\n \u2581\u2581 three spaces\r\n \u2581 three spaces after newline\r\n```\r\nwhere the first line has two U+2581 and the second line has one U+2581.\r\n\r\nEdit: the output with `--verbose-prompt`\r\n```\r\n     1 -> ''\r\n  1678 -> ' \u2581\u2581'\r\n  2211 -> ' three'\r\n  8162 -> ' spaces'\r\n    13 -> '\r\n'\r\n   259 -> ' \u2581'\r\n  2211 -> ' three'\r\n  8162 -> ' spaces'\r\n  1156 -> ' after'\r\n 25899 -> ' newline'\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-08-23T14:06:36+00:00",
    "closed_at": "2023-08-24T09:26:02+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/2743/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/2743"
  },
  {
    "number": 37,
    "title": "can't compile main",
    "body": "I\u2019m trying to compile main to play around with it and failing with error:\r\n\r\n```\r\nld: symbol(s) not found for architecture arm64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n\r\non macOS M1\r\n\r\ntrying to compile by running  `g++ main.cpp -o main -v -std=c++11`\r\n\r\nanyone know what I'm missing?",
    "labels": [
      "build"
    ],
    "state": "closed",
    "created_at": "2023-03-12T06:17:06+00:00",
    "closed_at": "2023-03-12T08:17:07+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/37/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/37"
  },
  {
    "number": 12588,
    "title": "Eval bug: T5Encoder support broken",
    "body": "### Name and Version\n\nllama-cli --version\nversion: 4940 (fac63a3d)\nbuilt with Apple clang version 16.0.0 (clang-1600.0.26.6) for arm64-apple-darwin24.2.0\n\n### Operating systems\n\nMac\n\n### GGML backends\n\nMetal\n\n### Hardware\n\nMac mini M4\n\n### Models\n\nT5Encoder\n\n### Problem description & steps to reproduce\n\nRelated to: https://github.com/HighDoping/Wan2.1/issues/2\nT5Encoder support is broken after a recent code refactoring. \nThe model support is commented out at https://github.com/ggml-org/llama.cpp/blob/5ed38b6852bd509d56acfdae54bceec8ab3cc396/src/llama-model.cpp#L11849-L11852\n\n### First Bad Commit\n\nhttps://github.com/ggml-org/llama.cpp/commit/e0dbec0bc6cd4b6230cda7a6ed1e9dac08d1600b\n\n### Relevant log output\n\n```shell\nbuild: 4940 (fac63a3d) with Apple clang version 16.0.0 (clang-1600.0.26.6) for arm64-apple-darwin24.2.0\nllama_model_load_from_file_impl: using device Metal (Apple M4) - 21845 MiB free\nllama_model_loader: loaded meta data with 32 key-value pairs and 242 tensors from ../Wan2.1-T2V-1.3B/umt5-xxl-encode-only-Q4_K_M_kai.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = t5encoder\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Umt5 Xxl Encode Only\nllama_model_loader: - kv   3:                           general.finetune str              = encode-only\nllama_model_loader: - kv   4:                           general.basename str              = umt5\nllama_model_loader: - kv   5:                         general.size_label str              = xxl\nllama_model_loader: - kv   6:                          general.file_type u32              = 15\nllama_model_loader: - kv   7:                       tokenizer.ggml.model str              = t5\nllama_model_loader: - kv   8:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv   9:                      tokenizer.ggml.tokens arr[str,256384]  = [\"<pad>\", \"</s>\", \"<s>\", \"<unk>\", \"[e...\nllama_model_loader: - kv  10:                      tokenizer.ggml.scores arr[f32,256384]  = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  11:                  tokenizer.ggml.token_type arr[i32,256384]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  12:            tokenizer.ggml.add_space_prefix bool             = true\nllama_model_loader: - kv  13:    tokenizer.ggml.remove_extra_whitespaces bool             = false\nllama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = true\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\nllama_model_loader: - kv  21:                   t5encoder.context_length u32              = 512\nllama_model_loader: - kv  22:                 t5encoder.embedding_length u32              = 4096\nllama_model_loader: - kv  23:              t5encoder.feed_forward_length u32              = 10240\nllama_model_loader: - kv  24:                      t5encoder.block_count u32              = 24\nllama_model_loader: - kv  25:             t5encoder.attention.head_count u32              = 64\nllama_model_loader: - kv  26:             t5encoder.attention.key_length u32              = 64\nllama_model_loader: - kv  27:           t5encoder.attention.value_length u32              = 64\nllama_model_loader: - kv  28:     t5encoder.attention.layer_norm_epsilon f32              = 0.000001\nllama_model_loader: - kv  29: t5encoder.attention.relative_buckets_count u32              = 32\nllama_model_loader: - kv  30: t5encoder.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  31:                              dummy.padding arr[u8,23]       = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nllama_model_loader: - type  f32:   49 tensors\nllama_model_loader: - type  f16:   24 tensors\nllama_model_loader: - type q4_K:  144 tensors\nllama_model_loader: - type q6_K:   25 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 3.40 GiB (5.14 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 4\nload: token to piece cache size = 1.8697 MB\nprint_info: arch             = t5encoder\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 512\nprint_info: n_embd           = 4096\nprint_info: n_layer          = 24\nprint_info: n_head           = 64\nprint_info: n_head_kv        = 64\nprint_info: n_rot            = 64\nprint_info: n_swa            = 0\nprint_info: n_swa_pattern    = 1\nprint_info: n_embd_head_k    = 64\nprint_info: n_embd_head_v    = 64\nprint_info: n_gqa            = 1\nprint_info: n_embd_k_gqa     = 4096\nprint_info: n_embd_v_gqa     = 4096\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = -1\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 512\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = ?B\nprint_info: model params     = 5.68 B\nprint_info: general.name     = Umt5 Xxl Encode Only\nprint_info: vocab type       = UGM\nprint_info: n_vocab          = 256384\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<s>'\nprint_info: EOS token        = 1 '</s>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 273 '\u2581'\nprint_info: EOG token        = 1 '</s>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nmake_cpu_buft_list: disabling extra buffer types (i.e. repacking) since a GPU device is available\nload_tensors: offloading 24 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 25/25 layers to GPU\nload_tensors: Metal_Mapped model buffer size =  3479.66 MiB\nload_tensors:   CPU_Mapped model buffer size =   821.54 MiB\n...............................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 512\nllama_context: n_ctx_per_seq = 512\nllama_context: n_batch       = 512\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: freq_base     = 10000.0\nllama_context: freq_scale    = 1\nggml_metal_init: allocating\nggml_metal_init: found device: Apple M4\nggml_metal_init: picking default device: Apple M4\nggml_metal_load_library: using embedded metal library\nggml_metal_init: GPU name:   Apple M4\nggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\nggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\nggml_metal_init: simdgroup reduction   = true\nggml_metal_init: simdgroup matrix mul. = true\nggml_metal_init: has residency sets    = true\nggml_metal_init: has bfloat            = true\nggml_metal_init: use bfloat            = false\nggml_metal_init: hasUnifiedMemory      = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\nggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\nggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\nggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\nggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\nggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\nggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\nggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\nllama_context:        CPU  output buffer size =     0.02 MiB\ninit: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1\ninit:      Metal KV buffer size =   192.00 MiB\nllama_context: KV self size  =  192.00 MiB, K (f16):   96.00 MiB, V (f16):   96.00 MiB\n/tmp/llama.cpp-20250322-5336-mb6vx4/src/llama-model.cpp:11885: fatal error\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-03-26T11:30:09+00:00",
    "closed_at": "2025-03-27T10:43:34+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/12588/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/12588"
  },
  {
    "number": 789,
    "title": "[User] Insert summary of your issue or enhancement..",
    "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [x] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Expected Behavior\r\n\r\nPlease provide a detailed written description of what you were trying to do, and what you expected `llama.cpp` to do.\r\n\r\n# Current Behavior\r\n\r\nPlease provide a detailed written description of what `llama.cpp` did, instead. \r\n\r\n# Environment and Context \r\n\r\nPlease provide detailed information about your computer setup. This is important in case the issue is not reproducible except for under certain specific conditions.\r\n\r\n* Physical (or virtual) hardware you are using, e.g. for Linux:\r\n\r\n`$ lscpu`\r\n\r\n* Operating System, e.g. for Linux:\r\n\r\n`$ uname -a`\r\n\r\n* SDK version, e.g. for Linux:\r\n\r\n```\r\n$ python3 --version\r\n$ make --version\r\n$ g++ --version\r\n```\r\n\r\n# Failure Information (for bugs)\r\n\r\nPlease help provide information about the failure if this is a bug. If it is not a bug, please remove the rest of this template.\r\n\r\n# Steps to Reproduce\r\n\r\nPlease provide detailed steps for reproducing the issue. We are not sitting in front of your screen, so the more detail the better.\r\n\r\n1. step 1\r\n2. step 2\r\n3. step 3\r\n4. etc.\r\n\r\n# Failure Logs\r\n\r\nPlease include any relevant log snippets or files. If it works under one configuration but not under another, please provide logs for both configurations and their corresponding outputs so it is easy to see where behavior changes.\r\n\r\nAlso, please try to **avoid using screenshots** if at all possible. Instead, copy/paste the console output and use [Github's markdown](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax) to cleanly format your logs for easy readability.\r\n\r\nExample environment info:\r\n```\r\nllama.cpp$ git log | head -1\r\ncommit 2af23d30434a677c6416812eea52ccc0af65119c\r\n\r\nllama.cpp$ lscpu | egrep \"AMD|Flags\"\r\nVendor ID:                       AuthenticAMD\r\nModel name:                      AMD Ryzen Threadripper 1950X 16-Core Processor\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sme sev\r\nVirtualization:                  AMD-V\r\n\r\nllama.cpp$ python3 --version\r\nPython 3.10.9\r\n\r\nllama.cpp$ pip list | egrep \"torch|numpy|sentencepiece\"\r\nnumpy                         1.24.2\r\nnumpydoc                      1.5.0\r\nsentencepiece                 0.1.97\r\ntorch                         1.13.1\r\ntorchvision                   0.14.1\r\n\r\nllama.cpp$ make --version | head -1\r\nGNU Make 4.3\r\n\r\n$ md5sum ./models/65B/ggml-model-q4_0.bin\r\ndbdd682cce80e2d6e93cefc7449df487  ./models/65B/ggml-model-q4_0.bin\r\n```\r\n\r\nExample run with the Linux command [perf](https://www.brendangregg.com/perf.html)\r\n```\r\nllama.cpp$ perf stat ./main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p \"Please close your issue when it has been answered.\"\r\nmain: seed = 1679149377\r\nllama_model_load: loading model from './models/65B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 8192\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 64\r\nllama_model_load: n_layer = 80\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 22016\r\nllama_model_load: n_parts = 8\r\nllama_model_load: ggml ctx size = 41477.73 MB\r\nllama_model_load: memory_size =  2560.00 MB, n_mem = 40960\r\nllama_model_load: loading model part 1/8 from './models/65B/ggml-model-q4_0.bin'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 2/8 from './models/65B/ggml-model-q4_0.bin.1'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 3/8 from './models/65B/ggml-model-q4_0.bin.2'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 4/8 from './models/65B/ggml-model-q4_0.bin.3'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 5/8 from './models/65B/ggml-model-q4_0.bin.4'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 6/8 from './models/65B/ggml-model-q4_0.bin.5'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 7/8 from './models/65B/ggml-model-q4_0.bin.6'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 8/8 from './models/65B/ggml-model-q4_0.bin.7'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\n\r\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \r\n\r\nmain: prompt: 'Please close your issue when it has been answered.'\r\nmain: number of tokens in prompt = 11\r\n     1 -> ''\r\n 12148 -> 'Please'\r\n  3802 -> ' close'\r\n   596 -> ' your'\r\n  2228 -> ' issue'\r\n   746 -> ' when'\r\n   372 -> ' it'\r\n   756 -> ' has'\r\n  1063 -> ' been'\r\n  7699 -> ' answered'\r\n 29889 -> '.'\r\n\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\r\nPlease close your issue when it has been answered.\r\n@duncan-donut: I'm trying to figure out what kind of \"support\" you need for this script and why, exactly? Is there a question about how the code works that hasn't already been addressed in one or more comments below this ticket, or are we talking something else entirely like some sorta bugfixing job because your server setup is different from mine??\r\nI can understand if your site needs to be running smoothly and you need help with a fix of sorts but there should really be nothing wrong here that the code itself could not handle. And given that I'm getting reports about how it works perfectly well on some other servers, what exactly are we talking? A detailed report will do wonders in helping us get this resolved for ya quickly so please take your time and describe the issue(s) you see as clearly & concisely as possible!!\r\n@duncan-donut: I'm not sure if you have access to cPanel but you could try these instructions. It is worth a shot! Let me know how it goes (or what error message, exactly!) when/if ya give that code a go? [end of text]\r\n\r\n\r\nmain: mem per token = 71159620 bytes\r\nmain:     load time = 19309.95 ms\r\nmain:   sample time =   168.62 ms\r\nmain:  predict time = 223895.61 ms / 888.47 ms per token\r\nmain:    total time = 246406.42 ms\r\n\r\n Performance counter stats for './main -m ./models/65B/ggml-model-q4_0.bin -t 16 -n 1024 -p Please close your issue when it has been answered.':\r\n\r\n        3636882.89 msec task-clock                #   14.677 CPUs utilized          \r\n             13509      context-switches          #    3.714 /sec                   \r\n              2436      cpu-migrations            #    0.670 /sec                   \r\n          10476679      page-faults               #    2.881 K/sec                  \r\n    13133115082869      cycles                    #    3.611 GHz                      (16.77%)\r\n       29314462753      stalled-cycles-frontend   #    0.22% frontend cycles idle     (16.76%)\r\n    10294402631459      stalled-cycles-backend    #   78.39% backend cycles idle      (16.74%)\r\n    23479217109614      instructions              #    1.79  insn per cycle         \r\n                                                  #    0.44  stalled cycles per insn  (16.76%)\r\n     2353072268027      branches                  #  647.002 M/sec                    (16.77%)\r\n        1998682780      branch-misses             #    0.08% of all branches          (16.76%)\r\n\r\n     247.802177522 seconds time elapsed\r\n\r\n    3618.573072000 seconds user\r\n      18.491698000 seconds sys\r\n```\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-05T19:59:06+00:00",
    "closed_at": "2023-04-06T00:29:02+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/789/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/789"
  },
  {
    "number": 6194,
    "title": "make -j LLAMA_CUBLAS=1 LLAMA_CUDA_F16=1 failed",
    "body": "make -j LLAMA_CUBLAS=1 LLAMA_CUDA_F16=1 failed to compile. Remove LLAMA_CUDA_F16=1 will fix.\r\n\r\n```\r\nggml-cuda.cu(9456): error: identifier \"cuda_pool_alloc\" is undefined\r\n      cuda_pool_alloc<half> src1_dfloat_a;\r\n      ^\r\n\r\nggml-cuda.cu(9456): error: type name is not allowed\r\n      cuda_pool_alloc<half> src1_dfloat_a;\r\n                      ^\r\n\r\nggml-cuda.cu(9456): error: identifier \"src1_dfloat_a\" is undefined\r\n      cuda_pool_alloc<half> src1_dfloat_a;\r\n```\r\n\r\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-03-21T09:02:39+00:00",
    "closed_at": "2024-03-21T12:59:54+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/6194/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/6194"
  },
  {
    "number": 10955,
    "title": "Compile bug: error: passing 'const ggml_fp16_t *' (aka 'const unsigned short *') to parameter of type 'ggml_fp16_t *' (aka 'unsigned short *') discards qualifiers [-Werror,-Wincompatible-pointer-types-discards-qualifiers]",
    "body": "### Git commit\n\n4381\n\n### Operating systems\n\nBSD\n\n### GGML backends\n\nCPU\n\n### Problem description & steps to reproduce\n\nBuild fails.\r\n\r\nTriggered by: ci/run.sh\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\n/usr/ports/misc/llama-cpp/work/llama.cpp-b4381/ggml/src/ggml-cpu/ggml-cpu.c:1602:39: error: passing 'const ggml_fp16_t *' (aka 'const unsigned short *') to parameter of type 'ggml_fp16_t *' (aka 'unsigned short *') discards qualifiers [-Werror,-Wincompatible-pointer-types-discards-qualifiers]\r\n 1602 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);\r\n      |                     ~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/ports/misc/llama-cpp/work/llama.cpp-b4381/ggml/src/ggml-cpu/ggml-cpu.c:1024:55: note: expanded from macro 'GGML_F16_VEC_LOAD'\r\n 1024 | #define GGML_F16_VEC_LOAD(p, i)      GGML_F32Cx4_LOAD(p)\r\n      |                                      ~~~~~~~~~~~~~~~~~^~\r\n/usr/ports/misc/llama-cpp/work/llama.cpp-b4381/ggml/src/ggml-cpu/ggml-cpu.c:1014:50: note: expanded from macro 'GGML_F32Cx4_LOAD'\r\n 1014 | #define GGML_F32Cx4_LOAD(x)     __sse_f16x4_load(x)\r\n      |                                                  ^\r\n/usr/ports/misc/llama-cpp/work/llama.cpp-b4381/ggml/src/ggml-cpu/ggml-cpu.c:989:52: note: passing argument to parameter 'x' here\r\n  989 | static inline __m128 __sse_f16x4_load(ggml_fp16_t *x) {\r\n      |                                                    ^\r\n2 errors generated.\r\n```\n```\n",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2024-12-23T09:46:26+00:00",
    "closed_at": "2024-12-23T19:25:53+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10955/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/10955"
  },
  {
    "number": 5737,
    "title": "Smarter slot handling",
    "body": "The current system of available slots with -np is frustrating in terms of how it forces one to only allow queries of greatly reduced max token count.  For example, if you have a context length of 16k, if you want four slots, each will only be 4k, and you can no longer run any 16k queries at all without them being heavily truncated.\r\n\r\nWhile a partial solution would be to allow the operator to specify the numbers of token in each slot so that they could at least leave one high-token-count slot, an ideal solution would be to have the server be adaptive - to look at what's in the queue, and using a combination of how long each query has been waiting and how well different queries could be packed into the max context length, determine which to run and how many slots to use of what size.\r\n\r\nWhile I wouldn't be an ideal person to write the slot-handling side of things, I'd be more than happy to write the queueing mechanism for you if this were of interest.  I would just need to know what sort of data structure you could provide for the queue and what limitations there would be on slots (including any performance considerations)",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-26T16:42:18+00:00",
    "closed_at": "2024-02-27T15:17:39+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5737/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/5737"
  },
  {
    "number": 1577,
    "title": "Builds after May 10 (master-cf348a6) not working at all",
    "body": "I am running this command: `main.exe -m ./models/ggml-alpaca-7b-q4.bin --color -f ./prompts/alpaca.txt --ctx_size 2048 -n -1 -ins -b 256 --top_k 10000 --temp 0.2 --repeat_penalty 1.1 -t 7`\r\n\r\nIn version master-cf348a6, it successfully loads the model.\r\n\r\nIn version master-7d87381, I see this on the console:\r\n```\r\nmain: build = 585 (7d87381)\r\nmain: seed  = 1684877783\r\nllama.cpp: loading model from ./models/ggml-alpaca-7b-q4.bin\r\n```\r\nThen the program abruptly exists.  If I run it in a debugger, I see that it encountered a \"CPP-EH-EXCEPTION\".\r\n\r\n# Environment and Context\r\n\r\nWindows 11\r\n16GB of RAM\r\nAMD Ryzen 7 5800H\r\n\r\n\r\nEdit:\r\n\r\nDies here:\r\n```\r\n    void read_raw(void * ptr, size_t len) const {\r\n        if (len == 0) {\r\n            return;\r\n        }\r\n        errno = 0;\r\n        std::size_t ret = std::fread(ptr, len, 1, fp);\r\n        if (ferror(fp)) {\r\n            throw std::runtime_error(format(\"read error: %s\", strerror(errno)));\r\n        }\r\n        if (ret != 1) {\r\n            throw std::runtime_error(std::string(\"unexpectedly reached end of file\"));   //<--- dies here\r\n        }\r\n    }\r\n```\r\n\r\nStack trace:\r\n```\r\n>\tmain.exe!llama_file::read_raw(void * ptr, unsigned __int64 len) Line 114\tC++\r\n \tmain.exe!llama_file::read_string(unsigned int len) Line 127\tC++\r\n \tmain.exe!llama_file_loader::read_tensor_metadata(unsigned __int64 file_idx, llama_load_tensors_map & tensors_map) Line 493\tC++\r\n \tmain.exe!llama_file_loader::llama_file_loader(const char * fname, unsigned __int64 file_idx, llama_load_tensors_map & tensors_map) Line 428\tC++\r\n \tmain.exe!llama_model_loader::llama_model_loader(const std::string & fname_base, bool use_mmap, bool vocab_only) Line 603\tC++\r\n \tmain.exe!llama_model_load_internal(const std::string & fname, llama_context & lctx, int n_ctx, int n_gpu_layers, ggml_type memory_type, bool use_mmap, bool use_mlock, bool vocab_only, void(*)(float, void *) progress_callback, void * progress_callback_user_data) Line 924\tC++\r\n \tmain.exe!llama_model_load(const std::string & fname, llama_context & lctx, int n_ctx, int n_gpu_layers, ggml_type memory_type, bool use_mmap, bool use_mlock, bool vocab_only, void(*)(float, void *) progress_callback, void * progress_callback_user_data) Line 1183\tC++\r\n \tmain.exe!llama_init_from_file(const char * path_model, llama_context_params params) Line 2247\tC++\r\n \tmain.exe!llama_init_from_gpt_params(const gpt_params & params) Line 477\tC++\r\n \tmain.exe!main(int argc, char * * argv) Line 105\tC++\r\n```\r\n\r\nIf this is due to the model file format changing, I am still getting a byte-identical file when I run \"Convert.py\".  It is not converting it to a new format.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-05-23T21:40:46+00:00",
    "closed_at": "2023-05-23T23:14:32+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1577/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/1577"
  },
  {
    "number": 7939,
    "title": "Bug: Weird output from CodeQwen converted from safetensors and unrecognized BPE pre-tokenizer for CodeQwen",
    "body": "### What happened?\n\nWhen trying to convert CodeQwen safetensors into GGUF, I get this error:\r\n```\r\npython3.10 convert-hf-to-gguf.py ./models--Qwen--CodeQwen1.5-7B-Chat/snapshots/7b0cc3380fe815e6f08fe2f80c03e05a8b1883d8/ --outfile test.gguf \r\n\r\nINFO:hf-to-gguf:Loading model: 7b0cc3380fe815e6f08fe2f80c03e05a8b1883d8\r\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\r\nINFO:hf-to-gguf:Set model parameters\r\nINFO:hf-to-gguf:gguf: context length = 65536\r\nINFO:hf-to-gguf:gguf: embedding length = 4096\r\nINFO:hf-to-gguf:gguf: feed forward length = 13440\r\nINFO:hf-to-gguf:gguf: head count = 32\r\nINFO:hf-to-gguf:gguf: key-value head count = 4\r\nINFO:hf-to-gguf:gguf: rope theta = 1000000\r\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\r\nINFO:hf-to-gguf:gguf: file type = 1\r\nINFO:hf-to-gguf:Set model tokenizer\r\nWARNING:hf-to-gguf:\r\n\r\nWARNING:hf-to-gguf:**************************************************************************************\r\nWARNING:hf-to-gguf:** WARNING: The BPE pre-tokenizer was not recognized!\r\nWARNING:hf-to-gguf:**          There are 2 possible reasons for this:\r\nWARNING:hf-to-gguf:**          - the model has not been added to convert-hf-to-gguf-update.py yet\r\nWARNING:hf-to-gguf:**          - the pre-tokenization config has changed upstream\r\nWARNING:hf-to-gguf:**          Check your model files and convert-hf-to-gguf-update.py and update them accordingly.\r\nWARNING:hf-to-gguf:** ref:     https://github.com/ggerganov/llama.cpp/pull/6920\r\nWARNING:hf-to-gguf:**\r\nWARNING:hf-to-gguf:** chkhsh:  cd88bc280b3debbdddec3304015afd7e215c61d674846a2dac7271275384810c\r\nWARNING:hf-to-gguf:**************************************************************************************\r\nWARNING:hf-to-gguf:\r\n\r\nTraceback (most recent call last):\r\n  File \"./llama.cpp/convert-hf-to-gguf.py\", line 1622, in set_vocab\r\n    self._set_vocab_sentencepiece()\r\n  File \"./llama.cpp/convert-hf-to-gguf.py\", line 582, in _set_vocab_sentencepiece\r\n    raise FileNotFoundError(f\"File not found: {tokenizer_path}\")\r\nFileNotFoundError: File not found: ./models--Qwen--CodeQwen1.5-7B-Chat/snapshots/7b0cc3380fe815e6f08fe2f80c03e05a8b1883d8/tokenizer.model\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"./llama.cpp/convert-hf-to-gguf.py\", line 2881, in <module>\r\n    main()\r\n  File \"./llama.cpp/convert-hf-to-gguf.py\", line 2866, in main\r\n    model_instance.set_vocab()\r\n  File \"./llama.cpp/convert-hf-to-gguf.py\", line 1624, in set_vocab\r\n    self._set_vocab_gpt2()\r\n  File \"./llama.cpp/convert-hf-to-gguf.py\", line 509, in _set_vocab_gpt2\r\n    tokens, toktypes, tokpre = self.get_vocab_base()\r\n  File \"./llama.cpp/convert-hf-to-gguf.py\", line 382, in get_vocab_base\r\n    tokpre = self.get_vocab_base_pre(tokenizer)\r\n  File \"./llama.cpp/convert-hf-to-gguf.py\", line 500, in get_vocab_base_pre\r\n    raise NotImplementedError(\"BPE pre-tokenizer was not recognized - update get_vocab_base_pre()\")\r\nNotImplementedError: BPE pre-tokenizer was not recognized - update get_vocab_base_pre()\r\n```\r\nWhen replacing the chkhsh from this (codeqwen) to qwen2 (or by adding codeqwen into convert-hf-to-gguf-update.py and adding new lines to llama.cpp and llama.h corresponding to qwen2), it works, and quantization also works, however, the output is with some unknown bytes. I've named the quantized model \"testbase\".\r\n```\r\nLog start\r\nmain: build = 3148 (e65bbf60)\r\nmain: built with cc (GCC) 14.1.1 20240522 for x86_64-pc-linux-gnu\r\nmain: seed  = 1718375838\r\nllama_model_loader: loaded meta data with 24 key-value pairs and 387 tensors from ./testbase (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\r\nllama_model_loader: - kv   1:                               general.name str              = 7b0cc3380fe815e6f08fe2f80c03e05a8b1883d8\r\nllama_model_loader: - kv   2:                          qwen2.block_count u32              = 32\r\nllama_model_loader: - kv   3:                       qwen2.context_length u32              = 65536\r\nllama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 4096\r\nllama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 13440\r\nllama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 32\r\nllama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 4\r\nllama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000,000000\r\nllama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0,000010\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 2\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = codeqwen\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,92416]   = [\"<unk>\", \"<s>\", \"<|endoftext|>\", \"<|...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,92416]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\r\nllama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,112005]  = [\"\u2581 t\", \"\u2581 a\", \"i n\", \"h e\", \"e r...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 4\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 92298\r\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\r\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:  161 tensors\r\nllama_model_loader: - type q4_0:  225 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special tokens cache size = 1507\r\nllm_load_vocab: token to piece cache size = 3,6114 MB\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = qwen2\r\nllm_load_print_meta: vocab type       = BPE\r\nllm_load_print_meta: n_vocab          = 92416\r\nllm_load_print_meta: n_merges         = 112005\r\nllm_load_print_meta: n_ctx_train      = 65536\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 4\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 8\r\nllm_load_print_meta: n_embd_k_gqa     = 512\r\nllm_load_print_meta: n_embd_v_gqa     = 512\r\nllm_load_print_meta: f_norm_eps       = 0,0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1,0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0,0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0,0e+00\r\nllm_load_print_meta: f_logit_scale    = 0,0e+00\r\nllm_load_print_meta: n_ff             = 13440\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 2\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000,0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_ctx_orig_yarn  = 65536\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q4_0\r\nllm_load_print_meta: model params     = 7,25 B\r\nllm_load_print_meta: model size       = 3,89 GiB (4,61 BPW) \r\nllm_load_print_meta: general.name     = 7b0cc3380fe815e6f08fe2f80c03e05a8b1883d8\r\nllm_load_print_meta: BOS token        = 2 '<|endoftext|>'\r\nllm_load_print_meta: EOS token        = 4 '<|im_end|>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 92298 '<fim_pad>'\r\nllm_load_print_meta: LF token         = 35526 '\u00c4'\r\nllm_load_print_meta: EOT token        = 2 '<|endoftext|>'\r\nllm_load_tensors: ggml ctx size =    0,18 MiB\r\nllm_load_tensors:        CPU buffer size =  3983,84 MiB\r\n..........................................................................................\r\nllama_new_context_with_model: n_ctx      = 65536\r\nllama_new_context_with_model: n_batch    = 2048\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000,0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:        CPU KV buffer size =  4096,00 MiB\r\nllama_new_context_with_model: KV self size  = 4096,00 MiB, K (f16): 2048,00 MiB, V (f16): 2048,00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0,35 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =  4256,01 MiB\r\nllama_new_context_with_model: graph nodes  = 1126\r\nllama_new_context_with_model: graph splits = 1\r\n\r\nsystem_info: n_threads = 6 / 12 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \r\nmain: interactive mode on.\r\nsampling: \r\n        repeat_last_n = 64, repeat_penalty = 1,000, frequency_penalty = 0,000, presence_penalty = 0,000\r\n        top_k = 40, tfs_z = 1,000, top_p = 0,950, min_p = 0,050, typical_p = 1,000, temp = 0,800\r\n        mirostat = 0, mirostat_lr = 0,100, mirostat_ent = 5,000\r\nsampling order: \r\nCFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \r\ngenerate: n_ctx = 65536, n_batch = 2048, n_predict = 512, n_keep = 0\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to the AI.\r\n - To return control without starting a new line, end your input with '/'.\r\n - If you want to submit another line, end your input with '\\'.\r\n\r\nYou are a helpful assistant.\r\n<?php //Create a Hook for EspoCRM for a module called Tester that checks if Entity name has length divisible by 3.\r\n<0x0A>You[UNK_BYTE_0xe29681\u2581are]are[UNK_BYTE_0xe29681\u2581a]a[UNK_BYTE_0xe29681\u2581helpful]helpful[UNK_BYTE_0xe29681\u2581assistant]assistant.\r\n```\r\n\r\nSteps to reproduce the BFE pretokenizer bug:\r\n1. Download Qwen/CodeQwen1.5-7B-Chat from huggingface\r\n2. Run convert-hf-to-gguf.py on the model\r\n\r\nSteps to reproduce the weird output bug:\r\n1. Download Qwen/CodeQwen1.5-7B-Chat from huggingface\r\n2. Replace the qwen2 chkhsh with cd88bc280b3debbdddec3304015afd7e215c61d674846a2dac7271275384810c\r\n3. Run convert-hf-to-gguf.py on the model\r\n4. Run ./llama-quantize <converted_model> <name_of_quantized_model> q4_0\r\n5. Run ./llama-cli -m <name_of_quantized_model> -n 512 --color -i -f prompts/chat-with-qwen.txt\r\n\r\nI'm not very skilled with AI, so I might be missing something important.\n\n### Name and Version\n\n./llama-cli --version\r\nversion: 3148 (e65bbf60)\r\nbuilt with cc (GCC) 14.1.1 20240522 for x86_64-pc-linux-gnu\r\n\n\n### What operating system are you seeing the problem on?\n\nLinux\n\n### Relevant log output\n\n_No response_",
    "labels": [
      "bug-unconfirmed",
      "high severity"
    ],
    "state": "closed",
    "created_at": "2024-06-14T15:22:09+00:00",
    "closed_at": "2024-06-15T06:48:49+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/7939/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/7939"
  },
  {
    "number": 14021,
    "title": "Eval bug: llama-server -hf nomic-ai/nomic-embed-text-v2-moe-GGUF --embeddings , broken on latest version",
    "body": "### Name and Version\n\n`llama-server -hf nomic-ai/nomic-embed-text-v2-moe-GGUF:Q4_K_M --embeddings`\nthis version is OK\n```\nllama-server --version\nversion: 5569 (e57bb87c)\nbuilt with cc (GCC) 11.5.0 20240719 (Red Hat 11.5.0-2.0.1) for x86_64-redhat-linux\n\n```\n\nAll subsequent versions include latest version have issues.\n\n### Operating systems\n\nLinux\n\n### GGML backends\n\nCPU\n\n### Hardware\n\nINTEL(R) XEON(R) GOLD 6530\n\n### Models\n\n_No response_\n\n### Problem description & steps to reproduce\n\n`llama-server -hf nomic-ai/nomic-embed-text-v2-moe-GGUF:Q4_K_M --embeddings`\nfail load model\n\n### First Bad Commit\n\n_No response_\n\n### Relevant log output\n\n```shell\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 321.66 MiB (5.68 BPW)\nload: model vocab missing newline token, using special_pad_id instead\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 4\nload: token to piece cache size = 2.1668 MB\nllama_model_load: error loading model: error loading model vocabulary: _Map_base::at\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model '/root/.cache/llama.cpp/nomic-ai_nomic-embed-text-v2-moe-GGUF_nomic-embed-text-v2-moe.Q4_K_M.gguf'\nsrv    load_model: failed to load model, '/root/.cache/llama.cpp/nomic-ai_nomic-embed-text-v2-moe-GGUF_nomic-embed-text-v2-moe.Q4_K_M.gguf'\nsrv    operator(): operator(): cleaning up before exit...\nmain: exiting due to model loading error\n```",
    "labels": [
      "bug-unconfirmed"
    ],
    "state": "closed",
    "created_at": "2025-06-05T06:20:34+00:00",
    "closed_at": "2025-06-05T07:29:19+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/14021/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/14021"
  },
  {
    "number": 410,
    "title": "Download ggml-alpaca-7b-q4.bin failed CHECKSUM",
    "body": "This may well be the end server issue. I tried several times with no luck, just wonder if people have seen this. \r\nI tried all 3 curl commands. \r\n\r\n",
    "labels": [
      "model"
    ],
    "state": "closed",
    "created_at": "2023-03-22T21:31:37+00:00",
    "closed_at": "2023-03-23T09:22:24+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/410/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/410"
  },
  {
    "number": 13467,
    "title": "Segfault when submitting image to ggml-org/Qwen2.5-VL-7B-Instruct-GGUF",
    "body": "I am getting the following error/segfault\nwhen submitting an image to ggml-org/Qwen2.5-VL-7B-Instruct-GGUF\nusing llama-server and the Python llm package as a client.\n\nThe same image is processed perfectly fine with\nggml-org/gemma-3-4b-it-GGUF\nand\nggml-org/SmolVLM2-2.2B-Instruct-GGUF.\n\nThis is the output of \"identify\" on the image file:\nneocube-one-layer-pattern.jpg JPEG 2592x1944 2592x1944+0+0 8-bit sRGB 858245B 0.000u 0:00.000\n\nAnd here is the output I get from llama-server before it segfaults.\nNote that it wants to allocate 44GB RAM, which is likely an error somewhere.\n\n```\nslot launch_slot_: id  0 | task 0 | processing task\nslot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 11\nslot update_slots: id  0 | task 0 | kv cache rm [0, end)\nslot update_slots: id  0 | task 0 | prompt processing progress, n_past = 4, n_tokens = 4, progress = 0.363636\nencoding image or slice...\nslot update_slots: id  0 | task 0 | kv cache rm [4, end)\nsrv  process_chun: processing image...\nggml_aligned_malloc: insufficient memory (attempted to allocate 44668.09 MB)\nggml_backend_cpu_buffer_type_alloc_buffer: failed to allocate buffer of size 46837887616\nggml_gallocr_reserve_n: failed to allocate CPU buffer of size 46837887616\nggml_aligned_malloc: insufficient memory (attempted to allocate 44668.09 MB)\nggml_backend_cpu_buffer_type_alloc_buffer: failed to allocate buffer of size 46837887616\nggml_gallocr_reserve_n: failed to allocate CPU buffer of size 46837887616\nmake: *** [Makefile:20: llm-api] Segmentation fault\n```\n",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-12T05:25:09+00:00",
    "closed_at": "2025-05-12T13:06:52+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/13467/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/13467"
  },
  {
    "number": 3239,
    "title": "Missing \"scripts\" module, but scripts appears to be a folder with some code.",
    "body": "I just updated my copy from the repo today, and everything stopped working.\r\nUbuntu 20.04\r\ngcc/g++ are 9.x.x\r\n\r\n(CodeLlama) developer@ai:~/llama.cpp$ make clean\r\nTraceback (most recent call last):\r\n  File \"/home/developer/mambaforge/envs/CodeLlama/bin/make\", line 5, in <module>\r\n    from scripts.proto import main\r\nModuleNotFoundError: No module named 'scripts.proto'\r\n(CodeLlama) developer@ai:~/llama.cpp$ vi Makefile\r\n(CodeLlama) developer@ai:~/llama.cpp$ LLAMA_CUBLAS=1 make\r\nTraceback (most recent call last):\r\n  File \"/home/developer/mambaforge/envs/CodeLlama/bin/make\", line 5, in <module>\r\n    from scripts.proto import main\r\nModuleNotFoundError: No module named 'scripts.proto'\r\n(CodeLlama) developer@ai:~/llama.cpp$\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-09-18T00:27:05+00:00",
    "closed_at": "2023-09-18T02:31:52+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3239/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/ggml-org/llama.cpp/issues/3239"
  }
]